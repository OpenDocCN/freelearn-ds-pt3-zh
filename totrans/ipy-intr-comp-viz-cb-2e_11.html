<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 11. Image and Audio Processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Image and Audio Processing</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Manipulating the exposure of an image</li><li class="listitem" style="list-style-type: disc">Applying filters on an image</li><li class="listitem" style="list-style-type: disc">Segmenting an image</li><li class="listitem" style="list-style-type: disc">Finding points of interest in an image</li><li class="listitem" style="list-style-type: disc">Detecting faces in an image with OpenCV</li><li class="listitem" style="list-style-type: disc">Applying digital filters to speech sounds</li><li class="listitem" style="list-style-type: disc">Creating a sound synthesizer in the notebook</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec92"/>Introduction</h1></div></div></div><p>In the previous chapter, we covered signal processing techniques for one-dimensional, time-dependent signals. In this chapter, we will see signal processing techniques for images and sounds.</p><p>Generic signal processing techniques can be applied to images and sounds, but many image or audio processing tasks require specialized algorithms. For example, we will see algorithms for segmenting images, detecting points of interest in an image, or detecting faces. We will also hear the effect of linear filters on speech sounds.</p><p>
<span class="strong"><strong>scikit-image</strong></span><a id="id1632" class="indexterm"/> is one of the main image processing packages in Python. We will use it in most of the image processing recipes in this chapter. For more on scikit-image, <a id="id1633" class="indexterm"/>refer to <a class="ulink" href="http://scikit-image.org">http://scikit-image.org</a>.</p><p>We will also use <span class="strong"><strong>OpenCV</strong></span><a id="id1634" class="indexterm"/> (<a class="ulink" href="http://opencv.org">http://opencv.org</a>), a C++ computer vision<a id="id1635" class="indexterm"/> library that has a Python wrapper. It implements algorithms for specialized image and video processing tasks, but it can be a bit difficult to use. An interesting (and simpler) alternative is <a id="id1636" class="indexterm"/><span class="strong"><strong>SimpleCV</strong></span> (<a class="ulink" href="http://simplecv.org">http://simplecv.org</a>).</p><p>In this introduction, we will discuss the particularities of images and sounds from a signal processing point of view.</p><div class="section" title="Images"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec348"/>Images</h2></div></div></div><p>A <span class="strong"><strong>grayscale</strong></span><a id="id1637" class="indexterm"/> image<a id="id1638" class="indexterm"/> is a bidimensional signal represented by a function, <span class="emphasis"><em>f</em></span>, that maps each pixel to an <a id="id1639" class="indexterm"/><span class="strong"><strong>intensity</strong></span>. The intensity can be a real value between 0 (dark) and 1 (light). In a colored image, this function maps each pixel to a triplet of intensities, generally, the <span class="strong"><strong>red</strong></span>, <span class="strong"><strong>green</strong></span>, and <span class="strong"><strong>blue</strong></span> (<span class="strong"><strong>RGB</strong></span>)<a id="id1640" class="indexterm"/> components.</p><p>On a computer, images are digitally sampled. The intensities are no longer real values, but integers or floating point numbers. On one hand, the mathematical formulation of continuous functions allows us to apply analytical tools such as derivatives and integrals. On the other hand, we need to take into account the digital nature of the images we deal with.</p></div><div class="section" title="Sounds"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec349"/>Sounds</h2></div></div></div><p>From a signal processing perspective, a <a id="id1641" class="indexterm"/>sound is a time-dependent signal that has sufficient power in the hearing frequency range (about 20 Hz to 20 kHz). Then, according to the Nyquist-Shannon theorem (introduced in <a class="link" href="ch10.html" title="Chapter 10. Signal Processing">Chapter 10</a>, <span class="emphasis"><em>Signal Processing</em></span>), the sampling rate of a digital sound signal needs to be at least 40 kHz. A sampling rate of 44100 Hz is frequently chosen.</p></div><div class="section" title="References"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec350"/>References</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Image processing<a id="id1642" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Image_processing">http://en.wikipedia.org/wiki/Image_processing</a></li><li class="listitem" style="list-style-type: disc">Advanced image processing <a id="id1643" class="indexterm"/>algorithms, by Gabriel Peyré, available at <a class="ulink" href="https://github.com/gpeyre/numerical-tours">https://github.com/gpeyre/numerical-tours</a></li><li class="listitem" style="list-style-type: disc">Audio signal processing<a id="id1644" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Audio_signal_processing">http://en.wikipedia.org/wiki/Audio_signal_processing</a></li><li class="listitem" style="list-style-type: disc">Particularities of the 44100 Hz sampling rate<a id="id1645" class="indexterm"/> explained at <a class="ulink" href="http://en.wikipedia.org/wiki/44,100_Hz">http://en.wikipedia.org/wiki/44,100_Hz</a></li></ul></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Manipulating the exposure of an image"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec93"/>Manipulating the exposure of an image</h1></div></div></div><p>The <span class="strong"><strong>exposure</strong></span> of an image tells <a id="id1646" class="indexterm"/>us whether the image is too dark, too light, or balanced. It can be measured with a histogram of the intensity values of all pixels. Improving the exposure of an image is a basic image-editing operation. As we will see in this recipe, that can be done easily with scikit-image.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec351"/>Getting ready</h2></div></div></div><p>You need scikit-image<a id="id1647" class="indexterm"/> for this recipe. You will find the installation instructions at <a class="ulink" href="http://scikit-image.org/download.html">http://scikit-image.org/download.html</a>. With Anaconda, you can just type <code class="literal">conda install scikit-image</code> in a terminal.</p><p>You also need to download the <span class="emphasis"><em>Beach</em></span> dataset from the book's GitHub repository at <a class="ulink" href="https://github.com/ipython-books/cookbook-data">https://github.com/ipython-books/cookbook-data</a>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec352"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import the packages:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import matplotlib.pyplot as plt
        import skimage.exposure as skie
        %matplotlib inline</pre></div></li><li class="listitem">We open an image with matplotlib. We only take a single RGB component to have a grayscale image (there are better ways to convert a colored image to a grayscale image):<div class="informalexample"><pre class="programlisting">In [2]: img = plt.imread('data/pic1.jpg')[...,0]</pre></div></li><li class="listitem">We create a function that displays the image along with its <span class="strong"><strong>histogram</strong></span><a id="id1648" class="indexterm"/> of the intensity values (that is, the exposure):<div class="informalexample"><pre class="programlisting">In [3]: def show(img):
            # Display the image.
            plt.subplot(121)
            plt.imshow(img, cmap=plt.cm.gray)
            plt.axis('off')
            # Display the histogram.
            plt.subplot(122)
            plt.hist(img.ravel(), lw=0, bins=256)
            plt.xlim(0, img.max())
            plt.yticks([])
            plt.show()</pre></div></li><li class="listitem">Let's display the image along with its histogram:<div class="informalexample"><pre class="programlisting">In [4]: show(img)</pre></div><div class="mediaobject"><img src="images/4818OS_11_01.jpg" alt="How to do it..."/><div class="caption"><p>An image and its histogram</p></div></div><p>The histogram is unbalanced and the image appears overexposed (many pixels are too bright).</p></li><li class="listitem">Now, we <a id="id1649" class="indexterm"/>rescale the intensity of the image using scikit-image's <code class="literal">rescale_intensity</code> function. The <code class="literal">in_range</code> and <code class="literal">out_range</code> parameters define a linear mapping from the original image to the modified image. The pixels that are outside <code class="literal">in_range</code> are clipped to the extremal values of <code class="literal">out_range</code>. Here, the darkest pixels (intensity less than 100) become completely black (0), whereas the brightest pixels (&gt;240) become completely white (255):<div class="informalexample"><pre class="programlisting">In [5]: show(skie.rescale_intensity(img,
             in_range=(100, 240), out_range=(0, 255)))</pre></div><div class="mediaobject"><img src="images/4818OS_11_02.jpg" alt="How to do it..."/><div class="caption"><p>A crude exposure manipulation technique</p></div></div><p>Many intensity values seem to be missing in the histogram, which reflects the poor quality of this basic exposure correction technique.</p></li><li class="listitem">We now use a more advanced exposure correction technique called<a id="id1650" class="indexterm"/> <span class="strong"><strong>Contrast Limited Adaptive Histogram Equalization</strong></span> (<span class="strong"><strong>CLAHE</strong></span>):<div class="informalexample"><pre class="programlisting">In [6]: show(skie.equalize_adapthist(img))</pre></div><div class="mediaobject"><img src="images/4818OS_11_03.jpg" alt="How to do it..."/><div class="caption"><p>Result of the Contrast Limited Adaptive Histogram Equalization method for exposure correction</p></div></div><p>The histogram seems more balanced, and the image now appears more contrasted.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec353"/>How it works...</h2></div></div></div><p>An image's histogram<a id="id1651" class="indexterm"/> represents the distribution of the pixels' intensity values. It is a central tool in image editing, image processing, and computer vision.</p><p>The <code class="literal">rescale_intensity()</code> function<a id="id1652" class="indexterm"/> stretches or shrinks the intensity levels of the image. One use case is to ensure that the whole range of values allowed by the data type is used by the image.</p><p>The <code class="literal">equalize_adapthist()</code> function<a id="id1653" class="indexterm"/> works by splitting the image into rectangular sections and computing the histogram for each section. Then, the intensity values of the pixels are redistributed to improve the contrast and enhance the details.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec354"/>There's more...</h2></div></div></div><p>Here are some references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Image histogram<a id="id1654" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Image_histogram">http://en.wikipedia.org/wiki/Image_histogram</a></li><li class="listitem" style="list-style-type: disc">Histogram equalization <a id="id1655" class="indexterm"/>on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Histogram_equalization">http://en.wikipedia.org/wiki/Histogram_equalization</a></li><li class="listitem" style="list-style-type: disc">Adaptive histogram equalization<a id="id1656" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Adaptive_histogram_equalization">http://en.wikipedia.org/wiki/Adaptive_histogram_equalization</a></li><li class="listitem" style="list-style-type: disc">Contrast<a id="id1657" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Contrast_(vision)">http://en.wikipedia.org/wiki/Contrast_(vision)</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec355"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Applying filters on an image</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Applying filters on an image"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec94"/>Applying filters on an image</h1></div></div></div><p>In this recipe, we apply filters on an <a id="id1658" class="indexterm"/>image for various purposes: blurring, denoising, and<a id="id1659" class="indexterm"/> edge detection.</p><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec356"/>How it works...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import the packages:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import matplotlib.pyplot as plt
        import skimage
        import skimage.filter as skif
        import skimage.data as skid
        %matplotlib inline</pre></div></li><li class="listitem">We create a function that displays a grayscale image:<div class="informalexample"><pre class="programlisting">In [2]: def show(img):
            plt.imshow(img, cmap=plt.cm.gray)
            plt.axis('off')
            plt.show()</pre></div></li><li class="listitem">Now, we load the Lena image (bundled in scikit-image). We select a single RGB component to get a grayscale image:<div class="informalexample"><pre class="programlisting">In [3]: img = skimage.img_as_float(skid.lena())[...,0]
In [4]: show(img)</pre></div><div class="mediaobject"><img src="images/4818OS_11_04.jpg" alt="How it works..."/></div></li><li class="listitem">Let's apply a blurring <span class="strong"><strong>Gaussian filter</strong></span><a id="id1660" class="indexterm"/> to the image:<div class="informalexample"><pre class="programlisting">In [5]: show(skif.gaussian_filter(img, 5.))</pre></div><div class="mediaobject"><img src="images/4818OS_11_05.jpg" alt="How it works..."/></div></li><li class="listitem">We now apply a <span class="strong"><strong>Sobel filter</strong></span><a id="id1661" class="indexterm"/> that enhances the edges in the image:<div class="informalexample"><pre class="programlisting">In [6]: sobimg = skif.sobel(img)
        show(sobimg)</pre></div><div class="mediaobject"><img src="images/4818OS_11_06.jpg" alt="How it works..."/></div></li><li class="listitem">We can threshold<a id="id1662" class="indexterm"/> the filtered image to get a <span class="emphasis"><em>sketch effect</em></span>. We <a id="id1663" class="indexterm"/>obtain a binary image that only contains the edges. We use a notebook widget to find an adequate thresholding value; by adding the <code class="literal">@interact</code> decorator, we display a slider on top of the image. This widget lets us control the threshold dynamically.<div class="informalexample"><pre class="programlisting">In [7]: from IPython.html import widgets
        @widgets.interact(x=(0.01, .4, .005))
        def edge(x):
            show(sobimg&lt;x)</pre></div><div class="mediaobject"><img src="images/4818OS_11_07.jpg" alt="How it works..."/></div></li><li class="listitem">Finally, we add some<a id="id1664" class="indexterm"/> noise to the image to illustrate the effect of a denoising filter:<div class="informalexample"><pre class="programlisting">In [8]: img = skimage.img_as_float(skid.lena())
        # We take a portion of the image to show the
        # details.
        img = img[200:-100, 200:-150]
        # We add Gaussian noise.
        img = np.clip(img + 0.3*np.random.rand(*img.shape),
                      0, 1)
In [9]: show(img)</pre></div><div class="mediaobject"><img src="images/4818OS_11_08.jpg" alt="How it works..."/></div></li><li class="listitem">The <code class="literal">denoise_tv_bregman()</code> function<a id="id1665" class="indexterm"/> implements total-variation denoising using the <a id="id1666" class="indexterm"/>Split Bregman method:<div class="informalexample"><pre class="programlisting">In [10]: show(skimage.restoration.denoise_tv_bregman(img,
                                                     5.))</pre></div><div class="mediaobject"><img src="images/4818OS_11_09.jpg" alt="How it works..."/></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec357"/>How it works...</h2></div></div></div><p>Many filters <a id="id1667" class="indexterm"/>used in image processing are linear filters. These filters are<a id="id1668" class="indexterm"/> very similar to those seen in <a class="link" href="ch10.html" title="Chapter 10. Signal Processing">Chapter 10</a>, <span class="emphasis"><em>Signal Processing</em></span>; the only difference is that they work in two dimensions. Applying a linear filter to an image amounts to performing a discrete <span class="strong"><strong>convolution</strong></span><a id="id1669" class="indexterm"/> of the image with a particular function. The Gaussian filter applies a convolution with a Gaussian function to blur the image.</p><p>The Sobel filter computes an approximation of the gradient of the image. Therefore, it can detect fast-varying spatial changes in the image, which generally correspond to edges.</p><p>
<span class="strong"><strong>Image denoising</strong></span><a id="id1670" class="indexterm"/> refers to the process of removing noise from an image. <span class="strong"><strong>Total variation denoising</strong></span><a id="id1671" class="indexterm"/> works by finding a <span class="emphasis"><em>regular</em></span> image close to the original (noisy) image. Regularity is quantified by the <span class="strong"><strong>total variation</strong></span><a id="id1672" class="indexterm"/> of the image:</p><div class="mediaobject"><img src="images/4818OS_11_10.jpg" alt="How it works..."/></div><p>The <span class="strong"><strong>Split Bregman method</strong></span><a id="id1673" class="indexterm"/> is a variant based on the L<sup>1</sup> norm. It is an instance of<a id="id1674" class="indexterm"/> <span class="strong"><strong>compressed sensing</strong></span>, which aims to find regular and sparse approximations of real-world noisy measurements.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec358"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">API reference of the skimage.filter module<a id="id1675" class="indexterm"/> available at <a class="ulink" href="http://scikit-image.org/docs/dev/api/skimage.filter.html">http://scikit-image.org/docs/dev/api/skimage.filter.html</a></li><li class="listitem" style="list-style-type: disc">Noise reduction<a id="id1676" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Noise_reduction">http://en.wikipedia.org/wiki/Noise_reduction</a></li><li class="listitem" style="list-style-type: disc">Gaussian filter<a id="id1677" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Gaussian_filter">http://en.wikipedia.org/wiki/Gaussian_filter</a></li><li class="listitem" style="list-style-type: disc">Sobel filter<a id="id1678" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Sobel_operator">http://en.wikipedia.org/wiki/Sobel_operator</a></li><li class="listitem" style="list-style-type: disc">Image denoising<a id="id1679" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Noise_reduction">http://en.wikipedia.org/wiki/Noise_reduction</a></li><li class="listitem" style="list-style-type: disc">Total variation denoising<a id="id1680" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Total_variation_denoising">http://en.wikipedia.org/wiki/Total_variation_denoising</a></li><li class="listitem" style="list-style-type: disc">The Split Bregman algorithm<a id="id1681" class="indexterm"/> explained at <a class="ulink" href="http://www.ece.rice.edu/~tag7/Tom_Goldstein/Split_Bregman.html">www.ece.rice.edu/~tag7/Tom_Goldstein/Split_Bregman.html</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec359"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Manipulating the exposure of an image</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Segmenting an image"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec95"/>Segmenting an image</h1></div></div></div><p>Image segmentation consists of <a id="id1682" class="indexterm"/>partitioning an image into different regions that share certain characteristics. This is a fundamental task in computer vision, facial recognition, and medical imaging. For example, an image segmentation algorithm can automatically detect the contours of an organ in a medical image.</p><p>scikit-image provides several segmentation methods. In this recipe, we will demonstrate how to segment an image containing different objects.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec360"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import the packages:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import matplotlib.pyplot as plt
        from skimage.data import coins
        from skimage.filter import threshold_otsu
        from skimage.segmentation import clear_border
        from skimage.morphology import closing, square
        from skimage.measure import regionprops, label
        from skimage.color import lab2rgb
        %matplotlib inline</pre></div></li><li class="listitem">We create a function that displays a grayscale image:<div class="informalexample"><pre class="programlisting">In [2]: def show(img, cmap=None):
            cmap = cmap or plt.cm.gray
            plt.imshow(img, cmap=cmap)
            plt.axis('off')
            plt.show()</pre></div></li><li class="listitem">We get a test image <a id="id1683" class="indexterm"/>bundled in scikit-image, showing various coins on a plain background:<div class="informalexample"><pre class="programlisting">In [3]: img = coins()
In [4]: show(img)</pre></div><div class="mediaobject"><img src="images/4818OS_11_11.jpg" alt="How to do it..."/></div></li><li class="listitem">The first step to segment the image is finding an intensity threshold separating the (bright) coins from the (dark) background. <span class="strong"><strong>Otsu's method</strong></span> defines a simple algorithm to automatically find such a threshold.<div class="informalexample"><pre class="programlisting">In [5]: threshold_otsu(img)
Out[5]: 107
In [6]: show(img&gt;107)</pre></div><div class="mediaobject"><img src="images/4818OS_11_12.jpg" alt="How to do it..."/><div class="caption"><p>The thresholded image using Otsu's method</p></div></div></li><li class="listitem">There appears to be a <a id="id1684" class="indexterm"/>problem in the top-left corner of the image, with part of the background being too bright. Let's use a notebook widget to find a better threshold:<div class="informalexample"><pre class="programlisting">In [7]: from IPython.html import widgets
        @widgets.interact(t=(10, 240))
        def threshold(t):
            show(img&gt;t)</pre></div><div class="mediaobject"><img src="images/4818OS_11_13.jpg" alt="How to do it..."/><div class="caption"><p>The thresholded image using a manually selected threshold</p></div></div></li><li class="listitem">The threshold 120 looks better. The next step consists of cleaning the binary image by smoothing the coins and removing the border. scikit-image contains a few functions for these purposes.<div class="informalexample"><pre class="programlisting">In [8]: img_bin = clear_border(closing(img&gt;120, square(5)))
        show(img_bin)</pre></div><div class="mediaobject"><img src="images/4818OS_11_14.jpg" alt="How to do it..."/><div class="caption"><p>The thresholded image with cleared borders</p></div></div></li><li class="listitem">Next, we perform the <a id="id1685" class="indexterm"/>segmentation task itself with the <code class="literal">label()</code> function. This function detects the connected components in the image and attributes a unique label to every component. Here, we color code the labels in the binary image:<div class="informalexample"><pre class="programlisting">In [9]: labels = label(img_bin)
        show(labels, cmap=plt.cm.rainbow)</pre></div><div class="mediaobject"><img src="images/4818OS_11_15.jpg" alt="How to do it..."/><div class="caption"><p>The segmented image</p></div></div></li><li class="listitem">Small artifacts in the image result in spurious labels that do not correspond to coins. Therefore, we only keep components with more than 100 pixels. The <code class="literal">regionprops()</code> function<a id="id1686" class="indexterm"/> allows us to retrieve specific properties of the components (here, the area and the bounding box):<div class="informalexample"><pre class="programlisting">In [10]: regions = regionprops(labels, 
                               ['Area', 'BoundingBox'])
         boxes = np.array([label['BoundingBox']
                           for label in regions 
                               if label['Area'] &gt; 100])
         print("There are {0:d} coins.".format(len(boxes)))
There are 24 coins.</pre></div></li><li class="listitem">Finally, we show the <a id="id1687" class="indexterm"/>label number on top of each component in the original image:<div class="informalexample"><pre class="programlisting">In [11]: plt.imshow(img, cmap=plt.cm.gray)
         plt.axis('off')
         xs = boxes[:,[1,3]].mean(axis=1)
         ys = boxes[:,[0,2]].mean(axis=1)
         for i, box in enumerate(boxes):
             plt.text(xs[i]-5, ys[i]+5, str(i))</pre></div><div class="mediaobject"><img src="images/4818OS_11_16.jpg" alt="How to do it..."/></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec361"/>How it works...</h2></div></div></div><p>To clean up the coins in the thresholded image, we used <span class="strong"><strong>mathematical morphology</strong></span><a id="id1688" class="indexterm"/> techniques. These methods, based on set theory, geometry, and topology, allow us to manipulate shapes.</p><p>For example, let's explain <span class="strong"><strong>dilation</strong></span><a id="id1689" class="indexterm"/> and <a id="id1690" class="indexterm"/><span class="strong"><strong>erosion</strong></span>. First, if <span class="emphasis"><em>A</em></span> is a set of pixels in an image, and <span class="emphasis"><em>b</em></span> is a 2D vector, we denote <span class="emphasis"><em>A<sub>b</sub></em></span> the set <span class="emphasis"><em>A</em></span> translated by <span class="emphasis"><em>b</em></span> as:</p><div class="mediaobject"><img src="images/4818OS_11_17.jpg" alt="How it works..."/></div><p>Let <span class="emphasis"><em>B</em></span> be a set of vectors with integer components. We call <span class="emphasis"><em>B</em></span> the <span class="strong"><strong>structuring element</strong></span><a id="id1691" class="indexterm"/> (here, we used a square). This set represents the neighborhood of a pixel. The dilation of <span class="emphasis"><em>A</em></span> by <span class="emphasis"><em>B</em></span> is:</p><div class="mediaobject"><img src="images/4818OS_11_18.jpg" alt="How it works..."/></div><p>The erosion of <span class="emphasis"><em>A</em></span> by <span class="emphasis"><em>B</em></span> is:</p><div class="mediaobject"><img src="images/4818OS_11_19.jpg" alt="How it works..."/></div><p>A dilation extends a <a id="id1692" class="indexterm"/>set by adding pixels close to its boundaries. An erosion removes the pixels of the set that are too close to the boundaries. The <span class="strong"><strong>closing</strong></span> of a set is a dilation followed by an erosion. This operation can remove small dark spots and connect small bright cracks. In this recipe, we used a square structuring element.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec362"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">SciPy lecture notes<a id="id1693" class="indexterm"/> on image processing available at <a class="ulink" href="http://scipy-lectures.github.io/packages/scikit-image/">http://scipy-lectures.github.io/packages/scikit-image/</a></li><li class="listitem" style="list-style-type: disc">Image segmentation<a id="id1694" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Image_segmentation">http://en.wikipedia.org/wiki/Image_segmentation</a></li><li class="listitem" style="list-style-type: disc">Otsu's method<a id="id1695" class="indexterm"/> to find a threshold explained at <a class="ulink" href="http://en.wikipedia.org/wiki/Otsu's_method">http://en.wikipedia.org/wiki/Otsu's_method</a></li><li class="listitem" style="list-style-type: disc">Segmentation tutorial with scikit-image<a id="id1696" class="indexterm"/> (which inspired this recipe) available at <a class="ulink" href="http://scikit-image.org/docs/dev/user_guide/tutorial_segmentation.html">http://scikit-image.org/docs/dev/user_guide/tutorial_segmentation.html</a></li><li class="listitem" style="list-style-type: disc">Mathematical morphology<a id="id1697" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Mathematical_morphology">http://en.wikipedia.org/wiki/Mathematical_morphology</a></li><li class="listitem" style="list-style-type: disc">API reference<a id="id1698" class="indexterm"/> of the <code class="literal">skimage.morphology</code> module available at <a class="ulink" href="http://scikit-image.org/docs/dev/api/skimage.morphology.html">http://scikit-image.org/docs/dev/api/skimage.morphology.html</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec363"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Computing connected components in an image</em></span> recipe in <a class="link" href="ch14.html" title="Chapter 14. Graphs, Geometry, and Geographic Information Systems">Chapter 14</a>, <span class="emphasis"><em>Graphs, Geometry, and Geographic Information Systems</em></span></li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Finding points of interest in an image"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec96"/>Finding points of interest in an image</h1></div></div></div><p>In an image, <span class="strong"><strong>points of interest</strong></span><a id="id1699" class="indexterm"/> are positions where there might be edges, corners, or interesting objects. For example, in a landscape picture, points of interest can be located near a house or a person. Detecting points of interest is useful in image recognition, computer vision, or medical imaging.</p><p>In this recipe, we will find points of interest in an image with scikit-image. This will allow us to crop an image around the subject of the picture, even when this subject is not in the center of the image.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec364"/>Getting ready</h2></div></div></div><p>Download the <span class="emphasis"><em>Child</em></span> dataset from the book's GitHub repository at <a class="ulink" href="https://github.com/ipython-books/cookbook-data">https://github.com/ipython-books/cookbook-data</a>, and extract it into the current directory.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec365"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import<a id="id1700" class="indexterm"/> the <a id="id1701" class="indexterm"/>packages:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import matplotlib.pyplot as plt
        import skimage
        import skimage.feature as sf
        %matplotlib inline</pre></div></li><li class="listitem">We create a function to display a colored or grayscale image:<div class="informalexample"><pre class="programlisting">In [2]: def show(img, cmap=None):
            cmap = cmap or plt.cm.gray
            plt.imshow(img, cmap=cmap)
            plt.axis('off')</pre></div></li><li class="listitem">We load an image:<div class="informalexample"><pre class="programlisting">In [3]: img = plt.imread('data/pic2.jpg')
In [4]: show(img)</pre></div><div class="mediaobject"><img src="images/4818OS_11_20.jpg" alt="How to do it..."/></div></li><li class="listitem">Let's find salient <a id="id1702" class="indexterm"/>points in the image with the <a id="id1703" class="indexterm"/>Harris corner method. The first step consists of computing the <span class="strong"><strong>Harris corner measure response image</strong></span><a id="id1704" class="indexterm"/> with the <code class="literal">corner_harris()</code> function (we will explain this measure in <span class="emphasis"><em>How it works...</em></span>). This <a id="id1705" class="indexterm"/>function requires a grayscale image, thus we select the first RGB component:<div class="informalexample"><pre class="programlisting">In [5]: corners = sf.corner_harris(img[:,:,0])
In [6]: show(corners)</pre></div><div class="mediaobject"><img src="images/4818OS_11_21.jpg" alt="How to do it..."/></div><p>We see that the patterns in the child's coat are well detected by this algorithm.</p></li><li class="listitem">The next step is to detect corners from this <a id="id1706" class="indexterm"/>measure image, using the <code class="literal">corner_peaks()</code> function:<div class="informalexample"><pre class="programlisting">In [7]: peaks = sf.corner_peaks(corners)
In [8]: show(img)
        plt.plot(peaks[:,1], peaks[:,0], 'or', ms=4)</pre></div><div class="mediaobject"><img src="images/4818OS_11_22.jpg" alt="How to do it..."/></div></li><li class="listitem">Finally, we <a id="id1707" class="indexterm"/>create a box around the corner points <a id="id1708" class="indexterm"/>to define our region of interest:<div class="informalexample"><pre class="programlisting">In [9]: ymin, xmin = peaks.min(axis=0)
        ymax, xmax = peaks.max(axis=0)
        w, h = xmax-xmin, ymax-ymin
In [10]: k = .25
         xmin -= k*w
         xmax += k*w
         ymin -= k*h
         ymax += k*h
In [11]: show(img[ymin:ymax,xmin:xmax])</pre></div><div class="mediaobject"><img src="images/4818OS_11_23.jpg" alt="How to do it..."/></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec366"/>How it works...</h2></div></div></div><p>Let's explain the method used in this recipe. The first step consists of computing the <span class="strong"><strong>structure tensor</strong></span><a id="id1709" class="indexterm"/> (or <a id="id1710" class="indexterm"/><span class="strong"><strong>Harris matrix</strong></span>) of the image:</p><div class="mediaobject"><img src="images/4818OS_11_24.jpg" alt="How it works..."/></div><p>Here, <span class="emphasis"><em>I(x,y)</em></span> is<a id="id1711" class="indexterm"/> the image, <span class="emphasis"><em>I<sub>x</sub></em></span> and <span class="emphasis"><em>I<sub>y</sub></em></span> are the partial derivatives, and <a id="id1712" class="indexterm"/>the brackets denote the local spatial average around neighboring values.</p><p>This tensor associates a <span class="emphasis"><em>(2,2)</em></span> positive symmetric matrix at each point. This matrix is used to calculate a sort of autocorrelation of the image at each point.</p><p>Let <span class="inlinemediaobject"><img src="images/4818OS_11_31.jpg" alt="How it works..."/></span> and <span class="inlinemediaobject"><img src="images/4818OS_11_32.jpg" alt="How it works..."/></span> be the two eigenvalues of this matrix (the matrix is diagonalizable because it is real and symmetric). Roughly, a corner is characterized by a large variation of the autocorrelation in all directions, or in large positive eigenvalues <span class="inlinemediaobject"><img src="images/4818OS_11_31.jpg" alt="How it works..."/></span> and <span class="inlinemediaobject"><img src="images/4818OS_11_32.jpg" alt="How it works..."/></span>. The corner measure image is defined as:</p><div class="mediaobject"><img src="images/4818OS_11_25.jpg" alt="How it works..."/></div><p>Here, <span class="emphasis"><em>k</em></span> is a tunable parameter. <span class="emphasis"><em>M</em></span> is large when there is a corner. Finally, <code class="literal">corner_peaks()</code> finds<a id="id1713" class="indexterm"/> corner points by looking at local maxima in the corner measure image.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec367"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A corner detection example with scikit-image<a id="id1714" class="indexterm"/> available at <a class="ulink" href="http://scikit-image.org/docs/dev/auto_examples/plot_corner.html">http://scikit-image.org/docs/dev/auto_examples/plot_corner.html</a></li><li class="listitem" style="list-style-type: disc">An image processing tutorial with scikit-image<a id="id1715" class="indexterm"/> available at <a class="ulink" href="http://blog.yhathq.com/posts/image-processing-with-scikit-image.html">http://blog.yhathq.com/posts/image-processing-with-scikit-image.html</a></li><li class="listitem" style="list-style-type: disc">Corner detection<a id="id1716" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Corner_detection">http://en.wikipedia.org/wiki/Corner_detection</a></li><li class="listitem" style="list-style-type: disc">Structure tensor<a id="id1717" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Structure_tensor">http://en.wikipedia.org/wiki/Structure_tensor</a></li><li class="listitem" style="list-style-type: disc">Interest point detection<a id="id1718" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Interest_point_detection">http://en.wikipedia.org/wiki/Interest_point_detection</a></li><li class="listitem" style="list-style-type: disc">API reference of the <code class="literal">skimage.feature</code> module<a id="id1719" class="indexterm"/> available at <a class="ulink" href="http://scikit-image.org/docs/dev/api/skimage.feature.html">http://scikit-image.org/docs/dev/api/skimage.feature.html</a></li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Detecting faces in an image with OpenCV"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec97"/>Detecting faces in an image with OpenCV</h1></div></div></div><p>
<span class="strong"><strong>OpenCV</strong></span> (<span class="strong"><strong>Open Computer Vision</strong></span>) is <a id="id1720" class="indexterm"/>an open source C++ library for computer vision. It features algorithms for image <a id="id1721" class="indexterm"/>segmentation, object recognition, augmented reality, face detection, and other computer vision tasks.</p><p>In this recipe, we will use OpenCV in Python to detect faces in a picture.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec368"/>Getting ready</h2></div></div></div><p>You need OpenCV<a id="id1722" class="indexterm"/> and the Python wrapper<a id="id1723" class="indexterm"/>. You can find installation instructions on OpenCV's website, <a class="ulink" href="http://docs.opencv.org/trunk/doc/py_tutorials/py_tutorials.html">http://docs.opencv.org/trunk/doc/py_tutorials/py_tutorials.html</a>.</p><p>On Windows, you can install Chris Gohlke's package, available at <a class="ulink" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv">www.lfd.uci.edu/~gohlke/pythonlibs/#opencv</a>.</p><p>You also need to download the <span class="emphasis"><em>Family</em></span> dataset from the book's GitHub repository at <a class="ulink" href="https://github.com/ipython-books/cookbook-data">https://github.com/ipython-books/cookbook-data</a>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note33"/>Note</h3><p>OpenCV is not compatible with Python 3 at the time of this writing. Therefore, this recipe requires Python 2.</p></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec369"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we import the packages:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import cv2
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">We open the JPG image with OpenCV:<div class="informalexample"><pre class="programlisting">In [2]: img = cv2.imread('data/pic3.jpg')</pre></div></li><li class="listitem">Then, we convert it to a grayscale<a id="id1724" class="indexterm"/> image using OpenCV's <code class="literal">cvtColor()</code> function. For face detection, it is sufficient and faster to use grayscale images.<div class="informalexample"><pre class="programlisting">In [3]: gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</pre></div></li><li class="listitem">To detect faces, we will use the<a id="id1725" class="indexterm"/> <span class="strong"><strong>Viola–Jones object detection framework</strong></span>. A cascade of Haar-like classifiers has been trained on many images to detect faces (more details are given in the next section). The result of the training is stored in an XML file (part of the <span class="emphasis"><em>Family</em></span> dataset available on the book's GitHub repository). We load this cascade <a id="id1726" class="indexterm"/>from this XML file with OpenCV's <code class="literal">CascadeClassifier</code> class:<div class="informalexample"><pre class="programlisting">In [4]: face_cascade = cv2.CascadeClassifier(
              'data/haarcascade_frontalface_default.xml')</pre></div></li><li class="listitem">Finally, the <code class="literal">detectMultiScale()</code> method of the classifier detects the objects on a grayscale image and returns a list of rectangles around these objects:<div class="informalexample"><pre class="programlisting">In [5]: for x,y,w,h in \
                 face_cascade.detectMultiScale(gray, 1.3):
            cv2.rectangle(gray, (x,y), (x+w,y+h),
                          (255,0,0), 2)
        plt.imshow(gray, cmap=plt.cm.gray)
        plt.axis('off')</pre></div><div class="mediaobject"><img src="images/4818OS_11_26.jpg" alt="How to do it..."/></div><p>We see that, although all detected objects are indeed faces, one face out of four is not detected. This is probably due to the fact that this face is not perfectly facing the camera, whereas the faces in the training set were. This shows that the efficacy of this method is limited by the quality and generality of the training set.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec370"/>How it works...</h2></div></div></div><p>The Viola–Jones object <a id="id1727" class="indexterm"/>detection framework works by training a cascade of boosted classifiers with Haar-like features. First, we consider a set of features:</p><div class="mediaobject"><img src="images/4818OS_11_27.jpg" alt="How it works..."/><div class="caption"><p>Haar-like features</p></div></div><p>A feature is positioned at a particular location and size in the image. It covers a small window in the image (for example, 24 x 24 pixels). The sum of all pixels in the black area is subtracted to the sum of the pixels in the white area. This operation can be done efficiently with integral images.</p><p>Then, the set of all classifiers is trained with a boosting technique; only the best features are kept for the next stage during training. The training set contains positive and negative images (with and without faces). Although the classifiers yield poor performance <span class="emphasis"><em>individually</em></span>, the cascade of boosted classifiers is both efficient and fast. This method is therefore well-adapted to real-time processing.</p><p>The XML file has been obtained in OpenCV's package. There are multiple files corresponding to different training sets. You can also train your own cascade with your own training set.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec371"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A cascade tutorial with OpenCV (C++)<a id="id1728" class="indexterm"/> available at <a class="ulink" href="http://docs.opencv.org/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html">http://docs.opencv.org/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html</a></li><li class="listitem" style="list-style-type: disc">Documentation to <a id="id1729" class="indexterm"/>train a cascade, available at <a class="ulink" href="http://docs.opencv.org/doc/user_guide/ug_traincascade.html">http://docs.opencv.org/doc/user_guide/ug_traincascade.html</a></li><li class="listitem" style="list-style-type: disc">Haar cascades library, available <a id="id1730" class="indexterm"/>at <a class="ulink" href="https://github.com/Itseez/opencv/tree/master/data/haarcascades">https://github.com/Itseez/opencv/tree/master/data/haarcascades</a></li><li class="listitem" style="list-style-type: disc">OpenCV's cascade classification API reference<a id="id1731" class="indexterm"/> available at <a class="ulink" href="http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html">http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html</a></li><li class="listitem" style="list-style-type: disc">The Viola–Jones object detection framework<a id="id1732" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework">http://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework</a></li><li class="listitem" style="list-style-type: disc">Boosting<a id="id1733" class="indexterm"/> or how to create one strong classifier from many weak classifiers, explained at <a class="ulink" href="http://en.wikipedia.org/wiki/Boosting_(machine_learning)">http://en.wikipedia.org/wiki/Boosting_(machine_learning)</a></li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Applying digital filters to speech sounds"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec98"/>Applying digital filters to speech sounds</h1></div></div></div><p>In this recipe, we will show how to play sounds in the notebook. We will also illustrate the effect of simple digital filters on speech sounds.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec372"/>Getting ready</h2></div></div></div><p>You need the<a id="id1734" class="indexterm"/> <span class="strong"><strong>pydub</strong></span> package. You can install it with <code class="literal">pip install pydub</code> or download it from <a class="ulink" href="https://github.com/jiaaro/pydub/">https://github.com/jiaaro/pydub/</a>.</p><p>This package requires the open source multimedia library FFmpeg<a id="id1735" class="indexterm"/> for the decompression of MP3 files, available at <a class="ulink" href="http://www.ffmpeg.org">www.ffmpeg.org</a>.</p><p>The code given here works with Python 3. You will find the Python 2 version in the book's GitHub repository.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec373"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's<a id="id1736" class="indexterm"/> import <a id="id1737" class="indexterm"/>the packages:<div class="informalexample"><pre class="programlisting">In [1]: import urllib
        from io import BytesIO
        import numpy as np
        import scipy.signal as sg
        import pydub
        import matplotlib.pyplot as plt
        from IPython.display import Audio, display
        %matplotlib inline</pre></div></li><li class="listitem">We create a Python function to generate a sound from an English sentence. This function uses Google's <span class="strong"><strong>Text-To-Speech</strong></span> (<span class="strong"><strong>TTS</strong></span>)<a id="id1738" class="indexterm"/> API. We retrieve the sound in the MP3 format, and convert it to the Wave format with pydub. Finally, we retrieve the raw sound data by removing the wave header with NumPy:<div class="informalexample"><pre class="programlisting">In [2]: def speak(sentence):
            url = ("http://translate.google.com/"
                    "translate_tts?tl=en&amp;q=") + 
                        urllib.parse.quote_plus(sentence)
            req = urllib.request.Request(url,
                             headers={'User-Agent': ''}) 
            mp3 = urllib.request.urlopen(req).read()
            # We convert the mp3 bytes to wav.
            audio = pydub.AudioSegment.from_mp3(
                                            BytesIO(mp3))
            wave = audio.export('_', format='wav')
            wave.seek(0)
            wave = wave.read()
            # We get the raw data by removing the 24 
            # first bytes of the header.
            x = np.frombuffer(wave, np.int16)[24:] / 2.**15</pre></div></li><li class="listitem">We create a <a id="id1739" class="indexterm"/>function that plays a<a id="id1740" class="indexterm"/> sound (represented by a NumPy vector) in the notebook, using IPython's <code class="literal">Audio</code> class:<div class="informalexample"><pre class="programlisting">In [3]: def play(x, fr, autoplay=False):
            display(Audio(x, rate=fr, autoplay=autoplay))</pre></div></li><li class="listitem">Let's play the sound "Hello world." We also display the waveform with matplotlib:<div class="informalexample"><pre class="programlisting">In [4]: x, fr = speak("Hello world")
        play(x, fr)
        t = np.linspace(0., len(x)/fr, len(x))
        plt.plot(t, x, lw=1)</pre></div><div class="mediaobject"><img src="images/4818OS_11_28.jpg" alt="How to do it…"/></div></li><li class="listitem">Now, we will hear the effect of a Butterworth low-pass filter applied to this sound (500 Hz cutoff frequency):<div class="informalexample"><pre class="programlisting">In [5]: b, a = sg.butter(4, 500./(fr/2.), 'low')
        x_fil = sg.filtfilt(b, a, x)
In [6]: play(x_fil, fr)
        plt.plot(t, x, lw=1)
        plt.plot(t, x_fil, lw=1)</pre></div><div class="mediaobject"><img src="images/4818OS_11_29.jpg" alt="How to do it…"/></div><p>We hear a muffled voice.</p></li><li class="listitem">Now, with a<a id="id1741" class="indexterm"/> high-pass<a id="id1742" class="indexterm"/> filter (1000 Hz cutoff frequency):<div class="informalexample"><pre class="programlisting">In [7]: b, a = sg.butter(4, 1000./(fr/2.), 'high')
        x_fil = sg.filtfilt(b, a, x)
In [8]: play(x_fil, fr)
        plt.plot(t, x, lw=1)
        plt.plot(t, x_fil, lw=1)</pre></div><div class="mediaobject"><img src="images/4818OS_11_30.jpg" alt="How to do it…"/></div><p>It sounds like a phone call.</p></li><li class="listitem">Finally, we can create a simple widget to quickly test the effect of a high-pass filter with an arbitrary cutoff frequency:<div class="informalexample"><pre class="programlisting">In [9]: from IPython.html import widgets
        @widgets.interact(t=(100., 5000., 100.))
        def highpass(t):
            b, a = sg.butter(4, t/(fr/2.), 'high')
            x_fil = sg.filtfilt(b, a, x)
            play(x_fil, fr, autoplay=True)</pre></div><p>We get a slider that lets us change the cutoff frequency and hear the effect in real-time.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec374"/>How it works...</h2></div></div></div><p>The <a id="id1743" class="indexterm"/>human ear can hear frequencies up to 20 kHz. The<a id="id1744" class="indexterm"/> human voice frequency band ranges from approximately 300 Hz to 3000 Hz.</p><p>Digital filters were described in <a class="link" href="ch10.html" title="Chapter 10. Signal Processing">Chapter 10</a>, <span class="emphasis"><em>Signal Processing</em></span>. The example given here allows us to hear the effect of low- and high-pass filters on sounds.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec375"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Audio signal processing<a id="id1745" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Audio_signal_processing">http://en.wikipedia.org/wiki/Audio_signal_processing</a></li><li class="listitem" style="list-style-type: disc">Audio filters<a id="id1746" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Audio_filter">http://en.wikipedia.org/wiki/Audio_filter</a></li><li class="listitem" style="list-style-type: disc">Voice frequency<a id="id1747" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Voice_frequency">http://en.wikipedia.org/wiki/Voice_frequency</a></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>PyAudio</strong></span>, an audio <a id="id1748" class="indexterm"/>Python package that uses the PortAudio library, available at <a class="ulink" href="http://people.csail.mit.edu/hubert/pyaudio/">http://people.csail.mit.edu/hubert/pyaudio/</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec376"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Creating a sound synthesizer in the notebook</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Creating a sound synthesizer in the notebook"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec99"/>Creating a sound synthesizer in the notebook</h1></div></div></div><p>In this recipe, we will create a small <a id="id1749" class="indexterm"/>electronic piano in the notebook. We<a id="id1750" class="indexterm"/> will synthesize sinusoidal sounds with NumPy instead of using recorded tones.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec377"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We import the modules:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import matplotlib.pyplot as plt
        from IPython.display import (Audio, display,
                                     clear_output)
        from IPython.html import widgets
        from functools import partial
        %matplotlib inline</pre></div></li><li class="listitem">We define the sampling rate and the duration of the notes:<div class="informalexample"><pre class="programlisting">In [2]: rate = 16000.
        duration = 0.5
        t = np.linspace(0., duration, rate * duration)</pre></div></li><li class="listitem">We create a function that generates and plays the sound of a note (sine function) at a given frequency, using NumPy and IPython's <code class="literal">Audio</code> class:<div class="informalexample"><pre class="programlisting">In [3]: def synth(f):
            x = np.sin(f * 2. * np.pi * t)
            display(Audio(x, rate=rate, autoplay=True))</pre></div></li><li class="listitem">Here is the fundamental 440 Hz note:<div class="informalexample"><pre class="programlisting">In [4]: synth(440)</pre></div></li><li class="listitem">Now, we generate the note frequencies of our piano. The chromatic scale is obtained by a geometric progression with the common ratio <span class="emphasis"><em>2<sup>1/12</sup></em></span>:<div class="informalexample"><pre class="programlisting">In [5]: notes = zip(('C,C#,D,D#,E,F,F#,G,G#,'
                     'A,A#,B,C').split(','),
                     440. * 2 ** (np.arange(3, 17) / 12.))</pre></div></li><li class="listitem">Finally, we create the piano with the notebook widgets. Each note is a button, and all buttons are contained in a horizontal box container. Clicking on one note plays a sound at the corresponding frequency. The piano layout is the same as the one used in the <span class="emphasis"><em>Using interactive widgets – a piano in the notebook</em></span> recipe of <a class="link" href="ch03.html" title="Chapter 3. Mastering the Notebook">Chapter 3</a>, <span class="emphasis"><em>Mastering the Notebook</em></span>.<div class="informalexample"><pre class="programlisting">In [6]: container = widgets.ContainerWidget()
        buttons = []
        for note, f in notes:
            button = widgets.ButtonWidget(description=note)
            def on_button_clicked(f, b):
                clear_output()
                synth(f)
            button.on_click(partial(on_button_clicked, f))
            button.set_css({...})
            buttons.append(button)
        container.children = buttons
        display(container)
        container.remove_class('vbox')
        container.add_class('hbox')</pre></div></li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note34"/>Note</h3><p>The IPython API used here to design the layout is based on IPython 2.x; it will be slightly different in IPython 3.0.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec378"/>How it works...</h2></div></div></div><p>A <span class="strong"><strong>pure tone</strong></span><a id="id1751" class="indexterm"/> is a tone with a sinusoidal waveform. It is the simplest way of representing a musical note. A note generated by a musical instrument is typically much more complex. Although the sound contains many frequencies, we generally perceive a musical <a id="id1752" class="indexterm"/>tone (<span class="strong"><strong>fundamental frequency</strong></span>).</p><p>By generating another periodic function instead of a sinusoidal waveform, we would hear the same tone, but <a id="id1753" class="indexterm"/>a different <a id="id1754" class="indexterm"/><span class="strong"><strong>timbre</strong></span>. Electronic <a id="id1755" class="indexterm"/>music synthesizers are based on this idea.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec379"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Synthesizer<a id="id1756" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Synthesizer">http://en.wikipedia.org/wiki/Synthesizer</a></li><li class="listitem" style="list-style-type: disc">Equal temperament<a id="id1757" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Equal_temperament">http://en.wikipedia.org/wiki/Equal_temperament</a></li><li class="listitem" style="list-style-type: disc">Chromatic scale<a id="id1758" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Chromatic_scale">http://en.wikipedia.org/wiki/Chromatic_scale</a></li><li class="listitem" style="list-style-type: disc">Pure tone<a id="id1759" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Pure_tone">http://en.wikipedia.org/wiki/Pure_tone</a></li><li class="listitem" style="list-style-type: disc">Timbre<a id="id1760" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Timbre">http://en.wikipedia.org/wiki/Timbre</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec380"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Applying digital filters to speech sounds</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Using interactive widgets – a piano in the notebook</em></span> recipe in <a class="link" href="ch03.html" title="Chapter 3. Mastering the Notebook">Chapter 3</a>, <span class="emphasis"><em>Mastering the Notebook</em></span></li></ul></div></div></div></div>
</body></html>