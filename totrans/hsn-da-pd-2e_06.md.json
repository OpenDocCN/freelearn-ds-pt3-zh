["```py\n>>> import pandas as pd\n>>> weather = pd.read_csv('data/nyc_weather_2018.csv')\n>>> weather.head()\n```", "```py\n>>> snow_data = weather.query(\n...     'datatype == \"SNOW\" and value > 0 '\n...     'and station.str.contains(\"US1NY\")'\n... ) \n>>> snow_data.head()\n```", "```py\nSELECT * FROM weather\nWHERE\n  datatype == \"SNOW\" AND value > 0 AND station LIKE \"%US1NY%\";\n```", "```py\n>>> weather[\n...     (weather.datatype == 'SNOW') & (weather.value > 0)\n...     & weather.station.str.contains('US1NY')\n... ].equals(snow_data)\nTrue\n```", "```py\n>>> station_info = pd.read_csv('data/weather_stations.csv')\n>>> station_info.head()\n```", "```py\n>>> station_info.id.describe()\ncount                   279\nunique                  279\ntop       GHCND:US1NJBG0029\nfreq                      1\nName: id, dtype: object\n>>> weather.station.describe()\ncount                 78780\nunique                  110\ntop       GHCND:USW00094789\nfreq                   4270\nName: station, dtype: object\n```", "```py\n>>> station_info.shape[0], weather.shape[0] # 0=rows, 1=cols\n(279, 78780)\n```", "```py\n>>> def get_row_count(*dfs):\n...     return [df.shape[0] for df in dfs]\n>>> get_row_count(station_info, weather)\n[279, 78780]\n```", "```py\n>>> inner_join = weather.merge(\n...     station_info, left_on='station', right_on='id'\n... )\n>>> inner_join.sample(5, random_state=0)\n```", "```py\n>>> weather.merge(\n...     station_info.rename(dict(id='station'), axis=1), \n...     on='station'\n... ).sample(5, random_state=0)\n```", "```py\n>>> left_join = station_info.merge(\n...     weather, left_on='id', right_on='station', how='left'\n... )\n>>> right_join = weather.merge(\n...     station_info, left_on='station', right_on='id',\n...     how='right'\n... )\n>>> right_join[right_join.datatype.isna()].head() # see nulls\n```", "```py\n>>> left_join.sort_index(axis=1)\\\n...     .sort_values(['date', 'station'], ignore_index=True)\\\n...     .equals(right_join.sort_index(axis=1).sort_values(\n...         ['date', 'station'], ignore_index=True\n...     ))\nTrue\n```", "```py\n>>> get_row_count(inner_join, left_join, right_join)\n[78780, 78949, 78949]\n```", "```py\n>>> outer_join = weather.merge(\n...     station_info[station_info.id.str.contains('US1NY')], \n...     left_on='station', right_on='id',\n...     how='outer', indicator=True\n... )\n# view effect of outer join\n>>> pd.concat([\n...     outer_join.query(f'_merge == \"{kind}\"')\\\n...         .sample(2, random_state=0)\n...     for kind in outer_join._merge.unique()\n... ]).sort_index()\n```", "```py\nSELECT *\nFROM left_table\n<JOIN_TYPE> right_table\nON left_table.<col> == right_table.<col>;\n```", "```py\n>>> dirty_data = pd.read_csv(\n...     'data/dirty_data.csv', index_col='date'\n... ).drop_duplicates().drop(columns='SNWD')\n>>> dirty_data.head()\n```", "```py\n>>> valid_station = dirty_data.query('station != \"?\"')\\\n...     .drop(columns=['WESF', 'station'])\n>>> station_with_wesf = dirty_data.query('station == \"?\"')\\\n...     .drop(columns=['station', 'TOBS', 'TMIN', 'TMAX'])\n```", "```py\n>>> valid_station.merge(\n...     station_with_wesf, how='left',\n...     left_index=True, right_index=True\n... ).query('WESF > 0').head()\n```", "```py\n>>> valid_station.merge(\n...     station_with_wesf, how='left',\n...     left_index=True, right_index=True, \n...     suffixes=('', '_?')\n... ).query('WESF > 0').head()\n```", "```py\n>>> valid_station.join(\n...     station_with_wesf, how='left', rsuffix='_?'\n... ).query('WESF > 0').head()\n```", "```py\n>>> weather.set_index('station', inplace=True)\n>>> station_info.set_index('id', inplace=True)\n```", "```py\n>>> weather.index.intersection(station_info.index)\nIndex(['GHCND:US1CTFR0039', ..., 'GHCND:USW1NYQN0029'],\n      dtype='object', length=110)\n```", "```py\n>>> weather.index.difference(station_info.index)\nIndex([], dtype='object')\n>>> station_info.index.difference(weather.index)\nIndex(['GHCND:US1CTFR0022', ..., 'GHCND:USW00014786'],\n      dtype='object', length=169)\n```", "```py\n>>> weather.index.unique().union(station_info.index)\nIndex(['GHCND:US1CTFR0022', ..., 'GHCND:USW00094789'],\n      dtype='object', length=279)\n```", "```py\n>>> import numpy as np\n>>> import pandas as pd\n>>> weather = pd.read_csv(\n...     'data/nyc_weather_2018.csv', parse_dates=['date']\n... )\n>>> fb = pd.read_csv(\n...     'data/fb_2018.csv', index_col='date', parse_dates=True\n... )\n```", "```py\n>>> fb.assign(\n...     abs_z_score_volume=lambda x: x.volume \\\n...         .sub(x.volume.mean()).div(x.volume.std()).abs()\n... ).query('abs_z_score_volume > 3')\n```", "```py\n>>> fb.assign(\n...     volume_pct_change=fb.volume.pct_change(),\n...     pct_change_rank=lambda x: \\\n...         x.volume_pct_change.abs().rank(ascending=False)\n... ).nsmallest(5, 'pct_change_rank')\n```", "```py\n>>> fb['2018-01-11':'2018-01-12']\n```", "```py\n>>> (fb > 215).any()\nopen          True\nhigh          True\nlow          False\nclose         True\nvolume        True\ndtype: bool\n```", "```py\n>>> (fb > 215).all()\nopen      False\nhigh      False\nlow       False\nclose     False\nvolume     True\ndtype: bool\n```", "```py\n>>> (fb.volume.value_counts() > 1).sum()\n0\n```", "```py\n>>> volume_binned = pd.cut(\n...     fb.volume, bins=3, labels=['low', 'med', 'high']\n... )\n>>> volume_binned.value_counts()\nlow     240\nmed       8\nhigh      3\nName: volume, dtype: int64\n```", "```py\n>>> fb[volume_binned == 'high']\\\n...     .sort_values('volume', ascending=False)\n```", "```py\n>>> fb['2018-07-25':'2018-07-26']\n```", "```py\n>>> fb['2018-03-16':'2018-03-20']\n```", "```py\n>>> volume_qbinned = pd.qcut(\n...     fb.volume, q=4, labels=['q1', 'q2', 'q3', 'q4']\n... )\n>>> volume_qbinned.value_counts()\nq1    63\nq2    63\nq4    63\nq3    62\nName: volume, dtype: int64\n```", "```py\n>>> central_park_weather = weather.query(\n...     'station == \"GHCND:USW00094728\"'\n... ).pivot(index='date', columns='datatype', values='value')\n```", "```py\n>>> oct_weather_z_scores = central_park_weather\\\n...     .loc['2018-10', ['TMIN', 'TMAX', 'PRCP']]\\\n...     .apply(lambda x: x.sub(x.mean()).div(x.std()))\n>>> oct_weather_z_scores.describe().T\n```", "```py\n>>> oct_weather_z_scores.query('PRCP > 3').PRCP\ndate\n2018-10-27    3.936167\nName: PRCP, dtype: float64\n```", "```py\n>>> central_park_weather.loc['2018-10', 'PRCP'].describe()\ncount    31.000000\nmean      2.941935\nstd       7.458542\nmin       0.000000\n25%       0.000000\n50%       0.000000\n75%       1.150000\nmax      32.300000\nName: PRCP, dtype: float64\n```", "```py\n>>> central_park_weather.loc['2018-10'].assign(\n...     rolling_PRCP=lambda x: x.PRCP.rolling('3D').sum()\n... )[['PRCP', 'rolling_PRCP']].head(7).T\n```", "```py\n>>> central_park_weather.loc['2018-10']\\\n...     .rolling('3D').mean().head(7).iloc[:,:6]\n```", "```py\n>>> central_park_weather\\\n...     ['2018-10-01':'2018-10-07'].rolling('3D').agg({\n...     'TMAX': 'max', 'TMIN': 'min',\n...     'AWND': 'mean', 'PRCP': 'sum'\n... }).join( # join with original data for comparison\n...     central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], \n...     lsuffix='_rolling'\n... ).sort_index(axis=1) # put rolling calcs next to originals\n```", "```py\n>>> central_park_weather.loc['2018-06'].assign(\n...     TOTAL_PRCP=lambda x: x.PRCP.cumsum(),\n...     AVG_PRCP=lambda x: x.PRCP.expanding().mean()\n... ).head(10)[['PRCP', 'TOTAL_PRCP', 'AVG_PRCP']].T\n```", "```py\n>>> central_park_weather\\\n...     ['2018-10-01':'2018-10-07'].expanding().agg({\n...     'TMAX': np.max, 'TMIN': np.min, \n...     'AWND': np.mean, 'PRCP': np.sum\n... }).join(\n...     central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], \n...     lsuffix='_expanding'\n... ).sort_index(axis=1)\n```", "```py\n>>> central_park_weather.assign(\n...     AVG=lambda x: x.TMAX.rolling('30D').mean(),\n...     EWMA=lambda x: x.TMAX.ewm(span=30).mean()\n... ).loc['2018-09-29':'2018-10-08', ['TMAX', 'EWMA', 'AVG']].T\n```", "```py\ndata.pipe(h)\\ # first call h(data)\n    .pipe(g, 20)\\ # call g on the result with positional arg 20\n    .pipe(f, x=True) # call f on result with keyword arg x=True\n```", "```py\n>>> def get_info(df):\n...     return '%d rows, %d cols and max closing Z-score: %d' \n...             % (*df.shape, df.close.max()) \n```", "```py\n>>> get_info(fb.loc['2018-Q1']\\\n...            .apply(lambda x: (x - x.mean())/x.std()))\n```", "```py\n>>> fb.loc['2018-Q1'].apply(lambda x: (x - x.mean())/x.std())\\\n...     .pipe(get_info)\n```", "```py\n>>> fb.pipe(pd.DataFrame.rolling, '20D').mean().equals(\n...     fb.rolling('20D').mean()\n... ) # the pipe is calling pd.DataFrame.rolling(fb, '20D')\nTrue\n```", "```py\n>>> from window_calc import window_calc\n>>> window_calc??\nSignature: window_calc(df, func, agg_dict, *args, **kwargs)\nSource:   \ndef window_calc(df, func, agg_dict, *args, **kwargs):\n    \"\"\"\n    Run a window calculation of your choice on the data.\n    Parameters:\n        - df: The `DataFrame` object to run the calculation on.\n        - func: The window calculation method that takes `df` \n          as the first argument.\n        - agg_dict: Information to pass to `agg()`, could be \n          a dictionary mapping the columns to the aggregation \n          function to use, a string name for the function, \n          or the function itself.\n        - args: Positional arguments to pass to `func`.\n        - kwargs: Keyword arguments to pass to `func`.\n\n    Returns:\n        A new `DataFrame` object.\n    \"\"\"\n    return df.pipe(func, *args, **kwargs).agg(agg_dict)\nFile:      ~/.../ch_04/window_calc.py\nType:      function\n```", "```py\n>>> window_calc(fb, pd.DataFrame.expanding, np.median).head()\n```", "```py\n>>> window_calc(fb, pd.DataFrame.ewm, 'mean', span=3).head()\n```", "```py\n>>> window_calc(\n...     central_park_weather.loc['2018-10'], \n...     pd.DataFrame.rolling, \n...     {'TMAX': 'max', 'TMIN': 'min',\n...      'AWND': 'mean', 'PRCP': 'sum'},\n...     '3D'\n... ).head()\n```", "```py\n>>> import numpy as np\n>>> import pandas as pd\n>>> fb = pd.read_csv(\n...     'data/fb_2018.csv', index_col='date', parse_dates=True\n... ).assign(trading_volume=lambda x: pd.cut(\n...     x.volume, bins=3, labels=['low', 'med', 'high'] \n... ))\n>>> weather = pd.read_csv(\n...     'data/weather_by_station.csv', \n...     index_col='date', parse_dates=True\n... )\n```", "```py\n>>> pd.set_option('display.float_format', lambda x: '%.2f' % x)\n```", "```py\n>>> fb.agg({\n...     'open': np.mean, 'high': np.max, 'low': np.min, \n...     'close': np.mean, 'volume': np.sum\n... })\nopen            171.45\nhigh            218.62\nlow             123.02\nclose           171.51\nvolume   6949682394.00\ndtype: float64\n```", "```py\n>>> weather.query('station == \"GHCND:USW00094728\"')\\\n...     .pivot(columns='datatype', values='value')\\\n...     [['SNOW', 'PRCP']].sum()\ndatatype\nSNOW   1007.00\nPRCP   1665.30\ndtype: float64\n```", "```py\n>>> fb.agg({\n...     'open': 'mean', \n...     'high': ['min', 'max'],\n...     'low': ['min', 'max'], \n...     'close': 'mean'\n... })\n```", "```py\n>>> fb.groupby('trading_volume').mean()\n```", "```py\n>>> fb.groupby('trading_volume')\\\n...     ['close'].agg(['min', 'max', 'mean'])\n```", "```py\n>>> fb_agg = fb.groupby('trading_volume').agg({\n...     'open': 'mean', 'high': ['min', 'max'],\n...     'low': ['min', 'max'], 'close': 'mean'\n... })\n>>> fb_agg\n```", "```py\n>>> fb_agg.columns\nMultiIndex([( 'open', 'mean'),\n            ( 'high',  'min'),\n            ( 'high',  'max'),\n            (  'low',  'min'),\n            (  'low',  'max'),\n            ('close', 'mean')],\n           )\n```", "```py\n>>> fb_agg.columns = ['_'.join(col_agg) \n...                   for col_agg in fb_agg.columns]\n>>> fb_agg.head()\n```", "```py\n>>> weather.loc['2018-10'].query('datatype == \"PRCP\"')\\ \n...     .groupby(level=0).mean().head().squeeze()\ndate\n2018-10-01    0.01\n2018-10-02    2.23\n2018-10-03   19.69\n2018-10-04    0.32\n2018-10-05    0.96\nName: value, dtype: float64\n```", "```py\n>>> weather.query('datatype == \"PRCP\"').groupby(\n...     ['station_name', pd.Grouper(freq='Q')]\n... ).sum().unstack().sample(5, random_state=1)\n```", "```py\n>>> weather.query('datatype == \"PRCP\"')\\\n...     .groupby(level=0).mean()\\\n...     .groupby(pd.Grouper(freq='M')).sum().value.nlargest()\ndate\n2018-11-30   210.59\n2018-09-30   193.09\n2018-08-31   192.45\n2018-07-31   160.98\n2018-02-28   158.11\nName: value, dtype: float64\n```", "```py\n>>> weather.query('datatype == \"PRCP\"')\\\n...     .rename(dict(value='prcp'), axis=1)\\\n...     .groupby(level=0).mean()\\\n...     .groupby(pd.Grouper(freq='M'))\\\n...     .transform(np.sum)['2018-01-28':'2018-02-03']\n```", "```py\n>>> weather.query('datatype == \"PRCP\"')\\\n...     .rename(dict(value='prcp'), axis=1)\\\n...     .groupby(level=0).mean()\\\n...     .assign(\n...         total_prcp_in_month=lambda x: x.groupby(\n...             pd.Grouper(freq='M')).transform(np.sum),\n...         pct_monthly_prcp=lambda x: \\\n...             x.prcp.div(x.total_prcp_in_month)\n...     ).nlargest(5, 'pct_monthly_prcp')\n```", "```py\n>>> fb.pivot_table(columns='trading_volume')\n```", "```py\n>>> weather.reset_index().pivot_table(\n...     index=['date', 'station', 'station_name'], \n...     columns='datatype', \n...     values='value', \n...     aggfunc='median'\n... ).reset_index().tail()\n```", "```py\n>>> pd.crosstab(\n...     index=fb.trading_volume, columns=fb.index.month,\n...     colnames=['month'] # name the columns index\n... )\n```", "```py\n>>> pd.crosstab(\n...     index=fb.trading_volume, columns=fb.index.month,\n...     colnames=['month'], values=fb.close, aggfunc=np.mean\n... )\n```", "```py\n>>> snow_data = weather.query('datatype == \"SNOW\"')\n>>> pd.crosstab(\n...     index=snow_data.station_name,\n...     columns=snow_data.index.month, \n...     colnames=['month'],\n...     values=snow_data.value,\n...     aggfunc=lambda x: (x > 0).sum(),\n...     margins=True, # show row and column subtotals\n...     margins_name='total observations of snow' # subtotals\n... )\n```", "```py\n>>> import numpy as np\n>>> import pandas as pd\n>>> fb = pd.read_csv(\n...     'data/fb_2018.csv', index_col='date', parse_dates=True\n... ).assign(trading_volume=lambda x: pd.cut( \n...     x.volume, bins=3, labels=['low', 'med', 'high']     \n... ))\n```", "```py\n>>> fb['2018-10-11':'2018-10-15']\n```", "```py\n>>> fb.loc['2018-q1'].equals(fb['2018-01':'2018-03'])\nTrue\n```", "```py\n>>> fb.first('1W')\n```", "```py\n>>> fb.last('1W')\n```", "```py\n>>> fb_reindexed = fb.reindex(\n...     pd.date_range('2018-01-01', '2018-12-31', freq='D')\n... )\n```", "```py\n>>> fb_reindexed.first('1D').isna().squeeze().all()\nTrue\n```", "```py\n>>> fb_reindexed.loc['2018-Q1'].first_valid_index()\nTimestamp('2018-01-02 00:00:00', freq='D')\n>>> fb_reindexed.loc['2018-Q1'].last_valid_index()\nTimestamp('2018-03-29 00:00:00', freq='D')\n```", "```py\n>>> fb_reindexed.asof('2018-03-31')\nopen                   155.15\nhigh                   161.42\nlow                    154.14\nclose                  159.79\nvolume            59434293.00\ntrading_volume            low\nName: 2018-03-31 00:00:00, dtype: object\n```", "```py\n>>> stock_data_per_minute = pd.read_csv(\n...     'data/fb_week_of_may_20_per_minute.csv', \n...     index_col='date', parse_dates=True, \n...     date_parser=lambda x: \\\n...         pd.to_datetime(x, format='%Y-%m-%d %H-%M')\n... )\n>>> stock_data_per_minute.head()\n```", "```py\n>>> stock_data_per_minute.groupby(pd.Grouper(freq='1D')).agg({\n...     'open': 'first', \n...     'high': 'max', \n...     'low': 'min', \n...     'close': 'last', \n...     'volume': 'sum'\n... })\n```", "```py\n>>> stock_data_per_minute.at_time('9:30')\n```", "```py\n>>> stock_data_per_minute.between_time('15:59', '16:00')\n```", "```py\n>>> shares_traded_in_first_30_min = stock_data_per_minute\\\n...     .between_time('9:30', '10:00')\\\n...     .groupby(pd.Grouper(freq='1D'))\\\n...     .filter(lambda x: (x.volume > 0).all())\\\n...     .volume.mean()\n>>> shares_traded_in_last_30_min = stock_data_per_minute\\\n...     .between_time('15:30', '16:00')\\\n...     .groupby(pd.Grouper(freq='1D'))\\\n...     .filter(lambda x: (x.volume > 0).all())\\\n...     .volume.mean()\n```", "```py\n>>> shares_traded_in_first_30_min \\\n... - shares_traded_in_last_30_min\n18592.967741935485\n```", "```py\n>>> fb.assign(\n...     prior_close=lambda x: x.close.shift(),\n...     after_hours_change_in_price=lambda x: \\\n...         x.open - x.prior_close,\n...     abs_change=lambda x: \\\n...         x.after_hours_change_in_price.abs()\n... ).nlargest(5, 'abs_change')\n```", "```py\n>>> (fb.drop(columns='trading_volume') \n...  - fb.drop(columns='trading_volume').shift()\n... ).equals(fb.drop(columns='trading_volume').diff())\nTrue\n```", "```py\n>>> fb.drop(columns='trading_volume').diff().head()\n```", "```py\n>>> stock_data_per_minute.resample('1D').agg({\n...     'open': 'first', \n...     'high': 'max', \n...     'low': 'min', \n...     'close': 'last', \n...     'volume': 'sum'\n... })\n```", "```py\n>>> fb.resample('Q').mean()\n```", "```py\n>>> fb.drop(columns='trading_volume').resample('Q').apply(\n...     lambda x: x.last('1D').values - x.first('1D').values\n... )\n```", "```py\n>>> melted_stock_data = pd.read_csv(\n...     'data/melted_stock_data.csv', \n...     index_col='date', parse_dates=True\n... )\n>>> melted_stock_data.head()\n```", "```py\n>>> melted_stock_data.resample('1D').ohlc()['price']\n```", "```py\n>>> fb.resample('6H').asfreq().head()\n```", "```py\n>>> import sqlite3\n>>> with sqlite3.connect('data/stocks.db') as connection:\n...     fb_prices = pd.read_sql(\n...         'SELECT * FROM fb_prices', connection, \n...         index_col='date', parse_dates=['date']\n...     )\n...     aapl_prices = pd.read_sql(\n...         'SELECT * FROM aapl_prices', connection, \n...         index_col='date', parse_dates=['date']\n...     )\n```", "```py\n>>> fb_prices.index.second.unique()\nInt64Index([0], dtype='int64', name='date')\n>>> aapl_prices.index.second.unique()\nInt64Index([ 0, 52, ..., 37, 28], dtype='int64', name='date')\n```", "```py\n>>> pd.merge_asof(\n...     fb_prices, aapl_prices, \n...     left_index=True, right_index=True,\n...     # merge with nearest minute\n...     direction='nearest',\n...     tolerance=pd.Timedelta(30, unit='s')\n... ).head()\n```", "```py\n>>> pd.merge_ordered(\n...     fb_prices.reset_index(), aapl_prices.reset_index()\n... ).set_index('date').head()\n```"]