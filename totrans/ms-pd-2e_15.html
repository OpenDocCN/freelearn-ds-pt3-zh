<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">A Brief Tour of Bayesian Statistics and Maximum Likelihood Estimates</h1>
                </header>
            
            <article>
                
<p>In this chapter, we take a brief tour of an alternative approach to statistical inference called <strong>Bayesian statistics</strong>. It is not intended to be a full primer, but will simply serve as an introduction to the Bayesian approach. We will also explore the associated Python-related libraries and learn how to use <kbd>pandas</kbd> and <kbd>matplotlib</kbd> to help with the data analysis. The various topics that will be discussed are as follows:</p>
<ul>
<li>Introduction to Bayesian statistics</li>
<li>The mathematical framework for Bayesian statistics</li>
<li>Probability distributions</li>
<li>Bayesian versus frequentist statistics</li>
<li>Introduction to PyMC and Monte Carlo simulations</li>
<li>Bayesian analysis example – switchpoint detection</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Bayesian statistics</h1>
                </header>
            
            <article>
                
<p>The field of Bayesian statistics is built on the work of Reverend Thomas Bayes, an 18th-century statistician, philosopher, and Presbyterian minister. His famous Bayes' theorem, which forms the theoretical underpinnings of Bayesian statistics, was published posthumously in 1763 as a solution to the problem of <em>inverse probability</em>. For more details on this topic, refer to <a href="http://en.wikipedia.org/wiki/Thomas_Bayes"><span class="URLPACKT">http://en.wikipedia.org/wiki/Thomas_Bayes</span></a>.</p>
<p>Inverse probability problems were all the rage in the early 18th century, and were often formulated as follows.</p>
<p class="mce-root"/>
<p>Suppose you play a game with a friend. There are 10 green balls and 7 red balls in bag 1 and 4 green balls and 7 red balls in bag 2. Your friend tosses a coin (without telling you the result), picks a ball from one of the bags at random, and shows it to you. The ball is red. What is the probability that the ball was drawn from bag 1?</p>
<p>These problems were called inverse probability problems because we are trying to estimate the probability of an event that has already occurred (which bag the ball was drawn from) in light of the subsequent result (the ball is red):</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1662 image-border" src="assets/41d6fc3b-2cd9-4dc9-b7e7-6404259f6133.png" style="width:65.00em;height:39.33em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Bayesian balls illustration</div>
<p>Let's quickly illustrate how one would go about solving the inverse probability problem illustrated earlier. We want to calculate the probability that the ball was drawn from bag 1, given that it is red. This can be denoted as <em>P(Bag 1 | Red Ball)</em>.</p>
<p>Let's start by calculating the probability of selecting a red ball. This can be calculated by following the two paths in red, as shown in the preceding figure. Therefore, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><span class="IconPACKT"><img class="fm-editor-equation" src="assets/b27f4fd8-be83-4294-9c5d-a0e227ded600.png" style="width:19.75em;height:1.75em;"/></span></p>
<p>Now, the probability of choosing a red ball from bag 1 is shown by <span>only</span><span> </span><span>taking the upper path and is calculated as follows:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/f0e3c44d-5792-43db-aaca-6bd6712e2b0c.png" style="width:24.33em;height:1.75em;"/></div>
<p>The probability of choosing a red ball from bag 2 is calculated as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/d0bcf020-58a3-4253-96ae-9201e437e95d.png" style="width:25.42em;height:1.83em;"/></div>
<p>Note that this probability can also be written as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7bd4d489-3825-4d8d-9693-5f63c39ab262.png" style="width:23.25em;height:1.33em;"/></div>
<p>We can see that <em>P(Bag 1)=1/2</em>, and the final branch of the tree is only traversed if the ball is firstly in bag 1 and is a red ball. Therefore, we'll get the following outcome:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2f911fb7-4eb6-43bb-a4ce-0c96cdcff090.png" style="width:19.50em;height:2.75em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/e3efe7e1-3a87-4354-b21e-ac7937c9f667.png" style="width:16.75em;height:2.83em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e66819cb-c421-45ce-8648-29f5ad15af27.png" style="width:8.50em;height:1.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The mathematical framework for Bayesian statistics</h1>
                </header>
            
            <article>
                
<p>Bayesian methods are an alternative way of making a statistical inference. We will first look at Bayes' theorem, the fundamental equation from which all Bayesian inference is derived.</p>
<p>A few definitions regarding probability are in order before we begin:</p>
<ul>
<li><em><strong>A</strong></em>,<em><strong>B</strong></em>: These are events that can occur with a certain probability.</li>
<li><strong><em>P</em>(<em>A</em>)</strong> and <strong><em>P</em>(<em>B</em>)</strong>: This is the probability of the occurrence of a particular event.</li>
<li><strong><span class="IconPACKT"><em>P</em>(<em>A</em>|<em>B</em>)</span></strong>: This is the probability of A happening, given that B has occurred. This is known as a <strong>conditional probability</strong>.</li>
<li><strong><em>P</em>(<em>AB</em>) = <em>P</em>(<em>A</em> and <em>B</em>)</strong>: This is the probability of A and B occurring together.</li>
</ul>
<p>We begin with the following basic assumption:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <em>P(AB) = P(B) * P(A|B)</em></div>
<p>The preceding equation shows the relation of the joint probability of <em>P</em>(<em>AB</em>) to the conditional probability <em>P</em>(<em>A</em>|<em>B</em>) and what is known as the marginal probability, <em>P</em>(<em>B</em>). If we rewrite the equation, we have the following expression for conditional probability:</p>
<p class="CDPAlignCenter CDPAlign"><em>P(A|B) = P(AB)/P(B)</em></p>
<p>This is somewhat intuitive—the probability of <em>A</em> given <em>B</em> is obtained by dividing the probability of both <em>A</em> and <em>B</em> occurring by the probability that <em>B</em> occurred. The idea is that <em>B</em> is given, so we divide by its probability. A more rigorous treatment of this equation can be found at <a href="http://bit.ly/1bCYXRd"><span class="URLPACKT">http://bit.ly/1bCYXRd</span></a>, which is titled <em>Probability: Joint, Marginal and Conditional Probabilities</em>.</p>
<p>Similarly, by symmetry we have <em>P(AB) = P(BA) = P(A) * P(B|A</em>). Thus, we have <em>P(A) * P(B|A) = P(B) * P(A|B)</em>. By dividing the expression by <em>P(B)</em> on both sides and assuming <em>P(B)!=0</em>, we obtain the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/eb804081-dd3b-48b3-b61f-00b5b173de93.png" style="width:13.67em;height:2.92em;"/></div>
<p>The preceding equation is referred to as <strong>Bayes' theorem</strong>, the bedrock of all Bayesian statistical inference. In order to link Bayes' theorem to inferential statistics, we will recast the equation as what is called the <strong>diachronic</strong> <strong>interpretation</strong>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/940489d3-fe93-4be2-977a-4deeff0004a3.png" style="width:13.83em;height:2.83em;"/></p>
<p>Here, <em><strong>H</strong></em> represents a hypothesis and <em><strong>D</strong></em> represents an event that has already occurred that we are using in our statistical study, and is also referred to as data.</p>
<p>The expression <strong>(<em>H</em>)</strong> is the probability of our hypothesis before we observe the data. This is known as the <strong>prior probability</strong>. The use of prior probability is often touted as an advantage by Bayesian statisticians since prior knowledge of previous results can be used as input for the current model, resulting in increased accuracy. For more information on this, refer to <a href="http://www.bayesian-inference.com/advantagesbayesian"><span class="URLPACKT">http://www.bayesian-inference.com/advantagesbayesian</span></a>.</p>
<p><span class="IconPACKT"><strong><em>P</em>(<em>D</em>)</strong></span> is the probability of obtaining the data that we observe regardless of the hypothesis. This is called the <strong>normalizing constant</strong>. The normalizing constant doesn't always need to be calculated, especially in many popular algorithms, such as <strong>Markov Chain Monte Carlo</strong><span> (</span><strong>MCMC</strong><span>)</span>, which we will examine later in this chapter.</p>
<p><span class="IconPACKT"><strong><em>P</em>(<em>H</em>|<em>D</em>)</strong></span> is the probability that the hypothesis is true, given the data that we observe. This is called the <strong>posterior</strong>.</p>
<p><strong><em>P</em>(<em>D</em>|<em>H</em>)<em> </em></strong>is the probability of obtaining the data, considering our hypothesis. This is called the <strong>likelihood</strong>.</p>
<p>Thus, Bayesian statistics amounts to applying Bayes' rule to solve problems in inferential statistics with <em><strong>H</strong></em> representing our hypothesis and <em><strong>D</strong></em> the data.</p>
<p>A Bayesian statistical model is cast in terms of parameters, and the uncertainty in these parameters is represented by probability distributions. This is different from the frequentist approach where the values are regarded as deterministic. An alternative representation is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/31826c2d-6a8a-4fe8-8aed-8d5e58794e57.png" style="width:3.92em;height:1.42em;"/></div>
<p>Here, <strong><em>Θ</em></strong> is our unknown data and <strong><em>x</em></strong> is our observed data.</p>
<p>In Bayesian statistics, we make assumptions about the prior data and use the likelihood to update the posterior probability using Bayes' rule. As an illustration, let's consider the following problem. Here is a classic case of what is commonly known as the <strong>urn problem</strong>:</p>
<ul>
<li>Two urns contain colored balls</li>
<li>Urn 1 contains 50 red and 50 blue balls</li>
<li>Urn 2 contains 30 red and 70 blue balls</li>
<li>One of the two urns is randomly chosen (50 percent probability) and then a ball is drawn at random from one of the two urns</li>
</ul>
<p>If a red ball is drawn, what is the probability that it came from urn 1? We want <em>P(H|D)</em>—that is <em><span class="IconPACKT">P(ball came from Urn 1 | Red ball is drawn)</span></em>.</p>
<p>Here, <em>H</em> denotes that the ball is drawn from urn 1, and <em>D </em>denotes that the drawn ball is red:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><em><img class="fm-editor-equation" src="assets/5f51ffcc-a0b4-4d21-9d20-49ffa0df0aed.png" style="width:21.83em;height:1.33em;"/></em></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We know that <em>P(H|D) = P(H) * P(D|H)/P(D), P(D|H) = 0.5, P(D) = (50 + 30)/(100 + 100) = 0.4</em>. This can also be phrased as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/34d84e7c-fa89-49c5-beb1-a2a842d7dba6.png" style="width:33.67em;height:1.08em;"/></p>
<p>Therefore, we conclude that <span class="IconPACKT"><em>P(H|D) = 0.5 * 0.5/0.4 = 0.25/0.4 = 0.625</em></span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayes' theory and odds</h1>
                </header>
            
            <article>
                
<p>Bayes' theorem can sometimes be represented in a more natural and convenient form by using an alternative formulation of probability called <em>odds</em>. Odds are generally expressed in terms of ratios and are <span>commonly </span><span>used. A 3-to-1 odds (written often as 3:1) of a horse winning a race represents the fact that there is a 75 percent probability that the horse will win.</span></p>
<p>Given a probability <em>p</em>, the odds can be calculated as odds = <em>p:(1 - p)</em>, which in the case of <em>p=0.75</em> becomes 0.75:0.25, which is 3:1.</p>
<p><span>Using odds,</span><span> w</span>e can rewrite Bayes' theorem as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/18ea1ab8-a709-4713-a7ca-807d79b3cbb3.png" style="width:13.00em;height:1.17em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applications of Bayesian statistics</h1>
                </header>
            
            <article>
                
<p>Bayesian statistics can be applied to many problems that we encounter in classical statistics, such as the following:</p>
<ul>
<li>Parameter estimation</li>
<li>Prediction</li>
<li>Hypothesis testing</li>
<li>Linear regression</li>
</ul>
<p>There are many compelling reasons for studying Bayesian statistics, such as using prior information to better inform a current model. The Bayesian approach works with probability distributions rather than point estimates, thus producing more realistic predictions. Bayesian inference bases a hypothesis on the available data—<em>P(hypothesis|data)</em>. The frequentist approach tries to fit the data based on a hypothesis. It can be argued that the Bayesian approach is the more logical and empirical one as it tries to base its belief on the facts rather than the other way round. For more information on this, refer to <a href="http://www.bayesian-inference.com/advantagesbayesian"><span class="URLPACKT">http://www.bayesian-inference.com/advantagesbayesian</span></a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Probability distributions</h1>
                </header>
            
            <article>
                
<p>In this section, we will briefly examine the properties of various probability distributions. Many of these distributions are used for Bayesian analysis, and so a brief synopsis is needed before we can proceed. We will also illustrate how to generate and display these distributions using <kbd>matplotlib</kbd>. In order to avoid repeating the <kbd>import</kbd> statements for every code snippet in each section, we will be presenting the following standard set of Python code imports that need to be run before any of the code snippets mentioned in the following command. You only need to run these imports once per session. The imports are as follows:</p>
<pre>import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from matplotlib import colors
import matplotlib.pyplot as plt
import matplotlib
%matplotlib inline
  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fitting a distribution</h1>
                </header>
            
            <article>
                
<p>One of the steps that we have to take in a Bayesian analysis is to fit our data to a probability distribution. Selecting the correct distribution can be something of an art, and often requires statistical knowledge and experience, but we can follow a few guidelines to help us along the way. These guidelines are as follows:</p>
<ul>
<li>Determine whether the data is discrete or continuous</li>
<li>Examine the skewness/symmetry of the data, and if skewed, determine the direction</li>
<li>Determine the lower and upper limits, if there are any</li>
<li>Determine the likelihood of observing extreme values in the distribution</li>
</ul>
<p>A statistical trial is a repeatable experiment with a set of well-defined outcomes that is known as the sample space. A Bernoulli trial is a yes/no experiment where the random <em>X</em> variable is assigned the value of 1 in the case of a yes and 0 in the case of a no. The event of tossing a coin and seeing whether it lands heads up is an example of a Bernoulli trial.</p>
<p>There are two classes of probability distributions: discrete and continuous. In the following sections, we will discuss the differences between these two classes of distributions and learn about the main distributions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discrete probability distributions</h1>
                </header>
            
            <article>
                
<p>In this scenario, the variable can take only certain distinct values, such as integers. An example of a discrete random variable is the number of heads obtained when we flip a coin five times: the possible values are {0,1,2,3,4,5}—we cannot obtain 3.82 heads, for example. The range of values that the random variable can take is specified by what is known as a <strong>probability mass function</strong> (<strong>PMF</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discrete uniform distribution</h1>
                </header>
            
            <article>
                
<p>The discrete uniform distribution is a distribution that models an event with a finite set of possible outcomes where each outcome is equally likely to be observed. For <em>n</em> outcomes, each has a probability of occurrence of <em>1/n</em>.</p>
<p>An example of this is throwing a fair die. The probability of any of the six outcomes is <em>1</em><em>/6</em>. The PMF is given by <em>1/n</em>, and the expected value and variance are given by <em>(max + min)/2</em> and <em>(<span>n<sup>2 </sup>-1)/12 </span></em>respectively:</p>
<pre>from matplotlib import pyplot as plt
import matplotlib.pyplot as plt
X=range(0,11)
Y=[1/6.0 if x in range(1,7) else 0.0 for x in X]
plt.plot(X,Y,'go-', linewidth=0, drawstyle='steps-pre', 
    label="p(x)=1/6")
plt.legend(loc="upper left")<br/>plt.vlines(range(1,7),0,max(Y), linestyle='-')<br/>plt.xlabel('x')
plt.ylabel('p(x)')
plt.ylim(0,0.5)
plt.xlim(0,10)
plt.title('Discrete uniform probability distribution with 
          p=1/6')
plt.show()</pre>
<p>The following is the output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/196aa30a-0528-4fda-bad5-776bb8d864aa.png" style="width:22.83em;height:16.58em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Discrete uniform distribution</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Bernoulli distribution</h1>
                </header>
            
            <article>
                
<p>The Bernoulli distribution measures the probability of success in a trial, for example, the probability that a coin toss turns up a head or a tail. This can be represented by a random <em>X</em> variable that takes a value of 1 if the coin turns up as heads and 0 if it is tails. The probability of it turning up heads or tails is denoted by <em>p</em> and <em>q=1-p</em> respectively.</p>
<p>This can be represented by the following PMF:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/b0cd5693-c6e5-413f-b4cf-e32f8c6c49e9.png" style="width:9.92em;height:2.50em;"/></div>
<p>The expected value and variance are given by the following formula:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6b5c6df3-d4ad-4b41-a70d-eab3c0e750af.png" style="width:4.00em;height:1.17em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6a5f86c1-346d-4596-a895-87c509126ba3.png" style="width:7.75em;height:1.08em;"/></div>
<p>For more information, go to <a href="http://en.wikipedia.org/wiki/Bernoulli_distribution"><span class="URLPACKT">http://en.wikipedia.org/wiki/Bernoulli_distribution</span></a>.</p>
<p>We will now plot the Bernoulli distribution using <kbd>matplotlib</kbd> and <kbd>scipy.stats</kbd> as follows:</p>
<pre>In [20]:import matplotlib
        from scipy.stats import bernoulli
        a = np.arange(2)
    
        colors = matplotlib.rcParams['axes.color_cycle']
        plt.figure(figsize=(12,8))
        for i, p in enumerate([0.0, 0.2, 0.5, 0.75, 1.0]):
             ax = plt.subplot(1, 5, i+1)
             plt.bar(a, bernoulli.pmf(a, p), label=p, color=colors[i], alpha=0.5)
             ax.xaxis.set_ticks(a)
    
            plt.legend(loc=0)
                if i == 0:
                     plt.ylabel("PDF at $k$")
    
    
            plt.suptitle("Bernoulli probability for various values of $p$")</pre>
<p>The following is the output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a4098dfe-edfe-4110-9326-c6d2ca5cc42a.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Bernoulli distribution output</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The binomial distribution</h1>
                </header>
            
            <article>
                
<p>The binomial distribution is used to represent the number of successes in <em>n</em>-independent Bernoulli trials, which can be expressed as the following:</p>
<p class="CDPAlignCenter CDPAlign"><span class="IconPACKT"><em>Y = X<sub>1</sub> + X<sub>2</sub> + ..</em></span><em>. + X<sub>n</sub></em> </p>
<p>Using the coin toss example, this distribution models the chance of getting <em>X</em> heads over <em>n</em> trials. For 100 tosses, the binomial distribution models the likelihood of getting 0 heads (extremely unlikely) to 50 heads (highest likelihood) to 100 heads (also extremely unlikely). This ends up making the binomial distribution symmetrical when the odds are perfectly even and skewed when the odds are far less even. The PMF is given by the following expression:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/8e13d328-8677-46e3-b3a9-ed0b87abaaf7.png" style="width:15.33em;height:2.92em;"/></div>
<p>The expectation and variance are given respectively by the following expressions:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/639a8dc3-b41c-4c7b-b121-237eb9450624.png" style="width:4.92em;height:1.25em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7d232401-9732-454a-984d-462be5c08854.png" style="width:9.50em;height:1.25em;"/></div>
<p>This is shown using the following code block:</p>
<pre>     from scipy.stats import binom
       clrs = ['blue','green','red','cyan','magenta']     plt.figure(figsize=(12,6))
      k = np.arange(0, 22)
      for p, color in zip([0.001, 0.1, 0.3, 0.6, 0.999], clrs):
           rv = binom(20, p)
           plt.plot(k, rv.pmf(k), lw=2, color=color, label="$p$=" + str(round(p,1)))
    
            plt.legend()
      plt.title("Binomial distribution PMF")
      plt.tight_layout()
      plt.ylabel("PDF at $k$")
      plt.xlabel("$k$")
  </pre>
<p>The following is the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1663 image-border" src="assets/37442818-7135-48bc-94ff-1bc568e34525.png" style="width:72.75em;height:36.92em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Binomial distribution</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Poisson distribution</h1>
                </header>
            
            <article>
                
<p>The Poisson distribution models the probability of a number of events within a given time interval, assuming that these events occur with a known average rate and successive events occur independently of the time that has passed since the previous event.</p>
<p>A concrete example of a process that can be modeled by a Poisson distribution would be if an individual received an average of, say, 23 emails per day. If we assume that the arrival times for the emails are independent of each other, then the total number of emails an individual receives each day can be modeled by a Poisson distribution.</p>
<p>Another example could be the number of trains that stop at a particular station each hour. The PMF for a Poisson distribution is given by the following expression:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/12e39851-b53f-4f77-8290-d223a896ceea.png" style="width:6.92em;height:2.67em;"/></div>
<p>Here, <strong>λ</strong> is the rate parameter, which, represents the expected number of events/arrivals that occur per unit time, and <strong><em>k</em></strong> is the random variable that represents the number of events/arrivals.</p>
<p>The expectation and variance are given respectively by the following expressions:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/17e854ab-f1df-4d2f-9048-055fe6cd38e1.png" style="width:4.33em;height:1.25em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bf6fc199-c828-43c3-bbbe-a40943d3655f.png" style="width:5.75em;height:1.33em;"/></p>
<p>For more information, refer to <a href="http://en.wikipedia.org/wiki/Poisson_process"><span class="URLPACKT">http://en.wikipedia.org/wiki/Poisson_process</span></a>.</p>
<p>The PMF is plotted using <kbd>matplotlib</kbd> for various values as follows:</p>
<pre>    In [11]: %matplotlib inline
       import numpy as np
       import matplotlib
       import matplotlib.pyplot as plt
       from scipy.stats import poisson
       colors = matplotlib.rcParams['axes.color_cycle']
       k=np.arange(15)
       plt.figure(figsize=(12,8))
       for i, lambda_ in enumerate([1,2,4,6]):
            plt.plot(k, poisson.pmf(k, lambda_), '-o',  
            label="$\lambda$=" + str(lambda_), color=colors[i])
            plt.legend()
       plt.title("Possion distribution PMF for various $\lambda$")
       plt.ylabel("PMF at $k$")
       plt.xlabel("$k$")
       plt.show()
  </pre>
<p>The following is the output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/d76cc92f-0a38-40f2-9b7a-5d4ac0835b5d.png"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Poisson distribution</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The geometric distribution</h1>
                </header>
            
            <article>
                
<p>The geometric distribution is used f<span>or independent Bernoulli trials</span><span> and </span><span>measures </span><span>the number of trials (<em>X</em>) that are needed to get one success. It can also represent the number of failures (</span><em>Y = X-1</em><span>) before the first success.</span></p>
<p>The PMF is expressed as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/0bb196e0-45af-40d2-9840-129a97a651d6.png" style="width:9.50em;height:1.58em;"/></div>
<p>The preceding expression makes sense because <span class="IconPACKT"><em>f(k) = P(X =k)</em>,</span> and if it takes <em>k</em> trials to get one success (<em>p</em>), then this means that we must have had <em>k-1 </em>failures, which are equal to<span class="IconPACKT"> <em>(1-p)</em></span>.</p>
<p>The expectation and variance are given as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/edd2251a-db95-48b2-931d-bb8ec4f49ef6.png" style="width:5.25em;height:1.25em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ea239973-c582-4e22-9605-66a7ff59a7a7.png" style="width:9.33em;height:1.25em;"/></div>
<p>The following command explains the preceding formula clearly:</p>
<pre>    In [12]: from scipy.stats import geom
          p_vals=[0.01,0.2,0.5,0.8,0.9]
       x = np.arange(geom.ppf(0.01,p),geom.ppf(0.99,p))
            colors = matplotlib.rcParams['axes.color_cycle']
            for p,color in zip(p_vals,colors):
              x = np.arange(geom.ppf(0.01,p),geom.ppf(0.99,p))
                 plt.plot(x,geom.pmf(x,p),'-o',ms=8,label='$p$=' + str(p))
              plt.legend(loc='best')
            plt.ylim(-0.5,1.5)
            plt.xlim(0,7.5)
            plt.ylabel("Pmf at $k$")
            plt.xlabel("$k$")
            plt.title("Geometric distribution PMF")</pre>
<p>The following is the output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/9da8cd2c-f58f-4eae-aeac-222598dc2050.png" style="width:22.42em;height:16.25em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Geometric distribution</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The negative binomial distribution</h1>
                </header>
            
            <article>
                
<p>The negative binomial distribution is used for <span>independent Bernoulli trials and </span><span>measures the number of tries (</span><span class="IconPACKT"><em>X=k</em></span><span>) that are needed before a specified number of successes (<em>r</em>) occur. An example would be the number of coin tosses it would take to obtain five heads. The PMF is given as follows:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/30d65342-aed7-42aa-8d37-4a612d9eafca.png" style="width:17.83em;height:2.58em;"/></div>
<p>The expectation and variance are given respectively by the following expressions:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/d92080e8-a583-4b9f-8aca-c84c0625d789.png" style="width:5.50em;height:1.83em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/88bb3682-ee52-4bd6-b669-5d9471efc8ed.png" style="width:7.25em;height:1.92em;"/></div>
<p>We can see that the negative binomial is a generalization of the geometric distribution, with the geometric distribution being a special case of the negative binomial, where <span class="IconPACKT"><em>r=1</em></span>.</p>
<p>The code and plot are shown as follows:</p>
<pre>    In [189]: from scipy.stats import nbinom
           from matplotlib import colors
        clrs = matplotlib.rcParams['axes.color_cycle']
        x = np.arange(0,11)
        n_vals = [0.1,1,3,6]
        p=0.5
              for n, clr in zip(n_vals, clrs):
                  rv = nbinom(n,p)
                  plt.plot(x,rv.pmf(x), label="$n$=" + str(n), color=clr)
                  plt.legend()
              plt.title("Negative Binomial Distribution PMF")
              plt.ylabel("PMF at $x$")
              plt.xlabel("$x$")</pre>
<p>The following is the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1664 image-border" src="assets/c38c6b79-474d-4e5a-80b8-7db8bf94687f.png" style="width:28.25em;height:20.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Negative binomial distribution</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous probability distributions</h1>
                </header>
            
            <article>
                
<p>In a continuous probability distribution, the variable can take on any real number. It is not limited to a finite set of values, as it is in a discrete probability distribution; for example, the average weight of a healthy newborn baby can range from <span>approximately 6–9 lbs (its weight can be 7.3 lbs, for example). A continuous probability distribution is characterized by a</span> <strong>probability density function</strong> <span>(</span><strong>PDF</strong><span>).</span></p>
<p>The sum of all the probabilities that the random variable can assume is 1. Thus, the area under the graph of the probability density function adds up to 1.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The continuous uniform distribution</h1>
                </header>
            
            <article>
                
<p>The uniform distribution models a random variable, <em>X,</em> that can take any value within the range <em>[a, b] </em>with equal probability.</p>
<p>The PDF is given by <img class="fm-editor-equation" src="assets/9f0003f2-af42-4de4-9b82-37aeab3a755f.png" style="width:5.33em;height:2.08em;"/> for <em>a ≤ x ≤ b</em>, and 0 otherwise.</p>
<p>The expectation and variance are given respectively by the following expressions:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/177876da-7f87-4816-9d9f-8a3c150d43ab.png" style="width:8.17em;height:1.25em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/470ae43e-1c0e-4d2d-a58c-4ceb432e1707.png" style="width:10.08em;height:1.33em;"/></div>
<p>A continuous uniform probability distribution is generated and plotted for various sample sizes in the following code:</p>
<pre>    In [11]: np.random.seed(100)  # seed the random number generator
                                  # so plots are reproducible
             subplots = [111,211,311]
             ctr = 0 
             fig, ax = plt.subplots(len(subplots), figsize=(10,12))
             nsteps=10
             for i in range(0,3):
               cud = np.random.uniform(0,1,nsteps) # generate distrib
               count, bins, ignored = ax[ctr].hist(cud,15,normed=True)
               ax[ctr].plot(bins,np.ones_like(bins),linewidth=2, color='r')
               ax[ctr].set_title('sample size=%s' % nsteps)
               ctr += 1
               nsteps *= 100
             fig.subplots_adjust(hspace=0.4)
             plt.suptitle("Continuous Uniform probability distributions for various sample sizes" , fontsize=14)
 <strong> </strong></pre>
<p>The following is the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1772 image-border" src="assets/c5d15705-521e-4d31-be41-fdadad5af9f6.png" style="width:34.75em;height:39.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Continuous uniform distribution</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The exponential distribution</h1>
                </header>
            
            <article>
                
<p>The exponential distribution models the waiting time between two events in a Poisson process. A Poisson process is a process that follows a Poisson distribution in which events occur unpredictably with a known average rate. The exponential distribution can be described as the <em>continuous limit</em> of the geometric distribution, and is also Markovian (memoryless).</p>
<p>A memoryless random variable exhibits the property that its future state depends only on relevant information about the current time and not the information from further in the past. An example of modeling a Markovian/memoryless random variable is modeling short-term stock price behavior based on the idea that it follows a random walk. This leads to what is called the efficient market hypothesis in finance. For more information, refer to <a href="http://en.wikipedia.org/wiki/Random_walk_hypothesis"><span class="URLPACKT">http://en.wikipedia.org/wiki/Random_walk_hypothesis</span></a>.</p>
<p>The PDF of the exponential distribution is given by <em>f(x)= λe<sup>-λx</sup></em><span class="IconPACKT">.</span> The expectation and variance are given respectively by the following expressions:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><em>E(X) = 1/λ</em></div>
<div class="packt_figure CDPAlignCenter CDPAlign"><em>Var(X) = 1/λ<sup>2</sup></em></div>
<p>For reference, refer to the link at <a href="http://en.wikipedia.org/wiki/Exponential_distribution"><span class="URLPACKT">http://en.wikipedia.org/wiki/Exponential_distribution</span></a>.</p>
<p>The plot of the distribution and code is given as follows:</p>
<pre>    In [15]: import scipy.stats
             clrs = colors.cnames
               x = np.linspace(0,4, 100)
            expo = scipy.stats.expon
            lambda_ = [0.5, 1, 2, 5]
           plt.figure(figsize=(12,4))
           for l,c in zip(lambda_,clrs):
               plt.plot(x, expo.pdf(x, scale=1./l), lw=2,
                          color=c, label = "$\lambda = %.1f$"%l)
                 plt.legend()
                 plt.ylabel("PDF at $x$")
          plt.xlabel("$x$")
          plt.title("Pdf of an Exponential random variable for various $\lambda$");
  </pre>
<p>The following is the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1666 image-border" src="assets/827f7798-a218-482a-bd30-480b22484ccd.png" style="width:59.75em;height:23.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Exponential distribution</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The normal distribution</h1>
                </header>
            
            <article>
                
<p>The most important distribution in statistics is arguably the normal/Gaussian distribution. It models the probability distribution around a central value with no left or right bias. There are many examples of phenomena that follow the normal distribution, such as the following:</p>
<ul>
<li>The birth weights of babies</li>
<li>Measurement errors</li>
<li>Blood pressure</li>
<li>Test scores</li>
</ul>
<p>The normal distribution's importance is underlined by the central limit theorem, which states that the mean of many random variables drawn independently from the same distribution is approximately normal, regardless of the form of the original distribution. Its expected value and variance are given as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><em>E(X) = μ</em></div>
<div class="packt_figure CDPAlignCenter CDPAlign"><em>Var(X) = σ<sup>2</sup></em></div>
<p>The PDF of the normal distribution is given by the following expression:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c9b5cd60-9150-4c82-a3c7-130e320c1deb.png" style="width:15.50em;height:3.00em;"/></div>
<p>The following code and plot explain the formula:</p>
<pre>    In [54]: import matplotlib
        from scipy.stats import norm
          X = 2.5
       dx = 0.1
       R = np.arange(-X,X+dx,dx)
    
       L = list()
       sdL = (0.5,1,2,3)
       for sd in sdL:
            f = norm.pdf
            L.append([f(x,loc=0,scale=sd) for x in R])
    
       colors = matplotlib.rcParams['axes.color_cycle']
       for sd,c,P in zip(sdL,colors,L):
            plt.plot(R,P,zorder=1,lw=1.5,color=c,
                    label="$\sigma$=" + str(sd))
            plt.legend()
    
       ax = plt.axes()
       ax.set_xlim(-2.1,2.1)
       ax.set_ylim(0,1.0)
       plt.title("Normal distribution Pdf")
       plt.ylabel("PDF at $\mu$=0, $\sigma$")
  </pre>
<p>The following is the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1773 image-border" src="assets/016bcf9c-4410-447c-8170-ca277b48cf20.png" style="width:23.67em;height:16.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Normal distribution</div>
<p>A reference for the Python code to the plot the distributions can be found at <a href="http://bit.ly/1E17nYx"><span class="URLPACKT">http://bit.ly/1E17nYx</span></a>.</p>
<p>The normal distribution can also be regarded as the continuous limit of the binomial distribution, and other distributions as <img class="fm-editor-equation" src="assets/17367aee-d5be-4fec-9960-9563349eaa5c.png" style="width:3.00em;height:0.67em;"/>. We can see this in the following code and plot:</p>
<pre>    In [18]:from scipy.stats import binom
         from matplotlib import colors
      cols = colors.cnames
      n_values = [1, 5,10, 30, 100]
    
      subplots = [111+100*x for x in range(0,len(n_values))]
      ctr = 0 
      fig, ax = plt.subplots(len(subplots), figsize=(6,12))
      k = np.arange(0, 200)
      p=0.5
    
      for n, color in zip(n_values, cols):
              k=np.arange(0,n+1)
              rv = binom(n, p)
              ax[ctr].plot(k, rv.pmf(k), lw=2, color=color)
              ax[ctr].set_title("$n$=" + str(n))
              ctr += 1
    
      fig.subplots_adjust(hspace=0.5)
      plt.suptitle("Binomial distribution PMF (p=0.5) for various values of n", fontsize=14)
  </pre>
<p>The following is the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1669 image-border" src="assets/02ca92ac-5aec-421d-8aa9-19baf2f6669a.png" style="width:24.67em;height:45.92em;"/></div>
<p>As <em>n</em> increases, the binomial distribution approaches the normal distribution. In fact, <span>this is clearly seen in the preceding plots </span><span>for</span> <em>n&gt;=30</em><span>.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayesian statistics versus frequentist statistics</h1>
                </header>
            
            <article>
                
<p>In statistics today, there are two schools of thought as to how we interpret data and make statistical inferences. The classical and more dominant approach to date has been what is termed the frequentist approach (refer to <a href="4ff075ff-5491-4503-a1ca-eb62f1704cf4.xhtml"><span class="ChapterrefPACKT">Chapter 7</span></a>, <em>A Tour of Statistics – The Classical Approach</em>). We are looking at the Bayesian approach in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is probability?</h1>
                </header>
            
            <article>
                
<p>At the heart of the debate between the Bayesian and frequentist worldview is the question of how we define probability.</p>
<p>In the frequentist worldview, probability is a notion that is derived from the frequencies of repeated events—for example, when we define the probability of getting heads when a fair coin is tossed as being equal to half. This is because when we repeatedly toss a fair coin, the number of heads divided by the total number of coin tosses approaches 0.5 when the number of coin tosses is sufficiently large.</p>
<p>The Bayesian worldview is different, and the notion of probability is that it is related to one's degree of belief in the event happening. Thus, for a Bayesian statistician, having a belief that the probability of a fair die turning up five is <em>1/6</em> relates to our belief in the chances of that event occurring.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How the model is defined</h1>
                </header>
            
            <article>
                
<p>From the model definition point of view, frequentists analyze how data and calculated metrics vary by making use of repeated experiments while keeping the model parameters fixed. Bayesians, on the other hand, utilize fixed experimental data, but vary their degrees of belief in the model parameters. This is explained as follows:</p>
<ul>
<li><strong>Frequentists</strong>: If the models are fixed, data varies.</li>
<li><strong>Bayesians</strong>: If the data is fixed, models vary.</li>
</ul>
<p>The frequentist approach is to use what is known as the maximum likelihood method to estimate model parameters. It involves generating data from a set of independent and identically distributed observations and fitting the observed data to the model. The value of the model parameter that best fits the data is the <strong>maximum likelihood estimator</strong> (<strong>MLE</strong>), which can sometimes be a function of the observed data.</p>
<p class="mce-root"/>
<p>Bayesianism approaches the problem differently from a probabilistic framework. A probability distribution is used to describe the uncertainty in the values. Bayesian practitioners estimate probabilities using observed data. In order to compute these probabilities, they make use of a single estimator, which is the Bayes formula. This produces a distribution rather than just a point estimate, as in the case of the frequentist approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confidence (frequentist) versus credible (Bayesian) intervals</h1>
                </header>
            
            <article>
                
<p>Let's compare what is meant by a 95 percent confidence interval (a term used by frequentists) with a 95 percent credible interval (a term used by Bayesian practitioners).</p>
<p>In a frequentist framework, a 95 percent confidence interval means that if you repeat your experiment an infinite number of times, generating intervals in the process, 95 percent of these intervals would contain the parameter we're trying to estimate, which is often referred to as θ. In this case, the interval is the random variable and not the parameter estimate, θ, which is fixed in the frequentist worldview.</p>
<p>In the case of the Bayesian credible interval, we have an interpretation that is more in line with the conventional interpretation ascribed to that of a frequentist confidence interval. Thus, we conclude that <em>Pr(a(Y) &lt; θ &lt; b(Y)|θ) = 0.95</em><span class="IconPACKT">.</span> In this case, we can properly conclude that there is a 95 percent chance that θ lies within the interval.</p>
<p>For more information, refer to <em>Frequentism and Bayesianism: What's the Big Deal?</em> (Jake VanderPlas<em>, </em><em>SciPy, 2014</em><em>)</em> <span>at</span> <a href="https://www.youtube.com/watch?v=KhAUfqhLakw"><span class="URLPACKT">https://www.youtube.com/watch?v=KhAUfqhLakw</span></a><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conducting Bayesian statistical analysis</h1>
                </header>
            
            <article>
                
<p>Conducting a Bayesian statistical analysis involves the following steps:</p>
<ol>
<li><strong>Specifying a probability model</strong>: In this step, we fully describe the model using a probability distribution. Based on the distribution of a sample that we have taken, we try to fit a model to it and attempt to assign probabilities to unknown parameters.</li>
<li><strong>Calculating a posterior distribution</strong>: The posterior distribution is a distribution that we calculate in light of observed data. In this case, we will directly apply Bayes' formula. It will be specified as a function of the probability model that we specified in the previous step.</li>
</ol>
<p class="mce-root"/>
<ol start="3">
<li><strong>Checking our model</strong>: This is a necessary step where we review our model and its outputs before we make inferences. Bayesian inference methods use probability distributions to assign probabilities to possible outcomes.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monte Carlo estimation of the likelihood function and PyMC</h1>
                </header>
            
            <article>
                
<p>Bayesian statistics isn't just another method. It is <span>an</span><span> </span><span>entirely different paradigm for practicing statistics. It uses probability models for making inferences, given the data that has been collected. This can be expressed in a fundamental expression as <em>P</em>(<em>H</em>|<em>D</em>).</span></p>
<p>Here, <em>H</em> is our hypothesis, that is, the thing we're trying to prove, and <em>D</em> is our data or observations.</p>
<p>As a reminder of our previous discussion, the diachronic form of Bayes' theorem is as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/f977f4b1-d41e-4066-8fb5-68df68f63ef2.png" style="width:12.92em;height:2.67em;"/></div>
<p>Here, <em>P(H)</em> is an unconditional prior probability that represents what we know before we conduct our trial. <em>P(D|H)</em> is our likelihood function, or probability of obtaining the data we observe, given that our hypothesis is true.</p>
<p><em>P(D)</em> is the probability of the data, also known as the normalizing constant. This can be obtained by integrating the numerator over <em>H</em>.</p>
<p>The likelihood function is the most important piece of our Bayesian calculation and encapsulates all the information about the unknowns in the data. It has some semblance to a reverse probability mass function.</p>
<p>One argument against adopting a Bayesian approach is that the calculation of the prior can be subjective. There are many arguments in favor of this approach, one being that external prior information can be included, as mentioned previously.</p>
<p>The likelihood value represents an unknown integral, which in simple cases can be obtained by analytic integration.</p>
<p><strong>Monte Carlo</strong> (<strong>MC</strong>) integration is needed for more complicated use cases involving higher-dimensional integrals and can be used to compute the likelihood function.</p>
<p>MC integration can be computed through a variety of sampling methods, such as uniform sampling, stratified sampling, and importance sampling. In Monte Carlo integration, we can approximate the integral as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/5f0e1ceb-ffad-4e02-afdb-f9cdaff9f523.png" style="width:6.75em;height:2.83em;"/></div>
<p>The following is the finite sum:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e024771a-b52d-449d-ab59-4f62abef098c.png" style="width:10.25em;height:3.58em;"/></div>
<p>Here, <em>x</em> is a sample vector from <em>g</em>. The proof that this estimate is a good one can be obtained from the law of large numbers and by making sure that the simulation error is small.</p>
<p>When conducting Bayesian analysis in Python, we will need a module that will enable us to calculate the likelihood function using the Monte Carlo method. The <kbd>PyMC</kbd> library fulfills that need. It provides a Monte Carlo method known commonly as MCMC. We will not delve further into the technical details of MCMC, but the interested reader can find out more about MCMC implementation in <kbd>PyMC</kbd> from the following sources:</p>
<ul>
<li><em>Monte Carlo Integration in Bayesian Estimation</em>:<em> </em><a href="http://bit.ly/1bMALeu"><span class="URLPACKT">http://bit.ly/1bMALeu</span></a></li>
<li><em>Markov Chain Monte Carlo Maximum Likelihood</em>: <a href="http://bit.ly/1KBP8hH"><span class="URLPACKT">http://bit.ly/1KBP8hH</span></a></li>
<li><em>Bayesian Statistical Analysis Using Python–Part 1</em>, <em>SciPy 2014</em>, <em>Chris Fonnesbeck</em>: <a href="http://www.youtube.com/watch?v=vOBB_ycQ0RA"><span class="URLPACKT">http://www.youtube.com/watch?v=vOBB_ycQ0RA</span></a></li>
</ul>
<p>MCMC is not a universal panacea; there are some drawbacks to the approach, and one of them is the slow convergence of the algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayesian analysis example – switchpoint detection</h1>
                </header>
            
            <article>
                
<p>Here, we will try to use Bayesian inference and model an interesting dataset. The dataset in question consists of the author's <strong>Facebook</strong> (<strong>FB</strong>) post history over time. We have scrubbed the FB history data and saved the dates in the <kbd>fb_post_dates.txt</kbd> file. Here is what the data in the file looks like:</p>
<pre>    head -2 ../fb_post_dates.txt 
    Tuesday, September 30, 2014 | 2:43am EDT
    Tuesday, September 30, 2014 | 2:22am EDT</pre>
<p>Thus, we see a datetime series, representing the date and time at which the author posted on FB. First, we read the file into DataFrame, separating the timestamp into <kbd>Date</kbd> and <kbd>Time</kbd> columns:</p>
<pre>    In [91]: filePath="./data/fb_post_dates.txt"
             fbdata_df=pd.read_csv(filePath,  sep='|', parse_dates=[0], header=None,names=['Date','Time'])</pre>
<p>Next, we inspect the data as follows:</p>
<pre>    In [92]: fbdata_df.head()  #inspect the data
   <strong> Out[92]:   Date       Time
    0  2014-09-30   2:43am EDT
    1  2014-09-30   2:22am EDT
    2  2014-09-30   2:06am EDT
    3  2014-09-30   1:07am EDT
    4  2014-09-28   9:16pm EDT
  </strong></pre>
<p>Now, we index the data by <kbd>Date</kbd>, creating a <kbd>DatetimeIndex</kbd> so that we can run resample on it to count by month, as follows:</p>
<pre>    In [115]: fbdata_df_ind=fbdata_df.set_index('Date')
              fbdata_df_ind.head(5)
    <strong>Out[115]:                      
             Date        Time
             2014-09-30  2:43am EDT
             2014-09-30  2:22am EDT
             2014-09-30  2:06am EDT
             2014-09-30  1:07am EDT
             2014-09-28  9:16pm EDT </strong> </pre>
<p>We then display information about the index as follows:</p>
<pre>    In [116]: fbdata_df_ind.index
    <strong>Out[116]: &lt;class 'pandas.tseries.index.DatetimeIndex'&gt;
              [2014-09-30, ..., 2007-04-16]
              Length: 7713, Freq: None, Timezone: None </strong></pre>
<p>Next, we obtain a count of posts by month using resample:</p>
<pre>    In [99]: fb_mth_count_=fbdata_df_ind.resample('M', how='count')
             fb_mth_count_.rename(columns={'Time':'Count'},
                                  inplace=True)   # Rename 
             fb_mth_count_.head()
<strong>    Out[99]:            Count
            Date
            2007-04-30  1
            2007-05-31  0
            2007-06-30  5
            2007-07-31  50
            2007-08-31  24 </strong>  </pre>
<p>The <kbd>Date</kbd> format is shown as the last day of the month. Now, we create a scatter plot of FB post counts from 2007–2015, and we make the size of the dots proportional to the values in <kbd>matplotlib</kbd>:</p>
<pre>    In [108]: %matplotlib inline
              import datetime as dt
    #Obtain the count data from the DataFrame as a dictionary
              year_month_count=fb_bymth_count.to_dict()['Count'] 
              size=len(year_month_count.keys())
    #get dates as list of strings
              xdates=[dt.datetime.strptime(str(yyyymm),'%Y%m') 
                      for yyyymm in year_month_count.keys()] 
              counts=year_month_count.values()
              plt.scatter(xdates,counts,s=counts)
             plt.xlabel('Year')
              plt.ylabel('Number of Facebook posts')
              plt.show() 
  </pre>
<p>The following is the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1670 image-border" src="assets/f8f54069-a899-4d51-b70c-973445ee602d.png" style="width:24.83em;height:17.08em;"/></div>
<p>The question we would like to investigate is whether there was a change in behavior at some point over the time period. Specifically, we want to identify whether there was a specific period at which the mean number of FB posts changed. This is often referred to as the switchpoint or changepoint in a time series.</p>
<p>We can make use of the Poisson distribution to model this. You might recall that the Poisson distribution can be used to model time series count data. (Refer to <a href="http://bit.ly/1JniIqy"><span class="URLPACKT">http://bit.ly/1JniIqy</span></a> for more about this.)</p>
<p>If we represent our monthly FB post count by <span class="IconPACKT"><em>C<sub>i</sub></em>,</span> we can represent our model as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8e251d82-2dfc-4635-b9d2-7e9283ff1894.png" style="width:10.75em;height:1.17em;"/></div>
<p>The <span class="IconPACKT"><em>r<sub>i</sub></em></span> parameter is the rate parameter of the Poisson distribution, but we don't know what its value is. If we examine the scatter plot of the FB time series count data, we can see that there was a jump in the number of posts sometime around mid to late 2010, perhaps coinciding with the start of the 2010 World Cup in South Africa, which the author attended.</p>
<p>The <em>s</em> parameter is the switchpoint, which is when the rate parameter changes, while <em>e</em> and <em>l</em> are the values of the <em>r<sub>i</sub></em> parameter before and after the switchpoint respectively. This can be represented as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/361ba479-3bcd-49ed-bd8c-a0ec8ca56a8f.png" style="width:8.92em;height:3.00em;"/></div>
<p>Note that the variables specified here—<em>C, s, e, r</em> and <em>l—</em>are all Bayesian random variables. For Bayesian random variables that represent one's beliefs about their values, we need to model them using a probability distribution. We would like to infer the values of <em>e</em> and <em>l</em><span class="IconPACKT">,</span> which are unknown. In <kbd>PyMC</kbd>, we can represent random variables using the stochastic and deterministic classes. We note that the exponential distribution is the amount of time between Poisson events. Therefore, in the case of <em>e</em> and <em>l</em>, we choose the exponential distribution to model them since they can be any positive number:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7756ac98-afa8-4b65-8691-33945a71f9e7.png" style="width:4.92em;height:1.25em;"/></div>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/df4d13c3-7654-43ff-a018-4deed64fff6a.png" style="width:5.00em;height:1.33em;"/></div>
<p>In the case of <em>s</em>, we will choose to model it using the uniform distribution, which reflects our belief that it is equally likely that the switchpoint can occur on any day within the entire time period. This means that we have the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f1e4231b-aff6-4f0c-a7e3-9b297f2c112d.png" style="width:14.83em;height:1.42em;"/></div>
<p>Here, <em>t<sub>0</sub> , t<sub>f</sub></em> corresponds to the lower and upper boundaries of the year, <em>i</em><span class="IconPACKT">.</span> Let's now use <kbd>PyMC</kbd> to represent the model that we developed earlier. We will now use <kbd>PyMC</kbd> to see whether we can detect a switchpoint in the FB post data. In addition to the scatter plot, we can also display the data in a bar chart. To do that, we first need to obtain a count of FB posts ordered by month in a list:</p>
<pre>    In [69]: fb_activity_data = [year_month_count[k] for k in 
                                 sorted(year_month_count.keys())]
            fb_activity_data[:5]
    
<strong>    Out[70]: [1, 0, 5, 50, 24]</strong>
    
    In [71]: fb_post_count=len(fb_activity_data)</pre>
<p>We render the bar plot using <kbd>matplotlib</kbd>:</p>
<pre>    In [72]: from IPython.core.pylabtools import figsize
             import matplotlib.pyplot as plt
               figsize(8, 5)
            plt.bar(np.arange(fb_post_count),
             fb_activity_data, color="#49a178")
            plt.xlabel("Time (months)")
            plt.ylabel("Number of FB posts")
            plt.title("Monthly Facebook posts over time")
            plt.xlim(0,fb_post_count);  </pre>
<p>The following is the output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/55734643-b76f-4798-972c-d844d30bbd60.png" style="width:31.67em;height:21.33em;"/></div>
<p>Looking at the preceding bar chart, can we conclude that there was a change in FB frequency posting behavior over time? We can use <kbd>PyMC</kbd> on the model that we have developed to help us find out the change as follows:</p>
<pre>    In [88]: # Define data and stochastics
            import pymc as pm
            switchpoint = pm.DiscreteUniform('switchpoint',
                                              lower=0,
                                        upper=len(fb_activity_data)-1,
                                           doc='Switchpoint[month]')
            avg = np.mean(fb_activity_data)
            early_mean = pm.Exponential('early_mean', beta=1./avg)
            late_mean = pm.Exponential('late_mean', beta=1./avg)
            late_mean
<strong>    Out[88]:&lt;pymc.distributions.Exponential 'late_mean' at 0x10ee56d50&gt; </strong> </pre>
<p>Here, we define a method for the rate parameter, <em>r</em>, and we model the count data using a Poisson distribution as discussed previously:</p>
<pre>    In [89]: @pm.deterministic(plot=False)
             def rate(s=switchpoint, e=early_mean, l=late_mean):
                 ''' Concatenate Poisson means '''
                 out = np.zeros(len(fb_activity_data))
                 out[:s] = e
                 out[s:] = l
             return out
    
             fb_activity = pm.Poisson('fb_activity', mu=rate,   
                           value=fb_activity_data, observed=True)
             fb_activity
<strong>    </strong><strong>Out[89]: &lt;pymc.distributions.Poisson 'fb_activity' at 0x10ed1ee50&gt; </strong> </pre>
<p>In the preceding code snippet, <kbd>@pm.deterministic</kbd> is a decorator that denotes that the rate function is deterministic, meaning that its values are entirely determined by other variables—in this case, <em>e</em>, <em>s</em>, and <em>l</em>. The decorator is necessary to tell <kbd>PyMC</kbd> to convert the rate function into a deterministic object. If we do not specify the decorator, an error occurs. For more information about <span>Python decorators</span><span>, refer to</span> <span class="URLPACKT"><a href="http://bit.ly/1zj8U0o">http://bit.ly/1zj8U0o</a></span><span>.</span></p>
<p>For more information, refer to the following web pages:</p>
<ul>
<li><span class="URLPACKT"><a href="http://en.wikipedia.org/wiki/Poisson_process">http://en.wikipedia.org/wiki/Poisson_process</a></span></li>
<li><a href="http://pymc-devs.github.io/pymc/tutorial.html"><span class="URLPACKT">http://pymc-devs.github.io/pymc/tutorial.html</span></a></li>
<li><a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers"><span class="URLPACKT">https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers</span></a></li>
</ul>
<p>We will now create a model with the FB count data (<kbd>fb_activity</kbd>) and the <em>e, s, l</em> (<kbd>early_mean</kbd>, <kbd>late_mean</kbd>, and <kbd>rate</kbd> respectively) parameters.</p>
<p>Next, using <kbd>PyMC</kbd>, we create an <kbd>MCMC</kbd> object that enables us to fit our data using MCMC methods. We then call sample on the resulting <kbd>MCMC</kbd> object to do the fitting:</p>
<pre>    In [94]: fb_activity_model=pm.Model([fb_activity,early_mean,
                                         late_mean,rate])
    In [95]: from pymc import MCMC
               fbM=MCMC(fb_activity_model)
    In [96]: fbM.sample(iter=40000,burn=1000, thin=20)
               [-----------------100%-----------------] 40000 of 40000 
              complete in 11.0 sec
  </pre>
<p>Fitting the model using MCMC involves using Markov Chain Monte Carlo methods to generate a probability distribution for the posterior, <em>P(s,e,l | D)</em>. It uses the Monte Carlo process to repeatedly simulate sampling of the data and does this until the algorithm seems to converge to a steady state, based on multiple criteria. This is a Markov process because successive samples are dependent only on the previous sample. For further information on Markov chain convergence, refer to <a href="http://bit.ly/1IETkhC"><span class="URLPACKT">http://bit.ly/1IETkhC</span></a>.</p>
<p>The generated samples are referred to as <strong>traces</strong>. We can view what the marginal posterior distribution of the parameters looks like by looking at a histogram of its trace:</p>
<pre>    In [97]: from pylab import hist,show
               %matplotlib inline
            hist(fbM.trace('late_mean')[:])
    <strong>Out[97]: (array([  15.,   61.,  214.,  421.,  517.,  426.,  202.,</strong>
    <strong>                   70.,   21.,    3.]),</strong>
    <strong>          array([ 102.29451192,  103.25158404,  104.20865616,</strong>
    <strong>                  105.16572829,  106.12280041,  107.07987253,</strong>
    <strong>                  108.03694465,  108.99401677,  109.95108889,</strong>
    <strong>                  110.90816101,  111.86523313]),</strong>
    <strong>           &lt;a list of 10 Patch objects&gt;)</strong></pre>
<p>The following is the output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/1491b72e-3d78-41fe-91be-3f54de7b51fa.png" style="width:23.83em;height:16.17em;"/></div>
<p>Next, we find the early mean:</p>
<pre>    In [98]:plt.hist(fbM.trace('early_mean')[:])<strong>
    Out[98]: (array([  20.,  105.,  330.,  489.,  470.,  314.,  147.,
                       60.,    3.,   12.]),
              array([ 49.19781192,  50.07760882,  50.95740571,
                      51.83720261,  52.71699951,  53.59679641,
                      54.47659331,  55.35639021,  56.2361871 ,
                      57.115984  ,  57.9957809 ]),
            &lt;a list of 10 Patch objects&gt;)
  </strong></pre>
<p>The following is the output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/29eaa6b2-eb05-44a5-bb86-c1bd8bf65222.png" style="width:22.50em;height:15.25em;"/></div>
<p class="mce-root"/>
<p>Here, we see what the switchpoint <span>looks like</span><span> in terms of the number of months:</span></p>
<pre>    In [99]: fbM.trace('switchpoint')[:]<strong>
    Out[99]: array([38, 38, 38, ..., 35, 35, 35])</strong>
    
    In [150]: plt.hist(fbM.trace('switchpoint')[:])<strong>
    Out[150]: (array([ 1899.,     0.,     0.,     0.,     0.,     0.,</strong>
    <strong>                   0., 0., 0.,    51.]),</strong>
    <strong>           array([ 35. ,  35.3,  35.6,  35.9,  36.2,  36.5,  36.8,</strong>
    <strong>                   37.1,  37.4, 37.7,  38. ]),</strong>
    <strong>           &lt;a list of 10 Patch objects&gt;)</strong>
  </pre>
<p>The following is the output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/be583c20-b4c2-4dcd-ad02-1153c778bdf7.png" style="width:22.83em;height:15.17em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Histogram of the switchpoint for the number of months</div>
<p>We can see that the switchpoint is in the neighborhood of the months 35–38. Here, we use <kbd>matplotlib</kbd> to display the marginal posterior distributions of <em>e</em>, <em>s</em>, and <em>l</em> in a single diagram:</p>
<pre>    In [141]: early_mean_samples=fbM.trace('early_mean')[:]
              late_mean_samples=fbM.trace('late_mean')[:]
              switchpoint_samples=fbM.trace('switchpoint')[:]
    In [142]: from IPython.core.pylabtools import figsize
           figsize(12.5, 10)
        # histogram of the samples:
        fig = plt.figure()
        fig.subplots_adjust(bottom=-0.05)
    
        n_mths=len(fb_activity_data)
        ax = plt.subplot(311)
        ax.set_autoscaley_on(False)
    
         plt.hist(early_mean_samples, histtype='stepfilled',
                  bins=30, alpha=0.85, label="posterior of $e$",
                  color="turquoise", normed=True)
        plt.legend(loc="upper left")
        plt.title(r"""Posterior distributions of the variables
                   $e, l, s$""",fontsize=16)
        plt.xlim([40, 120])
         plt.ylim([0, 0.6])
        plt.xlabel("$e$ value",fontsize=14)
    
         ax = plt.subplot(312)
        ax.set_autoscaley_on(False)
        plt.hist(late_mean_samples, histtype='stepfilled',
                  bins=30, alpha=0.85, label="posterior of $l$",
                  color="purple", normed=True)
        plt.legend(loc="upper left")
        plt.xlim([40, 120])
         plt.ylim([0, 0.6])
        plt.xlabel("$l$ value",fontsize=14)
        plt.subplot(313)
        w = 1.0 / switchpoint_samples.shape[0] *
             np.ones_like(switchpoint_samples)
        plt.hist(switchpoint_samples, bins=range(0,n_mths), alpha=1,
                  label=r"posterior of $s$", color="green",
                  weights=w, rwidth=2.)
        plt.xlim([20, n_mths - 20])
        plt.xlabel(r"$s$ (in days)",fontsize=14)
        plt.ylabel("probability")
        plt.legend(loc="upper left")
    
         plt.show()
  </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following is the output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1774 image-border" src="assets/2b3b9538-cad8-46fb-b4b0-6e5a0ef48b40.png" style="width:42.17em;height:42.17em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Marginal posterior distributions</div>
<p><kbd>PyMC</kbd> also has plotting functionality as it uses <kbd>matplotlib</kbd>. In the following plots, we display a time series plot, an <strong>autocorrelation plot</strong> (<strong>acorr</strong>), and a histogram of the samples drawn for the early mean, late mean, and switchpoint. The histogram is useful for visualizing the posterior distribution. The autocorrelation plot shows whether values in the previous period are strongly related to values in the current period:</p>
<pre>    In [100]: from pymc.Matplot import plot
           plot(fbM)
        Plotting late_mean
        Plotting switchpoint
        Plotting early_mean
  </pre>
<p>The following is the late mean plot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/bfa3afc4-e774-4ae0-918a-c51fd38c37d5.png" style="width:39.50em;height:24.17em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Graph for pymc_comprehensive_late_mean</div>
<p class="mce-root"/>
<p>Here, we display the switchpoint plot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/d093d961-60f7-4e48-aebd-1c755d1fc781.png" style="width:36.25em;height:22.00em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">PyMC comprehensive switchpoint</div>
<p>Here, we display the early mean plot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/027d6b82-1340-4f1f-bec2-929cba701772.png" style="width:35.25em;height:21.83em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">PyMC comprehensive early mean</div>
<p>From the output of <kbd>PyMC</kbd>, we can conclude that the switchpoint is around 35–38 months from the start of the time series. This corresponds to some time between March and July 2010. The author can testify that this was a banner year for him with respect to the use of FB since it was the year of the FIFA World Cup finals that were held in South Africa, which he attended.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maximum likelihood estimate</h1>
                </header>
            
            <article>
                
<p><strong>Maximum likelihood estimate</strong> (<strong>MLE</strong>) is a method that is used to estimate the probability distribution parameters of a population from the available sample data. MLE methods can also be considered as a Bayesian alternative.</p>
<p>Probability distribution gives the probability of observing a data point given distribution parameters such as mean, standard deviation, and degree of freedom.</p>
<p>The probability of data point given distribution parameters is expressed as Prob(<em>X</em>|µ,α) -------1.</p>
<p>MLE deals with the inverse problem. It is used to find the most likely values of distribution parameters given the set of data points. For that purpose, another statistic called likelihood is defined. The likelihood is defined as the probability of observing the distribution parameters given the data points.</p>
<p>The probability of observing the distribution parameters given the data point is expressed as L(µ,α|X)----2.</p>
<p>The quantities in equations 1 and 2 are the same probabilities, just stated differently. Therefore, we can write the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4bb4acad-a045-490d-9c6a-feb61b320ef3.png" style="width:11.92em;height:1.33em;"/></p>
<p>To understand this concept better, have a look at the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1671 image-border" src="assets/9637d944-50f0-4d7e-a0e2-5bcc15c752be.png" style="width:50.33em;height:25.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">MLE estimation illustration for normal distribution with two data points</div>
<p><span>The graph shows three normal distributions with different means and standard deviations. The two vertical lines represent values V</span><sub>1</sub><span>=-2.5 and V</span><sub>2</sub><span>=2.5.</span></p>
<p>Suppose that we represent the probability that a data point V<sub>1</sub>=-2.5 belongs to the red (mean=0, std dev=1.5) distribution by <em>P</em>(red|<em>V</em><sub>1</sub>=-2.5). Similarly, the probability that a data point V<sub>1</sub>=-2.5 belongs to the blue (mean=0, std dev=4) distribution is <em>P</em>(blue|<em>V</em><sub>1</sub>=-2.5).</p>
<p>Now, looking at the graph shown here, we can state the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fae0992a-f40e-494e-b8c4-b12c1e0a001a.png" style="width:12.00em;height:1.25em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/249c806c-3a4a-46bb-bea2-5876169e662e.png" style="width:13.33em;height:1.42em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/561cc532-0621-47b9-9bd8-f9fe21a0b783.png" style="width:14.08em;height:1.42em;"/></p>
<p>If we had to make a decision based on only available datapoints, then we would decide that <em>V</em><sub>1</sub>=2.5 belongs to the blue distribution as <em>V</em><sub>1</sub><em>B</em><sub>1</sub> &gt; <em>V</em><sub>1</sub><em>R</em><sub>1</sub> &gt; <em>V</em><sub>1</sub><em>G</em><sub>1</sub>, and we choose the distribution with the largest probability of that data point belonging to that distribution.</p>
<p>But what if we have one more data point, or many more? For this situation, let's add another data point called <em>V</em><sub>2</sub> to our dataset. The individual probabilities for <em>V</em><sub>2</sub> are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5dc480e2-4bdc-460c-8411-8333f2c3f09d.png" style="width:12.08em;height:1.42em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8875ef97-7d76-42d3-aa77-4ce4aa42b299.png" style="width:12.25em;height:1.42em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/893ba73f-a3e6-4723-8fb3-909a56cd3be9.png" style="width:13.08em;height:1.42em;"/></p>
<p>Since there are two points now, we can't base our decisions on individual probabilities and would have to calculate joint probabilities. If we assume that the event of one data point occurring is independent of the event of another data point occurring, then the joint probabilities are given by the individual probabilities.</p>
<p>Suppose the joint probability that the two points belong to the red distribution given the two data points is denoted by <em>P(red|V<sub>1</sub>=-2.5, V<sub>2</sub>=2.5)</em>. Then the following will be true:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/72265fda-4817-48b5-a819-5ce4803a8558.png" style="width:19.92em;height:1.25em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/90249c71-fee9-45da-9088-adf998f56c71.png" style="width:21.67em;height:1.42em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8b8bd5ac-db67-4b7d-a29d-60f40025f122.png" style="width:22.50em;height:1.42em;"/></p>
<p>We should choose the distribution that has the maximum joint probability. In this case, the blue would have the highest joint probability, as we can conclude by looking at the graph.</p>
<p>As the number of points in the dataset increase, it no longer remains feasible to draw inferences about the maximum joint probability by looking at charts like the preceding charts. We need to resort to algebraic and calculus methods to find the parameters (defining the distribution) to find the distribution that maximises the joint probability of the dataset to belong to that distribution.</p>
<p>If we assume that the individual data points are independent of each other, then the likelihood or probability of observing distribution parameters given all the data points is given by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1a60f846-d1f4-405a-87ac-5d0921e1f3c8.png" style="width:46.67em;height:1.50em;"/></p>
<p>In the MLE calculations, we try to find the values of µ, α that maximize <em>L</em>(µ,α|<em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, <em>X</em><sub>3</sub>, ...., <em>X<sub>n</sub></em>). In that endeavor, taking the log of both sides helps a lot. It doesn't change the objective function as the log is a monotonically increasing function that makes the calculations a lot easier. The log of the likelihood is often called the log-likelihood and is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/09f0ced7-26d1-453c-926d-edb3a6f70f50.png" style="width:56.67em;height:1.42em;"/></p>
<p>To find the maxima of the log-likelihood function, log(<em>L</em>(µ,α|<em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, <em>X</em><sub>3</sub>, ...., <em>X<sub>n</sub></em>)), we can do the following:</p>
<ul>
<li>Take the first derivative of <em>log(L(µ,α|X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub>, ...., X<sub>n</sub>))</em>, function w.r.t µ,α and equate it to 0</li>
<li>Take the second derivative of <em>log(L(µ,α|X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub>, ...., X<sub>n</sub>))</em>, function w.r.t µ,α and confirm that it is negative</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MLE calculation examples</h1>
                </header>
            
            <article>
                
<p>We will now look at two MLE calculation examples. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Uniform distribution</h1>
                </header>
            
            <article>
                
<p>Suppose we have a probability distribution of <em>X</em> that means the following is true:</p>
<p class="CDPAlignCenter CDPAlign"><em>Pr(X=x) = 1/b-a for all a&lt;X&lt;b</em></p>
<p class="CDPAlignCenter CDPAlign"><em>Pr(X=x) = 0 for all other X</em></p>
<p class="CDPAlignLeft CDPAlign">Here, <em>a</em> and <em>b</em> are parameters of uniform distribution.</p>
<p>Since the probability is the same (or uniform) for all the values, it is called a uniform distribution.</p>
<p>Suppose there are <em>n</em> data points in a dataset that is hypothesized to follow a uniform distribution. Based on these points, we aim to find the values of <em>a</em> and <em>b</em> to define the distribution to which these data points most likely belong. For this, we can use the maximum likelihood estimate method:</p>
<p class="mce-root"><img class="fm-editor-equation" src="assets/e7fcec66-609f-4180-903d-8e1f4231b71c.png" style="width:68.92em;height:1.83em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6f4b8981-5693-494e-a6ec-3c9f19c7f053.png" style="width:56.17em;height:1.92em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/04d35de8-0bd9-40d2-9839-9d5c023f042f.png" style="width:23.50em;height:1.33em;"/></p>
<p>We have to find the <em>a</em>, <em>b</em> that maximises <em>log</em>(<em>L</em>(<em>a</em>,<em>b</em>|<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,<em>x</em><sub>3</sub>,.....,<em>x<sub>n</sub></em>).</p>
<p>For this, we would differentiate <em>log</em>(<em>L</em>(<em>a</em>,<em>b</em>|<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,<em>x</em><sub>3</sub>,.....,<em>x<sub>n</sub></em>) with respect to <em>b</em>-<em>a</em>.</p>
<p>This gives -<em>n</em>/(<em>b</em>-<em>a</em>), which is always less than zero, indicating that <em>log</em>(<em>L</em>(<em>a</em>,<em>b</em>|<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,<em>x</em><sub>3</sub>,.....,<em>x<sub>n</sub></em>) is a monotonically decreasing function and its value would decrease as (<em>b</em>-<em>a</em>) increased. Therefore, a large <em>b</em>-<em>a</em> would maximize the probability.</p>
<p>Keeping that in mind, we get <em>b</em>=<em>max</em>(<em>X</em>), <em>a</em> = <em>min</em>(<em>X</em>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Poisson distribution</h1>
                </header>
            
            <article>
                
<p>The Poisson distribution has been explained in an earlier section of this chapter. In a nutshell, the Poisson distribution is a binomial distribution with an infinitely large number of samples, so large that the discrete nature of the binomial distribution gives way to the Poisson distribution. The Poisson distribution also deals with the probability of the occurrence of events. But rather than thinking in terms of the probability of the occurrence of the event in each trial, we think in terms of time intervals and ask ourselves how many times the event of interest would occur in that time interval. The parameter also moves from the probability of success for each trial to the number of successes in a given time interval.</p>
<p>Here's a summary:</p>
<ul>
<li><strong>Binomial</strong>: Probability of a number of successes in a given number of trials given a probability of success for each trial</li>
<li><strong>Poisson</strong>: Probability of a number of successes in a given interval of time given the arrival or success rate—that is, the average number of successes in a given time interval</li>
</ul>
<p>The Poisson probability distribution is expressed by the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2343d4cc-4b5a-4091-b203-3af20735921a.png" style="width:13.42em;height:1.33em;"/></p>
<p>Here, <strong>λ</strong> is the arrival or success rate.</p>
<p>This expression gives the probability of observing <em>x</em> successes in the given time interval (the same interval in which the arrival rate is defined).</p>
<p>We are interested in the MLE estimation of λ given a set of datasets that are supposed to be following a Poisson distribution:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8d678950-6278-43a9-b0b0-952420fcad0c.png" style="width:12.75em;height:1.58em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1672 image-border" src="assets/014a90e3-7e2c-4b2b-a30c-b7b3cf5da8e8.png" style="width:21.00em;height:18.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Maths of the MLE calculation for a Poisson distribution</div>
<p>Note how taking the log eases the calculation algebraically. It introduces some numerical challenges though—for example, making sure that the likelihood is never 0, as log cannot be defined as 0. Numerical methods also result in an invalid value if the log-likelihood is infinitely small.</p>
<p>The MLE finds that the estimate of the arrival rate is equal to the mean of the dataset—that is, the number of observed arrivals in the given time interval in the past. The preceding calculation can be done using NumPy and other supporting packages in Python.</p>
<p>There are several steps that we need to take to perform this calculation in Python:</p>
<ol>
<li> Write a function to calculate the Poisson probability for each point:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import math as mh<br/>np.seterr(divide='ignore', invalid='ignore') #ignore division by zero and invalid numbers<br/>def poissonpdf(x,lbd):<br/>    val = (np.power(lbd,x)*np.exp(-lbd))/(mh.factorial(x))<br/>    return val</pre>
<ol start="2">
<li>Write a function to calculate the log-likelihood over the data given a value for the arrival rate:</li>
</ol>
<pre style="padding-left: 60px">def loglikelihood(data,lbd):<br/>    lkhd=1<br/>    for i in range(len(data)):<br/>        lkhd=lkhd*poissonpdf(data[i],lbd)<br/>    if lkhd!=0:<br/>        val=np.log(lkhd)<br/>    else:<br/>        val=0<br/>    return val</pre>
<ol start="3">
<li>Write a function to calculate the derivative of the log-likelihood for the arrival rate λ:</li>
</ol>
<pre style="padding-left: 60px">def diffllhd(data,lbd):<br/>    diff = -len(data) + sum(data)/lbd<br/>    return diff</pre>
<ol start="4">
<li>Generate test data with 100 data points—a random number of arrivals/unit time between 3 and 12:</li>
</ol>
<pre style="padding-left: 60px">data=[randint(3, 12) for p in range(100)]</pre>
<ol start="5">
<li>Calculate the log-likelihood for different values of arrival rates (1 to 9) and plot them to find the arrival rate that maximizes:</li>
</ol>
<pre style="padding-left: 60px">y=[loglikelihood(data,i) for i in range(1,10)]<br/>y=[num for num in y if num ]<br/>x=[i for i in range(1,10) if loglikelihood(data,i)]<br/>plt.plot(x,y)<br/>plt.axvline(x=6,color='k')<br/>plt.title('Log-Likelihoods for different lambdas')<br/>plt.xlabel('Log Likelihood')<br/>plt.ylabel('Lambda')</pre>
<p style="padding-left: 60px">From this, we get the following plot, which shows that the maximum value of the log-likelihood on the test data is obtained when the arrival rate is 6/unit time:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f4734d99-d1b2-4af6-9f62-ce306d04020b.png" style="width:23.67em;height:15.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Log-likelihood values at different values of lambda (that is, arrival rate)</div>
<ol start="6">
<li>Use the Newton–Raphson method to find the global maximum of the log-likelihood:</li>
</ol>
<pre style="padding-left: 60px">def newtonRaphson(data,lbd):<br/>    h = loglikelihood(data,lbd) / diffllhd(data,lbd)<br/>    while abs(h) &gt;= 0.0001:<br/>        if diffllhd!=0:<br/>            h = loglikelihood(data,lbd) / diffllhd(data,lbd)<br/><br/>        # x(i+1) = x(i) - f(x) / f'(x)<br/>            lbd = lbd - h<br/>        else:<br/>            lbd=lbd<br/>    return lbd</pre>
<div class="packt_infobox"><strong>Note</strong>: The <kbd>lbd</kbd> parameter in the function definition is the initial value to start the search with.</div>
<p>The Newton–Raphson method is a popular computation method used to find the roots of complex equations. It is an iterative process that finds different values of independent variables until the dependent variable reaches 0. More information can be found at <a href="http://www.math.ubc.ca/~anstee/math104/newtonmethod.pdf">http://www.math.ubc.ca/~anstee/math104/newtonmethod.pdf</a>.</p>
<p>The results are greatly affected by the initial value of the parameter that is provided to start the search. The iterative search can go in very different directions if the start values are different, so be careful while using it.</p>
<p>The concept of MLE can be extended to perform distribution-based regression. Suppose that we hypothesize that the arrival rate is a function of one or several parameters. Then the lambda would be defined by a function of the parameters:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/06c870f1-c4fe-4900-afcf-02c8044f7b36.png" style="width:12.25em;height:1.08em;"/></p>
<p><span>In this case, </span>the arrival rate would be calculated as follows:</p>
<ol>
<li>Use the value of the arrival rate from the previous equation in the log-likelihood calculation.</li>
<li>Find the partial derivative of the log-likelihood with regard to <em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, and <em>w</em><sub>2</sub>.</li>
<li>Equate all the partial derivates to 0 and find the optimum values of <em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, and <em>w</em><sub>2</sub>.</li>
<li>Find the optimum value of the arrival rate based on these parameters.</li>
</ol>
<p>To use the MLE calculations, do the following:</p>
<ol>
<li>Find population parameters, such as the mean, std dev, arrival rate, and density, from the sample parameters.</li>
<li>Fit the distribution-based regression on data where simple linear regression wouldn't work, such as the parameter-based arrival rate example discussed previously, or logistic regression weights.</li>
</ol>
<p>Its usage in fitting the regression model brings it into the same league of optimization methods such as OLS, gradient descent, Adam optimization, RMSprop, and other methods. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>For a more in-depth look at other Bayesian statistics topics that we touched upon, please take a look at the following references:</p>
<ul>
<li><em>Probabilistic Programming and Bayesian Methods for Hackers:</em> <a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers"><span class="URLPACKT">https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers</span></a></li>
<li><em>Bayesian Data Analysis</em>, <em>Third Edition</em>, <em>Andrew Gelman</em>: <a href="http://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954"><span class="URLPACKT">http://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954</span></a></li>
<li><em>The Bayesian Choice</em>, <em>Christian P Robert</em> (this is more theoretical): <a href="http://www.springer.com/us/book/9780387952314"><span class="URLPACKT">http://www.springer.com/us/book/9780387952314</span></a></li>
<li><em>PyMC documentation</em>: <a href="http://pymc-devs.github.io/pymc/index.html"><span class="URLPACKT">http://pymc-devs.github.io/pymc/index.html</span></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we undertook a whirlwind tour of one of the hottest trends in statistics and data analysis in the past few years—the Bayesian approach to statistical inference. We covered a lot of ground here.</p>
<p>We examined what the Bayesian approach to statistics entails and discussed the various reasons why the Bayesian view is a compelling one, such as the fact that it values facts over belief. We explained the key statistical distributions and showed how we can use the various statistical packages to generate and plot them in <kbd>matplotlib</kbd>.</p>
<p>We tackled a rather difficult topic without too much oversimplification and demonstrated how we can use the <kbd>PyMC</kbd> package and Monte Carlo simulation methods to showcase the power of Bayesian statistics to formulate models, perform trend analysis, and make inferences on a real-world dataset (Facebook user posts). <span>The concept of maximum likelihood estimation was also introduced and explained with several examples. It is a popular method for estimating distribution parameters and fitting a probability distribution to a given dataset.</span></p>
<p>In the<span> </span>next chapter,<span> </span>we will discuss how we can solve real-life data case studies using pandas.</p>


            </article>

            
        </section>
    </body></html>