<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer014">
			<h1 id="_idParaDest-32"><a id="_idTextAnchor032"/>Chapter 2: Data Ingestion</h1>
			<p><strong class="bold">Data ingestion</strong> is the process of moving data from disparate operational systems to a central location such as a data warehouse or a data lake to be processed and made conducive for data analytics. It is the first step of the data analytics process and is necessary for creating centrally accessible, persistent storage, where data engineers, data scientists, and data analysts can access, process, and analyze data to generate business analytics.</p>
			<p>You will be introduced to the capabilities of Apache Spark as a data ingestion engine for both batch and real-time processing. Various data sources supported by Apache Spark and how to access them using Spark's DataFrame interface will be presented. </p>
			<p>Additionally, you will learn how to use Apache Spark's built-in functions to access data from external data sources, such as a <strong class="bold">Relational Database Management System</strong> (<strong class="bold">RDBMS</strong>), and message queues such as Apache Kafka, and ingest them into data lakes. The different data storage formats, such as structured, unstructured, and semi-structured file formats, along with the key differences between them, will also be explored. Spark's real-time streams processing engine called <strong class="bold">Structured Streaming</strong> will also be introduced. You will learn to create end-to-end data ingestion pipelines using batch processing as well as real-time stream processing. Finally, will explore a technique to unify batch and streams processing, called <strong class="bold">Lambda Architecture</strong>, and its implementation using Apache Spark.</p>
			<p>In this chapter, you will learn about the essential skills that are required to perform both batch and real-time ingestion using Apache Spark. Additionally, you will acquire the knowledge and tools required for building end-to-end scalable and performant big data ingestion pipelines.</p>
			<p>In this chapter, we will cover the following main topics:</p>
			<ul>
				<li>Introduction to Enterprise Decision Support Systems </li>
				<li>Ingesting data from data sources</li>
				<li>Ingesting data into data sinks</li>
				<li>Using file formats for data storage in data lakes</li>
				<li>Building data ingestion pipelines in batches and real time</li>
				<li>Unifying batch and real-time data ingestion using Lambda architecture</li>
			</ul>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor033"/>Technical requirements</h1>
			<p>In this chapter, we will be using the Databricks Community Edition to run our code. This can be found at <a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a>. </p>
			<p>Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. </p>
			<p>The code used in this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter02">https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter02</a>. </p>
			<p>The datasets used for this chapter can be found at<a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data"> </a><a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data"/>.</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor034"/>Introduction to Enterprise Decision Support Systems</h1>
			<p>An <strong class="bold">Enterprise Decision Support System</strong> (<strong class="bold">Enterprise DSS</strong>) is an end-to-end data processing system that takes operational and transactional data generated by a business organization and converts them into actionable insights. Every Enterprise DSS has a few standard components, <a id="_idIndexMarker061"/>such as data sources, data sinks, and data processing frameworks. An Enterprise DSS takes raw transactional data as its input and converts this into actionable insights such as operational reports, enterprise performance dashboards, and predictive analytics.</p>
			<p>The following diagram illustrates the components of a typical Enterprise DSS in a big data context:</p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="Images/B16736_02_01.jpg" alt="Figure 2.1 – The Enterprise DSS architecture&#13;&#10;" width="1633" height="727"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – The Enterprise DSS architecture</p>
			<p>A big data analytics system is also an Enterprise DSS operating at much larger <em class="italic">Volumes</em>, with more <em class="italic">Variety</em> of data, and <a id="_idIndexMarker062"/>arriving at much faster <em class="italic">Velocity</em>. Being a type of Enterprise DSS, a big data analytics system has components that are similar to that of a traditional Enterprise DSS. The first step of building an Enterprise DSS is data ingestion from data sources into data sinks. You will learn about this process throughout this chapter. Let's elaborate on the different components of a big data analytics system, starting with data sources.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor035"/>Ingesting data from data sources</h1>
			<p>In this section, we will <a id="_idIndexMarker063"/>learn about various data sources that a big data analytics system uses as a source <a id="_idIndexMarker064"/>of data. Typical data sources include transactional systems <a id="_idIndexMarker065"/>such as RDBMSes, file-based data sources such as <strong class="bold">data lakes</strong>, and <strong class="bold">message queues</strong> such as <strong class="bold">Apache Kafka</strong>. Additionally, you will learn about Apache Spark's built-in connectors to <a id="_idIndexMarker066"/>ingest data from these data sources and also write code so that you can view these connectors in action.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor036"/>Ingesting from relational data sources</h2>
			<p>A <strong class="bold">Transactional System</strong>, or an <strong class="bold">Operational System</strong>, is a data processing system that helps an organization carry out its day-to-day business functions. These transactional systems <a id="_idIndexMarker067"/>deal with individual business transactions, such <a id="_idIndexMarker068"/>as a point-of-service transaction at a retail kiosk, an order placed on an online retail portal, an airline ticket booked, or a banking transaction. A historical aggregate of these transactions forms the basis of data analytics, and <a id="_idIndexMarker069"/>analytics systems ingest, store, and process these transactions over long periods. Therefore, such Transactional <a id="_idIndexMarker070"/>Systems form the source of data of analytics systems and a starting point for data analytics.</p>
			<p>Transactional systems come in many forms; however, the most common ones are RDBMSes. In the following section, we will learn how to ingest data from an RDBMS.</p>
			<p>Relational data sources are a collection of relational databases and relational tables that consist of rows <a id="_idIndexMarker071"/>and named columns. The primary programming abstraction used to communicate with and query an RDBMS is called <strong class="bold">Structured Query Language</strong> (<strong class="bold">SQL</strong>). External systems can communicate with an RDBMS via communication <a id="_idIndexMarker072"/>protocols such as JDBC and ODBC. Apache Spark comes with a built-in JDBC data source that can be used to communicate with and query data stored in RDBMS tables. </p>
			<p>Let's take a look at the code required to ingest data from an RDBMS table using PySpark, as shown in the following code snippet:</p>
			<p class="source-code">dataframe_mysql = spark.read.format("jdbc").options(</p>
			<p class="source-code">    url="jdbc:mysql://localhost:3306/pysparkdb",</p>
			<p class="source-code">    driver = "org.mariadb.jdbc.Driver",</p>
			<p class="source-code">    dbtable = "authors",</p>
			<p class="source-code">    user="#####",</p>
			<p class="source-code">    password="@@@@@").load()</p>
			<p class="source-code">dataframe_mysql.show()</p>
			<p>In the previous code snippet, we use the <strong class="source-inline">spark.read()</strong> method to load data from a JDBC data source by specifying the format to be <strong class="source-inline">jdbc</strong>. Here, we connect to a popular open source RDBMS called <strong class="bold">MySQL</strong>. We pass a few options such as a <strong class="source-inline">url</strong> that specifies the <strong class="source-inline">jdbc url</strong> for the MySQL server <a id="_idIndexMarker073"/>along with its <strong class="source-inline">hostname</strong>, <strong class="source-inline">port number</strong>, and <strong class="source-inline">database name</strong>. The <strong class="source-inline">driver</strong> option specifies the JDBC driver to be used by Spark to connect and communicate with the RDBMS. The <strong class="source-inline">dtable</strong>, <strong class="source-inline">user</strong>, and <strong class="source-inline">password</strong> options specify the <a id="_idIndexMarker074"/>table name to be queried and the credentials that are needed to authenticate with the RDBMS. Finally, the <strong class="source-inline">show()</strong> function <a id="_idIndexMarker075"/>reads sample data from the RDBMS table and displays it onto the console. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The previous code snippet, which uses dummy database credentials, shows them in plain text. This poses a huge data security risk and is not a recommended practice. The appropriate best practices to handle sensitive information such as using config files or other mechanisms provided by big data software vendors such as obscuring or hiding sensitive information should be followed.</p>
			<p>To run this code, you can either use your own MySQL server and configure it with your Spark cluster, or you can use the sample code provided with this chapter to set up a simple MySQL server. The required code can be found at <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/mysql-setup.ipynb">https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/mysql-setup.ipynb</a>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Apache Spark provides a JDBC data source and is capable of connecting to virtually any RDBMS that supports JDBC connections and has a JDBC driver available. However, it doesn't come bundled with <a id="_idIndexMarker076"/>any drivers; they need to be procured from the respective RDBMS provider, and the driver needs to be configured to your Spark cluster to be available to your Spark application.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor037"/>Ingesting from file-based data sources</h2>
			<p>File-based data <a id="_idIndexMarker077"/>sources are very common when data is <a id="_idIndexMarker078"/>exchanged between different data processing systems. Let's consider <a id="_idIndexMarker079"/>an example of a retailer who wants to enrich their internal data sources with external data such as Zip Code data, as provided by a postal service provider. This data between the <a id="_idIndexMarker080"/>two organizations is usually exchanged via file-based data formats such as XML or JSON or more commonly using a <a id="_idIndexMarker081"/>delimited plain-text or CSV formats.</p>
			<p>Apache Spark supports various file formats, such as plain-text, CSV, JSON as well as binary file formats <a id="_idIndexMarker082"/>such as Apache Parquet and ORC. These files <a id="_idIndexMarker083"/>need to be located on a distributed filesystem <a id="_idIndexMarker084"/>such as <strong class="bold">Hadoop Distributed File System</strong> (<strong class="bold">HDFS</strong>), or a cloud-based <a id="_idIndexMarker085"/>data lake such as <strong class="bold">AWS S3</strong>, <strong class="bold">Azure Blob</strong>, or <strong class="bold">ADLS</strong> storage.</p>
			<p>Let's take a look at how to ingest data from CSV files using PySpark, as shown in the following block of code:</p>
			<p class="source-code">retail_df = (spark</p>
			<p class="source-code">         .read</p>
			<p class="source-code">         .format("csv")</p>
			<p class="source-code">         .option("inferSchema", "true")</p>
			<p class="source-code">         .option("header","true")</p>
			<p class="source-code">         .load("dbfs:/FileStore/shared_uploads/snudurupati@outlook.com/")</p>
			<p class="source-code">      )</p>
			<p class="source-code">retail_df.show()</p>
			<p>In the previous code snippet, we use the <strong class="source-inline">spark.read()</strong> function to read a CSV file. We specify the <strong class="source-inline">inferSchema</strong> and <strong class="source-inline">header</strong> options to be <strong class="source-inline">true</strong>. This helps Spark infer the column names and data type information by reading a sample set of data. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The file-based data source needs to be on a distributed filesystem. The Spark Framework leverages data parallel processing, and each Spark Executor tries to read a subset of the data into its own local memory. Therefore, it is essential that the file be located on a distributed filesystem and accessible by all the Executors and the Driver. HDFS and cloud-based data lakes, such as AWS S3, Azure Blob, and ADLS storage, are all distributed data storage layers that are good candidates to be used as file-based data sources with Apache Spark. </p>
			<p>Here, we <a id="_idIndexMarker086"/>read the CSV files from a <strong class="source-inline">dbfs/</strong> location, which is <a id="_idIndexMarker087"/>Databricks' proprietary filesystem called <strong class="bold">Databricks Filesystem</strong> (<strong class="bold">DBFS</strong>). DBFS is an abstraction layer that actually <a id="_idIndexMarker088"/>utilizes either AWS S3 or Azure Blob or ADLS storage underneath.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Given that each Executor tries to read only a subset of data, it is important that the file type being used is splittable. If the file cannot be split, an Executor might try to read a file larger than its available memory, run out of memory, and throw an "Out of Memory" error. One example of such an unsplittable file is a <strong class="source-inline">gzipped</strong> CSV or a text file.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor038"/>Ingesting from message queues</h2>
			<p>Another type of data source commonly used in real-time streaming analytics is a <strong class="bold">message queue</strong>. A message <a id="_idIndexMarker089"/>queue offers a publish-subscribe <a id="_idIndexMarker090"/>pattern of data consumption, where a <a id="_idIndexMarker091"/>publisher publishes data to a queue while multiple subscribers could consume the data asynchronously. In a <strong class="bold">Distributed Computing</strong> context, a message <a id="_idIndexMarker092"/>queue needs to be distributed, fault-tolerant, and scalable, in order to serve as a data source for a distributed data processing system.</p>
			<p>One such message queue is Apache Kafka, which is quite prominent in real-time streaming workloads with Apache Spark. Apache Kafka is more than just a message queue; it is an end-to-end distributed streams processing platform in itself. However, for our discussion, we will consider Kafka to be just a distributed, scalable, and fault-tolerant message queue. </p>
			<p>Let's take a look at what the code to ingest from Kafka using PySpark looks like, as shown in the following block of code:</p>
			<p class="source-code">kafka_df = (spark.read</p>
			<p class="source-code">        .format("kafka")</p>
			<p class="source-code">        .option("kafka.bootstrap.servers", "localhost:9092")</p>
			<p class="source-code">        .option("subscribe", "wordcount")</p>
			<p class="source-code">        .option("startingOffsets", "earliest")</p>
			<p class="source-code">        .load()</p>
			<p class="source-code">        )</p>
			<p class="source-code">kafka_df.show()</p>
			<p>In the previous code example, we use <strong class="source-inline">spark.read()</strong> to load data from a Kafka server by providing its <em class="italic">hostname</em> and <em class="italic">port number</em>, a <em class="italic">topic</em> named <strong class="source-inline">wordcount</strong><em class="italic">.</em> We also specify that Spark should start reading <em class="italic">events</em> from the very beginning of the queue, using the <strong class="source-inline">StartingOffsets</strong> option. Even though Kafka is more commonly used for streaming use cases with <a id="_idIndexMarker093"/>Apache Spark, this preceding code <a id="_idIndexMarker094"/>example makes use of Kafka as a data source for batch processing of data. You will learn to use Kafka with Apache Spark for processing streams in the <em class="italic">Data ingestion in real time using Structured Streaming</em> section.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">In Kafka terminology, an individual <a id="_idIndexMarker095"/>queue is called a <em class="italic">topic</em>, and each event is called an <em class="italic">offset</em>. Kafka is a queue, so it serves <a id="_idIndexMarker096"/>events in the same order in which they were published onto a topic, and individual consumers can choose their own starting and ending offsets.</p>
			<p>Now that you are familiar with ingesting data from a few different types of <strong class="bold">data sources</strong> using Apache <a id="_idIndexMarker097"/>Spark, in the following section, let's learn how to ingest data into <strong class="bold">d</strong><strong class="bold">ata sinks</strong>.</p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor039"/>Ingesting data into data sinks</h1>
			<p>A data sink, as its name suggests, is a storage layer for storing raw or processed data either for short-term staging or long-term persistent storage. Though the term of <em class="italic">data sink</em> is commonly <a id="_idIndexMarker098"/>used in real-time data processing, there is no specific harm in calling <a id="_idIndexMarker099"/>any storage layer where ingested data lands a data sink. Just like data sources, there are also different types of data sinks. You will learn about a few of the most common ones in the following sections.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>Ingesting into data warehouses</h2>
			<p><strong class="bold">Data warehouses</strong> are a specific type of persistent data storage most prominent in Business Intelligence <a id="_idIndexMarker100"/>type workloads. There is an entire field of study <a id="_idIndexMarker101"/>dedicated to Business Intelligence and data warehousing. Typically, a data warehouse uses an RDBMS as its data store. However, a data warehouse is different from a traditional database in that it follows a specific type of data modeling technique, called <strong class="bold">dimensional modeling</strong>. Dimensional models are very intuitive for representing real-world business attributes <a id="_idIndexMarker102"/>and are conducive for Business Intelligence types of queries used in building business reports and dashboards. A data warehouse could be built on any commodity RDBMS or using specialist hardware and software. </p>
			<p>Let's use PySpark to save a DataFrame to an RDBMS table, as shown in the following code block:</p>
			<p class="source-code">wordcount_df = spark.createDataFrame(</p>
			<p class="source-code">    [("data", 10), ("parallel", 2), ("Processing", 30),       	     ("Spark", 50), ("Apache", 10)], ("word", "count"))</p>
			<p>In the previous block of code, we programmatically create a DataFrame with two columns from a Python <strong class="source-inline">List</strong> object. Then, we save the Spark DataFrame to a MySQL table using the <strong class="source-inline">spark.write()</strong> function, as shown in the following code snippet:</p>
			<p class="source-code">wordcount_df.write.format("jdbc").options(</p>
			<p class="source-code">    url="jdbc:mysql://localhost:3306/pysparkdb",</p>
			<p class="source-code">    driver = "org.mariadb.jdbc.Driver",</p>
			<p class="source-code">    dbtable = "word_counts",</p>
			<p class="source-code">    user="######",</p>
			<p class="source-code">    password="@@@@@@").save()</p>
			<p>The preceding snippet <a id="_idIndexMarker103"/>of code to write data to an RDBMS is almost the same as the one to read data from an RDBMS. We still need to use the MySQL JDBC driver and specify the <em class="italic">hostname</em>, <em class="italic">port number</em>, <em class="italic">database name</em>, and <em class="italic">database credentials</em>. The only difference is that here, we need to use the <strong class="source-inline">spark.write()</strong> function instead of <strong class="source-inline">spark.read()</strong>.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>Ingesting into data lakes</h2>
			<p>Data warehouses are excellent for intuitively representing real-world business data and storing highly structured relational data in a way that is conducive for Business Intelligence types <a id="_idIndexMarker104"/>of workloads. However, data warehouses fall short when handling unstructured data that is required by data science and machine learning types of workloads. Data warehouses are not good at handling the high <em class="italic">Volume</em> and <em class="italic">Velocity</em> of big data. That's where data lakes step in to fill the gap left by data warehouses. </p>
			<p>By design, data lakes are highly scalable and flexible when it comes to storing various types of data, including highly structured relational data and unstructured data such as images, text, social media, videos, and audio. Data lakes are also adept at handling data in batches as well as in streams. With the emergence of the cloud, data lakes have become very common these days, and they seem to be the future of persistent storage for all big data analytics workloads. A few examples of data lakes include Hadoop HDFS, AWS S3, Azure Blob or ADLS storage, and Google Cloud Storage. </p>
			<p>Cloud-based data lakes <a id="_idIndexMarker105"/>have a few advantages over their on-premises counterparts:</p>
			<ul>
				<li>They are on-demand and infinitely scalable.</li>
				<li>They are pay-per-use, thus saving you on upfront investments.</li>
				<li>They are completely independent of computing resources; so, storage can scale independently of computing resources.</li>
				<li>They support both structured and unstructured data, along with simultaneous batch and streaming, allowing <a id="_idIndexMarker106"/>the same storage layer to be used by multiple workloads.</li>
			</ul>
			<p>Because of the preceding <a id="_idIndexMarker107"/>advantages, cloud-based data lakes have become prominent over the past few years. Apache Spark treats these data lakes as yet another file-based data storage. Therefore, working with data lakes using Spark is as simple as working with any other file-based data storage layer. </p>
			<p>Let's take a look at how easy it is to save data to a data lake using PySpark, as shown in the following code example:</p>
			<p class="source-code">(wordcount_df</p>
			<p class="source-code">        .write</p>
			<p class="source-code">        .option("header", "true")</p>
			<p class="source-code">        .mode("overwrite")</p>
			<p class="source-code">        .save("/tmp/data-lake/wordcount.csv")</p>
			<p class="source-code">)</p>
			<p>In the preceding code block, we take the <strong class="source-inline">wordcount_df</strong> DataFrame that we created in the previous section and save it to the data lake in CSV format using the DataFrame's <strong class="source-inline">write()</strong> function. The <strong class="source-inline">mode</strong> option instructs <strong class="source-inline">DataFrameWriter</strong> to overwrite any existing data in the specified file location; note that you could also use <strong class="source-inline">append</strong> mode.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>Ingesting into NoSQL and in-memory data stores</h2>
			<p>Data warehouses <a id="_idIndexMarker108"/>have always been the traditional persistent storage layer<a id="_idIndexMarker109"/> of choice for data analytics use cases, and <a id="_idIndexMarker110"/>data lakes are emerging as<a id="_idIndexMarker111"/> the new choice to cater to a wider range of workloads. However, there are other big data analytics use cases involving ultra-low latency query response times that require special types of storage layers. Two such <a id="_idIndexMarker112"/>types of storage layers are NoSQL databases and in-memory databases, which <a id="_idIndexMarker113"/>we will explore in this section.</p>
			<h3>NoSQL databases for operational analytics at scale</h3>
			<p>NoSQL databases are an alternative to traditional relational databases, where there is a requirement for handling messy and unstructured data. NoSQL databases are very good at storing large <a id="_idIndexMarker114"/>amounts of unstructured data in the form of <strong class="bold">Key-Value</strong> pairs and very efficient at retrieving the <strong class="bold">Value</strong> for any given <strong class="bold">Key</strong> in constant time, at high concurrency.</p>
			<p>Let's consider a use case where a business wants to provide precalculated, hyper-personalized content to their individual customers using millisecond query response times in a highly concurrent manner. A NoSQL database such as Apache Cassandra or MongoDB would be an ideal candidate for the use case. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Apache Spark doesn't come out of the box with any connectors for NoSQL databases. However, they are built <a id="_idIndexMarker115"/>and maintained by the respective database provider and can be downloaded for the respective provider and then configured with Apache Spark.</p>
			<h3>In-memory database for ultra-low latency analytics</h3>
			<p>In-memory databases store data purely in memory only, and persistent storage such as disks are not involved. This <a id="_idIndexMarker116"/>property of in-memory databases makes them faster in terms of data access speeds compared to their disk-based counterparts. A few <a id="_idIndexMarker117"/>examples of in-memory databases include <strong class="bold">Redis</strong> and <strong class="bold">Memcached</strong>. Since system memory is limited and data stored in memory is not durable over power <a id="_idIndexMarker118"/>cycles, in-memory databases are not suitable for the persistent storage of large amounts of historical data, which is typical for big data analytics systems. They do have their use in real-time analytics involving ultra-low latency response times. </p>
			<p>Let's consider the example of an online retailer wanting to show the estimated shipment delivery time of a product to a customer at the time of checkout on their online portal. Most of the parameters that are needed to estimate delivery lead time can be precalculated. However, certain parameters, such as customer Zip Code and location, are only available when the customer provides them during checkout. Here, data needs to be instantly collected from the web portal, processed using an ultra-fast event processing system, and the results need to be calculated and stored in an ultra-low latency storage layer to be accessed and served back to the customer via the web app. All this processing should happen in a matter of seconds, and an in-memory database such as Redis or Memcached would serve the purpose of an ultra-low latency data storage layer.</p>
			<p>So far, you have learned about accessing data from a few different data sources and ingesting them into <a id="_idIndexMarker119"/>various data sinks. Additionally, you have learned that you do not have much control over the data source. However, you do have complete control of your data sinks. Choosing the right data storage layer for certain high concurrency, ultra-low latency use cases is important. However, for most big data analytics use cases, data lakes are becoming the de facto standard as the preferred persistent data storage layer. </p>
			<p>Another key factor for optimal data storage is the actual format of the data. In the following section, we will explore a few data storage formats and their relative merits.</p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor043"/>Using file formats for data storage in data lakes</h1>
			<p>The file format you choose to store data in a data lake is key in determining the ease of data storage and <a id="_idIndexMarker120"/>retrieval, query performance, and<a id="_idIndexMarker121"/> storage space. So, it is vital that you choose the optimal data format that can balance these factors. Data storage formats can be broadly classified into structured, unstructured, and semi-structured formats. In this section, we will explore each of these types with the help of code examples.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/>Unstructured data storage formats</h2>
			<p>Unstructured data is any data that is not represented by a predefined data model and can be either human <a id="_idIndexMarker122"/>or machine-generated. For instance, unstructured data could be data stored in plain text documents, PDF documents, sensor data, log files, video files, images, audio files, social media feeds, and more.</p>
			<p>Unstructured data might contain important patterns, and extracting these patterns could lead to valuable insights. However, storing data in unstructured format is not very useful due to the following reasons:</p>
			<ul>
				<li>Unstructured data <a id="_idIndexMarker123"/>might not always have an inherent compression mechanism and can take up large amounts of storage space.</li>
				<li>Externally compressing unstructured files saves space but expends the processing power for the compression and decompression of files.</li>
				<li>Storing and accessing unstructured files is somewhat difficult because they inherently lack any schema information.</li>
			</ul>
			<p>Given the preceding reasons, it makes sense to ingest unstructured data and convert it into a structured format before storing it inside the data lake. This makes the downstream processing of data easier and more efficient. Let's take a look at an example where we take a set of unstructured image files and convert them into a DataFrame of image attributes. Then, we store them using the CSV file format, as shown in the following code snippet:</p>
			<p class="source-code">Raw_df = spark.read.format("image").load("/FileStore/FileStore/shared_uploads/images/")</p>
			<p class="source-code">raw_df.printSchema()</p>
			<p class="source-code">image_df = raw_df.select("image.origin", "image.height", "image.width", "image.nChannels", "image.mode", "image.data")</p>
			<p class="source-code">image_df.write.option("header", "true").mode("overwrite").csv("/tmp/data-lake/images.csv") </p>
			<p> In the previous code block, the following occurs: </p>
			<ul>
				<li>We load a set of images files using Spark's built-in <strong class="source-inline">image</strong> format; the result is a Spark DataFrame of image attributes.</li>
				<li>We use the <strong class="source-inline">printSchema()</strong> function to take a look at the DataFrame's schema and discover that the DataFrame has a single nested column named <strong class="source-inline">image</strong> with <strong class="source-inline">origin</strong>, <strong class="source-inline">height</strong>, <strong class="source-inline">width</strong>, <strong class="source-inline">nChannels</strong>, and more, as its inner attributes.</li>
				<li>Then, we bring up the inner attributes to the top level using the <strong class="source-inline">image</strong> prefix with each inner attribute, such as <strong class="source-inline">image.origin</strong>, and create a new DataFrame named <strong class="source-inline">image_df</strong> with all of the image's individual attributes as top-level columns.</li>
				<li>Now that we have <a id="_idIndexMarker124"/>our final DataFrame, we write it out to the data lake using the CSV format.</li>
				<li>Upon browsing the data lake, you can see that the process writes a few CSV files to the data lake with file sizes of, approximately, 127 bytes.<p class="callout-heading">Tip</p><p class="callout">The number of files written out to storage depends on the number of partitions of the DataFrame. The number of DataFrame partitions depends on the number of executors cores and the <strong class="source-inline">spark.sql.shuffle.partitions</strong> Spark configuration. This number also changes every time the DataFrame <a id="_idIndexMarker125"/>undergoes a shuffle operation. In Spark 3.0, <strong class="bold">Adaptive Query Execution</strong> automatically manages the optimal number of shuffle partitions.</p></li>
			</ul>
			<p>Along with file sizes, query performance is also an important factor when considering the file format. So, let's run a quick test where we perform a moderately complex operation on the DataFrame, as shown in the following block of code:</p>
			<p class="source-code">from pyspark.sql.functions import max, lit</p>
			<p class="source-code">temp_df = final_df.withColumn("max_width", lit(final_df.agg(max("width")).first()[0]))</p>
			<p class="source-code">temp_df.where("width == max_width").show()</p>
			<p>The previous block of code, first, creates a new column with every row value as the maximum width among all rows. Then, it filters out the row that has this maximum value for the <strong class="source-inline">width</strong> column. The query is moderately complex and typical of the kind of queries used in data analytics. In our sample test case, the query running on an unstructured binary file took, approximately, <em class="italic">5.03 seconds</em>. In the following sections, we will look at the same query on other file formats and compare query performances.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>Semi-structured data storage formats</h2>
			<p>In the preceding example, we were able to take a binary image file, extract its attributes, and store them in CSV format, which makes the data structured but still keeps it in a human-readable format. CSV format is another type of data storage format called semi-structured data format. Semi-structured data formats, like unstructured data formats, do not have a predefined data model. However, they organize data in a way that makes it easier to infer <a id="_idIndexMarker126"/>schema information from the files themselves, without any external metadata being supplied. They are a popular data format for the exchange of data between distinct data processing systems. Examples of semi-structured data formats include CSV, XML, and JSON.</p>
			<p>Let's take a look an example of how we can use PySpark to handle semi-structured data, as shown in the following code block:</p>
			<p class="source-code">csv_df = spark.read.options(header="true", inferSchema="true").csv("/tmp/data-lake/images.csv")</p>
			<p class="source-code">csv_df.printSchema()</p>
			<p class="source-code">csv_df.show() </p>
			<p>The previous code example takes the CSV files generated during the previous image processing example and loads them as a Spark DataFrame. We have enabled options to infer column names and data types from the actual data. The <strong class="source-inline">printSchema()</strong> function shows that Spark was able to infer column data types correctly for all columns except for the binary data column from the semi-structured files. The <strong class="source-inline">show()</strong> function shows that a DataFrame was correctly reconstructed from the CSV files along with column names.</p>
			<p>Let's run a moderately complex query on the <strong class="source-inline">csv_df</strong> DataFrame, as shown in the following block of code:</p>
			<p class="source-code">from pyspark.sql.functions import max, lit</p>
			<p class="source-code">temp_df = csv_df.withColumn("max_width", lit(csv_df.agg(max("width")).first()[0]))</p>
			<p class="source-code">temp_df.where("width == max_width").show()</p>
			<p>In the preceding block of code, we perform a few DataFrame operations to get the row with the maximum value for the <strong class="source-inline">width</strong> column. The code took <em class="italic">1.24 seconds</em> to execute using CSV data format, compared to the similar code that we executed in the <em class="italic">Unstructured Data Storage Formats</em> section, which took, approximately, <em class="italic">5 seconds</em>. Thus, seemingly, semi-structured file formats are better than unstructured files for data storage, as <a id="_idIndexMarker127"/>it is relatively easier to infer schema information from this data storage format. </p>
			<p>However, pay attention to the results of the <strong class="source-inline">show()</strong> function in the preceding code snippet. The data column containing binary data is inferred incorrectly as string type, and the column data is truncated. Therefore, it should be noted that semi-structured formats are not suitable for representing all data types, and they could also lose information with certain data types during the conversion from one data format into another. </p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/>Structured data storage formats</h2>
			<p>Structured data follows a predefined data model and has a tabular format with well-defined rows and named columns along with defined data types. A few examples of structured <a id="_idIndexMarker128"/>data formats are relational database tables and data generated by transactional systems. Note that there are also file formats that are fully structured data along with their data models, such as Apache Parquet, Apache Avro, and ORC files, that can be easily stored on data lakes. </p>
			<p><strong class="bold">Apache Parquet</strong> is a binary, compressed, and columnar storage format that was designed to be efficient <a id="_idIndexMarker129"/>at data storage as well as query performance. Parquet is a first-class citizen of the Apache Spark framework, and Spark's in-memory <a id="_idIndexMarker130"/>storage format, called <strong class="bold">Tungsten</strong>, was designed to take full advantage of the Parquet format. Therefore, you will get the best performance and efficiency out of Spark when your data is stored in Parquet format.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A Parquet <a id="_idIndexMarker131"/>file is a binary file format, meaning that the contents of the file have a binary encoding. Therefore, they are not human-readable, unlike text-based file formats such as JSON or CSV. However, one advantage of this is that they are easily interpreted by machines without losing any time during the encoding and decoding processes.</p>
			<p>Let's convert the <strong class="source-inline">image_df</strong> DataFrame, containing image attribute data from the <em class="italic">Unstructured data storage formats</em> section, into Parquet format, as shown in the following code block:</p>
			<p class="source-code">final_df.write.parquet("/tmp/data-lake/images.parquet")</p>
			<p class="source-code">parquet_df = spark.read.parquet("/tmp/data-lake/images.parquet")</p>
			<p class="source-code">parquet_df.printSchema()</p>
			<p class="source-code">parquet_df.show()</p>
			<p>The previous block <a id="_idIndexMarker132"/>of code loads binary image files into a Spark DataFrame and writes the data back into the data lake in Parquet format. The result of the <strong class="source-inline">show()</strong> function reveals that the binary data in the <em class="italic">data</em> column was not truncated and has been preserved from the source image files as-is. </p>
			<p>Let's perform a moderately complex operation, as shown in the following block of code:</p>
			<p class="source-code">temp_df = parquet_df.withColumn("max_width", lit(parquet_df.agg(max("width")).first()[0]))</p>
			<p class="source-code">temp_df.where("width == max_width").show()</p>
			<p>The preceding code block extracts the row with the maximum value for the column named <strong class="source-inline">width</strong>. The query takes, approximately, <em class="italic">4.86 seconds</em> to execute, as compared to over <em class="italic">5 seconds</em> with the original unstructured image data. Therefore, this makes the structured Parquet file format the optimal format to be used to store data in data lakes with Apache Spark. Seemingly, the semi-structured CSV files took less time to execute the query, but they also truncated the data, making it not the right fit for every use case. As a general rule of thumb, the Parquet data format is recommended for almost all Apache Spark use cases, that is, unless a specific use case calls for another type of data storage format.</p>
			<p>So far, you have seen that choosing the right data format can affect the correctness, ease of use, storage efficiency, query performance, and scalability of the data. Additionally, there is another factor that needs to be considered when storing data into data lakes no matter which data format you use. This technique is called <strong class="bold">data partitioning</strong> and can really <a id="_idIndexMarker133"/>make or break your downstream query performance. </p>
			<p>Put simply, data partitioning is the process of physically dividing your data across multiple folders or <a id="_idIndexMarker134"/>partitions. Apache Spark uses this partition information to only read the relevant data files required by <a id="_idIndexMarker135"/>the query into memory. This mechanism is called <strong class="bold">partition pruning</strong>. You will learn more about data partitioning in <a href="B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Cleansing and Integration</em>.</p>
			<p>So far, you have learned about the individual components of an Enterprise DSS, namely, data sources, data sinks, and data storage formats. Additionally, you gained a certain level of familiarity with the Apache Spark Framework as a big data processing engine in the previous chapter. Now, let's put this knowledge to use and build an end-to-end data ingestion pipeline in the following section.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor047"/>Building data ingestion pipelines in batch and real time</h1>
			<p>An end-to-end data ingestion pipeline involves reading data from data sources and ingesting it into a data sink. In the context of big data and data lakes, data ingestion involves a large <a id="_idIndexMarker136"/>number of data sources and, thus, requires a data processing engine that is highly scalable. There are specialist <a id="_idIndexMarker137"/>tools available in the market that <a id="_idIndexMarker138"/>are purpose-built for handling data ingestion at scale, such as StreamSets, Qlik, Fivetran, Infoworks, and more, from <a id="_idIndexMarker139"/>third-party vendors. In addition to this, cloud providers have their own native offerings such as AWS Data Migration Service, Microsoft Azure Data Factory, and Google Dataflow. There are also free and open source data ingestion tools available that you could consider such as Apache Sqoop, Apache Flume, Apache Nifi, to name a few.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Apache Spark is good enough for ad hoc data ingestion, but it is not a common industry practice to use Apache Spark as a dedicated data ingestion engine. Instead, you should consider a dedicated, purpose-built data ingestion tool for your dedicated data ingestion needs. You <a id="_idIndexMarker140"/>can either choose from third-party vendors or choose to manage one of the open source offerings by yourself.</p>
			<p>In this section, we will explore Apache Spark's capabilities for data ingestion in both a batch and streams processing manner.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/>Data ingestion using batch processing</h2>
			<p>Batch processing refers to processing <a id="_idIndexMarker141"/>one group or batch of data at a time. Batch processes are scheduled to run at specified intervals without any user intervention. Customarily, batch <a id="_idIndexMarker142"/>processes are run at <a id="_idIndexMarker143"/>night, after business hours. The simple reason for this is that batch processes tend to read a large number of transactions from the operational systems, which adds a lot of load to the operational systems. This is undesirable because operational systems are critical for the day-to-day operations of a business, and we do not want to unnecessarily burden the transactional system with workloads that are non-critical to daily business operations. </p>
			<p>Additionally, batch processing jobs tend to be repetitive as they run at regular intervals, bringing in the new set of data generated after the last successful batch process has been run. Batch processing can be of two types, namely, <strong class="bold">full data load</strong> and <strong class="bold">incremental data load</strong>.</p>
			<h3>Full data loads</h3>
			<p>A full data load involves completely overwriting an existing dataset. This is useful for datasets that are <a id="_idIndexMarker144"/>relatively small in size and that do not change often. It is also an easier process to implement, as we just have to scan the entire <a id="_idIndexMarker145"/>source dataset and completely overwrite the destination dataset. There is no need to maintain any state information regarding the previous data ingestion job. Let's take an example of a dimensional table from a data warehouse, such as a calendar table or a table containing the data of all the physical stores of a retailer. These tables do not change often and are relatively small, making them ideal candidates for full data loads. Though easy to implement, full data loads have their drawbacks when it comes to dealing with very large source datasets that change regularly. </p>
			<p>Let's consider the transactional data of a large retailer with more than a thousand stores all over the country, generating about 500 transactions per month per store. This translates to, approximately, 15,000 transactions ingested into the data lake per day. This number quickly adds up when we also consider historical data. Let's say that we just started building our data lake, and so far, we have only about 6 months of transactional data ingested. Even at this scale, we already have 3 million transactional records in our dataset, and <a id="_idIndexMarker146"/>completely truncating and loading the <a id="_idIndexMarker147"/><a id="_idIndexMarker148"/>dataset is not a trivial task. </p>
			<p>Another important factor to consider here is that typically, operational systems only retain historical data for small time intervals. Here, a full load means losing history from the data lake as well. At this point, you should consider an incremental load for data ingestion.</p>
			<h3>Incremental data loads</h3>
			<p>During an incremental data load, we only ingest the new set of data that was created in the data source after the previous case of successful data ingestion. This incremental dataset is generally <a id="_idIndexMarker149"/>referred to as the delta. An incremental load ingests datasets that are smaller in size compared to a full load, and since <a id="_idIndexMarker150"/>we already maintain the full historical data in our delta lake, incremental doesn't need to depend on the data source maintaining a full history.</p>
			<p>Building on the same retailer example from earlier, let's assume that we run our incremental batch load once per night. In this scenario, we only need to ingest 15,000 transactions into the data lake per day, which is pretty easy to manage. </p>
			<p>Designing an incremental data ingestion pipeline is not as simple compared to a full load pipeline. State information about the previous run of the incremental job needs to be maintained so that we can identify all of the new records from the data source that have not already been ingested <a id="_idIndexMarker151"/>into the data lake. This state information is stored in a special data structure, called a <em class="italic">watermark</em> table. This watermark table needs to be updated and maintained by the data ingestion job. A typical data ingestion pipeline is illustrated as follows:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="Images/B16736_02_02.jpg" alt="Figure 2.2 – Data ingestion&#13;&#10;" width="885" height="511"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Data ingestion</p>
			<p>The preceding diagram shows a typical data ingestion pipeline using Apache Spark, along with the watermark <a id="_idIndexMarker152"/>table for incremental loads. Here, we ingest raw transactional data from source systems using Spark's built-in <a id="_idIndexMarker153"/>data sources, process them using DataFrame operations and then send the data back to a data lake.</p>
			<p>Expanding on the retail example from the previous section, let's build an end-to-end data ingestion pipeline with batch processing using PySpark. One of the prerequisites for building a data pipeline is, of course, data, and for this example, we will make use of the <em class="italic">Online Retail</em> dataset made available by <em class="italic">UC Irvine Machine Learning Repository</em>. The dataset is available in CSV format in the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section of this chapter. The <em class="italic">Online Retail</em> dataset contains transactional data for an online retailer.</p>
			<p>We will download the dataset, consisting of two CSV files, and upload them to the <em class="italic">Databricks Community Edition</em> notebook environment via the upload interface that is present within the notebook's file menu. Once the dataset has been uploaded, we will make a note of the file location. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you are using your own Spark environment, make sure that you have the datasets available at a location that is accessible for your Spark cluster.</p>
			<p>Now we can get started with the actual code for the data ingestion pipeline, as shown in the following code example:</p>
			<p class="source-code">retail_df = (spark</p>
			<p class="source-code">                 .read</p>
			<p class="source-code">                 .option("header", "true")</p>
			<p class="source-code">                 .option("inferSchema", "true")</p>
			<p class="source-code">                 .csv("/FileStore/shared_uploads/online_retail/online_retail.csv")</p>
			<p class="source-code">            )</p>
			<p>In the preceding code block, we loaded the CSV files with the <strong class="source-inline">header</strong> and <strong class="source-inline">inferSchema</strong> options <a id="_idIndexMarker154"/>enabled. This creates a Spark DataFrame with eight columns along with their respective data types and <a id="_idIndexMarker155"/>column names. Now, let's ingest this data into the data lake in Parquet format, as shown in the following code block:</p>
			<p class="source-code">(retail_df</p>
			<p class="source-code">       .write</p>
			<p class="source-code">       .mode("overwrite")</p>
			<p class="source-code">       .parquet("/tmp/data-lake/online_retail.parquet")</p>
			<p class="source-code">)</p>
			<p>Here, we save the <strong class="source-inline">retail_df</strong> Spark DataFrame, containing raw retail transactions, to the data lake in Parquet format using the DataFrameWriter's <strong class="source-inline">write()</strong> function. We also specify the <strong class="source-inline">mode</strong> option to <strong class="source-inline">overwrite</strong> and, essentially, implement a full data load. </p>
			<p>One thing to note here is that the entire data ingestion job is a mere <strong class="bold">10</strong> lines of code, and this can easily be scaled up to tens of millions of records, processing up to many petabytes of data. This is the power and simplicity of Apache Spark, which has made it the de facto standard for big data processing in a very short period of time. Now, how would you actually scale the preceding data ingestion batch job and then, eventually, productionize it? </p>
			<p>Apache Spark was built from the ground up to be scalable, and its scalability is entirely dependent on the <a id="_idIndexMarker156"/>number of cores available to a job on the cluster. So, to scale your Spark job, all you need to do is to allocate more <a id="_idIndexMarker157"/>processing cores to the job. Most commercially available Spark-as-a-managed-service offerings provide a nifty <strong class="bold">Autoscaling</strong> functionality. With this <a id="_idIndexMarker158"/>autoscaling functionality, you just need to specify the minimum and the maximum <a id="_idIndexMarker159"/>number of nodes for your cluster, and <strong class="bold">Cluster Manager</strong> dynamically figures out the optimal number of cores for your job. </p>
			<p>Most commercial Spark offerings also come with a built-in <strong class="bold">Job Scheduler</strong> and support directly scheduling notebooks <a id="_idIndexMarker160"/>as jobs. External schedulers, ranging from the rudimentary <strong class="bold">crontab</strong> to sophisticated job orchestrators such as <strong class="bold">Apache Airflow</strong>, can also be <a id="_idIndexMarker161"/>used to productionize your Spark jobs. This really makes the process <a id="_idIndexMarker162"/>of cluster capacity planning easier for you, freeing up your time to focus on actual data analytics rather than spending your time and energy on capacity planning, tuning, and maintaining Spark clusters.</p>
			<p>So far, in this section, you have viewed an example of a full load batch ingestion job that loads the entire data from the data source and then overwrites the dataset in the data lake. You would need to add a little more business logic to maintain the ingestion job's state in a watermark data structure and then calculate the delta to perform an incremental load. You could build all this logic by yourself, or, alternatively, you could simply use Spark's structured streaming engine to do the heavy lifting for you, as discussed in the following section.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>Data ingestion in real time using structured streaming</h2>
			<p>Often, businesses need to make tactical decisions in real time along with strategic decision-making <a id="_idIndexMarker163"/>in order to stay competitive. Therefore, the need to ingest data into a data lake arises in real time. However, keeping up with the fast data <em class="italic">Velocity </em>of big data requires a robust and scalable streams <a id="_idIndexMarker164"/>processing engine. Apache Spark has one such streams processing engine, called <strong class="bold">Structured Streaming</strong>, which we will explore next.</p>
			<h3>Structured Streaming primer</h3>
			<p><strong class="bold">Structured Streaming</strong> is a Spark streams processing engine based on the Spark SQL engine. Just like all other components <a id="_idIndexMarker165"/>of Spark, Structured Streaming is also scalable and fault-tolerant. Since Structured Streaming is based on the Spark SQL engine, you can use the same Spark DataFrame API that you have already been using for batch processing for streams processing, too. Structured Streaming supports all of the functions and constructs supported by the DataFrame API.</p>
			<p>Structured Streaming treats each incoming stream of data just like tiny a batch of data, called a <em class="italic">micro-batch,</em> and keeps appending each micro-batch to the target dataset. Structured Streaming's programming <a id="_idIndexMarker166"/>model continuously processes micro-batches, treating each micro-batch just like a batch job. So, an existing Spark batch job can be easily converted into a streaming job with a few minor changes. Structured Streaming is designed to provide maximum throughput, which means that a Structured Streaming job can scale out to multiple nodes on a cluster and process very large amounts of incoming data in a distributed fashion.</p>
			<p>Structured Streaming comes with additional fault tolerance to failures and guarantees exactly once semantics. To achieve this, Structured Streaming keeps track of the data processing progress. It keeps track <a id="_idIndexMarker167"/>of the offsets or events processed at any point in time using concepts such as <strong class="bold">checkpointing</strong> and <strong class="bold">write-ahead logs</strong>. Write-ahead logging is a concept from relational databases and is used to provide atomicity <a id="_idIndexMarker168"/>and durability to databases. In this technique, records are first written to the log before any changes are written to the final database. Checkpointing is another technique in Structured Streaming, where the position of the current offset being read is recorded on a persistent storage system.</p>
			<p>Together, these techniques enable Structured Streaming to keep a record of the position of the last offset processed within the stream, giving it the ability to resume processing the stream exactly where it left off, just in case the streaming job fails. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">We recommended that checkpoints are stored in persistent storage with high availability and partition tolerance support, such as a cloud-based data lake.</p>
			<p>These techniques of checkpointing, write-ahead logs, and repayable streaming data sources, along with streaming data sinks that support the reprocessing of data, enable Structured Streaming to guarantee <a id="_idIndexMarker169"/>that every event of the stream is processed exactly once. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Structured Streaming's micro-batch processing model is not suitable for processing an event as soon as it occurs at the source. There are other streams processing engines such as Apache Flink or Kafka Streams that are more suitable for ultra-low latency streams processing.</p>
			<h3>Loading data incrementally</h3>
			<p>Since Structured Streaming has built-in mechanisms to help you to easily maintain the state information that is <a id="_idIndexMarker170"/>required for an incremental load, you can simply choose Structured Streaming for all your incremental loads and really simplify your architectural complexity. Let's build a pipeline to perform incremental loads in a real-time streaming fashion.</p>
			<p>Typically, our data ingestion starts with data already loaded onto a data source such as a data lake or a message queue such as Kafka. Here, we, first, need to load some data into a Kafka topic. You can start with an existing Kafka cluster with some data already in a topic, or you can set up a quick Kafka server and load the <em class="italic">Online Retail</em> dataset using the code provided at <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/kafka-setup.ipynb">https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/blob/main/Chapter02/utils/kafka-setup.ipynb</a>. </p>
			<p>Let's take a look at how to perform real-time data ingestion from Kafka into a data lake using Structured Streaming, as shown in the following snippets of code:</p>
			<p class="source-code">from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType</p>
			<p class="source-code">eventSchema = ( StructType()</p>
			<p class="source-code">  .add('InvoiceNo', StringType()) </p>
			<p class="source-code">  .add('StockCode', StringType())</p>
			<p class="source-code">  .add('Description', StringType()) </p>
			<p class="source-code">  .add('Quantity', IntegerType()) </p>
			<p class="source-code">  .add('InvoiceDate', StringType()) </p>
			<p class="source-code">  .add('UnitPrice', DoubleType()) </p>
			<p class="source-code">  .add('CustomerID', IntegerType()) </p>
			<p class="source-code">  .add('Country', StringType())     </p>
			<p class="source-code">)</p>
			<p>In the preceding block of code, we declare all the columns that we intend to read from a Kafka event along with their data types. Structured Streaming requires that the data schema be <a id="_idIndexMarker171"/>declared upfront. Once the schema has been defined, we can start reading data from a Kafka topic and load it into a Spark DataFrame, as shown in the following block of code:</p>
			<p class="source-code">kafka_df = (spark</p>
			<p class="source-code">                 .readStream</p>
			<p class="source-code">                   .format("kafka")</p>
			<p class="source-code">                   .option("kafka.bootstrap.servers", </p>
			<p class="source-code">                           "localhost:9092")</p>
			<p class="source-code">                   .option("subscribe", "retail_events")</p>
			<p class="source-code">                   .option("startingOffsets", "earliest")</p>
			<p class="source-code">                 .load()</p>
			<p class="source-code">            )</p>
			<p>In the preceding code block, we start reading a stream of events from a Kafka topic, called <strong class="source-inline">retail_events</strong>, and tell Kafka that we want to start loading the events from the beginning of the stream using the <strong class="source-inline">startingOffsets</strong> option. The events in a Kafka topic follow a key-value pattern. This means that our actual data is encoded within a JSON object in the <strong class="source-inline">value</strong> column that we need to extract, as shown in the following code block:</p>
			<p class="source-code">from pyspark.sql.functions import col, from_json, to_date</p>
			<p class="source-code">retail_df = (kafka_df</p>
			<p class="source-code">              .select(from_json(col("value").cast(StringType()), eventSchema).alias("message"), col("timestamp").alias("EventTime"))</p>
			<p class="source-code">              .select("message.*", "EventTime")</p>
			<p class="source-code">)</p>
			<p>In the preceding code block, we extract the data using the <strong class="source-inline">from_json()</strong> function by passing in the data schema object that we defined earlier. This results in a <strong class="source-inline">retail_df</strong> DataFrame that has all of the columns of the event that we require. Additionally, we append an <strong class="source-inline">EventTime</strong> column from the Kafka topic, which shows when the event actually arrived in Kafka. This could be of some use later, during further data processing. Since this DataFrame <a id="_idIndexMarker172"/>was created using the <strong class="source-inline">readStream()</strong> function, Spark inherently knows this is a Streaming DataFrame and makes Structured Streaming APIs available to this DataFrame.</p>
			<p>Once we have extracted the raw event data from the Kafka stream, we can persist it to the data lake, as shown in the following block of code:</p>
			<p class="source-code">base_path = "/tmp/data-lake/retail_events.parquet"</p>
			<p class="source-code">(retail_df</p>
			<p class="source-code">  .withColumn("EventDate", to_date(retail_df.EventTime))</p>
			<p class="source-code">    .writeStream</p>
			<p class="source-code">      .format('parquet')</p>
			<p class="source-code">      .outputMode("append")</p>
			<p class="source-code">      .trigger(once=True)</p>
			<p class="source-code">      .option('checkpointLocation', base_path + '/_checkpoint')</p>
			<p class="source-code">  .start(base_path)</p>
			<p class="source-code">)</p>
			<p>In the preceding code block, we make use of the <strong class="source-inline">writeStream()</strong> function that is available to Streaming DataFrames to save data to the data lake in a streaming fashion. Here, we write data in Parquet format, and the resultant data on the data lake will be a set of <strong class="source-inline">.parquet</strong> files. Once saved, these Parquet files are no different from any other Parquet files, whether created by batch processing or streams processing.</p>
			<p>Additionally, we use <strong class="source-inline">outputMode</strong> as <strong class="source-inline">append</strong> to indicate that we will treat this as an unbounded dataset and will keep appending new Parquet files. The <strong class="source-inline">checkpointLocation</strong> option stores the Structured Streaming write-ahead log and other checkpointing information. This makes it an incremental data load job as the stream only picks <a id="_idIndexMarker173"/>up new and unprocessed events based on the offset information stored at the checkpoint location.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Structured Streaming supports <strong class="source-inline">complete</strong> and <strong class="source-inline">update</strong> modes in addition to <strong class="source-inline">append</strong> mode. A description <a id="_idIndexMarker174"/>of these modes and when to use them can be found <a id="_idIndexMarker175"/>in Apache Spark's official documentation at <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes">https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes</a>.</p>
			<p>But what if you need to run the incremental data load job as a less frequent batch process instead of running it in a continuous streaming manner?</p>
			<p>Well, Structured Streaming supports this too via the <strong class="source-inline">trigger</strong> option. We can use <strong class="source-inline">once=True</strong> for this option, and the streaming job will process all new and unprocessed events when the job is externally triggered and then stop the stream when there are no new events to be processed. We can schedule this job to run periodically based on a time interval and it will just behave like a batch job but with all the benefits of an incremental load.</p>
			<p>In summary, the Spark SQL engine's DataFrame API is both powerful and easy to use for batch data processing and streams processing. There are slight differences between the functions and utilities provided between a static DataFrame and streaming DataFrame. However, for the most part, the programming models between batch processing and streams processing that use DataFrames are very similar. This minimizes the learning curve and helps to unify batch and streams processing using Apache Spark's unified analytics engine. </p>
			<p>Now, in the next section, let's examine how to implement a unified data processing architecture with Apache Spark using a concept called <strong class="bold">Lambda Architecture</strong>. </p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor050"/>Unifying batch and real time using Lambda Architecture</h1>
			<p>Both batch and real-time <a id="_idIndexMarker176"/>data processing are important elements of any modern Enterprise DSS, and an architecture <a id="_idIndexMarker177"/>that seamlessly <a id="_idIndexMarker178"/>implements both these data processing <a id="_idIndexMarker179"/>techniques can help increase throughput, minimize latency, and allow you to get to fresh data much more quickly. One such architecture is called <strong class="bold">Lambda Architecture</strong>, which we will examine next.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>Lambda Architecture</h2>
			<p>Lambda Architecture is a data processing technique that is used to ingest, process, and query both historical and real-time <a id="_idIndexMarker180"/>data with a single architecture. Here, the goal is to increase throughput, data freshness, and fault tolerance while maintaining a single view of both historical and real-time data for end users. The following diagram illustrates a typical Lambda Architecture:</p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="Images/B16736_02_03.jpg" alt="Figure 2.3 – Lambda Architecture&#13;&#10;" width="1079" height="597"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Lambda Architecture</p>
			<p>As shown in the preceding diagram, a Lambda Architecture consists of three main components, namely, the <strong class="bold">Batch Layer</strong>, the <strong class="bold">Speed Layer</strong>, and the <strong class="bold">Serving Layer</strong>. We will discuss each of these layers in the following sections.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor052"/>The Batch layer</h2>
			<p>The Batch layer is like any typical ETL layer involving the batch processing of data from the source system. This layer <a id="_idIndexMarker181"/>usually involves <a id="_idIndexMarker182"/>scheduled jobs that run periodically, typically, at night.</p>
			<p>Apache Spark can be used to build batch processing jobs or Structured Streaming jobs that get triggered on a schedule, and it can also be used for the batch layer to build historical data in the data lake.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor053"/>The Speed layer</h2>
			<p>The Speed layer continuously ingests data from the same data source as the Batch layer from the data lake <a id="_idIndexMarker183"/>into real-time views. The Speed layer continuously delivers the latest data that the Batch layer cannot provide yet due to its <a id="_idIndexMarker184"/>inherent latency. Spark Structured Steaming can be used to implement low latency streaming jobs to continuously ingest the latest data from the source system.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/>The Serving layer</h2>
			<p>The Serving layer combines the historical data from the Batch layer and the latest data from the Speed layer into a single view to support ad hoc queries by end users. Spark SQL makes a good <a id="_idIndexMarker185"/>candidate for the Serving layer <a id="_idIndexMarker186"/>as it can help users query historical data from the Batch layer, as well as the latest data from the Speed layer, and present the user with a unified view of data for low-latency, ad hoc queries.</p>
			<p>In the previous sections, you implemented data ingestion jobs for batch as well as streaming using Apache Spark. Now, let's explore how you can combine the two views to give users a single unified view, as shown in the following code snippet:</p>
			<p class="source-code">batch_df = spark.read.parquet("/tmp/data-lake/online_retail.parquet")</p>
			<p class="source-code">speed_df = spark.read.parquet("/tmp/data-lake/retail_events.parquet").drop("EventDate").drop("EventTime")</p>
			<p class="source-code">serving_df = batch_df.union(speed_df)</p>
			<p class="source-code">serving_df.createOrReplaceGlobalTempView("serving_layer")</p>
			<p>In the preceding <a id="_idIndexMarker187"/>code block, we create two DataFrames, one from the <strong class="bold">Batch Layer</strong> location and the other from the <strong class="bold">Speed Layer</strong> location <a id="_idIndexMarker188"/>on the data lake. We call the  <strong class="source-inline">union</strong> function to <a id="_idIndexMarker189"/>unite these two DataFrames and then create a <strong class="bold">Spark Global Temp View</strong> using the combined <a id="_idIndexMarker190"/>DataFrame. The result is a view that is accessible across all <strong class="bold">Spark Sessions</strong> across the cluster, which gives you a <a id="_idIndexMarker191"/>unified view of data across both the batch and speed layers, as shown in the following line of code:</p>
			<p class="source-code">%sql</p>
			<p class="source-code">SELECT count(*) FROM global_temp.serving_layer;</p>
			<p>The preceding line of code is a SQL query that queries data from the Spark global view, which acts as the <strong class="bold">Serving Layer</strong> and can be presented to end users for ad hoc queries across both the latest data and historical data.</p>
			<p>In this way, you can make use of Apache Spark's SQL engine's DataFrame, Structured Streaming, and SQL APIs to build a Lambda Architecture that improves data freshness, throughput, and provides a unified view of data. However, the Lambda Architecture is somewhat complex to maintain because there are two separate data ingestion pipelines for batch and real-time <a id="_idIndexMarker192"/>processing along with two separate data sinks. Indeed, there is an easier way to unify the batch and speed layers using an open source storage layer called <strong class="bold">Delta Lake</strong>. You will learn about this in <a href="B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Cleansing and Integration</em>.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>Summary</h1>
			<p>In this chapter, you learned about Enterprise DSS in the context of big data analytics and its components. You learned about various types of data sources such as RDBMS-based operational systems, message queues, and file sources, and data sinks, such as data warehouses and data lakes, and their relative merits.</p>
			<p>Additionally, you explored different types of data storage formats such as unstructured, structured, and semistructured and learned about the benefits of using structured formats such as Apache Parquet with Spark. You were introduced to data ingestion in a batch and real-time manner and learned how to implement them using Spark DataFrame APIs. We also introduced Spark's Structured Streaming framework for real-time streams processing, and you learned how to use Structured Streaming to implement incremental data loads using minimal programming overheads. Finally, you explored the Lambda Architecture to unify batch and real-time data processing and its implementation using Apache Spark. The skills learned in this chapter will help you to implement scalable and performant distributed data ingestion pipelines via Apache Spark using both batch and streams processing models.</p>
			<p>In the next chapter, you will learn about the techniques to process, cleanse, and integrate the raw data that was ingested into a data lake in this chapter into clean, consolidated, and meaningful datasets that are ready for end users to perform business analytics on and generate meaningful insights. </p>
		</div>
	</div></body></html>