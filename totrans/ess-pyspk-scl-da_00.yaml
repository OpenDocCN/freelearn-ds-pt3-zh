- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a unified data analytics engine designed to process huge volumes
    of data in a fast and efficient way. PySpark is the Python language API of Apache
    Spark that provides Python developers an easy-to-use scalable data analytics framework.
  prefs: []
  type: TYPE_NORMAL
- en: '*Essential PySpark for Scalable Data Analytics* starts by exploring the distributed
    computing paradigm and provides a high-level overview of Apache Spark. You''ll
    then begin your data analytics journey with the data engineering process, learning
    to perform data ingestion, data cleansing, and integration at scale.'
  prefs: []
  type: TYPE_NORMAL
- en: This book will also help you build real-time analytics pipelines that enable
    you to gain insights much faster. Techniques for building cloud-based data lakes
    are presented along with Delta Lake, which brings reliability and performance
    to data lakes.
  prefs: []
  type: TYPE_NORMAL
- en: A newly emerging paradigm called the Data Lakehouse is presented, which combines
    the structure and performance of a data warehouse with the scalability of cloud-based
    data lakes. You'll learn how to perform scalable data science and machine learning
    using PySpark, including data preparation, feature engineering, model training,
    and model productionization techniques. Techniques to scale out standard Python
    machine learning libraries are also presented, along with a new pandas-like API
    on top of PySpark called Koalas.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is intended for practicing data engineers, data scientists, data analysts,
    citizen data analysts, and data enthusiasts who are already using data analytics
    to delve into the world of distributed and scalable data analytics. It's recommended
    that you have knowledge of the field of data analytics and data manipulation to
    gain actionable insights.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014), *Distributed
    Computing Primer*, introduces the distributed computing paradigm. It also talks
    about how distributed computing became a necessity with the ever-increasing data
    sizes over the last decade and ends with the in-memory data-parallel processing
    concept with the Map Reduce paradigm, and finally, contains introduction to the
    latest features in Apache Spark 3.0 engine.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032), *Data Ingestion*,
    covers various data sources, such as databases, data lakes, message queues, and
    how to ingest data from these data sources. You will also learn about the uses,
    differences, and efficiency of various data storage formats at storing and processing
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056), *Data Cleansing
    and Integration*, discusses various data cleansing techniques, how to handle bad
    incoming data, data reliability challenges and how to cope with them, and data
    integration techniques to build a single integrated view of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B16736_04_Final_JM_ePub.xhtml#_idTextAnchor075), *Real-time Data
    Analytics*, explains how to perform real-time data ingestion and processing, discusses
    the unique challenges that real-time data integration presents and how to overcome,
    and also the benefits it provides.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094), *Scalable Machine
    Learning with PySpark*, briefly talks about the need to scale out machine learning
    and discusses various techniques available to achieve this from using natively
    distributed machine learning algorithms to embarrassingly parallel processing
    to distributed hyperparameter search. It also provides an introduction to PySpark
    MLlib library and an overview of its various distributed machine learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107), *Feature Engineering
    â€“ Extraction, Transformation, and Selection*, explores various techniques for
    converting raw data into features that are suitable to be consumed by machine
    learning models, including techniques for scaling, transforming features.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B16736_07_Final_JM_ePub.xhtml#_idTextAnchor128), *Supervised
    Machine Learning*, explores supervised learning techniques for machine learning
    classification and regression problems including linear regression, logistic regression,
    and gradient boosted trees.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B16736_08_Final_JM_ePub.xhtml#_idTextAnchor150), *Unsupervised
    Machine Learning*, covers unsupervised learning techniques such as clustering,
    collaborative filtering, and dimensionality reduction to reduce the number of
    features prior to applying supervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164), *Machine Learning
    Life Cycle Management*, explains that it is not just sufficient to just build
    and train models, but in the real world, multiple versions of the same model are
    built and different versions are suitable for different applications. Thus, it
    is necessary to track various experiments, their hyperparameters, metrics, and
    also the version of the data they were trained on. It is also necessary to track
    and store the various models in a centrally accessible repository so models can
    be easily productionized and shared; and finally, mechanisms are needed to automate
    this repeatedly occurring process. This chapter introduces these techniques using
    an end-to-end open source machine learning life cycle management library called
    MLflow.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176)*, Scaling Out
    Single-Node Machine Learning Using PySpark*, explains that in [*Chapter 5*](B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094)*,
    Scalable Machine Learning with PySpark*, you learned how to use the power of Apache
    Spark''s distributed computing framework to train and score machine learning models
    at scale. Spark''s native machine learning library provides good coverage of standard
    tasks that data scientists typically perform; however, there is a wide variety
    of functionality provided by standard single-node Python libraries that were not
    designed to work in a distributed manner. This chapter deals with techniques for
    horizontally scaling out standard Python data processing and machine learning
    libraries such as pandas, scikit-learn, and XGBoost. This chapter covers scaling
    out typical data science tasks such as exploratory data analysis, model training,
    model inference, and finally also covers a scalable Python library named Koalas
    that lets you effortlessly write PySpark code using very familiar and easy-to-use
    pandas-like syntax.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B16736_11_Final_JM_ePub.xhtml#_idTextAnchor188)*,* *Data Visualization
    with PySpark*, covers data visualizations, which are an important aspect of conveying
    meaning from data and gleaning insights into it. This chapter covers how the most
    popular Python visualization libraries can be used along with PySpark.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B16736_12_Final_JM_ePub.xhtml#_idTextAnchor199)*, Spark SQL
    Primer*, covers SQL, which is an expressive language for ad hoc querying and data
    analysis. This chapter will introduce Spark SQL for data analysis and also show
    how to interchangeably use PySpark with data analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B16736_13_Final_JM_ePub.xhtml#_idTextAnchor214)*, Integrating
    External Tools with Spark SQL*, explains that once we have clean, curated, and
    reliable data in our performant data lake, it would be a missed opportunity to
    not democratize this data across the organization to citizen analysts. The most
    popular way of doing this is via various existing **Business Intelligence** (**BI**)
    tools. This chapter deals with requirements for BI tool integration.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B16736_14_Final_JM_ePub.xhtml#_idTextAnchor222)*, The Data Lakehouse*,
    explains that traditional descriptive analytics tools such as BI tools are designed
    around data warehouses and expect data to be presented in a certain way and modern
    advanced analytics and data science tools are geared toward working with large
    amounts of data that''s easily accessible in data lakes. It is also not practical
    or cost-effective to store redundant data in separate storage locations to be
    able to cater to these individual use cases. This chapter will present a new paradigm
    called Data Lakehouse that tries to overcome the limitations of data warehouses
    and data lakes and bridge the gap by combining the best elements of both.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Basic to intermediate knowledge of the disciplines of data engineering, data
    science, and SQL analytics is expected. A general level of proficiency using any
    programming language, especially Python, and a working knowledge of performing
    data analytics using frameworks such as pandas and SQL will help you to get the
    most out of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16736_Preface_Table_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The book makes use of Databricks Community Edition to run all code: [https://community.cloud.databricks.com](https://community.cloud.databricks.com).
    Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).'
  prefs: []
  type: TYPE_NORMAL
- en: The entire code base used in this book can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/all_chapters/ess_pyspark.dbc](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/all_chapters/ess_pyspark.dbc).
  prefs: []
  type: TYPE_NORMAL
- en: The datasets used for this chapter can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data).
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are using the digital version of this book, we advise you to type
    the code yourself or access the code from the book''s GitHub repository (a link
    is available in the next section). Doing so will help you avoid any potential
    errors related to the copying and pasting of code.**'
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics).
    If there's an update to the code, it will be updated in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that has color images of the screenshots and diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781800568877_ColorImages.pdf](_ColorImages.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: "The `readStream()` method of the DataStreamReader
    object is used to create the streaming DataFrame."'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold**. Here is an example:
    "There can be multiple **Map** stages followed by multiple **Reduce** stages."'
  prefs: []
  type: TYPE_NORMAL
- en: Tips or important notes
  prefs: []
  type: TYPE_NORMAL
- en: Appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '`customercare@packtpub.com` and mention the book title in the subject of your
    message.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  prefs: []
  type: TYPE_NORMAL
- en: '`copyright@packt.com` with a link to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Share your thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you've read *Essential PySpark for Scalable Data Analytics*, we'd love
    to hear your thoughts! Please [https://packt.link/r/1-800-56887-8](https://packt.link/r/1-800-56887-8)
    for this book and share your feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we're delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
