<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer091">
			<h1 id="_idParaDest-222"><a id="_idTextAnchor222"/>Chapter 14: The Data Lakehouse</h1>
			<p>Throughout this book, you have encountered two primary data analytics use cases: descriptive analytics, which includes BI and SQL analytics, and advanced analytics, which includes data science and machine learning. You learned how Apache Spark, as a unified data analytics platform, can cater to all these use cases. Apache Spark, being a computational platform, is data storage-agnostic and can work with any traditional storage mechanisms, such as databases and data warehouses, and modern distributed data storage systems, such as data lakes. However, traditional descriptive analytics tools, such as BI tools, are designed around data warehouses and expect data to be presented in a certain way. Modern advanced analytics and data science tools are geared toward working with large amounts of data that can easily be accessed on data lakes. It is also not practical or cost-effective to store redundant data in separate storage to be able to cater to these individual use cases. </p>
			<p>This chapter will present a new paradigm called the <strong class="bold">data lakehouse</strong>, which tries to overcome the limitations of data warehouses and data lakes and bridge the gap by combining the best elements of both. </p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Moving from BI to AI</li>
				<li>The data lakehouse paradigm</li>
				<li>Advantages of data lakehouses</li>
			</ul>
			<p>By the end of this chapter, you will have learned about the key challenges of existing data storage architectures, such as data warehouses and data lakes, and how a data lakehouse helps bridge this gap. You will gain an understanding of the core requirements of a data lakehouse and its reference architecture, as well as explore a few existing and commercially available data lakehouses and their limitations. Finally, you will learn about the reference architecture for the data lakehouse, which makes use of Apache Spark and Delta Lake, as well as some of their advantages.</p>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor223"/>Moving from BI to AI</h1>
			<p><strong class="bold">Business intelligence</strong> (<strong class="bold">BI</strong>) remains<a id="_idIndexMarker988"/> the staple of data analytics. In BI, organizations collect raw transactional from a myriad of data sources and ETL it into a format that is conducive for building operational reports and enterprise dashboards, which depict the overall enterprise over a past period. This also helps business executives make informed decisions on the future strategy of an organization. However, if the amount of transactional data that's been generated has increased by several orders of magnitude, it is difficult (if not impossible) to surface relevant and timely insights that can help businesses make decisions. Moreover, it is also not sufficient to just rely on structured transactional data for business decision-making. Instead, new types of unstructured data, such as customer feedback in the form of natural language, voice transcripts from a customer service center, and videos and images of products and customer reviews need to be considered if you wish to understand the current state of a business, the state of the market, and customer and social trends for businesses to stay relevant and profitable. Thus, you must move on from traditional BI and decision support systems and supplement the operational reports and executive <a id="_idIndexMarker989"/>dashboards with predictive analytics, if not completely replace <strong class="bold">BI</strong> with <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>). Traditional BI and data warehouse tools completely fall apart when catering to AI use cases. This will be explored in more detail in the next few sections.</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor224"/>Challenges with data warehouses</h2>
			<p>Traditionally, data <a id="_idIndexMarker990"/>warehouses have been the primary data sources of BI tools. Data warehouses expect data to be transformed and stored in a predefined schema that makes it easy for BI tools to query it. BI tools have evolved to take advantage of data warehouses, which makes the process very efficient and performant. The following diagram represents the typical reference architecture<a id="_idIndexMarker991"/> of a <strong class="bold">BI and DW</strong> system:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="Images/B16736_14_01.jpg" alt="Figure 14.1 – BI and DW architecture&#13;&#10;" width="1371" height="781"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – BI and DW architecture</p>
			<p>As shown in the <a id="_idIndexMarker992"/>preceding diagram, a <strong class="bold">BI and DW</strong> system extracts raw transactional <a id="_idIndexMarker993"/>data from transactional systems, transforms the data according to a schema that has been defined by the data warehouse, and then loads the data into the data warehouse. This entire process is scheduled to be run periodically, typically nightly. Modern ETL and data warehouse systems have also evolved to support data loads more frequently, such as hourly. However, there are a few key drawbacks to this approach that limit these systems from truly supporting modern AI use cases. They are as follows:</p>
			<ul>
				<li>The compute and storage of traditional data warehouses are typically located on-premises on a single server machine. Their capacity is generally planned for peak workloads as the storage and compute capacity of such databases is tied to the machine or server they are running on. They cannot be scaled easily, if at all. This means that traditional on-premises data warehouses have their data capacity set and cannot handle the rapid influx of data that big data typically brings on. This makes their architecture rigid and not future-proof.</li>
				<li>Data warehouses were designed to only be loaded periodically, and almost all traditional data warehouses were not designed to handle real-time data ingestion. This means data analysts and business executives working off such data warehouses usually only get to work on stale data, delaying their decision-making process.</li>
				<li>Finally, data warehouses are based on relational databases, which, in turn, cannot handle unstructured data such as video, audio, or natural language. This makes them incapable of expanding to cater to advanced analytics use cases such as <a id="_idIndexMarker994"/>data science, machine learning, or AI.</li>
			</ul>
			<p>To overcome the aforementioned shortcomings of data warehouses, especially the inability to separate compute and storage, and thus the inability to scale on-demand and their shortcomings with dealing with real-time and unstructured data, enterprises have moved toward data lake architectures. These were first introduced by the <strong class="bold">Hadoop</strong> ecosystem. We will look at this in more detail in the following sections.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor225"/>Challenges with data lakes</h2>
			<p>Data lakes are low-cost storage <a id="_idIndexMarker995"/>systems with filesystem-like APIs that can hold any form of data, whether it's structured or unstructured, such as the <strong class="bold">Hadoop Distributed File System</strong> (<strong class="bold">HDFS</strong>). Enterprises adopted the data lake paradigm to solve the <a id="_idIndexMarker996"/>scalability and segregation of compute and storage. With the advent of big data and Hadoop, the first HDFS was adopted as data lakes were being stored in generic and open file formats such as Apache Parquet and ORC. With the advent of the cloud, object stores such as Amazon S3, Microsoft Azure ADLS, and Google Cloud Storage were adopted as data lakes. These are very inexpensive and allow us to automatically archive data. </p>
			<p>While data lakes are highly scalable, relatively inexpensive, and can support a myriad of data and file types, they do not conform to the strict schema-on-write requirements of BI tools. Thus, the cloud-based data lake architecture was supplemented with an additional layer of cloud-based data warehouses to specifically cater to BI use cases. The architecture of a typical decision support system in the cloud is shown in the following diagram:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="Images/B16736_14_02.jpg" alt="Figure 14.2 – Data lake architecture in the cloud&#13;&#10;" width="1263" height="608"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.2 – Data lake architecture in the cloud</p>
			<p>In the previous diagram, we <a id="_idIndexMarker997"/>can see a typical data lake architecture in the cloud. First, raw data is ingested into the data lake as is, without any transformations applied, in a streaming or fashionable manner. The raw data is then ELTed and put back into the data lake for consumption by downstream use cases such as data science, machine learning, and AI. Part of the data that's required for BI and operational reporting is cleaned, integrated, and loaded into a cloud-based data warehouse to be consumed by BI tools. This architecture solves all the issues of a traditional data warehouse. Data lakes are infinitely scalable and completely independent of any compute. This is because compute is only required while either ingesting, transforming, or consuming data. Data lakes can also handle different data types, ranging from structured and semi-structured data to unstructured data.</p>
			<p>However, the data lake architecture does present a few key challenges:</p>
			<ul>
				<li>Data lakes do not possess any built-in transactional controls or data quality checks, so data engineers need to build additional code into their data processing pipelines to perform transactional control. This helps ensure the data is consistent and of the right quality for downstream consumption.</li>
				<li>Data lakes can store data in structured files such as Parquet and ORC; however, traditional BI tools may not be capable of reading data in such files, so another layer of a data warehouse must be introduced to cater to these use cases. This introduces an additional layer of complexity as two separate data processing pipelines need to exist – one for ELTing the data to be used by advanced analytics use cases, and another for ETLing the data into the data warehouse. This also increases the operational costs as data storage almost doubles and two separate data storage systems need to be managed.</li>
				<li>While raw data could be streamed into the warehouse, it may not be readily available for downstream business analytics systems until the raw data has been ETLed into the <a id="_idIndexMarker998"/>data warehouse, presenting stale data to business users and data analysts, thereby delaying their decision-making process.</li>
			</ul>
			<p>The data lakehouse promises to overcome such challenges that are faced by traditional data warehouses, as well as modern data lakes, and help bring the best elements of both to end users. We will explore the data lakehouse paradigm in detail in the following section.</p>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor226"/>The data lakehouse paradigm</h1>
			<p>The data lakehouse paradigm <a id="_idIndexMarker999"/>combines the best aspects of the data warehouse with those of the data lake. A data lakehouse is based on open standards and implements data structures and data management features such as data warehouses. This paradigm also uses data lakes for its cost-effective and scalable data storage. By combining the best of both data warehousing and data lakes, data lakehouses cater to data analysts and data scientists simultaneously, without having to maintain multiple systems or having to maintain redundant copies of data. Data lakehouses help accelerate data projects as teams access data in a single place, without needing to access multiple systems. Data lakehouses also provide access to the freshest data, which is complete and up to date so that it can be used in BI, data science, machine learning, and AI projects. Though data lakehouses are based on data lakes such as cloud-based object stores, they need to adhere to certain requirements, as described in the following section.</p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor227"/>Key requirements of a data lakehouse</h2>
			<p>A data lakehouse needs to <a id="_idIndexMarker1000"/>satisfy a few key requirements for it to be able to provide the structure and data management capabilities of a data warehouse, as well as the scalability and ability to work with the unstructured data of a data lake. The following are some key requirements that must be taken into consideration:</p>
			<ul>
				<li>A data lakehouse needs to be able to <a id="_idIndexMarker1001"/>support <strong class="bold">ACID</strong> transactions to ensure data reads for SQL queries. In a data lakehouse, multiple data pipelines could be writing and reading data from the same dataset, and support for transactions guarantees that data readers and writers never have an inconsistent view of data.</li>
				<li>A data lakehouse should be able to decouple compute from storage to make sure that one can scale independently of the other. Not only does it make the data lakehouse more economical but it also helps support concurrent users using multiple clusters and very large datasets.</li>
				<li>A data lakehouse needs to be based on open standards. This allows a variety of tools, APIs, and libraries to be able to access the data lakehouse directly and prevents any sort of expensive vendor or data lock-ins.</li>
				<li>To be able to support structured data and data models such as <strong class="bold">Star/Snowflake</strong> schemas<a id="_idIndexMarker1002"/> from the data warehousing world, a data lakehouse must be able to support schema enforcement and evolution. The data lakehouse should support mechanisms for managing data integrity, governance, and auditing.</li>
				<li>Support for a variety of data types, including structured and unstructured data types, is required as a data lakehouse can be used to store, analyze, and process data, ranging from text, transactions, IoT data, and natural language to audio transcripts and video files.</li>
				<li>Support for traditional structured data analysis such as BI and SQL analytics, as well as advanced analytics workloads including data science, machine learning, and AI, is required. A data lakehouse should be able to directly support BI and data discovery by supporting <a id="_idIndexMarker1003"/>SQL standard connectivity over <strong class="bold">JDBC/ODBC</strong>.</li>
				<li>A data lakehouse should be able to support end-to-end stream processing, starting from the ability to ingest real-time data directly into the data lakehouse, to real-time ELT<a id="_idIndexMarker1004"/> of data and real-time business analytics. The data lakehouse should also support real-time machine learning and low-latency machine learning inference in real time.</li>
			</ul>
			<p>Now that you understand the key requirements of a data lakehouse, let's try to understand its core components and reference architecture.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor228"/>Data lakehouse architecture</h2>
			<p>Data lakehouse<a id="_idIndexMarker1005"/> features such as scalability, the ability to handle unstructured data, and being able to segregate storage and compute are afforded by the underlying data lakes that are used for persistent storage. However, a few core components are required to <a id="_idIndexMarker1006"/>provide data warehouse-like functionality, such as <strong class="bold">ACID</strong> transactions, indexes, data governance and audits, and other data-level optimizations. A scalable metadata layer is one of the core components. A metadata layer sits on top of open file formats such as Apache Parquet and helps keep track of file and table versions and features such as ACID transactions. The metadata layer also enables features such as streaming data ingestion, schema enforcement, and evolution, and enforcing data validation. </p>
			<p>Based on these core components, a reference data lakehouse architecture has been produced, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="Images/B16736_14_03.jpg" alt="Figure 14.3 – Data lakehouse architecture&#13;&#10;" width="1263" height="619"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.3 – Data lakehouse architecture</p>
			<p>Data lakehouses are<a id="_idIndexMarker1007"/> built on top of inexpensive cloud-based storage, which does not offer very high throughput data access. To be able to cater to low latency and highly concurrent use cases, a data lakehouse needs to be able to provide speedy data access via features such as data skipping indexes, folder- and file-level pruning, and the ability to collect and store table and file statistics to help the query execution engine derive an optimal query execution plan. The data lakehouse should possess a high-speed data caching layer to speed up data access for frequently accessed data. </p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor229"/>Examples of existing lakehouse architectures</h2>
			<p>A few <a id="_idIndexMarker1008"/>commercially available cloud-based offerings do satisfy the requirements of a data lakehouse to some extent, if not completely. Some of them are listed in this section.</p>
			<p><strong class="bold">Amazon Athena</strong> is an<a id="_idIndexMarker1009"/> interactive query service provided by AWS as part of its managed services. AWS is backed by the open source scalable query engine Presto and lets you query data residing in S3 buckets. It supports a metadata layer that's backed by Hive and lets you create table schema definitions. However, query engines such as Athena cannot solve all the problems of data lakes and data warehouses. They still lack basic data management features such as ACID transactions and performance improvement features such as indexes and caching.</p>
			<p>The cloud-based commercial data warehouses known as Snowflake come a close second as it offers all the features of a traditional data warehouse, along with more advanced features that support artificial intelligence, machine learning, and data science. It combines data warehouses, subject-specific data marts, and data lakes into a single version of the truth that can power multiple workloads, including traditional analytics and advanced analytics. However, Snowflake does not provide data management features; data is stored <a id="_idIndexMarker1010"/>within its storage system and it doesn't provide the same features for data stored in data lakes. Snowflake might also not be a good fit for large-scale ML projects as the data would need to be streamed into a data lake.</p>
			<p><strong class="bold">Google BigQuery</strong>, the <a id="_idIndexMarker1011"/>petabyte-scale, real-time data warehousing solution, offers almost all the features that a data lakehouse does. It supports simultaneous batch and streaming workloads, as well as ML workloads using SQL- and query-like language via its BigQuery ML offering, which even supports AutoML. However, even BigQuery requires data to be stored in its internal format, and it doesn't provide all the query performance-boosting features for external data stored in data lakes. In the following section, we will explore how we can leverage Apache Spark, along with Delta Lake and cloud-based data lakes, as a data lakehouse.</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor230"/>Apache Spark-based data lakehouse architecture</h2>
			<p>Apache Spark, when combined<a id="_idIndexMarker1012"/> with Delta Lake and cloud-based data lakes, satisfies almost all the requirements of a data lakehouse. We will explore this, along with an Apache Spark-based reference architecture, in this section. Let's get started:</p>
			<ul>
				<li>Delta Lake, via its <a id="_idIndexMarker1013"/>transaction logs, fully supports ACID transactions, similar to a traditional data warehouse, to ensure data that's written to Delta Lake is consistent and that any downstream readers never read any dirty data. This also allows multiple reads to occur and writes data to the same dataset from multiple Spark clusters, without compromising the integrity of the dataset.</li>
				<li>Apache Spark has always been data storage-agnostic and can read data from a myriad of data sources, This includes reading data into memory so that it can be processed and then writing to the results to persistent storage. Thus, Apache Spark, when coupled with a distributed, persistent storage system such as a cloud-based data lake, fully supports decoupling storage and compute.</li>
				<li>Apache Spark supports multiple ways of accessing data stored within Delta Lake, including direct access using Spark's Java, Scala, PySpark, and SparkR APIs. Apache Spark also supports JDBC/ODBC connectivity via Spark ThriftServer for BI tool connectivity. Delta Lake also supports plain Java APIs for connectivity outside of Apache Spark.</li>
				<li>Delta Lake supports<a id="_idIndexMarker1014"/> its own built-in metadata layer via its transaction log. Transaction logs provide Delta Lake with version control, an audit trail of table changes, and Time Travel to be able to traverse between various versions of a table, as well as the ability to restore any snapshot of the table at a given point in time.</li>
				<li>Both Apache Spark and <a id="_idIndexMarker1015"/>Delta Lake support all types of structured and unstructured data. Moreover, Delta Lake supports schema enforcement, along with support and schema evolution.</li>
				<li>Apache Spark has support for real-time analytics via Structured Streaming, and Delta Lake fully supports simultaneous batch and streaming.</li>
			</ul>
			<p>Thus, Apache Spark, along with Delta Lake coupled with cloud-based data lakes, supports in-memory data caching and performance improvement features such as data-skipping indexes and collecting table and file statistics. This combination makes for a great candidate for a data lakehouse.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Spark Structured Streaming only <a id="_idIndexMarker1016"/>supports micro-batch-based streaming and doesn't support event processing. Also, Apache Spark and Delta Lake together make SQL query performance very fast. However, Spark's inherent JVM scheduling delays still introduce a considerable delta in query processing times, making it unsuitable for ultra-low latency queries. Moreover, Apache Spark can support concurrent users via multiple clusters and by tuning certain Spark cluster parameters, though this complexity needs to be managed by the user. These reasons make Apache Spark, despite being used with Delta Lake, not suitable for very high concurrency, ultra-low latency use cases. Databricks has developed a next-generation query processing engine named Photon that can overcome these issues of open source Spark. However, Databricks has not released Photon for open source Apache Spark at the time of writing.</p>
			<p>Now that you have seen how Apache Spark<a id="_idIndexMarker1017"/> and Delta Lake can work together as a data lakehouse, let's see what that reference architecture looks like:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="Images/B16736_14_04.jpg" alt="Figure 14.4 – Data lakehouse powered by Apache Spark and Delta Lake &#13;&#10;" width="1263" height="640"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.4 – Data lakehouse powered by Apache Spark and Delta Lake </p>
			<p>The preceding diagram shows a data lakehouse architecture using Apache Spark and Delta Lake. Here, data ingestion can be done in real time or batch using Structured Streaming, via regular Spark batch jobs, or using some third-party data integration tool. Raw data from various sources, such as transactional databases, IoT data, clickstream data, server logs, and so on, is directly streamed into the data lakehouse and stored in Delta file format. Then, that raw data is transformed using Apache Spark DataFrame APIs or using Spark SQL. Again, this can be accomplished in either batch or streaming fashion using Structured Streaming. Table metadata, indexes, and statistics all are handled by Delta Lake transaction logs, and Hive can be used as the metastore. BI and SQL analytics tools can consume the data in the data lakehouse directly using JDBC/ODBC connections, and advanced analytics tools and libraries can directly interface with the data in the lakehouse using Spark SQL or DataFrame APIs vis the Spark clusters. This way, the combination of Apache Spark and Delta Lake within the cloud, with object stores as data lakes, can be used to implement the data lakehouse paradigm. Now that we have implemented a reference Data Lakehouse architecture, let's understand some of its advantages.</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor231"/>Advantages of data lakehouses</h1>
			<p>Data lakehouses address <a id="_idIndexMarker1018"/>most of the challenges of using data warehouses and data lakes. Some advantages of using data lakehouses are that they reduce data redundancies, which are caused by two-tier systems such as a data lake along with a data warehouse in the cloud. This translates to reduced storage costs and simplified maintenance and data governance as any data governance features, such as access control and audit logging, can be implemented in a single place. This eliminates the operational overhead of managing data governance on multiple tools.</p>
			<p>You should have all the data in a single storage system so that you have simplified data processing and ETL architectures, which also means easier to maintain and manage pipelines. Data engineers do not need to maintain separate code bases for disparate systems, and this greatly helps in reducing errors in data pipelines. It also makes it easier to track data lineage and fix data issues when they are identified.</p>
			<p>Data lakehouses provide data analysts, business executives, and data scientists with direct access to the most recent data in the lakehouse. This reduces their dependence on IT teams for data access and helps them with timely and informed decision-making. Data lakehouses ultimately reduce the total cost of ownership as they eliminate data redundancy, reduce operational overhead, and provide performant data processing and storage systems at a fraction of the cost compared to certain commercially available specialist data warehouses. </p>
			<p>Despite all the advantages offered by data lakehouses, the technology is still nascent, so it might lag behind certain purpose-built products that have had decades of research and development behind them. As the technology matures, data lakehouses will become more performant and offer connectivity to more common workflows and tools, while still being simple to use and cost-effective.</p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor232"/>Summary</h1>
			<p>In this chapter, you saw the challenges that are faced by data warehouses and data lakes in designing and implementing large-scale data processing systems that deal with large-scale data. We also looked at the need for businesses to move from advanced analytics to simple descriptive analytics and how the existing systems cannot solve both problems simultaneously. Then, the data lakehouse paradigm was introduced, which solves the challenges of both data warehouses and data lakes and how it bridges the gap of both systems by combining the best elements from both. The reference architecture for data lakehouses was presented and a few data lakehouse candidates were presented from existing commercially available, large-scale data processing systems, along with their drawbacks. Next, an Apache Spark-based data lakehouse architecture was presented that made use of the Delta Lake and cloud-based data lakes. Finally, some advantages of data lakehouses were presented, along with a few of their shortcomings.</p>
		</div>
	</div></body></html>