- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Time Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to *Modern Time Series Forecasting with Python*! This book is intended
    for data scientists or **machine learning** (**ML**) engineers who want to level
    up their time series analysis skills by learning new and advanced techniques from
    the ML world. **Time series analysis** is something that is commonly overlooked
    in regular ML books, courses, and so on. They typically start with classification,
    touch upon regression, and then move on. But it is also something that is immensely
    valuable and ubiquitous in business. We look at the world from a three-dimensional
    perspective. Time is the hidden dimension that we rarely think about, but is all-pervasive.
    And as long as time is one of the four dimensions in the world we live in, time
    series data is all-pervasive too.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing time series data unlocks a lot of value for a business. Time series
    analysis isn’t new—it’s been around since the 1920s. But in the current age of
    data, the time series that are collected by businesses are growing larger and
    wider by the minute. Combined with an explosion in the quantum of data collected
    and the renewed interest in ML, the landscape of time series analysis also changed
    considerably. This book attempts to take you beyond classical statistical methods
    such as **AutoRegressive Integrated Moving Average** (**ARIMA**) and introduce
    to you the latest techniques from the ML world in time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to start with some fundamental concepts and quickly scale up to
    more complex topics. In this chapter, we’re going to cover the following main
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a time series?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data-generating process (DGP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can we forecast?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting terminology and notation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need to set up the **Anaconda** environment by following the instructions
    in the *Preface* of the book to get a working environment with all the libraries
    and datasets required for the code in this book. Any additional library will be
    installed while running the notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: The associated code for the chapter can be found at [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter01](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter01).
  prefs: []
  type: TYPE_NORMAL
- en: What is a time series?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To keep it simple, a **time series** is a set of observations taken sequentially
    in time. The focus is on the word *time*. If we keep taking the same observation
    at different points in time, we will get a time series. For example, if you keep
    recording the number of bars of chocolate you have in a month, you’ll end up with
    a time series of your chocolate consumption. Suppose you are recording your weight
    at the beginning of every month. You get another time series of your weight. Is
    there any relation between the two time series? Most likely, yeah. But we will
    be able to analyze that scientifically by the end of this book.
  prefs: []
  type: TYPE_NORMAL
- en: A few other examples of time series are the weekly closing price of a stock
    that you follow, daily rainfall or snowfall in your city, and hourly readings
    of your pulse rate from your smartwatch.
  prefs: []
  type: TYPE_NORMAL
- en: Types of time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two types of time series data based on time intervals, as outlined
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regular time series**: This is the most common type of time series, where
    we have observations coming in at regular intervals of time, such as every hour
    or every month. For example, if we take a time series of temperature in a city,
    we will get the time series in a regular interval (whichever frequency we choose
    for observation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Irregular time series**: There are a few time series where we do not have
    observations at regular intervals of time. For example, consider we have a sequence
    of readings from lab tests of a patient. We see an observation in the time series
    only when the patient heads to the clinic and carries out the lab test, and this
    may not happen at regular intervals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book only focuses on regular time series, which are evenly spaced in time.
    Irregular time series are slightly more advanced and require specialized techniques
    to handle them. A couple of survey papers on the topic is a good way to get started
    on irregular time series, and you can find them in the *Further reading* section
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Main areas of application for time series analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are broadly three important areas of application for time series analysis,
    outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time series forecasting**: Predicting the future values of a time series,
    given the past values—for example, predict the next day’s temperature using the
    last 5 years of temperature data. This use case is one of the most popular and
    important ones because any kind of planning we need to do needs some visibility
    into the future. For instance, planning how many chocolates to produce next month
    needs a forecast of expected demand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time series classification**: Sometimes, instead of predicting the future
    value of the time series, we may also want to predict an action based on past
    values. For example, given historical measurements from an **electroencephalogram**
    (**EEG**; tracking electrical activity in the brain) or an **electrocardiogram**
    (**EKG**; tracking electrical activity in the heart), we need to predict whether
    the result of an EEG or an EKG is normal or abnormal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier detection**: There are some situations where we only want to detect
    if something is going wrong or if something is out of the ordinary. In such cases,
    we need to use classification or forecasting, but instead, we can do outlier detection.
    For instance, the wearable tech on your body records accelerometer readings across
    time and can use outlier detection to identify falls or accidents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation and causality**: You can use time-series analysis to understand
    the whats and whys of the time series based on past values, understand the relationships
    between several related time series, or derive causal inferences based on time
    series data. For example, we have a time series of market share for a brand and
    another time series of advertising spend. Using interpretation and causality techniques,
    we can start to understand how much advertising investment is affecting the market
    share and possibly take appropriate action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The focus of this book is predominantly on *time series forecasting*, but the
    techniques that you learn will help you approach *time series classification*
    problems also, with minimal changes in the approach. *Interpretation* is also
    addressed, although only briefly, but *causality* is an area that this book does
    not address because it warrants a whole different approach.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an overview of the time series landscape, let’s build a mental
    model of how time series data is generated.
  prefs: []
  type: TYPE_NORMAL
- en: Data-generating process (DGP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen that time series data is a collection of observations made sequentially
    along the time dimension. Any time series is, in turn, generated by some kind
    of *mechanism*. For example, time series data of daily shipments of your favorite
    chocolate from the manufacturing plant is affected by a lot of factors, such as
    the time of the year (holiday season, for example), the availability of cocoa,
    the uptime of the machines working on the plant, and so on. In statistics, this
    underlying process that generates the time series is referred to as the **DGP**.
    Time series data is produced by stochastic and deterministic processes. The deterministic
    processes involve quantities that evolve in a predictable manner over time. An
    example of this is the radioactive decay of an element, where the remaining quantity
    diminishes according to a precise mathematical formula, leading to a consistent
    reduction over time. But most of the interesting time series (from a forecasting
    perspective) are generated by a stochastic process. A stochastic process is a
    way to describe how things change over time in a random but somewhat predictable
    manner, like how the weather changes daily with some patterns and probabilities
    involved. So, let’s discuss more about time series generated from stochastic processes.
  prefs: []
  type: TYPE_NORMAL
- en: If we had complete and perfect knowledge of reality, all we would need to do
    would be to put this DGP together in a mathematical form and you would get the
    most accurate forecast possible. But sadly, nobody has complete and perfect knowledge
    of reality. So, what we try to do is approximate the DGP, mathematically, as much
    as possible so that our imitation of the DGP gives us the best possible forecast
    (or any other output we want from the analysis). This imitation is called a **model**
    that provides a useful approximation of the DGP.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we must remember that the model is not the DGP, but a representation of
    some essential aspects of reality. For example, let’s consider an aerial view
    of Bengaluru and a map of Bengaluru, as represented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – An aerial view of Bengaluru (left) and a map of Bengaluru (right)
    ](img/B22389_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: An aerial view of Bengaluru (left) and a map of Bengaluru (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The map of Bengaluru is certainly useful—we can use it to go from point A to
    point B. But a map of Bengaluru is not the same as a photo of Bengaluru. It doesn’t
    showcase the bustling nightlife or the insufferable traffic. A map is just a model
    that represents some useful features of a location, such as roads and places.
    The following diagram might help us internalize the concept and remember it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – DGP, model, and time series ](img/B22389_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: DGP, model, and time series'
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, the next question would be this: *Do we have a useful model?* Every
    model has limitations and challenges. As we have seen, a map of Bengaluru does
    not perfectly represent Bengaluru. But if our purpose is to navigate Bengaluru,
    then a map is a very useful model. What if we want to understand the culture?
    A map doesn’t give you a flavor of that. So, now, the same model that was useful
    is utterly useless in the new context.'
  prefs: []
  type: TYPE_NORMAL
- en: Different kinds of models are required in different situations and for different
    objectives. For example, the best model for forecasting may not be the same as
    the best model for making a causal inference.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the concept of DGPs to generate multiple synthetic time series of
    varying degrees of complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Generating synthetic time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synthetic time series, or artificial time series, are excellent tools with which
    you can understand the time series space, experiment with different techniques,
    and even test new models or modeling setups. These time series are designed to
    be predictable, even though a bit challenging. Let’s take a look at a few practical
    examples where we can generate a few time series using a set of fundamental building
    blocks. You can get creative and mix and match any of these components, or even
    add them together to generate a time series of arbitrary complexity.
  prefs: []
  type: TYPE_NORMAL
- en: White and red noise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An extreme case of a stochastic process that generates a time series is a **white
    noise** process. It has a sequence of random numbers with zero mean and constant
    variance. This is also one of the most popular assumptions of noise in a time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can generate such a time series and plot it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – White noise process ](img/B22389_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: White noise process'
  prefs: []
  type: TYPE_NORMAL
- en: '**Red noise**, on the other hand, has zero mean and constant variance but is
    serially correlated in time. This serial correlation or redness is parameterized
    by a correlation coefficient *r*, such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22389_01_001.png)'
  prefs: []
  type: TYPE_IMG
- en: where *w* is a random sample from a white noise distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can generate that, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Red noise process ](img/B22389_01_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Red noise process'
  prefs: []
  type: TYPE_NORMAL
- en: Cyclical or seasonal signals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Among the most common signals you see in time series are seasonal or cyclical
    signals. Therefore, you can introduce seasonality into your generated series in
    a few ways.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the help of a very useful library to generate the rest of the time
    series—`TimeSynth`. For more information, refer to [https://github.com/TimeSynth/TimeSynth](https://github.com/TimeSynth/TimeSynth).
  prefs: []
  type: TYPE_NORMAL
- en: This is a useful library for generating time series. It has all kinds of DGPs
    that you can mix and match and create an authentic synthetic time series.
  prefs: []
  type: TYPE_NORMAL
- en: '**Notebook alert**:'
  prefs: []
  type: TYPE_NORMAL
- en: For the exact code and usage, please refer to the associated Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can use a sinusoidal function to create cyclicity. There is
    a helpful function in `TimeSynth` called `generate_timeseries` that helps us combine
    signals and generate time series. Have a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Sinusoidal waves ](img/B22389_01_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Sinusoidal waves'
  prefs: []
  type: TYPE_NORMAL
- en: Note the two sinusoidal waves are different with respect to the frequency (how
    fast the time series crosses zero) and amplitude (how far away from zero the time
    series travels).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Pseudo-periodic signal ](img/B22389_01_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Pseudo-periodic signal'
  prefs: []
  type: TYPE_NORMAL
- en: Autoregressive signals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another very popular signal in the real world is an **autoregressive (AR) signal**.
    We will go into this in more detail in *Chapter 4*, *Setting a Strong Baseline
    Forecast*, but for now, an AR signal refers to when the value of a time series
    for the current timestep is dependent on the values of the time series in the
    previous timesteps. This serial correlation is a key property of the AR signal,
    and it is parametrized by a few parameters, outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Order of serial correlation—or, in other words, the number of previous timesteps
    the signal is dependent on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coefficients to combine the previous timesteps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see how we can generate an AR signal and see what it looks like, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – AR signal ](img/B22389_01_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: AR signal'
  prefs: []
  type: TYPE_NORMAL
- en: Mix and match
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many more components that you can use to create your DGP and thereby
    generate a time series, but let’s quickly look at how we can combine the components
    we have already seen to generate a realistic time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use a pseudo-periodic signal with white noise and combine it with an
    AR signal, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Pseudo-periodic signal with AR and white noise ](img/B22389_01_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: Pseudo-periodic signal with AR and white noise'
  prefs: []
  type: TYPE_NORMAL
- en: Stationary and non-stationary time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In time series, **stationarity** is of great significance and is a key assumption
    in many modeling approaches. Ironically, many (if not most) real-world time series
    are non-stationary. So, let’s understand what a stationary time series is from
    a layman’s point of view.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to look at stationarity, but one of the clearest and
    most intuitive ways is to think of the probability distribution or the data distribution
    of a time series. We call a time series stationary when the probability distribution
    remains the same at every point in time. In other words, if you pick different
    windows in time, the data distribution across all those windows should be the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: 'A standard Gaussian distribution is defined by two parameters—the mean and
    the variance. So, there are two ways the stationarity assumption can be broken,
    as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Change in mean over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change in variance over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at these assumptions in detail and understand them better.
  prefs: []
  type: TYPE_NORMAL
- en: Change in mean over time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the most popular way a non-stationary time series presents itself. If
    there is an upward/downward trend in the time series, the mean across two windows
    of time would not be the same.
  prefs: []
  type: TYPE_NORMAL
- en: Another way non-stationarity manifests itself is in the form of seasonality.
    Suppose we are looking at the time series of average temperature measurements
    per month for the last 5 years. From our experience, we know that temperature
    peaks during summer and falls in winter. So, when we take the mean temperature
    of winter and the mean temperature of summer, they will be different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate a time series with trend and seasonality and see how it manifests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Sinusoidal signal with trend and white noise ](img/B22389_01_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: Sinusoidal signal with trend and white noise'
  prefs: []
  type: TYPE_NORMAL
- en: If you examine the time series in *Figure 1.9*, you will be able to see a definite
    trend and the seasonality, which together make the mean of the data distribution
    change wildly across different windows of time.
  prefs: []
  type: TYPE_NORMAL
- en: Change in variance over time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Non-stationarity can also present itself in the fluctuating variance of a time
    series. If the time series starts off with low variance and as time progresses,
    the variance keeps getting bigger and bigger, we have a non-stationary time series.
    In statistics, there is a scary name for this phenomenon—**heteroscedasticity**.
    The Air Passengers dataset, which is the “iris dataset” of time series (the most
    popular, over-used, and useless) is a classic example of a heteroscedastic time
    series. Let’s look at the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22389_01_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: Air Passengers dataset—Example of a heteroscedastic time series'
  prefs: []
  type: TYPE_NORMAL
- en: In the figure, you can see that the seasonal peaks keep getting wider and wider
    as we move through time, and this is a classic sign that the time series is heteroscedastic.
    But not all heteroscedastic time series are easy to spot. We have statistical
    tests to check for each of the stationarity cases, which we will cover in *Chapter
    7*, *Target Transformations for Time Series Forecasting*.
  prefs: []
  type: TYPE_NORMAL
- en: This book just tries to give you an understanding of stationary and non-stationary
    time series. There is a lot of statistical theory and depth in this discussion
    that we are skipping to keep our focus on the practical aspects of time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Armed with the mental model of the DGP, we are at the right place to think
    about another important question: *what can we forecast?*'
  prefs: []
  type: TYPE_NORMAL
- en: What can we forecast?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we move ahead, there is another aspect of time series forecasting that
    we have to understand—*the predictability of a time series*. The most basic assumption
    when we forecast a time series is that the future depends on the past. But not
    all time series are equally predictable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at a few examples and try to rank these in order of predictability
    (from easiest to hardest), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: High tide next Monday
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lottery numbers next Sunday
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stock price of Tesla next Friday
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intuitively, it is very easy for us to rank them. High tide next Monday is going
    to be the easiest to predict because it is so predictable, the stock price of
    Tesla next Friday is going to be difficult to predict, but not impossible, and
    the lottery numbers are going to be very hard to predict because they are pretty
    much random.
  prefs: []
  type: TYPE_NORMAL
- en: However, for people thinking that they can forecast stock prices with the advanced
    techniques covered in the book and get rich, that (most likely) won’t happen.
    Although it is worthy of a lengthy discussion, we can summarize the key points
    in a short paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Share prices are not a function of their past values but an anticipation of
    their future values, and this thereby violates our first assumption while forecasting.
    And if that is not bad enough, financial stock prices typically have a very low
    signal-to-noise ratio. The final wrench in the process is the **efficient market
    hypothesis** (**EMH**). This seemingly innocent hypothesis proclaims that all
    known information about a stock price is already factored into the price of the
    stock. The implication of the hypothesis is that if you can forecast accurately,
    many others will also be able to do that, and thereby the market price of the
    stock already reflects the change in price that this forecast brought about.
  prefs: []
  type: TYPE_NORMAL
- en: The M6 competition chose to tackle this problem head-on to evaluate if the EMH
    holds true by conducting a year-long forecasting and investment strategy competition.
    Although not conclusive, the results show that the EMH holds true for the vast
    majority of the participants, barring a few top teams. And even in that, they
    found out that in the top teams, there was no significant correlation between
    forecasting accuracy and the selection of stocks into the portfolio, i.e. the
    teams weren’t choosing stocks which they were able to forecast better (a link
    to the full report is provided in the *Further reading* section).
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to the topic at hand—predictability—three main factors form a mental
    model for this, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding the DGP**: The better you understand the DGP, the higher the
    predictability of a time series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amount of data**: The more data you have, the better your predictability
    is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adequately repeating pattern**: For any mathematical model to work well,
    there should be an adequately repeating pattern in your time series. The more
    repeatable the pattern is, the better your predictability is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though you have a mental model of how to think about predictability, we
    will look at more concrete ways of assessing the predictability of time series
    in *Chapter 3*, *Analyzing and Visualizing Time Series Data*, but the key takeaway
    is that not all time series are equally predictable.
  prefs: []
  type: TYPE_NORMAL
- en: In order to fully follow the discussion in the coming chapters, we need to establish
    a standard notation and learn the terminology that is specific to time series
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a few terms that will help you understand this book as well as other
    literature on time series. These terms are described in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forecasting**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting is the prediction of future values of a time series using the known
    past values of the time series and/or some other related variables. This is very
    similar to prediction in ML, where we use a model to predict unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multivariate forecasting**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multivariate time series consist of more than one time series variable that
    is not only dependent on its past values but also has some dependency on the other
    variables. For example, a set of macroeconomic indicators, such as **gross domestic
    product** (**GDP**) and inflation, of a particular country can be considered a
    multivariate time series. The aim of multivariate forecasting is to come up with
    a model that captures the interrelationship between the different variables along
    with its relationship with its past and forecast all the time series together
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '**Explanatory forecasting**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the past values of a time series, we might use some other information
    to predict the future values of a time series. For example, when predicting retail
    store sales, information regarding promotional offers (both historical and future
    ones) is usually helpful. This type of forecasting, which uses information other
    than its own history, is called explanatory forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Backtesting**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting aside a validation set from your training data to evaluate your models
    is a practice that is common in the ML world. Backtesting is the time series equivalent
    of validation, whereby you use the history to evaluate a trained model. We will
    cover the different ways of doing validation and cross-validation for time series
    data later.
  prefs: []
  type: TYPE_NORMAL
- en: '**In-sample and out-sample**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again drawing parallels with ML, in-sample refers to training data and out-sample
    refers to unseen or testing data. When you hear in-sample metrics, this refers
    to metrics calculated on training data, and out-sample metrics refers to metrics
    calculated on testing data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exogenous and endogenous variables**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exogenous variables are parallel time series variables that are not modeled
    directly for output but used to help us model the time series that we are interested
    in. Typically, exogenous variables are not affected by other variables in the
    system. Endogenous variables are variables that are affected by other variables
    in the system. A purely endogenous variable is a variable that is entirely dependent
    on the other variables in the system. Relaxing the strict assumptions a bit, we
    can consider the target variable as the endogenous variable and the explanatory
    regressors we include in the model as exogenous variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**Forecast combination**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecast combinations in the time series world are similar to ensembles from
    the ML world. Forecast combination is a process by which we combine multiple forecasts
    by using a function, either learned or heuristic-based, such as a simple average
    of three forecast models.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot more terms that are specific to time series, some of which we
    will be covering throughout the book. But these terms should be a good starting
    point to give you basic familiarity in the field.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we had our first look at time series as we discussed the different
    types of time series, looked at how a DGP generates a time series, and saw how
    we can think about the important question: *how well can we forecast a time series?*
    We also had a quick review of the terminology required to understand the rest
    of the book. In the next chapter, we will be getting our hands dirty and will
    learn how to acquire and process time series data. If you have not set up the
    environment yet, take a break and put some time into doing that.'
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*A Survey on Principles, Models and Methods for Learning from Irregularly Sampled
    Time Series: From Discretization to Attention and Invariance* by S.N. Shukla and
    B.M. Marlin (2020): [https://arxiv.org/abs/2012.00168](https://arxiv.org/abs/2012.00168)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning from Irregularly-Sampled Time Series: A Missing Data Perspective*
    by S.C. Li and B.M. Marlin (2020), ICML: [https://arxiv.org/abs/2008.07599](https://arxiv.org/abs/2008.07599)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The M6 forecasting competition: Bridging the gap between forecasting and investment
    decisions by Spyros Makridakis et al. (2023)*:[https://arxiv.org/abs/2310.13357](https://arxiv.org/abs/2310.13357)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mts](https://packt.link/mts)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code15080603222089750.png)'
  prefs: []
  type: TYPE_IMG
