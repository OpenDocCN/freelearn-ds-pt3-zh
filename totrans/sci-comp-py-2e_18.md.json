["```py\nconda install mpi4py\n```", "```py\nimport mpi4py as mpi\n```", "```py\nmpiexec -n 4 python script.py\n```", "```py\nmpiexec --hostfile=hosts.txt python script.py\n```", "```py\n# Content of hosts.txt\n192.168.1.25 :4 # master computer with 4 cores\n192.168.1.101:2 # worker computer with 2 cores\n```", "```py\nfrom mpi4py import MPI\ncomm=MPI.COMM_WORLD  # making a communicator instance\nrank=comm.Get_rank() # querrying for the numeric identifyer of the core \nsize=comm.Get_size() # the total number of cores assigned\n```", "```py\n\nfrom mpi4py import MPI\ncomm=MPI.COMM_WORLD # making a communicator instance\nrank=comm.Get_rank() # querrying for the numeric identifyer of the core \nsize=comm.Get_size() # the total number of cores assigned \na=15\nb=2\nif rank==0:\n    print(f'Core {rank} computes {a}+{b}={a+b}')\nif rank==1:\n    print(f'Core {rank} computes {a}*{b}={a*b}')\nif rank==2:\n    print(f'Core {rank} computes {a}**{b}={a**b}')\n```", "```py\nmpiexec -n 3 python basicoperations.py\n```", "```py\nCore 0 computes 15+2=17\nCore 2 computes 15**2=225\nCore 1 computes 15*2=3\n```", "```py\nfrom mpi4py import MPI\ncomm=MPI.COMM_WORLD # making a communicator instance\nrank=comm.Get_rank() # querying for the numeric identifier of the core \nsize=comm.Get_size() # the total number of cores assigned \nif not (size==2):\n    raise Exception(f\"This examples requires two processes. \\\n                    {size} processes given.\")\n```", "```py\ncount = 0\ntext=['Ping','Pong']\nprint(f\"Rank {rank} activities:\\n==================\")\nwhile count < 5:\n    if rank == count%2:\n        print(f\"In round {count}: Rank {rank} says {text[count%2]}\"\"\n               \"and sends the ball to rank {(rank+1)%2}\")\n        count += 1\n        comm.send(count, dest=(rank+1)%2)\n    elif rank == (count+1)%2:\n        count = comm.recv(source=(rank+1)%2)\n```", "```py\nmpiexec -n 2 python pingpong.py \n```", "```py\nRank 0 activities:\n==================\nIn round 0: Rank 0 says Ping and sends the ball to rank 1\nIn round 2: Rank 0 says Ping and sends the ball to rank 1\nIn round 4: Rank 0 says Ping and sends the ball to rank 1\nRank 1 activities:\n==================\nIn round 1: Rank 1 says Pong and sends the ball to rank 0\nIn round 3: Rank 1 says Pong and sends the ball to rank 0\n```", "```py\nfrom mpi4py import MPI\ncomm=MPI.COMM_WORLD # making a communicator instance\nrank=comm.Get_rank() # querying for the numeric identifier of the core\nsize=comm.Get_size() # the total number of cores assigned\n\nif rank==0:\n    def func():\n        return 'Function called'\n    comm.send(func, dest=1)\nif rank==1:\n    f=comm.recv(source=0)    # <<<<<< This line reports an error\n    print(f())One-to-all and all-to-one communication\n```", "```py\nfrom mpi4py import MPI\ncomm=MPI.COMM_WORLD # making a communicator instance\nrank=comm.Get_rank() # querying for the numeric identifier of the core\nsize=comm.Get_size() # the total number of cores assigned\n\ndef func():\n    return 'Function called'\nif rank==0:\n    comm.send(func, dest=1)\nif rank==1:\n    f=comm.recv(source=0) \n    print(f())\n```", "```py\nfrom mpi4py import MPI\ncomm=MPI.COMM_WORLD # making a communicator instance\nrank=comm.Get_rank() # querying for the numeric identifier of the core\nsize=comm.Get_size() # the total number of cores assigned\nimport numpy as np\n\nif rank==0:\n    A = np.arange(700)\n    comm.Send(A, dest=1)\nif rank==1:\n    A = np.empty(700, dtype=int)  # This is needed for memory allocation \n                                  # of the buffer on Processor 1\n    comm.Recv(A, source=0)        # Note, the difference to recv in \n                                  # providing the data.\n    print(f'An array received with last element {A[-1]}')\n\n```", "```py\nfrom mpi4py import MPI\ncomm=MPI.COMM_WORLD # making a communicator instance\nrank=comm.Get_rank() # querrying for the numeric identifier of the core\nsize=comm.Get_size() # the total number of cores assigned\n\nif rank==0:\n    msg=['Message from rank 0',list(range(101000))]\n    comm.send(msg, dest=1)\n    print(f'Process {rank} sent its message')\n    s=comm.recv(source=1)\n    print(f'I am rank {rank} and got a {s[0]} with a list of \\\n          length {len(s[1])}')\nif rank==1:\n    msg=['Message from rank 1',list(range(-101000,1))]\n    comm.send(msg,dest=0)\n    print(f'Process {rank} sent its message')\n    s=comm.recv(source=0)\n    print(f'I am rank {rank} and got a {s[0]} with a list of \\\n          length {len(s[1])}')\n```", "```py\nfrom mpi4py import MPI\ncomm=MPI.COMM_WORLD # making a communicator instance\nrank=comm.Get_rank() # querrying for the numeric identifier of the core\nsize=comm.Get_size() # the total number of cores assigned\n\nif rank==0:\n    msg=['Message from rank 0',list(range(101000))]\n    comm.send(msg, dest=1)\n    print(f'Process {rank} sent its message')\n    s=comm.recv(source=1)\n    print(f'I am rank {rank} and got a {s[0]} with a list of \\\n          length {len(s[1])}')\nif rank==1:\n    s=comm.recv(source=0)\n    print(f'I am rank {rank} and got a {s[0]} with a list of \\\n          length {len(s[1])}')\n msg=['Message from rank 1',list(range(-101000,1))]\n    comm.send(msg,dest=0)\n    print(f'Process {rank} sent its message')\n    print(f'I am rank {rank} and got a {s[0]} with a list of \\\n          length {len(s[1])}')\n```", "```py\ndef split_array(vector, n_processors):\n # splits an array into a number of subarrays \n # vector one dimensional ndarray or a list\n # n_processors integer, the number of subarrays to be formed\n\n    n=len(vector)\n    n_portions, rest = divmod(n,n_processors) # division with remainder\n    # get the amount of data per processor and distribute the res on\n    # the first processors so that the load is more or less equally \n    # distributed\n    # Construction of the indexes needed for the splitting\n    counts = [0]+ [n_portions + 1 \\\n                 if p < rest else n_portions for p in range(n_processors)]\n    counts=numpy.cumsum(counts)\n    start_end=zip(counts[:-1],counts[1:]) # a generator\n    slice_list=(slice(*sl) for sl in start_end) # a generator comprehension\n    return [vector[sl] for sl in slice_list] # a list of subarrays\n```", "```py\nfrom mpi4py import MPI\nimport numpy as np\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nnprocessors = comm.Get_size()\nimport splitarray as spa \n\nif rank == 0:\n    # Here we generate data for the example\n    n = 150\n    u = 0.1*np.arange(n)\n    v = - u\n    u_split = spa.split_array(u, nprocessors)\n    v_split = spa.split_array(v, nprocessors)\nelse:\n    # On all processor we need variables with these names,\n    # otherwise we would get an Exception \"Variable not defined\" in \n    # the scatter command below\n    u_split = None\n    v_split = None\n# These commands run now on all processors\nu_split = comm.scatter(u_split, root=0) # the data is portion wise \n                                        # distributed from root\nv_split = comm.scatter(v_split, root=0)\n# Each processor computes its part of the scalar product\npartial_dot = u_split@v_split\n# Each processor reports its result back to the root\npartial_dot = comm.gather(partial_dot,root=0)\n\nif rank==0:\n    # partial_dot is a list of all collected results\n    total_dot=np.sum(partial_dot)\n    print(f'The parallel scalar product of u and v'\n        f'on {nprocessors} processors is {total_dot}.\\n'\n        f'The difference to the serial computation is \\\n        {abs(total_dot-u@v)}')\n```", "```py\nmexec -n 5 python parallel_dot.py\n```", "```py\nThe parallel scalar product of u and v on 5 processors is -11137.75.\nThe difference to the serial computation is 0.0\n```", "```py\n......... modification of the script above .....\n# Each processor reports its result back to the root\n# and these results are summed up\ntotal_dot = comm.reduce(partial_dot, op=MPI.SUM, root=0)\n\nif rank==0:\n   print(f'The parallel scalar product of u and v'\n         f' on {nprocessors} processors is {total_dot}.\\n'\n         f'The difference to the serial computation \\\n         is {abs(total_dot-u@v)}') \n```", "```py\ndata = comm.bcast(data, root=0)\n```"]