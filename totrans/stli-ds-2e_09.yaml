- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Connecting to Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters we focused entirely on data stored in individual files,
    but most of the real-world, work-based applications center around data stored
    in databases. Companies tend to store their data in the cloud, and therefore,
    being able to perform analyses on this data is a critical skill. In this chapter,
    we will explore how to access and use data stored in popular databases such as
    Snowflake and BigQuery. For each database, we’ll connect to the database, write
    SQL queries, and then make an example app.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are looking to perform ad hoc analysis on large datasets or build
    data-driven applications, the ability to efficiently retrieve and manipulate data
    from databases is essential. By the end of this chapter, you will have a strong
    understanding of how to use Streamlit to connect to and interact with databases,
    empowering you to extract insights and make data-driven decisions with confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to Snowflake with Streamlit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting to BigQuery with Streamlit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding user input to queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a list of software and hardware installations that are required
    for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Snowflake account**: To get a Snowflake account, go to ([https://signup.snowflake.com/](https://signup.snowflake.com/))
    and start a free trial.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snowflake Python Connector**: The Snowflake Python Connector allows you to
    run queries from Python. If you installed the requirements for this book, then
    you already have the library. If not, `pip install` `snowflake-connector-python`
    to get started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BigQuery account**: To get a BigQuery account, go to ([https://console.cloud.google.com/bigquery](https://console.cloud.google.com/bigquery))
    and start a free trial.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BigQuery Python Connector**: BigQuery also has a Python Connector that works
    the same way as the Snowflake Python Connector does! It also is in the requirements
    file that you installed at the beginning of the book, but you can also pip install
    `google-cloud-bigquery` if you do not have the library yet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have everything set up, let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to Snowflake with Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To connect to any database within Streamlit, we mostly need to think about how
    to connect to that service **in** Python and then add some Streamlit-specific
    functions (like caching!) to improve the user experience. Luckily, Snowflake has
    invested a lot of time in making it incredibly easy to connect to Snowflake from
    Python; all you need to do is specify your account info and the Snowflake Python
    connector does the rest.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll create and work in a new folder called `database_examples`
    and add a `streamlit_app.py` file, along with a Streamlit `secrets` file to get
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Within the `secrets.toml` file, we need to add our username, password, account,
    and warehouse. Our username and password are the ones we added when we signed
    up for our Snowflake account, the warehouse is the virtual computer that Snowflake
    uses to run the query (the default one is called `COMPUTE_WH`), and your account
    identifier is the only one left! To find your account identifier, the easiest
    way to find up to date info is through this link ([https://docs.snowflake.com/en/user-guide/admin-account-identifier](https://docs.snowflake.com/en/user-guide/admin-account-identifier)).
    Now that we have all the info we need, we can add them to our secrets file! Our
    file should look like the following, with your info instead of mine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the account info from the result of the SQL query above, we
    have all the info we need, and we can add it to our `secrets` file! Our file should
    look like the following, with your info instead of mine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can start making our Streamlit app. Our first step is going to create
    our Snowflake connection, run a basic SQL query, and then output that to our Streamlit
    app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This code does a few things; first, it uses the Snowflake Python Connector to
    programmatically connect to our Snowflake account using the secrets in our `secrets`
    file, then it runs the SQL query that just returns `1`, and finally, it shows
    that output in our app.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our app should now look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Snowflake Query Result'
  prefs: []
  type: TYPE_NORMAL
- en: 'Every time we run this app it will reconnect to Snowflake. This isn’t a great
    user experience, as it will make our app slower. In the past we would have cached
    this by wrapping it in a function and caching it with `st.cache_data`, but that
    will actually not work here as the connection is not data. Instead, we should
    cache it with `st.cache_resource`, similar to how we dealt with the HuggingFace
    model earlier in this book. Our session initialization code should now look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our app should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: SQL GROUPBY'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we also want to cache the result of the data to speed up our app and reduce
    the cost. This is something we’ve done before; we can wrap the query call in a
    function and use `st.cache_data` to cache it! It should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Our last step for this app is to dress up the appearance a bit. Right now it’s
    fairly basic, so we can add a graph, a title, and also what column we should use
    to graph as the user. Also, we will make sure our results are of the type `float`
    (which is roughly a non-integer number), as a good, general practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our app is interactive, and it shows a great graph! It will look like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: The TCP-H final app'
  prefs: []
  type: TYPE_NORMAL
- en: That is it for our section on connecting to Snowflake with Streamlit! There
    are currently Snowflake products in preview that let you create Streamlit apps
    directly inside of Snowflake. If you want access to products like these, reach
    out to your Snowflake admin and they should be able to help you get access!
  prefs: []
  type: TYPE_NORMAL
- en: Now, on to BigQuery!
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to BigQuery with Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step to getting BigQuery connected to your Streamlit app is to gather
    the authentication information necessary from BigQuery. There is a wonderful Quickstart
    doc that Google keeps (and maintains!) that you should follow, which can be found
    here: [https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries](https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries).
    This link will help you sign up for a free account, and create a project. After
    you create your project, you need to create a service account ([https://console.cloud.google.com/apis/credentials](https://console.cloud.google.com/apis/credentials))
    and download the credentials as a JSON file. Once you have this file, you have
    all the data needed and can return to this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, we will create a new file in our `database_example` folder
    called `bigquery_app.py`, and we will add a new section to the `secrets.toml`
    file we already created. First, we can add to the `secrets.toml` file and finally,
    let you create and view your service account credentials using this link ([https://console.cloud.google.com/apis/credentials](https://console.cloud.google.com/apis/credentials)).
    Go ahead and paste your service account credentials into a new section of your
    `secrets.toml` file like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to create and open our new app file, called `bigquery_app.py`,
    and connect to BigQuery from there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we want to run a query, we can use the client variable that we created
    with our authentication to run it! To show an example, Google kindly provides
    a free dataset that stores how often people download Python libraries. We can
    write a quick query of that dataset that counts the last 5 days of Streamlit downloads
    in our app, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this app, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: The BigQuery result'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, I ran the query around 8pm PST on March 29^(th), which means
    that parts of the world had already moved on to March 30^(th) and started downloading
    libraries. This is the reason for the big drop on the 30^(th)! Next, as an improvement,
    we can graph the downloads over time with `st.line_chart()`, as we have done quite
    a few times in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: The BigQuery graph'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will notice, it takes quite a while to run these queries. This is because
    we are caching neither the result nor the connection. Let’s add some functions
    to do that into our app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And the bottom of our app will use the new `get_dataframe_from_sql` that we’ve
    just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: And that is it! Now you know how to get data from BigQuery and cache the results
    and the authentication process. This will be extremely useful as you start using
    Streamlit in work environments, as data rarely lives entirely in .`csv` files
    and instead exists in cloud databases. This next section will cover a couple more
    strategies to work with queries and databases in Streamlit.
  prefs: []
  type: TYPE_NORMAL
- en: Adding user input to queries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the major benefits of using Streamlit is making user interactivity extremely
    easy, and we want to enable this while we write the apps that connect to databases.
    So far, we have written queries that we convert into DataFrames, and on top of
    these DataFrames, we can add our typical Streamlit widgets to further filter,
    group by, and then graph our data. However, this situation will only truly work
    on relatively small datasets, and often, we will have to change the underlying
    query for better performance in our apps. Let’s prove this point with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us return to our Streamlit app in `bigquery_app.py`. We had a relatively
    arbitrary lookback period for our app, where we simply pulled the last 5 days
    in our query. What if we wanted to let the user define the lookback period? If
    we insisted on not changing the query and filtering after the query ran, then
    we would have to pull all the data from the `bigquery-public-data.pypi.file_downloads`
    table, which would be extremely slow and cost a huge amount of money. Instead,
    we can do the following to add a slider that changes the underlying query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this situation, we added a slider that has appropriate minimum and maximum
    values, inputting the result of the slider into our query. This will cause the
    query to rerun every time the slider is moved, but it is still much more efficient
    than pulling the entire dataset. Now our app should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Dynamic SQL'
  prefs: []
  type: TYPE_NORMAL
- en: We could have just as easily added dynamic SQL to our Snowflake queries with
    the same method, but this shows a wonderful example of it with BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: One word of warning here is to **never** use text input as input into a database
    query. If you allow freeform text as an input and put that into your queries,
    you functionally give your users the same access to your database that you have.
    You can use any of the other Streamlit widgets you would like without the same
    ramification because you have a guarantee of the output of widgets like `st.slider`,
    which will always return a number and never a malicious query.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about adding user input to our queries, we can head
    over to our last section, organizing queries in Streamlit apps.
  prefs: []
  type: TYPE_NORMAL
- en: Organizing queries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you create more and more Streamlit apps that rely on database queries, your
    Streamlit apps often tend to get extremely long and will include long queries
    stored as strings. This tends to make apps harder to read, and less understandable
    when collaborating with others. It is not uncommon for the Streamlit Data Team
    to have half a dozen 30-line queries powering one Streamlit app that we created!
    There are two strategies to improve this setup:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating downstream tables with a tool like `dbt`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing queries in separate files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will really only cover the first of these, creating downstream tables, briefly.
    As we noticed in the last example, every time the user changed the slider, the
    query would rerun in the app. This can get rather inefficient! We could use a
    tool like dbt, which is a very popular tool that lets us schedule SQL queries,
    to create a smaller table that already had the larger table filtered down to contain
    only the last 30 days of Streamlit data inside `bigquery-public-data.pypi.file_downloads`.
    This way, our query would be fewer lines and would not crowd our app, and it would
    also be more cost-effective and fast! We use this tip very often in the Streamlit
    Data Team, and we often have smaller downstream tables created in dbt that power
    our Streamlit apps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second option is to store our queries in entirely separate files, and then
    import them into our apps. To do this, create a new file called `queries.py` in
    the same directory as our Streamlit app. Inside this file, we want to create a
    function that returns the `pypi` data query that we have already created, with
    the input to the function being the day filter we need for our app. It should
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, inside our Streamlit app file, we can import this function from our file
    and use it like so (I omitted the two cached functions for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Perfect! Now our app is much smaller, and the Streamlit sections are logically
    separated from the query sections of our app. We consistently use strategies like
    this on the Streamlit Data Team, and we recommend strategies like this to folks
    who develop Streamlit apps in production.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This concludes *Chapter 9*, *Connecting to Databases*. In this chapter, we learned
    a whole host of things, from connecting to Snowflake and BigQuery data in Streamlit
    to how to cache our queries and our database connections, saving us money and
    improving the user experience. In the next chapter, we will focus on improving
    job applications in Streamlit.
  prefs: []
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/sl](https://packt.link/sl)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code13440134443835796.png)'
  prefs: []
  type: TYPE_IMG
