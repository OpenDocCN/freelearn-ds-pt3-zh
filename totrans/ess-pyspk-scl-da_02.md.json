["```py\nlines = sc.textFile(\"/databricks-datasets/README.md\")\nwords = lines.flatMap(lambda s: s.split(\" \"))\nword_tuples = words.map(lambda s: (s, 1))\n```", "```py\nlines = sc.textFile(\"/databricks-datasets/README.md\")\nwords = lines.flatMap(lambda s: s.split(\" \"))\nword_tuples = words.map(lambda s: (s, 1))\nword_count = word_tuples.reduceByKey(lambda x, y:  x + y)\nword_count.take(10)\nword_count.saveAsTextFile(\"/tmp/wordcount.txt\")\n```", "```py\nwords = lines.flatMap(lambda s: s.split(\" \"))\n```", "```py\nfrom pyspark.sql.functions import split, explode\nlinesDf = spark.read.text(\"/databricks-datasets/README.md\")\nwordListDf = linesDf.select(split(\"value\", \" \").alias(\"words\"))\nwordsDf = wordListDf.select(explode(\"words\").alias(\"word\"))\nwordCountDf = wordsDf.groupBy(\"word\").count()\nwordCountDf.show()\nwordCountDf.write.csv(\"/tmp/wordcounts.csv\")\n```", "```py\nCREATE TABLE word_counts (word STRING)\nUSING csv\nOPTIONS(\"delimiter\"=\" \")\nLOCATION \"/databricks-datasets/README.md\"\n```", "```py\nSELECT word, COUNT(word) AS count\nFROM word_counts\nGROUP BY word\n```"]