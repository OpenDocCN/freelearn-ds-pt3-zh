<html><head></head><body>
		<div id="_idContainer428">
			<h1 id="_idParaDest-189"><em class="italic"><a id="_idTextAnchor188"/>Chapter 9</em>: Getting Started with Machine Learning in Python</h1>
			<p>This chapter will expose us to the vernacular of machine learning and the common tasks that machine learning can be used to solve. Afterward, we will learn how we can prepare our data for use in machine learning models. We have discussed data cleaning already, but only for human consumption—machine learning models require different <strong class="bold">preprocessing</strong> (cleaning) techniques. There are quite a few nuances here, so we will take our time with this topic and discuss how we can use <strong class="source-inline">scikit-learn</strong> to build preprocessing pipelines that streamline this procedure, since our models will only be as good as the data they are trained on.</p>
			<p>Next, we will walk through how we can use <strong class="source-inline">scikit-learn</strong> to build a model and evaluate its performance. Scikit-learn has a very user-friendly API, so once we know how to build one model, we can build any number of them. We won't be going into any of the mathematics behind the models; there are entire books on this, and the goal of this chapter is to serve as an introduction to the topic. By the end of this chapter, we will be able to identify what type of problem we are looking to solve and some algorithms that can help us, as well as how to implement them.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Overview of the machine learning landscape</li>
				<li>Performing exploratory data analysis using skills learned in previous chapters</li>
				<li>Preprocessing data for use in a machine learning model</li>
				<li>Clustering to help understand unlabeled data</li>
				<li>Learning when regression is appropriate and how to implement it with scikit-learn</li>
				<li>Understanding classification tasks and learning how to use logistic regression</li>
			</ul>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor189"/>Chapter materials</h1>
			<p>In this chapter, we will be working with three datasets. The first two come from data on wine quality that was donated to the UCI Machine Learning Data Repository (<a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a>) by P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis, which contains information on the chemical properties of various wine samples, along with a rating of the quality from a blind tasting by a panel of wine experts. These files can be found in the <strong class="source-inline">data/</strong> folder inside this chapter's folder in the GitHub repository (<a href="https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_09">https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_09</a>) as <strong class="source-inline">winequality-red.csv</strong> and <strong class="source-inline">winequality-white.csv</strong> for red and white wine, respectively.</p>
			<p>Our third dataset was collected using the Open Exoplanet Catalogue database, which can be found at <a href="https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/">https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/</a>. This database <a id="_idIndexMarker1120"/>provides data in <strong class="bold">eXtensible Markup Language</strong> (<strong class="bold">XML</strong>) format, which is similar to HTML. The <strong class="source-inline">planet_data_collection.ipynb</strong> notebook on GitHub contains the code that was used to parse this information into the CSV files we will use in this chapter; while we won't be going over this explicitly, I encourage you to take a look at it. The data files can be found in the <strong class="source-inline">data/</strong> folder, as well. We will use <strong class="source-inline">planets.csv</strong> for this chapter; however, the parsed data for the other hierarchies is provided for exercises and further exploration. These are <strong class="source-inline">binaries.csv</strong>, <strong class="source-inline">stars.csv</strong>, and <strong class="source-inline">systems.csv</strong>, which contain data on binaries (stars or binaries forming a group of two), data on a single star, and data on planetary systems, respectively.</p>
			<p>We will be using the <strong class="source-inline">red_wine.ipynb</strong> notebook to predict red wine quality, the <strong class="source-inline">wine.ipynb</strong> notebook to classify wines as red or white based on their chemical properties, and the <strong class="source-inline">planets_ml.ipynb</strong> notebook to build a regression model to predict the year length of planets and perform clustering to find similar planet groups. We will use the <strong class="source-inline">preprocessing.ipynb</strong> notebook for the section on preprocessing.</p>
			<p>Back in <a href="B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Data Analysis</em>, when we set up our environment, we installed a package from GitHub called <strong class="source-inline">ml_utils</strong>. This package contains utility functions and classes that we will use for our three chapters on machine learning. Unlike the last two chapters, we won't be discussing how to make this package; however, those interested can look through the code at https://github.com/stefmolin/ml-utils/tree/2nd_edition and follow the instructions from <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>, to install it in editable mode.</p>
			<p>The following are the reference links for the data sources:</p>
			<ul>
				<li><em class="italic">Open Exoplanet Catalogue database</em>, available at <a href="https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure">https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure</a>.</li>
				<li><em class="italic">P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.</em> Available online at <a href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">http://archive.ics.uci.edu/ml/datasets/Wine+Quality</a>.</li>
				<li><em class="italic">Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [</em><a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a><em class="italic">]. Irvine, CA: University of California, School of Information and Computer Science.</em></li>
			</ul>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor190"/>Overview of the machine learning landscape</h1>
			<p><strong class="bold">Machine learning</strong> is a subset of <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) whereby an algorithm can learn to <a id="_idIndexMarker1121"/>predict values from input data without explicitly <a id="_idIndexMarker1122"/>being taught rules. These algorithms rely on statistics to make inferences as they learn; they then use what they learn to make predictions.</p>
			<p>Applying for <a id="_idIndexMarker1123"/>a loan, using a search engine, sending a robot vacuum to clean a specific room with a voice command—machine learning can be found everywhere we look. This is because it can be used for many purposes, for example, voice recognition by AI assistants such as Alexa, Siri, or Google Assistant, mapping floor plans by exploring surroundings, determining who will default on a loan, figuring out which search results are relevant, and even painting (<a href="https://www.boredpanda.com/computer-deep-learning-algorithm-painting-masters/">https://www.boredpanda.com/computer-deep-learning-algorithm-painting-masters/</a>).</p>
			<p>Machine learning models can be made to adapt to changes in the input over time and are a huge help in making decisions without the need for a human each time. Think about applying for a loan or a credit line increase on a credit card; the bank or credit card company will rely on a machine learning algorithm to look up things from the applicant's credit score and history with them to determine whether the applicant should be approved. Most likely, they will only approve the applicant at that moment if the model predicts a strong chance he or she can be trusted with the loan or new credit limit. In the <a id="_idIndexMarker1124"/>case where the model can't be so sure, they can send it over to a human to make the final decision. This reduces the amount of applications employees have to sift through to just the borderline cases, while also providing faster answers for those non-borderline cases (the process can be nearly instantaneous).</p>
			<p>One important thing to call out here is that models that are used for tasks such as loan approvals, by law, have to be interpretable. There needs to be a way to explain to the applicant why they were rejected—sometimes, reasons beyond technology can influence and limit what approaches or data we use.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor191"/>Types of machine learning</h2>
			<p>Machine learning is typically divided into three categories: unsupervised learning, supervised <a id="_idIndexMarker1125"/>learning, and reinforcement learning. We use <strong class="bold">unsupervised learning</strong> when we don't <a id="_idIndexMarker1126"/>have labeled data telling us what our model should say for each data point. In many cases, gathering labeled data is costly or just not feasible, so unsupervised learning will be used. Note that it is more difficult to optimize the performance of these models because we don't know how <a id="_idIndexMarker1127"/>well they are performing. If we do have access to the labels, we can use <strong class="bold">supervised learning</strong>; this makes it much easier for us to evaluate our models and look to improve them since we can calculate metrics on their performance compared to the true labels.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Since unsupervised learning looks to find meaning in the data without a correct answer, it can be used to learn more about the data as a part of the analysis or before moving on to supervised learning.</p>
			<p><strong class="bold">Reinforcement learning</strong> is concerned <a id="_idIndexMarker1128"/>with reacting to feedback from the environment; this is used for things such as robots and AI in games. It is well beyond the scope of this book, but there are resources in the <em class="italic">Further reading</em> section for more information.</p>
			<p>Note that not all machine learning approaches fit neatly into the aforementioned categories. One <a id="_idIndexMarker1129"/>example is <strong class="bold">deep learning</strong>, which aims to learn data representations using methods such as <strong class="bold">neural networks</strong>. Deep learning methods are often seen <a id="_idIndexMarker1130"/>as black boxes, which has prevented their use in certain domains where interpretable models are required; however, they are used for tasks such as speech recognition and image classification. Deep learning is also beyond the scope of this book, but it is good to be aware that it is also machine learning.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Interpretable machine learning is an active area of research. Check out the resources in the <em class="italic">Further reading</em> section for more information.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor192"/>Common tasks</h2>
			<p>The most common machine learning tasks are clustering, classification, and regression. In <strong class="bold">clustering</strong>, we look to <a id="_idIndexMarker1131"/>assign data into groups, with the goal being that the groups are well-defined, meaning that members of the group are <a id="_idIndexMarker1132"/>close together and groups are separated from other groups. Clustering can be used in an unsupervised manner in an attempt to gain a better understanding of the data, or in a supervised manner to try to predict which cluster data belongs to (essentially classification). Note that clustering can be used for prediction in an unsupervised manner; however, we will need to decipher what each cluster means. Labels that are obtained from clustering can even be used as the input for a supervised learner to model <a id="_idIndexMarker1133"/>how observations are mapped to each group; this is called <strong class="bold">semi-supervised learning</strong>.</p>
			<p><strong class="bold">Classification</strong>, as we <a id="_idIndexMarker1134"/>discussed in the previous chapter, looks to assign a class label to the data, such as <em class="italic">benign</em> or <em class="italic">malicious</em>. This may sound like assigning it to a cluster, however, we aren't worried about how similar the values that are assigned to <em class="italic">benign</em> are, just marking them as <em class="italic">benign</em>. Since we are assigning to a class or category, this class <a id="_idIndexMarker1135"/>of models is used to predict a discrete label. <strong class="bold">Regression</strong>, on the other hand, is for predicting numeric values, such as housing prices or book sales; it models the strength and magnitude of the relationships between variables. Both can be performed as unsupervised or supervised learning; however, supervised models are more likely to perform better.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor193"/>Machine learning in Python</h2>
			<p>Now that we know <a id="_idIndexMarker1136"/>what machine learning is, we need to know <a id="_idIndexMarker1137"/>how we can build our own models. Python offers many packages for building machine learning models; some libraries we should be <a id="_idIndexMarker1138"/>aware of include the following:</p>
			<ul>
				<li><strong class="source-inline">scikit-learn</strong>: Easy to use (and learn), it features a consistent API for machine <a id="_idIndexMarker1139"/>learning in Python (<a href="https://scikit-learn.org/stable/index.html">https://scikit-learn.org/stable/index.html</a>)</li>
				<li><strong class="source-inline">statsmodels</strong>: A statistical <a id="_idIndexMarker1140"/>modeling library that also provides statistical tests (<a href="https://www.statsmodels.org/stable/index.html">https://www.statsmodels.org/stable/index.html</a>)</li>
				<li><strong class="source-inline">tensorflow</strong>: A machine <a id="_idIndexMarker1141"/>learning library developed by Google that features faster calculations (<a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>)</li>
				<li><strong class="source-inline">keras</strong>: A high-level <a id="_idIndexMarker1142"/>API for running deep learning from libraries such as TensorFlow (<a href="https://keras.io/">https://keras.io/</a>)</li>
				<li><strong class="source-inline">pytorch</strong>: A deep <a id="_idIndexMarker1143"/>learning library developed by Facebook (<a href="https://pytorch.org">https://pytorch.org</a>)<p class="callout-heading">Tip</p><p class="callout">Most of these libraries use NumPy and SciPy, a library built on top of NumPy for statistics, mathematics, and engineering purposes. SciPy can be used to handle linear algebra, interpolation, integration, and clustering algorithms, among other things. More information <a id="_idIndexMarker1144"/>on SciPy can be found at <a href="https://docs.scipy.org/doc/scipy/reference/tutorial/general.html">https://docs.scipy.org/doc/scipy/reference/tutorial/general.html</a>.</p></li>
			</ul>
			<p>In this book, we will be using <strong class="source-inline">scikit-learn</strong> for its user-friendly API. In <strong class="source-inline">scikit-learn</strong>, our base class is an <strong class="bold">estimator</strong> (not to be confused with a model when used in statistical terms), which is capable of learning from the data via its <strong class="source-inline">fit()</strong> method. We use <strong class="bold">transformers</strong> to prepare our data with their <strong class="source-inline">transform()</strong> method—transforming the data into something <strong class="bold">predictors</strong> (classes for supervised or unsupervised learning) can <a id="_idIndexMarker1145"/>use with their <strong class="source-inline">predict()</strong> method. The <strong class="bold">model</strong> classes are capable of calculating <a id="_idIndexMarker1146"/>how well they perform using a <strong class="source-inline">score()</strong> method. Knowing <a id="_idIndexMarker1147"/>just these four methods, we can easily build any machine learning model offered by <strong class="source-inline">scikit-learn</strong>. More information on this design pattern can be found at <a href="https://scikit-learn.org/stable/developers/develop.html">https://scikit-learn.org/stable/developers/develop.html</a>.</p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor194"/>Exploratory data analysis</h1>
			<p>As we have learned throughout this book, our first step should be to engage in some <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) to get familiar with our data. In the interest of brevity, this section <a id="_idIndexMarker1148"/>will include a subset of the EDA that's available in each of the notebooks—be sure to check out the respective notebooks for the full version.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">While we will use <strong class="source-inline">pandas</strong> code to perform our EDA, be sure to check out the <strong class="source-inline">pandas-profiling</strong> package (<a href="https://github.com/pandas-profiling/pandas-profiling">https://github.com/pandas-profiling/pandas-profiling</a>), which <a id="_idIndexMarker1149"/>can be used to quickly perform some initial EDA on the data via an interactive HTML report.</p>
			<p>Let's start with our imports, which will be the same across the notebooks we will use in this chapter:</p>
			<p class="source-code">&gt;&gt;&gt; %matplotlib inline</p>
			<p class="source-code">&gt;&gt;&gt; import matplotlib.pyplot as plt</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; import seaborn as sns</p>
			<p>We will start our EDA with the wine quality data before moving on to the planets.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor195"/>Red wine quality data</h2>
			<p>Let's read <a id="_idIndexMarker1150"/>in our red wine data and <a id="_idIndexMarker1151"/>do some EDA using techniques we have learned throughout this book:</p>
			<p class="source-code">&gt;&gt;&gt; red_wine = pd.read_csv('data/winequality-red.csv')</p>
			<p>We have data on 11 different chemical properties of red wine, along with a column indicating the quality score from the wine experts that participated in the blind taste testing. We can try to predict the quality score by looking at the chemical properties:</p>
			<div>
				<div id="_idContainer374" class="IMG---Figure">
					<img src="image/Figure_9.1_B16834.jpg" alt="Figure 9.1 – Red wine dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Red wine dataset</p>
			<p>Let's see what the distribution of the <strong class="source-inline">quality</strong> column looks like:</p>
			<p class="source-code">&gt;&gt;&gt; def plot_quality_scores(df, kind):</p>
			<p class="source-code">...     ax = df.quality.value_counts().sort_index().plot.barh(</p>
			<p class="source-code">...         title=f'{kind.title()} Wine Quality Scores',</p>
			<p class="source-code">...         figsize=(12, 3)</p>
			<p class="source-code">...     ) </p>
			<p class="source-code">...     ax.axes.invert_yaxis()</p>
			<p class="source-code">...     for bar in ax.patches:</p>
			<p class="source-code">...         ax.text(</p>
			<p class="source-code">...             bar.get_width(),</p>
			<p class="source-code">...             bar.get_y() + bar.get_height()/2,</p>
			<p class="source-code">...             f'{bar.get_width()/df.shape[0]:.1%}',</p>
			<p class="source-code">...             verticalalignment='center'</p>
			<p class="source-code">...         )</p>
			<p class="source-code">...     plt.xlabel('count of wines')</p>
			<p class="source-code">...     plt.ylabel('quality score')</p>
			<p class="source-code">...  </p>
			<p class="source-code">...     for spine in ['top', 'right']:</p>
			<p class="source-code">...         ax.spines[spine].set_visible(False)</p>
			<p class="source-code">... </p>
			<p class="source-code">...     return ax</p>
			<p class="source-code">&gt;&gt;&gt; plot_quality_scores(red_wine, 'red')</p>
			<p>The information <a id="_idIndexMarker1152"/>on the dataset says that <strong class="source-inline">quality</strong> varies from 0 (terrible) to 10 (excellent); however, we only have values in the middle of that range. An <a id="_idIndexMarker1153"/>interesting task for this dataset could be to see if we can predict high-quality red wines (a quality score of 7 or higher):</p>
			<div>
				<div id="_idContainer375" class="IMG---Figure">
					<img src="image/Figure_9.2_B16834.jpg" alt="Figure 9.2 – Distribution of red wine quality scores&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Distribution of red wine quality scores</p>
			<p>All of our <a id="_idIndexMarker1154"/>data is numeric, so we <a id="_idIndexMarker1155"/>don't have to worry about handling text values; we also don't have any missing values:</p>
			<p class="source-code">&gt;&gt;&gt; red_wine.info()</p>
			<p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p>
			<p class="source-code">RangeIndex: 1599 entries, 0 to 1598</p>
			<p class="source-code">Data columns (total 12 columns):</p>
			<p class="source-code"> #   Column                Non-Null Count  Dtype  </p>
			<p class="source-code">---  ------                --------------  -----  </p>
			<p class="source-code"> 0   fixed acidity         1599 non-null   float64</p>
			<p class="source-code"> 1   volatile acidity      1599 non-null   float64</p>
			<p class="source-code"> 2   citric acid           1599 non-null   float64</p>
			<p class="source-code"> 3   residual sugar        1599 non-null   float64</p>
			<p class="source-code"> 4   chlorides             1599 non-null   float64</p>
			<p class="source-code"> 5   free sulfur dioxide   1599 non-null   float64</p>
			<p class="source-code"> 6   total sulfur dioxide  1599 non-null   float64</p>
			<p class="source-code"> 7   density               1599 non-null   float64</p>
			<p class="source-code"> 8   pH                    1599 non-null   float64</p>
			<p class="source-code"> 9   sulphates             1599 non-null   float64</p>
			<p class="source-code"> 10  alcohol               1599 non-null   float64</p>
			<p class="source-code"> 11  quality               1599 non-null   int64  </p>
			<p class="source-code">dtypes: float64(11), int64(1)</p>
			<p class="source-code">memory usage: 150.0 KB</p>
			<p>We can <a id="_idIndexMarker1156"/>use <strong class="source-inline">describe()</strong> to get an idea of what scale each <a id="_idIndexMarker1157"/>of the columns is on:</p>
			<p class="source-code">&gt;&gt;&gt; red_wine.describe()</p>
			<p>The result indicates that we will definitely have to do some scaling if our model uses distance metrics for anything because our columns aren't all on the same range:</p>
			<div>
				<div id="_idContainer376" class="IMG---Figure">
					<img src="image/Figure_9.3_B16834.jpg" alt="Figure 9.3 – Summary statistics for the red wine dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – Summary statistics for the red wine dataset</p>
			<p>Lastly, let's use <strong class="source-inline">pd.cut()</strong> to bin our high-quality red wines (roughly 14% of the data) for later:</p>
			<p class="source-code">&gt;&gt;&gt; red_wine['high_quality'] = pd.cut(</p>
			<p class="source-code">...     red_wine.quality, bins=[0, 6, 10], labels=[0, 1]</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; red_wine.high_quality.value_counts(normalize=True)</p>
			<p class="source-code">0    0.86429</p>
			<p class="source-code">1    0.13571</p>
			<p class="source-code">Name: high_quality, dtype: float64</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We are stopping our EDA here for brevity; however, we should make sure to fully explore our data and consult domain experts before attempting any modeling. One thing to <a id="_idIndexMarker1158"/>pay particular attention to <a id="_idIndexMarker1159"/>is correlations between variables and what we are trying to predict (high-quality red wine, in this case). Variables with strong correlations may be good features to include in a model. However, note that correlation does not imply causation. We already learned a few ways to use visualizations to look for correlations: the scatter matrix we discussed in <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>, and the heatmap and pair plot from <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>. A pair plot is included in the <strong class="source-inline">red_wine.ipynb</strong> notebook.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor196"/>White and red wine chemical properties data</h2>
			<p>Now, let's look at the red and white wine data together. Since the data comes <a id="_idIndexMarker1160"/>in separate files, we need to read in both <a id="_idIndexMarker1161"/>and concatenate them into a single dataframe. The <a id="_idIndexMarker1162"/>white wine file is actually semi-colon (<strong class="source-inline">;</strong>) separated, so we must provide the <strong class="source-inline">sep</strong> argument to <strong class="source-inline">pd.read_csv()</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; red_wine = pd.read_csv('data/winequality-red.csv')</p>
			<p class="source-code">&gt;&gt;&gt; white_wine = \</p>
			<p class="source-code">...     pd.read_csv('data/winequality-white.csv', sep=';')</p>
			<p>We can <a id="_idIndexMarker1163"/>also look at the quality scores of the white wines, just as we did with the red ones, and we will find that the white wines tend to be rated higher overall. This might bring us to question whether the judges preferred white wine over red wine, thus creating a bias in their ratings. As it is, the rating system that was used seems to be pretty subjective:</p>
			<div>
				<div id="_idContainer377" class="IMG---Figure">
					<img src="image/Figure_9.4_B16834.jpg" alt="Figure 9.4 – Distribution of white wine quality scores &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Distribution of white wine quality scores </p>
			<p>Both of <a id="_idIndexMarker1164"/>these dataframes <a id="_idIndexMarker1165"/>have the same columns, so we can combine them without further work. Here, we use <strong class="source-inline">pd.concat()</strong> to stack <a id="_idIndexMarker1166"/>the white wine data on top of <a id="_idIndexMarker1167"/>the red wine data after adding a column to identify which wine type each observation belongs to:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">wine = pd.concat([</strong></p>
			<p class="source-code">...     <strong class="bold">white_wine.assign(kind='white'),</strong></p>
			<p class="source-code">...     <strong class="bold">red_wine.assign(kind='red')</strong></p>
			<p class="source-code">... <strong class="bold">])</strong></p>
			<p class="source-code">&gt;&gt;&gt; wine.sample(5, random_state=10)</p>
			<p>As we did with the red wine dataset, we can run <strong class="source-inline">info()</strong> to check whether we need to perform type conversion or whether we are missing any data; thankfully, we have no need here either. Our combined wine dataset looks like this:</p>
			<div>
				<div id="_idContainer378" class="IMG---Figure">
					<img src="image/Figure_9.5_B16834.jpg" alt="Figure 9.5 – Combined wine dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Combined wine dataset</p>
			<p>Using <strong class="source-inline">value_counts()</strong>, we can <a id="_idIndexMarker1168"/>see that we have many more white wines than red wines in the data:</p>
			<p class="source-code">&gt;&gt;&gt; wine.kind.value_counts()</p>
			<p class="source-code">white    4898</p>
			<p class="source-code">red      1599</p>
			<p class="source-code">Name: kind, dtype: int64</p>
			<p>Lastly, let's <a id="_idIndexMarker1169"/>examine box <a id="_idIndexMarker1170"/>plots for each chemical property <a id="_idIndexMarker1171"/>broken out by wine type using <strong class="source-inline">seaborn</strong>. This can help us identify <strong class="bold">features</strong> (model inputs) that will be helpful when building our model to distinguish between red and white wine:</p>
			<p class="source-code">&gt;&gt;&gt; import math</p>
			<p class="source-code">&gt;&gt;&gt; chemical_properties = [col for col in wine.columns</p>
			<p class="source-code">...                        if col not in ['quality', 'kind']]</p>
			<p class="source-code">&gt;&gt;&gt; melted = \</p>
			<p class="source-code">...     wine.drop(columns='quality').melt(id_vars=['kind'])</p>
			<p class="source-code">&gt;&gt;&gt; fig, axes = plt.subplots(</p>
			<p class="source-code">...     math.ceil(len(chemical_properties) / 4), 4, </p>
			<p class="source-code">...     figsize=(15, 10)</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; axes = axes.flatten()</p>
			<p class="source-code">&gt;&gt;&gt; for prop, ax in zip(chemical_properties, axes):</p>
			<p class="source-code">...     sns.boxplot(</p>
			<p class="source-code">...         data=melted[melted.variable.isin([prop])], </p>
			<p class="source-code">...         x='variable', y='value', hue='kind', ax=ax</p>
			<p class="source-code">...     ).set_xlabel('')</p>
			<p class="source-code">&gt;&gt;&gt; for ax in axes[len(chemical_properties):]:</p>
			<p class="source-code">...     ax.remove() # remove the extra subplots</p>
			<p class="source-code">&gt;&gt;&gt; plt.suptitle(</p>
			<p class="source-code">...     'Comparing Chemical Properties of Red and White Wines'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; plt.tight_layout()</p>
			<p>Given <a id="_idIndexMarker1172"/>the following result, we might look to use <a id="_idIndexMarker1173"/>fixed acidity, volatile acidity, total sulfur dioxide, and <a id="_idIndexMarker1174"/>sulphates <a id="_idIndexMarker1175"/>when building a model since they seem to be distributed differently for red and white wines:</p>
			<div>
				<div id="_idContainer379" class="IMG---Figure">
					<img src="image/Figure_9.6_B16834.jpg" alt="Figure 9.6 – Comparing red and white wine on a chemical level&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Comparing red and white wine on a chemical level</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Comparing <a id="_idIndexMarker1176"/>the distributions of variables across classes can help inform feature selection for our model. If <a id="_idIndexMarker1177"/>we see that <a id="_idIndexMarker1178"/>the distribution for a variable is very different between classes, that variable may be very useful to include in our model. It is essential <a id="_idIndexMarker1179"/>that we perform an in-depth exploration of our data before moving on to modeling. Be sure to use the visualizations we covered in <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>, and <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>, as they will prove invaluable for this process.</p>
			<p>We will come back to this visualization in <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>, when we examine incorrect predictions made by our model. Now, let's take a look at the other dataset we will be working with.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor197"/>Planets and exoplanets data</h2>
			<p>An <strong class="bold">exoplanet</strong> is simply <a id="_idIndexMarker1180"/>a planet that orbits a star outside of our solar <a id="_idIndexMarker1181"/>system, so from here on out we will <a id="_idIndexMarker1182"/>refer to both collectively as <strong class="bold">planets</strong>. Let's <a id="_idIndexMarker1183"/>read in our planets data now:</p>
			<p class="source-code">&gt;&gt;&gt; planets = pd.read_csv('data/planets.csv')</p>
			<p>Some interesting tasks we can do with this data would be to find clusters of similar planets based on their orbits and try to predict the orbit period (how long a year is on a planet), in Earth days:</p>
			<div>
				<div id="_idContainer380" class="IMG---Figure">
					<img src="image/Figure_9.7_B16834.jpg" alt="Figure 9.7 – Planets dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Planets dataset</p>
			<p>We can build a correlation matrix heatmap to help find the best features to use:</p>
			<p class="source-code">&gt;&gt;&gt; fig = plt.figure(figsize=(7, 7))</p>
			<p class="source-code">&gt;&gt;&gt; sns.heatmap(</p>
			<p class="source-code">...     planets.drop(columns='discoveryyear').corr(), </p>
			<p class="source-code">...     center=0, vmin=-1, vmax=1, square=True, annot=True,</p>
			<p class="source-code">...     cbar_kws={'shrink': 0.8}</p>
			<p class="source-code">... )</p>
			<p>The heatmap shows us that the semi-major axis of a planet's orbit is highly positively correlated <a id="_idIndexMarker1184"/>to the length <a id="_idIndexMarker1185"/>of its period, which makes <a id="_idIndexMarker1186"/>sense since the semi-major axis (along <a id="_idIndexMarker1187"/>with eccentricity) helps define the path that a planet travels around its star:</p>
			<div>
				<div id="_idContainer381" class="IMG---Figure">
					<img src="image/Figure_9.8_B16834.jpg" alt="Figure 9.8 – Correlations between features in the planets dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Correlations between features in the planets dataset</p>
			<p>To predict <strong class="source-inline">period</strong>, we probably want to look at <strong class="source-inline">semimajoraxis</strong>, <strong class="source-inline">mass</strong>, and <strong class="source-inline">eccentricity</strong>. The orbit eccentricity quantifies how much the orbit differs from a perfect circle:</p>
			<div>
				<div id="_idContainer382" class="IMG---Figure">
					<img src="image/Figure_9.9_B16834.jpg" alt="Figure 9.9 – Understanding eccentricity&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – Understanding eccentricity</p>
			<p>Let's <a id="_idIndexMarker1188"/>see what <a id="_idIndexMarker1189"/>shapes <a id="_idIndexMarker1190"/>the orbits we have are:</p>
			<p class="source-code">&gt;&gt;&gt; planets.eccentricity.min(), planets.eccentricity.max()</p>
			<p class="source-code">(0.0, 0.956) # circular and elliptical eccentricities</p>
			<p class="source-code">&gt;&gt;&gt; planets.eccentricity.hist()</p>
			<p class="source-code">&gt;&gt;&gt; plt.xlabel('eccentricity')</p>
			<p class="source-code">&gt;&gt;&gt; plt.ylabel('frequency')</p>
			<p class="source-code">&gt;&gt;&gt; plt.title('Orbit Eccentricities')</p>
			<p>It looks <a id="_idIndexMarker1191"/>like nearly everything is an ellipse, which we would expect since these are planets:</p>
			<div>
				<div id="_idContainer383" class="IMG---Figure">
					<img src="image/Figure_9.10_B16834.jpg" alt="Figure 9.10 – Distribution of orbit eccentricities&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.10 – Distribution of orbit eccentricities</p>
			<p>An ellipse, being an elongated circle, has two axes: <em class="italic">major</em> and <em class="italic">minor</em> for the longest and shortest <a id="_idIndexMarker1192"/>ones, respectively. The semi-major <a id="_idIndexMarker1193"/>axis is half the major axis. When compared to a circle, the <a id="_idIndexMarker1194"/>axes are analogous to the diameter, crossing the entire shape, and <a id="_idIndexMarker1195"/>the semi-axes are akin to the radius, being half the diameter. The following is how this would look in the case where the planet orbited a star that was exactly in the center of its elliptical orbit (due to gravity from other objects, in reality, the star can be anywhere inside the orbit path):</p>
			<div>
				<div id="_idContainer384" class="IMG---Figure">
					<img src="image/Figure_9.11_B16834.jpg" alt="Figure 9.11 – Understanding the semi-major axis&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.11 – Understanding the semi-major axis</p>
			<p>Now that we understand what these columns mean, let's do some more EDA. This data isn't as clean as our wine data was—it's certainly much easier to measure everything when we <a id="_idIndexMarker1196"/>can reach out and touch it. We only <a id="_idIndexMarker1197"/>have <strong class="source-inline">eccentricity</strong>, <strong class="source-inline">semimajoraxis</strong>, or <strong class="source-inline">mass</strong> data for a <a id="_idIndexMarker1198"/>fraction of the planets, despite <a id="_idIndexMarker1199"/>knowing most of the <strong class="source-inline">period</strong> values:</p>
			<p class="source-code">&gt;&gt;&gt; planets[[</p>
			<p class="source-code">...     'period', 'eccentricity', 'semimajoraxis', 'mass'</p>
			<p class="source-code">... ]].info()</p>
			<p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p>
			<p class="source-code">RangeIndex: 4094 entries, 0 to 4093</p>
			<p class="source-code">Data columns (total 4 columns):</p>
			<p class="source-code"> #   Column         Non-Null Count  Dtype  </p>
			<p class="source-code">---  ------         --------------  -----  </p>
			<p class="source-code"> 0   period         3930 non-null   float64</p>
			<p class="source-code"> 1   eccentricity   1388 non-null   float64</p>
			<p class="source-code"> 2   semimajoraxis  1704 non-null   float64</p>
			<p class="source-code"> 3   mass           1659 non-null   float64</p>
			<p class="source-code">dtypes: float64(4)</p>
			<p class="source-code">memory usage: 128.1 KB</p>
			<p>If we were to drop data where any of these columns was null, we would be left with about 30% of it:</p>
			<p class="source-code">&gt;&gt;&gt; planets[[</p>
			<p class="source-code">...     'period', 'eccentricity', 'semimajoraxis', 'mass'</p>
			<p class="source-code">... ]].dropna().shape</p>
			<p class="source-code"><strong class="bold">(1222, 4)</strong></p>
			<p>If we are simply looking for a way to predict the length of the year (when we have these values available) to learn more about their relationship, we wouldn't necessarily worry about throwing out the missing data. Imputing it here could be far worse for our model. At least <a id="_idIndexMarker1200"/>everything is properly encoded as a decimal (<strong class="source-inline">float64</strong>); however, let's <a id="_idIndexMarker1201"/>check whether we need to do some scaling (beneficial if our model is sensitive to differences in magnitude):</p>
			<p class="source-code">&gt;&gt;&gt; planets[[</p>
			<p class="source-code">...     'period', 'eccentricity', 'semimajoraxis', 'mass'</p>
			<p class="source-code">... ]].describe()</p>
			<p>This shows <a id="_idIndexMarker1202"/>us that, depending on our model, we <a id="_idIndexMarker1203"/>will definitely have to do some scaling because the values in the <strong class="source-inline">period</strong> column are much larger than the others:</p>
			<div>
				<div id="_idContainer385" class="IMG---Figure">
					<img src="image/Figure_9.12_B16834.jpg" alt="Figure 9.12 – Summary statistics for the planets dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.12 – Summary statistics for the planets dataset</p>
			<p>We could also look at some scatter plots. Note that there is a <strong class="source-inline">list</strong> column for the group the planet belongs to, such as <strong class="source-inline">Solar System</strong> or <strong class="source-inline">Controversial</strong>. We might want to see if the period (and distance from the star) influences this:</p>
			<p class="source-code">&gt;&gt;&gt; sns.scatterplot(</p>
			<p class="source-code">...     x=planets.semimajoraxis, y=planets.period, </p>
			<p class="source-code">...     hue=planets.list, alpha=0.5</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; plt.title('period vs. semimajoraxis')</p>
			<p class="source-code">&gt;&gt;&gt; plt.legend(title='') </p>
			<p>The controversial <a id="_idIndexMarker1204"/>planets appear to be <a id="_idIndexMarker1205"/>spread throughout and have larger <a id="_idIndexMarker1206"/>semi-major <a id="_idIndexMarker1207"/>axes and periods. Perhaps they are controversial because they are very far from their star:</p>
			<div>
				<div id="_idContainer386" class="IMG---Figure">
					<img src="image/Figure_9.13_B16834.jpg" alt="Figure 9.13 – Planet period versus semi-major axis&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.13 – Planet period versus semi-major axis</p>
			<p>Unfortunately, we can see that the scale of <strong class="source-inline">period</strong> is making this pretty difficult to read, so we could try a log transformation on the <em class="italic">y</em>-axis to get more separation in the denser section on the lower-left. Let's just point out the planets in our solar system this time:</p>
			<p class="source-code">&gt;&gt;&gt; fig, ax = plt.subplots(1, 1, figsize=(10, 10))</p>
			<p class="source-code">&gt;&gt;&gt; in_solar_system = (planets.list == 'Solar System')\</p>
			<p class="source-code">...     .rename('in solar system?')</p>
			<p class="source-code">&gt;&gt;&gt; sns.scatterplot(</p>
			<p class="source-code">...     x=planets.semimajoraxis, y=planets.period, </p>
			<p class="source-code">...     hue=in_solar_system, ax=ax</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; ax.set_yscale('log')</p>
			<p class="source-code">&gt;&gt;&gt; solar_system = planets[planets.list == 'Solar System']</p>
			<p class="source-code">&gt;&gt;&gt; for planet in solar_system.name:</p>
			<p class="source-code">...     data = solar_system.query(f'name == "{planet}"')</p>
			<p class="source-code">...     ax.annotate(</p>
			<p class="source-code">...         planet, </p>
			<p class="source-code">...         (data.semimajoraxis, data.period), </p>
			<p class="source-code">...         (7 + data.semimajoraxis, data.period),</p>
			<p class="source-code">...         arrowprops=dict(arrowstyle='-&gt;')</p>
			<p class="source-code">...     )</p>
			<p class="source-code">&gt;&gt;&gt; ax.set_title('log(orbital period) vs. semi-major axis')</p>
			<p>There <a id="_idIndexMarker1208"/>were certainly a lot of planets<a id="_idIndexMarker1209"/> hiding in <a id="_idIndexMarker1210"/>that lower-left corner of the plot. We <a id="_idIndexMarker1211"/>can see many planets with years shorter than Mercury's 88 Earth-day year now:</p>
			<div>
				<div id="_idContainer387" class="IMG---Figure">
					<img src="image/Figure_9.14_B16834.jpg" alt="Figure 9.14 – Our solar system compared to exoplanets&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.14 – Our solar system compared to exoplanets</p>
			<p>Now that we have a feel for the data we will be working with, let's learn how to prepare it for use in a machine learning model.</p>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor198"/>Preprocessing data</h1>
			<p>In this section, we will be working in the <strong class="source-inline">preprocessing.ipynb</strong> notebook before we return <a id="_idIndexMarker1212"/>to the notebooks we used for EDA. We will begin with our imports and read in the data:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; planets = pd.read_csv('data/planets.csv')</p>
			<p class="source-code">&gt;&gt;&gt; red_wine = pd.read_csv('data/winequality-red.csv')</p>
			<p class="source-code">&gt;&gt;&gt; wine = pd.concat([</p>
			<p class="source-code">...     pd.read_csv(</p>
			<p class="source-code">...         'data/winequality-white.csv', sep=';'</p>
			<p class="source-code">...     ).assign(kind='white'), </p>
			<p class="source-code">...     red_wine.assign(kind='red')</p>
			<p class="source-code">... ])</p>
			<p>Machine learning models follow the garbage in, garbage out principle. We have to make sure that we <strong class="bold">train</strong> our models (have them learn) on the best possible version of the data. What this means will depend on the model we choose. For instance, models that use a distance metric to calculate how similar observations are will easily be confused if our features <a id="_idIndexMarker1213"/>are on wildly different scales. Unless we are working with a <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) problem to try and understand the meaning of words, our model will have no use for—or worse, be unable to interpret—textual values. Missing or invalid data will also cause problems; we will have to decide whether to drop them or impute them. All of the adjustments we make to our <a id="_idIndexMarker1214"/>data before giving it to our model to learn from are collectively called <strong class="bold">preprocessing</strong>.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor199"/>Training and testing sets</h2>
			<p>So far, machine learning sounds pretty great, though—we can build a model that will learn how to <a id="_idIndexMarker1215"/>perform a task for us. Therefore, we should give it all the <a id="_idIndexMarker1216"/>data we have so that it learns well, right? Unfortunately, it's not that simple. If we give the model all of our data, we risk <strong class="bold">overfitting</strong> it, meaning that it won't be able to generalize well to new data points because it was fit to the sample rather than the population. On the other hand, if we don't give it enough data, it will <strong class="bold">underfit</strong> and be unable to capture the underlying information in the data.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">When a model fits the randomness in the data, it is said to fit the <strong class="bold">noise</strong> in the data.</p>
			<p>Another thing to consider is that if we use all of our data to train the model, how can we evaluate its performance? If we test it on the data we used for training, we will be overestimating how good it is because our model will always perform better on the training data. For these reasons, it's important to split our data into a <strong class="bold">training set</strong> and <strong class="bold">testing set</strong>. To do so, we could shuffle our dataframe and select the top <em class="italic">x</em>% of the rows for training and leave the rest for testing:</p>
			<p class="source-code">shuffled = \</p>
			<p class="source-code">    planets.reindex(np.random.permutation(planets.index))</p>
			<p class="source-code">train_end_index = int(np.ceil(shuffled.shape[0] * .75))</p>
			<p class="source-code">training = shuffled.iloc[:train_end_index,]</p>
			<p class="source-code">testing = shuffled.iloc[train_end_index:,]</p>
			<p>This would work, but it's a lot to write every time. Thankfully, <strong class="source-inline">scikit-learn</strong> provides us with the <strong class="source-inline">train_test_split()</strong> function in the <strong class="source-inline">model_selection</strong> module, which is a more robust, easier-to-use solution. It requires us to separate our input data (<strong class="source-inline">X</strong>) from our output data (<strong class="source-inline">y</strong>) beforehand. Here, we will pick 75% of the data to be used for the training set (<strong class="source-inline">X_train</strong>, <strong class="source-inline">y_train</strong>) and 25% for the testing set (<strong class="source-inline">X_test</strong>, <strong class="source-inline">y_test</strong>). We will set a seed (<strong class="source-inline">random_state=0</strong>) so that the split is reproducible:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.model_selection import train_test_split</strong></p>
			<p class="source-code">&gt;&gt;&gt; X = planets[['eccentricity', 'semimajoraxis', 'mass']]</p>
			<p class="source-code">&gt;&gt;&gt; y = planets.period</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">X_train, X_test, y_train, y_test = train_test_split(</strong></p>
			<p class="source-code">...     <strong class="bold">X, y, test_size=0.25, random_state=0</strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p>While there are no specific criteria for what constitutes a good size for the test set, a rule of thumb is usually between 10% and 30% of the data. However, if we don't have much data, we will <a id="_idIndexMarker1217"/>shift toward a 10% testing set to make sure that we have <a id="_idIndexMarker1218"/>enough data to learn from. Conversely, if we have a lot of data, we may move toward 30% testing, since, not only do we not want to overfit, but we want to give our model a good amount of data to prove its worth. Note that there is a big caveat with this rule of thumb: there are diminishing returns on the amount of training data we use. If we have a ton of data, we will most likely use much less than 70% of it for training because our computational costs may rise significantly for possibly minuscule improvements and an increased risk of overfitting.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">When building models that require tuning, we split the data into training, validation, and testing sets. We will introduce validation sets in <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>.</p>
			<p>Let's take a look at the dimensions of our training and testing sets now. Since we are using three features (<strong class="source-inline">eccentricity</strong>, <strong class="source-inline">semimajoraxis</strong>, and <strong class="source-inline">mass</strong>), <strong class="source-inline">X_train</strong> and <strong class="source-inline">X_test</strong> have three columns. The <strong class="source-inline">y_train</strong> and <strong class="source-inline">y_test</strong> sets will be a single column each. The number of observations in the <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> data for training will be equal, as will be the case for the testing set:</p>
			<p class="source-code">&gt;&gt;&gt; X.shape, y.shape # original data</p>
			<p class="source-code">((4094, 3), (4094,))</p>
			<p class="source-code">&gt;&gt;&gt; X_train.shape, y_train.shape # training data</p>
			<p class="source-code">((3070, 3), (3070,))</p>
			<p class="source-code">&gt;&gt;&gt; X_test.shape, y_test.shape # testing data</p>
			<p class="source-code">((1024, 3), (1024,))</p>
			<p><strong class="source-inline">X_train</strong> and <strong class="source-inline">X_test</strong> are returned to us as dataframes since that is the format we passed them in as. If we <a id="_idIndexMarker1219"/>are working with data in NumPy directly, we will <a id="_idIndexMarker1220"/>get NumPy arrays or <strong class="source-inline">ndarrays</strong> back instead. We are going to work with this data for other examples in the <em class="italic">Preprocessing data</em> section, so let's take a look at the first five rows of the <strong class="source-inline">X_train</strong> dataframe. Don't worry about the <strong class="source-inline">NaN</strong> values for now; we will discuss different ways of handling them in the <em class="italic">Imputing</em> section:</p>
			<p class="source-code">&gt;&gt;&gt; X_train.head()</p>
			<p class="source-code">      eccentricity  semimajoraxis  mass</p>
			<p class="source-code">1390           NaN            NaN   NaN</p>
			<p class="source-code">2837           NaN            NaN   NaN</p>
			<p class="source-code">3619           NaN         0.0701   NaN</p>
			<p class="source-code">1867           NaN            NaN   NaN</p>
			<p class="source-code">1869           NaN            NaN   NaN</p>
			<p>Both <strong class="source-inline">y_train</strong> and <strong class="source-inline">y_test</strong> are series since that is what we passed into the <strong class="source-inline">train_test_split()</strong> function. If we had passed in a NumPy array, that is what we would have gotten back instead. The rows in <strong class="source-inline">y_train</strong> and <strong class="source-inline">y_test</strong> must line up with the rows in <strong class="source-inline">X_train</strong> and <strong class="source-inline">X_test</strong>, respectively. Let's confirm this by looking at the first five rows of <strong class="source-inline">y_train</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; y_train.head()</p>
			<p class="source-code">1390     1.434742</p>
			<p class="source-code">2837    51.079263</p>
			<p class="source-code">3619     7.171000</p>
			<p class="source-code">1867    51.111024</p>
			<p class="source-code">1869    62.869161</p>
			<p class="source-code">Name: period, dtype: float64</p>
			<p>Indeed, everything matches up, as expected. Note that for our wine models, we need to use stratified <a id="_idIndexMarker1221"/>sampling, which can also be done with <strong class="source-inline">train_test_split()</strong> by passing <a id="_idIndexMarker1222"/>the values to stratify on in the <strong class="source-inline">stratify</strong> argument. We will see this in the <em class="italic">Classification</em> section. For now, let's move on to the rest of our preprocessing.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor200"/>Scaling and centering data</h2>
			<p>We've seen that our dataframes had columns with very different scales; if we want to use any model <a id="_idIndexMarker1223"/>that calculates a distance metric (such as k-means, which we <a id="_idIndexMarker1224"/>will discuss in this chapter, or <strong class="bold">k-nearest neighbors</strong> (<strong class="bold">k-NN</strong>), which we <a id="_idIndexMarker1225"/>will discuss briefly in <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>), we will need to scale these. As we discussed back in <a href="B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Data Analysis</em>, we have quite a few options for doing so. Scikit-learn provides options in the <strong class="source-inline">preprocessing</strong> module for standardizing (scaling by calculating Z-scores) and min-max scaling (to normalize data to be in the range [0, 1]), among others.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We should check the requirements of the model we are building to see if the data needs to be scaled.</p>
			<p>For standard scaling, we use the <strong class="source-inline">StandardScaler</strong> class. The <strong class="source-inline">fit_transform()</strong> method combines <strong class="source-inline">fit()</strong>, which figures out the mean and standard deviation needed to center and scale, and <strong class="source-inline">transform()</strong>, which applies the transformation to the data. Note that, when instantiating a <strong class="source-inline">StandardScaler</strong> object, we can choose to not subtract the mean or not divide by the standard deviation by passing <strong class="source-inline">False</strong> to <strong class="source-inline">with_mean</strong> or <strong class="source-inline">with_std</strong>, respectively. Both are <strong class="source-inline">True</strong> by default:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.preprocessing import StandardScaler</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">standardized = StandardScaler().fit_transform(X_train)</strong></p>
			<p class="source-code"># examine some of the non-NaN values</p>
			<p class="source-code">&gt;&gt;&gt; standardized[~np.isnan(standardized)][:30]</p>
			<p class="source-code">array([-5.43618156e-02,  1.43278593e+00,  1.95196592e+00,</p>
			<p class="source-code">        4.51498477e-03, -1.96265630e-01,  7.79591646e-02, </p>
			<p class="source-code">        ...,</p>
			<p class="source-code">       -2.25664815e-02,  9.91013258e-01, -7.48808523e-01,</p>
			<p class="source-code">       -4.99260165e-02, -8.59044215e-01, -5.49264158e-02])</p>
			<p>After this transformation, the data is in <strong class="bold">scientific notation</strong>. The information after the character <strong class="source-inline">e</strong> tells us <a id="_idIndexMarker1226"/>where the decimal point got moved to. For a <strong class="source-inline">+</strong> sign, we move <a id="_idIndexMarker1227"/>the decimal point to the right by the number of places indicated; we move to the left for a <strong class="source-inline">-</strong> sign. Therefore, <strong class="source-inline">1.00e+00</strong> is equivalent to <strong class="source-inline">1</strong>, <strong class="source-inline">2.89e-02</strong> is equivalent to <strong class="source-inline">0.0289</strong>, and <strong class="source-inline">2.89e+02</strong> is equivalent to <strong class="source-inline">289</strong>. The transformed planets data is mostly between -3 and 3 because everything is now a Z-score.</p>
			<p>Other scalers can be used with the same syntax. Let's use the <strong class="source-inline">MinMaxScaler</strong> class to transform the planets data into the range [0, 1]:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.preprocessing import MinMaxScaler</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">normalized = MinMaxScaler().fit_transform(X_train)</strong></p>
			<p class="source-code"># examine some of the non-NaN values</p>
			<p class="source-code">&gt;&gt;&gt; normalized[~np.isnan(normalized)][:30]</p>
			<p class="source-code">array([2.28055906e-05, 1.24474091e-01, 5.33472803e-01,</p>
			<p class="source-code">       1.71374569e-03, 1.83543340e-02, 1.77824268e-01, </p>
			<p class="source-code">       ...,</p>
			<p class="source-code">       9.35966714e-04, 9.56961137e-02, 2.09205021e-02, </p>
			<p class="source-code">       1.50201619e-04, 0.00000000e+00, 6.59028789e-06])</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Another <a id="_idIndexMarker1228"/>option is the <strong class="source-inline">RobustScaler</strong> class, which uses the median <a id="_idIndexMarker1229"/>and IQR for robust to outliers scaling. There is an example of this in the notebook. More <a id="_idIndexMarker1230"/>preprocessing classes can be found at <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing</a>.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor201"/>Encoding data</h2>
			<p>All of the scalers discussed so far address the preprocessing of our numeric data, but how can we deal <a id="_idIndexMarker1231"/>with categorical data? We need to encode the categories into integer values. There are a few options here, depending on what the categories represent. If our category is binary (such as <strong class="source-inline">0</strong>/<strong class="source-inline">1</strong>, <strong class="source-inline">True</strong>/<strong class="source-inline">False</strong>, or <strong class="source-inline">yes</strong>/<strong class="source-inline">no</strong>), then we will <strong class="bold">encode</strong> these as a single column for both options, where <strong class="source-inline">0</strong> is one option and <strong class="source-inline">1</strong> is the other. We can easily do this with the <strong class="source-inline">np.where()</strong> function. Let's encode the wine data's <strong class="source-inline">kind</strong> field as <strong class="source-inline">1</strong> for red and <strong class="source-inline">0</strong> for white:</p>
			<p class="source-code">&gt;&gt;&gt; np.where(wine.kind == 'red', 1, 0)</p>
			<p class="source-code">array([0, 0, 0, ..., 1, 1, 1])</p>
			<p>This is effectively a column that tells us whether or not the wine is red. Remember, we concatenated the red wines to the bottom of the white wines when we created our <strong class="source-inline">wine</strong> dataframe, so <strong class="source-inline">np.where()</strong> will return zeros for the top rows and ones for the bottom rows, just like we saw in the previous result.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can also use the <strong class="source-inline">LabelBinarizer</strong> class from <strong class="source-inline">scikit-learn</strong> to encode the <strong class="source-inline">kind</strong> field. Note that if our data is actually continuous, but we want to treat it as a binary categorical value, we could use the <strong class="source-inline">Binarizer</strong> class and provide a threshold or <strong class="source-inline">pd.cut()</strong>/<strong class="source-inline">pd.qcut()</strong>. There are examples of these in the notebook.</p>
			<p>If our categories <a id="_idIndexMarker1232"/>are ordered, we may want to use <strong class="bold">ordinal encoding</strong> on those columns; this will preserve the ordering of the categories. For instance, if we wanted to classify the red wines as low, medium, or high quality, we could encode this as <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, and <strong class="source-inline">2</strong>, respectively. The advantages of this are that we can use regression <a id="_idIndexMarker1233"/>techniques to predict the quality, or we can use this as a feature in the model to predict something else; this model would be able to use the fact that high is better than medium, which is better than low quality. We can achieve this with the <strong class="source-inline">LabelEncoder</strong> class. Note that the labels will be created according to alphabetical order, so the first category alphabetically will be <strong class="source-inline">0</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.preprocessing import LabelEncoder</strong></p>
			<p class="source-code">&gt;&gt;&gt; pd.Series(<strong class="bold">LabelEncoder().fit_transform</strong>(pd.cut(</p>
			<p class="source-code">...     red_wine.quality, </p>
			<p class="source-code">...     bins=[-1, 3, 6, 10], </p>
			<p class="source-code">...     labels=['0-3 (low)', '4-6 (med)', '7-10 (high)']</p>
			<p class="source-code">... ))).value_counts()</p>
			<p class="source-code">1    1372</p>
			<p class="source-code">2     217</p>
			<p class="source-code">0      10</p>
			<p class="source-code">dtype: int64</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Scikit-learn provides the <strong class="source-inline">OrdinalEncoder</strong> class, but our data is not in the correct format—it expects 2D data (such as a <strong class="source-inline">DataFrame</strong> or <strong class="source-inline">ndarray</strong> object), instead of the 1D <strong class="source-inline">Series</strong> object we are working with here. We still need to ensure that the categories are in the proper order beforehand.</p>
			<p>However, note that the ordinal encoding may create a potential data issue. In our example, if high-quality wines are now <strong class="source-inline">2</strong> and medium-quality wines are <strong class="source-inline">1</strong>, the model may interpret that <strong class="source-inline">2 * med = high</strong>. This is implicitly creating an association between the levels of quality that we may not agree with.</p>
			<p>Alternatively, a safer <a id="_idIndexMarker1234"/>approach would be to perform <strong class="bold">one-hot encoding</strong> to create <a id="_idIndexMarker1235"/>two new columns—<strong class="source-inline">is_low</strong> and <strong class="source-inline">is_med</strong>, which take only <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>. Using those two, we automatically know whether the wine quality was high (when <strong class="source-inline">is_low</strong> = <strong class="source-inline">is_med</strong> = <strong class="source-inline">0</strong>). These are called <strong class="bold">dummy variables</strong> or <strong class="bold">indicator variables</strong>; they numerically represent group membership for use in <a id="_idIndexMarker1236"/>machine learning. If the indicator or dummy has a value of <strong class="source-inline">1</strong>, that row is a member of <a id="_idIndexMarker1237"/>that group; in our example of wine quality categories, if <strong class="source-inline">is_low</strong> is <strong class="source-inline">1</strong>, then that row is a member of the low-quality group. This can be achieved with the <strong class="source-inline">pd.get_dummies()</strong> function and the <strong class="source-inline">drop_first</strong> argument, which will remove the redundant column.</p>
			<p>Let's use one-hot encoding to encode the <strong class="source-inline">list</strong> column in the planets data, since the categories have no inherent order. Before we do any transformations, let's take a look at the lists we have in the data:</p>
			<p class="source-code">&gt;&gt;&gt; planets.list.value_counts()</p>
			<p class="source-code">Confirmed planets                    3972</p>
			<p class="source-code">Controversial                          97</p>
			<p class="source-code">Retracted planet candidate             11</p>
			<p class="source-code">Solar System                            9</p>
			<p class="source-code">Kepler Objects of Interest              4</p>
			<p class="source-code">Planets in binary systems, S-type       1</p>
			<p class="source-code">Name: list, dtype: int64</p>
			<p>We can use the <strong class="source-inline">pd.get_dummies()</strong> function to create dummy variables if we want to include the planet list in our models:</p>
			<p class="source-code">&gt;&gt;&gt; pd.get_dummies(planets.list).head()</p>
			<p>This turns our <a id="_idIndexMarker1238"/>single series into the following dataframe, where the dummy variables were created in the order they appeared in the data:</p>
			<div>
				<div id="_idContainer388" class="IMG---Figure">
					<img src="image/Figure_9.15_B16834.jpg" alt="Figure 9.15 – One-hot encoding&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.15 – One-hot encoding</p>
			<p>As we discussed previously, one of these columns is redundant because the values in the remaining ones can be used to determine the value for the redundant one. Some models may <a id="_idIndexMarker1239"/>be significantly affected by the high correlation between these columns (referred to as <strong class="bold">multicollinearity</strong>), so we should remove one redundant column by passing in the <strong class="source-inline">drop_first</strong> argument:</p>
			<p class="source-code">&gt;&gt;&gt; pd.get_dummies(planets.list, <strong class="bold">drop_first=True</strong>).head()</p>
			<p>Note that the first column from the previous result has been removed, but we can still determine that all but the last row were in the <strong class="source-inline">Confirmed Planets</strong> list:</p>
			<div>
				<div id="_idContainer389" class="IMG---Figure">
					<img src="image/Figure_9.16_B16834.jpg" alt="Figure 9.16 – Dropping redundant columns after one-hot encoding&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.16 – Dropping redundant columns after one-hot encoding</p>
			<p>Note that we can obtain a similar result by using the <strong class="source-inline">LabelBinarizer</strong> class and its <strong class="source-inline">fit_transform()</strong> method on our planets list. This won't drop a redundant feature, so we once again <a id="_idIndexMarker1240"/>have the first feature belonging to the confirmed planets list, which can be seen in bold in the following result:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.preprocessing import LabelBinarizer</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">LabelBinarizer()</strong>.fit_transform(planets.list)</p>
			<p class="source-code">array([[<strong class="bold">1</strong>, 0, 0, 0, 0, 0],</p>
			<p class="source-code">       [<strong class="bold">1</strong>, 0, 0, 0, 0, 0], </p>
			<p class="source-code">       [<strong class="bold">1</strong>, 0, 0, 0, 0, 0],</p>
			<p class="source-code">       ..., </p>
			<p class="source-code">       [<strong class="bold">1</strong>, 0, 0, 0, 0, 0],</p>
			<p class="source-code">       [<strong class="bold">1</strong>, 0, 0, 0, 0, 0],</p>
			<p class="source-code">       [<strong class="bold">1</strong>, 0, 0, 0, 0, 0]])</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Scikit-learn provides the <strong class="source-inline">OneHotEncoder</strong> class, but our data is not in the correct format—it expects the data to come in a 2D array, and our series is just 1D. We will see an example of how to use this in the <em class="italic">Additional transformers</em> section.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor202"/>Imputing</h2>
			<p>We already know that we have some missing values in our planet data, so let's discuss a few of the options <strong class="source-inline">scikit-learn</strong> offers for handling them, which can be found in the <strong class="source-inline">impute</strong> module: imputing with a value (using constants or summary statistics), imputing based <a id="_idIndexMarker1241"/>on similar observations, and indicating what is missing.</p>
			<p>Back in the <em class="italic">Exploratory data analysis</em> section, we ran <strong class="source-inline">dropna()</strong> on the planets data we planned to model with. Let's say we don't want to get rid of it, and we want to try imputing it instead. The last few rows of our data have some missing values for <strong class="source-inline">semimajoraxis</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; planets[['semimajoraxis', 'mass', 'eccentricity']].tail()</p>
			<p class="source-code">      semimajoraxis    mass  eccentricity</p>
			<p class="source-code">4089        0.08150  1.9000         0.000</p>
			<p class="source-code">4090        0.04421  0.7090         0.038</p>
			<p class="source-code">4091            NaN  0.3334         0.310</p>
			<p class="source-code">4092            NaN  0.4000         0.270</p>
			<p class="source-code">4093            NaN  0.4200         0.160</p>
			<p>We can use the <strong class="source-inline">SimpleImputer</strong> class to impute with a value, which will be the mean by default:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.impute import SimpleImputer</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">SimpleImputer()</strong>.fit_transform(</p>
			<p class="source-code">...     planets[['semimajoraxis', 'mass', 'eccentricity']]</p>
			<p class="source-code">... )</p>
			<p class="source-code">array([[ 1.29      , 19.4       ,  0.231     ],</p>
			<p class="source-code">       [ 1.54      , 11.2       ,  0.08      ],</p>
			<p class="source-code">       [ 0.83      ,  4.8       ,  0.        ],</p>
			<p class="source-code">       ...,</p>
			<p class="source-code">       [ <strong class="bold">5.83796389</strong>,  0.3334    ,  0.31      ],</p>
			<p class="source-code">       [ <strong class="bold">5.83796389</strong>,  0.4       ,  0.27      ],</p>
			<p class="source-code">       [ <strong class="bold">5.83796389</strong>,  0.42      ,  0.16      ]])</p>
			<p>The mean hardly seems like a good strategy here since the planets we know about may share something in common, and surely things like what system a planet is a part of and its orbit can <a id="_idIndexMarker1242"/>be good indicators of some of the missing data points. We have the option to provide the <strong class="source-inline">strategy</strong> parameter with a method other than the mean; currently, it can be <strong class="source-inline">median</strong>, <strong class="source-inline">most_frequent</strong>, or <strong class="source-inline">constant</strong> (specify the value with <strong class="source-inline">fill_value</strong>). None of these is really appropriate for us; however, <strong class="source-inline">scikit-learn</strong> also provides the <strong class="source-inline">KNNImputer</strong> class for imputing missing values based on similar observations. By default, it uses the five nearest neighbors and runs k-NN, which we will discuss in <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>, using the features that aren't missing: </p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.impute import KNNImputer</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">KNNImputer()</strong>.fit_transform(</p>
			<p class="source-code">...     planets[['semimajoraxis', 'mass', 'eccentricity']]</p>
			<p class="source-code">... )</p>
			<p class="source-code">array([[ 1.29    , 19.4     ,  0.231   ],</p>
			<p class="source-code">       [ 1.54    , 11.2     ,  0.08    ],</p>
			<p class="source-code">       [ 0.83    ,  4.8     ,  0.      ],</p>
			<p class="source-code">       ...,</p>
			<p class="source-code">       [ <strong class="bold">0.404726</strong>,  0.3334  ,  0.31    ],</p>
			<p class="source-code">       [ <strong class="bold">0.85486</strong> ,  0.4     ,  0.27    ],</p>
			<p class="source-code">       [ <strong class="bold">0.15324</strong> ,  0.42    ,  0.16    ]])</p>
			<p>Notice that each of the bottom three rows has a unique value imputed for the semi-major axis now. This is because the mass and eccentricity were used to find similar planets from which to impute the semi-major axis. While this is certainly better than using the <strong class="source-inline">SimpleImputer</strong> class for the planets data, imputing can be dangerous. </p>
			<p>Rather than imputing the data, in some cases, we may be more interested in noting where we have <a id="_idIndexMarker1243"/>missing data and using that as a feature in our model. This can be achieved with the <strong class="source-inline">MissingIndicator</strong> class:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.impute import MissingIndicator</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">MissingIndicator()</strong>.fit_transform(</p>
			<p class="source-code">...     planets[['semimajoraxis', 'mass', 'eccentricity']]</p>
			<p class="source-code">... )</p>
			<p class="source-code">array([[False, False, False],</p>
			<p class="source-code">       [False, False, False],</p>
			<p class="source-code">       [False, False, False],</p>
			<p class="source-code">       ...,</p>
			<p class="source-code">       [ <strong class="bold">True</strong>, False, False],</p>
			<p class="source-code">       [ <strong class="bold">True</strong>, False, False],</p>
			<p class="source-code">       [ <strong class="bold">True</strong>, False, False]])</p>
			<p>As we turn our attention to the final set of preprocessors that we will discuss, notice that all of them have a <strong class="source-inline">fit_transform()</strong> method, along with <strong class="source-inline">fit()</strong> and <strong class="source-inline">transform()</strong> methods. This API design decision makes it very easy to figure out how to use new classes and is one of the reasons why <strong class="source-inline">scikit-learn</strong> is so easy to learn and use—it's very consistent.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor203"/>Additional transformers</h2>
			<p>What if, rather <a id="_idIndexMarker1244"/>than scaling our data or encoding it, we want to run a mathematical operation, such as taking the square root or the logarithm? The <strong class="source-inline">preprocessing</strong> module also has some classes for this. While there are a few that perform a specific transformation, such as the <strong class="source-inline">QuantileTransformer</strong> class, we will focus our attention on the <strong class="source-inline">FunctionTransformer</strong> class, which lets us provide an arbitrary function to use:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.preprocessing import FunctionTransformer</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">FunctionTransformer(</strong></p>
			<p class="source-code">...     <strong class="bold">np.abs, validate=True</strong></p>
			<p class="source-code">... <strong class="bold">)</strong>.fit_transform(X_train.dropna())</p>
			<p class="source-code">array([[0.51   , 4.94   , 1.45   ],</p>
			<p class="source-code">       [0.17   , 0.64   , 0.85   ],</p>
			<p class="source-code">       [0.08   , 0.03727, 1.192  ],</p>
			<p class="source-code">       ...,</p>
			<p class="source-code">       [0.295  , 4.46   , 1.8    ],</p>
			<p class="source-code">       [0.34   , 0.0652 , 0.0087 ],</p>
			<p class="source-code">       [0.3    , 1.26   , 0.5    ]])</p>
			<p>Here, we took the absolute value of every number. Take note of the <strong class="source-inline">validate=True</strong> argument; the <strong class="source-inline">FunctionTransformer</strong> class knows that <strong class="source-inline">scikit-learn</strong> models won't accept <strong class="source-inline">NaN</strong> values, infinite values, or missing ones, so it will throw an error if we get those back. For this reason, we run <strong class="source-inline">dropna()</strong> here as well.</p>
			<p>Notice that <a id="_idIndexMarker1245"/>for scaling, encoding, imputing, and transforming data, everything we passed was transformed. If we have features of different data types, we can use the <strong class="source-inline">ColumnTransformer</strong> class to map transformations to a column (or group of columns) in a single call:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.compose import ColumnTransformer</strong> </p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.impute import KNNImputer</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import (</p>
			<p class="source-code">...     MinMaxScaler, StandardScaler</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">ColumnTransformer([</strong></p>
			<p class="source-code">...     <strong class="bold">('impute', KNNImputer(), [0]),</strong></p>
			<p class="source-code">...     <strong class="bold">('standard_scale', StandardScaler(), [1]),</strong></p>
			<p class="source-code">...     <strong class="bold">('min_max', MinMaxScaler(), [2])</strong></p>
			<p class="source-code">... <strong class="bold">])</strong>.fit_transform(X_train)[10:15] </p>
			<p class="source-code">array([[ 0.17      , -0.04747176,  0.0107594 ],</p>
			<p class="source-code">       [ 0.08      , -0.05475873,  0.01508851],</p>
			<p class="source-code">       [ 0.15585591,         nan,  0.13924042],</p>
			<p class="source-code">       [ 0.15585591,         nan,         nan],</p>
			<p class="source-code">       [ 0.        , -0.05475111,  0.00478471]])</p>
			<p>There is also the <strong class="source-inline">make_column_transformer()</strong> function, which will name the transformers <a id="_idIndexMarker1246"/>for us. Let's make a <strong class="source-inline">ColumnTransformer</strong> object that will treat categorical data and numerical data differently:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.compose import make_column_transformer</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import (</p>
			<p class="source-code">...     OneHotEncoder, StandardScaler</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; categorical = [</p>
			<p class="source-code">...     col for col in planets.columns</p>
			<p class="source-code">...     if col in [</p>
			<p class="source-code">...         'list', 'name', 'description', </p>
			<p class="source-code">...         'discoverymethod', 'lastupdate'</p>
			<p class="source-code">...     ]</p>
			<p class="source-code">... ]</p>
			<p class="source-code">&gt;&gt;&gt; numeric = [</p>
			<p class="source-code">...     col for col in planets.columns</p>
			<p class="source-code">...     if col not in categorical</p>
			<p class="source-code">... ]</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">make_column_transformer(</strong></p>
			<p class="source-code">...     <strong class="bold">(StandardScaler(), numeric),</strong></p>
			<p class="source-code">...     <strong class="bold">(OneHotEncoder(sparse=False), categorical)</strong></p>
			<p class="source-code">... <strong class="bold">)</strong>.fit_transform(planets.dropna())</p>
			<p class="source-code">array([[ 3.09267587, -0.2351423 , -0.40487424, ...,  </p>
			<p class="source-code">         0.        ,  0.        ],</p>
			<p class="source-code">       [ 1.432445  , -0.24215395, -0.28360905, ...,  </p>
			<p class="source-code">         0.        ,  0.        ],</p>
			<p class="source-code">       [ 0.13665505, -0.24208849, -0.62800218, ...,  </p>
			<p class="source-code">         0.        ,  0.        ],</p>
			<p class="source-code">       ...,</p>
			<p class="source-code">       [-0.83289954, -0.76197788, -0.84918988, ...,  </p>
			<p class="source-code">         1.        ,  0.        ],</p>
			<p class="source-code">       [ 0.25813535,  0.38683239, -0.92873984, ...,  </p>
			<p class="source-code">         0.        ,  0.        ],</p>
			<p class="source-code">       [-0.26827931, -0.21657671, -0.70076129, ...,  </p>
			<p class="source-code">         0.        ,  1.        ]])</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We are passing <strong class="source-inline">sparse=False</strong> upon instantiating our <strong class="source-inline">OneHotEncoder</strong> object so that we <a id="_idIndexMarker1247"/>can see our result. In practice, we don't need to do this since <strong class="source-inline">scikit-learn</strong> models know how to handle NumPy sparse matrices.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor204"/>Building data pipelines</h2>
			<p>It sure seems like there are a lot of steps involved in preprocessing our data, and they need to be <a id="_idIndexMarker1248"/>applied in the correct order for both training and testing data—quite tedious. Thankfully, <strong class="source-inline">scikit-learn</strong> offers the ability to create pipelines to streamline the preprocessing and ensure that the training and testing sets are treated the same. This prevents issues, such as calculating the mean using all the data in order to standardize it and then splitting it into training and testing sets, which will create a model that looks like it will perform better than it actually will.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">When information <a id="_idIndexMarker1249"/>from outside the training set (such as using the full dataset to calculate the mean for standardization) is used to train the model, it is referred to as <strong class="bold">data leakage</strong>.</p>
			<p>We are learning about pipelines before we build our first models because they ensure that the models are built properly. Pipelines can contain all the preprocessing steps and the model itself. Making a pipeline is as simple as defining the steps and naming them:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.pipeline import Pipeline</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LinearRegression</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">Pipeline([</strong></p>
			<p class="source-code">...     <strong class="bold">('scale', StandardScaler()), ('lr', LinearRegression())</strong></p>
			<p class="source-code">... <strong class="bold">])</strong></p>
			<p class="source-code">Pipeline(steps=[('scale', StandardScaler()), </p>
			<p class="source-code">                ('lr', LinearRegression())])</p>
			<p>We aren't limited to using pipelines with models—they can also be used inside other <strong class="source-inline">scikit-learn</strong> objects, for example, <strong class="source-inline">ColumnTransformer</strong> objects. This makes it possible for us to first use k-NN imputing on the semi-major axis data (the column at index <strong class="source-inline">0</strong>) and then <a id="_idIndexMarker1250"/>standardize the result. We can then include this as part of a pipeline, which gives us tremendous flexibility in how we build our models:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.compose import ColumnTransformer</strong> </p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.impute import KNNImputer</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.pipeline import Pipeline</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import (</p>
			<p class="source-code">...     MinMaxScaler, StandardScaler</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">ColumnTransformer([</strong></p>
			<p class="source-code">...<strong class="bold">     ('impute', Pipeline([</strong></p>
			<p class="source-code">...<strong class="bold">         ('impute', KNNImputer()),</strong></p>
			<p class="source-code">...<strong class="bold">         ('scale', StandardScaler())</strong></p>
			<p class="source-code">...<strong class="bold">     ]), [0]),</strong></p>
			<p class="source-code">...     <strong class="bold">('standard_scale', StandardScaler(), [1]),</strong></p>
			<p class="source-code">...     <strong class="bold">('min_max', MinMaxScaler(), [2])</strong></p>
			<p class="source-code">... <strong class="bold">])</strong>.fit_transform(X_train)[10:15]</p>
			<p class="source-code">array([[ 0.13531604, -0.04747176,  0.0107594 ],</p>
			<p class="source-code">       [-0.7257111 , -0.05475873,  0.01508851],</p>
			<p class="source-code">       [ 0.        ,         nan,  0.13924042],</p>
			<p class="source-code">       [ 0.        ,         nan,         nan],</p>
			<p class="source-code">       [-1.49106856, -0.05475111,  0.00478471]])</p>
			<p>Just like with the <strong class="source-inline">ColumnTransformer</strong> class, we have a function that can make pipelines for <a id="_idIndexMarker1251"/>us without having to name the steps. Let's make another pipeline, but this time we will use the <strong class="source-inline">make_pipeline()</strong> function:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.pipeline import make_pipeline</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">make_pipeline(StandardScaler(), LinearRegression())</strong></p>
			<p class="source-code">Pipeline(steps=[('standardscaler', StandardScaler()),</p>
			<p class="source-code">                ('linearregression', LinearRegression())])</p>
			<p>Note that the steps have been automatically named the lowercase version of the class name. As we will see in the next chapter, naming the steps will make it easier to optimize model parameters by name. The consistency of the <strong class="source-inline">scikit-learn</strong> API will also allow us to use this pipeline to fit our model and make predictions using the same object, which we will see in the next section.</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor205"/>Clustering</h1>
			<p>We use clustering to divide our data points into groups of similar points. The points in each group are <a id="_idIndexMarker1252"/>more like their fellow group members than those of other groups. Clustering is commonly used for tasks such as recommendation systems (think of how Netflix recommends what to watch based on what other people who've watched similar things are watching) and market segmentation.</p>
			<p>For example, say we work at an online retailer and want to segment our website users for more targeted marketing efforts; we can gather data on time spent on the site, page visits, products viewed, products purchased, and much more. Then, we can have an unsupervised clustering algorithm find groups of users with similar behavior; if we make three groups, we can come up with labels for each group according to its behavior:</p>
			<div>
				<div id="_idContainer390" class="IMG---Figure">
					<img src="image/Figure_9.17_B16834.jpg" alt="Figure 9.17 – Clustering website users into three groups&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.17 – Clustering website users into three groups</p>
			<p>Since we can use <a id="_idIndexMarker1253"/>clustering for unsupervised learning, we will need to interpret the groups that are created and then try to derive a meaningful name for each group. If our clustering algorithm identified the three clusters in the preceding scatter plot, we may be able to make the following behavioral observations:</p>
			<ul>
				<li><strong class="bold">Frequent customers (group 0)</strong>: Purchase a lot and look at many products.</li>
				<li><strong class="bold">Occasional customers (group 1)</strong>: Have made some purchases, but less than the most frequent customers.</li>
				<li><strong class="bold">Browsers (group 2)</strong>: Visit the website, but haven't bought anything.</li>
			</ul>
			<p>Once these groups have been identified, the marketing team can focus on marketing to each of these groups differently; it's clear that the frequent customers will do more for the bottom line, but if they are already buying a lot, perhaps the marketing budget is better utilized trying to increase the purchases of the occasional customers or converting browsers into occasional customers.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Deciding on the number of groups to create can clearly influence how the groups are later interpreted, meaning that this is not a trivial decision. We should at least visualize our data and obtain some domain knowledge on it before attempting to guess the number of groups to split it into.</p>
			<p>Alternatively, clustering can be used in a supervised fashion if we know the group labels for some of the <a id="_idIndexMarker1254"/>data for training purposes. Say we collected data on login activity, like in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>, but we had some examples of what attacker activity looks like; we could gather those data points for all activity and then use a clustering algorithm to assign to the valid users group or to the attacker group. Since we have the labels, we can tweak our input variables and/or the clustering algorithm we use to best align these groups to their true group.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor206"/>k-means</h2>
			<p>The clustering <a id="_idIndexMarker1255"/>algorithms offered by <strong class="source-inline">scikit-learn</strong> can be found in the <strong class="source-inline">cluster</strong> module's documentation at <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster</a>. We will take a look at <strong class="bold">k-means</strong>, which iteratively <a id="_idIndexMarker1256"/>assigns points to the nearest group using the distance from the <strong class="bold">centroid</strong> of the group (center point), making <em class="italic">k</em> groups. Since this model uses <a id="_idIndexMarker1257"/>distance calculations, it is imperative that we understand the effect scale will have on our results beforehand; we can then decide which columns, if any, to scale.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">There are many ways to measure the distance between points in space. Often, Euclidean distance, or straight-line distance, is the default; however, another common one is Manhattan distance, which can be thought of as city-block distance.</p>
			<p>When we plotted out the period versus the semi-major axis for all the planets using a log scale for the period, we saw a nice separation of the planets along an arc. We are going to use k-means to find groups of planets with similar orbits along that arc.</p>
			<h3>Grouping planets by orbit characteristics</h3>
			<p>As we discussed in the <em class="italic">Preprocessing data</em> section, we can build a pipeline to scale and then model our data. Here, our model will be a <strong class="source-inline">KMeans</strong> object that makes eight clusters (for the number of planets in our solar system—sorry, Pluto). Since the k-means algorithm <a id="_idIndexMarker1258"/>randomly picks its starting centroids, it's possible to get different cluster results unless we specify the seed. Therefore, we also provide <strong class="source-inline">random_state=0</strong> for reproducibility:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.cluster import KMeans</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">kmeans_pipeline = Pipeline([</strong></p>
			<p class="source-code">...     <strong class="bold">('scale', StandardScaler()),</strong> </p>
			<p class="source-code">...     <strong class="bold">('kmeans', KMeans(8, random_state=0))</strong></p>
			<p class="source-code">... <strong class="bold">])</strong></p>
			<p>Once we have our pipeline, we fit it on all the data since we aren't trying to predict anything (in this case)—we just want to find similar planets:</p>
			<p class="source-code">&gt;&gt;&gt; kmeans_data = planets[['semimajoraxis', 'period']].dropna()</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">kmeans_pipeline.fit(kmeans_data)</strong></p>
			<p class="source-code">Pipeline(steps=[('scale', StandardScaler()),</p>
			<p class="source-code">                ('kmeans', KMeans(random_state=0))])</p>
			<p>Once the model is fit to our data, we can use the <strong class="source-inline">predict()</strong> method to get the cluster labels for each point (on the same data that we used previously). Let's take a look at the clusters that k-means identified:</p>
			<p class="source-code">&gt;&gt;&gt; fig, ax = plt.subplots(1, 1, figsize=(7, 7))</p>
			<p class="source-code">&gt;&gt;&gt; sns.scatterplot(</p>
			<p class="source-code">...     x=kmeans_data.semimajoraxis, </p>
			<p class="source-code">...     y=kmeans_data.period, </p>
			<p class="source-code">...     hue=<strong class="bold">kmeans_pipeline.predict(kmeans_data)</strong>,</p>
			<p class="source-code">...     ax=ax, palette='Accent'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; ax.set_yscale('log')</p>
			<p class="source-code">&gt;&gt;&gt; solar_system = planets[planets.list == 'Solar System']</p>
			<p class="source-code">&gt;&gt;&gt; for planet in solar_system.name:</p>
			<p class="source-code">...     data = solar_system.query(f'name == "{planet}"')</p>
			<p class="source-code">...     ax.annotate(</p>
			<p class="source-code">...         planet, </p>
			<p class="source-code">...         (data.semimajoraxis, data.period), </p>
			<p class="source-code">...         (7 + data.semimajoraxis, data.period),</p>
			<p class="source-code">...         arrowprops=dict(arrowstyle='-&gt;')</p>
			<p class="source-code">...     )</p>
			<p class="source-code">&gt;&gt;&gt; ax.get_legend().remove()</p>
			<p class="source-code">&gt;&gt;&gt; ax.set_title('KMeans Clusters')</p>
			<p>Mercury <a id="_idIndexMarker1259"/>and Venus landed in the same cluster, as did Earth and Mars. Jupiter, Saturn, and Uranus each belong to separate clusters, while Neptune and Pluto share a cluster:</p>
			<div>
				<div id="_idContainer391" class="IMG---Figure">
					<img src="image/Figure_9.18_B16834.jpg" alt="Figure 9.18 – Eight clusters of planets identified by k-means&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.18 – Eight clusters of planets identified by k-means</p>
			<p>We picked eight clusters arbitrarily here, since this is the number of planets in our solar system. Ideally, we would have some domain knowledge about the true groupings or need to <a id="_idIndexMarker1260"/>pick a specific number. For example, say we want to fit wedding guests at five tables so that they all get along, then our <em class="italic">k</em> is 5; if we can run three marketing campaigns on user groups, we have a <em class="italic">k</em> of 3. If we have no intuition as to the number of groups there will be in the data, a rule of thumb is to try the square root of our observations, but this can yield an unmanageable amount of clusters. Therefore, if it doesn't take too long to create many k-means models on our data, we can use the elbow point method.</p>
			<h3>The elbow point method for determining k</h3>
			<p>The <strong class="bold">elbow point method</strong> involves creating multiple models with many values of <em class="italic">k</em> and plotting <a id="_idIndexMarker1261"/>each model's <strong class="bold">inertia</strong> (<strong class="bold">within-cluster sum of squares</strong>) versus the number of clusters. We want to minimize <a id="_idIndexMarker1262"/>the sum of squared distances from points to their cluster's center while not creating too many clusters.</p>
			<p>The <strong class="source-inline">ml_utils.elbow_point</strong> module contains our <strong class="source-inline">elbow_point()</strong> function, which has been reproduced here:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">def elbow_point(data, pipeline, kmeans_step_name='kmeans',    </p>
			<p class="source-code">                k_range=range(1, 11), ax=None):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Plot the elbow point to find an appropriate k for</p>
			<p class="source-code">    k-means clustering.</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - data: The features to use</p>
			<p class="source-code">        - pipeline: The scikit-learn pipeline with `KMeans`</p>
			<p class="source-code">        - kmeans_step_name: Name of `KMeans` step in pipeline</p>
			<p class="source-code">        - k_range: The values of `k` to try</p>
			<p class="source-code">        - ax: Matplotlib `Axes` to plot on.</p>
			<p class="source-code">    Returns: </p>
			<p class="source-code">        A matplotlib `Axes` object</p>
			<p class="source-code">    """</p>
			<p class="source-code">    scores = []</p>
			<p class="source-code">    for k in k_range:</p>
			<p class="source-code">        <strong class="bold">pipeline.named_steps[kmeans_step_name].n_clusters = k</strong></p>
			<p class="source-code">        <strong class="bold">pipeline.fit(data)</strong></p>
			<p class="source-code">        # score is -1*inertia so we multiply by -1 for inertia</p>
			<p class="source-code">        <strong class="bold">scores.append(pipeline.score(data) * -1)</strong></p>
			<p class="source-code">    if not ax:</p>
			<p class="source-code">        fig, ax = plt.subplots()</p>
			<p class="source-code">    ax.plot(k_range, scores, 'bo-')</p>
			<p class="source-code">    ax.set_xlabel('k')</p>
			<p class="source-code">    ax.set_ylabel('inertias')</p>
			<p class="source-code">    ax.set_title('Elbow Point Plot')</p>
			<p class="source-code">    return ax</p>
			<p>Let's <a id="_idIndexMarker1263"/>use the elbow point method to find an appropriate value for <em class="italic">k</em>:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.elbow_point import elbow_point</p>
			<p class="source-code">&gt;&gt;&gt; ax = elbow_point(</p>
			<p class="source-code">...     kmeans_data, </p>
			<p class="source-code">...     Pipeline([</p>
			<p class="source-code">...         ('scale', StandardScaler()), </p>
			<p class="source-code">...         ('kmeans', KMeans(random_state=0))</p>
			<p class="source-code">...     ])</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; ax.annotate(</p>
			<p class="source-code">...     'possible appropriate values for k', xy=(2, 900), </p>
			<p class="source-code">...     xytext=(2.5, 1500), arrowprops=dict(arrowstyle='-&gt;')</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; ax.annotate(</p>
			<p class="source-code">...     '', xy=(3, 3480), xytext=(4.4, 1450), </p>
			<p class="source-code">...     arrowprops=dict(arrowstyle='-&gt;')</p>
			<p class="source-code">... )</p>
			<p>The point <a id="_idIndexMarker1264"/>at which we see diminishing returns is an appropriate <em class="italic">k</em>, which may be around two or three here:</p>
			<div>
				<div id="_idContainer392" class="IMG---Figure">
					<img src="image/Figure_9.19_B16834.jpg" alt="Figure 9.19 – Interpreting an elbow point plot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.19 – Interpreting an elbow point plot</p>
			<p>If we create just two clusters, we divide the planets into a group with most of the planets (orange) and a second group with only a few in the upper-right (blue), which are likely to be outliers:</p>
			<div>
				<div id="_idContainer393" class="IMG---Figure">
					<img src="image/Figure_9.20_B16834.jpg" alt="Figure 9.20 – Two clusters of planets identified by k-means&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.20 – Two clusters of planets identified by k-means</p>
			<p>Note that <a id="_idIndexMarker1265"/>while this may have been an appropriate amount of clusters, it doesn't tell us as much as the previous attempt. If we wanted to know about planets that are similar to each of the planets in our solar system, we would want to use a larger <em class="italic">k</em>.</p>
			<h3>Interpreting centroids and visualizing the cluster space</h3>
			<p>Since we standardized <a id="_idIndexMarker1266"/>our data before clustering, we can look at the <strong class="bold">centroids</strong>, or cluster <a id="_idIndexMarker1267"/>centers, to see the Z-score that the members <a id="_idIndexMarker1268"/>are closest to. A centroid's location will be the average of each of the dimensions of the points in the cluster. We can grab this with the <strong class="source-inline">cluster_centers_</strong> attribute of the model. The centroid of the blue cluster is located at (18.9, 20.9), which is in the (semi-major axis, period) format; remember, these are Z-scores, so these are quite far from the rest of the data. The orange cluster, on the other hand, is centered at (-0.035, -0.038).</p>
			<p>Let's build a visualization that shows us the location of the centroids with the scaled input data and the cluster distance space (where points are represented as the distance to their cluster's centroid). First, we will set up our layout for a smaller plot inside of a larger one:</p>
			<p class="source-code">&gt;&gt;&gt; fig = plt.figure(figsize=(8, 6))</p>
			<p class="source-code">&gt;&gt;&gt; outside = fig.add_axes([0.1, 0.1, 0.9, 0.9])</p>
			<p class="source-code">&gt;&gt;&gt; inside = fig.add_axes([0.6, 0.2, 0.35, 0.35])</p>
			<p>Next, we grab the scaled version of the input data and the distances between those data points and the <a id="_idIndexMarker1269"/>centroid of the cluster they belong to. We <a id="_idIndexMarker1270"/>can use the <strong class="source-inline">transform()</strong> and <strong class="source-inline">fit_transform()</strong> (<strong class="source-inline">fit()</strong> followed by <strong class="source-inline">transform()</strong>) methods to convert the input data into cluster distance space. We get NumPy <strong class="source-inline">ndarrays</strong> back, where each value in the outer array represents the coordinates of a point:</p>
			<p class="source-code">&gt;&gt;&gt; scaled = kmeans_pipeline_2.named_steps['scale']\ </p>
			<p class="source-code">...     .fit_transform(kmeans_data) </p>
			<p class="source-code">&gt;&gt;&gt; cluster_distances = kmeans_pipeline_2\</p>
			<p class="source-code">...     .fit_transform(kmeans_data)</p>
			<p>Since we know that each array in the outer array will have the semi-major axis as the first entry and the period as the second, we use <strong class="source-inline">[:,0]</strong> to select all the semi-major axis values and <strong class="source-inline">[:,1]</strong> to select all the period values. These will be the <em class="italic">x</em> and <em class="italic">y</em> for our scatter plot. Note that we actually don't need to call <strong class="source-inline">predict()</strong> to get the cluster labels for the data because we want the labels for the data we trained the model on; this means that we can use the <strong class="source-inline">labels_</strong> attribute of the <strong class="source-inline">KMeans</strong> object:</p>
			<p class="source-code">&gt;&gt;&gt; for ax, data, title, axes_labels in zip(</p>
			<p class="source-code">...     [outside, inside], [scaled, cluster_distances], </p>
			<p class="source-code">...     ['Visualizing Clusters', 'Cluster Distance Space'], </p>
			<p class="source-code">...     ['standardized', 'distance to centroid']</p>
			<p class="source-code">... ):</p>
			<p class="source-code">...     sns.scatterplot(</p>
			<p class="source-code">...         x=data[:,0], y=data[:,1], ax=ax, alpha=0.75, s=100,</p>
			<p class="source-code">...         hue=kmeans_pipeline_2.named_steps['kmeans'].<strong class="bold">labels_</strong></p>
			<p class="source-code">...     )</p>
			<p class="source-code">... </p>
			<p class="source-code">...     ax.get_legend().remove()</p>
			<p class="source-code">...     ax.set_title(title)</p>
			<p class="source-code">...     ax.set_xlabel(f'semimajoraxis ({axes_labels})')</p>
			<p class="source-code">...     ax.set_ylabel(f'period ({axes_labels})')</p>
			<p class="source-code">...     ax.set_ylim(-1, None)</p>
			<p>Lastly, we <a id="_idIndexMarker1271"/>annotate the location of the centroids on the <a id="_idIndexMarker1272"/>outer plot, which shows the scaled data:</p>
			<p class="source-code">&gt;&gt;&gt; cluster_centers = kmeans_pipeline_2\</p>
			<p class="source-code">...     .named_steps['kmeans'].cluster_centers_</p>
			<p class="source-code">&gt;&gt;&gt; for color, centroid in zip(</p>
			<p class="source-code">...     ['blue', 'orange'], cluster_centers</p>
			<p class="source-code">... ):</p>
			<p class="source-code">...     outside.plot(*centroid, color=color, marker='x')</p>
			<p class="source-code">...     outside.annotate(</p>
			<p class="source-code">...         f'{color} center', xy=centroid, </p>
			<p class="source-code">...         xytext=centroid + [0, 5], </p>
			<p class="source-code">...         arrowprops=dict(arrowstyle='-&gt;')</p>
			<p class="source-code">...     )</p>
			<p>In the resulting plot, we can easily see that the three blue points are quite different from the rest and that they are the only members of the second cluster:</p>
			<div>
				<div id="_idContainer394" class="IMG---Figure">
					<img src="image/Figure_9.21_B16834.jpg" alt="Figure 9.21 – Visualizing the planets in the cluster distance space&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.21 – Visualizing the planets in the cluster distance space</p>
			<p>So far, we <a id="_idIndexMarker1273"/>have been using <strong class="source-inline">transform()</strong> or combination <a id="_idIndexMarker1274"/>methods, such as <strong class="source-inline">fit_predict()</strong> or <strong class="source-inline">fit_transform()</strong>, but not all models will support these methods. We will see a slightly different workflow in the <em class="italic">Regression</em> and <em class="italic">Classification</em> sections. In general, most <strong class="source-inline">scikit-learn</strong> objects will support the following, based on what they are used for:</p>
			<div>
				<div id="_idContainer395" class="IMG---Figure">
					<img src="image/Figure_9.22_B16834.jpg" alt="Figure 9.22 – General reference for the scikit-learn model API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.22 – General reference for the scikit-learn model API</p>
			<p>Now that <a id="_idIndexMarker1275"/>we have built a few models, we are ready for <a id="_idIndexMarker1276"/>the next step: quantifying their performance. The <strong class="source-inline">metrics</strong> module in <strong class="source-inline">scikit-learn</strong> contains various metrics for evaluating model performance across clustering, regression, and classification tasks; the API lists the functions at <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics</a>. Let's discuss how to evaluate an unsupervised clustering model next.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor207"/>Evaluating clustering results</h2>
			<p>The most important criterion for evaluating our clustering results is that they are useful for what we <a id="_idIndexMarker1277"/>set out to do; we used the elbow point method to pick an appropriate value for <em class="italic">k</em>, but that wasn't as useful to us as the original model with eight clusters. That being said, when looking to quantify the performance, we need to pick metrics that match the type of learning we performed.</p>
			<p>When we know the true clusters for our data, we can check that our clustering model places the points together in a cluster as they are in the true cluster. The cluster label given by our <a id="_idIndexMarker1278"/>model can be different than the true one—all that matters is that the points in the same true cluster are also together in the predicted clusters. One such metric is the <strong class="bold">Fowlkes-Mallows Index</strong>, which we will see in the end-of-chapter exercises.</p>
			<p>With the planets data, we performed unsupervised clustering because we don't have labels for each data point, and therefore, we can't measure how well we did against those. This means that we have to use metrics that evaluate aspects of the clusters themselves, such as how far apart they are and how close the points in a cluster are together. We can compare multiple metrics to get a more well-rounded evaluation of the performance.</p>
			<p>One such <a id="_idIndexMarker1279"/>method is called the <strong class="bold">silhouette coefficient</strong>, which helps quantify cluster separation. It is calculated by subtracting the mean of the distances between every two points in a cluster (<em class="italic">a</em>) from the mean of distances between points in a <a id="_idIndexMarker1280"/>given cluster and the closest different cluster (<em class="italic">b</em>) and dividing by the maximum of the two:</p>
			<div>
				<div id="_idContainer396" class="IMG---Figure">
					<img src="image/Formula_09_001.jpg" alt=""/>
				</div>
			</div>
			<p>This metric returns values in the range [-1, 1], where -1 is the worst (clusters are wrongly assigned) and 1 is the best; values near 0 indicate overlapping clusters. The higher this number is, the better defined (more separated) the clusters are:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import silhouette_score</p>
			<p class="source-code">&gt;&gt;&gt; silhouette_score(</p>
			<p class="source-code">...     kmeans_data, kmeans_pipeline.predict(kmeans_data)</p>
			<p class="source-code">... )</p>
			<p class="source-code">0.7579771626036678</p>
			<p>Another score we could use to evaluate our clustering result is the ratio of <strong class="bold">within-cluster distances</strong> (distances between points in a cluster) to the <strong class="bold">between-cluster distances</strong> (distances between points in different clusters), called the <strong class="bold">Davies-Bouldin score</strong>. Values <a id="_idIndexMarker1281"/>closer to zero indicate better partitions between clusters:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import davies_bouldin_score</p>
			<p class="source-code">&gt;&gt;&gt; davies_bouldin_score(</p>
			<p class="source-code">...     kmeans_data, kmeans_pipeline.predict(kmeans_data)</p>
			<p class="source-code">... )</p>
			<p class="source-code">0.4632311032231894 </p>
			<p>One last <a id="_idIndexMarker1282"/>metric for unsupervised clustering that we will <a id="_idIndexMarker1283"/>discuss here is the <strong class="bold">Calinski and Harabasz score</strong>, or <strong class="bold">Variance Ratio Criterion</strong>, which is the ratio of dispersion within a cluster to dispersion <a id="_idIndexMarker1284"/>between clusters. Higher values indicate better defined (more separated) clusters:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import calinski_harabasz_score</p>
			<p class="source-code">&gt;&gt;&gt; calinski_harabasz_score(</p>
			<p class="source-code">...     kmeans_data, kmeans_pipeline.predict(kmeans_data)</p>
			<p class="source-code">... )</p>
			<p class="source-code">21207.276781867335</p>
			<p>For a complete list of clustering evaluation metrics offered by <strong class="source-inline">scikit-learn</strong> (including supervised clustering) and when to use them, check out the <em class="italic">Clustering performance evaluation</em> section of their guide at <a href="https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation">https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation</a>.</p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor208"/>Regression</h1>
			<p>With the planets dataset, we want to predict the length of the year, which is a numeric value, so we will <a id="_idIndexMarker1285"/>turn to regression. As mentioned at the beginning of this chapter, regression is a technique for modeling the strength and magnitude of the relationship between <a id="_idIndexMarker1286"/>independent variables (our <strong class="source-inline">X</strong> data)—often called <strong class="bold">regressors</strong>—and the dependent variable (our <strong class="source-inline">y</strong> data) that we want to predict.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor209"/>Linear regression</h2>
			<p>Scikit-learn provides many algorithms that can handle regression tasks, ranging from decision trees <a id="_idIndexMarker1287"/>to linear regression, spread across modules according to the various algorithm classes. However, typically, the best starting point is a linear <a id="_idIndexMarker1288"/>regression, which can be found in the <strong class="source-inline">linear_model</strong> module. In <strong class="bold">simple linear regression</strong>, we fit our data to a line of the following form:</p>
			<div>
				<div id="_idContainer397" class="IMG---Figure">
					<img src="image/Formula_09_002.jpg" alt=""/>
				</div>
			</div>
			<p>Here, epsilon (<em class="italic">ε</em>) is the error term and betas (<em class="italic">β</em>) are coefficients.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The coefficients we get from our model are those that minimize the <strong class="bold">cost function</strong>, or error between the observed values (<em class="italic">y</em>) and those predicted with the model (<em class="italic">ŷ</em>, pronounced <em class="italic">y-hat</em>). Our model gives us estimates of these coefficients, and we write them as <img src="image/Formula_09_004.png" alt=""/> (pronounced <em class="italic">beta-hat</em>).</p>
			<p>However, if we <a id="_idIndexMarker1289"/>want to model additional relationships, we need to use <strong class="bold">multiple linear regression</strong>, which contains multiple regressors:</p>
			<div>
				<div id="_idContainer399" class="IMG---Figure">
					<img src="image/Formula_09_005.jpg" alt=""/>
				</div>
			</div>
			<p>Linear regression in <strong class="source-inline">scikit-learn</strong> uses <strong class="bold">ordinary least squares</strong> (<strong class="bold">OLS</strong>), which yields the <a id="_idIndexMarker1290"/>coefficients that minimize the sum of squared errors (measured as the distance between <em class="italic">y</em> and <em class="italic">ŷ</em>). The coefficients can be found using <a id="_idIndexMarker1291"/>the closed-form solution, or estimated with optimization methods, such as <strong class="bold">gradient descent</strong>, which uses the negative gradient (direction of steepest ascent calculated with partial derivatives) to determine which coefficients <a id="_idIndexMarker1292"/>to try next (see the link in the <em class="italic">Further reading</em> section for more information). We will use gradient descent in <a href="B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Machine Learning Anomaly Detection</em>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Linear regression makes some assumptions about the data, which we must keep in mind when choosing to use this technique. It assumes that the residuals are normally distributed and homoskedastic and that there is no multicollinearity (high correlations between the regressors).</p>
			<p>Now that we have a little background on how linear regression works, let's build a model to predict the orbit period of a planet.</p>
			<h3>Predicting the length of a year on a planet</h3>
			<p>Before we <a id="_idIndexMarker1293"/>can build our model, we must isolate the columns that are used to predict (<strong class="source-inline">semimajoraxis</strong>, <strong class="source-inline">mass</strong>, and <strong class="source-inline">eccentricity</strong>) from the column that will be predicted (<strong class="source-inline">period</strong>):</p>
			<p class="source-code">&gt;&gt;&gt; data = planets[</p>
			<p class="source-code">...     ['semimajoraxis', 'period', 'mass', 'eccentricity']</p>
			<p class="source-code">... ].dropna()</p>
			<p class="source-code">&gt;&gt;&gt; X = data[['semimajoraxis', 'mass', 'eccentricity']]</p>
			<p class="source-code">&gt;&gt;&gt; y = data.period</p>
			<p>This is a supervised task. We want to be able to predict the length of a year on a planet using its semi-major axis, mass, and eccentricity of orbit, and we have the period lengths for most of the planets in the data. Let's create a 75/25 split of training to testing data so that we can assess how well this model predicts year length:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.model_selection import train_test_split</p>
			<p class="source-code">&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(</p>
			<p class="source-code">...     X, y, <strong class="bold">test_size=0.25</strong>, random_state=0</p>
			<p class="source-code">... )</p>
			<p>Once we have separated the data into the training and testing sets, we can create and fit the model:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LinearRegression</p>
			<p class="source-code">&gt;&gt;&gt; lm = LinearRegression().fit(X_train, y_train)</p>
			<p>This fitted model can be used to examine the estimated coefficients and also to predict the value of the dependent variable for a given set of independent variables. We will cover both of these use cases in the next two sections.</p>
			<h3>Interpreting the linear regression equation</h3>
			<p>The equation derived from a linear regression model gives coefficients to quantify the relationships between the variables. Care must be exercised when attempting to interpret <a id="_idIndexMarker1294"/>these coefficients if we are dealing with more than a single regressor. In the case of multicollinearity, we can't interpret them because we are unable to hold all other regressors constant to examine the effect of a single one.</p>
			<p>Thankfully, the regressors we are using for the planets data aren't correlated, as we saw from the correlation matrix heatmap we made in the <em class="italic">Exploratory data analysis</em> section (<em class="italic">Figure 9.8</em>). So, let's get the intercept and coefficients from the fitted linear model object:</p>
			<p class="source-code"># get intercept</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">lm.intercept_</strong></p>
			<p class="source-code">-622.9909910671811 </p>
			<p class="source-code"># get coefficients</p>
			<p class="source-code">&gt;&gt;&gt; [(col, coef) for col, coef in </p>
			<p class="source-code">...  zip(X_train.columns, <strong class="bold">lm.coef</strong>_)]</p>
			<p class="source-code">[('semimajoraxis', 1880.4365990440929),</p>
			<p class="source-code"> ('mass', -90.18675916509196),</p>
			<p class="source-code"> ('eccentricity', -3201.078059333091)] </p>
			<p>This yields the following equation for our linear regression model of planet year length:</p>
			<div>
				<div id="_idContainer400" class="IMG---Figure">
					<img src="image/Formula_09_006.jpg" alt=""/>
				</div>
			</div>
			<p>In order to <a id="_idIndexMarker1295"/>interpret this more completely, we need to understand the units everything is in:</p>
			<ul>
				<li><strong class="source-inline">period</strong> (length of year): Earth days</li>
				<li><strong class="source-inline">semimajoraxis</strong>: <strong class="bold">Astronomical units</strong> (<strong class="bold">AUs</strong>)</li>
				<li><strong class="source-inline">mass</strong>: Jupiter masses (planet mass divided by Jupiter's mass)</li>
				<li><strong class="source-inline">eccentricity</strong>: N/A<p class="callout-heading">Tip</p><p class="callout">An astronomical unit is the average distance between the Earth and the Sun, which is equivalent to 149,597,870,700 meters.</p></li>
			</ul>
			<p>The intercept in this particular model doesn't have any meaning: if the planet had a semi-major axis of zero, no mass, and a perfect circle eccentricity, its year would be -623 Earth days long. A planet must have a non-negative, non-zero period, semi-major axis, and mass, so this clearly makes no sense. We can, however, interpret the other coefficients. The equation says that, holding mass and eccentricity constant, adding one additional AU to the semi-major axis distance increases the year length by 1,880 Earth days. Holding the <a id="_idIndexMarker1296"/>semi-major axis and eccentricity constant, each additional Jupiter mass decreases the year length by 90.2 Earth days.</p>
			<p>Going from a perfectly circular orbit (<strong class="source-inline">eccentricity=0</strong>) to a nearly parabolic escape orbit (<strong class="source-inline">eccentricity=1</strong>) will decrease the year length by 3,201 Earth days; note that these are approximate for this term because, with a parabolic escape orbit, the planet will never return, and consequently, this equation wouldn't make sense. In fact, if we tried to use this equation for eccentricities greater than or equal to 1, we would be extrapolating because we have no such values in the training data. This is a clear example of when extrapolation doesn't work. The equation tells us that the larger the eccentricity, the shorter the year, but once we get to eccentricities of one and beyond, the planets never come back (they have reached escape orbits), so the year is infinite. </p>
			<p>All of the eccentricity values in the training data are in the range [0, 1), so we are interpolating (predicting period values using data in the ranges we trained on). This means that as long as the eccentricity of the planet we want to predict is also in the range [0, 1), we can use this model to make the prediction.</p>
			<h3>Making predictions</h3>
			<p>Now that <a id="_idIndexMarker1297"/>we have an idea of the effect each of our regressors has, let's use our model to make predictions of year length for the planets in the test set:</p>
			<p class="source-code">&gt;&gt;&gt; preds = lm.<strong class="bold">predict(X_test)</strong></p>
			<p>Let's visualize how well we did by plotting the actual and predicted values:</p>
			<p class="source-code">&gt;&gt;&gt; fig, axes = plt.subplots(1, 1, figsize=(5, 3))</p>
			<p class="source-code">&gt;&gt;&gt; axes.plot(</p>
			<p class="source-code">...     X_test.semimajoraxis, y_test, 'ob',</p>
			<p class="source-code">...     label='actuals', alpha=0.5</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; axes.plot(</p>
			<p class="source-code">...     X_test.semimajoraxis, preds, 'or', </p>
			<p class="source-code">...     label='predictions', alpha=0.5</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; axes.set(xlabel='semimajoraxis', ylabel='period')</p>
			<p class="source-code">&gt;&gt;&gt; axes.legend()</p>
			<p class="source-code">&gt;&gt;&gt; axes.set_title('Linear Regression Results')</p>
			<p>The predicted <a id="_idIndexMarker1298"/>values seem pretty close to the actual values and follow a similar pattern:</p>
			<div>
				<div id="_idContainer401" class="IMG---Figure">
					<img src="image/Figure_9.23_B16834.jpg" alt="Figure 9.23 – Predictions versus actual values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.23 – Predictions versus actual values</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Try running this regression with just the <strong class="source-inline">semimajoraxis</strong> regressor. Some reshaping of the data will be necessary, but this will show how much better this performs as we add in <strong class="source-inline">eccentricity</strong> and <strong class="source-inline">mass</strong>. In practice, we often have to build many versions of our model to find one we are happy with.</p>
			<p>We can check their correlation to see how well our model tracks the true relationship:</p>
			<p class="source-code">&gt;&gt;&gt; np.corrcoef(y_test, preds)[0][1]</p>
			<p class="source-code">0.9692104355988059</p>
			<p>Our predictions <a id="_idIndexMarker1299"/>are very strongly positively correlated with the actual values (0.97 correlation coefficient). Note that the correlation coefficient will tell us whether our model moves with the actual data; however, it will not tell us whether we are off magnitude-wise. For that, we will use the metrics discussed in the following section.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor210"/>Evaluating regression results</h2>
			<p>When looking to evaluate a regression model, we are interested in how much of the variance in <a id="_idIndexMarker1300"/>the data our model is able to capture, as well as how accurate the predictions are. We can use a combination of metrics and visuals to assess the model for each of these aspects.</p>
			<h3>Analyzing residuals</h3>
			<p>Whenever we work <a id="_idIndexMarker1301"/>with linear regression, we should visualize our <strong class="bold">residuals</strong>, or the discrepancies between the actual values and the model's predictions; as we learned in <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>, they should <a id="_idIndexMarker1302"/>be centered around zero and homoskedastic (similar variance throughout). We can use a kernel density estimate to assess whether the residuals are centered around zero and a scatter plot to see if they are homoskedastic.</p>
			<p>Let's look at the utility function in <strong class="source-inline">ml_utils.regression</strong>, which will create these subplots for checking the residuals:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">def plot_residuals(y_test, preds):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Plot residuals to evaluate regression.</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - y_test: The true values for y</p>
			<p class="source-code">        - preds: The predicted values for y</p>
			<p class="source-code">    Returns:</p>
			<p class="source-code">        Subplots of residual scatter plot and residual KDE</p>
			<p class="source-code">    """</p>
			<p class="source-code">    <strong class="bold">residuals = y_test – preds</strong></p>
			<p class="source-code">    fig, axes = plt.subplots(1, 2, figsize=(15, 3))</p>
			<p class="source-code">    axes[0].scatter(np.arange(residuals.shape[0]), residuals)</p>
			<p class="source-code">    axes[0].set(xlabel='Observation', ylabel='Residual')</p>
			<p class="source-code">    residuals.plot(kind='kde', ax=axes[1])</p>
			<p class="source-code">    axes[1].set_xlabel('Residual')</p>
			<p class="source-code">    plt.suptitle('Residuals')</p>
			<p class="source-code">    return axes</p>
			<p>Now, let's look at the residuals for this linear regression:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.regression import plot_residuals</p>
			<p class="source-code">&gt;&gt;&gt; plot_residuals(y_test, preds)</p>
			<p>It looks <a id="_idIndexMarker1303"/>like our predictions don't have a pattern (left subplot), which is good; however, they aren't quite centered around zero and the distribution has negative skew (right subplot). These negative residuals occur when the predicted year was longer than the actual year:</p>
			<div>
				<div id="_idContainer402" class="IMG---Figure">
					<img src="image/Figure_9.24_B16834.jpg" alt="Figure 9.24 – Examining the residuals&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.24 – Examining the residuals</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If we find patterns in the residuals, our data isn't linear and the chances are that visualizing the residuals could help us plan our next move. This may mean employing strategies such as polynomial regression or log transformations of the data.</p>
			<h3>Metrics</h3>
			<p>In addition <a id="_idIndexMarker1304"/>to examining the residuals, we should calculate <a id="_idIndexMarker1305"/>metrics to evaluate our regression model. Perhaps the most common is <strong class="bold">R</strong><strong class="bold">2</strong> (pronounced <em class="italic">R-squared</em>), or the <strong class="bold">coefficient of determination</strong>, which <a id="_idIndexMarker1306"/>quantifies the proportion of variance in the dependent variable that we can predict from our independent variables. It is calculated by subtracting the ratio of the sum of squared residuals to the total sum of squares from 1:</p>
			<div>
				<div id="_idContainer403" class="IMG---Figure">
					<img src="image/Formula_09_007.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">Tip</p>
			<p class="callout">Sigma (<em class="italic">Σ</em>) represents the sum. The average of the <em class="italic">y</em> values is denoted as <em class="italic">ȳ</em> (pronounced <em class="italic">y-bar</em>). The predictions are denoted with <em class="italic">ŷ</em> (pronounced <em class="italic">y-hat</em>).</p>
			<p>This value <a id="_idIndexMarker1307"/>will be in the range [0, 1], where higher values are better. Objects of the <strong class="source-inline">LinearRegression</strong> class in <strong class="source-inline">scikit-learn</strong> use R<span class="superscript">2</span> as their scoring method. Therefore, we can simply use the <strong class="source-inline">score()</strong> method to calculate it for us:</p>
			<p class="source-code">&gt;&gt;&gt; lm.<strong class="bold">score(X_test, y_test)</strong></p>
			<p class="source-code">0.9209013475842684 </p>
			<p>We can also get R<span class="superscript">2</span> from the <strong class="source-inline">metrics</strong> module:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import r2_score</p>
			<p class="source-code">&gt;&gt;&gt; r2_score(y_test, preds)</p>
			<p class="source-code">0.9209013475842684 </p>
			<p>This model has a very good R<span class="superscript">2</span>; however, keep in mind that there are many factors that affect the period, such as the stars and other planets, which exert a gravitational force on the planet in question. Despite this abstraction, our simplification does pretty well because the orbital period of a planet is determined in large part by the distance that must be traveled, which we account for by using the semi-major axis data.</p>
			<p>There is a problem with R<span class="superscript">2</span>, though; we can keep adding regressors, which would make our model more and more complex while at the same time increasing R<span class="superscript">2</span>. We need a metric that penalizes model complexity. For that, we have <strong class="bold">adjusted R</strong><span class="superscript">2</span>, which will only increase if the added regressor improves the model more than what would be expected by chance:</p>
			<div>
				<div id="_idContainer404" class="IMG---Figure">
					<img src="image/Formula_09_008.jpg" alt=""/>
				</div>
			</div>
			<p>Unfortunately, <strong class="source-inline">scikit-learn</strong> doesn't offer this metric; however, it is very easy to implement ourselves. The <strong class="source-inline">ml_utils.regression</strong> module contains a function for calculating <a id="_idIndexMarker1308"/>the adjusted R<span class="superscript">2</span> for us. Let's take a look at it:</p>
			<p class="source-code">from sklearn.metrics import r2_score</p>
			<p class="source-code">def adjusted_r2(model, X, y):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Calculate the adjusted R^2.</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - model: Estimator object with a `predict()` method</p>
			<p class="source-code">        - X: The values to use for prediction.</p>
			<p class="source-code">        - y: The true values for scoring.</p>
			<p class="source-code">    Returns: </p>
			<p class="source-code">        The adjusted R^2 score.</p>
			<p class="source-code">    """</p>
			<p class="source-code">    r2 = r2_score(y, model.predict(X))</p>
			<p class="source-code">    n_obs, n_regressors = X.shape</p>
			<p class="source-code">    <strong class="bold">adj_r2 = \</strong></p>
			<p class="source-code">        <strong class="bold">1 - (1 - r2) * (n_obs - 1)/(n_obs - n_regressors - 1)</strong></p>
			<p class="source-code">    return adj_r2</p>
			<p>Adjusted R<span class="superscript">2</span> will always be lower than R<span class="superscript">2</span>. By using the <strong class="source-inline">adjusted_r2()</strong> function, we can see that our adjusted R<span class="superscript">2</span> is slightly lower than the R<span class="superscript">2</span> value:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.regression import adjusted_r2</p>
			<p class="source-code">&gt;&gt;&gt; adjusted_r2(lm, X_test, y_test)</p>
			<p class="source-code"><strong class="bold">0.9201155993814631 </strong></p>
			<p>Unfortunately, R<span class="superscript">2</span> (and adjusted R<span class="superscript">2</span>) values don't tell us anything about our prediction error or <a id="_idIndexMarker1309"/>even whether we specified our model correctly. Think back to when we discussed Anscombe's quartet in <a href="B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Data Analysis</em>. These four different datasets have the same summary statistics. They also have the same R<span class="superscript">2</span> when fit with a linear regression line (0.67), despite some of them not indicating a linear relationship:</p>
			<div>
				<div id="_idContainer405" class="IMG---Figure">
					<img src="image/Figure_9.25_B16834.jpg" alt="Figure 9.25 – R2 can be misleading&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.25 – R<span class="superscript">2</span> can be misleading</p>
			<p>Another <a id="_idIndexMarker1310"/>metric offered by <strong class="source-inline">scikit-learn</strong> is the <strong class="bold">explained variance score</strong>, which tells us the percentage of the variance that is explained by our model. We want this as close to 1 as possible:</p>
			<div>
				<div id="_idContainer406" class="IMG---Figure">
					<img src="image/Formula_09_009.jpg" alt=""/>
				</div>
			</div>
			<p>We can see that our model explains 92% of the variance:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import explained_variance_score</p>
			<p class="source-code">&gt;&gt;&gt; explained_variance_score(y_test, preds)</p>
			<p class="source-code">0.9220144218429371 </p>
			<p>We aren't limited to looking at variance when evaluating our regression models; we can also look <a id="_idIndexMarker1311"/>at the magnitude of the errors themselves. The remaining metrics we will discuss in this section all yield errors in the same unit of measurement we are using for prediction (Earth days here), so we can understand the meaning of the size of the error.</p>
			<p><strong class="bold">Mean absolute error</strong> (<strong class="bold">MAE</strong>) tells <a id="_idIndexMarker1312"/>us the average error our model made in either direction. Values range from 0 to ∞ (infinity), with smaller values being better:</p>
			<div>
				<div id="_idContainer407" class="IMG---Figure">
					<img src="image/Formula_09_010.jpg" alt=""/>
				</div>
			</div>
			<p>By using the <strong class="source-inline">scikit-learn</strong> function, we can see that our MAE was 1,369 Earth days:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import mean_absolute_error</p>
			<p class="source-code">&gt;&gt;&gt; mean_absolute_error(y_test, preds)</p>
			<p class="source-code">1369.441817073533 </p>
			<p><strong class="bold">Root mean squared error</strong> (<strong class="bold">RMSE</strong>) allows <a id="_idIndexMarker1313"/>for further penalization of poor predictions:</p>
			<div>
				<div id="_idContainer408" class="IMG---Figure">
					<img src="image/Formula_09_011.jpg" alt=""/>
				</div>
			</div>
			<p>Scikit-learn provides a function for the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>), which is the portion of <a id="_idIndexMarker1314"/>the preceding equation inside the square root; therefore, we simply have to take the square root of the result. We would use this metric when large errors are undesirable:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import mean_squared_error</p>
			<p class="source-code">&gt;&gt;&gt; np.sqrt(mean_squared_error(y_test, preds))</p>
			<p class="source-code">3248.499961928374 </p>
			<p>An alternative <a id="_idIndexMarker1315"/>to all these mean-based measures is the <strong class="bold">median absolute error</strong>, which is the median of the residuals. This can be used in cases where <a id="_idIndexMarker1316"/>we have a few outliers in our residuals, and we want a more accurate description of the bulk of the errors. Note that this is smaller than the MAE for our data:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import median_absolute_error</p>
			<p class="source-code">&gt;&gt;&gt; median_absolute_error(y_test, preds)</p>
			<p class="source-code"><strong class="bold">759.8613358335442 </strong></p>
			<p>There is also a <strong class="source-inline">mean_squared_log_error()</strong> function, which can only be used for non-negative values. Some of the predictions are negative, which prevents us from using this. Negative predictions happen when the semi-major axis is very small (less than 1) since that is the only portion of the regression equation with a positive coefficient. If the semi-major axis isn't large enough to balance out the rest of our equation, the prediction will be negative and, thus, automatically incorrect. For a complete list <a id="_idIndexMarker1317"/>of regression metrics offered by <strong class="source-inline">scikit-learn</strong>, check out <a href="https://scikit-learn.org/stable/modules/classes.html#regression-metrics">https://scikit-learn.org/stable/modules/classes.html#regression-metrics</a>.</p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor211"/>Classification</h1>
			<p>The goal of <a id="_idIndexMarker1318"/>classification is to determine how to label data using a set of discrete labels. This probably sounds similar to supervised clustering; however, in this case, we don't care how close members of the groups are spatially. Instead, we concern ourselves with classifying them with the correct class label. Remember, in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>, when we classified the IP addresses as valid user or attacker? We didn't care how well-defined clusters of IP addresses were—we just wanted to find the attackers.</p>
			<p>Just as with regression, <strong class="source-inline">scikit-learn</strong> provides many algorithms for classification tasks. These <a id="_idIndexMarker1319"/>are spread across modules, but will usually say <em class="italic">Classifier</em> at the end for classification tasks, as opposed to <em class="italic">Regressor</em> for regression tasks. Some <a id="_idIndexMarker1320"/>common methods are logistic regression, <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>), k-NN, decision trees, and random forests; here, we will discuss logistic regression.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor212"/>Logistic regression</h2>
			<p>Logistic regression is <a id="_idIndexMarker1321"/>a way to use linear regression to solve classification tasks. However, it uses the logistic sigmoid function to return probabilities in the range [0, 1] that can be mapped to class labels:</p>
			<div>
				<div id="_idContainer409" class="IMG---Figure">
					<img src="image/Figure_9.26_B16834.jpg" alt="Figure 9.26 – The logistic sigmoid function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.26 – The logistic sigmoid function</p>
			<p>Let's use logistic regression to classify red wines as high or low quality and to classify wines as red or white based on their chemical properties. We can treat logistic regression as we did the linear regression in the previous section, using the <strong class="source-inline">linear_model</strong> module in <strong class="source-inline">scikit-learn</strong>. Just like the linear regression problem, we will be <a id="_idIndexMarker1322"/>using a supervised method, so we have to split our data into testing and training sets.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">While the examples discussed in this section are both binary classification problems (two classes), <strong class="source-inline">scikit-learn</strong> provides support for multiclass problems as well. The process of building multiclass models will be nearly identical to the binary case but may require passing an additional parameter to let the model know that there are more than two classes. You will have a chance to build a multiclass classification model in the exercises at the end of this chapter.</p>
			<h3>Predicting red wine quality</h3>
			<p>We made the <strong class="source-inline">high_quality</strong> column back at the beginning of this chapter, but remember <a id="_idIndexMarker1323"/>that there was a large imbalance in the number of red wines that were high quality. So, when we split our data, we will stratify by that column for a stratified random sample to make sure that both the training and testing sets preserve the ratio of high-quality to low-quality wines in the data (roughly 14% are high quality):</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.model_selection import train_test_split</p>
			<p class="source-code">&gt;&gt;&gt; red_y = red_wine.pop('high_quality')</p>
			<p class="source-code">&gt;&gt;&gt; red_X = red_wine.drop(columns='quality')</p>
			<p class="source-code">&gt;&gt;&gt; r_X_train, r_X_test, \</p>
			<p class="source-code">... r_y_train, r_y_test = train_test_split(</p>
			<p class="source-code">...     red_X, red_y, test_size=0.1, random_state=0,</p>
			<p class="source-code">...     <strong class="bold">stratify=red_y</strong></p>
			<p class="source-code">... )</p>
			<p>Let's make a pipeline that will first standardize all of our data and then build a logistic regression. We will provide the seed (<strong class="source-inline">random_state=0</strong>) for reproducibility and <strong class="source-inline">class_weight='balanced'</strong> to have <strong class="source-inline">scikit-learn</strong> compute the weights of the classes, since we have an imbalance:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.linear_model import LogisticRegression</strong></p>
			<p class="source-code">&gt;&gt;&gt; red_quality_lr = Pipeline([</p>
			<p class="source-code">...     ('scale', StandardScaler()), </p>
			<p class="source-code">...     ('lr', <strong class="bold">LogisticRegression(</strong></p>
			<p class="source-code">...         <strong class="bold">class_weight='balanced', random_state=0</strong></p>
			<p class="source-code">...<strong class="bold">     )</strong>)</p>
			<p class="source-code">... ])</p>
			<p>The class weights determine how much the model will be penalized for wrong predictions <a id="_idIndexMarker1324"/>for each class. By selecting balanced weights, wrong predictions on smaller classes will carry more weight, where the weight will be inversely proportional to the frequency of the class in the data. These weights are used for regularization, which we will discuss more in <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>.</p>
			<p>Once we have our pipeline, we can fit it to the data with the <strong class="source-inline">fit()</strong> method:</p>
			<p class="source-code">&gt;&gt;&gt; red_quality_lr.fit(r_X_train, r_y_train)</p>
			<p class="source-code">Pipeline(steps=[('scale', StandardScaler()),</p>
			<p class="source-code">                ('lr', LogisticRegression(</p>
			<p class="source-code">                     class_weight='balanced',</p>
			<p class="source-code">                     random_state=0))])</p>
			<p>Lastly, we can use our model fit on the training data to predict the red wine quality for the test data:</p>
			<p class="source-code">&gt;&gt;&gt; quality_preds = red_quality_lr.predict(r_X_test)</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Scikit-learn makes it easy to switch between models because we can count on them to have the same methods, such as <strong class="source-inline">score()</strong>, <strong class="source-inline">fit()</strong>, and <strong class="source-inline">predict()</strong>. In some cases, we also can use <strong class="source-inline">predict_proba()</strong> for probabilities or <strong class="source-inline">decision_function()</strong> to evaluate a point with the equation derived by the model instead of <strong class="source-inline">predict()</strong>.</p>
			<p>Before we <a id="_idIndexMarker1325"/>move on to evaluating the performance of this model, let's build another classification model using the full wine dataset.</p>
			<h3>Determining wine type by chemical properties</h3>
			<p>We want to know whether it is possible to tell red and white wine apart based solely on their <a id="_idIndexMarker1326"/>chemical properties. To test this, we will build a second logistic regression model, which will predict whether a wine is red or white. First, let's split our data into testing and training sets:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.model_selection import train_test_split </p>
			<p class="source-code">&gt;&gt;&gt; wine_y = np.where(wine.kind == 'red', 1, 0)</p>
			<p class="source-code">&gt;&gt;&gt; wine_X = wine.drop(columns=['quality', 'kind'])</p>
			<p class="source-code">&gt;&gt;&gt; w_X_train, w_X_test, \</p>
			<p class="source-code">... w_y_train, w_y_test = train_test_split(</p>
			<p class="source-code">...     wine_X, wine_y, test_size=0.25, </p>
			<p class="source-code">...     random_state=0, stratify=wine_y</p>
			<p class="source-code">... )</p>
			<p>We will once again use logistic regression in a pipeline:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; white_or_red = Pipeline([</p>
			<p class="source-code">...     ('scale', StandardScaler()), </p>
			<p class="source-code">...     ('lr', LogisticRegression(random_state=0))</p>
			<p class="source-code">... ]).fit(w_X_train, w_y_train)</p>
			<p>Finally, we will save our predictions of which kind of wine each observation in the test set was:</p>
			<p class="source-code">&gt;&gt;&gt; kind_preds = white_or_red.predict(w_X_test)</p>
			<p>Now <a id="_idIndexMarker1327"/>that we have predictions for both of our logistic regression models using their respective testing sets, we are ready to evaluate their performance.</p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor213"/>Evaluating classification results</h2>
			<p>We evaluate the performance of classification models by looking at how well each class in the <a id="_idIndexMarker1328"/>data was predicted by the model. The <strong class="bold">positive class</strong> is the <a id="_idIndexMarker1329"/>class of interest to us; all other classes are considered <strong class="bold">negative classes</strong>. In our red wine classification, the positive class is high quality, while the <a id="_idIndexMarker1330"/>negative class is low quality. Despite our problem only being a binary classification problem, the metrics that are discussed in this section extend to multiclass classification problems.</p>
			<h3>Confusion matrix</h3>
			<p>As we discussed in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>, a classification problem <a id="_idIndexMarker1331"/>can be evaluated by comparing the predicted labels to the actual labels using a <strong class="bold">confusion matrix</strong>:</p>
			<div>
				<div id="_idContainer410" class="IMG---Figure">
					<img src="image/Figure_9.27_B16834.jpg" alt="Figure 9.27 – Evaluating classification results with a confusion matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.27 – Evaluating classification results with a confusion matrix</p>
			<p>Each prediction <a id="_idIndexMarker1332"/>can be one of four outcomes, based on how it matches up to the actual value:</p>
			<ul>
				<li><strong class="bold">True Positive (TP)</strong>: Correctly predicted to be the positive class</li>
				<li><strong class="bold">False Positive (FP)</strong>: Incorrectly predicted to be the positive class</li>
				<li><strong class="bold">True Negative (TN)</strong>: Correctly predicted to not be the positive class</li>
				<li><strong class="bold">False Negative (FN)</strong>: Incorrectly predicted to not be the positive class<p class="callout-heading">Important note</p><p class="callout">False positives are <a id="_idIndexMarker1333"/>also referred to as <strong class="bold">type I errors</strong>, while false negatives <a id="_idIndexMarker1334"/>are <strong class="bold">type II errors</strong>. Given a certain classifier, an effort to reduce one will cause an increase in the other.</p></li>
			</ul>
			<p>Scikit-learn provides the <strong class="source-inline">confusion_matrix()</strong> function, which we can pair with the <strong class="source-inline">heatmap()</strong> function from <strong class="source-inline">seaborn</strong> to visualize our confusion matrix. In the <strong class="source-inline">ml_utils.classification</strong> module, the <strong class="source-inline">confusion_matrix_visual()</strong> function handles this for us:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import seaborn as sns</p>
			<p class="source-code">from sklearn.metrics import confusion_matrix</p>
			<p class="source-code">def confusion_matrix_visual(y_true, y_pred, class_labels, </p>
			<p class="source-code">                            normalize=False, flip=False, </p>
			<p class="source-code">                            ax=None, title=None, **kwargs):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Create a confusion matrix heatmap</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - y_test: The true values for y</p>
			<p class="source-code">        - preds: The predicted values for y</p>
			<p class="source-code">        - class_labels: What to label the classes.</p>
			<p class="source-code">        - normalize: Whether to plot the values as percentages.</p>
			<p class="source-code">        - flip: Whether to flip the confusion matrix. This is </p>
			<p class="source-code">          helpful to get TP in the top left corner and TN in </p>
			<p class="source-code">          the bottom right when dealing with binary </p>
			<p class="source-code">          classification with labels True and False.</p>
			<p class="source-code">        - ax: The matplotlib `Axes` object to plot on.</p>
			<p class="source-code">        - title: The title for the confusion matrix</p>
			<p class="source-code">        - kwargs: Additional keyword arguments to pass down.</p>
			<p class="source-code">    Returns: A matplotlib `Axes` object.</p>
			<p class="source-code">    """</p>
			<p class="source-code">    mat = confusion_matrix(y_true, y_pred)</p>
			<p class="source-code">    if normalize:</p>
			<p class="source-code">        fmt, mat = '.2%', mat / mat.sum()</p>
			<p class="source-code">    else:</p>
			<p class="source-code">        fmt = 'd'</p>
			<p class="source-code">    if flip:</p>
			<p class="source-code">        class_labels = class_labels[::-1]</p>
			<p class="source-code">        mat = np.flip(mat)</p>
			<p class="source-code">    axes = sns.heatmap(</p>
			<p class="source-code">        mat.T, square=True, annot=True, fmt=fmt,</p>
			<p class="source-code">        cbar=True, cmap=plt.cm.Blues, ax=ax, **kwargs</p>
			<p class="source-code">    )</p>
			<p class="source-code">    axes.set(xlabel='Actual', ylabel='Model Prediction')</p>
			<p class="source-code">    tick_marks = np.arange(len(class_labels)) + 0.5</p>
			<p class="source-code">    axes.set_xticks(tick_marks)</p>
			<p class="source-code">    axes.set_xticklabels(class_labels)</p>
			<p class="source-code">    axes.set_yticks(tick_marks)</p>
			<p class="source-code">    axes.set_yticklabels(class_labels, rotation=0)</p>
			<p class="source-code">    axes.set_title(title or 'Confusion Matrix')</p>
			<p class="source-code">    return axes</p>
			<p>Let's call our confusion matrix visualization function to see how we did for each of our classification models. First, we will look at how well the model identified high-quality red wines:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.classification import confusion_matrix_visual</p>
			<p class="source-code">&gt;&gt;&gt; confusion_matrix_visual(</p>
			<p class="source-code">...     r_y_test, quality_preds, ['low', 'high']</p>
			<p class="source-code">... )</p>
			<p>Using the <a id="_idIndexMarker1335"/>confusion matrix, we can see that the model had trouble finding the high-quality red wines consistently (bottom row):</p>
			<div>
				<div id="_idContainer411" class="IMG---Figure">
					<img src="image/Figure_9.28_B16834.jpg" alt="Figure 9.28 – Results for the red wine quality model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.28 – Results for the red wine quality model</p>
			<p>Now, let's look at how well the <strong class="source-inline">white_or_red</strong> model predicted the wine type:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.classification import confusion_matrix_visual</p>
			<p class="source-code">&gt;&gt;&gt; confusion_matrix_visual(</p>
			<p class="source-code">...     w_y_test, kind_preds, ['white', 'red']</p>
			<p class="source-code">... )</p>
			<p>It looks like <a id="_idIndexMarker1336"/>this model had a much easier time, with very few incorrect predictions:</p>
			<p class="figure-caption">`</p>
			<div>
				<div id="_idContainer412" class="IMG---Figure">
					<img src="image/Figure_9.29_B16834.jpg" alt="Figure 9.29 – Results for the white or red wine model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.29 – Results for the white or red wine model</p>
			<p>Now that we understand the composition of the confusion matrix, we can use it to calculate additional performance metrics.</p>
			<h3>Classification metrics</h3>
			<p>Using the <a id="_idIndexMarker1337"/>values in the confusion matrix, we can calculate metrics to help evaluate the performance of a classifier. The best metrics will depend on the goal for which we are building the model and whether our classes are balanced. The formulas in this section are derived from the data we get from the confusion <a id="_idIndexMarker1338"/>matrix, where <em class="italic">TP</em> is the number of true positives, <em class="italic">TN</em> is the number of true negatives, and so on.</p>
			<h4>Accuracy and error rate</h4>
			<p>When our <a id="_idIndexMarker1339"/>classes are roughly equal in size, we <a id="_idIndexMarker1340"/>can use <strong class="bold">accuracy</strong>, which will give us the percentage of correctly classified values:</p>
			<div>
				<div id="_idContainer413" class="IMG---Figure">
					<img src="image/Formula_09_012.jpg" alt=""/>
				</div>
			</div>
			<p>The <strong class="source-inline">accuracy_score()</strong> function in <strong class="source-inline">sklearn.metrics</strong> will calculate the accuracy as per the formula; however, the <strong class="source-inline">score()</strong> method of our model will also give us the accuracy (this isn't always the case, as we will see with grid search in <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>):</p>
			<p class="source-code">&gt;&gt;&gt; red_quality_lr.score(r_X_test, r_y_test)</p>
			<p class="source-code">0.775</p>
			<p>Since accuracy <a id="_idIndexMarker1341"/>is the percentage we correctly classified (our <strong class="bold">success rate</strong>), it follows that our <strong class="bold">error rate</strong> (the percentage we got wrong) can be calculated as follows:</p>
			<div>
				<div id="_idContainer414" class="IMG---Figure">
					<img src="image/Formula_09_013.jpg" alt=""/>
				</div>
			</div>
			<p>Our accuracy score tells us that we got 77.5% of the red wines correctly classified according to their quality. Conversely, the <strong class="source-inline">zero_one_loss()</strong> function gives us the percentage of values that were misclassified, which is 22.5% for the red wine quality model:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import zero_one_loss</p>
			<p class="source-code">&gt;&gt;&gt; zero_one_loss(r_y_test, quality_preds)</p>
			<p class="source-code">0.22499999999999998</p>
			<p>Note that while both of these are easy to compute and understand, they require a threshold. By default, this is 50%, but we can use any probability we wish as a cutoff when predicting <a id="_idIndexMarker1342"/>the class using the <strong class="source-inline">predict_proba()</strong> method in <strong class="source-inline">scikit-learn</strong>. In addition, accuracy and error rate can be misleading in cases of class imbalance.</p>
			<h4>Precision and recall</h4>
			<p>When we <a id="_idIndexMarker1343"/>have a <strong class="bold">class imbalance</strong>, accuracy can become an unreliable <a id="_idIndexMarker1344"/>metric for measuring our performance. For instance, if we had a 99/1 split between two classes, A and B, where the rare event, B, is our positive class, we could build a model that was 99% accurate by just saying everything belonged to class A. This problem stems from the fact that true negatives will be very large, and being in the numerator (in addition to the denominator), they will make the results look better than they are. Clearly, we shouldn't bother building a model if it doesn't do anything to identify class B; thus, we need different metrics that will discourage this behavior. For this, we use precision and recall instead of accuracy. <strong class="bold">Precision</strong> is the <a id="_idIndexMarker1345"/>ratio of true positives to everything flagged positive:</p>
			<div>
				<div id="_idContainer415" class="IMG---Figure">
					<img src="image/Formula_09_014.jpg" alt=""/>
				</div>
			</div>
			<p><strong class="bold">Recall</strong> gives <a id="_idIndexMarker1346"/>us the <strong class="bold">true positive rate</strong> (<strong class="bold">TPR</strong>), which is the ratio of true positives <a id="_idIndexMarker1347"/>to everything that was actually positive:</p>
			<div>
				<div id="_idContainer416" class="IMG---Figure">
					<img src="image/Formula_09_015.jpg" alt=""/>
				</div>
			</div>
			<p>In the case of the 99/1 split between classes A and B, the model that classifies everything as A would have a recall of 0% for the positive class, B (precision would be undefined—0/0). Precision and recall provide a better way of evaluating model performance in the face of a class imbalance. They will correctly tell us that the model has little value for our use case.</p>
			<p>Scikit-learn provides the <strong class="source-inline">classification_report()</strong> function, which will calculate precision and <a id="_idIndexMarker1348"/>recall for us. In addition to calculating these metrics per class label, it also calculates the <strong class="bold">macro</strong> average (unweighted average between classes) and the <strong class="bold">weighted</strong> average (average between classes weighted by the number of observations in each class). The <strong class="bold">support</strong> column indicates the count of observations that belong to each class using the labeled data.</p>
			<p>The classification report indicates that our model does well at finding the low-quality red wines, but not so great with the high-quality red wines:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(r_y_test, quality_preds))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           <strong class="bold">0       0.95      0.78</strong>      0.86       138</p>
			<p class="source-code">           <strong class="bold">1       0.35      0.73</strong>      0.47        22</p>
			<p class="source-code">    accuracy                           0.78       160</p>
			<p class="source-code">   macro avg       0.65      0.75      0.66       160</p>
			<p class="source-code">weighted avg       0.86      0.78      0.80       160</p>
			<p>Given that the quality scores are very subjective and not necessarily related to the chemical properties, it is no surprise that this simple model doesn't perform too well. On the other hand, chemical properties are different between red and white wines, so this information is more useful for the <strong class="source-inline">white_or_red</strong> model. As we can imagine, based on the confusion matrix for the <strong class="source-inline">white_or_red</strong> model, the metrics are good:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(w_y_test, kind_preds))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           <strong class="bold">0       0.99      1.00</strong>      0.99      1225</p>
			<p class="source-code">           <strong class="bold">1       0.99      0.98</strong>      0.98       400</p>
			<p class="source-code">    accuracy                           0.99      1625</p>
			<p class="source-code">   macro avg       0.99      0.99      0.99      1625</p>
			<p class="source-code">weighted avg       0.99      0.99      0.99      1625</p>
			<p>Just like <a id="_idIndexMarker1349"/>accuracy, both precision and recall are easy to compute and understand, but require thresholds. In addition, precision and recall each only consider half of the confusion matrix:</p>
			<div>
				<div id="_idContainer417" class="IMG---Figure">
					<img src="image/Figure_9.30_B16834.jpg" alt="Figure 9.30 – Confusion matrix coverage for precision and recall&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.30 – Confusion matrix coverage for precision and recall</p>
			<p>There is typically a trade-off between maximizing recall and maximizing precision, and we have to decide which is more important to us. This preference can be quantified using the F score.</p>
			<h4>F score</h4>
			<p>The classification <a id="_idIndexMarker1350"/>report also includes the <strong class="bold">F</strong><span class="subscript">1</span><strong class="bold"> score</strong>, which helps us <a id="_idIndexMarker1351"/>balance precision and recall using the <strong class="bold">harmonic mean</strong> of the two:</p>
			<div>
				<div id="_idContainer418" class="IMG---Figure">
					<img src="image/Formula_09_016.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">Important note</p>
			<p class="callout">The harmonic mean is the reciprocal of the arithmetic mean, and is used with rates to get a more accurate average (compared to the arithmetic mean of the rates). Both precision and recall are proportions in the range [0, 1], which we can treat as rates.</p>
			<p>The <strong class="bold">F</strong><span class="subscript">β</span><strong class="bold"> score</strong>, pronounced <em class="italic">F-beta</em>, is the more general formulation for the F score. By varying β, we can put more weight on precision (β between 0 and 1) or on recall (β greater than 1), where β is how many more times recall is valued over precision:</p>
			<div>
				<div id="_idContainer419" class="IMG---Figure">
					<img src="image/Formula_09_017.jpg" alt=""/>
				</div>
			</div>
			<p>Some commonly used values for β are as follows:</p>
			<ul>
				<li><strong class="bold">F</strong><span class="subscript">0.5</span><strong class="bold"> score</strong>: Precision twice as important as recall</li>
				<li><strong class="bold">F</strong><span class="subscript">1</span><strong class="bold"> score</strong>: Harmonic mean (equal importance)</li>
				<li><strong class="bold">F</strong><span class="subscript">2</span><strong class="bold"> score</strong>: Recall twice as important as precision</li>
			</ul>
			<p>The F score is also easy to compute and relies on thresholds. However, it doesn't consider true negatives and is hard to optimize due to the trade-offs between precision and recall. Note that when working with large class imbalances, we are typically more concerned with predicting the positive class correctly, meaning that we may be less interested in true negatives, so using a metric that ignores them isn't necessarily an issue.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Functions for precision, recall, F<span class="subscript">1</span> score, and F<span class="subscript">β</span> score can be found in the <strong class="source-inline">sklearn.metrics</strong> module.</p>
			<h4>Sensitivity and specificity</h4>
			<p>Along the lines of the precision and recall trade-off, we have another pair of metrics that can be used <a id="_idIndexMarker1352"/>to illustrate the delicate balance we strive to achieve with classification problems: sensitivity and specificity.</p>
			<p><strong class="bold">Sensitivity</strong> is the <a id="_idIndexMarker1353"/>true positive rate, or recall, which we saw previously. <strong class="bold">Specificity</strong>, however, is the <strong class="bold">true negative rate</strong>, or the proportion of true negatives <a id="_idIndexMarker1354"/>to everything that should have been classified as negative:</p>
			<div>
				<div id="_idContainer420" class="IMG---Figure">
					<img src="image/Formula_09_021.jpg" alt=""/>
				</div>
			</div>
			<p>Note that, together, specificity and sensitivity consider the full confusion matrix:</p>
			<div>
				<div id="_idContainer421" class="IMG---Figure">
					<img src="image/Figure_9.31_B16834.jpg" alt="Figure 9.31 – Confusion matrix coverage for sensitivity and specificity&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.31 – Confusion matrix coverage for sensitivity and specificity</p>
			<p>We would like to maximize both sensitivity and specificity; however, we could easily maximize <a id="_idIndexMarker1355"/>specificity by decreasing the number of times we classify something as the positive class, which would decrease sensitivity. Scikit-learn doesn't offer specificity as a metric—preferring precision and recall—however, we can easily make our own by writing a function or using the <strong class="source-inline">make_scorer()</strong> function from <strong class="source-inline">scikit-learn</strong>. We are discussing them here because they form the basis of the sensitivity-specificity plot, or ROC curve, which is the topic of the following section.</p>
			<h3>ROC curve</h3>
			<p>In addition to using metrics to evaluate classification problems, we can turn to visualizations. By plotting the true positive rate (<em class="italic">sensitivity</em>) versus the false positive rate (<em class="italic">1 - specificity</em>), we get the <strong class="bold">Receiver Operating Characteristic</strong> (<strong class="bold">ROC</strong>) <strong class="bold">curve</strong>. This <a id="_idIndexMarker1356"/>curve allows us to visualize the trade-off between the true positive rate and the false positive rate. We can identify a false positive rate that we are willing to accept and use that to find the threshold to use as a cutoff when predicting the class with probabilities using the <strong class="source-inline">predict_proba()</strong> method in <strong class="source-inline">scikit-learn</strong>. Say that we find the threshold to be 60%—we would require <strong class="source-inline">predict_proba()</strong> to return a value greater than or equal to 0.6 to predict the positive class (<strong class="source-inline">predict()</strong> uses 0.5 as the cutoff).</p>
			<p>The <strong class="source-inline">roc_curve()</strong> function from <strong class="source-inline">scikit-learn</strong> calculates the false and true positive rates at thresholds from 0 to 100% using the probabilities of an observation belonging <a id="_idIndexMarker1357"/>to a given class, as determined by the model. We can then plot this, with the goal being to maximize the <strong class="bold">area under the curve</strong> (<strong class="bold">AUC</strong>), which is in the range [0, 1]; values below 0.5 are worse than guessing and good scores are above 0.8. Note that when referring to the area under a ROC curve, the AUC <a id="_idIndexMarker1358"/>may also be written as <strong class="bold">AUROC</strong>. The AUROC summarizes the model's performance across thresholds.</p>
			<p>The following are examples of good ROC curves. The dashed line would be random guessing (no predictive value) and is used as a baseline; anything below that is considered worse than guessing. We want to be toward the top-left corner:</p>
			<div>
				<div id="_idContainer422" class="IMG---Figure">
					<img src="image/Figure_9.32_B16834.jpg" alt="Figure 9.32 – Comparing ROC curves&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.32 – Comparing ROC curves</p>
			<p>The <strong class="source-inline">ml_utils.classification</strong> module contains a function for plotting our ROC curve. Let's take a look at it:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">from sklearn.metrics import auc, roc_curve</p>
			<p class="source-code">def plot_roc(y_test, preds, ax=None):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Plot ROC curve to evaluate classification.</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - y_test: The true values for y</p>
			<p class="source-code">        - preds: The predicted values for y as probabilities</p>
			<p class="source-code">        - ax: The `Axes` object to plot on</p>
			<p class="source-code">    Returns: </p>
			<p class="source-code">        A matplotlib `Axes` object.</p>
			<p class="source-code">    """</p>
			<p class="source-code">    if not ax:</p>
			<p class="source-code">        fig, ax = plt.subplots(1, 1)</p>
			<p class="source-code">    <strong class="bold">fpr, tpr, thresholds = roc_curve(y_test, preds)</strong></p>
			<p class="source-code">    <strong class="bold">ax.plot(</strong></p>
			<p class="source-code">        <strong class="bold">[0, 1], [0, 1], color='navy', lw=2, </strong></p>
			<p class="source-code">        <strong class="bold">linestyle='--', label='baseline'</strong></p>
			<p class="source-code">    <strong class="bold">)</strong></p>
			<p class="source-code">    <strong class="bold">ax.plot(fpr, tpr, color='red', lw=2, label='model')</strong></p>
			<p class="source-code">    ax.legend(loc='lower right')</p>
			<p class="source-code">    ax.set_title('ROC curve')</p>
			<p class="source-code">    ax.set_xlabel('False Positive Rate (FPR)')</p>
			<p class="source-code">    ax.set_ylabel('True Positive Rate (TPR)')</p>
			<p class="source-code">    <strong class="bold">ax.annotate(</strong></p>
			<p class="source-code">        <strong class="bold">f'AUC: {auc(fpr, tpr):.2}', xy=(0.5, 0),</strong></p>
			<p class="source-code">        <strong class="bold">horizontalalignment='center'</strong></p>
			<p class="source-code">    <strong class="bold">)</strong></p>
			<p class="source-code">    return ax</p>
			<p>As we can imagine, our <strong class="source-inline">white_or_red</strong> model will have a very good ROC curve. Let's see what <a id="_idIndexMarker1359"/>that looks like by calling the <strong class="source-inline">plot_roc()</strong> function. Since we need to pass the probabilities of each entry belonging to the positive class, we need to use the <strong class="source-inline">predict_proba()</strong> method instead of <strong class="source-inline">predict()</strong>. This gives us the probabilities that each observation belongs to each class.</p>
			<p>Here, for every row in <strong class="source-inline">w_X_test</strong>, we have a NumPy array of <strong class="source-inline">[P(white), P(red)]</strong>. Therefore, we use slicing to select the probabilities that the wine is red for the ROC curve (<strong class="source-inline">[:,1]</strong>):</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.classification import plot_roc</p>
			<p class="source-code">&gt;&gt;&gt; plot_roc(</p>
			<p class="source-code">...     w_y_test, <strong class="bold">white_or_red.predict_proba(w_X_test)[:,1]</strong></p>
			<p class="source-code">... )</p>
			<p>Just as we expected, the ROC curve for the <strong class="source-inline">white_or_red</strong> model is very good, with an AUC of nearly 1:</p>
			<div>
				<div id="_idContainer423" class="IMG---Figure">
					<img src="image/Figure_9.33_B16834.jpg" alt="Figure 9.33 – ROC curve for the white or red wine model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.33 – ROC curve for the white or red wine model</p>
			<p>Given the other metrics we have looked at, we don't expect the red wine quality prediction <a id="_idIndexMarker1360"/>model to have a great ROC curve. Let's call our function to see what the ROC curve for the red wine quality model looks like:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.classification import plot_roc</p>
			<p class="source-code">&gt;&gt;&gt; plot_roc(</p>
			<p class="source-code">...     r_y_test, red_quality_lr.predict_proba(r_X_test)[:,1]</p>
			<p class="source-code">... )</p>
			<p>This ROC curve isn't as good as the previous one, as expected:</p>
			<div>
				<div id="_idContainer424" class="IMG---Figure">
					<img src="image/Figure_9.34_B16834.jpg" alt="Figure 9.34 – ROC curve for the red wine quality model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.34 – ROC curve for the red wine quality model</p>
			<p>Our AUROC is 0.85; however, note that the AUROC provides optimistic estimates under class <a id="_idIndexMarker1361"/>imbalance (since it considers true negatives). For this reason, we should also look at the precision-recall curve.</p>
			<h3>Precision-recall curve</h3>
			<p>When faced with a class imbalance, we use <strong class="bold">precision-recall curves</strong> instead of ROC curves. This <a id="_idIndexMarker1362"/>curve shows precision versus recall at various probability thresholds. The baseline is a horizontal line at the percentage of the data that <a id="_idIndexMarker1363"/>belongs to the positive class. We want our curve above this line, with an <strong class="bold">area under the precision-recall curve</strong> (<strong class="bold">AUPR</strong>) greater than that percentage (the higher the better).</p>
			<p>The <strong class="source-inline">ml_utils.classification</strong> module contains the <strong class="source-inline">plot_pr_curve()</strong> function for drawing precision-recall curves and providing the AUPR:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">from sklearn.metrics import (</p>
			<p class="source-code">    auc, average_precision_score, precision_recall_curve</p>
			<p class="source-code">)</p>
			<p class="source-code">def plot_pr_curve(y_test, preds, positive_class=1, ax=None):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Plot precision-recall curve to evaluate classification.</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - y_test: The true values for y</p>
			<p class="source-code">        - preds: The predicted values for y as probabilities</p>
			<p class="source-code">        - positive_class: Label for positive class in the data</p>
			<p class="source-code">        - ax: The matplotlib `Axes` object to plot on</p>
			<p class="source-code">    Returns: A matplotlib `Axes` object.</p>
			<p class="source-code">    """</p>
			<p class="source-code">    <strong class="bold">precision, recall, thresholds = \</strong></p>
			<p class="source-code">        <strong class="bold">precision_recall_curve(y_test, preds)</strong></p>
			<p class="source-code">    if not ax:</p>
			<p class="source-code">        fig, ax = plt.subplots()</p>
			<p class="source-code">    <strong class="bold">ax.axhline(</strong></p>
			<p class="source-code">        <strong class="bold">sum(y_test == positive_class) / len(y_test), </strong></p>
			<p class="source-code">        <strong class="bold">color='navy', lw=2, linestyle='--', label='baseline'</strong></p>
			<p class="source-code">    <strong class="bold">)</strong></p>
			<p class="source-code">    <strong class="bold">ax.plot(</strong></p>
			<p class="source-code"><strong class="bold">        recall, precision, color='red', lw=2, label='model'</strong></p>
			<p class="source-code"><strong class="bold">    )</strong></p>
			<p class="source-code">    ax.legend()</p>
			<p class="source-code">    ax.set_title(</p>
			<p class="source-code">        'Precision-recall curve\n'</p>
			<p class="source-code">        f"""<strong class="bold">AP: {average_precision_score(</strong></p>
			<p class="source-code">            <strong class="bold">y_test, preds, pos_label=positive_class</strong></p>
			<p class="source-code">        <strong class="bold">):.2} | """</strong></p>
			<p class="source-code">        f'<strong class="bold">AUC: {auc(recall, precision):.2}</strong>'</p>
			<p class="source-code">    )</p>
			<p class="source-code">    ax.set(xlabel='Recall', ylabel='Precision')</p>
			<p class="source-code">    ax.set_xlim(-0.05, 1.05)</p>
			<p class="source-code">    ax.set_ylim(-0.05, 1.05)</p>
			<p class="source-code">    return ax</p>
			<p>Since the <a id="_idIndexMarker1364"/>implementation of the AUC calculation in <strong class="source-inline">scikit-learn</strong> uses interpolation, it may give an optimistic result, so our function also calculates <strong class="bold">average precision</strong> (<strong class="bold">AP</strong>), which summarizes the precision-recall curve as the <a id="_idIndexMarker1365"/>weighted mean of the precision scores (<em class="italic">P</em><span class="subscript">n</span>) achieved at various thresholds. The weights are derived from the change in recall (<em class="italic">R</em><span class="subscript">n</span>) between one threshold and the next. Values are between 0 and 1, with higher values being better:</p>
			<div>
				<div id="_idContainer425" class="IMG---Figure">
					<img src="image/Formula_09_022.jpg" alt=""/>
				</div>
			</div>
			<p>Let's take a look at the precision-recall curve for the red wine quality model:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.classification import plot_pr_curve</p>
			<p class="source-code">&gt;&gt;&gt; plot_pr_curve(</p>
			<p class="source-code">...     r_y_test, red_quality_lr.predict_proba(r_X_test)[:,1]</p>
			<p class="source-code">... )</p>
			<p>This still shows <a id="_idIndexMarker1366"/>that our model is better than the baseline of random guessing; however, the performance reading we get here seems more in line with the lackluster performance we saw in the classification report. We can also see that the model loses lots of precision when going from a recall of 0.2 to 0.4. Here, the trade-off between precision and recall is evident, and we will likely choose to optimize one:</p>
			<div>
				<div id="_idContainer426" class="IMG---Figure">
					<img src="image/Figure_9.35_B16834.jpg" alt="Figure 9.35 – Precision-recall curve for the red wine quality model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.35 – Precision-recall curve for the red wine quality model</p>
			<p>Since we have a class imbalance between the high-quality and low-quality red wines (less than 14% are high quality), we must make a choice as to whether we optimize precision or recall. Our choice would depend on who we work for in the wine industry. If we are renowned for producing high-quality wine, and we are choosing which wines to provide to critics for reviews, we want to make sure we pick the best ones and would rather miss out on good ones (false negatives) than tarnish our name with low-quality ones that the model classifies as high quality (false positives). However, if we are trying to make the best profit from selling the wines, we wouldn't want to sell such a high-quality wine for the same price as a low-quality wine (false negative), so we would rather overprice some low-quality wines (false positives).</p>
			<p>Note that we <a id="_idIndexMarker1367"/>could easily have classified everything as low quality to never disappoint or as high quality to maximize our profit selling them; however, this isn't too practical. It's clear that we need to strike an acceptable balance between false positives and false negatives. To do so, we need to quantify this trade-off between the two extremes in terms of what matters to us more. Then, we can use the precision-recall curve to find a threshold that meets our precision and recall targets. In <a href="B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Machine Learning Anomaly Detection</em>, we will work through an example of this.</p>
			<p>Let's now take a look at the precision-recall curve for our white or red wine classifier:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.classification import plot_pr_curve</p>
			<p class="source-code">&gt;&gt;&gt; plot_pr_curve(</p>
			<p class="source-code">...     w_y_test, white_or_red.predict_proba(w_X_test)[:,1]</p>
			<p class="source-code">... )</p>
			<p>Note that this curve is in the upper right-hand corner. With this model, we can achieve high precision and high recall:</p>
			<div>
				<div id="_idContainer427" class="IMG---Figure">
					<img src="image/Figure_9.36_B16834.jpg" alt="Figure 9.36 – Precision-recall curve for the white or red wine model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.36 – Precision-recall curve for the white or red wine model</p>
			<p>As we saw with the red wine quality model, AUPR works very well with class imbalance. However, it can't <a id="_idIndexMarker1368"/>be compared across datasets, is expensive to compute, and is hard to optimize. Note that this was just a subset of the metrics we can use to evaluate classification problems. All the classification metrics offered by <strong class="source-inline">scikit-learn</strong> can be found at <a href="https://scikit-learn.org/stable/modules/classes.html#classification-metrics">https://scikit-learn.org/stable/modules/classes.html#classification-metrics</a>.</p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor214"/>Summary</h1>
			<p>This chapter served as an introduction to machine learning in Python. We discussed the terminology that's commonly used to describe learning types and tasks. Then, we practiced EDA using the skills we learned throughout this book to get a feel for the wine and planet datasets. This gave us some ideas about what kinds of models we would want to build. A thorough exploration of the data is essential before attempting to build a model.</p>
			<p>Next, we learned how to prepare our data for use in machine learning models and the importance of splitting the data into training and testing sets before modeling. In order to prepare our data efficiently, we used pipelines in <strong class="source-inline">scikit-learn</strong> to package up everything from our preprocessing through our model.</p>
			<p>We used unsupervised k-means to cluster the planets using their semi-major axis and period; we also discussed how to use the elbow point method to find a good value for <em class="italic">k</em>. Then, we moved on to supervised learning and made a linear regression model to predict the period of a planet using its semi-major axis, eccentricity of orbit, and mass. We learned how to interpret the model coefficients and how to evaluate the model's predictions. Finally, we turned to classification to identify high-quality red wines (which had a class imbalance) and distinguish between red and white wine by their chemical properties. Using precision, recall, F<span class="subscript">1</span> score, confusion matrices, ROC curves, and precision-recall curves, we discussed how to evaluate classification models.</p>
			<p>It's important to remember that machine learning models make assumptions about the underlying data, and while this wasn't a chapter on the mathematics of machine learning, we should make sure that we understand that there are consequences for violating these assumptions. In practice, when looking to build models, it's crucial that we have a solid understanding of statistics and domain-level expertise. We saw that there is a multitude of metrics for evaluating our models. Each metric has its strengths and weaknesses, and, depending on the problem, some are better than others; we must take care to choose the appropriate metrics for the task at hand.</p>
			<p>In the next chapter, we will learn how to tune our models to improve their performance, so make sure to complete the exercises to practice this chapter's material before moving on.</p>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor215"/>Exercises</h1>
			<p>Practice building and evaluating machine learning models in <strong class="source-inline">scikit-learn</strong> with the following exercises:</p>
			<ol>
				<li>Build a clustering model to distinguish between red and white wine by their chemical properties:<p>a) Combine the red and white wine datasets (<strong class="source-inline">data/winequality-red.csv</strong> and <strong class="source-inline">data/winequality-white.csv</strong>, respectively) and add a column for the kind of wine (red or white).</p><p>b) Perform some initial EDA.</p><p>c) Build and fit a pipeline that scales the data and then uses k-means clustering to make two clusters. Be sure not to use the <strong class="source-inline">quality</strong> column.</p><p>d) Use the Fowlkes-Mallows Index (the <strong class="source-inline">fowlkes_mallows_score()</strong> function is in <strong class="source-inline">sklearn.metrics</strong>) to evaluate how well k-means is able to make the distinction between red and white wine.</p><p>e) Find the center of each cluster.</p></li>
				<li>Predict star temperature:<p>a) Using the <strong class="source-inline">data/stars.csv</strong> file, perform some initial EDA and then build a linear regression model of all the numeric columns to predict the temperature of the star.</p><p>b) Train the model on 75% of the initial data.</p><p>c) Calculate the R<span class="superscript">2</span> and RMSE of the model.</p><p>d) Find the coefficients for each regressor and the intercept of the linear regression equation.</p><p>e) Visualize the residuals using the <strong class="source-inline">plot_residuals()</strong> function from the <strong class="source-inline">ml_utils.regression</strong> module.</p></li>
				<li>Classify planets that have shorter years than Earth:<p>a) Using the <strong class="source-inline">data/planets.csv</strong> file, build a logistic regression model with the <strong class="source-inline">eccentricity</strong>, <strong class="source-inline">semimajoraxis</strong>, and <strong class="source-inline">mass</strong> columns as regressors. You will need to make a new column to use for the <em class="italic">y</em> (year shorter than Earth).</p><p>b) Find the accuracy score.</p><p>c) Use the <strong class="source-inline">classification_report()</strong> function from <strong class="source-inline">scikit-learn</strong> to see the precision, recall, and F<span class="subscript">1</span> score for each class.</p><p>d) With the <strong class="source-inline">plot_roc()</strong> function from the <strong class="source-inline">ml_utils.classification</strong> module, plot the ROC curve.</p><p>e) Create a confusion matrix using the <strong class="source-inline">confusion_matrix_visual()</strong> function from the <strong class="source-inline">ml_utils.classification</strong> module.</p></li>
				<li>Multiclass classification of white wine quality:<p>a) Using the <strong class="source-inline">data/winequality-white.csv</strong> file, perform some initial EDA on the white wine data. Be sure to look at how many wines had a given quality score.</p><p>b) Build a pipeline to standardize the data and fit a multiclass logistic regression model. Pass <strong class="source-inline">multi_class='multinomial'</strong> and <strong class="source-inline">max_iter=1000</strong> to the <strong class="source-inline">LogisticRegression</strong> constructor.</p><p>c) Look at the classification report for your model.</p><p>d) Create a confusion matrix using the <strong class="source-inline">confusion_matrix_visual()</strong> function from the <strong class="source-inline">ml_utils.classification</strong> module. This will work as is for multiclass classification problems.</p><p>e) Extend the <strong class="source-inline">plot_roc()</strong> function to work for multiple class labels. To do so, you will need to create a ROC curve for each class label (which are quality scores here), where a true positive is correctly predicting that quality score and a false positive is predicting any other quality score. Note that <strong class="source-inline">ml_utils</strong> has a function for this, but try to build your own implementation.</p><p>f) Extend the <strong class="source-inline">plot_pr_curve()</strong> function to work for multiple class labels by following a similar method to part <em class="italic">e)</em>. However, give each class its own subplot. Note that <strong class="source-inline">ml_utils</strong> has a function for this, but try to build your own implementation.</p></li>
				<li>We have seen how easy the <strong class="source-inline">scikit-learn</strong> API is to navigate, making it a cinch to change which algorithm we are using for our model. Rebuild the red wine quality model that we created in this chapter using an SVM instead of logistic regression. We haven't discussed this model, but you should still be able to use it in <strong class="source-inline">scikit-learn</strong>. Check out the link in the <em class="italic">Further reading</em> section to learn more about the algorithm. Some guidance for this exercise is as follows:<p>a) You will need to use the <strong class="source-inline">SVC</strong> (support vector classifier) class from <strong class="source-inline">scikit-learn</strong>, which can be found at <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</a>.</p><p>b) Use <strong class="source-inline">C=5</strong> as an argument to the <strong class="source-inline">SVC</strong> constructor.</p><p>c) Pass <strong class="source-inline">probability=True</strong> to the <strong class="source-inline">SVC</strong> constructor to be able to use the <strong class="source-inline">predict_proba()</strong> method.</p><p>d) Build a pipeline first using the <strong class="source-inline">StandardScaler</strong> class and then the <strong class="source-inline">SVC</strong> class.</p><p>e) Be sure to look at the classification report, precision-recall curve, and confusion matrix for the model.</p></li>
			</ol>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor216"/>Further reading</h1>
			<p>Check out the following resources for more information on the topics that were covered in this chapter:</p>
			<ul>
				<li><em class="italic">A Beginner's Guide to Deep Reinforcement Learning</em>: <a href="https://pathmind.com/wiki/deep-reinforcement-learning">https://pathmind.com/wiki/deep-reinforcement-learning</a></li>
				<li><em class="italic">An Introduction to Gradient Descent and Linear Regression</em>: <a href="https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/">https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/</a></li>
				<li><em class="italic">Assumptions of Multiple Linear Regression</em>: <a href="https://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/">https://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/</a></li>
				<li><em class="italic">Clustering</em>: <a href="https://scikit-learn.org/stable/modules/clustering.html">https://scikit-learn.org/stable/modules/clustering.html</a></li>
				<li><em class="italic">Generalized Linear Models</em>: <a href="https://scikit-learn.org/stable/modules/linear_model.html">https://scikit-learn.org/stable/modules/linear_model.html</a> </li>
				<li><em class="italic">Guide to Interpretable Machine Learning – Techniques to dispel the black box myth of deep learning</em>: <a href="https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf">https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf</a></li>
				<li><em class="italic">In Depth: k-Means</em>: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html">https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html</a></li>
				<li><em class="italic">Interpretable Machine Learning – A Guide for Making Black Box Models Explainable</em>: <a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a></li>
				<li><em class="italic">Interpretable Machine Learning – Extracting human understandable insights from any Machine Learning model</em>: <a href="https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b">https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b</a></li>
				<li><em class="italic">MAE and RMSE  – Which Metric is Better?</em>: <a href="https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d">https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d</a></li>
				<li><em class="italic">Model evaluation: quantifying the quality of predictions</em>: <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</a></li>
				<li><em class="italic">Preprocessing data</em>: <a href="https://scikit-learn.org/stable/modules/preprocessing.html">https://scikit-learn.org/stable/modules/preprocessing.html</a></li>
				<li><em class="italic">Scikit-learn Glossary of Common Terms and API Elements</em>: <a href="https://scikit-learn.org/stable/glossary.html#glossary">https://scikit-learn.org/stable/glossary.html#glossary</a></li>
				<li><em class="italic">Scikit-learn User Guide</em>: <a href="https://scikit-learn.org/stable/user_guide.html">https://scikit-learn.org/stable/user_guide.html</a></li>
				<li><em class="italic">Seeing Theory </em><a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a><em class="italic">: Regression Analysis</em>: <a href="https://seeing-theory.brown.edu/index.html#secondPage/chapter6">https://seeing-theory.brown.edu/index.html#secondPage/chapter6</a></li>
				<li><em class="italic">Simple Beginner's Guide to Reinforcement Learning &amp; its implementation</em>: <a href="https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/">https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/</a></li>
				<li><em class="italic">Support Vector Machine  – Introduction to Machine Learning Algorithms</em>: <a href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47">https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47</a></li>
				<li><em class="italic">The 5 Clustering Algorithms Data Scientists Need to Know</em>: <a href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68</a></li>
			</ul>
		</div>
	</body></html>