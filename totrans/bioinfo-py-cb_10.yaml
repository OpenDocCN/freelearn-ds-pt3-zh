- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bioinformatics Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pipelines are fundamental within any data science environment. Data processing
    is never a single task. Many pipelines are implemented via ad hoc scripts. This
    can be done in a useful way, but in many cases, they fail several fundamental
    viewpoints, chiefly reproducibility, maintainability, and extensibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'In bioinformatics, you can find three main types of pipeline system:'
  prefs: []
  type: TYPE_NORMAL
- en: Frameworks such as Galaxy ([https://usegalaxy.org](https://usegalaxy.org)),
    which are geared toward users, that is, they expose easy-to-use user interfaces
    and hide most of the underlying machinery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programmatic workflows – geared toward code interfaces that, while generic,
    originate from the bioinformatics space. Two examples are Snakemake ([https://snakemake.readthedocs.io/](https://snakemake.readthedocs.io/))
    and Nextflow ([https://www.nextflow.io/](https://www.nextflow.io/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Totally generic workflow systems such as Apache Airflow ([https://airflow.incubator.apache.org/](https://airflow.incubator.apache.org/)),
    which take a less data-centric approach to workflow management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will discuss Galaxy, which is especially important for bioinformaticians
    supporting users that are less inclined to code their own solutions. While you
    may not be a typical user of these pipeline systems, you might still have to support
    them. Fortunately, Galaxy provides APIs, which will be our main focus.
  prefs: []
  type: TYPE_NORMAL
- en: We will also be discussing Snakemake and Nextflow as generic workflow tools
    with programmatic interfaces that originated in the bioinformatics space. We will
    cover both tools, as they are the most common in the field. We will solve a similar
    bioinformatics problem using both Snakemake and Nextflow. We will get a taste
    of both frameworks and hopefully be able to decide on a favorite.
  prefs: []
  type: TYPE_NORMAL
- en: The code for these recipes is presented not as notebooks, but as Python scripts
    available in the `Chapter09` directory of the book’s repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will find recipes for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Galaxy servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing Galaxy using the API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a variant analysis pipeline with Snakemake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a variant analysis pipeline with Nextflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Galaxy servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Galaxy ([https://galaxyproject.org/tutorials/g101/](https://galaxyproject.org/tutorials/g101/))
    is an open source system that empowers non-computational users to do computational
    biology. It is the most widely used, user-friendly pipeline system available.
    Galaxy can be installed on a server by any user, but there are also plenty of
    other servers on the web with public access, the flagship being [http://usegalaxy.org](http://usegalaxy.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our focus in the following recipes will be the programming side of Galaxy:
    interfacing using the Galaxy API and developing a Galaxy tool to extend its functionality.
    Before you start, you are strongly advised to approach Galaxy as a user. You can
    do this by creating a free account at [http://usegalaxy.org](http://usegalaxy.org),
    and playing around with it a bit. Reaching a level of understanding that includes
    knowledge of the workflows is recommended.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will carry out a local installation of a Galaxy server using
    Docker. As such, a local Docker installation is required. The level of complexity
    will vary across operating systems: easy on Linux, medium on macOS, and medium
    to hard on Windows.'
  prefs: []
  type: TYPE_NORMAL
- en: This installation is recommended for the next two recipes but you may also be
    able to use existing public servers. Note that the interfaces of public servers
    can change over time, so what works today may not work tomorrow. Instructions
    on how to use public servers for the next two recipes are available in the *There’s
    more...* section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take a look at the following steps. These assume that you have a Docker-enabled
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we pull the Galaxy Docker image with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will pull Björn Grüning’s amazing Docker Galaxy image. Use the `20.09`
    label, as shown in the preceding command; anything more recent could break this
    recipe and the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Create a directory on your system. This directory will hold the persistent output
    of the Docker container across runs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Docker containers are transient with regard to disk space. This means that when
    you stop the container, all disk changes will be lost. This can be solved by mounting
    volumes from the host on Docker, as in the next step. All content in the mounted
    volumes will persist.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now run the image with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Replace `YOUR_DIRECTORY` with the full path to the directory that you created
    in *step 2*. If the preceding command fails, make sure you have permission to
    run Docker. This will vary across operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: Check the content of `YOUR_DIRECTORY`. The first time the image runs, it will
    create all of the files needed for persistent execution across Docker runs. That
    means maintaining user databases, datasets, and workflows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Point your browser to `http://localhost:8080`. If you get any errors, wait
    a few seconds. You should see the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 - The Galaxy Docker home page ](img/B17942_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 - The Galaxy Docker home page
  prefs: []
  type: TYPE_NORMAL
- en: 'Now log in (see the top bar) with the default username and password combination:
    `admin` and `password`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the top menu, choose **User**, and inside, choose **Preferences**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, choose **Manage API Key**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Do not change the API key. The purpose of the preceding exercise is for you
    to know where the API key is. In real scenarios, you will have to go to this screen
    to get your key. Just note the API key: `fakekey`. In normal situations, this
    will be an MD5 hash, by the way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, at this stage, we have our server installed with the following (default)
    credentials: the user as `admin`, password as `password`, and API key as `fakekey`.
    The access point is `localhost:8080`.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The way Björn Grüning’s image is going to be used throughout this chapter is
    quite simple; after all, this is not a book on system administration or DevOps,
    but a programming one. If you visit [https://github.com/bgruening/docker-galaxy-stable](https://github.com/bgruening/docker-galaxy-stable),
    you will see that there is an infinite number of ways to configure the image,
    and all are well documented. Our simple approach here works for our development
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t want to install Galaxy on your local computer, you can use a public
    server such as [https://usegalaxy.org](https://usegalaxy.org) to do the next recipe.
    It is not 100% foolproof, as services change over time, but it will probably be
    very close. Take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an account on a public server ([https://usegalaxy.org](https://usegalaxy.org)
    or other).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the previous instructions for accessing your API key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next recipe, you will have to replace the host, user, password, and API
    key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accessing Galaxy using the API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Galaxy’s main use case is via an easy-to-use web interface, it also provides
    a REST API for programmatic access. There are interfaces provided in several languages,
    for example, Python support is available from BioBlend ([https://bioblend.readthedocs.io](https://bioblend.readthedocs.io)).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are going to develop a script that will load a BED file into Galaxy
    and call a tool to convert it to GFF format. We will load the file using Galaxy’s
    FTP server.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you did not go through the previous recipe, please read the corresponding
    *There’s more...* section. The code was tested in a local server, as prepared
    in the preceding recipe, so it might require some adaptations if you run it against
    a public server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our code will need to authenticate itself against the Galaxy server in order
    to perform the necessary operations. Because security is an important issue, this
    recipe will not be totally naive with regard to it. Our script will be configured
    via a YAML file, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Our script will not accept this file as plain text, but it will require it
    to be encrypted. That being said, there is a big hole in our security plan: we
    will be using HTTP (instead of HTTPS), which means that passwords will pass in
    the clear over the network. Obviously, this is a bad solution, but space considerations
    put a limit on what we can do (especially in the preceding recipe). Really secure
    solutions will require HTTPS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need a script that takes a YAML file and generates an encrypted version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding file can be found at `Chapter09/pipelines/galaxy/encrypt.py` in
    the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to input a password for encryption.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code is not Galaxy-related: it reads a `YAML` file and encrypts
    it with a password supplied by the user. It uses the `cryptography` module encryption
    and `ruaml.yaml` for `YAML` processing. Two files are output: the encrypted `YAML`
    file and the `salt` file for encryption. For security reasons, the `salt` file
    should not be public.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach to securing credentials is far from sophisticated; it is mostly
    illustrative that you have to be careful with your code when dealing with authentication
    tokens. There are far more instances on the web of hardcoded security credentials.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take a look at the following steps, which can be found in `Chapter09/pipelines/galaxy/api.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by decrypting our configuration file. We need to supply a password:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last line summarizes it all: the `YAML` module will load the configuration
    from a decrypted file. Note that we also read `salt` in order to be able to decrypt
    the file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll now get all configuration variables, prepare the server URL, and specify
    the name of the Galaxy history that we will be creating (`bioinf_example`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we are able to connect to the Galaxy server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now list all `histories` available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: On the first execution, you will get an unnamed history, but on the other executions,
    you will also get `bioinf_example`, which we will delete at this stage so that
    we start with a clean slate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, we create the `bioinf_example` history :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you want, you can check on the web interface, and you will find the new history
    there.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to upload the file now; this requires an SFTP connection. The
    file is supplied with this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now tell Galaxy to load the file on the FTP server into its internal
    database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s summarize the contents of our history:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We only have one entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s inspect the metadata for our `BED` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s turn our attention to the existing tools on the server and get metadata
    about them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will print a long list of tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s get some information about our tool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The tool’s name was available in the preceding step. Note that we get the first
    element of a list as, in theory, there could be more than one version of the tool
    installed. The abridged output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s run a tool to convert our `BED` file into `GFF`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The parameters of the tool can be inspected in the preceding step. If you go
    to the web interface, you will see something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 - Checking the results of our script via Galaxy’s web interface
    ](img/B17942_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 - Checking the results of our script via Galaxy’s web interface
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have accessed Galaxy using its REST API.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a variant analysis pipeline with Snakemake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Galaxy is mostly geared toward users who are less inclined to program. Knowing
    how to deal with it, even if you prefer a more programmer-friendly environment,
    is important because of its pervasiveness. It is reassuring that an API exists
    to interact with Galaxy. But if you want a more programmer-friendly pipeline,
    there are many alternatives available. In this chapter, we explore two widely
    used programmer-friendly pipelines: `snakemake` and Nextflow. In this recipe,
    we consider `snakemake`.'
  prefs: []
  type: TYPE_NORMAL
- en: Snakemake is implemented in Python and shares many traits with it. That being
    said, its fundamental inspiration is a Makefile, the framework used by the venerable
    `make`-building system.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will develop a mini variant analysis pipeline with `snakemake`. The
    objective here is not to get the scientific part right – we cover that in other
    chapters – but to see how to create pipelines with `snakemake`. Our mini pipeline
    will download HapMap data, subsample it at 1%, do a simple PCA, and draw it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will need Plink 2 installed alongside `snakemake`. To display execution
    strategies, you will also need Graphviz to draw the execution. We will define
    the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uncompressing it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sub-sampling it at 1%
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing the PCA on the 1% sub-sample
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Charting the PCA
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our pipeline recipe will have two parts: the actual coding of the pipeline
    in `snakemake` and a support script in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: The `snakemake` code for this can be found in `Chapter09/snakemake/Snakefile`,
    whereas the Python support script is in `Chapter09/snakemake/plot_pca.py`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first task is downloading the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Snakemake’s language is Python-dependent, as you can see from the very first
    lines, which should be easy to understand from a Python perspective. The fundamental
    part is the rule. It has a set of input streams, which are rendered via `HTTP.remote`
    in our case, as we are dealing with remote files, followed by the output. We put
    two files in a `scratch` directory (the ones that are still uncompressed) and
    one in the `data` directory. Finally, our pipeline code is a simple shell script
    that moves the downloaded HTTP files to their final location. Note how the shell
    script refers to inputs and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this script, downloading the files is easy. Run the following on the command
    line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This tells `snakemake` that you want to materialize `data/relationships.txt`
    . We will be using a single core, `-c1` . As this is an output of the `plink_download`
    rule, the rule will then be run (unless the file is already available – in that
    case, `snakemake` will do nothing). Here is an abridged version of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Snakemake gives you some information about which jobs will be executed and starts
    running those.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the data, let’s see the rule for uncompressing it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The most interesting feature here is the way that we can specify multiple files
    to download. Note how the `PLINKEXTS` list is converted into discrete `plinkext`
    elements in the code. You can execute by requesting an output from the rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s subsample our data to 1%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The new content is in the last two lines: we are not using `script`, but `run`.
    This tells `snakemake` that the execution is Python-based with a few extra functions
    available. Here we see the shell function, which executes a shell script. The
    string is a Python `f`-string – note the reference to the `snakemake` `input`
    and `output` variables in the string. You could put more complex Python code here
    – for example, you could iterate over the inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: TIP
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are assuming that Plink is available, as we pre-installed it, but `snakemake`
    does provide some functionality to deal with dependencies. More specifically,
    `snakemake` rules can be annotated with a `YAML` file pointing to `conda` dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our data sub-sampled, let’s compute the PCA. In this case,
    we will use Plink’s internal PCA framework to do the computation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As with most pipeline systems, `snakemake` constructs a `snakemake` to present
    you a DAG of what you will execute to generate your request. For example, to generate
    the PCA, use the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This would generate the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 - The DAG to compute the PCA ](img/B17942_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 - The DAG to compute the PCA
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s generate the `plot` rule for the PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `plot` rule introduces a new type of execution, `script`. In this case,
    an external Python script is called to process the rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Python script to generate the chart is the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Python script has access to the `snakemake` object. This object exposes
    the content of the rule: note how we make use of `input` to get the PCA data and
    `output` to generate the image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the code to produce a rough chart is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.4 - A very rough PCA produced by the Snakemake pipeline ](img/B17942_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 - A very rough PCA produced by the Snakemake pipeline
  prefs: []
  type: TYPE_NORMAL
- en: There’s more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding recipe was made to run on a simple configuration of `snakemake`.
    There are many more ways to construct rules in `snakemake`.
  prefs: []
  type: TYPE_NORMAL
- en: The most important issue that we didn’t discuss is the fact that `snakemake`
    can execute code in many different environments, from the local computer (as in
    our case), on-premises clusters, to the cloud. It would be unreasonable to ask
    for anything more than using a local computer to try `snakemake`, but don’t forget
    that `snakemake` can manage complex computing environments.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that `snakemake`, while implemented in Python, is conceptually based
    on `make`. It’s a subjective analysis to decide whether you like the (snake)make
    design. For an alternative design approach, check the next recipe, which uses
    Nextflow.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a variant analysis pipeline with Nextflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two main players in the pipeline framework space in bioinformatics:
    `snakemake` and Nextflow. They provide pipeline functionality whilst having different
    design approaches. Snakemake is based on Python, but its language and philosophy
    come from the `make` tool used to compile complex programs with dependencies.
    Nextflow is Java-based (more precisely, it’s implemented in Groovy – a language
    that works on top of the Java Virtual Machine) and has its own `snakemake` and
    choose the one that better suits your needs.'
  prefs: []
  type: TYPE_NORMAL
- en: TIP
  prefs: []
  type: TYPE_NORMAL
- en: There are many perspectives on how to evaluate a pipeline system. Here, we present
    a perspective based on the language used to specify the pipeline. However, there
    are others that you should consider when choosing a pipeline system. For example,
    how well does it support your execution environment (such as an on-premises cluster
    or a cloud), does it support your tools (or allow easy development of extensions
    to deal with new tools), and does it provide good recovery and monitoring functionalities?
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will develop a pipeline with Nextflow that provides the same functionality
    as we implemented with `snakemake`, thus allowing for a fair comparison from the
    pipeline design point of view. The objective here is not to get the scientific
    part right – we cover that in other chapters – but to see how to create pipelines
    with `snakemake`. Our mini pipeline will download HapMap data, sub-sample it at
    1%, do a simple PCA, and draw it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will need Plink 2 installed along with Nextflow. Nextflow itself requires
    some software from the Java space: notably the Java Runtime Environment and Groovy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uncompressing it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sub-sampling it at 1%
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing the PCA on the 1% sub-sample
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Charting the PCA
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Nextflow code for this can be found in `Chapter09/nextflow/pipeline.nf`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first task is downloading the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remember that the underlying language for the pipeline is not Python but Groovy,
    so the syntax will be a bit different, such as using braces for blocks or ignoring
    indentation.
  prefs: []
  type: TYPE_NORMAL
- en: We create a process (a pipeline building block in Nextflow) called `plink_download`,
    which downloads the Plink files. It only specifies outputs. The first output will
    be the `hapmap.map.gz` file and the second output will be `hapmap.ped.gz`. This
    process will have two output channels (another Nextflow concept, akin to a stream),
    which can be consumed by another process.
  prefs: []
  type: TYPE_NORMAL
- en: The code for the process is, by default, a bash script. It’s important to note
    how the script outputs files with names that are synchronized with the output
    section. Also, see how we refer to the variables defined in the pipeline (`download_root`,
    in our case).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now define a process to consume the channels with the HapMap files and
    decompress them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are three issues of note in this process: we now have a couple of inputs
    (remember that we have a couple of outputs from the previous process). Our script
    also now refers to input variables (`$mapgz` and `$pedgz`). Finally, we publish
    the output by using `publishDir`. Therefore, any files that are not published
    will only be stored temporarily.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s specify a first version of the workflow that downloads and uncompresses
    the files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can execute the workflow by running the following on the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `resume` flag at the end will make sure that the pipeline will continue
    from whatever step was already completed. The steps are, on local execution, stored
    in the `work` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we remove the `work` directory, we don’t want to download the HapMap files
    if they were already published. As this is outside the `work` directory, hence
    not directly tracked, we need to change the workflow to track the data in the
    published directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are alternative ways of doing this, but I wanted to introduce a bit of
    Groovy code, as you might sometimes have to write code in Groovy. There are ways
    to use Python code, as you will see soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to subsample the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now compute the PCA using Plink:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s plot the PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The new feature of this code is that we specify the bash script using the shebang
    (`#!`) operator, which allows us to call an external scripting language to process
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our final workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We either download the data or use the already downloaded data.
  prefs: []
  type: TYPE_NORMAL
- en: While there are other dialects for designing the complete workflow, I would
    like you to note how we use `subsample_1p` when the files are available; we can
    explicitly pass two channels to a process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run the pipeline and request an HTML report on execution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The report is quite exhaustive and will allow you to figure out the parts of
    the pipeline that are expensive from different perspectives, whether related to
    time, memory, CPU consumption, or I/O.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a simple introductory example of Nextflow, which hopefully will allow
    you to get a flavor of the framework, particularly so that you can compare it
    with `snakemake`. Nextflow has many more functionalities and you are encouraged
    to check out its website.
  prefs: []
  type: TYPE_NORMAL
- en: As with `snakemake`, the most important issue that we didn’t discuss is the
    fact that Nextflow can execute code in many different environments, from the local
    computer, on-premises clusters, to the cloud. Check Nextflow’s documentation to
    see which computing environments are currently supported.
  prefs: []
  type: TYPE_NORMAL
- en: As important as the underlying language is, Groovy with Nextflow and Python
    with `snakemake`, make sure to compare other factors. This includes not only where
    both pipelines can execute (locally, in a cluster, or in a cloud) but also the
    design of the framework, as they use quite different paradigms.
  prefs: []
  type: TYPE_NORMAL
