<html><head></head><body>
  <div id="_idContainer1215" class="Basic-Text-Frame">
    <h1 class="chapterNumber">20</h1>
    <h1 id="_idParaDest-500" class="chapterTitle">Evaluating Forecasts—Validation Strategies</h1>
    <p class="normal">Throughout the last few chapters, we have been looking at a few relevant, but seldom discussed, aspects of time series forecasting. While we learned about different forecasting metrics in the previous chapter, we now move on to the final piece of the puzzle—validation strategies. This is another integral part of evaluating forecasts.</p>
    <p class="normal">In this chapter, we try to answer the question <em class="italic">How do we choose the validation strategy to evaluate models from a time series forecasting perspective?</em> We will look at different strategies and their merits and demerits so that, by the end of the chapter, you can make an informed decision to set up the validation strategy for your time series problem.</p>
    <p class="normal">In this chapter, we will be covering these main topics:</p>
    <ul>
      <li class="bulletList">Model validation</li>
      <li class="bulletList">Holdout strategies</li>
      <li class="bulletList">Cross-validation strategies</li>
      <li class="bulletList">Choosing a validation strategy</li>
      <li class="bulletList">Validation strategies for datasets with multiple time series</li>
    </ul>
    <h1 id="_idParaDest-501" class="heading-1">Technical requirements</h1>
    <p class="normal">You will need to set up the Anaconda environment by following the instructions in the <em class="italic">Preface</em> of the book to get a working environment with all the packages and datasets required for the code in this book.</p>
    <p class="normal">The associated code for the chapter can be found at <a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter20"><span class="url">https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter20</span></a>.</p>
    <h1 id="_idParaDest-502" class="heading-1">Model validation</h1>
    <p class="normal">In <em class="chapterRef">Chapter 19</em>, <em class="italic">Evaluating Forecast Errors</em>—<em class="italic">A Survey of Forecast Metrics</em>, we learned about different<a id="_idIndexMarker1675"/> forecast metrics that can be used to measure the quality of a forecast. One of the main uses for this is to measure how well our forecast is doing on test data (new and unseen data), but this comes after we train a model, tweak it, and tinker with it until we are happy with it. How do we know whether a model we are training or tweaking is good enough?</p>
    <p class="normal">Model validation is the process of evaluating a trained model using data to assess how good the model is. We use the metrics we learned about in <em class="chapterRef">Chapter 19</em>, <em class="italic">Evaluating Forecast Errors</em>—<em class="italic">A Survey of Forecast Metrics</em>, to calculate the goodness of the forecast. But there is one question we haven’t answered. Which part of the data do we use to evaluate? In a standard machine learning setup (classification or regression), we randomly sample a portion of the training data and call it validation data, and it is based on this data that all the modeling decisions are taken. The best practice in the field is to use cross-validation. <strong class="keyWord">Cross-validation</strong> is a <a id="_idIndexMarker1676"/>resampling procedure where we sample different portions of the training dataset to train and test in multiple iterations. In addition to repeated evaluation, cross-validation also makes the most efficient use of the data.</p>
    <p class="normal">However, in the field of time series forecasting, such a consensus on best practice does not exist. This is mainly because of the temporal nature and the sheer variety of ways we can go about it. Different time series might have different lengths of history and we may choose different ways to model it, or there might be different horizons to forecast for, and so on. Because of the temporal dependence in the data, standard assumptions of i.i.d. don’t hold true; therefore, techniques such as cross-validation have their own complications. When randomly chosen, the validation and training datasets may not be independent, which will lead to an optimistic and misleading estimate of error.</p>
    <p class="normal">There are two main paradigms of validation:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">In-sample validation</strong>: As<a id="_idIndexMarker1677"/> the name suggests, the model is evaluated on the same or a subset of the same data that was used to train it.</li>
      <li class="bulletList"><strong class="keyWord">Out-of-sample validation</strong>: Under <a id="_idIndexMarker1678"/>this paradigm, the data we use to evaluate the model has no intersection with the data used to train the model.</li>
    </ul>
    <p class="normal">In-sample validation <a id="_idIndexMarker1679"/>helps you understand how well your model fits the data you have. This was very popular in the era of statistics, where the models were meticulously designed and primarily used for inferencing and not predicting. In such cases, the in-sample error shows how well the specified model fits the data and how valid the inferences we make from that model are. But in a predictive paradigm, like most machine learning, the in-sample error is not the right measure of the <em class="italic">goodness</em> of a model. Complex models can easily fit the training data, memorize it, and not work well on new and unseen data. Therefore, out-of-sample validations are almost exclusively used in today’s<a id="_idIndexMarker1680"/> predictive model evaluations. Since this book is solely concerned with forecasting, which is a predictive task, we will be sticking to out-of-sample evaluations only.</p>
    <p class="normal">As discussed earlier, deciding on a validation strategy for forecasting problems is not as trivial as standard machine learning. There are two major schools of thought here:</p>
    <ul>
      <li class="bulletList">Holdout-based strategies, which respect the temporal integrity of the problem</li>
      <li class="bulletList">Cross-validation-based strategies, which sample validation splits with no or a very loose sense of temporal ordering</li>
    </ul>
    <p class="normal">Let’s discuss the major ones in each category. What we have to keep in mind is that all the validation strategies that we discuss in the book are not exhaustive. They are merely a few popular ones. In the explanations that follow, the length of the validation period is <em class="italic">L</em><sub class="subscript-italic" style="font-style: italic;">v</sub> and the length of the training period is <em class="italic">L</em><sub class="subscript-italic" style="font-style: italic;">t</sub>.</p>
    <p class="normal">Now, let’s look at the first school of thought.</p>
    <h1 id="_idParaDest-503" class="heading-1">Holdout strategies</h1>
    <p class="normal">There are three <a id="_idIndexMarker1681"/>aspects of a holdout strategy, and they can be mixed and matched to create many variations of the strategy. For instance, we might have a sampling strategy with a fixed split, a rolling window for the training data, and a recalibration of the model for each iteration. The three aspects are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Sampling strategy</strong>: A <a id="_idIndexMarker1682"/>sampling strategy<a id="_idIndexMarker1683"/> is how we sample the validation split(s) from training data.</li>
      <li class="bulletList"><strong class="keyWord">Window strategy</strong>: A <a id="_idIndexMarker1684"/>window strategy decides <a id="_idIndexMarker1685"/>how we sample the window of training split(s) from training data.</li>
      <li class="bulletList"><strong class="keyWord">Calibration strategy</strong>: A<a id="_idIndexMarker1686"/> calibration strategy<a id="_idIndexMarker1687"/> decides whether a model should be recalibrated or not.</li>
    </ul>
    <p class="normal">That said, designing <a id="_idIndexMarker1688"/>a holdout validation strategy for a time series problem includes making decisions on these three aspects.</p>
    <p class="normal">Sampling strategies are ways to pick one or more origins in the training data. These <strong class="keyWord">origins</strong> are points in time that determine the starting point of the validation split and the ending point of the training split. The exact length of the validation split is governed by a parameter <em class="italic">L</em><sub class="subscript-italic" style="font-style: italic;">v</sub>, which is the horizon chosen for validation. The length of the training split depends on the window strategy.</p>
    <h2 id="_idParaDest-504" class="heading-2">Window strategy</h2>
    <p class="normal">There <a id="_idIndexMarker1689"/>are two ways we can draw the windows of <a id="_idIndexMarker1690"/>training split—expanding window and rolling window. <em class="italic">Figure 20.1</em> shows the difference between the two setups:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01.png" alt="Figure 19.1 – Expanding (left) versus rolling (right) strategy "/></figure>
    <p class="packt_figref">Figure 20.1: Expanding (left) versus rolling (right) window strategies</p>
    <p class="normal">Under the <a id="_idIndexMarker1691"/>expanding window strategy, the training split expands as the origin moves forward in time. In other words, under the expanding window strategy, we choose all the data that is available before the origin as the training split. This effectively increases the training length every time the origin moves forward in time.</p>
    <p class="normal">In the<a id="_idIndexMarker1692"/> rolling window strategy, we keep the length of the training split constant (<em class="italic">L</em><sub class="subscript-italic" style="font-style: italic;">t</sub>). Therefore, when we move the origin forward by three timesteps, the training split drops three timesteps from the start of the time series.</p>
    <div class="note">
      <p class="normal">Although the expanding and rolling window concept may remind you of the window we use for feature engineering or use as the context in deep learning models, this window is not the same. The window we talk about in this chapter is the window of training data that we chose to train our model. For instance, the features of a machine learning model may only extend to the 5 days before, and we can have the training split using the last 5 years of data.</p>
    </div>
    <p class="normal">There are merits and demerits to both of these window strategies. Let’s summarize them in a few key points:</p>
    <ul>
      <li class="bulletList">The expanding window is a good setup for a short time series, where the expanding window leads to more data being available for the models.</li>
      <li class="bulletList">The rolling window removes the oldest data from training. If the time series is non-stationary and the behavior is bound to change as time passes, having a rolling window will be beneficial to keep the model up to date.</li>
      <li class="bulletList">When we use the expanding window strategy for repeated evaluation, such as in cross-validation, the increase in time series length used for training can introduce some bias toward windows with a longer history. The rolling window strategy takes care of that bias by maintaining the same length of the series.</li>
    </ul>
    <p class="normal">Now, let’s look at another aspect of a validation strategy.</p>
    <h2 id="_idParaDest-505" class="heading-2">Calibration strategy</h2>
    <p class="normal">The<a id="_idIndexMarker1693"/> calibration strategy is<a id="_idIndexMarker1694"/> only valid in cases where we do multiple evaluations with different origins. There are two ways we can do evaluations with different origins—recalibrate every origin or update every origin (terminology from Tashman, reference <em class="italic">1</em>).</p>
    <p class="normal">Under the <em class="italic">recalibrate</em> strategy, the model is retrained with the new training split for every origin. This retrained model is used to evaluate the validation split. For the <em class="italic">update</em> strategy, we do not retrain the model but use the trained model to evaluate the new validation split.</p>
    <p class="normal">Let’s summarize a couple of key points to be considered for choosing a strategy here:</p>
    <ul>
      <li class="bulletList">The golden standard is to recalibrate every new origin, but many times, this may not be feasible. In the econometrics/classical statistical models, the norm was to recalibrate every origin. That was feasible because those models are relatively less compute-intensive and the datasets at the time were also small. So, one could refit a model in a very short time. Nowadays, the datasets have grown in size, and so have the models. Retraining a deep learning model every time we move the origin may not be as easy.</li>
    </ul>
    <p class="normal-one">Therefore, if you are using modern, complex models with long training times, an update strategy might be better.</p>
    <ul>
      <li class="bulletList">For classical models that run fast, we can explore the recalibration strategy. However, if the time series you are forecasting is so dynamic that the behavior changes very frequently, then the recalibration strategy might be the way to go.</li>
    </ul>
    <p class="normal">Now, let’s get on to the third part of the validation strategy.</p>
    <h2 id="_idParaDest-506" class="heading-2">Sampling strategy</h2>
    <p class="normal">In the<a id="_idIndexMarker1695"/> holdout strategy, we sample a point (<em class="italic">origin</em>) on<a id="_idIndexMarker1696"/> the time series, preferably toward the end, such that the portion of the time series after the origin is shorter than the portion of the time series before. From this origin, we can use either the <em class="italic">expanding</em> or <em class="italic">rolling window</em> strategy to generate training and validation splits. The model is trained on the training split and tested on the held-out validation split. This strategy is simply called the <strong class="keyWord">holdout</strong> strategy. The calibration strategy is fixed at <em class="italic">recalibrate</em> because we are only testing and evaluating the model once.</p>
    <p class="normal">The<a id="_idIndexMarker1697"/> simple holdout strategy has one disadvantage—the forecast measure we have calculated on the held-out data may not be robust enough because of the single evaluation paradigm. We are relying on a single split of data to calculate the predictive performance of the model. For non-stationary series, this can be a problem because we might be selecting a model that captures the idiosyncrasies of the split that we have chosen.</p>
    <p class="normal">We can get over this problem by repeating the holdout evaluation multiple times. We can either hand-tailor the different origins using business domain knowledge, such as taking into account seasonality or some other factor, or we could sample the origin points randomly. If we repeat this <em class="italic">n</em> times, there will be <em class="italic">n</em> validation splits, and they may or may not overlap with each other. The performance metric from these repeated trials can be aggregated <a id="_idIndexMarker1698"/>using a function such as the mean, maximum, and minimum. This is called the <strong class="keyWord">repeated holdout</strong> (<strong class="keyWord">Rep-Holdout</strong>) strategy.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">A note on implementation:</strong></p>
      <p class="normal">The simple holdout strategy is very simple to implement because we decide the size of the validation split and keep that much from the end of the time series aside for the validation. The <strong class="keyWord">Rep-Holdout</strong> strategy involves sampling multiple windows randomly or using predefined windows as validation splits. We can make use of the <code class="inlineCode">PredefinedSplit</code> class from scikit-learn to this effect.</p>
    </div>
    <p class="normal"><em class="italic">Figure 20.2</em> shows the two holdout strategies using an expanding window approach:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02.png" alt="Figure 19.2 – Holdout strategy (a) and Rep-Holdout strategy (b) "/></figure>
    <p class="packt_figref">Figure 20.2: Holdout strategy (a) and Rep-Holdout strategy (b)</p>
    <p class="normal">The Rep-Holdout strategy<a id="_idIndexMarker1699"/> has a few more variants. The vanilla <em class="italic">Rep-Holdout</em> strategy evaluates multiple <a id="_idIndexMarker1700"/>validation datasets, is mostly hand-crafted, and can have overlapping validation datasets. A variation of the Rep-Holdout strategy that insists that multiple validation splits should have no overlap is a more popular option. We call this <strong class="keyWord">Repeated Holdout (No Overlap) (Rep-Holdout-O)</strong>. This<a id="_idIndexMarker1701"/> has some properties from the cross-validation family and tries to use more data systematically. <em class="italic">Figure 20.3 (a)</em> shows this strategy:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03.png" alt="Figure 19.3 – Variations of Rep-Holdout strategy "/></figure>
    <p class="packt_figref">Figure 20.3: Variations of the Rep-Holdout strategy</p>
    <p class="normal">The <em class="italic">Rep-Holdout-O</em> strategy is<a id="_idIndexMarker1702"/> easy to implement in scikit-learn using the <code class="inlineCode">TimeSeriesSplit</code> class for single time series datasets.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert:</strong></p>
      <p class="normal">The associated notebook that shows how to implement different validation strategies can be found in the <code class="inlineCode">Chapter20</code> folder under the name <code class="inlineCode">01-Validation_Strategies.ipynb</code>.</p>
    </div>
    <p class="normal">The <code class="inlineCode">TimeSeriesSplit</code> class from <code class="inlineCode">sklearn.model_selection</code> implements the Rep-Holdout validation<a id="_idIndexMarker1703"/> strategy and even supports expanding or rolling window variants. The main parameter is <code class="inlineCode">n_splits</code>. This determines how many splits you want from the data, and the validation split size is decided automatically according to this formula:</p>
    <p class="center"><img src="../Images/B22389_20_001.png" alt=""/></p>
    <p class="normal">In the default configuration, this implements an expanding window Rep-Holdout-O strategy. But there is a <code class="inlineCode">max_train_size</code> parameter. If we set this parameter, then it will use a window of <code class="inlineCode">max_train_size</code> in a rolling window manner.</p>
    <p class="normal">Yet another variant of the Rep-Holdout strategy introduces a gap of length <em class="italic">L</em><sub class="subscript-italic" style="font-style: italic;">g</sub> between the train and validation splits. This is to increase the independence between the train and validation splits, hence getting a better error estimate through the procedure. We call this <a id="_idIndexMarker1704"/>strategy <strong class="keyWord">Repeated Holdout (No Overlap) with Gaps (Rep-Holdout-O(G))</strong>. This strategy is depicted in <em class="italic">Figure 20.3 (b)</em>.</p>
    <p class="normal">We can implement this using the <code class="inlineCode">TimeSeriesSplit</code> class as well. All we need to do is use a parameter called <code class="inlineCode">gap</code>. By default, the gap is set to 0. But if we change to a non-zero number, it inserts that much timestep gap between the end of the training and the beginning of validation.</p>
    <p class="normal">Before we move on to the next set of strategies, let’s summarize and discuss some key points about the holdout strategies:</p>
    <ul>
      <li class="bulletList">Holdout strategies respect the temporal integrity of the problem and have been the preferred way of evaluating forecasting models for a long time. However, they do have a weakness in the inefficient use of available data. For short time series, holdout or Rep-Holdout may not have enough training data to train a model.</li>
      <li class="bulletList">A simple holdout depends on a single evaluation, and the error estimate is not robust. Even in a stationary series, this procedure does not guarantee a good estimate of the error. In non-stationary time series, such as a seasonal time series, this problem is exacerbated. But Rep-Holdout and its variants take care of that issue.</li>
    </ul>
    <p class="normal">Now, let’s look at the other major school of thought.</p>
    <h1 id="_idParaDest-507" class="heading-1">Cross-validation strategies</h1>
    <p class="normal">Cross-validation is one of<a id="_idIndexMarker1705"/> the most important tools when evaluating standard regression and classification methods. This is because of two reasons:</p>
    <ul>
      <li class="bulletList">A simple holdout approach doesn’t use all the data available and, in cases where data is scarce, cross-validation makes the best use of the available data.</li>
      <li class="bulletList">Theoretically, the time series we have observed is one realization of a stochastic process, and so the acquired error measure of the data is also a stochastic variable. Therefore, it is essential to sample multiple error estimates to get an idea about the distribution of the stochastic variable. Intuitively, we can think of this as a “lack of reliability” on the error measure derived from a single slice of data.</li>
    </ul>
    <p class="normal">The most common strategy that is used in standard machine learning is called <strong class="keyWord">k-fold cross-validation</strong>. Under <a id="_idIndexMarker1706"/>this strategy, we randomly shuffle and partition the training data into <em class="italic">k</em> equal parts. Now, the whole training of the model and calculating the error is repeated <em class="italic">k</em> times such that every <em class="italic">k</em> subset we have kept aside is used as a test set once, and only once. When we use a particular subset as testing data, we use all the other subsets as the training data. After we acquire <em class="italic">k</em> different estimates of the error measure, we aggregate it using a function such as an average. This mean will typically be more robust than a single error measure.</p>
    <p class="normal">However, there is one assumption that is central to the validity procedure: <em class="italic">i.i.d samples</em>. This is one assumption that is invalid in time series problems because, by definition, the different samples in time series are dependent on each other through autocorrelation.</p>
    <div class="note">
      <p class="normal">Some argue that when we use time delay embedding to convert time series to a regression problem, we can start to use k-fold cross-validation on time series problems. While there are obvious theoretical problems, <em class="italic">Bergmeir et al</em>. (Reference <em class="italic">2</em>) showed that, empirically, the k-fold cross-validation is not a bad option, but the caveat is that the time series needs to be stationary. We will talk about this in more detail in the next section, where we will discuss the merits and demerits of these strategies.</p>
    </div>
    <p class="normal">However, there have been modifications to the <em class="italic">k</em>-fold strategy, specifically aimed at sequential data.</p>
    <p class="normal"><em class="italic">Snijders et al.</em> (Reference 4) proposed a modification we call the <strong class="keyWord">Blocked Cross-Validation</strong> (<strong class="keyWord">Bl-CV</strong>) strategy. It is <a id="_idIndexMarker1707"/>similar to the standard <em class="italic">k-fold</em> strategy, but we do not randomly shuffle the dataset before partitioning it into <em class="italic">k</em> subsets of length <em class="italic">L</em><sub class="subscript-italic" style="font-style: italic;">v</sub>. So, this partitioning strategy results in <em class="italic">k</em> contiguous blocks of observations. Then, like a standard k-fold strategy, we train and test each of these <em class="italic">k</em> blocks and aggregate the error measure over these multiple evaluations, so the temporal integrity of the problem is satisfied partially. </p>
    <p class="normal">In other words, temporal integrity is maintained within each of the blocks, but not between the blocks. <em class="italic">Figure 20.4 (a)</em> shows this strategy:</p>
    <figure class="mediaobject"><img src="../Images/B22389_04.png" alt="Figure 19.4 – Bl-CV strategies "/></figure>
    <p class="packt_figref">Figure 20.4: Bl-CV strategies</p>
    <p class="normal">To implement the <strong class="keyWord">Bl-CV strategy</strong>, we can use the same <code class="inlineCode">Kfold</code> class from scikit-learn. As we saw earlier, the <a id="_idIndexMarker1708"/>main parameter the cross-validation classes in scikit-learn takes in is <code class="inlineCode">n_splits</code>. Here, <code class="inlineCode">n_splits</code> also defines the number of equally sized folds it selects. There is another parameter, <code class="inlineCode">shuffle</code>, which is set to <code class="inlineCode">True</code> by default. If we make sure our data is sorted according to time and then use the <code class="inlineCode">Kfold</code> class with <code class="inlineCode">shuffle=False</code>, it will imitate the <strong class="keyWord">Bl-CV</strong> strategy. The associated notebook shows this usage. I urge you to check the notebook for a better understanding of how this is implemented.</p>
    <p class="normal">In the previous section, we talked about introducing gaps between train and validation splits, to increase independence<a id="_idIndexMarker1709"/> between them. Another variant of the Bl-CV is a version that uses these gaps. We call it <strong class="keyWord">Blocked Cross-Validation with Gaps</strong> (<strong class="keyWord">Bl-CV(G)</strong>). We can see this in action in <em class="italic">Figure 20.4 (b)</em>.</p>
    <p class="normal">Unfortunately, the <code class="inlineCode">Kfold</code> implementation in scikit-learn does not support this variant, but it’s simple to extend the <code class="inlineCode">Kfold</code> implementation to include gaps as well. The associated notebook has an implementation of this. It has an additional parameter, <code class="inlineCode">gap</code>, that lets us set the gap between the train and validation splits.</p>
    <p class="normal">We saw many different strategies for validation; now let’s try and lay down a few points that will help you in deciding the right strategy for your problem.</p>
    <h1 id="_idParaDest-508" class="heading-1">Choosing a validation strategy</h1>
    <p class="normal">Choosing<a id="_idIndexMarker1710"/> the right validation strategy is one of the most important but overlooked tasks in the machine learning workflow. A good validation setup will go a long way in all the different steps in the modeling process, such as feature engineering, feature selection, model selection, and hyperparameter tuning. Although there are no hard and fast rules in setting up a validation strategy, there are a few guidelines we can follow. Some of them are from experience (both mine and others) and some of them are from empirical and theoretical studies that have been published as research papers:</p>
    <ul>
      <li class="bulletList">One guiding principle in the design is that we try to make the validation strategy replicate the real use of the model as much as possible. For instance, if the model is going to be used to predict the next 24 timesteps, we make the length of the validation split 24 timesteps. Of course, it’s not as simple as that because other practical constraints such as the availability of enough data, time, and computers have to be kept in mind while designing a validation strategy.</li>
      <li class="bulletList">Rep-Holdout strategies that respect the temporal order of the time series problem are the preferred option, especially in cases where there is sufficient data available.</li>
      <li class="bulletList">For purely autoregressive formulations of stationary time series, regular <code class="inlineCode">Kfold</code> can also be used, and Bergmeir et al. (Reference 2) empirically show that they perform better than holdout strategies. But Bl-CV is a better alternative among cross-validated strategies. <em class="italic">Cerqueira et al.</em> (Reference 3) corroborated the findings in their empirical study for stationary time series.</li>
      <li class="bulletList">If the time series is non-stationary, then <em class="italic">Cerqueira et al.</em> showed empirically that the holdout strategies (especially Rep-Holdout strategies) are the best ones to choose.</li>
      <li class="bulletList">If the time series is short, using Bl-CV after making the time series stationary is a good strategy for autoregressive formulations, such as time delay embedding. However, for models that use some kind of memory of the history to forecast, such as exponential smoothing or deep learning models such as RNN, cross-validation strategies may not be safe to use.</li>
      <li class="bulletList">If we<a id="_idIndexMarker1711"/> have exogenous variables in addition to the autoregressive part, it may not be safe to use cross-validation strategies. It is best to stick to holdout-based strategies.</li>
      <li class="bulletList">For a strongly seasonal time series, it is beneficial to use validation periods that mimic the forecast horizon. For instance, if we are forecasting for October, November, and December, it is beneficial to check the performance of the model in October, November, and December of last year.</li>
    </ul>
    <p class="normal">Up until now, we have been talking about validation strategies for a single time series. But in the context of global models, we are at a point where we need to think about validation strategies for such cases as well.</p>
    <h1 id="_idParaDest-509" class="heading-1">Validation strategies for datasets with multiple time series</h1>
    <p class="normal">All the<a id="_idIndexMarker1712"/> strategies we have seen until<a id="_idIndexMarker1713"/> now are perfectly valid for datasets with multiple time series, such as the London Smart Meters dataset we have been working with in this book. The insights we discussed in the last section are also valid. The implementation of such strategies can be slightly tricky because the scikit-learn classes we discussed work for a single time series. Those implementations assume that we have a single time series, sorted according to the temporal order. If there are multiple time series, the splits will be haphazard and messy.</p>
    <p class="normal">There are a <a id="_idIndexMarker1714"/>couple of options we can adopt<a id="_idIndexMarker1715"/> for datasets with multiple time series:</p>
    <ul>
      <li class="bulletList">We can loop over the different time series and use the methods we discussed to do the train-validation split, and then concatenate the resulting sets across all the time series. But that is not going to be so efficient.</li>
      <li class="bulletList">We can write some code and design the validation strategies to use datetime or a time index (such as the one we saw in PyTorch forecasting in <em class="chapterRef">Chapter 15</em>, <em class="italic">Strategies for Global Deep Learning Forecasting Models</em>). I have provided a link to a brilliant notebook from <em class="italic">Konrad Banachewicz</em> in the <em class="italic">Further reading</em> section of this chapter, where he uses a custom <code class="inlineCode">GroupSplit</code> class that uses the time index as the group. This is one way to use Rep-Holdout strategies on a dataset with multiple time series.</li>
    </ul>
    <p class="normal">There are a few points that we need to keep in mind for datasets with multiple time series:</p>
    <ul>
      <li class="bulletList">Do not use different time windows for different time series. This is because different windows in time would have different errors, and that would skew the aggregate error metric we are tracking.</li>
      <li class="bulletList">If different time series have different lengths, align the length of the validation period across all the series. Training length can be different, but validation windows should be the same so that every time series equally contributes to the aggregate error metric.</li>
      <li class="bulletList">It is easy to get carried away by complicated validation schemes, but always keep the technical debt you incur by choosing a specific technique in mind.</li>
    </ul>
    <p class="normal">With that, we have come to the end of a short but important chapter.</p>
    <h1 id="_idParaDest-510" class="heading-1">Summary</h1>
    <p class="normal">We have come to the end of our journey through the world of time series forecasting. In the last couple of chapters, we addressed a few mechanics of forecasting, such as how to do multi-step forecasting and how to evaluate forecasts. Different validation strategies for evaluating forecasts and forecasting models were the topics of the current chapter. </p>
    <p class="normal">We started by enlightening you as to why model validation is an important task. Then, we looked at a few different validation strategies, such as the holdout strategies, and navigated the controversial use of cross-validation for time series. We spent some time summarizing and laying down a few guidelines to be used to select a validation strategy. To top it all off, we looked at how these validation strategies are applicable to datasets with multiple time series and talked about how to adapt them to such scenarios.</p>
    <p class="normal">With that, we have come to the end of the book. Congratulations on making it all the way through, and I hope you have gained enough skills from the book to tackle the next time series problem that comes your way. I strongly urge you to start putting into practice the skills that you have gained from the book because, as Richard Feynman rightly put it, <em class="italic">“You do not know anything until you have practiced.”</em></p>
    <h1 id="_idParaDest-511" class="heading-1">References</h1>
    <p class="normal">The following are the references used in this chapter:</p>
    <ol>
      <li class="numberedList" value="1">Tashman, Len. (2000). <em class="italic">Out-of-sample tests of forecasting accuracy: An analysis and review</em>. International Journal of Forecasting. 16. 437–450. 10.1016/S0169-2070(00)00065-0: <a href="https://www.researchgate.net/publication/223319987_Out-of-sample_tests_of_forecasting_accuracy_An_analysis_and_review"><span class="url">https://www.researchgate.net/publication/223319987_Out-of-sample_tests_of_forecasting_accuracy_An_analysis_and_review</span></a>.</li>
      <li class="numberedList">Bergmeir, Christoph and Benítez, José M. (2012). <em class="italic">On the use of cross-validation for time series predictor evaluation</em>. In Information Sciences, Volume 191, 2012, Pages 192–213: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025511006773"><span class="url">https://www.sciencedirect.com/science/article/abs/pii/S0020025511006773</span></a>.</li>
      <li class="numberedList">Cerqueira, V., Torgo, L., and Mozetič, I. (2020). <em class="italic">Evaluating time series forecasting models: an empirical study on performance estimation methods.</em> Mach Learn 109, 1997–2028 (2020): <a href="https://doi.org/10.1007/s10994-020-05910-7"><span class="url">https://doi.org/10.1007/s10994-020-05910-7</span></a>.</li>
      <li class="numberedList">Snijders, T.A.B. (1988). <em class="italic">On Cross-Validation for Predictor Evaluation in Time Series.</em> In: <em class="italic">Dijkstra, T.K.</em> (eds) <em class="italic">On Model Uncertainty and its Statistical Implications</em>. Lecture Notes in Economics and Mathematical Systems, vol 307. Springer, Berlin, Heidelberg. <a href="https://doi.org/10.1007/978-3-642-61564-1_4"><span class="url">https://doi.org/10.1007/978-3-642-61564-1_4</span></a>.</li>
    </ol>
    <h1 id="_idParaDest-512" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList"><em class="italic">TS-10: Validation methods for time series</em> by<em class="italic"> Konrad Banachewicz</em>: <a href="https://www.kaggle.com/code/konradb/ts-10-validation-methods-for-time-series "><span class="url">https://www.kaggle.com/code/konradb/ts-10-validation-methods-for-time-series</span></a></li>
    </ul>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
  </div>
  <div id="_idContainer1217" class="Basic-Text-Frame">
    <h1 id="_idParaDest-513" class="heading-1">Leave a Review!</h1>
    <p class="normal">Thank you for purchasing this book from Packt Publishing—we hope you enjoyed it! Your feedback is invaluable and helps us improve and grow. Please take a moment to leave an <a href="https://packt.link/r/1835883192"><span class="url">Amazon review</span></a>; it will only take a minute, but it makes a big difference for readers like you.</p>
    <p class="normal">Scan the QR code below to receive a free ebook of your choice.</p>
    <figure class="mediaobject"><img src="../Images/review2.jpg" alt="A qr code with black squares Description automatically generated"/></figure>
	<p class="packt_figref"><a href="https://packt.link/NzOWQ"><span class="url">https://packt.link/NzOWQ</span></a></p>
  </div>


  <div id="_idContainer1221">
    <p class="BM-packtLogo"><img src="../Images/New_Packt_Logo1.png" alt=""/></p>
    <p class="normal"><a href="https://www.packt.com"><span class="url">packt.com</span></a></p>
    <p class="normal">Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.</p>
    <h1 id="_idParaDest-514" class="heading-1">Why subscribe?</h1>
    <ul>
      <li class="bulletList">Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</li>
      <li class="bulletList">Improve your learning with Skill Plans built especially for you</li>
      <li class="bulletList">Get a free eBook or video every month</li>
      <li class="bulletList">Fully searchable for easy access to vital information</li>
      <li class="bulletList">Copy and paste, print, and bookmark content</li>
    </ul>
    <p class="normal">At <a href="https://www.packt.com"><span class="url">www.packt.com</span></a>, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.</p>
    <p class="eop"/>
    <h1 id="_idParaDest-515" class="mainHeading">Other Books You May Enjoy</h1>
    <p class="normal">If you enjoyed this book, you may be interested in these other books by Packt:</p>
    <p class="normal"><a href="https://www.packtpub.com/en-in/product/pandas-cookbook-9781836205869"><img src="../Images/9781836205876.jpg" alt=""/></a></p>
    <p class="normal"><strong class="keyWord">Pandas Cookbook</strong></p>
    <p class="normal">William Ayd, Matthew Harrison</p>
    <p class="normal">ISBN: 9781836205876</p>
    <ul>
      <li class="bulletList">The pandas type system and how to best navigate it</li>
      <li class="bulletList">Import/export DataFrames to/from common data formats</li>
      <li class="bulletList">Data exploration in pandas through dozens of practice problems</li>
      <li class="bulletList">Grouping, aggregation, transformation, reshaping, and filtering data</li>
      <li class="bulletList">Merge data from different sources through pandas SQL-like operations</li>
      <li class="bulletList">Leverage the robust pandas time series functionality in advanced analyses</li>
      <li class="bulletList">Scale pandas operations to get the most out of your system</li>
      <li class="bulletList">The large ecosystem that pandas can coordinate with and supplement</li>
    </ul>
    <p class="normal"><a href="https://www.packtpub.com/en-in/product/mastering-pytorch-9781801079969"><img src="../Images/9781801074308.jpg" alt=""/></a></p>
    <p class="eop"/>
    <p class="normal"><strong class="keyWord">Mastering PyTorch</strong></p>
    <p class="normal">Ashish Ranjan Jha</p>
    <p class="normal">ISBN: 9781801074308</p>
    <ul>
      <li class="bulletList">Implement text, vision, and music generation models using PyTorch</li>
      <li class="bulletList">Build a deep Q-network (DQN) model in PyTorch</li>
      <li class="bulletList">Deploy PyTorch models on mobile devices (Android and iOS)</li>
      <li class="bulletList">Become well versed in rapid prototyping using PyTorch with fastai</li>
      <li class="bulletList">Perform neural architecture search effectively using AutoML</li>
      <li class="bulletList">Easily interpret machine learning models using Captum</li>
      <li class="bulletList">Design ResNets, LSTMs, and graph neural networks (GNNs)</li>
      <li class="bulletList">Create language and vision transformer models using Hugging Face</li>
    </ul>
    <p class="eop"/>
    <h1 id="_idParaDest-516" class="heading-1">Packt is searching for authors like you</h1>
    <p class="normal">If you’re interested in becoming an author for Packt, please visit <a href="https://authors.packtpub.com"><span class="url">authors.packtpub.com</span></a> and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</p>
  </div>
</body></html>