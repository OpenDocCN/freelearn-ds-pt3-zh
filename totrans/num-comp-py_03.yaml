- en: K-Nearest Neighbors and Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we have learned about computationally intensive methods.
    In contrast, this chapter discusses the simple methods to balance it out! We will
    be covering the two techniques, called **k-nearest neighbors** (**KNN**)and Naive
    Bayes here. Before touching on KNN, we explained the issue with the curse of dimensionality
    with a simulated example. Subsequently, breast cancer medical examples have been
    utilized to predict whether the cancer is malignant or benign using KNN. In the
    final section of the chapter, Naive Bayes has been explained with spam/ham classification,
    which also involves the application of the **natural language processing** (**NLP**)
    techniques consisting of the following basic preprocessing and modeling steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Punctuation removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word tokenization and lowercase conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stopwords removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatization with POS tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversion of words into TF-IDF to create numerical representation of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application of the Naive Bayes model on TF-IDF vectors to predict if the message
    is either spam or ham on both train and test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-nearest neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-nearest neighbors is a non-parametric machine learning model in which the
    model memorizes the training observation for classifying the unseen test data.
    It can also be called instance-based learning. This model is often termed as lazy
    learning, as it does not learn anything during the training phase like regression,
    random forest, and so on. Instead, it starts working only during the testing/evaluation
    phase to compare the given test observations with the nearest training observations,
    which will take significant time in comparing each test data point. Hence, this
    technique is not efficient on big data; also, performance does deteriorate when
    the number of variables is high due to the **curse of dimensionality**.
  prefs: []
  type: TYPE_NORMAL
- en: KNN voter example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'KNN is explained better with the following short example. The objective is
    to predict the party for which voter will vote based on their neighborhood, precisely
    geolocation (latitude and longitude). Here we assume that we can identify the
    potential voter to which political party they would be voting based on majority
    voters did vote for that particular party in that vicinity so that they have a
    high probability to vote for the majority party. However, tuning the k-value (number
    to consider, among which majority should be counted) is the million-dollar question
    (as same as any machine learning algorithm):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6e598c7-2e69-459d-b77a-32f57fb361f4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see that the voter of the study will vote for
    **Party 2**. As within the vicinity, one neighbor has voted for **Party 1** and
    the other voter voted for **Party 3**. But three voters voted for **Party 2**.
    In fact, by this way, KNN solves any given classification problem. Regression
    problems are solved by taking mean of its neighbors within the given circle or
    vicinity or k-value.
  prefs: []
  type: TYPE_NORMAL
- en: Curse of dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KNN completely depends on distance. Hence, it is worth studying about the curse
    of dimensionality to understand when KNN deteriorates its predictive power with
    the increase in the number of variables required for prediction. This is an obvious
    fact that high-dimensional spaces are vast. Points in high-dimensional spaces
    tend to be dispersing from each other more compared with the points in low-dimensional
    space. Though there are many ways to check the curve of dimensionality, here we
    are using uniform random values between zero and one generated for 1D, 2D, and
    3D space to validate this hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following lines of codes, the mean distance between 1,000 observations
    has been calculated with the change in dimensions. It is apparent that with the
    increase in dimensions, distance between points increases logarithmically, which
    gives us the hint that we need to have an exponential increase in data points
    with increase in dimensions in order to make machine learning algorithms work
    correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code generates random numbers between zero and one from uniform
    distribution with the given dimension, which is equivalent of length of array
    or list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function calculates root mean sum of squares of Euclidean distances
    (2-norm) between points by taking the difference between points and sum the squares
    and finally takes the square root of total distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Both dimension and number of pairs are utilized for calculating the distances
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The experiment has been done by changing dimensions from 1 to 201 with an increase
    of 5 dimensions to check the increase in distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Both minimum and average distances have been calculated to check, however,
    both illustrate the similar story:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/befd95be-272c-43f4-81a1-8595f9aa4d17.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding graph, it is proved that with the increase in dimensions,
    mean distance increases logarithmically. Hence the higher the dimensions, the
    more data is needed to overcome the curse of dimensionality!
  prefs: []
  type: TYPE_NORMAL
- en: Curse of dimensionality with 1D, 2D, and 3D example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A quick analysis has been done to see how distance 60 random points are expanding
    with the increase in dimensionality. Initially, random points are drawn for one-dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If we observe the following graph, all 60 data points are very nearby in one-dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e05c54d8-cedf-4930-90ac-361d35736b77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we are repeating the same experiment in a 2D space, by taking 60 random
    numbers with *x* and *y* coordinate space and plotted them visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'By observing the 2D graph we can see that more gaps have been appearing for
    the same 60 data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9e699fd-c223-4755-948a-53ccbfef48c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, 60 data points are drawn for 3D space. We can see a further increase
    in spaces, which is very apparent. This has proven to us visually by now that
    with the increase in dimensions, it creates a lot of space, which makes a classifier
    weak to detect the signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d36eb8b9-0634-45b7-8c12-41eb2711b1a6.png)'
  prefs: []
  type: TYPE_IMG
- en: KNN classifier with breast cancer Wisconsin data example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Breast cancer data has been utilized from the UCI machine learning repository [http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)
    for illustration purposes. Here the task is to find whether the cancer is malignant
    or benign based on various collected features such as clump thickness and so on
    using the KNN classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the first few rows to show how the data looks like. The `Class`
    value has class `2` and `4`. Value `2` and `4` represent benign and malignant
    class, respectively. Whereas all the other variables do vary between value `1`
    and `10`, which are very much categorical in nature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66458e11-de44-4ba8-9bbd-d96eacae2b68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Only the `Bare_Nuclei` variable has some missing values, here we are replacing
    them with the most frequent value (category value `1`) in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to convert the classes to a `0` and `1` indicator for
    using in the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are dropping non-value added variables from analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As KNN is very sensitive to distances, here we are standardizing all the columns
    before applying algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'KNN classifier is being applied with neighbor value of `3` and `p` value indicates
    it is 2-norm, also known as Euclidean distance for computing classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/915405ac-f99e-4459-9f70-e50a7cd73812.png)'
  prefs: []
  type: TYPE_IMG
- en: From the results, it is appearing that KNN is working very well in classifying
    malignant and benign classes well, obtaining test accuracy of 97.6 percent with
    96 percent of recall on malignant class. The only deficiency of KNN classifier
    would be, it is computationally intensive during test phase, as each test observation
    will be compared with all the available observations in train data, which practically
    KNN does not learn a thing from training data. Hence, we are also calling it a
    lazy classifier!
  prefs: []
  type: TYPE_NORMAL
- en: 'The R code for KNN classifier is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Tuning of k-value in KNN classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we just checked with only the k-value of three. Actually,
    in any machine learning algorithm, we need to tune the knobs to check where the
    better performance can be obtained. In the case of KNN, the only tuning parameter
    is k-value. Hence, in the following code, we are determining the best k-value
    with grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d729f290-0d04-4a49-9cd4-540513d9c272.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It appears that with less value of k-value, it has more overfitting problems
    due to the very high value of accuracy on train data and less on test data, with
    the increase in k-value more the train and test accuracies are converging and
    becoming more robust. This phenomenon illustrates the typical machine learning
    phenomenon. As for further analysis, readers are encouraged to try k-values higher
    than five and see how train and test accuracies are changing. The R code for tuning
    of k-value in KNN classifier is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayes algorithm concept is quite old and exists from the 18th century. Thomas
    Bayes developed the foundational mathematical principles for determining the probability
    of unknown events from the known events. For example, if all apples are red in
    color and average diameter would be about 4 inches then, if at random one fruit
    is selected from the basket with red color and diameter of 3.7 inches, what is
    the probability that the particular fruit would be an apple? Naive term does assume
    independence of particular features in a class with respect to others. In this
    case, there would be no dependency between color and diameter. This independence
    assumption makes the Naive Bayes classifier most effective in terms of computational
    ease for particular tasks such as email classification based on words in which
    high dimensions of vocab do exist, even after assuming independence between features.
    Naive Bayes classifier performs surprisingly really well in practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian classifiers are best applied to problems in which information from
    a very high number of attributes should be considered simultaneously to estimate
    the probability of final outcome. Bayesian methods utilize all available evidence
    to consider for prediction even features have weak effects on the final outcome
    to predict. However, we should not ignore the fact that a large number of features
    with relatively minor effects, taken together its combined impact would form strong
    classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Probability fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into Naive Bayes, it would be good to reiterate the fundamentals.
    Probability of an event can be estimated from observed data by dividing the number
    of trails in which an event occurred with the total number of trails. For instance,
    if a bag contains red and blue balls and randomly picked *10* balls one by one
    with replacement and out of *10*, *3* red balls appeared in trails we can say
    that probability of red is *0.3*, *p[red] = 3/10 = 0.3*. Total probability of
    all possible outcomes must be 100 percent.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a trail has two outcomes such as email classification either it is spam
    or ham and both cannot occur simultaneously, these events are considered as mutually
    exclusive with each other. In addition, if those outcomes cover all possible events,
    it would be called as **exhaustive events**. For example, in email classification
    if *P (spam) = 0.1*, we will be able to calculate *P (ham) = 1- 0.1 = 0.9*, these
    two events are mutually exclusive. In the following Venn diagram, all the email
    possible classes are represented (the entire universe) with the type of outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c902cba-e1f7-4e92-a1e4-79ad6f2499b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Joint probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though mutually exclusive cases are simple to work upon, most of the actual
    problems do fall under the category of non-mutually exclusive events. By using
    the joint appearance, we can predict the event outcome. For example, if emails
    messages present the word like *lottery*, which is very highly likely of being
    spam rather than ham. The following Venn diagram indicates the joint probability
    of spam with *lottery*. However, if you notice in detail, lottery circle is not
    contained completely within the spam circle. This implies that not all spam messages
    contain the word *lottery* and not every email with the word *lottery* is spam.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4b84585-55e4-4155-8fec-f8d907e9e476.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following diagram, we have expanded the spam and ham category in addition
    to the *lottery* word in Venn diagram representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6499a1f6-038f-48c8-ba33-b628ef50fb8b.png)'
  prefs: []
  type: TYPE_IMG
- en: We have seen that 10 percent of all the emails are spam and 4 percent of emails
    have the word *lottery* and our task is to quantify the degree of overlap between
    these two proportions. In other words, we need to identify the joint probability
    of both *p(spam)* and *p(lottery)* occurring, which can be written as *p(spam
    ∩ lottery)*. In case if both the events are totally unrelated, they are called
    **independent events** and their respective value is *p(spam ∩ lottery) = p(spam)
    * p(lottery) = 0.1 * 0.04 = 0.004*, which is 0.4 percent of all messages are spam
    containing the word Lottery. In general, for independent events *P(A∩ B) = P(A)
    * P(B)*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Bayes theorem with conditional probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Conditional probability provides a way of calculating relationships between
    dependent events using Bayes theorem. For example, *A* and *B* are two events
    and we would like to calculate *P(A\B)* can be read as the probability of an event
    occurring *A* given the fact that event *B* already occurred, in fact, this is
    known as **conditional probability**, the equation can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0123470a-764e-4575-a679-9e30c8340e0e.png)'
  prefs: []
  type: TYPE_IMG
- en: To understand better, we will now talk about the email classification example.
    Our objective is to predict whether an email is a spam given the word lottery
    and some other clues. In this case, we already knew the overall probability of
    spam, which is 10 percent also known as **prior probability**. Now suppose you
    have obtained an additional piece of information that probability of word lottery
    in all messages, which is 4 percent, also known as **marginal likelihood**. Now,
    we know the probability that *lottery* was used in previous spam messages and
    is called the **likelihood**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d3a04f4-4cd8-42e8-b577-71e1f8e778e5.png)'
  prefs: []
  type: TYPE_IMG
- en: By applying the Bayes theorem to the evidence, we can calculate the posterior
    probability that calculates the probability that the message is how likely a spam;
    given the fact that lottery was appearing in the message. On average if the probability
    is greater than 50 percent it indicates that the message is spam rather than ham.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a22f2b5e-d947-4383-a267-42b02ebbc4d6.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous table, the sample frequency table that records the number of
    times *Lottery* appeared in spam and ham messages and its respective likelihood
    has been shown. Likelihood table reveals that *P(Lottery\Spam)= 3/22 = 0.13*,
    indicating that probability is 13 percent that a spam message contains the term
    *Lottery*. Subsequently we can calculate the *P(Spam ∩ Lottery) = P(Lottery\Spam)
    * P(Spam) = (3/22) * (22/100) = 0.03*. In order to calculate the posterior probability,
    we divide *P(Spam ∩ Lottery)* with *P(Lottery)*, which means *(3/22)*(22/100)
    / (4/100) = 0.75*. Therefore, the probability is 75 percent that a message is
    spam, given that message contains the word *Lottery*. Hence, don't believe in
    quick fortune guys!
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the past example, we have seen with a single word called *lottery*, however,
    in this case, we will be discussing with a few more additional words such as *Million* and
    *Unsubscribe* to show how actual classifiers do work. Let us construct the likelihood
    table for the appearance of the three words (*W1*, *W2*, and *W3*), as shown in
    the following table for *100* emails:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a213ad61-ceab-4f9a-b03a-1d86c3651334.png)'
  prefs: []
  type: TYPE_IMG
- en: When a new message is received, the posterior probability will be calculated
    to determine that email message is spam or ham. Let us assume that we have an
    email with terms *Lottery* and *Unsubscribe*, but it does not have word *Million*
    in it, with this details, what is the probability of spam?
  prefs: []
  type: TYPE_NORMAL
- en: 'By using Bayes theorem, we can define the problem as *Lottery = Yes*, *Million
    = No* and *Unsubscribe = Yes*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c004c3e-e6bc-43d3-9163-710c9b5798be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Solving the preceding equations will have high computational complexity due
    to the dependency of words with each other. As a number of words are added, this
    will even explode and also huge memory will be needed for processing all possible
    intersecting events. This finally leads to intuitive turnaround with independence
    of words (**cross-conditional independence**) for which it got name of the Naive
    prefix for Bayes classifier. When both events are independent we can write *P(A
    ∩ B) = P(A) * P(B)*. In fact, this equivalence is much easier to compute with
    less memory requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c4917c8-5737-4273-bf9d-8e906149c672.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a similar way, we will calculate the probability for ham messages as well,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0510142-46a4-4834-8928-885388599293.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By substituting the preceding likelihood table in the equations, due to the
    ratio of spam/ham we can just simply ignore the denominator terms in both the
    equations. Overall likelihood of spam is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3008e654-9d2b-42b9-9f29-4cdd2f619bd8.png)![](img/f26373a6-03d9-45f1-839a-e033b6d9ded8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After calculating the ratio, *0.008864/0.004349 = 2.03*, which means that this
    message is two times more likely to be spam than ham. But we can calculate the
    probabilities as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Spam) = 0.008864/(0.008864+0.004349) = 0.67*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(Ham) = 0.004349/(0.008864+0.004349) = 0.33*'
  prefs: []
  type: TYPE_NORMAL
- en: By converting likelihood values into probabilities, we can show in a presentable
    way for either to set-off some thresholds, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Laplace estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous calculation, all the values are nonzeros, which makes calculations
    well. Whereas in practice some words never appear in past for specific category
    and suddenly appear at later stages, which makes entire calculations as zeros.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the previous equation *W[3]* did have a *0* value instead of
    *13*, and it will convert entire equations to *0* altogether:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4613c8e9-71b9-43dd-95e1-607855b3edf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to avoid this situation, Laplace estimator essentially adds a small
    number to each of the counts in the frequency table, which ensures that each feature
    has a nonzero probability of occurring with each class. Usually, Laplace estimator
    is set to *1*, which ensures that each class-feature combination is found in the
    data at least once:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ed79e3d-5253-46f6-9efb-7ba0d5737898.png)'
  prefs: []
  type: TYPE_IMG
- en: If you observe the equation carefully, value *1* is added to all three words
    in the numerator and at the same time, three has been added to all denominators
    to provide equivalence.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes SMS spam classification example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Naive Bayes classifier has been developed using the SMS spam collection data
    available at [http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).
    In this chapter, various techniques available in NLP techniques have been discussed
    to preprocess prior to build the Naive Bayes model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `sys` package lines code can be used in case of any `utf-8` errors
    encountered while using older versions of Python, or else does not necessary with
    the latest version of Python 3.6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Normal coding starts from here as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code prints the top 5 lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f57add7c-995c-4e5c-8979-de794701d32e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After getting preceding output run following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/99598fba-a2c0-42ac-ae0e-f3cd9e01cccd.png)'
  prefs: []
  type: TYPE_IMG
- en: Out of 5,572 observations, 4,825 are ham messages, which are about 86.5 percent
    and 747 spam messages are about remaining 13.4 percent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using NLP techniques, we have preprocessed the data for obtaining finalized
    word vectors to map with final outcomes spam or ham. Major preprocessing stages
    involved are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Removal of punctuations**: Punctuations needs to be removed before applying
    any further processing. Punctuations from the `string` library are *!"#$%&\''()*+,-./:;<=>?@[\\]^_`{|}~*,
    which are removed from all the messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word tokenization**: Words are chunked from sentences based on white space
    for further processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Converting words into lowercase**: Converting to all lower case provides
    removal of duplicates, such as *Run* and *run*, where the first one comes at start
    of the sentence and the later one comes in the middle of the sentence, and so
    on, which all needs to be unified to remove duplicates as we are working on bag
    of words technique.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop word removal**: Stop words are the words that repeat so many times in
    literature and yet are not a differentiator in the explanatory power of sentences.
    For example: *I*, *me*, *you*, *this*, *that*, and so on, which needs to be removed
    before further processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**of length at least three**: Here we have removed words with length less than
    three.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stemming of words**: Stemming process stems the words to its respective root
    words. Example of stemming is bringing down running to run or runs to run. By
    doing stemming we reduce duplicates and improve the accuracy of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Part-of-speech (POS) tagging**:  This applies the speech tags to words, such
    as noun, verb, adjective, and so on. For example, POS tagging for *running* is
    verb, whereas for *run* is noun. In some situation *running* is noun and lemmatization
    will not bring down the word to root word *run*, instead, it just keeps the *running*
    as it is. Hence, POS tagging is a very crucial step necessary for performing prior
    to applying the lemmatization operation to bring down the word to its root word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lemmatization of words**: Lemmatization is another different process to reduce
    the dimensionality. In lemmatization process, it brings down the word to root
    word rather than just truncating the words. For example, bring *ate* to its root
    word as *eat* when we pass the *ate* word into lemmatizer with the POS tag as
    verb.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `nltk` package has been utilized for all the preprocessing steps, as it
    consists of all the necessary NLP functionality in one single roof:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Function has been written (preprocessing) consists of all the steps for convenience.
    However, we will be explaining all the steps in each section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line of the code splits the word and checks each character if
    it is in standard punctuations if so it will be replaced with blank and or else
    it just does not replace with blanks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code tokenizes the sentences into words based on white spaces
    and put them together as a list for applying further steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Converting all the cases (upper, lower, and proper) into lowercase reduces
    duplicates in corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, stop words are the words that do not carry much weight
    in understanding the sentence; they are used for connecting words, and so on.
    We have removed them with the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Keeping only the words with length greater than `3` in the following code for
    removing small words, which hardly consists of much of a meaning to carry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Stemming is applied on the words using `PorterStemmer` function, which stems
    the extra suffixes from the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'POS tagging is a prerequisite for lemmatization, based on whether the word
    is noun or verb, and so on, it will reduce it to the root word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pos_tag` function returns the part of speed in four formats for noun and
    six formats for verb. `NN` (noun, common, singular), `NNP` (noun, proper, singular),
    `NNPS` (noun, proper, plural), `NNS` (noun, common, plural), `VB` (verb, base
    form), `VBD` (verb, past tense), `VBG` (verb, present participle), `VBN` (verb,
    past participle), `VBP` (verb, present tense, not third person singular), `VBZ` (verb,
    present tense, third person singular):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The `prat_lemmatize` function has been created only for the reasons of mismatch
    between the `pos_tag` function and intake values of the lemmatize function. If
    the tag for any word falls under the respective noun or verb tags category, `n`
    or `v` will be applied accordingly in the lemmatize function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'After performing tokenization and applied all the various operations, we need
    to join it back to form stings and the following function performs the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following step applies the preprocessing function to the data and generates
    new corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Data will be split into train and test based on 70-30 split and converted to
    the NumPy array for applying machine learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code converts the words into a vectorizer format and applies
    **term frequency-inverse document frequency** (**TF-IDF**) weights, which is a
    way to increase weights to words with high frequency and at the same time penalize
    the general terms such as *the*, *him*, *at*, and so on. In the following code,
    we have restricted to most frequent 4,000 words in the vocabulary, none the less
    we can tune this parameter as well for checking where the better accuracies are
    obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The TF-IDF transformation has been shown as follows on both train and test
    data. The `todense` function is used to create the data to visualize the content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Multinomial Naive Bayes classifier is suitable for classification with discrete
    features (example word counts), which normally requires large feature counts.
    However, in practice, fractional counts such as TF-IDF will also work well. If
    we do not mention any Laplace estimator, it does take the value of *1.0* means
    and it will add *1.0* against each term in numerator and total for denominator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/34b61efc-2814-4234-8d08-21c2e68c1264.png)'
  prefs: []
  type: TYPE_IMG
- en: From the previous results, it is appearing that Naive Bayes has produced excellent
    results of 96.6 percent test accuracy with significant recall value of 76 percent for
    spam and almost 100 percent for ham.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we would like to check what are the top 10 features based on their
    coefficients from Naive Bayes, the following code will be handy for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b31d330c-79e0-405f-a60d-08a3940ea1b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Though the R language is not a popular choice for NLP processing, here we have
    presented the code. Readers are encouraged to change the code and see how accuracies
    are changing for a better understanding of concepts. The R code for Naive Bayes
    classifier on SMS spam/ham data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned about KNN and Naive Bayes techniques, which
    require somewhat a little less computational power. KNN, in fact, is called a
    lazy learner, as it does not learn anything apart from comparing with training
    data points to classify them into class. Also, you have seen how to tune the k-value
    using grid search technique. Whereas explanation has been provided for Naive Bayes
    classifier, NLP examples have been provided with all the famous NLP processing
    techniques to give you a flavor of this field in a very crisp manner. Though in
    text processing, either Naive Bayes or SVM techniques could be used as these two
    techniques can handle data with high dimensionality, which is very relevant in
    NLP, as the number of word vectors is relatively high in dimensions and sparse
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be covering the details of unsupervised learning,
    more precisely, clustering and principal component analysis models.
  prefs: []
  type: TYPE_NORMAL
