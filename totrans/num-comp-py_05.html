<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p><strong>Reinforcement learning</strong> (<strong>RL</strong>) is the third major section of machine learning after supervised and unsupervised learning. These techniques have gained a lot of traction in recent years in the application of artificial intelligence. In reinforcement learning, sequential decisions are to be made rather than one shot decision making, which makes it difficult to train the models in a few cases. In this chapter, we would be covering various techniques used in reinforcement learning with practical examples to support with. Though covering all topics are beyond the scope of this book, but we did cover the most important fundamentals here for a reader to create enough enthusiasm on this subject. Topics discussed in this chapter are:</p>
<ul>
<li>Markov decision process</li>
<li>Bellman equations</li>
<li>Dynamic programming</li>
<li>Monte Carlo methods</li>
<li>Temporal difference learning</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning basics</h1>
                </header>
            
            <article>
                
<p>Before we deep dive into the details of reinforcement learning, I would like to cover some of the basics necessary for understanding the various nuts and bolts of RL methodologies. These basics appear across various sections of this chapter, which we will explain in detail whenever required:</p>
<ul>
<li><strong>Environment:</strong> This is any system that has states, and mechanisms to transition between states. For example, the environment for a robot is the landscape or facility it operates.</li>
<li><strong>Agent:</strong> This is an automated system that interacts with the environment.</li>
<li><strong>State:</strong> The state of the environment or system is the set of variables or features that fully describe the environment.</li>
<li><strong>Goal or absorbing state or terminal state:</strong> This is the state that provides a higher discounted cumulative reward than any other state. A high cumulative reward prevents the best policy from being dependent on the initial state during training. Whenever an agent reaches its goal, we will finish one episode.</li>
<li><strong>Action:</strong> This defines the transition between states. The agent is responsible for performing, or at least recommending an action. Upon execution of the action, the agent collects a reward (or punishment) from the environment.</li>
<li><strong>Policy:</strong> This defines the action to be selected and executed for any state of the environment. In other words, policy is the agent's behavior; it is a map from state to action. Policies could be either deterministic or stochastic.</li>
<li><strong>Best policy:</strong> This is the policy generated through training. It defines the model in Q-learning and is constantly updated with any new episode.</li>
<li><strong>Rewards</strong>: This quantifies the positive or negative interaction of the agent with the environment. Rewards are usually immediate earnings made by the agent reaching each state.</li>
<li><strong>Returns or value function</strong>: A value function (also called returns) is a prediction of future rewards of each state. These are used to evaluate the goodness/badness of the states, based on which, the agent will choose/act on for selecting the next best state:</li>
</ul>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/97df1bcf-51c9-4747-a6fe-23daa16edd22.jpg" style="width:24.75em;height:1.83em;"/></div>
<ul>
<li><strong>Episode:</strong> This defines the number of steps necessary to reach the goal state from an initial state. Episodes are also known as trials.</li>
<li><strong>Horizon:</strong> This is the number of future steps or actions used in the maximization of the reward. The horizon can be infinite, in which case, the future rewards are discounted in order for the value of the policy to converge.</li>
<li><strong>Exploration versus Exploitation:</strong> RL is a type of trial and error learning. The goal is to find the best policy; and at the same time, remain alert to explore some unknown policies. A classic example would be treasure hunting: if we just go to the locations greedily (exploitation), we fail to look for other places where hidden treasure might also exist (exploration). By exploring the unknown states, and by taking chances, even when the immediate rewards are low and without losing the maximum rewards, we might achieve greater goals. In other words, we are escaping the local optimum in order to achieve a global optimum (which is exploration), rather than just a short-term focus purely on the immediate rewards (which is exploitation). Here are a couple of examples to explain the difference:
<ul>
<li><strong>Restaurant selection</strong>: By exploring unknown restaurants once in a while, we might find a much better one than our regular favorite restaurant:
<ul>
<li><strong>Exploitation</strong>: Going to your favorite restaurant</li>
<li><strong>Exploration</strong>: Trying a new restaurant</li>
</ul>
</li>
<li><strong>Oil drilling example:</strong> By exploring new untapped locations, we may get newer insights that are more beneficial that just exploring the same place:
<ul>
<li><strong>Exploitation</strong>: Drill for oil at best known location</li>
<li><strong>Exploration</strong>: Drill at a new location</li>
</ul>
</li>
</ul>
</li>
<li><strong>State-Value versus State-Action Function:</strong> In action-value, Q represents the expected return (cumulative discounted reward) an agent is to receive when taking Action <em>A</em> in State <em>S</em>, and behaving according to a certain policy π(a|s) afterwards (which is the probability of taking an action in a given state).</li>
</ul>
<p style="padding-left: 90px">In state-value, the value is the expected return an agent is to receive from being in state <em>s</em> behaving under a policy <em>π(a|s)</em>. More specifically, the state-value is an expectation over the action-values under a policy:</p>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/9b1b62a2-aae2-4d68-bea1-4ca30750e064.jpg" style="width:14.00em;height:3.58em;"/></div>
<ul>
<li><strong>On-policy versus off-policy TD control:</strong> An off-policy learner learns the value of the optimal policy independently of the agent's actions. Q-learning is an off-policy learner. An on-policy learner learns the value of the policy being carried out by the agent, including the exploration steps.</li>
<li><strong>Prediction and control problems:</strong> Prediction talks about how well I do, based on the given policy: meaning, if someone has given me a policy and I implement it, how much reward I will get for that. Whereas, in control, the problem is to find the best policy so that I can maximize the reward.</li>
<li><strong>Prediction:</strong> Evaluation of the values of states for a given policy.</li>
</ul>
<p style="padding-left: 120px">For the uniform random policy, what is the value function for all states?</p>
<ul>
<li><strong>Control:</strong> Optimize the future by finding the best policy.</li>
</ul>
<p style="padding-left: 90px">What is the optimal value function over all possible policies, and what is the optimal policy?</p>
<p style="padding-left: 90px">Usually, in reinforcement learning, we need to solve the prediction problem first, in order to solve the control problem after, as we need to figure out all the policies to figure out the best or optimal one.</p>
<ul>
<li><strong>RL Agent Taxonomy:</strong> An RL agent includes one or more of the following components:
<ul>
<li><strong>Policy:</strong> Agent's behavior function (map from state to action); Policies can be either deterministic or stochastic</li>
<li><strong>Value function:</strong> How good is each state (or) prediction of expected future reward for each state</li>
<li><strong>Model:</strong> Agent's representation of the environment. A model predicts what the environment will do next:
<ul>
<li><strong>Transitions:</strong> p predicts the next state (that is, dynamics):</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/88cda7e7-bfe8-482d-ad09-c50612f77308.jpg" style="width:17.17em;height:1.58em;"/></div>
<ul>
<li style="list-style-type: none">
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Rewards:</strong> R predicts the next (immediate) reward</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="padding-left: 150px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/036b8b26-fd87-4bd1-8172-8231499871f4.jpg" style="width:16.00em;height:1.92em;"/></div>
<p>Let us explain the various categories possible in RL agent taxonomy, based on combinations of policy and value, and model individual components with the following maze example. In the following maze, you have both the start and the goal; the agent needs to reach the goal as quickly as possible, taking a path to gain the total maximum reward and the minimum total negative reward. Majorly five categorical way this problem can be solved:</p>
<ul>
<li>Value based</li>
<li>Policy based</li>
<li>Actor critic</li>
<li>Model free</li>
<li>Model based</li>
</ul>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/20718ee8-c5f3-4aa4-b4ed-6ff3e9ec9b60.png" style="width:20.17em;height:18.33em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Category 1 - value based </h1>
                </header>
            
            <article>
                
<p>Value function does look like the right-hand side of the image (the sum of discounted future rewards) where every state has some value. Let's say, the state one step away from the goal has a value of -1; and two steps away from the goal has a value of -2. In a similar way, the starting point has a value of -16. If the agent gets stuck in the wrong place, the value could be as much as -24. In fact, the agent does move across the grid based on the best possible values to reach its goal. For example, the agent is at a state with a value of -15. Here, it can choose to move either north or south, so it chooses to move north due to the high reward, which is -14 rather, than moving south, which has a value of -16. In this way, the agent chooses its path across the grid until it reaches the goal.</p>
<ul>
<li><strong>Value Function</strong>: Only values are defined at all states</li>
<li><strong>No Policy (Implicit)</strong>: No exclusive policy is present; policies are chosen based on the values at each state</li>
</ul>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/273b4f5e-d2bc-440c-a62b-eecfd30159cd.png" style="width:49.00em;height:23.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Category 2 - policy based </h1>
                </header>
            
            <article>
                
<p>The arrows in the following image represent what an agent chooses as the direction of the next move while in any of these states. For example, the agent first moves east and then north, following all the arrows until the goal has been reached. This is also known as mapping from states to actions. Once we have this mapping, an agent just needs to read it and behave accordingly.</p>
<ul>
<li><strong>Policy</strong>: Policies or arrows that get adjusted to reach the maximum possible future rewards. As the name suggests, only policies are stored and optimized to maximize rewards.</li>
<li><strong>No value function</strong>: No values exist for the states.</li>
</ul>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/eba78992-d0d0-4236-8fda-8d8ee466cbc8.png" style="width:26.50em;height:22.17em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Category 3 - actor-critic</h1>
                </header>
            
            <article>
                
<p>In Actor-Critic, we have both policy and value functions (or a combination of value-based and policy-based). This method is the best of both worlds:</p>
<ul>
<li>Policy</li>
<li>Value Function</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Category 4 - model-free</h1>
                </header>
            
            <article>
                
<p>In RL, a fundamental distinction is if it is model-based or model-free. In model-free, we do not explicitly model the environment, or we do not know the entire dynamics of a complete environment. Instead, we just go directly to the policy or value function to gain the experience and figure out how the policy affects the reward:</p>
<ul>
<li>Policy and/or value function
<ul>
<li>No model</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Category 5 - model-based</h1>
                </header>
            
            <article>
                
<p>In model-based RL, we first build the entire dynamics of the environment:</p>
<ul>
<li>Policy and/or value function</li>
<li>Model</li>
</ul>
<p>After going through all the above categories, the following Venn diagram shows the entire landscape of the taxonomy of an RL agent at one single place. If you pick up any paper related to reinforcement learning, those methods can fit in within any section of this landscape.</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/c859d161-fdc0-4d4f-af65-fb4ddfec3627.png" style="width:26.42em;height:26.08em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fundamental categories in sequential decision making</h1>
                </header>
            
            <article>
                
<p>There are two fundamental types of problems in sequential decision making:</p>
<ul>
<li><strong>Reinforcement learning</strong> (for example, autonomous helicopter, and so on):
<ul>
<li>Environment is initially unknown</li>
<li>Agent interacts with the environment and obtain policies, rewards, values from the environment</li>
<li>Agent improves its policy</li>
</ul>
</li>
<li><strong>Planning</strong> (for example, chess, Atari games, and so on):
<ul>
<li>Model of environment or complete dynamics of environment is known</li>
<li>Agent performs computation with its model (without any external interaction)</li>
<li>Agent improves its policy</li>
<li>These are the type of problems also known as reasoning, searching, introspection, and so on</li>
</ul>
</li>
</ul>
<p>Though the preceding two categories can be linked together as per the given problem, but this is basically a broad view of the two types of setups.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Markov decision processes and Bellman equations</h1>
                </header>
            
            <article>
                
<p><strong>Markov decision process</strong> (<strong>MDP</strong>) formally describes an environment for reinforcement learning. Where:</p>
<ul>
<li>Environment is fully observable</li>
<li>Current state completely characterizes the process (which means the future state is entirely dependent on the current state rather than historic states or values)</li>
<li>Almost all RL problems can be formalized as MDPs (for example, optimal control primarily deals with continuous MDPs)</li>
</ul>
<p><strong>Central idea of MDP:</strong> MDP works on the simple Markovian property of a state; for example, <em>S<sub>t+1</sub></em> is entirely dependent on latest state <em>S<sub>t</sub></em> rather than any historic dependencies. In the following equation, the current state captures all the relevant information from the history, which means the current state is a sufficient statistic of the future:</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/923c18bd-709d-4898-8198-f2dea3196190.jpg" style="width:23.33em;height:2.67em;"/></div>
<p>An intuitive sense of this property can be explained with the autonomous helicopter example: the next step is for the helicopter to move either to the right, left, to pitch, or to roll, and so on, entirely dependent on the current position of the helicopter, rather than where it was five minutes before.</p>
<p><strong>Modeling of MDP:</strong> RL problems models the world using MDP formulation as a five tuple (<em>S, A, {P<sub>sa</sub>}, y, R</em>)</p>
<ul>
<li><em>S</em> - Set of States (set of possible orientations of the helicopter)</li>
<li><em>A</em> - Set of Actions (set of all possible positions that can pull the control stick)</li>
<li><em>P<sub>sa</sub></em> - State transition distributions (or state transition probability distributions) provide transitions from one state to another and the respective probabilities needed for the Markov process:</li>
</ul>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/db779c49-a499-4e9b-a735-39c6f3337401.jpg" style="width:18.58em;height:4.58em;"/></div>
<div style="padding-left: 60px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/4f549720-521a-45c0-9ebd-aa388abc1c26.jpg" style="width:36.08em;height:8.83em;"/></div>
<ul>
<li>γ - Discount factor:</li>
</ul>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/f9570224-0dcc-4670-bad4-c9a0f269444c.jpg" style="width:6.42em;height:2.25em;"/></div>
<ul>
<li>R - Reward function (maps set of states to real numbers, either positive or negative):</li>
</ul>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/52688e9d-6663-4453-ae5b-f429b7d6eef4.jpg" style="width:6.50em;height:1.92em;"/></div>
<p>Returns are calculated by discounting the future rewards until terminal state is reached.</p>
<p><strong>Bellman Equations for MDP:</strong> Bellman equations are utilized for the mathematical formulation of MDP, which will be solved to obtain the optimal policies of the environment. Bellman equations are also known as <strong>dynamic programming equations</strong> and are a necessary condition for the optimality associated with the mathematical optimization method that is known as dynamic programming. Bellman equations are linear equations which can be solvable for the entire environment. However, the time complexity for solving these equations is <em>O (n<sup>3</sup>)</em>, which becomes computationally very expensive when the number of states in an environment is large; and sometimes, it is not feasible to explore all the states because the environment itself is very large. In those scenarios, we need to look at other ways of solving problems.</p>
<p>In Bellman equations, value function can be decomposed into two parts:</p>
<ul>
<li>Immediate reward <em>R<sub>t+1</sub></em>, from the successor state you will end up with</li>
<li>Discounted value of successor states <em>yv(S<sub>t+1</sub>)</em> you will get from that timestep onwards:</li>
</ul>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/74cb4e86-c22d-4b73-9204-3fefb68e908f.jpg" style="width:12.00em;height:1.92em;"/></div>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/b0aef024-1db2-439c-bc0c-dec7e0b7ea20.jpg" style="width:24.42em;height:1.92em;"/></div>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/6828d717-c803-4810-8a1d-8ed77e5462d8.jpg" style="width:24.67em;height:2.00em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/eee8b434-ddc4-4fe4-8231-33e4479447a7.jpg" style="width:17.67em;height:1.83em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/b05a1d44-b098-45b7-96dd-b8f2a8996362.jpg" style="width:18.25em;height:1.92em;"/></div>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/865ad437-11e1-4a35-9d51-65873a08a4d0.jpg" style="width:20.83em;height:1.92em;"/></div>
<p><strong>Grid world example of MDP:</strong> Robot navigation tasks live in the following type of grid world. An obstacle is shown the cell (2,2), through which the robot can't navigate. We would like the robot to move to the upper-right cell (4,3) and when it reaches that position, the robot will get a reward of +1. The robot should avoid the cell (4,2), as, if it moved into that cell, it would receive a-1 reward.</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/01aa79ca-4cde-4eb6-bfea-81a7fc272b62.png" style="width:16.67em;height:12.58em;"/></div>
<p>Robot can be in any of the following positions:</p>
<ul>
<li><em>11 States</em> - (except cell (2,2), in which we have got an obstacle for the robot)</li>
<li>A = {N-north, S-south, E-east, W-west}</li>
</ul>
<p>In the real world, robot movements are noisy, and a robot may not be able to move exactly where it has been asked to. Examples might include that some of its wheels slipped, its parts were loosely connected, it had incorrect actuators, and so on. When asked to move by 1 meter, it may not be able to move exactly 1 meter; instead, it may move 90-105 centimeters, and so on.</p>
<p>In a simplified grid world, stochastic dynamics of a robot can be modeled as follows. If we command the robot to go north, there is a 10% chance that the robot could drag towards the left and a 10 % chance that it could drag towards the right. Only 80 percent of the time it may actually go north. When a robot bounces off the wall (including obstacles) and just stays at the same position, nothing happens:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/941f1293-81d5-498e-93ef-7b181cda00b5.png" style="width:15.50em;height:8.00em;"/></div>
<p>Every state in this grid world example is represented by (x, y) coordinates. Let's say it is at state (3,1) and we asked the robot to move north, then the state transition probability matrices are as follows:</p>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/02519596-96cd-4180-b594-c978f3eb763f.jpg" style="width:26.00em;height:2.50em;"/></div>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/63f2607c-00dd-45aa-8fb6-c0f155aa777d.jpg" style="width:10.58em;height:1.92em;"/></div>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/399f84ac-a979-417a-8946-c114a76ea574.jpg" style="width:10.92em;height:2.00em;"/></div>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/6c3be965-fa7b-42ad-9a3d-39ea351993a5.jpg" style="width:11.17em;height:2.00em;"/></div>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/09268d1d-7300-4bcb-a092-ff56615f9e03.jpg" style="width:10.42em;height:1.75em;"/></div>
<p>The probability of staying in the same position is 0 for the robot.</p>
<p>As we know, that sum of all the state transition probabilities sums up to 1:</p>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/d3c6be8f-f270-454a-a54e-3baf328a9a86.jpg" style="width:9.67em;height:4.17em;"/></div>
<p>Reward function:</p>
<div style="padding-left: 150px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/0188acae-45ca-4cf2-81cd-cf8c22722737.jpg" style="width:7.58em;height:1.75em;"/></div>
<div style="padding-left: 150px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/71493e81-cbf5-4b56-9791-1b67a42e5382.jpg" style="width:7.83em;height:1.75em;"/></div>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/58dac8d9-b8d7-40e4-8902-689333af3cff.jpg" style="width:20.67em;height:1.50em;"/></div>
<p>For all the other states, there are small negative reward values, which means it charges the robot for battery or fuel consumption when running around the grid, which creates solutions that do not waste moves or time while reaching the goal of reward +1, which encourages the robot to reach the goal as quickly as possible with as little fuel used as possible.</p>
<p>The world ends when the robot reaches either +1 or -1 states. No more rewards are possible after reaching any of these states; these can be called absorbing states. These are zero-cost absorbing states and the robot stays there forever.</p>
<p>MDP working model:</p>
<ul>
<li>At state <em>S<sub>0</sub></em></li>
<li>Choose <em>a<sub>0</sub></em></li>
<li>Get to <em>S<sub>1</sub> ~ P</em><sub><em>s0</em>, a0</sub></li>
<li>Choose <em>a<sub>1</sub></em></li>
<li>Get to <em>S<sub>2</sub> ~ P</em><sub><em>s1</em>, <em>a1</em></sub></li>
<li>and so on ....</li>
</ul>
<p>After a while, it takes all the rewards and sums up to obtain:</p>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/e5cd5a27-28c1-47dd-9376-6c8fc379dba5.jpg" style="width:27.67em;height:1.75em;"/></div>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/3d2fc2bd-25eb-48d6-a360-153642a335d7.jpg" style="width:10.00em;height:1.25em;"/></div>
<p>Discount factor models an economic application, in which one dollar earned today is more valuable than one dollar earned tomorrow.</p>
<p>The robot needs to choose actions over time (a<sub>0</sub>, a<sub>1</sub>, a<sub>2, ....</sub>) to maximize the expected payoff:</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/ab87b8ba-c9da-409e-9a9f-23520ed62b1c.jpg" style="width:19.50em;height:1.75em;"/></div>
<p>Over the period, a reinforcement learning algorithm learns a policy which is a mapping of actions for each state, which means it is a recommended action, which the robot needs to take based on the state in which it exists:</p>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/ac60d23d-5e7a-4425-b650-cb68a836a197.jpg" style="width:10.00em;height:1.58em;"/></div>
<p><strong>Optimal Policy for Grid World:</strong> Policy maps from states to actions, which means that, if you are in a particular state, you need to take this particular action. The following policy is the optimal policy which maximizes the expected value of the total payoff or sum of discounted rewards. Policy always looks into the current state rather than previous states, which is the Markovian property:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/77bc9123-6017-4a05-b603-72c8782e13be.png" style="width:21.17em;height:14.83em;"/></div>
<p>One tricky thing to look at is at the position (3,1): optimal policy shows to go left (West) rather than going (north), which may have a fewer number of states; however, we have an even riskier state that we may step into. So, going left may take longer, but it safely arrives at the destination without getting into negative traps. These types of things can be obtained from computing, which do not look obvious to humans, but a computer is very good at coming up with these policies:</p>
<p>Define: <em>V<sup>π</sup>, V*, π*</em></p>
<p><em>V<sup>π</sup></em> = For any given policy π, value function is <em>V<sup>π</sup> : S -&gt; R</em> such that <em>V<sup>π</sup> (S)</em> is expected total payoff starting in state S, and execute π</p>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/2e06c006-31da-4b3f-bc50-98ac530295f7.jpg" style="width:28.00em;height:2.00em;"/></div>
<p><strong>Random policy for grid world:</strong> The following is an example of a random policy and its value functions. This policy is a rather bad policy with negative values. For any policy, we can write down the value function for that particular policy:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/bf01ee52-fe48-4591-bc7b-097498609169.png" style="width:33.67em;height:11.50em;"/></div>
<div style="padding-left: 90px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/7e0c0c6c-5af6-495d-a087-199d2b79b60e.jpg" style="width:31.58em;height:2.08em;"/></div>
<div style="padding-left: 90px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/ca313eed-3330-4634-92bf-1a9dc2384f66.jpg" style="width:25.08em;height:2.00em;"/></div>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/3b2db0f4-74d5-4f46-b1b5-d2ed7fa080aa.jpg" style="width:10.92em;height:1.50em;"/></div>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/7cb27a71-2014-4494-80e6-c0643be76761.jpg" style="width:28.17em;height:1.67em;"/></div>
<div style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><img src="assets/2f6ff475-d699-49c6-8e8e-f126b98603cd.jpg" style="width:36.67em;height:3.58em;"/></div>
<p>In simple English, Bellman equations illustrate that the value of the current state is equal to the immediate reward and discount factor applied to the expected total payoff of new states (<em>S'</em>) multiplied by their probability to take action (policy) into those states.</p>
<p>Bellman equations are used to solve value functions for a policy in close form, given fixed policy, how to solve the value function equations.</p>
<p>Bellman equations impose a set of linear constraints on value functions. It turns out that we solve the value function at any state <em>S</em> by solving a set of linear equations.</p>
<p><strong>Example of Bellman equations with a grid world problem:</strong></p>
<p>The chosen policy for cell <em>(3,1)</em> is to move north. However, we have stochasticity in the system that about 80 percent of the time it moves in the said direction, and <em>20%</em> of the time it drifts sideways, either left (10 percent) or right (10 percent).</p>
<div style="padding-left: 180px" class="CDPAlignLeft CDPAlign"><img src="assets/994596ff-4652-4fe9-922a-feed7fd009af.jpg" style="width:9.92em;height:1.17em;"/></div>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/4d2fdf17-c7b3-4874-8674-795915340535.png" style="width:16.67em;height:12.17em;"/></div>
<div style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><img src="assets/84f35a41-5cfc-4808-bf9b-300e4c122e1e.jpg" style="width:39.25em;height:1.83em;"/></div>
<p>Similar equations can be written for all the 11 states of the MDPs within the grid. We can obtain the following metrics, from which we will solve all the unknown values, using a system of linear equation methods:</p>
<ul>
<li>11 equations</li>
<li>11 unknown value function variables</li>
<li>11 constraints</li>
</ul>
<p>This is solving an <kbd>n</kbd> variables with <kbd>n</kbd> equations problem, for which we can find the exact form of a solution using a system of equations easily to get an exact solution for V (π) for the entire closed form of the grid, which consists of all the states.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dynamic programming</h1>
                </header>
            
            <article>
                
<p>Dynamic programming is a sequential way of solving complex problems by breaking them down into sub-problems and solving each of them. Once it solves the sub-problems, then it puts those subproblem solutions together to solve the original complex problem. In the reinforcement learning world, Dynamic Programming is a solution methodology to compute optimal policies given a perfect model of the environment as a Markov Decision Process (MDP).</p>
<p>Dynamic programming holds good for problems which have the following two properties. MDPs, in fact, satisfy both properties, which makes DP a good fit for solving them by solving Bellman Equations:</p>
<ul>
<li>Optimal substructure
<ul>
<li>Principle of optimality applies</li>
<li>Optimal solution can be decomposed into sub-problems</li>
</ul>
</li>
<li>Overlapping sub-problems
<ul>
<li>Sub-problems recur many times</li>
<li>Solutions can be cached and reused</li>
</ul>
</li>
<li>MDP satisfies both the properties - luckily!
<ul>
<li>Bellman equations have recursive decomposition of state-values</li>
<li>Value function stores and reuses solutions</li>
</ul>
</li>
</ul>
<p>Though, classical DP algorithms are of limited utility in reinforcement learning, both because of their assumptions of a perfect model and high computational expense. However, it is still important, as they provide an essential foundation for understanding all the methods in the RL domain.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Algorithms to compute optimal policy using dynamic programming</h1>
                </header>
            
            <article>
                
<p>Standard algorithms to compute optimal policies for MDP utilizing Dynamic Programming are as follows, and we will be covering both in detail in later sections of this chapter:</p>
<ul>
<li><strong>Value Iteration algorithm:</strong> An iterative algorithm, in which state values are iterated until it reaches optimal values; and, subsequently, optimum values are utilized to determine the optimal policy</li>
<li><strong>Policy Iteration algorithm:</strong> An iterative algorithm, in which policy evaluation and policy improvements are utilized alternatively to reach optimal policy</li>
</ul>
<p><strong>Value Iteration algorithm:</strong> Value Iteration algorithms are easy to compute for the very reason of applying iteratively on only state values. First, we will compute the optimal value function <em>V*</em>, then plug those values into the optimal policy equation to determine the optimal policy. Just to give the size of the problem, for 11 possible states, each state can have four policies (N-north, S-south, E-east, W-west), which gives an overall 11<sup>4</sup> possible policies. The value iteration algorithm consists of the following steps:</p>
<ol>
<li>Initialize <em>V(S) = 0</em> for all states S</li>
<li>For every S, update:</li>
</ol>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/c9d080c2-be67-4ae3-af04-e0b4c8d7058b.jpg" style="width:23.08em;height:3.75em;"/></div>
<ol start="3">
<li>By repeatedly computing step 2, we will eventually converge to optimal values for all the states:</li>
</ol>
<div class="CDPAlignCenter CDPAlign">
<div style="padding-left: 150px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/6e4c7025-ccd4-467c-83f2-acac295a8f01.jpg" style="width:7.50em;height:1.25em;"/></div>
</div>
<p>There are two ways of updating the values in step 2 of the algorithm</p>
<ul>
<li><strong>Synchronous update</strong> - By performing synchronous update (or Bellman backup operator) we will perform RHS computing and substitute LHS of the equation represented as follows:</li>
</ul>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/ae39ce46-6137-4a13-9820-90e986fd4ab1.jpg" style="width:33.25em;height:1.42em;"/></div>
<ul>
<li><strong>Asynchronous update</strong> - Update the values of the states one at a time rather than updating all the states at the same time, in which states will be updated in a fixed order (update state number 1, followed by 2, and so on.). During convergence, asynchronous updates are a little faster than synchronous updates.</li>
</ul>
<p><strong>Illustration of value iteration on grid world example:</strong> The application of the Value iteration on a grid world is explained in the following image, and the complete code for solving a real problem is provided at the end of this section. After applying the previous value iteration algorithm on MDP using Bellman equations, we've obtained the following optimal values V* for all the states (Gamma value chosen as <em>0.99</em>):</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/803ddf4b-5afc-4c68-a2e6-3e1f8ac21182.png" style="width:12.58em;height:9.50em;"/></div>
<p>When we plug these values in to our policy equation, we obtain the following policy grid:</p>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/7988638f-e34f-4639-aee5-3703faad8c84.jpg" style="width:29.50em;height:3.17em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="image-border" src="assets/399adc98-5126-4423-b205-12f0027853fd.png" style="width:12.08em;height:8.50em;"/></div>
<p>Here, at position (3,1) we would like to prove mathematically why an optimal policy suggests taking going left (west) rather than moving up (north):</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/eee54233-072a-4ab6-8025-b6baced8efe0.jpg" style="width:19.33em;height:3.75em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/01ddc3cb-9371-489a-af1f-d6a6aa7f7163.jpg" style="width:16.67em;height:1.25em;"/></div>
<div class="packt_tip">Due to the wall, whenever the robot tries to move towards South (downwards side), it will remain in the same place, hence we assigned the value of the current position 0.71 for a probability of 0.1.</div>
<p class="mce-root"/>
<p>Similarly, for north, we calculated the total payoff as follows:</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/b8aa1f41-aa36-4f09-ba60-dc1b22f0305d.jpg" style="width:19.17em;height:3.17em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/7473b012-bdea-4528-a114-b53f1e4d13fc.jpg" style="width:21.17em;height:1.33em;"/></div>
<p>So, it would be optimal to move towards the west rather than north, and therefore the optimal policy is chosen to do so.</p>
<p><strong>Policy Iteration Algorithm:</strong> Policy iterations are another way of obtaining optimal policies for MDP in which policy evaluation and policy improvement algorithms are applied iteratively until the solution converges to the optimal policy. Policy Iteration Algorithm consists of the following steps:</p>
<ol>
<li>Initialize random policy π</li>
<li>Repeatedly do the following until convergence happens
<ul>
<li>Solve Bellman equations for the current policy for obtaining V<sup>π</sup> for using system of linear equations:</li>
</ul>
</li>
</ol>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/129df1ca-f1a4-45dc-8821-555ebf29aed2.jpg" style="width:31.25em;height:1.42em;"/></div>
<ul>
<li style="list-style-type: none">
<ul>
<li>Update the policy as per the new value function to improve the policy by pretending the new value is an optimal value using argmax formulae:</li>
</ul>
</li>
</ul>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/f2cc7a6b-b385-42ac-9ff6-27e37de8abfa.jpg" style="width:34.08em;height:4.83em;"/></div>
<ol start="3">
<li>By repeating these steps, both value and policy will converge to optimal values:</li>
</ol>
<div style="padding-left: 210px" class="CDPAlignLeft CDPAlign"><img src="assets/98961d5f-f6e5-44f4-8614-d75b289e7716.jpg" style="width:3.50em;height:1.25em;"/></div>
<div style="padding-left: 210px" class="CDPAlignLeft CDPAlign"><img src="assets/6579aed2-6181-4017-8628-a14997334d8e.jpg" style="width:3.67em;height:1.17em;"/></div>
<p>Policy iterations tend to do well with smaller problems. If an MDP has an enormous number of states, policy iterations will be computationally expensive. As a result, large MDPs tend to use value iterations rather than policy iterations.</p>
<p><strong>What if we don't know exact state transition probabilities in real life examples</strong> <em>P<sub>s,a</sub></em> <strong>?</strong></p>
<p>We need to estimate the probabilities from the data by using the following simple formulae:</p>
<div style="padding-left: 90px" class="mce-root CDPAlignLeft CDPAlign"><img src="assets/c4845d43-ffbf-4f8b-bc88-89eaf86a1a7d.jpg" style="width:26.17em;height:2.42em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/9ff1e5fa-788d-43b7-91b8-e7abda3a9ea5.jpg" style="width:19.67em;height:2.17em;"/></div>
<p>If for some states no data is available, which leads to 0/0 problem, we can take a default probability from uniform distributions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grid world example using value and policy iteration algorithms with basic Python</h1>
                </header>
            
            <article>
                
<p>The classic grid world example has been used to illustrate value and policy iterations with Dynamic Programming to solve MDP's Bellman equations. In the following grid, the agent will start at the south-west corner of the grid in (1,1) position and the goal is to move towards the north-east corner, to position (4,3). Once it reaches the goal, the agent will get a reward of +1. During the journey, it should avoid the danger zone (4,2), because this will give out a negative penalty of reward -1. The agent cannot get into the position where the obstacle (2,2) is present from any direction. Goal and danger zones are the terminal states, which means the agent continues to move around until it reaches one of these two states. The reward for all the other states would be -0.02. Here, the task is to determine the optimal policy (direction to move) for the agent at every state (11 states altogether), so that the agent's total reward is the maximum, or so that the agent can reach the goal as quickly as possible. The agent can move in 4 directions: north, south, east and west.</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/4b027216-c27a-401d-88e6-e587ff48c87c.png" style="width:11.67em;height:8.00em;"/></div>
<div class="packt_figure">
<p>The complete code was written in the Python programming language with class implementation. For further reading, please refer to object oriented programming in Python to understand class, objects, constructors, and so on.</p>
<p>Import the <kbd>random</kbd> package for generating moves in any of the N, E, S, W directions:</p>
<pre><strong>&gt;&gt;&gt; import random,operator</strong></pre>
<p>The following <kbd>argmax</kbd> function calculated the maximum state among the given states, based on the value for each state:</p>
<pre><strong>&gt;&gt;&gt; def argmax(seq, fn):</strong><br/><strong>...     best = seq[0]; best_score = fn(best)</strong><br/><strong>...     for x in seq:</strong><br/><strong>...         x_score = fn(x)</strong><br/><strong>...     if x_score &gt; best_score:</strong><br/><strong>...         best, best_score = x, x_score</strong><br/><strong>...     return best</strong></pre>
<p>To add two vectors at component level, the following code has been utilized for:</p>
<pre><strong>&gt;&gt;&gt; def vector_add(a, b):</strong><br/><strong>...     return tuple(map(operator.add, a, b))</strong></pre>
<p>Orientations provide what the increment value would be, which needs to be added to the existing position of the agent; orientations can be applied on the <em>x</em>-axis or <em>y</em>-axis:</p>
<pre><strong>&gt;&gt;&gt; orientations = [(1,0), (0, 1), (-1, 0), (0, -1)]</strong></pre>
<p>The following function is used to turn the agent in the right direction, as we know at every command the agent moves in that direction about 80% of the time, whilst 10% of the time it would move right, and 10% it would move left.:</p>
<pre><strong>&gt;&gt;&gt; def turn_right(orientation):</strong><br/><strong>...     return orientations[orientations.index(orientation)-1]</strong><br/><strong>&gt;&gt;&gt; def turn_left(orientation):</strong><br/><strong>...     return orientations[(orientations.index(orientation)+1) % len(orientations)]</strong><br/><strong>&gt;&gt;&gt; def isnumber(x):</strong><br/><strong>...     return hasattr(x, '__int__')</strong></pre>
<p>The Markov decision process is defined as a class here. Every MDP is defined by an initial position, state, transition model, reward function, and gamma values.</p>
<pre><strong>&gt;&gt;&gt; class MDP:</strong><br/><strong>... def __init__(self, init_pos, actlist, terminals, transitions={}, states=None, gamma=0.99):</strong><br/><strong>...     if not (0 &lt; gamma &lt;= 1):</strong><br/><strong>...         raise ValueError("MDP should have 0 &lt; gamma &lt;= 1 values")</strong><br/><strong>...     if states:</strong><br/><strong>...         self.states = states</strong><br/><strong>...     else:</strong><br/><strong>...         self.states = set()</strong><br/><strong>...         self.init_pos = init_pos</strong><br/><strong>...         self.actlist = actlist</strong><br/><strong>...         self.terminals = terminals</strong><br/><strong>...         self.transitions = transitions</strong><br/><strong>...         self.gamma = gamma</strong><br/><strong>...         self.reward = {}</strong></pre>
<p>Returns a numeric reward for the state:</p>
<pre><strong>... def R(self, state):</strong><br/><strong>...     return self.reward[state]</strong></pre>
<p>Transition model with from a state and an action returns a list of (probability, result-state) pairs for each state:</p>
<pre><strong>... def T(self, state, action):</strong><br/><strong>...     if(self.transitions == {}):</strong><br/><strong>...         raise ValueError("Transition model is missing")</strong><br/><strong>...     else:</strong><br/><strong>...         return self.transitions[state][action]</strong></pre>
<p>Set of actions that can be performed at a particular state:</p>
<pre><strong>... def actions(self, state):</strong><br/><strong>...     if state in self.terminals:</strong><br/><strong>...         return [None]</strong><br/><strong>...     else:</strong><br/><strong>...         return self.actlist</strong></pre>
<p>Class <kbd>GridMDP</kbd> is created for modeling a 2D grid world with grid values at each state, terminal positions, initial position, and gamma value (discount):</p>
<pre><strong>&gt;&gt;&gt; class GridMDP(MDP):</strong><br/><strong>... def __init__(self, grid, terminals, init_pos=(0, 0), gamma=0.99):</strong></pre>
<p>The following code is used for reversing the grid, as we would like to see <em>row 0</em> at the bottom instead of at the top:</p>
<pre><strong>... grid.reverse()</strong></pre>
<p>The following <kbd>__init__</kbd> command is a constructor used within the grid class for initializing parameters:</p>
<pre><strong>... MDP.__init__(self, init_pos, actlist=orientations,</strong><br/><strong>terminals=terminals, gamma=gamma)</strong><br/><strong>... self.grid = grid</strong><br/><strong>... self.rows = len(grid)</strong><br/><strong>... self.cols = len(grid[0])</strong><br/><strong>... for x in range(self.cols):</strong><br/><strong>...     for y in range(self.rows):</strong><br/><strong>...         self.reward[x, y] = grid[y][x]</strong><br/><strong>...         if grid[y][x] is not None:</strong><br/><strong>...             self.states.add((x, y))</strong></pre>
<p>State transitions provide randomly 80% toward the desired direction and 10% for left and right. This is to model the randomness in a robot which might slip on the floor, and so on:</p>
<pre><strong>... def T(self, state, action):</strong><br/><strong>...     if action is None:</strong><br/><strong>...         return [(0.0, state)]</strong><br/><strong>...     else:</strong><br/><strong>...         return [(0.8, self.go(state, action)),</strong><br/><strong>...                (0.1, self.go(state, turn_right(action))),</strong><br/><strong>...                (0.1, self.go(state, turn_left(action)))]</strong></pre>
<p>Returns the state that results from going in the direction, subject to where that state is in the list of valid states. If the next state is not in the list, like hitting the wall, then the agent should remain in the same state:</p>
<pre><strong>... def go(self, state, direction):</strong><br/><strong>...     state1 = vector_add(state, direction)</strong><br/><strong>...     return state1 if state1 in self.states else state</strong></pre>
<p>Convert a mapping from (x, y) to v into [[..., v, ...]] grid:</p>
<pre><strong>... def to_grid(self, mapping):</strong><br/><strong>...     return list(reversed([[mapping.get((x, y), None)</strong><br/><strong>...                         for x in range(self.cols)]</strong><br/><strong>...                         for y in range(self.rows)]))</strong></pre>
<p>Convert orientations into arrows for better graphical representations:</p>
<pre><strong>... def to_arrows(self, policy):</strong><br/><strong>...     chars = {(1, 0): '&gt;', (0, 1): '^', (-1, 0): '&lt;', (0, -1):</strong><br/><strong>        'v', None: '.'}</strong><br/><strong>...     return self.to_grid({s: chars[a] for (s, a) in policy.items()})</strong></pre>
<p>The following code is used for solving an MDP, using value iterations, and returns optimum state values:</p>
<pre><strong>&gt;&gt;&gt; def value_iteration(mdp, epsilon=0.001):</strong><br/><strong>...     STSN = {s: 0 for s in mdp.states}</strong><br/><strong>...     R, T, gamma = mdp.R, mdp.T, mdp.gamma</strong><br/><strong>...     while True:</strong><br/><strong>...         STS = STSN.copy()</strong><br/><strong>...         delta = 0</strong><br/><strong>...         for s in mdp.states:</strong><br/><strong>...             STSN[s] = R(s) + gamma * max([sum([p * STS[s1] for </strong><br/><strong>...             (p, s1) in T(s,a)]) for a in mdp.actions(s)])</strong><br/><strong>...             delta = max(delta, abs(STSN[s] - STS[s]))</strong><br/><strong>...         if delta &lt; epsilon * (1 - gamma) / gamma:</strong><br/><strong>...             return STS</strong></pre>
<p>Given an MDP and a utility function <kbd>STS</kbd>, determine the best policy, as a mapping from state to action:</p>
<pre><strong>&gt;&gt;&gt; def best_policy(mdp, STS):</strong><br/><strong>...     pi = {}</strong><br/><strong>...     for s in mdp.states:</strong><br/><strong>...         pi[s] = argmax(mdp.actions(s), lambda a: expected_utility(a, s, STS, mdp))</strong><br/><strong>...     return pi</strong></pre>
<p>The expected utility of doing <kbd>a</kbd> in state <kbd>s</kbd>, according to the MDP and STS:</p>
<pre><strong>&gt;&gt;&gt; def expected_utility(a, s, STS, mdp):</strong><br/><strong>...     return sum([p * STS[s1] for (p, s1) in mdp.T(s, a)])</strong></pre>
<p>The following code is used to solve an MDP using policy iterations by alternatively performing policy evaluation and policy improvement steps:</p>
<pre><strong>&gt;&gt;&gt; def policy_iteration(mdp):</strong><br/><strong>...     STS = {s: 0 for s in mdp.states}</strong><br/><strong>...     pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}</strong><br/><strong>...     while True:</strong><br/><strong>...         STS = policy_evaluation(pi, STS, mdp)</strong><br/><strong>...         unchanged = True</strong><br/><strong>...         for s in mdp.states:</strong><br/><strong>...             a = argmax(mdp.actions(s),lambda a: expected_utility(a, s, STS, mdp))</strong><br/><strong>...             if a != pi[s]:</strong><br/><strong>...                 pi[s] = a</strong><br/><strong>...                 unchanged = False</strong><br/><strong>...         if unchanged:</strong><br/><strong>...             return pi</strong></pre>
<p>The following code is used to return an updated utility mapping <kbd>U</kbd> from each state in the MDP to its utility, using an approximation (modified policy iteration):</p>
<pre><strong>&gt;&gt;&gt; def policy_evaluation(pi, STS, mdp, k=20):</strong><br/><strong>...     R, T, gamma = mdp.R, mdp.T, mdp.gamma</strong><br/><strong> ..     for i in range(k):</strong><br/><strong>...     for s in mdp.states:</strong><br/><strong>...         STS[s] = R(s) + gamma * sum([p * STS[s1] for (p, s1) in T(s, pi[s])])</strong><br/><strong>...     return STS</strong><br/><br/><strong>&gt;&gt;&gt; def print_table(table, header=None, sep=' ', numfmt='{}'):</strong><br/><strong>...     justs = ['rjust' if isnumber(x) else 'ljust' for x in table[0]]</strong><br/><strong>...     if header:</strong><br/><strong>...         table.insert(0, header)</strong><br/><strong>...     table = [[numfmt.format(x) if isnumber(x) else x for x in row]</strong><br/><strong>...             for row in table]</strong><br/><strong>...     sizes = list(map(lambda seq: max(map(len, seq)),</strong><br/><strong>...                      list(zip(*[map(str, row) for row in table]))))</strong><br/><strong>...     for row in table:</strong><br/><strong>...         print(sep.join(getattr(str(x), j)(size) for (j, size, x)</strong><br/><strong>...             in zip(justs, sizes, row)))</strong></pre>
<p>The following is the input grid of a 4 x 3 grid environment that presents the agent with a sequential decision-making problem:</p>
<pre><strong>&gt;&gt;&gt; sequential_decision_environment = GridMDP([[-0.02, -0.02, -0.02, +1],</strong><br/><strong>...                                           [-0.02, None, -0.02, -1],</strong><br/><strong>...                                           [-0.02, -0.02, -0.02, -0.02]],</strong><br/><strong>...                                           terminals=[(3, 2), (3, 1)])</strong></pre>
<p>The following code is for performing a value iteration on the given sequential decision-making environment:</p>
<pre><strong>&gt;&gt;&gt; value_iter = best_policy(sequential_decision_environment,value_iteration (sequential_decision_environment, .01))</strong></pre>
<pre><strong>&gt;&gt;&gt; print("\n Optimal Policy based on Value Iteration\n")</strong><br/><strong>&gt;&gt;&gt; print_table(sequential_decision_environment.to_arrows(value_iter))</strong></pre>
<div class="mce-root CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/4daaf65e-72f1-4d5c-81a9-9411ca3e6166.png" style="width:19.83em;height:5.67em;"/></div>
<p>The code for policy iteration is:</p>
<pre><strong>&gt;&gt;&gt; policy_iter = policy_iteration(sequential_decision_environment)</strong><br/><strong>&gt;&gt;&gt; print("\n Optimal Policy based on Policy Iteration &amp; Evaluation\n")</strong><br/><strong>&gt;&gt;&gt; print_table(sequential_decision_environment.to_arrows(policy_iter))</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/e01853b9-4c08-4972-a213-c7f30ddb2b43.png" style="width:23.08em;height:5.25em;"/></div>
<p>From the preceding output with two results, we can conclude that both value and policy iterations provide the same optimal policy for an agent to move across the grid to reach the goal state in the quickest way possible. When the problem size is large enough, it is computationally advisable to go for value iteration rather than policy iteration, as in policy iterations, we need to perform two steps at every iteration of the policy evaluation and policy improvement.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monte Carlo methods</h1>
                </header>
            
            <article>
                
<p>Using <strong>Monte Carlo</strong> (<strong>MC</strong>) methods, we will compute the value functions first and determine the optimal policies. In this method, we do not assume complete knowledge of the environment. MC require only experience, which consists of sample sequences of states, actions, and rewards from actual or simulated interactions with the environment. Learning from actual experiences is striking because it requires no prior knowledge of the environment's dynamics, but still attains optimal behavior. This is very similar to how humans or animals learn from actual experience rather than any mathematical model. Surprisingly, in many cases, it is easy to generate experience sampled according to the desired probability distributions, but infeasible to obtain the distributions in explicit form.</p>
<p class="mce-root"/>
<p>Monte Carlo methods solve the reinforcement learning problem based on averaging the sample returns over each episode. This means that we assume experience is divided into episodes, and that all episodes eventually terminate, no matter what actions are selected. Values are estimated and policies are changed only after the completion of each episode. MC methods are incremental in an episode-by-episode sense, but not in a step-by-step (which is an online learning, and which we will cover the same in Temporal Difference learning section) sense.</p>
<p>Monte Carlo methods sample and average returns for each state-action pair over the episode. However, within the same episode, the return after taking an action in one stage depends on the actions taken in later states. Because all the action selections are undergoing learning, the problem becomes non-stationary from the point of view of the earlier state. In order to handle this non-stationarity, we adapt the idea of policy iteration from dynamic programming, in which, first, we compute the value function for a fixed arbitrary policy; and, later, we improve the policy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monte Carlo prediction</h1>
                </header>
            
            <article>
                
<p>As we know, Monte Carlo methods predict the state-value function for a given policy. The value of any state is the expected return or expected cumulative future discounted rewards starting from that state. These values are estimated in MC methods simply to average the returns observed after visits to that state. As more and more values are observed, the average should converge to the expected value based on the law of large numbers. In fact, this is the principle applicable in all Monte Carlo methods. The Monte Carlo Policy Evaluation Algorithm consist of the following steps:</p>
<ol>
<li>Initialize:</li>
</ol>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/5ee2d232-f6ef-433f-8537-9f55928bdc94.jpg" style="width:17.75em;height:3.08em;"/></div>
<ol start="2">
<li>Repeat forever:
<ul>
<li>Generate an episode using π</li>
<li>For each state <em>s</em> appearing in the episode:
<ul>
<li>G return following the first occurrence of <em>s</em></li>
<li>Append <em>G</em> to Returns(s)</li>
<li>V(s)  average(Returns(s))</li>
</ul>
</li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The suitability of Monte Carlo prediction on grid-world problems</h1>
                </header>
            
            <article>
                
<p>The following diagram has been plotted for illustration purposes. However, practically, Monte Carlo methods cannot be easily used for solving grid-world type problems, due to the fact that termination is not guaranteed for all the policies. If a policy was ever found that caused the agent to stay in the same state, then the next episode would never end. Step-by-step learning methods like (<strong>State-Action-Reward-State-Action</strong> (<strong>SARSA</strong>), which we will be covering in a later part of this chapter in TD Learning Control) do not have this problem because they quickly learn during the episode that such policies are poor, and switch to something else.</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/03344b7c-3d21-4779-8282-22fb609f9bb7.png" style="width:33.25em;height:39.33em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modeling Blackjack example of Monte Carlo methods using Python</h1>
                </header>
            
            <article>
                
<p>The objective of the popular casino card game Blackjack is to obtain cards, the sum of whose numerical values is as great as possible, without exceeding the value of 21. All face cards (king, queen, and jack) count as 10, and an ace can count as either 1 or as 11, depending upon the way the player wants to use it. Only the ace has this flexibility option. All the other cards are valued at face value. The game begins with two cards dealt with both dealer and players. One of the dealer's cards is face up and the other is face down. If the player has a 'Natural 21' from these first two cards (an ace and a 10-card), the player wins unless the dealer also has a Natural, in which case the game is a draw. If the player does not have a natural, then he can ask for additional cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If the player goes bust, he loses; if the player sticks, then it's the dealer's turn. The dealer hits or sticks according to a fixed strategy without choice: the dealer usually sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player automatically wins. If he sticks, the outcome would be either win, lose, or draw, determined by whether the dealer or the player's sum total is closer to 21.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a2357e08-6d21-41db-b26a-257ee659ea18.png" style="width:18.17em;height:10.75em;"/></div>
<p>The Blackjack problem can be formulated as an episodic finite MDP, in which each game of Blackjack is an episode. Rewards of +1, -1, and 0 are given for winning, losing, and drawing for each episode respectively at the terminal state and the remaining rewards within the state of game are given the value as 0 with no discount (gamma = 1). Therefore, the terminal rewards are also the returns for this game. We draw the cards from an infinite deck so that no traceable pattern exists. The entire game is modeled in Python in the following code.</p>
<p>The following snippets of code have taken inspiration from <em>Shangtong Zhang</em>'s Python codes for RL, and are published in this book with permission from the student of <em>Richard S. Sutton</em>, the famous author of <em>Reinforcement : Learning: An Introduction</em> (details provided in the <em>Further reading</em> section).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following package is imported for array manipulation and visualization:</p>
<pre><strong>&gt;&gt;&gt; from __future__ import print_function 
&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; import matplotlib.pyplot as plt 
&gt;&gt;&gt; from mpl_toolkits.mplot3d import Axes3D</strong> </pre>
<p>At each turn, the player or dealer can take one of the actions possible: either to hit or to stand. These are the only two states possible :</p>
<pre><strong>&gt;&gt;&gt; ACTION_HIT = 0 
&gt;&gt;&gt; ACTION_STAND = 1   
&gt;&gt;&gt; actions = [ACTION_HIT, ACTION_STAND]</strong> </pre>
<p>The policy for player is modeled with 21 arrays of values, as the player will get bust after going over the value of 21:</p>
<pre><strong>&gt;&gt;&gt; policyPlayer = np.zeros(22) 
 
&gt;&gt;&gt; for i in range(12, 20): 
...     policyPlayer[i] = ACTION_HIT </strong></pre>
<p>The player has taken the policy of stick if he gets a value of either 20 or 21, or else he will keep hitting the deck to draw a new card:</p>
<pre><strong>&gt;&gt;&gt; policyPlayer[20] = ACTION_STAND 
&gt;&gt;&gt; policyPlayer[21] = ACTION_STAND </strong></pre>
<p>Function form of target policy of a player:</p>
<pre><strong>&gt;&gt;&gt; def targetPolicyPlayer(usableAcePlayer, playerSum, dealerCard): 
...     return policyPlayer[playerSum]</strong> </pre>
<p>Function form of behavior policy of a player:</p>
<pre><strong>&gt;&gt;&gt; def behaviorPolicyPlayer(usableAcePlayer, playerSum, dealerCard): 
...     if np.random.binomial(1, 0.5) == 1: 
...         return ACTION_STAND 
...     return ACTION_HIT</strong> </pre>
<p>Fixed policy for the dealer is to keep hitting the deck until value is 17 and then stick between 17 to 21:</p>
<pre><strong>&gt;&gt;&gt; policyDealer = np.zeros(22) 
&gt;&gt;&gt; for i in range(12, 17): 
...     policyDealer[i] = ACTION_HIT 
&gt;&gt;&gt; for i in range(17, 22): 
...     policyDealer[i] = ACTION_STAND</strong> </pre>
<p>The following function is used for drawing a new card from the deck with replacement:</p>
<pre><strong>&gt;&gt;&gt; def getCard(): 
...     card = np.random.randint(1, 14) 
...     card = min(card, 10) 
...     return card</strong> </pre>
<p>Let's play the game!</p>
<pre><strong>&gt;&gt;&gt; def play(policyPlayerFn, initialState=None, initialAction=None):</strong> </pre>
<ol>
<li>Sum of the player, player's trajectory and whether player uses ace as 11:</li>
</ol>
<pre style="padding-left: 60px"><strong>...     playerSum = 0  </strong><br/><strong>...     playerTrajectory = [] 
...     usableAcePlayer = False</strong> </pre>
<ol start="2">
<li>Dealer status of drawing cards:</li>
</ol>
<pre style="padding-left: 60px"><strong>...     dealerCard1 = 0 
...     dealerCard2 = 0 
...     usableAceDealer = False 
 
...     if initialState is None:</strong> </pre>
<ol start="3">
<li>Generate a random initial state:</li>
</ol>
<pre style="padding-left: 60px"><strong>...         numOfAce = 0</strong> </pre>
<ol start="4">
<li>Initializing the player's cards:</li>
</ol>
<pre style="padding-left: 60px"><strong>...         while playerSum &lt; 12:</strong> </pre>
<ol start="5">
<li>If the sum of a player's cards is less than 12, always hit the deck for drawing card:</li>
</ol>
<pre style="padding-left: 60px"><strong>...             card = getCard() 
...             if card == 1: 
...                 numOfAce += 1 
...                 card = 11 
...                 usableAcePlayer = True 
...             playerSum += card</strong> </pre>
<p> </p>
<ol start="6">
<li>If the player's sum is larger than 21, he must hold at least one ace, but two aces are also possible. In that case, he will use ace as 1 rather than 11. If the player has only one ace, then he does not have a usable ace any more:</li>
</ol>
<pre style="padding-left: 60px"><strong>...         if playerSum &gt; 21: 
...             playerSum -= 10 
...             if numOfAce == 1: 
...                 usableAcePlayer = False</strong> </pre>
<ol start="7">
<li>Initializing the dealer cards:</li>
</ol>
<pre style="padding-left: 60px"><strong>...         dealerCard1 = getCard() 
...         dealerCard2 = getCard() 
 
...     else: 
...         usableAcePlayer = initialState[0] 
...         playerSum = initialState[1] 
...         dealerCard1 = initialState[2] 
...         dealerCard2 = getCard()</strong> </pre>
<ol start="8">
<li>Initialize the game state:</li>
</ol>
<pre style="padding-left: 60px"><strong>...     state = [usableAcePlayer, playerSum, dealerCard1]</strong> </pre>
<ol start="9">
<li>Initializing the dealer's sum:</li>
</ol>
<pre style="padding-left: 60px"><strong>...     dealerSum = 0 
...     if dealerCard1 == 1 and dealerCard2 != 1: 
...         dealerSum += 11 + dealerCard2 
...         usableAceDealer = True 
...     elif dealerCard1 != 1 and dealerCard2 == 1: 
...         dealerSum += dealerCard1 + 11 
...         usableAceDealer = True 
...     elif dealerCard1 == 1 and dealerCard2 == 1: 
...         dealerSum += 1 + 11 
...         usableAceDealer = True 
...     else: 
...         dealerSum += dealerCard1 + dealerCard2</strong> </pre>
<p> </p>
<ol start="10">
<li>The game starts from here, as the player needs to draw extra cards from here onwards:</li>
</ol>
<pre style="padding-left: 60px"><strong>...     while True: 
...         if initialAction is not None: 
...             action = initialAction 
...             initialAction = None 
...         else:</strong> </pre>
<ol start="11">
<li>Get action based on the current sum of a player:</li>
</ol>
<pre style="padding-left: 60px"><strong>...             action = policyPlayerFn(usableAcePlayer, playerSum, dealerCard1)</strong> </pre>
<ol start="12">
<li>Tracking the player's trajectory for importance sampling:</li>
</ol>
<pre style="padding-left: 60px"><strong>...         playerTrajectory.append([action, (usableAcePlayer, playerSum, dealerCard1)]) 
 
...         if action == ACTION_STAND: 
...             break</strong> </pre>
<ol start="13">
<li>Get new a card if the action is to hit the deck:</li>
</ol>
<pre style="padding-left: 60px"><strong>...         playerSum += getCard()</strong> </pre>
<ol start="14">
<li>Player busts here if the total sum is greater than 21, the game ends, and he gets a reward of -1. However, if he has an ace at his disposable, he can use it to save the game, or else he will lose.</li>
</ol>
<pre style="padding-left: 60px"><strong>...         if playerSum &gt; 21: 
...             if usableAcePlayer == True: 
...                 playerSum -= 10 
...                 usableAcePlayer = False 
...             else: 
...                 return state, -1, playerTrajectory</strong> </pre>
<ol start="15">
<li>Now it's the dealer's turn. He will draw cards based on a sum: if he reaches 17, he will stop, otherwise keep on drawing cards. If the dealer also has ace, he can use it to achieve the bust situation, otherwise, he goes bust:</li>
</ol>
<pre style="padding-left: 60px"><strong>...     while True: 
...         action = policyDealer[dealerSum] 
...         if action == ACTION_STAND: 
...             break 
...         dealerSum += getCard() 
...         if dealerSum &gt; 21: 
...             if usableAceDealer == True: 
...                 dealerSum -= 10 
...                 usableAceDealer = False 
...             else: 
...                 return state, 1, playerTrajectory</strong> </pre>
<ol start="16">
<li>Now we compare the player's sum with the dealer's sum to decide who wins without going bust:</li>
</ol>
<pre style="padding-left: 60px"><strong>...     if playerSum &gt; dealerSum: 
...         return state, 1, playerTrajectory 
...     elif playerSum == dealerSum: 
...         return state, 0, playerTrajectory 
...     else: 
...         return state, -1, playerTrajectory</strong> </pre>
<p>The following code illustrates the Monte Carlo sample with <em>On-Policy</em>:</p>
<pre><strong>&gt;&gt;&gt; def monteCarloOnPolicy(nEpisodes): 
...     statesUsableAce = np.zeros((10, 10)) 
...     statesUsableAceCount = np.ones((10, 10)) 
...     statesNoUsableAce = np.zeros((10, 10)) 
...     statesNoUsableAceCount = np.ones((10, 10)) 
...     for i in range(0, nEpisodes): 
...         state, reward, _ = play(targetPolicyPlayer) 
...         state[1] -= 12 
...         state[2] -= 1 
...         if state[0]: 
...             statesUsableAceCount[state[1], state[2]] += 1 
...             statesUsableAce[state[1], state[2]] += reward 
...         else: 
...             statesNoUsableAceCount[state[1], state[2]] += 1 
...             statesNoUsableAce[state[1], state[2]] += reward 
...     return statesUsableAce / statesUsableAceCount, statesNoUsableAce / statesNoUsableAceCount</strong> </pre>
<p>The following code discusses Monte Carlo with Exploring Starts, in which all the returns for each state-action pair are accumulated and averaged, irrespective of what policy was in force when they were observed:</p>
<pre><strong>&gt;&gt;&gt; def monteCarloES(nEpisodes): 
...     stateActionValues = np.zeros((10, 10, 2, 2)) 
...     stateActionPairCount = np.ones((10, 10, 2, 2))</strong> </pre>
<p>Behavior policy is greedy, which gets <kbd>argmax</kbd> of the average returns (s, a):</p>
<pre><strong>...     def behaviorPolicy(usableAce, playerSum, dealerCard): 
...         usableAce = int(usableAce) 
...         playerSum -= 12 
...         dealerCard -= 1 
...         return np.argmax(stateActionValues[playerSum, dealerCard, usableAce, :] 
                      / stateActionPairCount[playerSum, dealerCard, usableAce, :])</strong> </pre>
<p>Play continues for several episodes and, at each episode, randomly initialized state, action, and update values of state-action pairs:</p>
<pre><strong>...     for episode in range(nEpisodes): 
...         if episode % 1000 == 0: 
...             print('episode:', episode) 
...         initialState = [bool(np.random.choice([0, 1])), 
...                        np.random.choice(range(12, 22)), 
...                        np.random.choice(range(1, 11))] 
...         initialAction = np.random.choice(actions) 
...         _, reward, trajectory = play(behaviorPolicy, initialState, initialAction) 
...         for action, (usableAce, playerSum, dealerCard) in trajectory: 
...             usableAce = int(usableAce) 
...             playerSum -= 12 
...             dealerCard -= 1</strong> </pre>
<p>Update values of state-action pairs:</p>
<pre><strong>...             stateActionValues[playerSum, dealerCard, usableAce, action] += reward 
...             stateActionPairCount[playerSum, dealerCard, usableAce, action] += 1 
...     return stateActionValues / stateActionPairCount</strong> </pre>
<p>Print the state value:</p>
<pre><strong>&gt;&gt;&gt; figureIndex = 0 
&gt;&gt;&gt; def prettyPrint(data, tile, zlabel='reward'): 
...     global figureIndex 
...     fig = plt.figure(figureIndex) 
...     figureIndex += 1 
...     fig.suptitle(tile) 
...     ax = fig.add_subplot(111, projection='3d') 
...     x_axis = [] 
...     y_axis = [] 
...     z_axis = [] 
...     for i in range(12, 22): 
...         for j in range(1, 11): 
...             x_axis.append(i) 
...             y_axis.append(j) 
...             z_axis.append(data[i - 12, j - 1]) 
...     ax.scatter(x_axis, y_axis, z_axis,c='red') 
...     ax.set_xlabel('player sum') 
...     ax.set_ylabel('dealer showing') 
...     ax.set_zlabel(zlabel)</strong> </pre>
<p>On-Policy results with or without a usable ace for 10,000 and 500,000 iterations:</p>
<pre><strong>&gt;&gt;&gt; def onPolicy(): 
...     statesUsableAce1, statesNoUsableAce1 = monteCarloOnPolicy(10000) 
...     statesUsableAce2, statesNoUsableAce2 = monteCarloOnPolicy(500000) 
...     prettyPrint(statesUsableAce1, 'Usable Ace &amp; 10000 Episodes') 
...     prettyPrint(statesNoUsableAce1, 'No Usable Ace &amp; 10000 Episodes') 
...     prettyPrint(statesUsableAce2, 'Usable Ace &amp; 500000 Episodes') 
...     prettyPrint(statesNoUsableAce2, 'No Usable Ace &amp; 500000 Episodes') 
...     plt.show() 
    </strong> 
     </pre>
<p>Optimized or Monte Carlo control of policy iterations:</p>
<pre><strong>&gt;&gt;&gt; def MC_ES_optimalPolicy(): 
...     stateActionValues = monteCarloES(500000) 
...     stateValueUsableAce = np.zeros((10, 10)) 
...     stateValueNoUsableAce = np.zeros((10, 10)) 
    # get the optimal policy 
...     actionUsableAce = np.zeros((10, 10), dtype='int') 
...     actionNoUsableAce = np.zeros((10, 10), dtype='int') 
...     for i in range(10): 
...         for j in range(10): 
...             stateValueNoUsableAce[i, j] = np.max(stateActionValues[i, j, 0, :]) 
...             stateValueUsableAce[i, j] = np.max(stateActionValues[i, j, 1, :]) 
...             actionNoUsableAce[i, j] = np.argmax(stateActionValues[i, j, 0, :]) 
...             actionUsableAce[i, j] = np.argmax(stateActionValues[i, j, 1, :]) 
...     prettyPrint(stateValueUsableAce, 'Optimal state value with usable Ace') 
...     prettyPrint(stateValueNoUsableAce, 'Optimal state value with no usable Ace') 
...     prettyPrint(actionUsableAce, 'Optimal policy with usable Ace', 'Action (0 Hit, 1 Stick)') 
...     prettyPrint(actionNoUsableAce, 'Optimal policy with no usable Ace', 'Action (0 Hit, 1 Stick)') 
...     plt.show() 
 
# Run on-policy function 
&gt;&gt;&gt; onPolicy()</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/288909aa-3398-46a7-9096-a541ae6a6d7d.png" style="width:48.92em;height:49.08em;"/></div>
<p>From the previous diagram, we can conclude that a usable ace in a hand gives much higher rewards even at the low player sum combinations, whereas for a player without a usable ace, values are pretty distinguished in terms of earned reward if those values are less than 20.</p>
<pre><strong># Run Monte Carlo Control or Explored starts 
&gt;&gt;&gt; MC_ES_optimalPolicy()</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/3ae682af-301f-4a67-b74f-7b46025b5e1d.png" style="width:43.50em;height:43.33em;"/></div>
<p>From the optimum policies and state values, we can conclude that, with a usable ace at our disposal, we can hit more than stick, and also that the state values for rewards are much higher compared with when there is no ace in a hand. Though the results we are talking about are obvious, we can see the magnitude of the impact of holding an ace in a hand.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Temporal difference learning</h1>
                </header>
            
            <article>
                
<p><strong>Temporal Difference</strong> (<strong>TD</strong>) learning is the central and novel theme of reinforcement learning. TD learning is the combination of both <strong>Monte Carlo</strong> (<strong>MC</strong>) and <strong>Dynamic Programming</strong> (<strong>DP</strong>) ideas. Like Monte Carlo methods, TD methods can learn directly from the experiences without the model of the environment. Similar to Dynamic Programming, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome, unlike MC methods, in which estimates are updated after reaching the final outcome only.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/4256712b-f076-4482-8960-92d182c9c460.png" style="width:35.33em;height:33.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TD prediction</h1>
                </header>
            
            <article>
                
<p>Both TD and MC use experience to solve <em>z</em> prediction problem. Given some policy π, both methods update their estimate <em>v</em> of <em>v</em><sub>π</sub>  for the non-terminal states <em>S<sub>t</sub></em> occurring in that experience. Monte Carlo methods wait until the return following the visit is known, then use that return as a target for <em>V(S<sub>t</sub>)</em>.</p>
<div style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><img src="assets/6bffd9d6-a6a8-4337-b394-6d9e39c06077.jpg" style="width:35.33em;height:2.92em;"/></div>
<p>The preceding method can be called as a constant - <em>α MC</em>, where MC must wait until the end of the episode to determine the increment to <em>V(S<sub>t</sub>)</em> (only then is <em>G<sub>t</sub></em> known).</p>
<p>TD methods need to wait only until the next timestep. At time <em>t+1</em>, they immediately form a target and make a useful update using the observed reward <em>R<sub>t+1</sub></em> and the estimate <em>V(S<sub>t+1</sub>)</em>. The simplest TD method, known as <em>TD(0)</em>, is:</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/b9301f34-ddc2-48b2-b7a5-c3cbe1febc08.jpg" style="width:25.08em;height:1.67em;"/></div>
<p>Target for MC update is <em>G<sub>t</sub></em>, whereas the target for the TD update is <em>R<sub>t+1 </sub>+ y V(S<sub>t+1</sub>)</em>.</p>
<p>In the following diagram, a comparison has been made between TD with MC methods. As we've written in equation TD(0), we use one step of real data and then use the estimated value of the value function of next state. In a similar way, we can also use two steps of real data to get a better picture of the reality and estimate value function of the third stage. However, as we increase the steps, which eventually need more and more data to perform parameter updates, the more time it will cost.</p>
<p class="mce-root"/>
<p>When we take infinite steps until it touches the terminal point for updating parameters in each episode, TD becomes the Monte Carlo method.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/579d6e42-de4f-47c1-8fea-88eca610019f.png" style="width:25.00em;height:23.25em;"/></div>
<p>TD (0) for estimating <em>v</em> algorithm consists of the following steps:</p>
<ol>
<li>Initialize:</li>
</ol>
<div style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><img src="assets/fe4f7713-158f-4708-a82a-06e72dde76f3.jpg" style="width:38.00em;height:2.67em;"/></div>
<ol start="2">
<li>Repeat (for each episode):
<ul>
<li>Initialize <em>S</em></li>
<li>Repeat (for each step of episode):
<ul>
<li>A &lt;- action given by π for S</li>
<li>Take action A, observe <em>R,S'</em></li>
<li><img src="assets/645d3a19-7c01-4d55-9b6c-850d4cdd6a27.jpg" style="width:17.25em;height:1.25em;"/></li>
<li><img src="assets/5c3bcd7e-fdba-4a49-b716-b8099ced3401.jpg" style="width:3.67em;height:1.08em;"/></li>
</ul>
</li>
</ul>
</li>
<li>Until <em>S</em> is terminal.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Driving office example for TD learning</h1>
                </header>
            
            <article>
                
<p>In this simple example, you travel from home to the office every day and you try to predict how long it will take to get to the office in the morning. When you leave your home, you note that time, the day of the week, the weather (whether it is rainy, windy, and so on) any other parameter which you feel is relevant. For example, on Monday morning you leave at exactly 8 a.m. and you estimate it takes 40 minutes to reach the office. At 8:10 a.m., and you notice that a VIP is passing, and you need to wait until the complete convoy has moved out, so you re-estimate that it will take 45 minutes from then, or a total of 55 minutes. Fifteen minutes later you have completed the highway portion of your journey in good time. Now you enter a bypass road and you now reduce your estimate of total travel time to 50 minutes. Unfortunately, at this point, you get stuck behind a bunch of bullock carts and the road is too narrow to pass. You end up having to follow those bullock carts until you turn onto the side street where your office is located at 8:50. Seven minutes later, you reach your office parking. The sequence of states, times, and predictions are as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/cf127b34-153f-4784-801e-809df5455da9.png" style="width:40.58em;height:14.92em;"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Rewards in this example are the elapsed time at each leg of the journey and we are using a discount factor (gamma, <em>v = 1</em>), so the return for each state is the actual time to go from that state to the destination (office). The value of each state is the predicted time to go, which is the second column in the preceding table, also known the current estimated value for each state encountered.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/04b114de-7dbf-4065-9501-1fbcd65f3120.png" style="width:61.83em;height:26.25em;"/></div>
<p>In the previous diagram, Monte Carlo is used to plot the predicted total time over the sequence of events. Arrows always show the change in predictions recommended by the constant-α MC method. These are errors between the estimated value in each stage and the actual return (57 minutes). In the MC method, learning happens only after finishing, for which it needs to wait until 57 minutes passed. However, in reality, you can estimate before reaching the final outcome and correct your estimates accordingly. TD works on the same principle, at every stage it tries to predict and correct the estimates accordingly. So, TD methods learn immediately and do not need to wait until the final outcome. In fact, that is how humans predict in real life. Because of these many positive properties, TD learning is considered as novel in reinforcement learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SARSA on-policy TD control</h1>
                </header>
            
            <article>
                
<p><strong>State-action-reward-state-action</strong> (<strong>SARSA</strong>) is an on-policy TD control problem, in which policy will be optimized using policy iteration (GPI), only time TD methods used for evaluation of predicted policy. In the first step, the algorithm learns a SARSA function. In particular, for an on-policy method we estimate <em>q<sub>π</sub> (s, a)</em> for the current behavior policy π and for all states (s) and actions (a), using the TD method for learning v<sub>π.</sub></p>
<p class="mce-root"/>
<p>Now, we consider transitions from state-action pair to state-action pair, and learn the values of state-action pairs:</p>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/4a39bee3-683a-4610-8214-ba9d78ccc614.jpg" style="width:24.25em;height:1.67em;"/></div>
<p>This update is done after every transition from a non-terminal state <em>S<sub>t</sub></em>. If <em>S<sub>t+1</sub></em> is terminal, then <em>Q (S<sub>t+1,</sub> A<sub>t+1</sub>)</em> is defined as zero. This rule uses every element of the quintuple of events (<em>S<sub>t</sub></em>, <em>A<sub>t</sub></em>, <em>Rt</em>, <em>St<sub>+1</sub></em>, <em>A<sub>t+1</sub></em>), which make up a transition from one state-action pair to the next. This quintuple gives rise to the name SARSA for the algorithm.</p>
<p>As in all on-policy methods, we continually estimate q<sub>π</sub> for the behavior policy π, and at the same time change π toward greediness with respect to q<sub>π.</sub> The algorithm for computation of SARSA is given as follows:</p>
<ol>
<li>Initialize:</li>
</ol>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/dad25125-2e1c-48c5-89c9-97034a05a1ca.jpg" style="width:37.08em;height:1.58em;"/></div>
<ol start="2">
<li>Repeat (for each episode):
<ul>
<li>Initialize S</li>
<li>Choose A from S using policy derived from Q (for example, <span>ε</span>- greedy)</li>
<li>Repeat (for each step of episode):
<ul>
<li>Take action <em>A</em>, observe <em>R,S'</em></li>
<li>Choose <em>A'</em> from using <em>S'</em> policy derived from Q (for example, ε - greedy)</li>
<li><img src="assets/d83543f0-d7b0-4856-b065-c1f49e9bb8ac.jpg" style="width:24.33em;height:1.33em;"/></li>
<li><img src="assets/3d8f822d-6b84-44fa-8274-4fe9881c379b.jpg" style="width:9.08em;height:1.33em;"/></li>
</ul>
</li>
</ul>
</li>
<li>Until <em>S</em> is terminal</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Q-learning - off-policy TD control</h1>
                </header>
            
            <article>
                
<p>Q-learning is the most popular method used in practical applications for many reinforcement learning problems. The off-policy TD control algorithm is known as Q-learning. In this case, the learned action-value function, Q directly approximates <img src="assets/55c095cc-5895-4255-91b6-9c22cdb1caf7.png"/>, the optimal action-value function, independent of the policy being followed. This approximation simplifies the analysis of the algorithm and enables early convergence proofs. The policy still has an effect, in that it determines which state-action pairs are visited and updated. However, all that is required for correct convergence is that all pairs continue to be updated. As we know, this is a minimal requirement in the sense that any method guaranteed to find optimal behavior in the general case must require it. An algorithm of convergence is shown in the following steps:</p>
<ol>
<li>Initialize:</li>
</ol>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/c5488bbc-43aa-4794-9ecb-fc8ce42c4efb.jpg" style="width:35.17em;height:1.50em;"/></div>
<ol start="2">
<li>Repeat (for each episode):
<ul>
<li>Initialize S</li>
<li>Repeat (for each step of episode):
<ul>
<li>Choose A from S using policy derived from Q (for example, ε - greedy)</li>
<li>Take action A, observe <em>R,S'</em></li>
<li><img src="assets/751ba8b1-8f2c-4399-8418-a3dd70935dc5.jpg" style="width:23.67em;height:1.25em;"/></li>
<li><img src="assets/15d47def-acaf-4609-91ef-4f56531bccd4.jpg" style="width:8.08em;height:1.17em;"/></li>
</ul>
</li>
</ul>
</li>
<li>Until <em>S</em> is terminal</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cliff walking example of on-policy and off-policy of TD control</h1>
                </header>
            
            <article>
                
<p>A cliff walking grid-world example is used to compare SARSA and Q-learning, to highlight the differences between on-policy (SARSA) and off-policy (Q-learning) methods. This is a standard undiscounted, episodic task with start and end goal states, and with permitted movements in four directions (north, west, east and south). The reward of -1 is used for all transitions except the regions marked <em>The Cliff</em>, stepping on this region will penalize the agent with reward of -100 and sends the agent instantly back to the start position.</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/f23a61ae-c0ed-4be1-bcf5-92f3becb9e2e.png" style="width:36.83em;height:19.83em;"/></div>
<p>The following snippets of code have taken inspiration from Shangtong Zhang's Python codes for RL and are published in this book with permission from the student of <em>Richard S. Sutton</em>, the famous author of <em>Reinforcement Learning: An Introduction</em> (details provided in the <em>Further reading</em> section):</p>
<pre><strong># Cliff-Walking - TD learning - SARSA &amp; Q-learning 
&gt;&gt;&gt; from __future__ import print_function 
&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; import matplotlib.pyplot as plt 
 
# Grid dimensions 
&gt;&gt;&gt; GRID_HEIGHT = 4 
&gt;&gt;&gt; GRID_WIDTH = 12 
 
# probability for exploration, step size,gamma  
&gt;&gt;&gt; EPSILON = 0.1 
&gt;&gt;&gt; ALPHA = 0.5 
&gt;&gt;&gt; GAMMA = 1 
 
# all possible actions 
&gt;&gt;&gt; ACTION_UP = 0; ACTION_DOWN = 1;ACTION_LEFT = 2;ACTION_RIGHT = 3 
&gt;&gt;&gt; actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT] 
 
# initial state action pair values 
&gt;&gt;&gt; stateActionValues = np.zeros((GRID_HEIGHT, GRID_WIDTH, 4)) 
&gt;&gt;&gt; startState = [3, 0] 
&gt;&gt;&gt; goalState = [3, 11] 
 
# reward for each action in each state 
&gt;&gt;&gt; actionRewards = np.zeros((GRID_HEIGHT, GRID_WIDTH, 4)) 
&gt;&gt;&gt; actionRewards[:, :, :] = -1.0 
&gt;&gt;&gt; actionRewards[2, 1:11, ACTION_DOWN] = -100.0 
&gt;&gt;&gt; actionRewards[3, 0, ACTION_RIGHT] = -100.0 
 
# set up destinations for each action in each state 
&gt;&gt;&gt; actionDestination = [] 
&gt;&gt;&gt; for i in range(0, GRID_HEIGHT): 
...     actionDestination.append([]) 
...     for j in range(0, GRID_WIDTH): 
...         destinaion = dict() 
...         destinaion[ACTION_UP] = [max(i - 1, 0), j] 
...         destinaion[ACTION_LEFT] = [i, max(j - 1, 0)] 
...         destinaion[ACTION_RIGHT] = [i, min(j + 1, GRID_WIDTH - 1)] 
...         if i == 2 and 1 &lt;= j &lt;= 10: 
...             destinaion[ACTION_DOWN] = startState 
...         else: 
...             destinaion[ACTION_DOWN] = [min(i + 1, GRID_HEIGHT - 1), j] 
...         actionDestination[-1].append(destinaion) 
&gt;&gt;&gt; actionDestination[3][0][ACTION_RIGHT] = startState 
 
# choose an action based on epsilon greedy algorithm 
&gt;&gt;&gt; def chooseAction(state, stateActionValues): 
...     if np.random.binomial(1, EPSILON) == 1: 
...         return np.random.choice(actions) 
...     else: 
...         return np.argmax(stateActionValues[state[0], state[1], :]) 
 
 
# SARSA update 
 
&gt;&gt;&gt; def sarsa(stateActionValues, expected=False, stepSize=ALPHA): 
...     currentState = startState 
...     currentAction = chooseAction(currentState, stateActionValues) 
...     rewards = 0.0 
...     while currentState != goalState: 
 
...         newState = actionDestination[currentState[0]][currentState[1]] [currentAction] 
 
...         newAction = chooseAction(newState, stateActionValues) 
...         reward = actionRewards[currentState[0], currentState[1], currentAction] 
...         rewards += reward 
...         if not expected: 
...             valueTarget = stateActionValues[newState[0], newState[1], newAction] 
...         else: 
...             valueTarget = 0.0 
...             actionValues = stateActionValues[newState[0], newState[1], :] 
...             bestActions = np.argwhere(actionValues == np.max(actionValues)) 
...             for action in actions: 
...                 if action in bestActions: 
 
...                     valueTarget += ((1.0 - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[0], newState[1], action] 
 
...                 else: 
...                     valueTarget += EPSILON / len(actions) * stateActionValues[newState[0], newState[1], action] 
...         valueTarget *= GAMMA 
...         stateActionValues[currentState[0], currentState[1], currentAction] += stepSize * (reward+ valueTarget - stateActionValues[currentState[0], currentState[1], currentAction]) 
...         currentState = newState 
...         currentAction = newAction 
...     return rewards 
 
# Q-learning update 
&gt;&gt;&gt; def qlearning(stateActionValues, stepSize=ALPHA): 
...     currentState = startState 
...     rewards = 0.0 
...     while currentState != goalState: 
...         currentAction = chooseAction(currentState, stateActionValues) 
...         reward = actionRewards[currentState[0], currentState[1], currentAction] 
...         rewards += reward 
...         newState = actionDestination[currentState[0]][currentState[1]] [currentAction] 
...         stateActionValues[currentState[0], currentState[1], currentAction] += stepSize * (reward + GAMMA * np.max(stateActionValues[newState[0], newState[1], :]) - 
...             stateActionValues[currentState[0], currentState[1], currentAction]) 
...         currentState = newState 
...     return rewards 
 
 
# print optimal policy 
&gt;&gt;&gt; def printOptimalPolicy(stateActionValues): 
...     optimalPolicy = [] 
...     for i in range(0, GRID_HEIGHT): 
...         optimalPolicy.append([]) 
...         for j in range(0, GRID_WIDTH): 
...             if [i, j] == goalState: 
...                 optimalPolicy[-1].append('G') 
...                 continue 
...             bestAction = np.argmax(stateActionValues[i, j, :]) 
...             if bestAction == ACTION_UP: 
...                 optimalPolicy[-1].append('U') 
...             elif bestAction == ACTION_DOWN: 
...                 optimalPolicy[-1].append('D') 
...             elif bestAction == ACTION_LEFT: 
...                 optimalPolicy[-1].append('L') 
...             elif bestAction == ACTION_RIGHT: 
...                 optimalPolicy[-1].append('R') 
...     for row in optimalPolicy: 
...         print(row) 
 
&gt;&gt;&gt; def SARSAnQLPlot(): 
    # averaging the reward sums from 10 successive episodes 
...     averageRange = 10 
 
    # episodes of each run 
...     nEpisodes = 500 
 
    # perform 20 independent runs 
...     runs = 20 
 
...     rewardsSarsa = np.zeros(nEpisodes) 
...     rewardsQlearning = np.zeros(nEpisodes) 
...     for run in range(0, runs): 
...         stateActionValuesSarsa = np.copy(stateActionValues) 
...         stateActionValuesQlearning = np.copy(stateActionValues) 
...         for i in range(0, nEpisodes): 
            # cut off the value by -100 to draw the figure more elegantly 
...             rewardsSarsa[i] += max(sarsa(stateActionValuesSarsa), -100) 
...             rewardsQlearning[i] += max(qlearning(stateActionValuesQlearning), -100) 
 
    # averaging over independent runs 
...     rewardsSarsa /= runs 
...     rewardsQlearning /= runs 
 
    # averaging over successive episodes 
...     smoothedRewardsSarsa = np.copy(rewardsSarsa) 
...     smoothedRewardsQlearning = np.copy(rewardsQlearning) 
...     for i in range(averageRange, nEpisodes): 
...         smoothedRewardsSarsa[i] = np.mean(rewardsSarsa[i - averageRange: i + 1]) 
...         smoothedRewardsQlearning[i] = np.mean(rewardsQlearning[i - averageRange: i + 1]) 
 
    # display optimal policy 
...     print('Sarsa Optimal Policy:') 
...     printOptimalPolicy(stateActionValuesSarsa) 
...     print('Q-learning Optimal Policy:') 
...     printOptimalPolicy(stateActionValuesQlearning) 
 
    # draw reward curves 
...     plt.figure(1) 
...     plt.plot(smoothedRewardsSarsa, label='Sarsa') 
...     plt.plot(smoothedRewardsQlearning, label='Q-learning') 
...     plt.xlabel('Episodes') 
...     plt.ylabel('Sum of rewards during episode') 
...     plt.legend() 
 
 
# Sum of Rewards for SARSA versus Qlearning 
&gt;&gt;&gt; SARSAnQLPlot() </strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a1ed3b55-f95f-4692-9127-ac7a3bb988cb.png"/></div>
<p>After an initial transient, Q-learning learns the value of optimal policy to walk along the optimal path, in which the agent travels right along the edge of the cliff. Unfortunately, this will result in occasionally falling off the cliff because of ε-greedy action selection. Whereas SARSA, on the other hand, takes the action selection into account and learns the longer and safer path through the upper part of the grid. Although Q-learning learns the value of the optimal policy, its online performance is worse than that of the SARSA, which learns the roundabout and safest policy. Even if we observe the following sum of rewards displayed in the following diagram, SARSA has a less negative sum of rewards during the episode than Q-learning.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/6c28f559-985f-438f-a861-5a1e7566ecb2.png" style="width:35.58em;height:28.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>There are many classic resources available for reinforcement learning, and we encourage the reader to go through them:</p>
<ul>
<li>R.S. Sutton and A.G. Barto, <em>Reinforcement Learning: An Introduction</em>. <em>MIT Press</em>, Cambridge, MA, USA, 1998</li>
<li><em>RL Course</em> by <em>David Silver</em> from YouTube: <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT" target="_blank">https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT</a></li>
<li><em>Machine Learning</em> (Stanford) by <em>Andrew NG</em> form YouTube (Lectures 16- 20): <a href="https://www.youtube.com/watch?v=UzxYlbK2c7E&amp;list=PLA89DCFA6ADACE599" target="_blank">https://www.youtube.com/watch?v=UzxYlbK2c7E&amp;list=PLA89DCFA6ADACE599</a></li>
<li><em>Algorithms for reinforcement learning</em> by <em>Csaba</em> from <em>Morgan &amp; Claypool</em> Publishers</li>
<li><em>Artificial Intelligence: A Modern Approach</em> 3<sup>rd</sup> Edition, by <em>Stuart Russell</em> and <em>Peter Norvig</em>, <em>Prentice Hall</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you've learned various reinforcement learning techniques, like Markov decision process, Bellman equations, dynamic programming, Monte Carlo methods, Temporal Difference learning, including both on-policy (SARSA) and off-policy (Q-learning), with Python examples to understand its implementation in a practical way. You also learned how Q-learning is being used in many practical applications nowadays, as this method learns from trial and error by interacting with environments.</p>
<p>Finally, <em>Further reading</em> has been provided for you if you would like to pursue reinforcement learning full-time. We wish you all the best!</p>


            </article>

            
        </section>
    </body></html>