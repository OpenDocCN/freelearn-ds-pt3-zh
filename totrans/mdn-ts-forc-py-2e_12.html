<html><head></head><body>
  <div id="_idContainer464" class="Basic-Text-Frame">
    <h1 class="chapterNumber">10</h1>
    <h1 id="_idParaDest-214" class="chapterTitle">Global Forecasting Models</h1>
    <p class="normal">In previous chapters, we saw how we can use modern machine learning models on time series forecasting problems, essentially replacing traditional models such as ARIMA or exponential smoothing. However, before now, we were looking at the different time series in any dataset (such as households in the London Smart Meters dataset) in isolation, just as the traditional models did.</p>
    <p class="normal">However, we will now explore a different paradigm of modeling where we use a single machine learning model to forecast a bunch of time series together. As we will learn in the chapter, this paradigm brings many benefits with it, from the perspective of both computation and accuracy.</p>
    <p class="normal">In this chapter, we will be covering these main topics:</p>
    <ul>
      <li class="bulletList">Why Global Forecasting Models?</li>
      <li class="bulletList">Creating GFMs</li>
      <li class="bulletList">Strategies to improve GFMs</li>
      <li class="bulletList">Interpretability</li>
    </ul>
    <h1 id="_idParaDest-215" class="heading-1">Technical requirements</h1>
    <p class="normal">You will need to set up the <strong class="keyWord">Anaconda</strong> environment following the instructions in the <em class="italic">Preface</em> of the book to get a working environment with all the libraries and datasets required for the code in this book. Any additional library will be installed while running the notebooks.</p>
    <p class="normal">You will need to run the following notebooks before using the code in this chapter:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">02-Preprocessing_London_Smart_Meter_Dataset.ipynb</code> in <code class="inlineCode">Chapter02</code></li>
      <li class="bulletList"><code class="inlineCode">01-Setting_up_Experiment_Harness.ipynb</code> in <code class="inlineCode">Chapter04</code></li>
      <li class="bulletList">From the <code class="inlineCode">Chapter06</code> and <code class="inlineCode">Chapter07</code> folders:<ul>
          <li class="bulletList level-2"><code class="inlineCode">01-Feature_Engineering.ipynb</code></li>
          <li class="bulletList level-2"><code class="inlineCode">02-Dealing_with_Non-Stationarity.ipynb</code></li>
          <li class="bulletList level-2"><code class="inlineCode">02a-Dealing_with_Non-Stationarity-Train+Val.ipynb</code></li>
        </ul>
      </li>
      <li class="bulletList">From the <code class="inlineCode">Chapter08</code> folder:<ul>
          <li class="bulletList level-2"><code class="inlineCode">00-Single_Step_Backtesting_Baselines.ipynb</code></li>
          <li class="bulletList level-2"><code class="inlineCode">01-Forecasting_with_ML.ipynb</code></li>
          <li class="bulletList level-2"><code class="inlineCode">01a-Forecasting_with_ML_for_Test_Dataset.ipynb</code></li>
          <li class="bulletList level-2"><code class="inlineCode">02-Forecasting_with_Target_Transformation.ipynb</code></li>
          <li class="bulletList level-2"><code class="inlineCode">02a-Forecasting_with_Target_Transformation(Test).ipynb</code></li>
        </ul>
      </li>
    </ul>
    <p class="normal">The associated code for the chapter can be found at <a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter10"><span class="url">https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter10</span></a>.</p>
    <h1 id="_idParaDest-216" class="heading-1">Why Global Forecasting Models?</h1>
    <p class="normal">We talked about global models briefly in <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, where we mentioned related datasets. We can think of many scenarios where we would encounter related time series. We may need to forecast the sales for all the products <a id="_idIndexMarker739"/>of a retailer, the number of rides requested for a cab service across different areas of a city, or the energy consumption of all the households in a particular area (which is what the London Smart Meters dataset does). We call these related time series because all the different time series in the dataset can have a lot of factors in common with each other. For instance, the yearly seasonality that might occur in retail products might be present for a large section of products, or the way an external factor such as temperature affects energy consumption may be similar for a large number of households. Therefore, one way or the other, the different time series in a related time series dataset share attributes between them.</p>
    <p class="normal">Traditionally, we used to consider each time series an independent time series; in other words, each time series was assumed to be generated using a different data-generating process. Classical models such as ARIMA and exponential smoothing are trained for each time series. However, we can also consider all the time series in the dataset as being generated from a single data-generating process, and the subsequent modeling approach would be to train a single model to forecast all the time series in the dataset. The latter is what we refer to as <strong class="keyWord">Global Forecasting Models</strong> (<strong class="keyWord">GFMs</strong>). <strong class="keyWord">GFMs</strong> are models that are designed to handle multiple related time series, allowing for shared learning across those <a id="_idIndexMarker740"/>time series. In contrast, the traditional approach is referred to as <strong class="keyWord">Local Forecasting Models</strong> (<strong class="keyWord">LFMs</strong>).</p>
    <p class="normal">Although we briefly talked about the drawbacks of LFMs in <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, let’s summarize them in a more concrete fashion and see why GFMs help us smooth over a <a id="_idIndexMarker741"/>lot of those drawbacks.</p>
    <h2 id="_idParaDest-217" class="heading-2">Sample size</h2>
    <p class="normal">In most<a id="_idIndexMarker742"/> real-world applications (especially in business forecasting), the time series we have to forecast is not very long. Adopting a completely data-driven approach to modeling such a small time series is problematic. Training a highly flexible model with a handful of data points will lead to the model memorizing the training data, resulting in an overfit.</p>
    <p class="normal">Traditionally, this has been overcome by placing strong priors or inductive bias into the models we use for forecasting. Inductive bias loosely refers to a set of assumptions or restrictions that are built into a model that should help the model predict feature combinations it has not encountered while training. For instance, double exponential smoothing has strong assumptions about seasonality and trend. The model does not allow any other more complicated patterns to be learned from the data. Therefore, using these strong assumptions, we are restricting the model search to a small section of the hypothesis space. While this helps in low-data regimes, the flip side is that these assumptions may limit accuracy.</p>
    <p class="normal">Recent developments in the field of machine learning have shown us without a doubt that using a data-driven approach (with much fewer assumptions or priors) on large training sets will lead to us training better models. However, conventional statistical wisdom tells us that the number of data points needs to be at least 10 to 100 times the number of parameters that we are trying to learn from those data points.</p>
    <p class="normal">So, if we stick to LFMs, scenarios in which we can adopt a completely data-driven approach will be very rare. This is where GFMs shine. A GFM is able to use the history of <em class="italic">all</em> the time series in a dataset to train the model and learn a single set of parameters that work for all the time series in the dataset. Borrowing the terminology introduced in <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, we increase the <em class="italic">width</em> of the dataset, keeping the <em class="italic">length</em> the same (refer back to <em class="italic">Figure 5.2</em>). This explosion of historical information available to a single model lets us use completely data-driven techniques on time series datasets.</p>
    <h2 id="_idParaDest-218" class="heading-2">Cross-learning</h2>
    <p class="normal">GFMs, by design, promote <a id="_idIndexMarker743"/>cross-learning across different time series in a dataset. Imagine we have a time series that is quite new and does not have a history rich enough for teaching the model—for instance, the sales of a newly introduced retail product or the electricity consumption of a new household in a region. If we consider these time series in isolation, it will be a while before we start to get reasonable forecasts from the models we train on them, but GFMs make that process easier by enabling cross-learning. GFMs have an implicit sense of similarity between different time series and they will be able to use patterns they have seen in similar time series with a rich history to come up with a forecast on the new time series.</p>
    <p class="normal">Another way cross-learning helps is by acting like a regularizer while estimating common parameters such as seasonality. For instance, the seasonality exhibited by similar products in a retail scenario is best estimated at an aggregate level, because each individual time series will have some sort of noise that can creep into the seasonality extraction. By enforcing common seasonality across multiple products, we are essentially regularizing the seasonality estimation and, in the process, making the seasonality estimate more robust. The good thing about GFMs is that they take a data-driven approach to define the seasonality of which products should be estimated together and which ones have different patterns. If you have different seasonality patterns in different products, a GFM may struggle to model them together. However, when provided with enough information on how to distinguish between different products, the GFM will be able to learn that difference too.</p>
    <h2 id="_idParaDest-219" class="heading-2">Multi-task learning</h2>
    <p class="normal">GFMs can <a id="_idIndexMarker744"/>be considered multi-task learning paradigms where a single model is trained to learn multiple tasks (as forecasting each time series is a separate task). Multi-task learning is an active area of research, and there are many benefits to using multi-task models:</p>
    <ul>
      <li class="bulletList">When the model is learning from noisy, high-dimensional data, it becomes harder for the model to distinguish between useful and non-useful features. When we train the model on a multi-task paradigm, the model can understand useful features by looking at features that are useful for other tasks as well, thus providing the model with an additional perspective for discerning useful features.</li>
      <li class="bulletList">Sometimes, features<a id="_idIndexMarker745"/> such as seasonality might be hard to learn from a particularly noisy time series. However, under a multi-task framework, the model can learn the difficult features using other time series in the dataset.</li>
      <li class="bulletList">Finally, multi-task learning introduces a kind of regularization that forces the model to find a model that works well on all tasks, thus reducing the risk of overfitting.</li>
    </ul>
    <h2 id="_idParaDest-220" class="heading-2">Engineering complexity</h2>
    <p class="normal">LFMs pose a <a id="_idIndexMarker746"/>challenge from the engineering side as well for large datasets. If we have thousands or millions of time series to forecast, it becomes increasingly difficult to both train and manage the life cycle of these LFMs. In <em class="chapterRef">Chapter 8</em>, <em class="italic">Forecasting Time Series with Machine Learning Models</em>, we trained LFMs for just a subset of households in the dataset. It took almost 20 to 30 minutes to train a machine learning model for all 150 households and we ran them with the default hyperparameters. In a normal machine learning workflow, we train multiple machine learning models and do hyperparameter tuning to find the best configuration of the model. However, carrying out all these steps for thousands of time series in a dataset becomes increasingly complex and time-consuming.</p>
    <p class="normal">Equally, then there is the issue of managing the life cycle of these models. All these individual models need to be deployed to production, their performance needs to be monitored to check for model and data drift, and they need to be retrained at a set frequency. This becomes increasingly complex as we have more and more time series to forecast.</p>
    <p class="normal">However, by shifting to a GFM paradigm, we drastically reduce the time and effort required to train and manage a machine learning model throughout its life cycle. As we will see in this chapter, training a GFM on these 150 households takes only a fraction of the time it takes to train LFMs.</p>
    <p class="normal">Despite all the advantages of GFMs, they are not without some drawbacks. The main drawback is that we are assuming that all the time series in a dataset are generated by a single <strong class="keyWord">Data Generating Process</strong> (<strong class="keyWord">DGP</strong>). This might not be a valid assumption and this can lead to the GFM underfitting some specific types of time series patterns that are underrepresented in the dataset.</p>
    <p class="normal">Another open issue is whether a GFM is good for use with unrelated tasks or time series. The jury is out on this one, but Montero-Manso et al. proved that there are also gains in modeling unrelated time series with a GFM. The same finding has been put forward, although from another perspective, by Oreshkin et al., who trained a global model on the M4 dataset (a set of unrelated datasets) and obtained state-of-the-art performance. They attributed it to the meta-learning capabilities of the model. </p>
    <p class="normal">That being said, relatedness does help the GFM, as the learning task becomes easier this way. We will see a practical application of this in upcoming sections of this chapter as well.</p>
    <p class="normal">In the larger scheme of <a id="_idIndexMarker747"/>things, the benefits we derive from a GFM paradigm far outweigh the drawbacks. On most tasks, the GFMs either perform on par with or better than local models. It has been proven theoretically as well, by Montero-Manso et al., that a GFM, in a worst-case scenario, learns the same function as a local model. We will see this clearly in the models we are going to train in the upcoming sections. Finally, the training time and engineering complexity drop drastically as you move to a GFM paradigm.</p>
    <p class="normal">Now that we have explained why a GFM is a worthwhile paradigm to adopt, let’s see how we can train one.</p>
    <h1 id="_idParaDest-221" class="heading-1">Creating GFMs</h1>
    <p class="normal">Training a <a id="_idIndexMarker748"/>GFM is very straightforward. While we were training LFMs in <em class="chapterRef">Chapter 8</em>, <em class="italic">Forecasting Time Series with Machine Learning Models</em>, we were looping over different households in the London Smart Meters dataset and training a model for each household. However, if we just take all the households into a single dataframe (our dataset is already that way) and train a single model on it, we get a GFM. One thing we want to keep in mind is to make sure that all the time series in the dataset have the same frequency. In other words, if we mix daily time series with weekly ones while training these models, the performance drop will be noticeable—especially if we are using time-varying features and other time-based information. For a purely autoregressive model, mixing time series in this way is much less of a problem.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert:</strong></p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">01-Global_Forecasting_Models-ML.ipynb</code> in the <code class="inlineCode">Chapter10</code> folder.</p>
    </div>
    <p class="normal">The standard framework we developed in <em class="chapterRef">Chapter 8</em>, <em class="italic">Forecasting Time Series with Machine Learning Models</em>, is general enough to work for GFMs as well. So, as we did in that chapter, we define <code class="inlineCode">FeatureConfig</code> and <code class="inlineCode">MissingValueConfig</code> in the <code class="inlineCode">01-Global_Forecasting_Models-ML.ipynb</code> notebook. We also slightly tweaked the Python function to train and evaluate the machine learning to make it work for all households. The details and <a id="_idIndexMarker749"/>exact functions can be found in the notebook.</p>
    <p class="normal">Now, instead of looping over different households, we input the entire training dataset into the <code class="inlineCode">get_X_y</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Define the ModelConfig</span>
<span class="hljs-keyword">from</span> lightgbm <span class="hljs-keyword">import</span> LGBMRegressor
model_config = ModelConfig(
    model=LGBMRegressor(random_state=<span class="hljs-number">42</span>),
    name=<span class="hljs-string">"Global LightGBM Baseline"</span>,
    <span class="hljs-comment"># LGBM is not sensitive to normalized data</span>
    normalize=<span class="hljs-literal">False</span>,
    <span class="hljs-comment"># LGBM can handle missing values</span>
    fill_missing=<span class="hljs-literal">False</span>,
)
<span class="hljs-comment"># Get train and test data</span>
train_features, train_target, train_original_target = feat_config.get_X_y(
    train_df, categorical=<span class="hljs-literal">True</span>, exogenous=<span class="hljs-literal">False</span>
)
test_features, test_target, test_original_target = feat_config.get_X_y(
    test_df, categorical=<span class="hljs-literal">True</span>, exogenous=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">Now that we have the data, we need to train the model. Training the model is also exactly the same as we saw in <em class="chapterRef">Chapter 8</em>, <em class="italic">Forecasting Time Series with Machine Learning Models</em>. We will just choose LightGBM, which was the best-performing LFM model, and use functions we defined earlier to train the model and evaluate the results:</p>
    <pre class="programlisting code"><code class="hljs-code">y_pred, feat_df = train_model(
        model_config,
        _feat_config,
        missing_value_config,
        train_features,
        train_target,
        test_features,
    )
agg_metrics, eval_metrics_df = evaluate_forecast(
    y_pred, test_target, train_target, model_config
)
</code></pre>
    <p class="normal">Now, in <code class="inlineCode">y_pred</code>, we<a id="_idIndexMarker750"/> will have the forecast for all the households and <code class="inlineCode">feat_df</code> will have the feature importance. <code class="inlineCode">agg_metrics</code> will have the aggregated metric for all the selected households.</p>
    <p class="normal">Let’s look at how well our GFM model did:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_01.png" alt="Figure 10.1 – Aggregate metrics with the baseline GFM "/></figure>
    <p class="packt_figref">Figure 10.1: Aggregate metrics with the baseline GFM</p>
    <p class="normal">We are not doing better than the best LFM (in the first row) in terms of the metrics. However, one thing we should note is the time taken to train the model—~30 seconds. The LFM for all the selected households was taking ~30 minutes. This huge reduction in time taken gives us a lot of flexibility to iterate faster with different features and techniques.</p>
    <p class="normal">With that said, let’s now look at a few techniques with which we can improve the accuracy of the GFMs.</p>
    <h1 id="_idParaDest-222" class="heading-1">Strategies to improve GFMs</h1>
    <p class="normal">GFMs have<a id="_idIndexMarker751"/> been in use in many forecasting competitions in Kaggle and outside of it. They have been battle-tested <a id="_idIndexMarker752"/>empirically, although very little work has gone into examining why they work so well from a theoretical point of view. Montero-Manso and Hyndman (2020) have a working paper titled <em class="italic">Principles and Algorithms for Forecasting Groups of Time Series: Locality and Globality</em>, which is an in-depth investigation, both theoretical and empirical, of GFMs and the many techniques that have been developed by the data science community collectively. In this section, we will try to include strategies to improve GFMs and, wherever possible, try to give theoretical justifications for why they would work.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check:</strong></p>
      <p class="normal">The Montero-Manso and Hyndman (2020) research paper is cited in <em class="italic">References</em> as reference <em class="italic">1</em>.</p>
    </div>
    <p class="normal">In the paper, Montero-Manso and Hyndman use a basic result in machine learning about generalization error to carry out the theoretical analysis, and it is worth spending a bit of time understanding that, at least at a high level. <code class="inlineCode">Generalization error</code>, as we know, is the difference between out-of-sample error and in-sample error. Yaser S Abu-Mostafa has a free, online <strong class="keyWord">Massive Open Online Course</strong> (<strong class="keyWord">MOOC</strong>) and an associated book (both of which are linked in the <em class="italic">Further reading</em> section). It is a short course on machine learning and is a course that I would recommend to anyone in the machine learning field for developing a stronger theoretical and conceptual basis for what we do. One of the important concepts the course and book put forward is the use of Hoeffding’s inequality from probability theory to derive bounds on a learning problem. Let’s quickly look at the result to develop our understanding:</p>
    <p class="center"><img src="../Images/B22389_10_001.png" alt=""/></p>
    <p class="normal">It has a probability of at least 1<strong class="keyWord">-<img src="../Images/B22389_10_002.png" alt=""/>.</strong></p>
    <p class="normal"><em class="italic">E</em><sub class="subscript-italic" style="font-style: italic;">in</sub> is the in-sample average error and <em class="italic">E</em><sub class="subscript-italic" style="font-style: italic;">out</sub> is the expected out-of-sample error. <em class="italic">N</em> is the total number of samples in the dataset from which we are learning and <em class="italic">H</em> is the hypothesis class of models. It is a finite set of functions that can potentially fit the data. The size of <em class="italic">H</em>, denoted by |<em class="italic">H</em>|, is the complexity of <em class="italic">H</em>. Although the formula of the bound looks intimidating, let’s simplify the way we look at it to develop the necessary understanding.</p>
    <p class="normal">We want <em class="italic">E</em><sub class="subscript-italic" style="font-style: italic;">out</sub> to be as close to <em class="italic">E</em><sub class="subscript-italic" style="font-style: italic;">in</sub> as possible, and for that, we need the terms in the square root to be as small as possible. There are two terms under the square root that are in our <em class="italic">control</em>, so to speak—<em class="italic">N</em> and |<em class="italic">H</em>|. Therefore, to make the generalization error (<em class="italic">E</em><sub class="subscript-italic" style="font-style: italic;">in</sub>- <em class="italic">E</em><sub class="subscript-italic" style="font-style: italic;">out</sub>) as <a id="_idIndexMarker753"/>small as possible, we either need to increase <em class="italic">N</em> (have more data) or decrease |<em class="italic">H</em>| (have a less complex model). This is a result that is applicable to all machine learning but Montero-Manso and Hyndman, with a few assumptions, made this applicable to time series models as well. It is this result that they used to give theoretical backing to the arguments put forward in their working paper.</p>
    <p class="normal">Montero-Manso and Hyndman have taken Hoeffding’s inequality and applied it to LFMs and GFMs to compare them. We<a id="_idIndexMarker754"/> can see the result here (for a full mathematical and statistical understanding, refer to the original paper under <em class="italic">References</em>):</p>
    <p class="center"><img src="../Images/B22389_10_003.png" alt=""/></p>
    <p class="center"><img src="../Images/B22389_10_004.png" alt=""/></p>
    <p class="normal"><img src="../Images/B22389_10_005.png" alt=""/> and<strong class="keyWord"> <img src="../Images/B22389_10_006.png" alt=""/></strong> are the average in-sample errors across all the time series using the local and global approaches, respectively.<strong class="keyWord"> <img src="../Images/B22389_10_006.png" alt=""/></strong> and<strong class="keyWord"> <img src="../Images/B22389_10_008.png" alt=""/></strong> are the out-of-sample expectations under the local and global approaches, respectively. <em class="italic">H</em><sub class="subscript-italic" style="font-style: italic;">i</sub> is the hypothesis class for the <em class="italic">i</em>-th time series and <em class="italic">J</em> is the hypothesis class for the global approach (the global approach only fits a single function and hence, has just a single hypothesis class).</p>
    <p class="normal">One of the most interesting results that comes out of this is that the complexity term for LFMs (<img src="../Images/B22389_10_009.png" alt=""/>) grows the size of the dataset. The greater the number of time series we have in the dataset, the more complexity and the worse the generalization error, whereas with GFMs, the complexity term (<em class="italic">log</em>(|<em class="italic">J</em>|)) stays constant. Therefore, for a dataset of moderate size, the overall complexity of LFMs (such as exponential smoothing) can be much higher than a single GFM, no matter <a id="_idIndexMarker755"/>how complex the GFM is. As a corollary, we can also think that with the available dataset (<em class="italic">NK</em>), we <a id="_idIndexMarker756"/>can afford to train a model with much higher complexity than a model for LFMs. There are many ways to increase the complexity of the model, which we will see in the following section.</p>
    <p class="normal">Now, let’s return to the GFMs we were training. We saw that the performance of the GFM we trained was not up to the mark when we compared it with the best LFM (LightGBM), but it is better than the baseline and other models we tried, so right off the bat, we know the GFM we trained is not terrible. Now, let’s look at a few ways to improve the performance of the model.</p>
    <h2 id="_idParaDest-223" class="heading-2">Increasing memory</h2>
    <p class="normal">As we <a id="_idIndexMarker757"/>discussed in <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, the machine learning models that we discuss in this book are finite memory models or Markov models. A model such as exponential smoothing takes into account the entire history of a time series while forecasting, but models such as any of the machine learning models we discussed only take in finite memory to make their predictions. In a finite memory model, the amount of memory we allow the model to access is called the size of the memory (<em class="italic">M</em>) or order of autoregression (from econometrics).</p>
    <p class="normal">Providing a greater amount of memory to the model increases the complexity of the model. Therefore, one of the ways to increase the performance of the GFM is to increase the amount of memory the model has access to. There are many ways to increase the amount of memory.</p>
    <h3 id="_idParaDest-224" class="heading-3">Adding more lag features</h3>
    <p class="normal">If you have prior exposure <a id="_idIndexMarker758"/>to ARIMA models, you will know that the number of <strong class="keyWord">Autoregressive</strong> (<strong class="keyWord">AR</strong>) terms are sparingly used. We usually see AR models with single-digit lags. There is nothing stopping us from running ARIMA models with larger lags, but since we do run ARIMA in the LFM paradigm, the model has to learn the parameters of all the lags using limited data and therefore, in practice, practitioners commonly choose smaller lags. However, when we are moving to GFMs, we can afford to have much larger lags. Montero-Manso and Hyndman empirically showed the benefits of adding more lags to GFMs. For highly seasonal time series, a peculiar phenomenon was observed. The accuracy improves with an increase in lags, but it then saturates and suddenly worsens when the lag becomes equal to the seasonal cycle. On further increasing the lags beyond the seasonal cycle, the accuracy shows huge gains. This may be because of the overfitting that happens because of seasonality. It becomes very easy for the model to favor the seasonal lag because it works very well in a sample, so it’s better to add a few more lags on the plus side of the seasonal cycle.</p>
    <h3 id="_idParaDest-225" class="heading-3">Adding rolling features</h3>
    <p class="normal">Another <a id="_idIndexMarker759"/>way to increase the memory of the model is to include rolling averages as features. Rolling averages take information from extended windows on memory and encode that information by way of descriptive statistics (such as the mean or max). This is an efficient way of including the memory because we can take very large windows for memory and include the information as a single feature in the model.</p>
    <h3 id="_idParaDest-226" class="heading-3">Adding EWMA features</h3>
    <p class="normal">An <strong class="keyWord">Exponentially Weighted Moving Average</strong> (<strong class="keyWord">EWMA</strong>) is a way to include infinite memory<a id="_idIndexMarker760"/> in a finite memory model. The EWMA essentially takes the average of the entire history but is weighted according to the <img src="../Images/B22389_04_009.png" alt=""/> that we set. Therefore, with different values of <img src="../Images/B22389_04_009.png" alt=""/>, we get different kinds of memory, again encoded as a single feature. Including different EWMA features has also <a id="_idIndexMarker761"/>empirically proved beneficial.</p>
    <p class="normal">We have already included these kinds of features in our feature engineering (<em class="chapterRef">Chapter 6</em>, <em class="italic">Feature Engineering for Time Series Forecasting</em>), and they are part of the baseline GFM we trained, so let’s move on to the next strategy for improving the accuracy of GFMs.</p>
    <h2 id="_idParaDest-227" class="heading-2">Using time series meta-features</h2>
    <p class="normal">The<a id="_idIndexMarker762"/> baseline GFM we trained earlier in the <em class="italic">Creating Global Forecasting Models (GFMs)</em> section had lag features, rolling features, and EWMA features, but we have given no feature that helps the <a id="_idIndexMarker763"/>model distinguish between different time series in the dataset. The baseline GFM model learned a generalized function that generates a forecast provided the features. This might work well enough for homogenous datasets where all the time series are very similar in nature, but for heterogenous datasets, the information with which the model can distinguish each time series comes in handy.</p>
    <p class="normal">So, information about the time series itself is what we call meta-features. In a retail context, it can be the product ID, the category of products, the store number, and so on. In our dataset, we have features such as <code class="inlineCode">stdorToU</code>, <code class="inlineCode">Acorn</code>, <code class="inlineCode">Acorn_grouped</code>, and <code class="inlineCode">LCLid</code>, which give some information about the time series itself. Including these meta-features in the GFM will improve the performance of the model.</p>
    <p class="normal">However, there is just one problem—more often than not, these meta-features are categorical in nature. A feature is categorical when the values in the feature can only take discrete values. For instance, <code class="inlineCode">Acorn_grouped</code> can only have one of three values—<code class="inlineCode">Affluent</code>, <code class="inlineCode">Comfortable</code>, or <code class="inlineCode">Adversity</code>. Most machine learning models do not work well with categorical features. All the models in scikit-learn, the most popular machine learning library in the Python ecosystem, do not allow categorical features at all. To include categorical features in machine learning models, we need to encode them into numerical form, and there are many ways to encode categorical columns. Let’s review a few popular options.</p>
    <h3 id="_idParaDest-228" class="heading-3">Ordinal encoding and one-hot encoding</h3>
    <p class="normal">The most popular ways of encoding categorical features are ordinal encoding and one-hot encoding, but they are not always the best choices. Let’s quickly review what these techniques are and when they are suitable.</p>
    <p class="normal">Ordinal encoding<a id="_idIndexMarker764"/> is the simplest of them all. We<a id="_idIndexMarker765"/> simply assign a numerical code to the unique values of a category and then replace the categorical value with the numerical code. To encode the <code class="inlineCode">Acorn_grouped</code> feature from our dataset, all we need to do is assign codes, say <code class="inlineCode">1</code> for <code class="inlineCode">Affluent</code>, <code class="inlineCode">2</code> for <code class="inlineCode">Comfortable</code>, and <code class="inlineCode">3</code> for <code class="inlineCode">Adversity</code>, and replace all instances of the categorical values with the code we assigned. While this is really easy, this kind of encoding introduces meanings to the categorical values that we may or may not intend. When we assign numerical codes, we are implicitly saying that the categorical value that gets assigned <code class="inlineCode">2</code> as a code is better than the categorical value with <code class="inlineCode">1</code> as the code. This kind of encoding only works for ordinal features (features whose categorical values have an intrinsic sense of rank in their meaning) and should be sparingly used. Another way we can think about the problem is in terms of distance. When we do ordinal encoding, the distance between <code class="inlineCode">Comfortable</code> and <code class="inlineCode">Affluent</code> can be higher than the distance between <code class="inlineCode">Comfortable</code> and <code class="inlineCode">Adversity</code>, depending on the way we encode.</p>
    <p class="normal">One-hot encoding <a id="_idIndexMarker766"/>is a better way of representing<a id="_idIndexMarker767"/> categorical features with no ordinal meaning. It essentially encodes the categorical features in a higher dimension, placing the categorical values equally distant in that space. The size of the dimension it requires to encode the categorical values is equal to the cardinality of the categorical variable. Cardinality is the number of unique values in the categorical feature. Let’s see how sample data would be encoded in a one-hot encoding scheme:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_02.png" alt="Figure 10.2 – One-hot encoding of categorical features "/></figure>
    <p class="packt_figref">Figure 10.2: One-hot encoding of categorical features</p>
    <p class="normal">We can see that the<a id="_idIndexMarker768"/> resulting encoding will have a column for each unique value in the categorical feature and the value is indicated by <code class="inlineCode">1</code> in the column. For instance, the first row is <code class="inlineCode">Comfortable</code>, and therefore, every other column except the <code class="inlineCode">Comfortable</code> column will have <code class="inlineCode">0</code> and the <code class="inlineCode">Comfortable</code> column will have <code class="inlineCode">1</code>. If we calculate the Euclidean distance between any two categorical values, we can see that they are the same.</p>
    <p class="normal">However, there are three main issues with this encoding, all of which become a problem with high cardinality categorical variables:</p>
    <ul>
      <li class="bulletList">The embedding is inherently sparse and many machine learning models (for instance, tree-based models and neural networks) do not really work well with sparse data (sparse data is when a majority of values in the data are zeros). When the cardinality is just 5 or 10, the sparsity introduced may not be that much of a problem, but when we consider a cardinality of 100 or 500, the encoding becomes really sparse.</li>
      <li class="bulletList">Another issue is the explosion of dimensions of the problem. When we increase the total number of features of a problem due to the large number of new features that are created through one-hot encoding, we make the problem harder to solve. This can be explained by the curse of dimensionality. The <em class="italic">Further reading</em> section has a link with more information on the curse of dimensionality.</li>
      <li class="bulletList">The last problem is related to practical concerns. For a large dataset, if we one-hot encode a categorical value with hundreds or thousands of unique values, the resulting dataframe is not going to be easy to work with because it will not fit in the computer memory.</li>
    </ul>
    <p class="normal">There is a slightly different way of one-hot encoding where we drop one of these dimensions, called <strong class="keyWord">dummy variable encoding</strong>. This has the added benefit of making the encoding linearly<a id="_idIndexMarker769"/> independent, which, in turn, has some advantages, especially for vanilla linear regression. The <em class="italic">Further reading</em> section has a link if you want to know more.</p>
    <p class="normal">Since the categorical columns that we must encode have high cardinality (at least a few of them), we will not be doing this encoding. Instead, let’s look at a few encoding techniques that can handle high cardinality categorical variables better.</p>
    <h3 id="_idParaDest-229" class="heading-3">Frequency encoding</h3>
    <p class="normal">Frequency encoding<a id="_idIndexMarker770"/> is an encoding schema that does not increase the dimensions of the problem. It takes a single categorical <a id="_idIndexMarker771"/>array and returns a single numeric array. The logic is very simple—it replaces the categorical values with the number of times the value occurs in the training dataset. Although it’s not perfect, this works pretty well, as it lets the model distinguish between different categories based on how frequently they occur.</p>
    <p class="normal">There is a popular library, <code class="inlineCode">category_encoders</code>, that implements a lot of different encoding schemes in a standard scikit-learn style estimator, and we will be using that in our experiments as well. The standard framework we developed in <em class="chapterRef">Chapter 8</em>, <em class="italic">Forecasting Time Series with Machine Learning Models</em>, also had a couple of functionalities that we didn’t use—<code class="inlineCode">encode_categorical</code> and <code class="inlineCode">categorical_encoder</code>. </p>
    <p class="normal">So, let’s use them and train our model now:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> category_encoders <span class="hljs-keyword">import</span> CountEncoder
<span class="hljs-keyword">from</span> lightgbm <span class="hljs-keyword">import</span> LGBMRegressor
<span class="hljs-comment">#Define which columns names are categorical features</span>
cat_encoder = CountEncoder(cols=cat_features)
model_config = ModelConfig(
    model=LGBMRegressor(random_state=<span class="hljs-number">42</span>),
    name=<span class="hljs-string">"Global LightGBM with Meta Features (CountEncoder)"</span>,
    <span class="hljs-comment"># LGBM is not sensitive to normalized data</span>
    normalize=<span class="hljs-literal">False</span>,
    <span class="hljs-comment"># LGBM can handle missing values</span>
    fill_missing=<span class="hljs-literal">False</span>,
    <span class="hljs-comment"># Turn on categorical encoding</span>
    encode_categorical=<span class="hljs-literal">True</span>,
    <span class="hljs-comment"># Pass the categorical encoder to be used</span>
    categorical_encoder=cat_encoder
)
</code></pre>
    <p class="normal">The rest <a id="_idIndexMarker772"/>of the process is the same as what we saw in the <em class="italic">Creating Global Forecasting Models (GFMs)</em> section and we get the forecast <a id="_idIndexMarker773"/>using the encoded meta-features:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_03.png" alt="Figure 10.3 – Aggregate metrics with the GFM with meta-features (frequency encoding) "/></figure>
    <p class="packt_figref">Figure 10.3: Aggregate metrics with the GFM with meta-features (frequency encoding)</p>
    <p class="normal">Right away, we can see that there is a reduction in error, although it is minimal. We can also see that the training time has almost doubled. This may be because now we have an additional step of encoding the categorical features in addition to training the machine learning model.</p>
    <p class="normal">The main issue with frequency encoding is that it doesn’t work with features that are uniformly distributed in the dataset. For instance, the <code class="inlineCode">LCLid</code> feature, which is just a unique code for each household, is uniformly distributed in the dataset and when we use frequency encoding, all the <code class="inlineCode">LCLid</code> features will come to almost the same frequency, and hence the machine learning model considers them almost the same.</p>
    <p class="normal">Let’s now look at a slightly different approach.</p>
    <h3 id="_idParaDest-230" class="heading-3">Target mean encoding</h3>
    <p class="normal">Target<a id="_idIndexMarker774"/> mean encoding, in <a id="_idIndexMarker775"/>its most vanilla form, is a very simple concept. It is a <em class="italic">supervised</em> approach that uses the target in the training dataset to encode the categorical columns. Let’s look at an example:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_04.png" alt="Figure 10.4 – Target mean encoding "/></figure>
    <p class="packt_figref">Figure 10.4: Target mean encoding</p>
    <p class="normal">The <a id="_idIndexMarker776"/>vanilla target mean encoding has a few limitations. It increases the chance of overfitting the training data because <a id="_idIndexMarker777"/>we are using the mean targets directly and thereby leaking the target into the model in a way. Another problem with the approach is that when the categorical values are unevenly distributed, there may be a few categorical values with very small sample sizes, and therefore, the mean estimate becomes noisy. Extending this problem to the extreme, we get another case where an unseen categorical value comes up in test data. This is also not supported in the vanilla version. Therefore, in practice, this simple version is almost never used, but slightly more sophisticated versions of this concept are widely used and are an effective strategy for encoding categorical features.</p>
    <p class="normal">In <code class="inlineCode">category_encoders</code>, there are many variations of this concept, but let’s look at two popular and effective ones here.</p>
    <p class="normal">In 2001, Daniele Micci-Barreca proposed a variant of mean encoding. If we consider the target as a binary variable, say 1 and 0, the mean (which is the number of 1s or number of samples) is also the probability of having 1. Using this interpretation of the means, Daniele proposed blending two probabilities—prior and posterior probabilities—as the final encoding for the categorical features.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check:</strong></p>
      <p class="normal">The research paper by Daniele Micci-Barreca is cited in <em class="italic">References</em> as reference <em class="italic">2</em>.</p>
    </div>
    <p class="normal">The <a id="_idIndexMarker778"/>prior probability is defined as follows:</p>
    <p class="center"><img src="../Images/B22389_10_012.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">n</em><sub class="subscript-italic" style="font-style: italic;">y</sub> is <a id="_idIndexMarker779"/>the number of cases such that <em class="italic">target</em> = 1, and <em class="italic">n</em><sub class="subscript-italic" style="font-style: italic;">TR</sub> is the number of samples in the training data.</p>
    <p class="normal">The posterior probability is defined for category <em class="italic">i</em> as follows:</p>
    <p class="center"><img src="../Images/B22389_10_013.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">n</em><sub class="subscript-italic" style="font-style: italic;">iY</sub> is the number of samples in the dataset where <em class="italic">category</em> = <em class="italic">i</em> and <code class="inlineCode">Y = 1</code>, and <em class="italic">n</em><sub class="subscript-italic" style="font-style: italic;">i</sub> is the number of samples in the dataset where <em class="italic">category</em> = <em class="italic">i</em>.</p>
    <p class="normal">Now, the final encoding for category <em class="italic">i</em> is as follows:</p>
    <p class="center"><img src="../Images/B22389_10_014.png" alt=""/></p>
    <p class="normal">Here, <img src="../Images/B22389_05_008.png" alt=""/> is the weighting factor, which is a monotonically increasing function on <em class="italic">n</em><sub class="subscript-italic" style="font-style: italic;">i</sub> that is bounded between 0 and 1. So, this function gives a larger weight to the posterior probability as the number of samples increases.</p>
    <p class="normal">Adapting this to the regression setting, the probabilities change to expected values so that the formula becomes the following:</p>
    <p class="center"><img src="../Images/B22389_10_016.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">TR</em><sub class="subscript-italic" style="font-style: italic;">i</sub> is all the rows where <em class="italic">category</em> = 1 and <img src="../Images/B22389_10_017.png" alt=""/> is the sum of <em class="italic">Y</em> for <em class="italic">TR</em><sub class="subscript-italic" style="font-style: italic;">i</sub>.<img src="../Images/B22389_10_018.png" alt=""/> is the sum of <em class="italic">Y</em> for all the rows in the training dataset. As with the binary variable, we are mixing the expected value of <em class="italic">Y</em>, given <em class="italic">category</em> = <em class="italic">i</em> (<em class="italic">E</em>[<em class="italic">Y</em>|<em class="italic">category</em> = <em class="italic">i</em>]) and the expected value of <em class="italic">Y</em> (<em class="italic">E</em>[<em class="italic">Y</em>]) for the final categorical encoding.</p>
    <p class="normal">There <a id="_idIndexMarker780"/>are many functions that we can use for <img src="../Images/B22389_05_008.png" alt=""/>. Daniele mentions a very common<a id="_idIndexMarker781"/> functional form (sigmoid):</p>
    <p class="center"><img src="../Images/B22389_10_020.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">n</em><sub class="subscript-italic" style="font-style: italic;">i</sub> is the number of samples in the dataset where, <em class="italic">category</em> = <em class="italic">i</em> and <em class="italic">k</em> and <em class="italic">f</em> are tunable hyperparameters. <em class="italic">k</em> determines half of the minimal sample size for which we completely trust the estimate. If <code class="inlineCode">k = 1</code>, what we are saying is that we trust the posterior estimate from a category that has only two samples. <em class="italic">f</em> determines how fast the sigmoid transitions between the two extremes. As <em class="italic">f</em> tends to infinity, the transition becomes a hard threshold between prior and posterior probabilities. <code class="inlineCode">TargetEncoder</code> from <code class="inlineCode">category_encoders</code> has implemented this <img src="../Images/B22389_05_008.png" alt=""/>. The <em class="italic">k</em> parameter is called <code class="inlineCode">min_samples_leaf</code> with a default value of 1, and the <em class="italic">f</em> parameter is called <code class="inlineCode">smoothing</code> with a default value of 1. Let’s see how this encoding works on our problem. Using a different encoder in the framework we are working on is as simple as passing a different <code class="inlineCode">cat_encoder</code> (the initialized categorical encoder) to <code class="inlineCode">ModelConfig</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> category_encoders <span class="hljs-keyword">import</span> TargetEncoder
cat_encoder = TargetEncoder(cols=cat_features)
</code></pre>
    <p class="normal">The rest of the code is exactly the same. We can find the full code in the corresponding notebook. Let’s see how well the new encoding has done:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_05.png" alt="Figure 10.5 – Aggregate metrics with the GFM with meta-features (target encoding) "/></figure>
    <p class="packt_figref">Figure 10.5: Aggregate metrics with the GFM with meta-features (target encoding)</p>
    <p class="normal">It’s <a id="_idIndexMarker782"/>not doing that well, is it? As with machine learning <a id="_idIndexMarker783"/>models, the <strong class="keyWord">No Free Lunch Theorem</strong> (<strong class="keyWord">NFLT</strong>) applies to categorical encoding as well. There <a id="_idIndexMarker784"/>is no one encoding scheme that works well all the time. Although not directly related to the topic, if you want to know more about the NFLT, head to the<em class="italic"> Further reading </em>section.</p>
    <div class="note">
      <p class="normal">With all these <em class="italic">supervised</em> categorical encoding techniques, such as target mean encoding, we have to be really careful not to induce data leakage. The encoder should be fit using training data and not using the validation or test data. Another very popular technique is to generate categorical encoding using cross-validation and use the out-of-sample encodings to absolutely avoid data leakage or overfitting.</p>
    </div>
    <p class="normal">There are many more encoding schemes, such as <code class="inlineCode">MEstimateEncoder</code> (which uses additive smoothing as the <img src="../Images/B22389_05_008.png" alt=""/>), <code class="inlineCode">HashingEncoder</code>, and so on, in <code class="inlineCode">category_encoders</code>. Another very effective way of encoding categorical features is using embedding from deep learning. The <em class="italic">Further reading</em> section has a link to a tutorial for doing this.</p>
    <p class="normal">Before now, all this categorical encoding was a separate step before the modeling. Now, let’s look at a technique that considers categorical features natively for model training.</p>
    <h3 id="_idParaDest-231" class="heading-3">LightGBM’s native handling of categorical features</h3>
    <p class="normal">A few<a id="_idIndexMarker785"/> machine learning model implementations handle categorical features natively, especially gradient-boosting models. CatBoost and LightGBM, two of the most popular GBM implementations, handle categorical features out of the box. CatBoost has a unique way of encoding categorical features into numerical ones internally using something similar to additive smoothing. The <em class="italic">Further reading</em> section has links to further information on how this encoding is done. <code class="inlineCode">category_encoders</code> has implemented this logic as <code class="inlineCode">CatBoostEncoder</code> so that we can use this type of encoding for any machine learning model as well.</p>
    <p class="normal">While CatBoost handles this internal conversion into numerical features, LightGBM takes a more native approach to dealing with categorical features. LightGBM considers the categorical features as is while growing and splitting the trees. For a categorical feature with <em class="italic">k</em> unique values (cardinality of <em class="italic">k</em>), there are 2<sup class="superscript-italic" style="font-style: italic;">k</sup><sup class="superscript">-1</sup>-1 possible partitions. This soon becomes intractable, but for regression trees, Walter D. Fisher proposed a technique back in 1958 that makes the complexity of finding an optimal split much less. The essence of the method is to use average target statistics for each categorical value to order them and then find the optimal split in the ordered categorical values.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check:</strong></p>
      <p class="normal">The research paper by Fisher is cited in <em class="italic">References</em> as reference <em class="italic">3</em>.</p>
    </div>
    <p class="normal">LightGBM’s <code class="inlineCode">scikit-learn</code> API supports this feature by taking an argument, <code class="inlineCode">categorical_feature</code>, which has a list of categorical feature names, during <code class="inlineCode">fit</code>. We can use the <code class="inlineCode">fit_kwargs</code> argument in the fit of our <code class="inlineCode">MLModel</code> that we defined in <em class="chapterRef">Chapter 8</em>, <em class="italic">Forecasting Time Series with Machine Learning Models</em>, to pass in this parameter. Let’s see how we can do this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> lightgbm <span class="hljs-keyword">import</span> LGBMRegressor
model_config = ModelConfig(
    model=LGBMRegressor(random_state=<span class="hljs-number">42</span>),
    name=<span class="hljs-string">"Global LightGBM with Meta Features (NativeLGBM)"</span>,
    <span class="hljs-comment"># LGBM is not sensitive to normalized data</span>
    normalize=<span class="hljs-literal">False</span>,
    <span class="hljs-comment"># LGBM can handle missing values</span>
    fill_missing=<span class="hljs-literal">False</span>,
    <span class="hljs-comment"># We are using inbuilt categorical feature handling</span>
    encode_categorical=<span class="hljs-literal">False</span>,
)
<span class="hljs-comment"># Training the model and passing in fit_kwargs</span>
y_pred, feat_df = train_model(
    model_config,
    _feat_config,
    missing_value_config,
    train_features,
    train_target,
    test_features,
    fit_kwargs=<span class="hljs-built_in">dict</span>(categorical_feature=cat_features),
)
</code></pre>
    <p class="normal"><code class="inlineCode">y_pred</code> has <a id="_idIndexMarker786"/>the forecasts, which we evaluate as usual. Let’s also see the results:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_06.png" alt="Figure 10.6 – Aggregate metrics with the GFM with meta-features (native LightGBM) "/></figure>
    <p class="packt_figref">Figure 10.6: Aggregate metrics with the GFM with meta-features (native LightGBM)</p>
    <p class="normal">We can observe a good reduction in <code class="inlineCode">MAE</code> as well as <code class="inlineCode">meanMASE</code> with the native handling of categorical features. We can also see a reduction in the total training time because we don’t have a separate step for encoding the categorical feature. Empirically, the native handling of categorical features works better most of the time.</p>
    <p class="normal">Now that we have encoded the categorical features, let’s look at another way to improve accuracy.</p>
    <h2 id="_idParaDest-232" class="heading-2">Tuning hyperparameters</h2>
    <p class="normal">A <a id="_idIndexMarker787"/>hyperparameter is a setting that controls <a id="_idIndexMarker788"/>how a machine learning model is trained but is not learned from the data. In contrast, model parameters are learned from the data during training. For example, in <strong class="keyWord">Gradient Boosting Decision Trees</strong> (<strong class="keyWord">GBDT</strong>), model parameters<a id="_idIndexMarker789"/> are the <em class="italic">decision thresholds in each tree</em>, learned from the data. Hyperparameters, like the <em class="italic">number of trees</em>, <em class="italic">learning rate</em>, and <em class="italic">tree depth</em>, are set before training and control the model’s structure and how it learns. While parameters adjust based on the data, hyperparameters must be tuned externally.</p>
    <p class="normal">Although<a id="_idIndexMarker790"/> hyperparameter tuning is common practice in machine learning, we haven’t been able to do so because of the sheer number of models we had under the LFM paradigm. Now that we have a GFM that finishes training in 30 seconds, hyperparameter tuning becomes feasible. From a theoretical perspective, we also saw that GFMs can afford a larger complexity and can therefore evaluate a greater number of functions to pick the best without overfitting.</p>
    <p class="normal">Mathematical optimization is defined as the selection of a best element, with regard to some criterion, from some set of available alternatives. In most cases, this involves finding the maximum or minimum value of some <a id="_idIndexMarker791"/>function (an <strong class="keyWord">objective function</strong>) from a set of <a id="_idIndexMarker792"/>alternatives (the <strong class="keyWord">search space</strong>) subject to some <a id="_idIndexMarker793"/>conditions (<strong class="keyWord">constraints</strong>). The search space can be discrete variables, continuous variables, or a mixture of both, and the objective function can be differentiable or non-differentiable. There is a large body of research that tackles these variations.</p>
    <p class="normal">You may be wondering why we are talking about mathematical optimization now, right? Hyperparameter tuning is a mathematical optimization problem. The objective function here is non-differentiable and <a id="_idIndexMarker794"/>returns the metric for which we are optimizing—for instance, the <strong class="keyWord">Mean Absolute Error</strong> (<strong class="keyWord">MAE</strong>). </p>
    <p class="normal">The search space comprises the different hyperparameters we are tuning—say, the number of trees or depth of the trees. It could be a mixture<a id="_idIndexMarker795"/> of continuous and discrete variables and the constraints would be any restriction on the search space we impose—for instance, a particular hyperparameter cannot be negative, or a particular combination of hyperparameters cannot occur. Therefore, being aware of the terms used in mathematical optimization will help us in our discussion.</p>
    <p class="normal">Even though hyperparameter tuning is a standard machine learning concept, we will quickly review three main techniques (besides manual trial and error) for doing hyperparameter tuning.</p>
    <h3 id="_idParaDest-233" class="heading-3">Grid search</h3>
    <p class="normal">Grid search <a id="_idIndexMarker796"/>can be thought of as a brute-force method where we define a discrete grid over the search space, check the objective function at each point in the grid, and pick the best point in that grid. The grid is defined as a set of discrete points for each of the hyperparameters we choose to tune. Once the grid is defined, all the intersections of the grid are evaluated to search for the best objective value. If we are tuning 5 hyperparameters and the grid has 20 discrete values for each parameter, the total number of trials for a grid search would be 3,200,000 (20<sup class="superscript">5</sup>). This means training a model 3.2 million times and evaluating it. This becomes quite limiting because most modern machine learning models have many hyperparameters. For instance, LightGBM has more than 100, and out of those, at least 20 are highly impactful parameters when tuned. So, using a brute force approach such as grid search forces us to make the search space quite small so that it becomes feasible to carry out the tuning in a reasonable amount of time.</p>
    <p class="normal">For our case, we have defined a very small grid of just 27 trials by limiting ourselves to a really small search space. Let’s see how we do that:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> ParameterGrid
grid_params = {
    <span class="hljs-string">"num_leaves"</span>: [<span class="hljs-number">16</span>, <span class="hljs-number">31</span>, <span class="hljs-number">63</span>],
    <span class="hljs-string">"objective"</span>: [<span class="hljs-string">"regression"</span>, <span class="hljs-string">"regression_l1"</span>, <span class="hljs-string">"huber"</span>],
    <span class="hljs-string">"random_state"</span>: [<span class="hljs-number">42</span>],
    <span class="hljs-string">"colsample_bytree"</span>: [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">1.0</span>],
}
parameter_space = <span class="hljs-built_in">list</span>(ParameterGrid(grid_params))
</code></pre>
    <p class="normal">We just tune three hyperparameters (<code class="inlineCode">num_leaves</code>, <code class="inlineCode">objective</code>, and <code class="inlineCode">colsample_bytree</code>), and with just three options for each parameter. Performing the grid search after this is just about looping over the parameter space and evaluating the model at each combination of hyperparameters:</p>
    <pre class="programlisting code"><code class="hljs-code">scores = []
<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> tqdm(parameter_space, desc=<span class="hljs-string">"Performing Grid Search"</span>):
    _model_config = ModelConfig(
        model=LGBMRegressor(**p, verbose=-<span class="hljs-number">1</span>),
        name=<span class="hljs-string">"Global Meta LightGBM Tuning"</span>,
        <span class="hljs-comment"># LGBM is not sensitive to normalized data</span>
        normalize=<span class="hljs-literal">False</span>,
        <span class="hljs-comment"># LGBM can handle missing values</span>
        fill_missing=<span class="hljs-literal">False</span>,
    )
    y_pred, feat_df = train_model(
        _model_config,
        _feat_config,
        missing_value_config,
        train_features,
        train_target,
        test_features,
        fit_kwargs=<span class="hljs-built_in">dict</span>(categorical_feature=cat_features),
    )
    scores.append(ts_utils.mae(
                test_target[<span class="hljs-string">'energy_consumption'</span>], y_pred
            ))
</code></pre>
    <p class="normal">This <a id="_idIndexMarker797"/>takes about 15 minutes to complete and gives us the best MAE of <code class="inlineCode">0.73454</code>, which is already a great improvement from our untuned GFM.</p>
    <p class="normal">However, this makes us wonder whether there is an even better solution that we haven’t covered in the grid we defined. One option is to expand the grid and run the grid search again. This increases the number of trials exponentially and soon becomes infeasible.</p>
    <p class="normal">Let’s look at a different method where we can explore a larger search space with the same number of trials.</p>
    <h3 id="_idParaDest-234" class="heading-3">Random search</h3>
    <p class="normal">Random search <a id="_idIndexMarker798"/>takes a slightly different route. In random search, we also define the search space, but instead of discretely defining specific points in the space, we define probability distributions over the range we want to explore. These probability distributions can be anything from a uniform distribution (which says any point in the range is equally likely) to a Gaussian distribution (which has the familiar peak in the middle), or any other esoteric distributions, such as gamma or beta distributions. As long as we can sample from the distribution, we can use it for random search. Once we define the search space, we can sample points from the distribution and evaluate each of the points to find the best hyperparameter.</p>
    <p class="normal">While the number of trials is a function of the defined search space for grid search, it is a user input for random search, so we get to decide how much time or computational budget we need to use for hyperparameter tuning and, because of that, we can also search over a larger search space.</p>
    <p class="normal">With this new flexibility, let’s define a larger search space for our problem and use random search:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> scipy
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> ParameterSampler
random_search_params = {
    <span class="hljs-comment"># A uniform distribution between 10 and 100, but only integers</span>
    <span class="hljs-string">"num_leaves"</span>: scipy.stats.randint(<span class="hljs-number">10</span>,<span class="hljs-number">100</span>),
    <span class="hljs-comment"># A list of categorical string values</span>
    <span class="hljs-string">"objective"</span>: [<span class="hljs-string">"regression"</span>, <span class="hljs-string">"regression_l1"</span>, <span class="hljs-string">"huber"</span>],
    <span class="hljs-string">"random_state"</span>: [<span class="hljs-number">42</span>],
    <span class="hljs-comment"># List of floating point numbers between 0.3 and 1.0 with a resolution of 0.05</span>
    <span class="hljs-string">"colsample_bytree"</span>: np.arange(<span class="hljs-number">0.3</span>,<span class="hljs-number">1.0</span>,<span class="hljs-number">0.05</span>),
    <span class="hljs-comment"># List of floating point numbers between 0 and 10 with a resolution of 0.1</span>
    <span class="hljs-string">"lambda_l1"</span>:np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">0.1</span>),
    <span class="hljs-comment"># List of floating point numbers between 0 and 10 with a resolution of 0.1</span>
    <span class="hljs-string">"lambda_l2"</span>:np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">0.1</span>)
}
<span class="hljs-comment"># Sampling from the search space number of iterations times</span>
parameter_space = <span class="hljs-built_in">list</span>(ParameterSampler(random_search_params, n_iter=<span class="hljs-number">27</span>, random_state=<span class="hljs-number">42</span>))
</code></pre>
    <p class="normal">This<a id="_idIndexMarker799"/> also runs for about 15 minutes, but we have explored a larger search space. However, the best MAE reported was just <code class="inlineCode">0.73752</code>, which is lower than with grid search. Maybe if we run the search for a greater number of iterations, we will get a better score, but that is just a shot in the dark. Ironically, that is pretty much what random search also does. It closes its eyes and throws a dart at random places on the dartboard and hopes it hits the bull’s eye.</p>
    <p class="normal">There are two terms in mathematical optimization called exploration and exploitation. Exploration ensures the optimization algorithm reaches different regions of the search space, whereas exploitation makes sure we search more in regions that are giving us better results. Random search is purely explorative and is unaware of what is happening as it evaluates different trials.</p>
    <p class="normal">Let’s look at one last technique that tries to balance between exploration and exploitation.</p>
    <h3 id="_idParaDest-235" class="heading-3">Bayesian optimization</h3>
    <p class="normal">Bayesian optimization<a id="_idIndexMarker800"/> has a lot of similarities with random search. Both define their search space as probability distributions, and in both techniques, the user decides how many trials it needs to evaluate, but where they differ is the key advantage of Bayesian optimization. While random search is randomly sampling from the search space, Bayesian optimization is doing it intelligently. Bayesian optimization is aware of its past trials and the objective values that came out of those trials so that it can adapt future trials to exploit the regions where better objective values were seen. At a high level, it does this by building a probability model of the objective function and using it to focus trials on promising areas. The details of the algorithm are worth knowing and we have linked to a couple of resources in <em class="italic">Further reading</em> to help you along the way.</p>
    <p class="normal">Now, let’s use a popular library, <code class="inlineCode">optuna</code>, to implement Bayesian optimization for hyperparameter tuning on the GFM we have been training.</p>
    <p class="normal">The process is quite simple. We need to define a function that takes in a parameter called <code class="inlineCode">trial</code>. Inside the function, we sample the different parameters we want to tune from the <code class="inlineCode">trial</code> object, train the model, evaluate the forecast, and return the metric we want to optimize (the MAE). Let’s quickly do that:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">objective</span>(<span class="hljs-params">trial</span>):
    params = {
        <span class="hljs-comment"># Sample an integer between 10 and 100</span>
        <span class="hljs-string">"num_leaves"</span>: trial.suggest_int(<span class="hljs-string">"num_leaves"</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>),
        <span class="hljs-comment"># Sample a categorical value from the list provided</span>
        <span class="hljs-string">"objective"</span>: trial.suggest_categorical(
            <span class="hljs-string">"objective"</span>, [<span class="hljs-string">"regression"</span>, <span class="hljs-string">"regression_l1"</span>, <span class="hljs-string">"huber"</span>]
        ),
        <span class="hljs-string">"random_state"</span>: [<span class="hljs-number">42</span>],
        <span class="hljs-comment"># Sample from a uniform distribution between 0.3 and 1.0</span>
        <span class="hljs-string">"colsample_bytree"</span>: trial.suggest_uniform(<span class="hljs-string">"colsample_bytree"</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">1.0</span>),
        <span class="hljs-comment"># Sample from a uniform distribution between 0 and 10</span>
        <span class="hljs-string">"lambda_l1"</span>: trial.suggest_uniform(<span class="hljs-string">"lambda_l1"</span>, <span class="hljs-number">0</span>, <span class="hljs-number">10</span>),
        <span class="hljs-comment"># Sample from a uniform distribution between 0 and 10</span>
        <span class="hljs-string">"lambda_l2"</span>: trial.suggest_uniform(<span class="hljs-string">"lambda_l2"</span>, <span class="hljs-number">0</span>, <span class="hljs-number">10</span>),
    }
    _model_config = ModelConfig(
        <span class="hljs-comment"># Use the sampled params to initialize the model</span>
        model=LGBMRegressor(**params, verbose=-<span class="hljs-number">1</span>),
        name=<span class="hljs-string">"Global Meta LightGBM Tuning"</span>,
        <span class="hljs-comment"># LGBM is not sensitive to normalized data</span>
        normalize=<span class="hljs-literal">False</span>,
        <span class="hljs-comment"># LGBM can handle missing values</span>
        fill_missing=<span class="hljs-literal">False</span>,
    )
    y_pred, feat_df = train_model(
        _model_config,
        _feat_config,
        missing_value_config,
        train_features,
        train_target,
        test_features,
        fit_kwargs=<span class="hljs-built_in">dict</span>(categorical_feature=cat_features),
    )
    <span class="hljs-comment"># Return the MAE metric as the value</span>
    <span class="hljs-keyword">return</span> ts_utils.mae(test_target[<span class="hljs-string">"energy_consumption"</span>], y_pred)
</code></pre>
    <p class="normal">Once we <a id="_idIndexMarker801"/>have defined the objective function, we need to initialize a sampler. <code class="inlineCode">optuna</code> has many samplers, such as <code class="inlineCode">GridSampler</code>, <code class="inlineCode">RandomSampler</code>, and <code class="inlineCode">TPESampler</code>. For all standard use cases, <code class="inlineCode">TPESampler</code> is the one to use. <code class="inlineCode">GridSampler</code> does grid search and <code class="inlineCode">RandomSampler</code> does random search. When <a id="_idIndexMarker802"/>defining a <strong class="keyWord">Tree Parzen Estimator</strong> (<strong class="keyWord">TPE</strong>) sampler, there are two parameters that we should pay attention to:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">seed</code>: This sets the seed for the random sampling. This makes the process reproducible.</li>
      <li class="bulletList"><code class="inlineCode">n_startup_trials</code>: This is the number of trials that are purely exploratory. This is done to understand the search space before the exploitation kicks in. The default value is <code class="inlineCode">10</code>. We can reduce or increase this depending on how large our sample space is and how many trials we are planning to do.</li>
    </ul>
    <p class="normal">The rest of the parameters are best left untouched for the most common use cases.</p>
    <p class="normal">Now, we create a study, which is the object that runs the trials and stores all the details about the trials:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Create a study</span>
study = optuna.create_study(direction=<span class="hljs-string">"minimize"</span>, sampler=sampler)
<span class="hljs-comment"># Start the optimization run</span>
study.optimize(objective, n_trials=<span class="hljs-number">27</span>, show_progress_bar=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Here, we define the direction of optimization, and we pass in the sampler we initialized earlier. Once the study is defined, we need to call the <code class="inlineCode">optimize</code> method and pass the objective function we defined, the number of trials we need to run, and some other parameters. A full list of parameters for the <code class="inlineCode">optimize</code> method is available here—<a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize"><span class="url">https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize</span></a>.</p>
    <p class="normal">This runs slightly<a id="_idIndexMarker803"/> longer, maybe because of the additional computation required to generate new trials, but still only takes about 20 minutes for the 27 trials. As expected, this has come up with another combination of hyperparameters for which the objective value is <code class="inlineCode">0.72838</code> (the lowest before now).</p>
    <p class="normal">To fully illustrate the difference between the three, let’s compare how the three techniques spent their computational budget:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_07.png" alt="Figure 10.7 – Distribution of computational effort (grid versus random versus Bayesian optimization) "/></figure>
    <p class="packt_figref">Figure 10.7: Distribution of computational effort (grid versus random versus Bayesian optimization)</p>
    <p class="normal">We can see that the Bayesian optimization has a fat tail on the lower side, indicating that it spent most of its computational budget evaluating and exploiting the optimal regions in the search space.</p>
    <p class="normal">Let’s look at how the different trials with these techniques fared as the optimization procedure progressed.</p>
    <p class="normal">The notebook has a more detailed comparison and commentary on the three techniques.</p>
    <p class="normal">The <a id="_idIndexMarker804"/>bottom line is that if we have unlimited computation, grid search with a well-defined and fine-grained grid is the best option, but if we value the efficiency of our computational effort, we should go for Bayesian optimization.</p>
    <p class="normal">Let’s see how the new parameters worked out for us:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_08.png" alt="Figure 10.8 – Aggregate metrics with the tuned GFM with meta-features "/></figure>
    <p class="packt_figref">Figure 10.8: Aggregate metrics with the tuned GFM with meta-features</p>
    <p class="normal">We have had huge improvements in <code class="inlineCode">MAE</code> and <code class="inlineCode">meanMASE</code>, mostly because we were optimizing for the MAE when hyperparameter tuning. The MAE and MSE have slightly different priorities and we will spend more time on that in <em class="italic">Part 4</em>, <em class="italic">Mechanics of Forecasting</em>. The runtime also increased because the new parameters build more leaves for a tree than before and are more complex than the default parameters.</p>
    <p class="normal">Now, let’s look at another strategy for improving the performance of a GFM.</p>
    <h2 id="_idParaDest-236" class="heading-2">Partitioning</h2>
    <p class="normal">Out <a id="_idIndexMarker805"/>of all the <a id="_idIndexMarker806"/>strategies we have discussed so far, this is the most counter-intuitive, especially if you are coming from a standard machine learning or statistics background. Normally, we would expect the model to do well with more data, but partitioning or splitting the dataset into multiple, almost equal parts has been shown (empirically) to improve the accuracy of the <a id="_idIndexMarker807"/>model. While this has been seen empirically, why this happens is something that is still not quite clear. One explanation is that the GFMs have a slightly simpler job of learning when trained on a subset of similar entities and hence, can learn specific functions to subsets of similar entities. Another explanation for the phenomenon has been put forward by Montero-Manso and Hyndman (Reference <em class="italic">1</em>). They put forward that partitioning the data is another form of increasing the complexity because instead of having <em class="italic">log</em>(|<em class="italic">J</em>|) as the complexity term, we have </p>
    <p class="center"><img src="../Images/B22389_10_023.png" alt=""/></p>
    <p class="normal">where <em class="italic">P</em> is the number of partitions. With this rationale, the LFMs are special cases where <em class="italic">P</em> is equal to the number of time series in the dataset.</p>
    <p class="normal">There are many ways we can partition the data, each with varying degrees of complexity.</p>
    <h3 id="_idParaDest-237" class="heading-3">Random partition</h3>
    <p class="normal">The <a id="_idIndexMarker808"/>simplest<a id="_idIndexMarker809"/> method is to randomly split the dataset into <em class="italic">P</em>-equal partitions and train separate models for each partition. This method faithfully follows the explanation that Montero-Manso and Hyndman provide because we are splitting the dataset randomly, with no concern for the similarity of the different households. Let’s see how we can do that:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Define a function which splits a list into n partitions</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">partition</span> (list_in, n):
    random.shuffle(list_in)
    <span class="hljs-keyword">return</span> [list_in[i::n] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]
<span class="hljs-comment"># split the unique LCLids into partitions</span>
partitions = partition(train_df.LCLid.cat.categories.tolist(), <span class="hljs-number">3</span>)
</code></pre>
    <p class="normal">Then, we just loop over these partitions and train separate models for each partition. The exact code can be found in the notebook. Let’s see how well the random partition does:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_09.png" alt="Figure 10.9 – Aggregate metrics with the tuned GFM with meta-features and random partitioning "/></figure>
    <p class="packt_figref">Figure 10.9: Aggregate metrics with the tuned GFM with meta-features and random partitioning</p>
    <p class="normal">We can <a id="_idIndexMarker810"/>see a decrease in <code class="inlineCode">MAE</code> and <code class="inlineCode">meanMASE</code> even with a random <a id="_idIndexMarker811"/>partition. There is even a decrease in runtime because the individual models are working on less data and hence, train faster.</p>
    <p class="normal">Now, let’s see another way of partitioning, keeping the similarity of different time series in mind.</p>
    <h3 id="_idParaDest-238" class="heading-3">Judgmental partitioning</h3>
    <p class="normal">Judgmental <a id="_idIndexMarker812"/>partitioning is <a id="_idIndexMarker813"/>when we use some attribute of the time series to split the dataset, and this is called judgmental because, usually, this depends on the judgment of the person who is working on the model. There are many ways of doing this. We can use some meta-feature, or we can use some characteristics of the time series (such as volume, variability, intermittency, or a combination of them) to partition the dataset.</p>
    <p class="normal">Let’s use a meta-feature called <code class="inlineCode">Acorn_grouped</code> to partition the dataset. Again, we will just loop over the unique values in <code class="inlineCode">Acorn_grouped</code> and train a model for each value. We will also not use <code class="inlineCode">Acorn_grouped</code> as a feature. The exact code is in the notebook. Let’s see how well this partitioning does:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_10.png" alt="Figure 10.10 – Aggregate metrics with the tuned GFM with meta-features and Acorn_grouped partitioning "/></figure>
    <p class="packt_figref">Figure 10.10: Aggregate metrics with the tuned GFM with meta-features and Acorn_grouped partitioning</p>
    <p class="normal">This <a id="_idIndexMarker814"/>does even better than random partitioning. We <a id="_idIndexMarker815"/>can assume each of the partitions (<code class="inlineCode">Affluent</code>, <code class="inlineCode">Comfortable</code>, and <code class="inlineCode">Adversity</code>) has some kind of similarity, which makes the learning easier, and hence, we get better accuracy.</p>
    <p class="normal">Now, let’s look at another way to partition the dataset, again, using similarity.</p>
    <h3 id="_idParaDest-239" class="heading-3">Algorithmic partitioning</h3>
    <p class="normal">In<a id="_idIndexMarker816"/> judgmental partitioning, we pick some meta-features <a id="_idIndexMarker817"/>or time series characteristics for partitioning the dataset. We pick a handful of dimensions to partition the dataset because we are doing it in our minds and our mental faculties cannot handle more than two or three dimensions well, but we can see this partitioning as an unsupervised clustering approach and this approach is called algorithmic partitioning.</p>
    <p class="normal">There are two ways we can cluster time series:</p>
    <ul>
      <li class="bulletList">Extracting features for each time series and using those features to form clusters</li>
      <li class="bulletList">Using time series clustering techniques <a id="_idIndexMarker818"/>using the <strong class="keyWord">Dynamic Time Warping</strong> (<strong class="keyWord">DTW</strong>) distance</li>
    </ul>
    <p class="normal"><code class="inlineCode">tslearn</code> is an open source Python library that has implemented a few time series clustering approaches based on the distances between time series. There is a link in <em class="italic">Further reading</em> for more information on the library and how it can be used for time series clustering.</p>
    <p class="normal">In our <a id="_idIndexMarker819"/>example, we<a id="_idIndexMarker820"/> are going to use the first method, where we derive a few time series characteristics and use them for clustering. There are many features from statistical and temporal literature, such as autocorrelation, mean, variance, entropy, and peak-to-peak distance, that we can extract from the time series. </p>
    <p class="normal">We can use another open source Python library called<a id="_idIndexMarker821"/> the <strong class="keyWord">Time Series Feature Extraction Library</strong> (<code class="inlineCode">tsfel</code>) to make the process easier.</p>
    <p class="normal">The library has many classes of features—statistical, temporal, and spectral domains—that we can choose from, and the rest is handled by the library. Let’s see how we can generate these features and create a dataframe to perform clustering:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tsfel
cfg = tsfel.get_features_by_domain(<span class="hljs-string">"statistical"</span>)
cfg = {**cfg, **tsfel.get_features_by_domain(<span class="hljs-string">"temporal"</span>)}
uniq_ids = train_df.LCLid.cat.categories
stat_df = []
<span class="hljs-keyword">for</span> id_ <span class="hljs-keyword">in</span> tqdm(uniq_ids, desc=<span class="hljs-string">"Calculating features for all households"</span>):
    ts = train_df.loc[train_df.LCLid==id_, <span class="hljs-string">"energy_consumption"</span>]
    res = tsfel.time_series_features_extractor(cfg, ts, verbose=<span class="hljs-literal">False</span>)
    res[<span class="hljs-string">'LCLid'</span>] = id_
    stat_df.append(res)
stat_df = pd.concat(stat_df).set_index(<span class="hljs-string">"LCLid"</span>)
</code></pre>
    <p class="normal">The dataframe looks something like this:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_11.png" alt="Figure 10.11 – Features extracted from different time series "/></figure>
    <p class="packt_figref">Figure 10.11: Features extracted from different time series</p>
    <p class="normal">Now that <a id="_idIndexMarker822"/>we have the dataframe with each row representing a time series with different features, we can ideally apply any clustering <a id="_idIndexMarker823"/>method, such as k-means, k-medoids, or HDBSCAN, and find clusters. However, in high dimensions, a lot of the distance metrics (including Euclidean) do not work as well as they are supposed to. There is a seminal paper on the topic by Charu C. Agarwal et al. from 2001 that explores the topic. When we increase the dimensionality of the space, our common sense (which conceptualizes three dimensions) does not work as well and, as a consequence, common distance metrics such as Euclidean distance do not work very well with high dimensions. We have linked to a blog summarizing the paper (in <em class="italic">Further reading</em>) and the paper itself (Reference <em class="italic">5</em>), which make the concept clearer. So, a common way of handling high-dimensional clustering is by performing dimensionality reduction first and then using normal clustering.</p>
    <p class="normal"><strong class="keyWord">Principal Component Analysis</strong> (<strong class="keyWord">PCA</strong>) was the go-to tool in the field, but since PCA only captures and details linear relationships while reducing the dimensions, nowadays, another class of techniques is starting to become more popular—manifold learning.</p>
    <p class="normal"><strong class="keyWord">t-distributed Stochastic Neighbor Embeddings</strong> (<strong class="keyWord">t-SNE</strong>) is a popular technique from this category, which is really popular for high-dimensional visualization. It is a really clever technique where we project the points from a high-dimensional space to a lower dimension, keeping the distribution of distance in the original space as close as possible to the one in lower dimensions. There is a lot to learn here that is beyond the scope of this book. There are links in the <em class="italic">Further reading</em> section that can help you get started.</p>
    <p class="normal">To cut a long story short, we will be using t-SNE to reduce the dimensions of the dataset we have and then cluster the dataset with the reduced dimensions. If you really want to cluster time <a id="_idIndexMarker824"/>series and use those clusters in some other way, I would not suggest using t-SNE because it doesn’t preserve the distance between points and the density of points. The distil.pub article in <em class="italic">Further reading</em> throws more light on the issue. But in our case, we are using the clusters just as a grouping for training another model, so this approximation can do well. Let’s see how we do that:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans
<span class="hljs-keyword">from</span> src.utils.data_utils <span class="hljs-keyword">import</span> replace_array_in_dataframe
<span class="hljs-keyword">from</span> sklearn.manifold <span class="hljs-keyword">import</span> TSNE <span class="hljs-comment">#T-Distributed Stochastic Neighbor Embedding</span>
<span class="hljs-comment"># Standardizing to make distance calculation fair</span>
X_std = replace_array_in_dataframe(stat_df, StandardScaler().fit_transform(stat_df))
<span class="hljs-comment">#Non-Linear Dimensionality Reduction</span>
tsne = TSNE(n_components=<span class="hljs-number">2</span>, perplexity=<span class="hljs-number">50</span>, learning_rate=<span class="hljs-string">"auto"</span>, init=<span class="hljs-string">"pca"</span>, random_state=<span class="hljs-number">42</span>, metric=<span class="hljs-string">"cosine"</span>, square_distances=<span class="hljs-literal">True</span>)
X_tsne = tsne.fit_transform(X_std.values)
<span class="hljs-comment"># Clustering reduced dimensions into 3 clusters</span>
kmeans = KMeans(n_clusters=<span class="hljs-number">3</span>, random_state=<span class="hljs-number">42</span>).fit(X_tsne)
cluster_df = pd.Series(kmeans.labels_, index=X_std.index)
</code></pre>
    <p class="normal">Since we <a id="_idIndexMarker825"/>reduced the dimensions to two, we can also visualize the clusters formed:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_12.png" alt="Figure 10.12 – Clustered time series after t-SNE dimensionality reduction "/></figure>
    <p class="packt_figref">Figure 10.12: Clustered time series after t-SNE dimensionality reduction</p>
    <p class="normal">We have <a id="_idIndexMarker826"/>three well-defined clusters formed and now we are just going to use these clusters to train a model for each cluster. As usual, we loop over the three clusters and train the models. Let’s see how we did so:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_13.png" alt="Figure 10.13 – Aggregate metrics with the tuned GFM with meta-features and clustered partitioning "/></figure>
    <p class="packt_figref">Figure 10.13: Aggregate metrics with the tuned GFM with meta-features and clustered partitioning</p>
    <p class="normal">It looks as<a id="_idIndexMarker827"/> though this is the best MAE we have seen in<a id="_idIndexMarker828"/> all our experiments, but the three partition techniques have very similar MAEs. We can’t see whether any one is better than the other just by looking at a single hold-out set. For good measure, we can run these forecasts with a test dataset using the <code class="inlineCode">01a-Global_Forecasting_Models-ML-test.ipynb</code> notebook in the <code class="inlineCode">Chapter08</code> folder. Let’s see how the aggregate metrics are on the test dataset:</p>
    <figure class="mediaobject"><img src="../Images/B22389_10_14.png" alt="Figure 10.14 – Aggregate metrics on test data "/></figure>
    <p class="packt_figref">Figure 10.14: Aggregate metrics on test data</p>
    <p class="normal">As expected, the <a id="_idIndexMarker829"/>clustered partition is still the methodology that performs the best in this case.</p>
    <p class="normal">In <em class="chapterRef">Chapter 8</em>, <em class="italic">Forecasting Time Series with Machine Learning Models</em>, it took us 8 minutes and 20 seconds to train an LFM for all the households in our dataset. Now, with the GFM paradigm, we finished training a model in 57 seconds (in the worst-case scenario). That’s 777% less training time and this comes with an 8.78% decrease in the MAE.</p>
    <p class="normal">We chose to do these experiments with LightGBM. This does not mean that LightGBM or any other <a id="_idIndexMarker830"/>gradient-boosting model is the only choice for GFMs, but they are a pretty good default. A well-tuned gradient-boosted trees model is a very difficult baseline to beat, but as <a id="_idIndexMarker831"/>always in machine learning, we should check what works best using well-defined experiments.</p>
    <p class="normal">Although there are no hard and fast rules or cutoffs for when a GFM makes more sense than an LFM, as the number of time series in a dataset increases, the GFM becomes more favorable, both from the perspective of accuracy and computation.</p>
    <p class="normal">Although we have achieved good results using GFMs, typically the complex models that do well in this paradigm are black boxes. Let’s look at some ways to open the black box and understand and explain the model better.</p>
    <h1 id="_idParaDest-240" class="heading-1">Interpretability</h1>
    <p class="normal">Interpretability can<a id="_idIndexMarker832"/> be defined as the degree to which a human can understand the cause of a decision. In machine learning and artificial intelligence, that translates to the degree to which someone can understand the how and why of an algorithm and its predictions. There are two ways to look at interpretability—transparency and post hoc interpretation.</p>
    <p class="normal"><em class="italic">Transparency</em> is <a id="_idIndexMarker833"/>when the model is inherently simple and can be simulated or thought about using human cognition. A human should be able to fully understand the inputs and the process a model takes to convert these inputs to outputs. This is a very stringent condition that almost none of the model machine learning or deep learning models satisfy.</p>
    <p class="normal">This is <a id="_idIndexMarker834"/>where <em class="italic">post hoc interpretation</em> techniques shine. There is a wide variety of techniques that use the inputs and outputs of a model to understand why a model has made the predictions it has.</p>
    <p class="normal">There are many popular <a id="_idIndexMarker835"/>techniques such as <em class="italic">permutation feature importance</em>, <em class="italic">Shapley values</em>, and <em class="italic">LIME</em>. All of these <a id="_idIndexMarker836"/>are general-purpose<a id="_idIndexMarker837"/> interpretation techniques that can be used on any machine learning model and that includes the GFMs we were discussing. Let’s talk about a few of them at a high level.</p>
    <p class="normal"><strong class="keyWord">Mean decrease in impurity:</strong></p>
    <p class="normal">This is the<a id="_idIndexMarker838"/> regular “feature importance” that <a id="_idIndexMarker839"/>we get out of the box from tree-based models. This technique measures how much a feature reduces impurity (such as Gini impurity in classification or variance in regression) when used to split nodes in a decision tree. The higher the reduction in impurity, the more important the feature is considered. However, it’s biased towards continuous features or those with high cardinality. It is fast and readily available in libraries like scikit-learn but may give misleading results if features have varying scales or many categories.</p>
    <p class="normal"><strong class="keyWord">Drop column importance (Leave One Covariate Out </strong>(<strong class="keyWord">LOCO</strong>)<strong class="keyWord">):</strong></p>
    <p class="normal">This <a id="_idIndexMarker840"/>method assesses feature importance by iteratively removing one feature at a time and retraining the model. The drop in <a id="_idIndexMarker841"/>performance from the baseline model indicates the importance of that feature. It is model-agnostic and captures interactions between features, but is computationally expensive since it requires retraining the model for each feature removed. It can also give misleading results if collinear features exist, as the model may compensate for the removed feature.</p>
    <p class="normal"><strong class="keyWord">Permutation importance:</strong></p>
    <p class="normal">Permutation importance<a id="_idIndexMarker842"/> measures <a id="_idIndexMarker843"/>the drop in model performance when the values of a single feature are randomly shuffled, disrupting its relationship with the target. This technique is intuitive and model-agnostic, and it doesn’t require retraining the model, making it computationally efficient. However, it can inflate the importance of correlated features, as models can rely on related features to compensate for the permuted one.</p>
    <p class="normal"><strong class="keyWord">Partial Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots:</strong></p>
    <p class="normal">PDPs<a id="_idIndexMarker844"/> visualize the average effect of a feature on the model’s predictions, showing how the target variable<a id="_idIndexMarker845"/> changes <a id="_idIndexMarker846"/>as the feature’s values change, while ICE plots show<a id="_idIndexMarker847"/> the effect of a feature for individual instances. These plots help understand the feature-target relationship but assume independence between features, which can lead to misleading interpretations in the presence of correlated variables.</p>
    <p class="normal"><strong class="keyWord">Local Interpretable Model-agnostic Explanations (LIME):</strong></p>
    <p class="normal">LIME is <a id="_idIndexMarker848"/>a model-agnostic technique that explains individual predictions by approximating a<a id="_idIndexMarker849"/> complex model locally using simpler, interpretable models, like linear regression. It works by generating perturbations of the data point in question and fitting a local model to these samples. This method is intuitive and widely applicable to both structured and unstructured data (text and images), but defining the right locality for perturbations can be challenging, especially for tabular data.</p>
    <p class="normal"><strong class="keyWord">SHapley Additive exPlanations (SHAP):</strong></p>
    <p class="normal">SHAP unifies <a id="_idIndexMarker850"/>several interpretation methods, including <a id="_idIndexMarker851"/>Shapley values and LIME, into a single framework that attributes feature importance in a model-agnostic way. SHAP provides both local and global interpretations and benefits from fast implementations for tree-based models (TreeSHAP). It combines the theoretical strength of Shapley values with practical efficiency, although it can still be computationally intensive for large datasets.</p>
    <p class="normal">Each technique has its strengths and trade-offs, but SHAP stands out due to its strong theoretical foundation and ability to connect local and global interpretations effectively. For more extensive coverage of such techniques, I have included a few links in <em class="italic">Further reading</em>. The blog series by yours truly and the free book by Christoper Molnar are excellent resources for you to get up to speed (more on interpretability in <em class="chapterRef">Chapter 17</em>).</p>
    <p class="normal">Congratulations on finishing the second part of the book! It has been quite an intensive part where we went over quite a bit of theory and practical lessons, and we hope you are now comfortable with using machine learning for time series forecasting.</p>
    <h1 id="_idParaDest-241" class="heading-1">Summary</h1>
    <p class="normal">To round up the second part of the book nicely, we explored GFMs in detail and saw why they are important and why they are an exciting new direction in time series forecasting. We saw how we can use a GFM using machine learning models and also reviewed many techniques to make GFMs perform better, most of which are quite frequently used in competitions and industry use cases alike. We also took a high-level look at the interpretability techniques. Now that we have wrapped up the machine learning section of the book, we will move on to a specific type of machine learning that has become well-known over the past few years—<strong class="keyWord">deep learning</strong>—in the next chapter.</p>
    <h1 id="_idParaDest-242" class="heading-1">References</h1>
    <p class="normal">The following are sources that we have referenced throughout the chapter:</p>
    <ol>
      <li class="numberedList" value="1">Montero-Manso, P., Hyndman, R.J. (2020), <em class="italic">Principles and algorithms for forecasting groups of time series: Locality and globality</em>. arXiv:2008.00444[cs.LG]: <a href="https://arxiv.org/abs/2008.00444"><span class="url">https://arxiv.org/abs/2008.00444</span></a>.</li>
      <li class="numberedList">Micci-Barreca, D. (2001), <em class="italic">A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems</em>. <em class="italic">SIGKDD Explor. Newsl</em>. 3, 1 (July 2001), 27–32: <a href="https://doi.org/10.1145/507533.507538"><span class="url">https://doi.org/10.1145/507533.507538</span></a>.</li>
      <li class="numberedList">Fisher, W. D. (1958). <em class="italic">On Grouping for Maximum Homogeneity</em>. <em class="italic">Journal of the American Statistical Association</em>, 53(284), 789–798: <a href="https://doi.org/10.2307/2281952"><span class="url">https://doi.org/10.2307/2281952</span></a>.</li>
      <li class="numberedList">Fisher, W.D. (1958), <em class="italic">A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems</em>. <em class="italic">SIGKDD Explor. Newsl.</em> 3, 1 (July 2001), 27–32.</li>
      <li class="numberedList">Aggarwal, C. C., Hinneburg, A., and Keim, D. A. (2001). <em class="italic">On the Surprising Behavior of Distance Metrics in High Dimensional Spaces.</em> In <em class="italic">Proceedings of the 8th International Conference on Database Theory </em>(ICDT ‘01). Springer-Verlag, Berlin, Heidelberg, 420–434: <a href="https://dl.acm.org/doi/10.5555/645504.656414"><span class="url">https://dl.acm.org/doi/10.5555/645504.656414</span></a>.</li>
      <li class="numberedList">Oreshkin, B. N., Carpov D., Chapados N., and Bengio Y. (2020). <em class="italic">N-BEATS: Neural basis expansion analysis for interpretable time series forecasting</em>. <em class="italic">8th International Conference on Learning Representations, ICLR 2020</em>: <a href="https://openreview.net/forum?id=r1ecqn4YwB"><span class="url">https://openreview.net/forum?id=r1ecqn4YwB</span></a>.</li>
    </ol>
    <h1 id="_idParaDest-243" class="heading-1">Further reading</h1>
    <p class="normal">The following are a few resources that you can explore for a detailed study:</p>
    <ul>
      <li class="bulletList"><em class="italic">Learning From Data</em> by Yaser Abu-Mostafa: <a href="https://work.caltech.edu/lectures.html"><span class="url">https://work.caltech.edu/lectures.html</span></a></li>
      <li class="bulletList"><em class="italic">Curse of Dimensionality</em>—Georgia Tech: <a href="https://www.youtube.com/watch?v=OyPcbeiwps8"><span class="url">https://www.youtube.com/watch?v=OyPcbeiwps8</span></a></li>
      <li class="bulletList"><em class="italic">Dummy Variable Trap</em>: <a href="https://www.learndatasci.com/glossary/dummy-variable-trap/"><span class="url">https://www.learndatasci.com/glossary/dummy-variable-trap/</span></a></li>
      <li class="bulletList">Using deep learning to learn categorical embeddings: <a href="https://pytorch-tabular.readthedocs.io/en/latest/tutorials/03-Neural%20Embedding%20in%20Scikit-Learn%20Workflows/"><span class="url">https://pytorch-tabular.readthedocs.io/en/latest/tutorials/03-Neural%20Embedding%20in%20Scikit-Learn%20Workflows/</span></a></li>
      <li class="bulletList">Handling categorical features—CatBoost: <a href="https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic"><span class="url">https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic</span></a></li>
      <li class="bulletList"><em class="italic">Exploring Bayesian Optimization</em>—from Distil.pub: <a href="https://distill.pub/2020/bayesian-optimization/"><span class="url">https://distill.pub/2020/bayesian-optimization/</span></a></li>
      <li class="bulletList">Frazier, P.I. (2018). <em class="italic">A Tutorial on Bayesian Optimization</em>. arXiv:1807.02811 [stat.ML]: <a href="https://arxiv.org/abs/1807.02811"><span class="url">https://arxiv.org/abs/1807.02811</span></a></li>
      <li class="bulletList">Time series clustering using <code class="inlineCode">tslearn</code>: <a href="https://tslearn.readthedocs.io/en/stable/user_guide/clustering.html"><span class="url">https://tslearn.readthedocs.io/en/stable/user_guide/clustering.html</span></a></li>
      <li class="bulletList"><em class="italic">The Surprising Behaviour of Distance Metrics in High Dimensions</em>: <a href="https://towardsdatascience.com/the-surprising-behaviour-of-distance-metrics-in-high-dimensions-c2cb72779ea6"><span class="url">https://towardsdatascience.com/the-surprising-behaviour-of-distance-metrics-in-high-dimensions-c2cb72779ea6</span></a></li>
      <li class="bulletList"><em class="italic">An illustrated introduction to the t-SNE algorithm</em>: <a href="https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/"><span class="url">https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/</span></a></li>
      <li class="bulletList"><em class="italic">How to Use t-SNE Effectively</em>—from Distil.pub: <a href="https://distill.pub/2016/misread-tsne/"><span class="url">https://distill.pub/2016/misread-tsne/</span></a></li>
      <li class="bulletList">The NFLT: <a href="https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization"><span class="url">https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization</span></a></li>
      <li class="bulletList"><em class="italic">Interpretability: Cracking open the black box</em> – parts I, II, and III by Manu Joseph: <a href="https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-i/"><span class="url">https://deep-and-shallow.com/2019/11/13/interpretability-cracking-open-the-black-box-part-i/</span></a></li>
      <li class="bulletList"><em class="italic">Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em> by Christoph Molnar: <a href="https://christophm.github.io/interpretable-ml-book/"><span class="url">https://christophm.github.io/interpretable-ml-book/</span></a></li>
      <li class="bulletList"><em class="italic">Global models for time series forecasting</em>: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321006178"><span class="url">https://www.sciencedirect.com/science/article/abs/pii/S0031320321006178</span></a></li>
    </ul>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
    <h1 class="heading-1">Leave a Review!</h1>
    <p class="normal">Thank you for purchasing this book from Packt Publishing—we hope you enjoy it! Your feedback is invaluable and helps us improve and grow. Once you’ve completed reading it, please take a moment to leave an Amazon review; it will only take a minute, but it makes a big difference for readers like you.</p>
    <p class="normal">Scan the QR or visit the link to receive a free ebook of your choice.</p>
    <p class="normal"><a href="Chapter_10.xhtml"><span class="url">https://packt.link/NzOWQ</span></a></p>
    <p class="normal"><img src="../Images/review1.jpg" alt="A qr code with black squares  Description automatically generated"/></p>
  </div>
</body></html>