<html><head></head><body>
		<div id="_idContainer089">
			<h1 id="_idParaDest-36"><em class="italic"><a id="_idTextAnchor035"/>Chapter 2</em>: Working with Pandas DataFrames</h1>
			<p>The time has come for us to begin our journey into the <strong class="source-inline">pandas</strong> universe. This chapter will get us comfortable working with some of the basic, yet powerful, operations we will be performing when conducting our data analyses with <strong class="source-inline">pandas</strong>.</p>
			<p>We will begin with an introduction to the main <strong class="bold">data structures</strong> we will encounter when working with <strong class="source-inline">pandas</strong>. Data structures provide us with a format for organizing, managing, and storing data. Knowledge of <strong class="source-inline">pandas</strong> data structures will prove infinitely helpful when it comes to troubleshooting or looking up how to perform an operation on the data. Keep in mind that these data structures are different from the standard Python data structures for a reason: they were created for specific analysis tasks. We must remember that a given method may only work on a certain data structure, so we need to be able to identify the best structure for the problem we are looking to solve.</p>
			<p>Next, we will bring our first dataset into Python. We will learn how to collect data from an API, create <strong class="source-inline">DataFrame</strong> objects from other data structures in Python, read in files, and interact with databases. Initially, you may wonder why we would ever need to create a <strong class="source-inline">DataFrame</strong> object from other Python data structures; however, if we ever want to test something quickly, create our own data, pull data from an API, or repurpose Python code from another project, then we will find this knowledge indispensable. Finally, we will master ways to inspect, describe, filter, and summarize our data.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Pandas data structures</li>
				<li>Creating DataFrame objects from files, API requests, SQL queries, and other Python objects</li>
				<li>Inspecting DataFrame objects and calculating summary statistics</li>
				<li>Grabbing subsets of the data via selection, slicing, indexing, and filtering</li>
				<li>Adding and removing data</li>
			</ul>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>Chapter materials</h1>
			<p>The files we will be working with in this chapter can be found in the GitHub repository at <a href="https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_02">https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_02</a>. We will be working with earthquake data from the <strong class="bold">US Geological Survey</strong> (<strong class="bold">USGS</strong>) by using the USGS API and CSV files, which can be found in the <strong class="source-inline">data/</strong> directory.</p>
			<p>There are four CSV files and a SQLite database file, all of which will be used at different points throughout this chapter. The <strong class="source-inline">earthquakes.csv</strong> file contains data that's been pulled from the USGS API for September 18, 2018 through October 13, 2018. For our discussion of data structures, we will work with the <strong class="source-inline">example_data.csv</strong> file, which contains five rows and a subset of the columns from the <strong class="source-inline">earthquakes.csv</strong> file. The <strong class="source-inline">tsunamis.csv</strong> file is a subset of the data in the <strong class="source-inline">earthquakes.csv</strong> file for all earthquakes that were accompanied by tsunamis during the aforementioned date range. The <strong class="source-inline">quakes.db</strong> file contains a SQLite database with a single table for the tsunamis data. We will use this to learn how to read from and write to a database with <strong class="source-inline">pandas</strong>. Lastly, the <strong class="source-inline">parsed.csv</strong> file will be used for the end-of-chapter exercises, and we will also walk through the creation of it during this chapter.</p>
			<p>The accompanying code for this chapter has been divided into six Jupyter Notebooks, which are numbered in the order they are to be used. They contain the code snippets we will be running throughout this chapter, along with the full output of any command that has to be trimmed for this text. Each time we are to switch notebooks, the text will indicate to do so.</p>
			<p>In the <strong class="source-inline">1-pandas_data_structures.ipynb</strong> notebook, we will start learning about the main <strong class="source-inline">pandas</strong> data structures. Afterward, we will discuss the various ways to create <strong class="source-inline">DataFrame</strong> objects in the <strong class="source-inline">2-creating_dataframes.ipynb</strong> notebook. Our discussion on this topic will continue in the <strong class="source-inline">3-making_dataframes_from_api_requests.ipynb</strong> notebook, where we will explore the USGS API to gather data for use with <strong class="source-inline">pandas</strong>. After learning <a id="_idIndexMarker170"/>about how we can collect our data, we will begin to learn how to conduct <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) in the <strong class="source-inline">4-inspecting_dataframes.ipynb</strong> notebook. Then, in the <strong class="source-inline">5-subsetting_data.ipynb </strong>notebook, we will discuss various ways to select and filter data. Finally, we will learn how to add and remove data in the <strong class="source-inline">6-adding_and_removing_data.ipynb</strong> notebook. Let's get started.</p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Pandas data structures</h1>
			<p>Python has several data structures already, such as tuples, lists, and dictionaries. Pandas provides two <a id="_idIndexMarker171"/>main structures to facilitate working with data: <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong>. The <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> data structures each contain another <strong class="source-inline">pandas</strong> data structure, <strong class="source-inline">Index</strong>, that we must also be aware of. However, in order to understand these data structures, we need to first take a look at NumPy (<a href="https://numpy.org/doc/stable/">https://numpy.org/doc/stable/</a>), which provides the n-dimensional arrays that <strong class="source-inline">pandas</strong> builds upon.</p>
			<p>The aforementioned data structures are implemented as Python <strong class="bold">classes</strong>; when we actually create one, they are referred to as <strong class="bold">objects</strong> or <strong class="bold">instances</strong>. This is an important distinction, since, as we will see, some actions can be performed using the object itself (a <strong class="bold">method</strong>), whereas others will require that we pass our object in as an argument to some <strong class="bold">function</strong>. Note that, in Python, class names are traditionally written in <strong class="source-inline">CapWords</strong>, while objects are written in <strong class="source-inline">snake_case</strong>. (More Python style guidelines can be found at <a href="https://www.python.org/dev/peps/pep-0008/">https://www.python.org/dev/peps/pep-0008/</a>.)</p>
			<p>We use a <strong class="source-inline">pandas</strong> function to read a CSV file into an object of the <strong class="source-inline">DataFrame</strong> class, but we use methods on our <strong class="source-inline">DataFrame</strong> objects to perform actions on them, such as dropping columns or calculating summary statistics. With <strong class="source-inline">pandas</strong>, we will often want to access the <strong class="bold">attributes</strong> of the object we are working with. This won't generate an action as a method or function would; rather, we will be given information about our <strong class="source-inline">pandas</strong> object, such as dimensions, column names, data types, and whether it is empty.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For the remainder of this book, we will refer to <strong class="source-inline">DataFrame</strong> objects as dataframes, <strong class="source-inline">Series</strong> objects as series, and <strong class="source-inline">Index</strong> objects as index/indices, unless we are referring to the class itself.</p>
			<p>For this section, we will work in the <strong class="source-inline">1-pandas_data_structures.ipynb</strong> notebook. To begin, we will import <strong class="source-inline">numpy</strong> and use it to read the contents of the <strong class="source-inline">example_data.csv</strong> file into a <strong class="source-inline">numpy.array</strong> object. The data comes from the USGS API for earthquakes (source: <a href="https://earthquake.usgs.gov/fdsnws/event/1/">https://earthquake.usgs.gov/fdsnws/event/1/</a>). Note that this is the only time we will use NumPy <a id="_idIndexMarker172"/>to read in a file and that this is being done for illustrative purposes only; the important part is to look at the way the data is represented with NumPy:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; data = np.genfromtxt(</p>
			<p class="source-code">...     'data/example_data.csv', delimiter=';', </p>
			<p class="source-code">...     names=True, dtype=None, encoding='UTF'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; data</p>
			<p class="source-code"><strong class="bold">array([('2018-10-13 11:10:23.560',</strong></p>
			<p class="source-code"><strong class="bold">        '262km NW of Ozernovskiy, Russia', </strong></p>
			<p class="source-code"><strong class="bold">        'mww', 6.7, 'green', 1),</strong></p>
			<p class="source-code"><strong class="bold">       ('2018-10-13 04:34:15.580', </strong></p>
			<p class="source-code"><strong class="bold">        '25km E of Bitung, Indonesia', 'mww', 5.2, 'green', 0),</strong></p>
			<p class="source-code"><strong class="bold">       ('2018-10-13 00:13:46.220', '42km WNW of Sola, Vanuatu', </strong></p>
			<p class="source-code"><strong class="bold">        'mww', 5.7, 'green', 0),</strong></p>
			<p class="source-code"><strong class="bold">       ('2018-10-12 21:09:49.240', </strong></p>
			<p class="source-code"><strong class="bold">        '13km E of Nueva Concepcion, Guatemala',</strong></p>
			<p class="source-code"><strong class="bold">        'mww', 5.7, 'green', 0),</strong></p>
			<p class="source-code"><strong class="bold">       ('2018-10-12 02:52:03.620', </strong></p>
			<p class="source-code"><strong class="bold">        '128km SE of Kimbe, Papua New Guinea',</strong></p>
			<p class="source-code"><strong class="bold">        'mww', 5.6, 'green', 1)],</strong></p>
			<p class="source-code"><strong class="bold">      dtype=[('time', '&lt;U23'), ('place', '&lt;U37'),</strong></p>
			<p class="source-code"><strong class="bold">             ('magType', '&lt;U3'), ('mag', '&lt;f8'),</strong></p>
			<p class="source-code"><strong class="bold">             ('alert', '&lt;U5'), ('tsunami', '&lt;i8')])</strong></p>
			<p>We now have our data in a NumPy array. Using the <strong class="source-inline">shape</strong> and <strong class="source-inline">dtype</strong> attributes, we can gather information <a id="_idIndexMarker173"/>about the dimensions of the array and the data types it contains, respectively:</p>
			<p class="source-code">&gt;&gt;&gt; data.shape</p>
			<p class="source-code">(5,)</p>
			<p class="source-code">&gt;&gt;&gt; data.dtype</p>
			<p class="source-code">dtype([('time', '&lt;U23'), ('place', '&lt;U37'), ('magType', '&lt;U3'), </p>
			<p class="source-code">       ('mag', '&lt;f8'), ('alert', '&lt;U5'), ('tsunami', '&lt;i8')])</p>
			<p>Each of the entries in the array is a row from the CSV file. NumPy arrays contain a single data type (unlike lists, which allow mixed types); this allows for fast, vectorized operations. When we read in the data, we got an array of <strong class="source-inline">numpy.void</strong> objects, which are used to store flexible types. This is because NumPy had to store several different data types per row: four strings, a float, and an integer. Unfortunately, this means that we can't take advantage of the performance improvements NumPy provides for single data type objects.</p>
			<p>Say we want to find the maximum magnitude—we can use a <strong class="bold">list comprehension</strong> (<a href="https://www.python.org/dev/peps/pep-0202/">https://www.python.org/dev/peps/pep-0202/</a>) to select the third index of each row, which is represented as a <strong class="source-inline">numpy.void</strong> object. This makes a list, meaning that we can take the maximum using the <strong class="source-inline">max()</strong> function. We can use the <strong class="source-inline">%%timeit</strong> <strong class="bold">magic command</strong> from IPython (a special command preceded by <strong class="source-inline">%</strong>) to see how long this implementation takes (times will vary):</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">%%timeit</strong></p>
			<p class="source-code">&gt;&gt;&gt; max([row[3] for row in data])</p>
			<p class="source-code"><strong class="bold">9.74 µs ± 177 ns per loop</strong> </p>
			<p class="source-code">(mean ± std. dev. of 7 runs, 100000 loops each)</p>
			<p>Note that we should use a list comprehension whenever we would write a <strong class="source-inline">for</strong> loop with just a single line under it or want to run an operation against the members of some initial list. This is a rather simple list comprehension, but we can make them more complex with the addition of <strong class="source-inline">if...else</strong> statements. List comprehensions are an extremely powerful tool to have in our arsenal. More information can be found in the Python documentation at https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout"><strong class="bold">IPython</strong> (<a href="https://ipython.readthedocs.io/en/stable/index.html">https://ipython.readthedocs.io/en/stable/index.html</a>) provides an interactive <a id="_idIndexMarker174"/>shell for Python. Jupyter Notebooks are built on top of IPython. While knowledge of IPython is not required for this book, it can be helpful to be familiar with some of its functionality. IPython includes a tutorial in their documentation at <a href="https://ipython.readthedocs.io/en/stable/interactive/">https://ipython.readthedocs.io/en/stable/interactive/</a>.</p>
			<p>If we create <a id="_idIndexMarker175"/>a NumPy array for each column instead, this operation is much easier (and more efficient) to perform. To do so, we will use a <strong class="bold">dictionary comprehension</strong> (https://www.python.org/dev/peps/pep-0274/) to make a dictionary where the keys are the column names and the values are NumPy arrays of the data. Again, the important part here is how the data is now represented using NumPy:</p>
			<p class="source-code">&gt;&gt;&gt; array_dict = {</p>
			<p class="source-code">...     col: np.array([row[i] for row in data])</p>
			<p class="source-code">...     for i, col in enumerate(data.dtype.names)</p>
			<p class="source-code">... }</p>
			<p class="source-code">&gt;&gt;&gt; array_dict</p>
			<p class="source-code"><strong class="bold">{'time': array(['2018-10-13 11:10:23.560',</strong></p>
			<p class="source-code"><strong class="bold">        '2018-10-13 04:34:15.580', '2018-10-13 00:13:46.220',</strong></p>
			<p class="source-code"><strong class="bold">        '2018-10-12 21:09:49.240', '2018-10-12 02:52:03.620'],</strong></p>
			<p class="source-code"><strong class="bold">        dtype='&lt;U23'),</strong></p>
			<p class="source-code"><strong class="bold"> 'place': array(['262km NW of Ozernovskiy, Russia', </strong></p>
			<p class="source-code"><strong class="bold">        '25km E of Bitung, Indonesia',</strong></p>
			<p class="source-code"><strong class="bold">        '42km WNW of Sola, Vanuatu',</strong></p>
			<p class="source-code"><strong class="bold">        '13km E of Nueva Concepcion, Guatemala',</strong></p>
			<p class="source-code"><strong class="bold">        '128km SE of Kimbe, Papua New Guinea'], dtype='&lt;U37'),</strong></p>
			<p class="source-code"><strong class="bold"> 'magType': array(['mww', 'mww', 'mww', 'mww', 'mww'], </strong></p>
			<p class="source-code"><strong class="bold">        dtype='&lt;U3'),</strong></p>
			<p class="source-code"><strong class="bold"> 'mag': array([6.7, 5.2, 5.7, 5.7, 5.6]),</strong></p>
			<p class="source-code"><strong class="bold"> 'alert': array(['green', 'green', 'green', 'green', 'green'], </strong></p>
			<p class="source-code"><strong class="bold">        dtype='&lt;U5'),</strong></p>
			<p class="source-code"><strong class="bold"> 'tsunami': array([1, 0, 0, 0, 1])}</strong></p>
			<p>Grabbing the maximum magnitude is now simply a matter of selecting the <strong class="source-inline">mag</strong> key and calling the <strong class="source-inline">max()</strong> method on the NumPy array. This is nearly twice as fast as the list comprehension implementation, when dealing with just five entries—imagine how much worse <a id="_idIndexMarker176"/>the first attempt will perform on large datasets:</p>
			<p class="source-code">&gt;&gt;&gt; %%timeit</p>
			<p class="source-code">&gt;&gt;&gt; array_dict['mag'].max()</p>
			<p class="source-code"><strong class="bold">5.22 µs ± 100 ns</strong> per loop </p>
			<p class="source-code">(mean ± std. dev. of 7 runs, 100000 loops each)</p>
			<p>However, this representation has other issues. Say we wanted to grab all the information for the earthquake with the maximum magnitude; how would we go about that? We need to find the index of the maximum, and then for each of the keys in the dictionary, grab that index. The result is now a NumPy array of strings (our numeric values were converted), and we are now in the format that we saw earlier:</p>
			<p class="source-code">&gt;&gt;&gt; np.array([</p>
			<p class="source-code">...     value[array_dict['mag'].argmax()]</p>
			<p class="source-code">...     for key, value in array_dict.items()</p>
			<p class="source-code">... ])</p>
			<p class="source-code"><strong class="bold">array(['2018-10-13 11:10:23.560',</strong></p>
			<p class="source-code"><strong class="bold">       '262km NW of Ozernovskiy, Russia',</strong></p>
			<p class="source-code"><strong class="bold">       'mww', '6.7', 'green', '1'], dtype='&lt;U31')</strong></p>
			<p>Consider how we would go about sorting the data by magnitude from smallest to largest. In the first representation, we would have to sort the rows by examining the third index. With the second representation, we would have to determine the order of the indices <a id="_idIndexMarker177"/>from the <strong class="source-inline">mag</strong> column, and then sort all the other arrays with those same indices. Clearly, working with several NumPy arrays containing different data types at once is a bit cumbersome; however, <strong class="source-inline">pandas</strong> builds on top of NumPy arrays to make this easier. Let's start our exploration of <strong class="source-inline">pandas</strong> with an overview of the <strong class="source-inline">Series</strong> data structure.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Series</h2>
			<p>The <strong class="source-inline">Series</strong> class provides <a id="_idIndexMarker178"/>a data structure for arrays of a single type, just like the NumPy array. However, it comes with some additional functionality. This one-dimensional representation can be thought of as a column in a spreadsheet. We have a name for our column, and the data we hold in it is of the same type (since we are measuring the same variable):</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; place = pd.Series(array_dict['place'], name='place')</p>
			<p class="source-code">&gt;&gt;&gt; place</p>
			<p class="source-code"><strong class="bold">0          262km NW of Ozernovskiy, Russia</strong></p>
			<p class="source-code"><strong class="bold">1              25km E of Bitung, Indonesia</strong></p>
			<p class="source-code"><strong class="bold">2                42km WNW of Sola, Vanuatu</strong></p>
			<p class="source-code"><strong class="bold">3    13km E of Nueva Concepcion, Guatemala</strong></p>
			<p class="source-code"><strong class="bold">4      128km SE of Kimbe, Papua New Guinea</strong></p>
			<p class="source-code"><strong class="bold">Name: place, dtype: object</strong></p>
			<p>Note the numbers on the left of the result; these correspond to the row number in the original dataset (offset by 1 since, in Python, we start counting at 0). These row numbers form the index, which we will discuss in the following section. Next to the row numbers, we have the actual value of the row, which, in this example, is a string indicating where the earthquake occurred. Notice that we have <strong class="source-inline">dtype: object</strong> next to the name of the <strong class="source-inline">Series</strong> object; this is telling us that the data type of <strong class="source-inline">place</strong> is <strong class="source-inline">object</strong>. A string will be classified as <strong class="source-inline">object</strong> in <strong class="source-inline">pandas</strong>. </p>
			<p>To access <a id="_idIndexMarker179"/>attributes of the <strong class="source-inline">Series</strong> object, we use attribute notation of the form <strong class="source-inline">&lt;object&gt;.&lt;attribute_name&gt;</strong>. The following are some common attributes we will access. Notice that <strong class="source-inline">dtype</strong> and <strong class="source-inline">shape</strong> are available, just as we saw with the NumPy array:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/Figure_2.1_B16834.jpg" alt="Figure 2.1 – Commonly used series attributes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – Commonly used series attributes</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For the most part, <strong class="source-inline">pandas</strong> objects use NumPy arrays for their internal data representations. However, for some data types, <strong class="source-inline">pandas</strong> builds upon NumPy to create its own arrays (https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html). For this reason, depending on the data type, <strong class="source-inline">values</strong> can return either a <strong class="source-inline">pandas.array</strong> or a <strong class="source-inline">numpy.array</strong> object. Therefore, if we need to ensure we get a specific type back, it is recommended to use the <strong class="source-inline">array</strong> attribute or <strong class="source-inline">to_numpy()</strong> method, respectively, instead of <strong class="source-inline">values</strong>.</p>
			<p>Be sure to bookmark the <strong class="source-inline">pandas.Series</strong> documentation (<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html</a>) for reference later. It contains more information on how to create a <strong class="source-inline">Series</strong> object, the full list of attributes and methods that are available, as well as a link to the source code. With this high-level introduction to the <strong class="source-inline">Series</strong> class, we are ready to move on to the <strong class="source-inline">Index</strong> class.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Index</h2>
			<p>The addition of the <strong class="source-inline">Index</strong> class makes the <strong class="source-inline">Series</strong> class significantly more powerful than a NumPy array. The <strong class="source-inline">Index</strong> class gives us row labels, which enable selection by row. Depending <a id="_idIndexMarker180"/>on the type, we can provide a row number, a date, or even a string to select our row. It plays a key role in identifying entries in the data and is used for a multitude of operations in <strong class="source-inline">pandas</strong>, as we will see throughout this book. We can access the index through the <strong class="source-inline">index</strong> attribute:</p>
			<p class="source-code">&gt;&gt;&gt; place_index = place.index</p>
			<p class="source-code">&gt;&gt;&gt; place_index</p>
			<p class="source-code"><strong class="bold">RangeIndex(start=0, stop=5, step=1)</strong></p>
			<p>Note that this is a <strong class="source-inline">RangeIndex</strong> object. Its values start at <strong class="source-inline">0</strong> and end at <strong class="source-inline">4</strong>. The step of <strong class="source-inline">1</strong> indicates that the indices are all <strong class="source-inline">1</strong> apart, meaning that we have all the integers in that range. The default index class is <strong class="source-inline">RangeIndex</strong>; however, we can change the index, as we will discuss in <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>. Often, we will either work with an <strong class="source-inline">Index</strong> object of row numbers or date(time)s. </p>
			<p>As with <strong class="source-inline">Series</strong> objects, we can access the underlying data via the <strong class="source-inline">values</strong> attribute. Note that this <strong class="source-inline">Index</strong> object is built on top of a NumPy array:</p>
			<p class="source-code">&gt;&gt;&gt; place_index.values</p>
			<p class="source-code"><strong class="bold">array([0, 1, 2, 3, 4], dtype=int64)</strong></p>
			<p>Some of the useful attributes of <strong class="source-inline">Index</strong> objects include the following:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/Figure_2.2_B16834.jpg" alt="Figure 2.2 – Commonly used index attributes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Commonly used index attributes</p>
			<p>Both NumPy and <strong class="source-inline">pandas</strong> support arithmetic operations, which will be performed element-wise. NumPy will use the position in the array for this:</p>
			<p class="source-code">&gt;&gt;&gt; np.array([1, 1, 1]) + np.array([-1, 0, 1])</p>
			<p class="source-code">array([0, 1, 2])</p>
			<p>With <strong class="source-inline">pandas</strong>, this element-wise arithmetic is performed on matching index values. If we add a <strong class="source-inline">Series</strong> object with an index from <strong class="source-inline">0</strong> to <strong class="source-inline">4</strong> (stored in <strong class="source-inline">x</strong>) and another, <strong class="source-inline">y</strong>, from <strong class="source-inline">1</strong> to <strong class="source-inline">5</strong>, we will only get results were the indices align (<strong class="source-inline">1</strong> through <strong class="source-inline">4</strong>). In <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>, we will discuss some ways to change and align the index so that we <a id="_idIndexMarker181"/>can perform these types of operations without losing data:</p>
			<p class="source-code">&gt;&gt;&gt; numbers = np.linspace(0, 10, num=5) # [0, 2.5, 5, 7.5, 10]</p>
			<p class="source-code">&gt;&gt;&gt; x = pd.Series(numbers) # index is [0, 1, 2, 3, 4]</p>
			<p class="source-code">&gt;&gt;&gt; y = pd.Series(numbers, index=pd.Index([1, 2, 3, 4, 5]))</p>
			<p class="source-code">&gt;&gt;&gt; x + y</p>
			<p class="source-code">0     NaN</p>
			<p class="source-code"><strong class="bold">1     2.5</strong></p>
			<p class="source-code"><strong class="bold">2     7.5</strong></p>
			<p class="source-code"><strong class="bold">3    12.5</strong></p>
			<p class="source-code"><strong class="bold">4    17.5</strong></p>
			<p class="source-code">5     NaN</p>
			<p class="source-code">dtype: float64</p>
			<p>Now that we have had a primer on both the <strong class="source-inline">Series</strong> and <strong class="source-inline">Index</strong> classes, we are ready to learn about the <strong class="source-inline">DataFrame</strong> class. Note that more information on the <strong class="source-inline">Index</strong> class can be found in the respective documentation at <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html</a>.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>DataFrame</h2>
			<p>With <a id="_idIndexMarker182"/>the <strong class="source-inline">Series</strong> class, we essentially had columns of a spreadsheet, with the data all being of the same type. The <strong class="source-inline">DataFrame</strong> class builds upon the <strong class="source-inline">Series</strong> class and can have many columns, each with its own data type; we can think of it as representing the spreadsheet as a whole. We can turn either of the NumPy representations we built from the example data into a <strong class="source-inline">DataFrame</strong> object:</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.DataFrame(array_dict) </p>
			<p class="source-code">&gt;&gt;&gt; df</p>
			<p>This gives <a id="_idIndexMarker183"/>us a dataframe of six series. Note the column before the <strong class="source-inline">time</strong> column; this is the <strong class="source-inline">Index</strong> object for the rows. When creating a <strong class="source-inline">DataFrame</strong> object, <strong class="source-inline">pandas</strong> aligns all the series to the same index. In this case, it is just the row number, but we could easily use the <strong class="source-inline">time</strong> column for this, which would enable some additional <strong class="source-inline">pandas</strong> features, as we will see in <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/Figure_2.3_B16834.jpg" alt="Figure 2.3 – Our first dataframe&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Our first dataframe</p>
			<p>Our columns each have a single data type, but they don't all share the same data type:</p>
			<p class="source-code">&gt;&gt;&gt; df.dtypes</p>
			<p class="source-code"><strong class="bold">time        object</strong></p>
			<p class="source-code"><strong class="bold">place       object</strong></p>
			<p class="source-code"><strong class="bold">magType     object</strong></p>
			<p class="source-code"><strong class="bold">mag        float64</strong></p>
			<p class="source-code"><strong class="bold">alert       object</strong></p>
			<p class="source-code"><strong class="bold">tsunami      int64</strong></p>
			<p class="source-code">dtype: object</p>
			<p>The values of the dataframe look very similar to the initial NumPy representation we had:</p>
			<p class="source-code">&gt;&gt;&gt; df.values</p>
			<p class="source-code"><strong class="bold">array([['2018-10-13 11:10:23.560',</strong></p>
			<p class="source-code"><strong class="bold">        '262km NW of Ozernovskiy, Russia',</strong></p>
			<p class="source-code"><strong class="bold">        'mww', 6.7, 'green', 1],</strong></p>
			<p class="source-code"><strong class="bold">       ['2018-10-13 04:34:15.580', </strong></p>
			<p class="source-code"><strong class="bold">        '25km E of Bitung, Indonesia', 'mww', 5.2, 'green', 0],</strong></p>
			<p class="source-code"><strong class="bold">       ['2018-10-13 00:13:46.220', '42km WNW of Sola, Vanuatu', </strong></p>
			<p class="source-code"><strong class="bold">        'mww', 5.7, 'green', 0],</strong></p>
			<p class="source-code"><strong class="bold">       ['2018-10-12 21:09:49.240',</strong></p>
			<p class="source-code"><strong class="bold">        '13km E of Nueva Concepcion, Guatemala',</strong></p>
			<p class="source-code"><strong class="bold">        'mww', 5.7, 'green', 0],</strong></p>
			<p class="source-code"><strong class="bold">       ['2018-10-12 02:52:03.620','128 km SE of Kimbe, </strong></p>
			<p class="source-code"><strong class="bold">         Papua New Guinea', 'mww', 5.6, 'green', 1]], </strong></p>
			<p class="source-code"><strong class="bold">      dtype=object)</strong></p>
			<p>We can <a id="_idIndexMarker184"/>access the column names via the <strong class="source-inline">columns</strong> attribute. Note that they are actually stored in an <strong class="source-inline">Index</strong> object as well:</p>
			<p class="source-code">&gt;&gt;&gt; df.columns</p>
			<p class="source-code"><strong class="bold">Index(['time', 'place', 'magType', 'mag', 'alert', 'tsunami'], </strong></p>
			<p class="source-code"><strong class="bold">      dtype='object')</strong></p>
			<p>The following are some commonly used dataframe attributes:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/Figure_2.4_B16834.jpg" alt="Figure 2.4 – Commonly used dataframe attributes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Commonly used dataframe attributes</p>
			<p>Note that <a id="_idIndexMarker185"/>we can also perform arithmetic on dataframes. For example, we can add <strong class="source-inline">df</strong> to itself, which will sum the numeric columns and concatenate the string columns:</p>
			<p class="source-code">&gt;&gt;&gt; df + df</p>
			<p>Pandas will only perform the operation when both the index and column match. Here, <strong class="source-inline">pandas</strong> concatenated the string columns (<strong class="source-inline">time</strong>, <strong class="source-inline">place</strong>, <strong class="source-inline">magType</strong>, and <strong class="source-inline">alert</strong>) across dataframes. The numeric columns (<strong class="source-inline">mag</strong> and <strong class="source-inline">tsunami</strong>) were summed:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/Figure_2.5_B16834.jpg" alt="Figure 2.5 – Adding dataframes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – Adding dataframes</p>
			<p>More information on <strong class="source-inline">DataFrame</strong> objects and all the operations that can be performed directly on them is available in the official documentation at <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html</a>; be sure to bookmark it for future reference. Now, we are ready to begin learning how to create <strong class="source-inline">DataFrame</strong> objects from a variety of sources.</p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Creating a pandas DataFrame</h1>
			<p>Now that we understand the data structures we will be working with, we can discuss the different ways <a id="_idIndexMarker186"/>we can create them. Before we dive into the code however, it's important to know how to get help right from Python. Should we ever find ourselves unsure of how to use something in Python, we can utilize the built-in <strong class="source-inline">help()</strong> function. We simply run <strong class="source-inline">help()</strong>, passing in the package, module, class, object, method, or function that we want to read the documentation on. We can, of course, look up the documentation online; however, in most cases, the <strong class="bold">docstrings</strong> (the documentation text written in the code) that are returned with <strong class="source-inline">help()</strong> will be equivalent to this since they are used to generate the documentation.</p>
			<p>Assuming we first ran <strong class="source-inline">import pandas as pd</strong>, we can run <strong class="source-inline">help(pd)</strong> to display information about the <strong class="source-inline">pandas</strong> package; <strong class="source-inline">help(pd.DataFrame)</strong> for all the methods and attributes of <strong class="source-inline">DataFrame</strong> objects (note we can also pass in a <strong class="source-inline">DataFrame</strong> object instead); and <strong class="source-inline">help(pd.read_csv)</strong> to learn more about the <strong class="source-inline">pandas</strong> function for reading CSV files into Python and how to use it. We can also try using the <strong class="source-inline">dir()</strong> function and the <strong class="source-inline">__dict__</strong> attribute, which will give us a list or dictionary of what's available, respectively; these might not be as useful as the <strong class="source-inline">help()</strong> function, though.</p>
			<p>Additionally, we can use <strong class="source-inline">?</strong> and <strong class="source-inline">??</strong> to get help, thanks to IPython, which is part of what makes Jupyter Notebooks so powerful. Unlike the <strong class="source-inline">help()</strong> function, we can use question marks by putting them after whatever we want to know more about, as if we were asking Python a question; for example, <strong class="source-inline">pd.read_csv?</strong> and <strong class="source-inline">pd.read_csv??</strong>. These three will yield slightly different outputs: <strong class="source-inline">help()</strong> will give us the docstring; <strong class="source-inline">?</strong> will give the docstring, plus some additional information, depending on what we are inquiring about; and <strong class="source-inline">??</strong> will give us even more information and, if possible, the source code behind it. </p>
			<p>Let's now turn to the next notebook, <strong class="source-inline">2-creating_dataframes.ipynb</strong>, and import the packages we will need for the upcoming examples. We will be using <strong class="source-inline">datetime</strong> from the Python standard library, along with the third-party packages <strong class="source-inline">numpy</strong> and <strong class="source-inline">pandas</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; import datetime as dt</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We have <strong class="bold">aliased</strong> each of our imports. This allows us to use the <strong class="source-inline">pandas</strong> package by referring to it with the alias we assign to be <strong class="source-inline">pd</strong>, which is the most common way of importing it. In fact, we can only refer to it as <strong class="source-inline">pd</strong>, since that is what we imported into the namespace. Packages need to be imported before we can use them; installation puts the files we need on our computer, but, in the interest of memory, Python won't load every installed package when we start it up—just the ones we tell it to.</p>
			<p>We are <a id="_idIndexMarker187"/>now ready to begin using <strong class="source-inline">pandas</strong>. First, we will learn how to create <strong class="source-inline">pandas</strong> objects from other Python objects. Then, we will learn how to do so with flat files, tables in a database, and responses from API requests.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>From a Python object</h2>
			<p>Before we cover all the ways we can create a <strong class="source-inline">DataFrame</strong> object from a Python object, we should <a id="_idIndexMarker188"/>learn how to make a <strong class="source-inline">Series</strong> object. Remember that a <strong class="source-inline">Series</strong> object is essentially a column in a <strong class="source-inline">DataFrame</strong> object, so, once we know this, it should be easy to understand how to create a <strong class="source-inline">DataFrame</strong> object. Say that we wanted to create a series of five random numbers between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. We could use NumPy to generate the random numbers as an array and create the series from that.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">NumPy makes it very easy to generate numerical data. Aside from generating random numbers, we can use it to get evenly-spaced numbers in a certain range with the <strong class="source-inline">np.linspace()</strong> function; obtain a range of integers with the <strong class="source-inline">np.arange()</strong> function; sample from the standard normal with the <strong class="source-inline">np.random.normal()</strong> function; and easily create arrays of all zeros with the <strong class="source-inline">np.zeros()</strong> function and all ones with the <strong class="source-inline">np.ones()</strong> function. We will be using NumPy throughout this book.</p>
			<p>To ensure that the result is reproducible, we will set the seed here. The <strong class="bold">seed</strong> gives a starting point for the generation of pseudorandom numbers. No algorithms for random number generation are truly random—they are deterministic, so by setting this starting point, the numbers that are generated will be the same each time the code is run. This is good for testing things, but not for simulation (where we want randomness), which we will look at in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>. In this fashion, we can make a <strong class="source-inline">Series</strong> object with any list-like structure (such as NumPy arrays):</p>
			<p class="source-code">&gt;&gt;&gt; np.random.seed(0) # set a seed for reproducibility</p>
			<p class="source-code">&gt;&gt;&gt; pd.Series(np.random.rand(5), name='random')</p>
			<p class="source-code">0    0.548814</p>
			<p class="source-code">1    0.715189</p>
			<p class="source-code">2    0.602763</p>
			<p class="source-code">3    0.544883</p>
			<p class="source-code">4    0.423655</p>
			<p class="source-code">Name: random, dtype: float64</p>
			<p>Making a <strong class="source-inline">DataFrame</strong> object is an extension of making a <strong class="source-inline">Series</strong> object; it will be composed of one or more series, and each will be distinctly named. This should remind us of <a id="_idIndexMarker189"/>dictionary-like structures in Python: the keys are the column names, and the values are the contents of the columns. Note that if we want to turn a single <strong class="source-inline">Series</strong> object into a <strong class="source-inline">DataFrame</strong> object, we can use its <strong class="source-inline">to_frame()</strong> method.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">In computer science, a <strong class="bold">constructor</strong> is a piece of code that initalizes new instances of a class, preparing them for use. Python classes implement this with the <strong class="source-inline">__init__()</strong> method. When we run <strong class="source-inline">pd.Series()</strong>, Python calls <strong class="source-inline">pd.Series.__init__()</strong>, which contains instructions for instantiating a new <strong class="source-inline">Series</strong> object. We will learn more about the <strong class="source-inline">__init__()</strong> method in <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>.</p>
			<p>Since columns can all be different data types, let's get a little fancy with this example. We are going to create a <strong class="source-inline">DataFrame</strong> object containing three columns, with five observations each:</p>
			<ul>
				<li><strong class="source-inline">random</strong>: Five random numbers between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> as a NumPy array</li>
				<li><strong class="source-inline">text</strong>: A list of five strings or <strong class="source-inline">None</strong></li>
				<li><strong class="source-inline">truth</strong>: A list of five random Booleans</li>
			</ul>
			<p>We will also create a <strong class="source-inline">DatetimeIndex</strong> object with the <strong class="source-inline">pd.date_range()</strong> function. The index will contain five dates (<strong class="source-inline">periods=5</strong>), all one day apart (<strong class="source-inline">freq='1D'</strong>), ending with April 21, 2019 (<strong class="source-inline">end</strong>), and will be called <strong class="source-inline">date</strong>. Note that more information on the values the <strong class="source-inline">pd.date_range()</strong> function accepts for frequencies can be found at <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases">https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases</a>.</p>
			<p>All we <a id="_idIndexMarker190"/>have to do is package the columns in a dictionary using the desired column names as the keys and pass this in when we call the <strong class="source-inline">pd.DataFrame()</strong> constructor. The index gets passed as the <strong class="source-inline">index</strong> argument:</p>
			<p class="source-code">&gt;&gt;&gt; np.random.seed(0) # set seed so result is reproducible</p>
			<p class="source-code">&gt;&gt;&gt; pd.DataFrame(</p>
			<p class="source-code">...     {</p>
			<p class="source-code">...         'random': np.random.rand(5),</p>
			<p class="source-code">...         'text': ['hot', 'warm', 'cool', 'cold', None],</p>
			<p class="source-code">...         'truth': [np.random.choice([True, False]) </p>
			<p class="source-code">...                   for _ in range(5)]</p>
			<p class="source-code">...     }, </p>
			<p class="source-code">...     index=pd.date_range(</p>
			<p class="source-code">...         end=dt.date(2019, 4, 21),</p>
			<p class="source-code">...         freq='1D', periods=5, name='date'</p>
			<p class="source-code">...     )</p>
			<p class="source-code">... )</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">By convention, we use <strong class="source-inline">_</strong> to hold variables in a loop that we don't care about. Here, we use <strong class="source-inline">range()</strong> as a counter, and its values are unimportant. More information on the roles <strong class="source-inline">_</strong> plays in Python can be found at <a href="https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc">https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc</a>.</p>
			<p>Having <a id="_idIndexMarker191"/>dates in the index makes it easy to select entries by date (or even in a date range), as we will see in <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/Figure_2.6_B16834.jpg" alt="Figure 2.6 – Creating a dataframe from a dictionary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – Creating a dataframe from a dictionary</p>
			<p>In cases where the data isn't a dictionary, but rather a list of dictionaries, we can still use <strong class="source-inline">pd.DataFrame()</strong>. Data in this format is what we would expect when consuming from an API. Each entry in the list will be a dictionary, where the keys of the dictionary are the column names and the values of the dictionary are the values for that column at that index:</p>
			<p class="source-code">&gt;&gt;&gt; pd.DataFrame([</p>
			<p class="source-code">...     {'mag': 5.2, 'place': 'California'},</p>
			<p class="source-code">...     {'mag': 1.2, 'place': 'Alaska'},</p>
			<p class="source-code">...     {'mag': 0.2, 'place': 'California'},</p>
			<p class="source-code">... ])</p>
			<p>This gives us a dataframe of three rows (one for each entry in the list) with two columns (one for each key in the dictionaries):</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/Figure_2.7_B16834.jpg" alt="Figure 2.7 – Creating a dataframe from a list of dictionaries&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – Creating a dataframe from a list of dictionaries</p>
			<p>In fact, <strong class="source-inline">pd.DataFrame()</strong> also works for lists of tuples. Note that we can also pass in the column <a id="_idIndexMarker192"/>names as a list through the <strong class="source-inline">columns</strong> argument:</p>
			<p class="source-code">&gt;&gt;&gt; list_of_tuples = [(n, n**2, n**3) for n in range(5)]</p>
			<p class="source-code">&gt;&gt;&gt; list_of_tuples</p>
			<p class="source-code">[(0, 0, 0), (1, 1, 1), (2, 4, 8), (3, 9, 27), (4, 16, 64)]</p>
			<p class="source-code">&gt;&gt;&gt; pd.DataFrame(</p>
			<p class="source-code">...     list_of_tuples,</p>
			<p class="source-code">...     <strong class="bold">columns=['n', 'n_squared', 'n_cubed']</strong></p>
			<p class="source-code">... )</p>
			<p>Each tuple is treated like a record and becomes a row in the dataframe:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/Figure_2.8_B16834.jpg" alt="Figure 2.8 – Creating a dataframe from a list of tuples&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – Creating a dataframe from a list of tuples</p>
			<p>We also have the option of using <strong class="source-inline">pd.DataFrame()</strong> with NumPy arrays:</p>
			<p class="source-code">&gt;&gt;&gt; pd.DataFrame(</p>
			<p class="source-code">...     np.array([</p>
			<p class="source-code">...         [0, 0, 0],</p>
			<p class="source-code">...         [1, 1, 1],</p>
			<p class="source-code">...         [2, 4, 8],</p>
			<p class="source-code">...         [3, 9, 27],</p>
			<p class="source-code">...         [4, 16, 64]</p>
			<p class="source-code">...     ]), columns=['n', 'n_squared', 'n_cubed']</p>
			<p class="source-code">... )</p>
			<p>This <a id="_idIndexMarker193"/>will have the effect of stacking each entry in the array as rows in a dataframe, giving us a result that's identical to <em class="italic">Figure 2.8</em>.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>From a file</h2>
			<p>The data we want to analyze will most often come from outside Python. In many cases, we may <a id="_idIndexMarker194"/>obtain a <strong class="bold">data dump</strong> from a database or website and bring it into Python to sift through it. A data dump gets its name from containing a large amount of data (possibly at a very granular level) and often not discriminating against any of it initially; for this reason, they can be unwieldy.</p>
			<p>Often, these data dumps will come in the form of a text file (<strong class="source-inline">.txt</strong>) or a CSV file (<strong class="source-inline">.csv</strong>). Pandas provides many methods for reading in different types of files, so it is simply a matter of looking up the one that matches our file format. Our earthquake data is a CSV file; therefore, we use the <strong class="source-inline">pd.read_csv()</strong> function to read it in. However, we should always do an initial inspection of the file before attempting to read it in; this will inform us of whether we need to pass additional arguments, such as <strong class="source-inline">sep</strong> to specify the delimiter or <strong class="source-inline">names</strong> to provide the column names ourselves in the absence of a header row in the file.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout"><strong class="bold">Windows users</strong>: Depending on your setup, the commands in the next few code blocks may not work. The notebook contains alternatives if you encounter issues.</p>
			<p>We can perform our due diligence directly in our Jupyter Notebook thanks to IPython, provided we prefix our commands with <strong class="source-inline">!</strong> to indicate they are to be run as shell commands. First, we should check how big the file is, both in terms of lines and in terms of bytes. To check the number of lines, we use the <strong class="source-inline">wc</strong> utility (word count) with the <strong class="source-inline">–l</strong> flag to count the number of lines. We have 9,333 rows in the file:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">!wc -l data/earthquakes.csv</strong></p>
			<p class="source-code"><strong class="bold">9333</strong> data/earthquakes.csv</p>
			<p>Now, let's check the file's size. For this task, we will use <strong class="source-inline">ls</strong> on the <strong class="source-inline">data</strong> directory. This will show <a id="_idIndexMarker195"/>us the list of files in that directory. We can add the <strong class="source-inline">-lh</strong> flag to get information about the files in a human-readable format. Finally, we send this output to the <strong class="source-inline">grep</strong> utility, which will help us isolate the files we want. This tells us that the <strong class="source-inline">earthquakes.csv</strong> file is 3.4 MB:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">!ls -lh data | grep earthquakes.csv</strong></p>
			<p class="source-code">-rw-r--r-- 1 stefanie stefanie <strong class="bold">3.4M</strong> ... earthquakes.csv</p>
			<p>Note that IPython also lets us capture the result of the command in a Python variable, so if we aren't comfortable with pipes (<strong class="source-inline">|</strong>) or <strong class="source-inline">grep</strong>, we can do the following:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">files = !ls -lh data</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">[file for file in files if 'earthquake' in file]</strong></p>
			<p class="source-code">['-rw-r--r-- 1 stefanie stefanie 3.4M ... earthquakes.csv']</p>
			<p>Now, let's take a look at the top few rows to see if the file comes with headers. We will use the <strong class="source-inline">head</strong> utility and specify the number of rows with the <strong class="source-inline">-n</strong> flag. This tells us that the first row contains the headers for the data and that the data is delimited with commas (just because the file has the <strong class="source-inline">.csv</strong> extension does not mean it is comma-delimited):</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">!head -n 2 data/earthquakes.csv</strong></p>
			<p class="source-code"><strong class="bold">alert,cdi,code,detail,dmin,felt,gap,ids,mag,magType,mmi,net,nst,place,rms,sig,sources,status,time,title,tsunami,type,types,tz,updated,url</strong></p>
			<p class="source-code">,,37389218,https://earthquake.usgs.gov/[...],0.008693,,85.0,",ci37389218,",1.35,ml,,ci,26.0,"9km NE of Aguanga, CA",0.19,28,",ci,",automatic,1539475168010,"M 1.4 - 9km NE of Aguanga, CA",0,earthquake,",geoserve,nearby-cities,origin,phase-data,",-480.0,1539475395144,https://earthquake.usgs.gov/earthquakes/eventpage/ci37389218</p>
			<p>Note that we should also check the bottom rows to make sure there is no extraneous data that we will need to ignore by using the <strong class="source-inline">tail</strong> utility. This file is fine, so the result won't be reproduced here; however, the notebook contains the result.</p>
			<p>Lastly, we may be interested in seeing the column count in our data. While we could just count the fields in the first row of the result of <strong class="source-inline">head</strong>, we have the option of using the <strong class="source-inline">awk</strong> utility (for pattern scanning and processing) to count our columns. The <strong class="source-inline">-F</strong> flag allows us <a id="_idIndexMarker196"/>to specify the delimiter (a comma, in this case). Then, we specify what to do for each record in the file. We choose to print <strong class="source-inline">NF</strong>, which is a predefined variable whose value is the number of fields in the current record. Here, we say <strong class="source-inline">exit</strong> immediately after the print so that we print the number of fields in the first row of the file; then, we stop. This will look a little complicated, but this is by no means something we need to memorize:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">!awk -F',' '{print NF; exit}' data/earthquakes.csv</strong></p>
			<p class="source-code">26</p>
			<p>Since we know that the first line of the file contains headers and that the file is comma-separated, we can also count the columns by using <strong class="source-inline">head</strong> to get the headers and Python to parse them:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">headers = !head -n 1 data/earthquakes.csv</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">len(headers[0].split(','))</strong></p>
			<p class="source-code">26</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The ability to run shell commands directly from our Jupyter Notebook dramatically streamlines our workflow. However, if we don't have past experience with the command line, it may be complicated to learn these commands initially. IPython has some helpful information on running shell commands in their documentation at <a href="https://ipython.readthedocs.io/en/stable/interactive/reference.html#system-shell-access">https://ipython.readthedocs.io/en/stable/interactive/reference.html#system-shell-access</a>.</p>
			<p>To summarize, we now know that the file is 3.4 MB and is comma-delimited with 26 columns and 9,333 rows, with the first one being the header. This means that we can use the <strong class="source-inline">pd.read_csv()</strong> function with the defaults:</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.read_csv('earthquakes.csv')</p>
			<p>Note that we aren't limited to reading in data from files on our local machines; file paths can be URLs as well. As an example, let's read in the same CSV file from GitHub:</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.read_csv(</p>
			<p class="source-code">...     'https://github.com/stefmolin/'</p>
			<p class="source-code">...     'Hands-On-Data-Analysis-with-Pandas-2nd-edition'</p>
			<p class="source-code">...     '/blob/master/ch_02/data/earthquakes.csv?raw=True'</p>
			<p class="source-code">... )</p>
			<p>Pandas is <a id="_idIndexMarker197"/>usually very good at figuring out which options to use based on the input data, so we often won't need to add arguments to this call; however, there are many options available should we need them, some of which include the following:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/Figure_2.9_B16834.jpg" alt="Figure 2.9 – Helpful parameters when reading data from a file&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – Helpful parameters when reading data from a file</p>
			<p>Throughout <a id="_idIndexMarker198"/>this book, we will be working with CSV files; however, note that we can use the <strong class="source-inline">read_excel()</strong> function to read in Excel files, the <strong class="source-inline">read_json()</strong> function for <strong class="bold">JSON</strong> (<strong class="bold">JavaScript Object Notation</strong>) files, and for other <a id="_idIndexMarker199"/>delimited files, such as tab (<strong class="source-inline">\t</strong>), we can use the <strong class="source-inline">read_csv()</strong> function with the <strong class="source-inline">sep</strong> argument equal to the delimiter. </p>
			<p>It would be remiss if we didn't also learn how to save our dataframe to a file so that we can share it with others. To write our dataframe to a CSV file, we call its <strong class="source-inline">to_csv()</strong> method. We have to be careful here; if our dataframe's index is just row numbers, we probably don't want to write that to our file (it will have no meaning to the consumers of the data), but it is the default. We can write our data without the index by passing in <strong class="source-inline">index=False</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df.to_csv('output.csv', index=False)</p>
			<p>As with reading from files, <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> objects have methods to write data to Excel (<strong class="source-inline">to_excel()</strong>) and JSON files (<strong class="source-inline">to_json()</strong>). Note that, while we use functions from <strong class="source-inline">pandas</strong> to read our data in, we must use methods to write our data; the reading functions create the <strong class="source-inline">pandas</strong> objects that we want to work with, but the writing methods are actions that we take using the <strong class="source-inline">pandas</strong> object.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The preceding file paths to read from and write to were <strong class="bold">relative</strong> to our <strong class="bold">current directory</strong>. The current directory is where we are running our code from. An <strong class="bold">absolute</strong> path will be the full path to the file. For example, if the file we want to work with has an absolute path of <strong class="source-inline">/home/myuser/learning/hands_on_pandas/data.csv</strong> and our current directory is <strong class="source-inline">/home/myuser/learning/hands_on_pandas</strong>, then we can simply use the relative path of <strong class="source-inline">data.csv</strong> as the file path.</p>
			<p>Pandas provides <a id="_idIndexMarker200"/>us with capabilities to read and write from many other data sources, including databases, which we will discuss next; pickle files (containing serialized Python objects—see the <em class="italic">Further reading</em> section for more information); and HTML pages. Be sure to check out the following resource in the <strong class="source-inline">pandas</strong> documentation for the full list of capabilities: <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html">https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html</a>.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>From a database</h2>
			<p>Pandas can interact with SQLite databases without the need for us to install any additional <a id="_idIndexMarker201"/>packages; however, the SQLAlchemy package needs to be installed in order to interact with other database flavors. Interaction with a SQLite database can be achieved by opening a connection to the database using the <strong class="source-inline">sqlite3</strong> module in the Python standard library and then using either the <strong class="source-inline">pd.read_sql()</strong> function to query the database or the <strong class="source-inline">to_sql()</strong> method on a <strong class="source-inline">DataFrame</strong> object to write it to the database.</p>
			<p>Before we read from a database, let's write to one. We simply call <strong class="source-inline">to_sql()</strong> on our dataframe, telling it which table to write to, which database connection to use, and how to handle if the table already exists. There is already a SQLite database in the folder for this chapter in this book's GitHub repository: <strong class="source-inline">data/quakes.db</strong>. Note that, to create a new database, we can change <strong class="source-inline">'data/quakes.db'</strong> to the path for the new database file. Let's write the tsunami data from the <strong class="source-inline">data/tsunamis.csv</strong> file to a table in the database called <strong class="source-inline">tsunamis</strong>, replacing the table if it already exists:</p>
			<p class="source-code">&gt;&gt;&gt; import sqlite3</p>
			<p class="source-code">&gt;&gt;&gt; with sqlite3.connect('data/quakes.db') as connection:</p>
			<p class="source-code">...     pd.read_csv('data/tsunamis.csv').<strong class="bold">to_sql(</strong></p>
			<p class="source-code">...         <strong class="bold">'tsunamis', connection, index=False,</strong></p>
			<p class="source-code">...         <strong class="bold">if_exists='replace'</strong></p>
			<p class="source-code">...     <strong class="bold">)</strong></p>
			<p>Querying <a id="_idIndexMarker202"/>the database is just as easy as writing to it. Note this will require knowledge of <strong class="bold">Structured Query Language</strong> (<strong class="bold">SQL</strong>). While it's <a id="_idIndexMarker203"/>not required for this book, we will use some simple SQL statements to illustrate certain concepts. See the <em class="italic">Further reading</em> section for a resource on how <strong class="source-inline">pandas</strong> compares to SQL and <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>, for some examples of how <strong class="source-inline">pandas</strong> actions relate to SQL statements.</p>
			<p>Let's query our database for the full <strong class="source-inline">tsunamis</strong> table. When we write a SQL query, we first state the columns that we want to select, which in our case is all of them, so we write <strong class="source-inline">"SELECT *"</strong>. Next, we state the table to select the data from, which for us is <strong class="source-inline">tsunamis</strong>, so we add <strong class="source-inline">"FROM tsunamis"</strong>. This is our full query now (of course, it can get much more complicated than this). To actually query the database, we use <strong class="source-inline">pd.read_sql()</strong>, passing in our query and the database connection:</p>
			<p class="source-code">&gt;&gt;&gt; import sqlite3</p>
			<p class="source-code">&gt;&gt;&gt; with sqlite3.connect('data/quakes.db') as connection:</p>
			<p class="source-code">...     <strong class="bold">tsunamis = \</strong></p>
			<p class="source-code">...         <strong class="bold">pd.read_sql('SELECT * FROM tsunamis', connection)</strong></p>
			<p class="source-code">&gt;&gt;&gt; tsunamis.head()</p>
			<p>We now <a id="_idIndexMarker204"/>have the tsunamis data in a dataframe:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/Figure_2.10_B16834.jpg" alt="Figure 2.10 – Reading data from a database&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10 – Reading data from a database</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The <strong class="source-inline">connection</strong> object we created in both code blocks is an example of a <strong class="bold">context manager</strong>, which, when <a id="_idIndexMarker205"/>used with the <strong class="source-inline">with</strong> statement, automatically handles cleanup after the code in the block executes (closing the connection, in this case). This makes cleanup easy and makes sure we don't leave any loose ends. Be sure to check out <strong class="source-inline">contextlib</strong> from the standard library for utilities using the <strong class="source-inline">with</strong> statement and context managers. The documentation is at <a href="https://docs.python.org/3/library/contextlib.html">https://docs.python.org/3/library/contextlib.html</a>.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>From an API</h2>
			<p>We can now easily create <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> objects from data we have in Python or from files <a id="_idIndexMarker206"/>we obtain, but how can we get data from online resources, such as APIs? There is no guarantee that each data source will give us data in the same format, so we must remain flexible in our approach and be comfortable examining the data source to find the appropriate import method. In this section, we will request some earthquake data from the USGS API and see how we can make a dataframe out of the result. In <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>, we will work with another API to gather weather data.</p>
			<p>For this section, we will be working in the <strong class="source-inline">3-making_dataframes_from_api_requests.ipynb</strong> notebook, so we have to import the packages we need once again. As with the previous notebook, we need <strong class="source-inline">pandas</strong> and <strong class="source-inline">datetime</strong>, but we also need the <strong class="source-inline">requests</strong> package to make API requests:</p>
			<p class="source-code">&gt;&gt;&gt; import datetime as dt</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">import requests</strong></p>
			<p>Next, we will make a <strong class="source-inline">GET</strong> request to the USGS API for a JSON payload (a dictionary-like response containing the data that's sent with a request or response) by specifying the format of <strong class="source-inline">geojson</strong>. We will ask for earthquake data for the last 30 days (we can use <strong class="source-inline">dt.timedelta</strong> to perform arithmetic on <strong class="source-inline">datetime</strong> objects). Note that we are using <strong class="source-inline">yesterday</strong> as the <a id="_idIndexMarker207"/>end of our date range, since the API won't have complete information for today yet:</p>
			<p class="source-code">&gt;&gt;&gt; yesterday = dt.date.today() - dt.timedelta(days=1)</p>
			<p class="source-code">&gt;&gt;&gt; api = 'https://earthquake.usgs.gov/fdsnws/event/1/query'</p>
			<p class="source-code">&gt;&gt;&gt; payload = {</p>
			<p class="source-code">...     'format': 'geojson',</p>
			<p class="source-code">...     'starttime': yesterday - dt.timedelta(days=30),</p>
			<p class="source-code">...     'endtime': yesterday</p>
			<p class="source-code">... }</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">response = requests.get(api, params=payload)</strong></p>
			<p class="callout-heading">Important note</p>
			<p class="callout"><strong class="source-inline">GET</strong> is an HTTP method. This action tells the server we want to read some data. Different APIs may require that we use different methods to get the data; some will require a <strong class="source-inline">POST</strong> request, where we authenticate with the server. You can read more about API requests <a id="_idIndexMarker208"/>and HTTP methods at <a href="https://nordicapis.com/ultimate-guide-to-all-9-standard-http-methods/">https://nordicapis.com/ultimate-guide-to-all-9-standard-http-methods/</a>.</p>
			<p>Before we try to create a dataframe out of this, we should make sure that our request was successful. We can do this by checking the <strong class="source-inline">status_code</strong> attribute of the <strong class="source-inline">response</strong> object. A listing of status codes and their meanings can be found at <a href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes">https://en.wikipedia.org/wiki/List_of_HTTP_status_codes</a>. A <strong class="source-inline">200</strong> response will indicate that everything is OK:</p>
			<p class="source-code">&gt;&gt;&gt; response.status_code</p>
			<p class="source-code">200</p>
			<p>Our request was successful, so let's see what the data we got looks like. We asked the API for a JSON payload, which is essentially a dictionary, so we can use dictionary methods on it to get more information about its structure. This is going to be a lot of data; hence, we don't want to print it to the screen just to inspect it. We need to isolate the JSON payload from the HTTP response (stored in the <strong class="source-inline">response</strong> variable), and then look at the keys to view the main sections of the resulting data:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">earthquake_json = response.json()</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">earthquake_json.keys()</strong></p>
			<p class="source-code">dict_keys(['type', 'metadata', 'features', 'bbox'])</p>
			<p>We can <a id="_idIndexMarker209"/>inspect what kind of data we have as values for each of these keys; one of them will be the data we are after. The <strong class="source-inline">metadata</strong> portion tells us some information about our request. While this can certainly be useful, it isn't what we are after right now:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">earthquake_json['metadata']</strong></p>
			<p class="source-code">{'generated': 1604267813000,</p>
			<p class="source-code"> 'url': 'https://earthquake.usgs.gov/fdsnws/event/1/query?</p>
			<p class="source-code">format=geojson&amp;starttime=2020-10-01&amp;endtime=2020-10-31',</p>
			<p class="source-code"> 'title': 'USGS Earthquakes',</p>
			<p class="source-code"> 'status': 200,</p>
			<p class="source-code"> 'api': '1.10.3',</p>
			<p class="source-code"> 'count': 13706}</p>
			<p>The <strong class="source-inline">features</strong> key looks promising; if this does indeed contain all our data, we should check what type it is so that we don't end up trying to print everything to the screen:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">type(earthquake_json['features'])</strong></p>
			<p class="source-code">list</p>
			<p>This key contains a list, so let's take a look at the first entry to see if this is the data we want. Note that the USGS data may be altered or added to for dates in the past as more information on the earthquakes comes to light, meaning that querying for the same date range may yield a different number of results later on. For this reason, the following <a id="_idIndexMarker210"/>is an example of what an entry looks like:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">earthquake_json['features'][0]</strong></p>
			<p class="source-code">{'type': 'Feature',</p>
			<p class="source-code"> 'properties': {'mag': 1,</p>
			<p class="source-code">  'place': '50 km ENE of Susitna North, Alaska',</p>
			<p class="source-code">  'time': 1604102395919, 'updated': 1604103325550, 'tz': None,</p>
			<p class="source-code">  'url': 'https://earthquake.usgs.gov/earthquakes/eventpage/ak020dz5f85a',</p>
			<p class="source-code">  'detail': 'https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=ak020dz5f85a&amp;format=geojson',</p>
			<p class="source-code">  'felt': None, 'cdi': None, 'mmi': None, 'alert': None,</p>
			<p class="source-code">  'status': 'reviewed', 'tsunami': 0, 'sig': 15, 'net': 'ak',</p>
			<p class="source-code">  'code': '020dz5f85a', 'ids': ',ak020dz5f85a,',</p>
			<p class="source-code">  'sources': ',ak,', 'types': ',origin,phase-data,',</p>
			<p class="source-code">  'nst': None, 'dmin': None, 'rms': 1.36, 'gap': None,</p>
			<p class="source-code">  'magType': 'ml', 'type': 'earthquake',</p>
			<p class="source-code">  'title': 'M 1.0 - 50 km ENE of Susitna North, Alaska'},</p>
			<p class="source-code"> 'geometry': {'type': 'Point', 'coordinates': [-148.9807, 62.3533, 5]},</p>
			<p class="source-code"> 'id': 'ak020dz5f85a'} </p>
			<p>This is definitely the data we are after, but do we need all of it? Upon closer inspection, we only really care about what is inside the <strong class="source-inline">properties</strong> dictionary. Now, we have a problem <a id="_idIndexMarker211"/>because we have a list of dictionaries where we only want a specific key from inside them. How can we pull this information out so that we can make our dataframe? We can use a list comprehension to isolate the <strong class="source-inline">properties</strong> section from each of the dictionaries in the <strong class="source-inline">features</strong> list:</p>
			<p class="source-code">&gt;&gt;&gt; earthquake_properties_data = <strong class="bold">[</strong></p>
			<p class="source-code">...     <strong class="bold">quake['properties'] </strong></p>
			<p class="source-code">...     <strong class="bold">for quake in earthquake_json['features']</strong></p>
			<p class="source-code">... <strong class="bold">]</strong></p>
			<p>Finally, we are ready to create our dataframe. Pandas knows how to handle data in this format already (a list of dictionaries), so all we have to do is pass in the data when we call <strong class="source-inline">pd.DataFrame()</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.DataFrame(earthquake_properties_data)</p>
			<p>Now that we know how to create dataframes from a variety of sources, we can start learning how to work with them.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Inspecting a DataFrame object</h1>
			<p>The first thing we should do when we read in our data is inspect it; we want to make sure that our <a id="_idIndexMarker212"/>dataframe isn't empty and that the rows look as we would expect. Our main goal is to verify that it was read in properly and that all the data is there; however, this initial inspection will also give us ideas with regard to where we should direct our data wrangling efforts. In this section, we will explore ways in which we can inspect our dataframes in the <strong class="source-inline">4-inspecting_dataframes.ipynb</strong> notebook.</p>
			<p>Since this is a new notebook, we must once again handle our setup. This time, we need to import <strong class="source-inline">pandas</strong> and <strong class="source-inline">numpy</strong>, as well as read in the CSV file with the earthquake data:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.read_csv('data/earthquakes.csv')</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/>Examining the data</h2>
			<p>First, we want <a id="_idIndexMarker213"/>to make sure that we actually have data in our dataframe. We can check the <strong class="source-inline">empty</strong> attribute to find out:</p>
			<p class="source-code">&gt;&gt;&gt; df.empty</p>
			<p class="source-code">False</p>
			<p>So far, so good; we have data. Next, we should check how much data we read in; we want to know the number of observations (rows) and the number of variables (columns) we have. For this task, we use the <strong class="source-inline">shape</strong> attribute. Our data contains 9,332 observations of 26 variables, which matches our initial inspection of the file:</p>
			<p class="source-code">&gt;&gt;&gt; df.shape</p>
			<p class="source-code">(9332, 26)</p>
			<p>Now, let's use the <strong class="source-inline">columns</strong> attribute to see the names of the columns in our dataset:</p>
			<p class="source-code">&gt;&gt;&gt; df.columns</p>
			<p class="source-code">Index(['alert', 'cdi', 'code', 'detail', 'dmin', 'felt', 'gap', </p>
			<p class="source-code">       'ids', 'mag', 'magType', 'mmi', 'net', 'nst', 'place', </p>
			<p class="source-code">       'rms', 'sig', 'sources', 'status', 'time', 'title', </p>
			<p class="source-code">       'tsunami', 'type', 'types', 'tz', 'updated', 'url'],</p>
			<p class="source-code">      dtype='object')</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Having a list of columns doesn't necessarily mean that we know what all of them mean. Especially in cases where our data comes from the Internet, be sure to read up on what the columns mean before drawing any conclusions. Information on the fields in the <strong class="source-inline">geojson</strong> format, including what each field in the JSON payload means (along with some example values), can be found on the USGS website at <a href="https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php">https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php</a>.</p>
			<p>We know <a id="_idIndexMarker214"/>the dimensions of our data, but what does it actually look like? For this task, we can use the <strong class="source-inline">head()</strong> and <strong class="source-inline">tail()</strong> methods to look at the top and bottom rows, respectively. This will default to five rows, but we can change this by passing a different number to the method. Let's take a look at the first few rows:</p>
			<p class="source-code">&gt;&gt;&gt; df.head()</p>
			<p>The following are the first five rows we get using <strong class="source-inline">head()</strong>:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/Figure_2.11_B16834.jpg" alt="Figure 2.11 – Examining the top five rows of a dataframe&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11 – Examining the top five rows of a dataframe</p>
			<p>To get the last two rows, we use the <strong class="source-inline">tail()</strong> method and pass <strong class="source-inline">2</strong> as the number of rows:</p>
			<p class="source-code">&gt;&gt;&gt; df.tail(2)</p>
			<p>The following is the result:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/Figure_2.12_B16834.jpg" alt="Figure 2.12 – Examining the bottom two rows of a dataframe&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12 – Examining the bottom two rows of a dataframe</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">By default, when we print dataframes with many columns in a Jupyter Notebook, only a subset of them will be displayed. This is because <strong class="source-inline">pandas</strong> has a limit on the number of columns it will show. We can modify this behavior using <strong class="source-inline">pd.set_option('display.max_columns', &lt;new_value&gt;)</strong>. Consult the documentation at <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html">https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html</a> for additional information. The notebook also contains a few example commands.</p>
			<p>We can <a id="_idIndexMarker215"/>use the <strong class="source-inline">dtypes</strong> attribute to see the data types of the columns, which makes it easy to see when columns are being stored as the wrong type. (Remember that strings will be stored as <strong class="source-inline">object</strong>.) Here, the <strong class="source-inline">time</strong> column is stored as an integer, which is something we will learn how to fix in <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>:</p>
			<p class="source-code">&gt;&gt;&gt; df.dtypes</p>
			<p class="source-code">alert       object</p>
			<p class="source-code">...</p>
			<p class="source-code">mag        float64</p>
			<p class="source-code">magType     object</p>
			<p class="source-code">...</p>
			<p class="source-code"><strong class="bold">time         int64</strong></p>
			<p class="source-code">title       object</p>
			<p class="source-code">tsunami      int64</p>
			<p class="source-code">...</p>
			<p class="source-code">tz         float64</p>
			<p class="source-code">updated      int64</p>
			<p class="source-code">url         object</p>
			<p class="source-code">dtype: object</p>
			<p>Lastly, we can use the <strong class="source-inline">info()</strong> method to see how many non-null entries of each column we have <a id="_idIndexMarker216"/>and get information on our index. <strong class="bold">Null</strong> values are missing values, which, in <strong class="source-inline">pandas</strong>, will typically be represented as <strong class="source-inline">None</strong> for objects and <strong class="source-inline">NaN</strong> (<strong class="bold">Not a Number</strong>) for <a id="_idIndexMarker217"/>non-numeric values in a <strong class="source-inline">float</strong> or <strong class="source-inline">integer</strong> column:</p>
			<p class="source-code">&gt;&gt;&gt; df.info()</p>
			<p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p>
			<p class="source-code">RangeIndex: 9332 entries, 0 to 9331</p>
			<p class="source-code">Data columns (total 26 columns):</p>
			<p class="source-code"> #   Column   Non-Null Count  Dtype  </p>
			<p class="source-code">---  ------   --------------  -----  </p>
			<p class="source-code"> 0   alert    59 non-null     object </p>
			<p class="source-code"> ... </p>
			<p class="source-code"> 8   mag      9331 non-null   float64</p>
			<p class="source-code"> 9   magType  9331 non-null   object </p>
			<p class="source-code"> ... </p>
			<p class="source-code"> 18  time     9332 non-null   int64  </p>
			<p class="source-code"> 19  title    9332 non-null   object </p>
			<p class="source-code"> 20  tsunami  9332 non-null   int64  </p>
			<p class="source-code"> ... </p>
			<p class="source-code"> 23  tz       9331 non-null   float64</p>
			<p class="source-code"> 24  updated  9332 non-null   int64  </p>
			<p class="source-code"> 25  url      9332 non-null   object </p>
			<p class="source-code">dtypes: float64(9), int64(4), object(13)</p>
			<p class="source-code">memory usage: 1.9+ MB</p>
			<p>After this <a id="_idIndexMarker218"/>initial inspection, we know a lot about the structure of our data and can now begin to try and make sense of it.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Describing and summarizing the data</h2>
			<p>So far, we've examined the structure of the <strong class="source-inline">DataFrame</strong> object we created from the earthquake <a id="_idIndexMarker219"/>data, but we don't know anything <a id="_idIndexMarker220"/>about the data other than what a couple of rows look like. The next step is to calculate summary statistics, which will help us get to know our data better. Pandas provides several methods for easily doing so; one such method is <strong class="source-inline">describe()</strong>, which also works on <strong class="source-inline">Series</strong> objects if we are only interested in a particular column. Let's get a summary of the numeric columns in our data:</p>
			<p class="source-code">&gt;&gt;&gt; df.describe()</p>
			<p>This gives us the 5-number summary, along with the count, mean, and standard deviation of the numeric columns:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/Figure_2.13_B16834.jpg" alt="Figure 2.13 – Calculating summary statistics&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13 – Calculating summary statistics</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If we want different percentiles, we can pass them in with the <strong class="source-inline">percentiles</strong> argument. For example, if we wanted only the 5<span class="superscript">th</span> and 95<span class="superscript">th</span> percentiles, we would run <strong class="source-inline">df.describe(percentiles=[0.05, 0.95])</strong>. Note we will still get the 50<span class="superscript">th</span> percentile back because that is the median.</p>
			<p>By default, <strong class="source-inline">describe()</strong> won't give us any information about the columns of type <strong class="source-inline">object</strong>, but we can either provide <strong class="source-inline">include='all'</strong> as an argument or run it separately for the data of type <strong class="source-inline">np.object</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df.describe(<strong class="bold">include=np.object</strong>)</p>
			<p>When describing <a id="_idIndexMarker221"/>non-numeric data, we still get the <a id="_idIndexMarker222"/>count of non-null occurrences (<strong class="bold">count</strong>); however, instead of the other summary statistics, we get the number of unique values (<strong class="bold">unique</strong>), the mode (<strong class="bold">top</strong>), and the number of times the mode was observed (<strong class="bold">freq</strong>):</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/Figure_2.14_B16834.jpg" alt="Figure 2.14 – Summary statistics for categorical columns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.14 – Summary statistics for categorical columns</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The <strong class="source-inline">describe()</strong> method only gives us summary statistics for non-null values. This means that, if we had 100 rows and half of our data was null, then the average would be calculated as the sum of the 50 non-null rows divided by 50.</p>
			<p>It is easy to get a snapshot of our data using the <strong class="source-inline">describe()</strong> method, but sometimes, we just want a particular statistic, either for a specific column or for all the columns. Pandas makes this a cinch as well. The following table includes methods that will work for both <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> objects:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/Figure_2.15_B16834.jpg" alt="Figure 2.15 – Helpful calculation methods for series and dataframes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.15 – Helpful calculation methods for series and dataframes</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Python <a id="_idIndexMarker223"/>makes it easy to count how many times <a id="_idIndexMarker224"/>something is <strong class="source-inline">True</strong>. Under the hood, <strong class="source-inline">True</strong> evaluates to <strong class="source-inline">1</strong> and <strong class="source-inline">False</strong> evaluates to <strong class="source-inline">0</strong>. Therefore, we can run the <strong class="source-inline">sum()</strong> method on a series of Booleans and get the count of <strong class="source-inline">True</strong> outputs.</p>
			<p>With <strong class="source-inline">Series</strong> objects, we have some additional methods for describing our data:</p>
			<ul>
				<li><strong class="source-inline">unique()</strong>: Returns the distinct values of the column.</li>
				<li><strong class="source-inline">value_counts()</strong>: Returns a frequency table of the number of times each unique value in a given column appears, or, alternatively, the percentage of times each unique value appears when passed <strong class="source-inline">normalize=True</strong>.</li>
				<li><strong class="source-inline">mode()</strong>: Returns the most common value of the column.</li>
			</ul>
			<p>Consulting <a id="_idIndexMarker225"/>the USGS API documentation for the <strong class="source-inline">alert</strong> field (which can be found at <a href="https://earthquake.usgs.gov/data/comcat/data-eventterms.php#alert">https://earthquake.usgs.gov/data/comcat/data-eventterms.php#alert</a>) tells us that it can be <strong class="source-inline">'green'</strong>, <strong class="source-inline">'yellow'</strong>, <strong class="source-inline">'orange'</strong>, or <strong class="source-inline">'red'</strong> (when populated), and <a id="_idIndexMarker226"/>that it is the alert level from the <strong class="bold">Prompt Assessment of Global Earthquakes for Response</strong> (<strong class="bold">PAGER</strong>) earthquake <a id="_idIndexMarker227"/>impact scale. According to the USGS (<a href="https://earthquake.usgs.gov/data/pager/">https://earthquake.usgs.gov/data/pager/</a>), "<em class="italic">the PAGER system provides fatality and economic loss impact estimates following significant earthquakes worldwide</em>." From our initial inspection of the data, we know that the <strong class="source-inline">alert</strong> column is a string of two unique values and that the most common value is <strong class="source-inline">'green'</strong>, with many null values. What is the other unique value, though?</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">df.alert.unique()</strong></p>
			<p class="source-code">array([nan, 'green', <strong class="bold">'red'</strong>], dtype=object)</p>
			<p>Now that we understand what this field means and the values we have in our data, we expect there to be far more <strong class="source-inline">'green'</strong> than <strong class="source-inline">'red'</strong>; we can check our intuition with a frequency table by using <strong class="source-inline">value_counts()</strong>. Notice that we only get counts for the non-null entries:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">df.alert.value_counts()</strong></p>
			<p class="source-code">green    58</p>
			<p class="source-code">red       1</p>
			<p class="source-code">Name: alert, dtype: int64</p>
			<p>Note that <strong class="source-inline">Index</strong> objects also have several methods that can help us describe and summarize our data:</p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/Figure_2.16_B16834.jpg" alt="Figure 2.16 – Helpful methods for the index&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.16 – Helpful methods for the index</p>
			<p>When <a id="_idIndexMarker228"/>we used <strong class="source-inline">unique()</strong> and <strong class="source-inline">value_counts()</strong>, we got <a id="_idIndexMarker229"/>a preview of how to select subsets of our data. Now, let's go into more detail and cover selection, slicing, indexing, and filtering.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Grabbing subsets of the data</h1>
			<p>So far, we have learned how to work with and summarize the data as a whole; however, we will often <a id="_idIndexMarker230"/>be interested in performing operations and/or analyses on subsets of our data. There are many types of subsets we may look to isolate from our data, such as selecting only specific columns or rows as a whole or when a specific criterion is met. In order to obtain subsets of the data, we need to be familiar with selection, slicing, indexing, and filtering.</p>
			<p>For this section, we will work in the <strong class="source-inline">5-subsetting_data.ipynb</strong> notebook. Our setup is as follows:</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.read_csv('data/earthquakes.csv')</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Selecting columns</h2>
			<p>In the previous section, we saw an example of column selection when we looked at the unique values <a id="_idIndexMarker231"/>in the <strong class="source-inline">alert</strong> column; we accessed the column as an attribute of the dataframe. Remember that a column is a <strong class="source-inline">Series</strong> object, so, for example, selecting the <strong class="source-inline">mag</strong> column in the earthquake data gives us the magnitudes of the earthquakes as a <strong class="source-inline">Series</strong> object:</p>
			<p class="source-code">&gt;&gt;&gt; df.<strong class="bold">mag</strong></p>
			<p class="source-code">0       1.35</p>
			<p class="source-code">1       1.29</p>
			<p class="source-code">2       3.42</p>
			<p class="source-code">3       0.44</p>
			<p class="source-code">4       2.16</p>
			<p class="source-code">        ... </p>
			<p class="source-code">9327    0.62</p>
			<p class="source-code">9328    1.00</p>
			<p class="source-code">9329    2.40</p>
			<p class="source-code">9330    1.10</p>
			<p class="source-code">9331    0.66</p>
			<p class="source-code">Name: mag, Length: 9332, dtype: float64</p>
			<p>Pandas provides us with a few ways to select columns. An alternative to using attribute notation to select a column is to access it with a dictionary-like notation: </p>
			<p class="source-code">&gt;&gt;&gt; df<strong class="bold">['mag']</strong></p>
			<p class="source-code">0       1.35</p>
			<p class="source-code">1       1.29</p>
			<p class="source-code">2       3.42</p>
			<p class="source-code">3       0.44</p>
			<p class="source-code">4       2.16</p>
			<p class="source-code">        ... </p>
			<p class="source-code">9327    0.62</p>
			<p class="source-code">9328    1.00</p>
			<p class="source-code">9329    2.40</p>
			<p class="source-code">9330    1.10</p>
			<p class="source-code">9331    0.66</p>
			<p class="source-code">Name: mag, Length: 9332, dtype: float64</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can also select columns using the <strong class="source-inline">get()</strong> method. This has the benefits of not raising an error if the column doesn't exist and allowing us to provide a backup value—the default is <strong class="source-inline">None</strong>. For example, if we call <strong class="source-inline">df.get('event', False)</strong>, it will return <strong class="source-inline">False</strong> since we don't have an <strong class="source-inline">event</strong> column.</p>
			<p>Note that <a id="_idIndexMarker232"/>we aren't limited to selecting one column at a time. By passing a list to the dictionary lookup, we can select many columns, giving us a <strong class="source-inline">DataFrame</strong> object that is a subset of our original dataframe:</p>
			<p class="source-code">&gt;&gt;&gt; df[<strong class="bold">['mag', 'title']</strong>]</p>
			<p>This gives us the full <strong class="source-inline">mag</strong> and <strong class="source-inline">title</strong> columns from the original dataframe:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/Figure_2.17_B16834.jpg" alt="Figure 2.17 – Selecting multiple columns of a dataframe&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.17 – Selecting multiple columns of a dataframe</p>
			<p>String methods <a id="_idIndexMarker233"/>are a very powerful way to select columns. For example, if we wanted to select all the columns that start with <strong class="source-inline">mag</strong>, along with the <strong class="source-inline">title</strong> and <strong class="source-inline">time</strong> columns, we would do the following:</p>
			<p class="source-code">&gt;&gt;&gt; df[</p>
			<p class="source-code">...     ['title', 'time'] </p>
			<p class="source-code">...     + [col for col in df.columns if col.<strong class="bold">startswith</strong>('mag')]</p>
			<p class="source-code">... ]</p>
			<p>We get back a dataframe composed of the four columns that matched our criteria. Notice how the columns were returned in the order we requested, which is not the order they originally appeared in. This means that if we want to reorder our columns, all we have to do is select them in the order we want them to appear:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/Figure_2.18_B16834.jpg" alt="Figure 2.18 – Selecting columns based on names&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.18 – Selecting columns based on names</p>
			<p>Let's break <a id="_idIndexMarker234"/>this example down. We used a list comprehension to go through each of the columns in the dataframe and only keep the ones whose names started with <strong class="source-inline">mag</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; [col for col in df.columns if col.<strong class="bold">startswith</strong>('mag')]</p>
			<p class="source-code">['mag', 'magType']</p>
			<p>Then, we added this result to the other two columns we wanted to keep (<strong class="source-inline">title</strong> and <strong class="source-inline">time</strong>):</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">['title', 'time']</strong> \</p>
			<p class="source-code">... + [col for col in df.columns if col.startswith('mag')]</p>
			<p class="source-code">['title', 'time', 'mag', 'magType']</p>
			<p>Finally, we were able to use this list to run the actual column selection on the dataframe, resulting in the dataframe in <em class="italic">Figure 2.18</em>:</p>
			<p class="source-code">&gt;&gt;&gt; df[</p>
			<p class="source-code">...     <strong class="bold">['title', 'time']</strong> </p>
			<p class="source-code">...     <strong class="bold">+ [col for col in df.columns if col.startswith('mag')]</strong></p>
			<p class="source-code">... ]</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">A complete <a id="_idIndexMarker235"/>list of string methods can be found in the Python 3 documentation at <a href="https://docs.python.org/3/library/stdtypes.html#string-methods">https://docs.python.org/3/library/stdtypes.html#string-methods</a>.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/>Slicing</h2>
			<p>When we want <a id="_idIndexMarker236"/>to extract certain rows (slices) from our dataframe, we use <strong class="bold">slicing</strong>. <strong class="source-inline">DataFrame</strong> slicing works similarly to slicing with other Python objects, such <a id="_idIndexMarker237"/>as lists and tuples, with the first index being inclusive and the last index being exclusive:</p>
			<p class="source-code">&gt;&gt;&gt; df<strong class="bold">[100:103]</strong></p>
			<p>When specifying a slice of <strong class="source-inline">100:103</strong>, we get back rows <strong class="source-inline">100</strong>, <strong class="source-inline">101</strong>, and <strong class="source-inline">102</strong>:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/Figure_2.19_B16834.jpg" alt="Figure 2.19 – Slicing a dataframe to extract specific rows&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.19 – Slicing a dataframe to extract specific rows</p>
			<p>We can combine our row and column <a id="_idIndexMarker238"/>selections by using what is known as <strong class="bold">chaining</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df[['title', 'time']]<strong class="bold">[100:103]</strong></p>
			<p>First, we selected the <strong class="source-inline">title</strong> and <strong class="source-inline">time</strong> columns for all the rows, and then we pulled out rows with indices <strong class="source-inline">100</strong>, <strong class="source-inline">101</strong>, and <strong class="source-inline">102</strong>:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/Figure_2.20_B16834.jpg" alt="Figure 2.20 – Selecting specific rows and columns with chaining&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.20 – Selecting specific rows and columns with chaining</p>
			<p>In the preceding example, we selected the columns and then sliced the rows, but the order doesn't matter:</p>
			<p class="source-code">&gt;&gt;&gt; df[100:103][['title', 'time']].<strong class="bold">equals</strong>(</p>
			<p class="source-code">...     df[['title', 'time']][100:103]</p>
			<p class="source-code">... )</p>
			<p class="source-code"><strong class="bold">True</strong></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Note that we can slice on whatever is in our index; however, it would be difficult to determine the string or date after the last one we want, so with <strong class="source-inline">pandas</strong>, slicing dates and strings is different from integer slicing and is inclusive of both endpoints. Date slicing will work as long as the strings we provide can be parsed into a <strong class="source-inline">datetime</strong> object. In <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>, we'll see some examples of this and also learn how to change what we use as the index, thus making this type of slicing possible.</p>
			<p>If we decide <a id="_idIndexMarker239"/>to use chaining to update the values in our data, we will find <strong class="source-inline">pandas</strong> complaining that we aren't doing so correctly (even if it works). This is to warn us that setting data with a sequential selection may not give us the result we anticipate. (More information can be found at <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy">https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy</a>.)</p>
			<p>Let's trigger this warning to understand it better. We will try to update the entries in the <strong class="source-inline">title</strong> column for a few earthquakes so that they're in lowercase:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">df[110:113]['title'] = df[110:113]['title'].str.lower()</strong></p>
			<p class="source-code">/.../book_env/lib/python3.7/[...]:1: SettingWithCopyWarning:  </p>
			<p class="source-code"><strong class="bold">A value is trying to be set on a copy of a slice from a DataFrame.</strong></p>
			<p class="source-code"><strong class="bold">Try using .loc[row_indexer,col_indexer] = value instead</strong></p>
			<p class="source-code">See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy</p>
			<p class="source-code">  """Entry point for launching an IPython kernel.</p>
			<p>As indicated <a id="_idIndexMarker240"/>by the warning, to be an effective <strong class="source-inline">pandas</strong> user, it's not enough to know selection and slicing—we must also master <strong class="bold">indexing</strong>. Since this is just a warning, our values have been updated, but this may not always be the case:</p>
			<p class="source-code">&gt;&gt;&gt; df[110:113]['title']</p>
			<p class="source-code">110               <strong class="bold">m 1.1 - 35km s of ester, alaska</strong></p>
			<p class="source-code">111    <strong class="bold">m 1.9 - 93km wnw of arctic village, alaska</strong></p>
			<p class="source-code">112      <strong class="bold">m 0.9 - 20km wsw of smith valley, nevada</strong></p>
			<p class="source-code">Name: title, dtype: object</p>
			<p>Now, let's discuss how to use indexing to set values properly.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor052"/>Indexing</h2>
			<p>Pandas indexing operations provide us with a one-method way to select both the rows and the <a id="_idIndexMarker241"/>columns we want. We can use <strong class="source-inline">loc[]</strong> and <strong class="source-inline">iloc[]</strong> to subset our dataframe using label-based or integer-based lookups, respectively. A good way to remember the difference is to think of them as <strong class="bold">loc</strong>ation versus <strong class="bold">i</strong>nteger <strong class="bold">loc</strong>ation. For all indexing methods, we provide the row indexer first and then the column indexer, with a comma separating them:</p>
			<p class="source-code">df.loc[row_indexer, column_indexer]</p>
			<p>Note that by using <strong class="source-inline">loc[]</strong>, as indicated in the warning message, we no longer trigger any warnings from <strong class="source-inline">pandas</strong> for this operation. We also changed the end index from <strong class="source-inline">113</strong> to <strong class="source-inline">112</strong> because <strong class="source-inline">loc[]</strong> is inclusive of endpoints:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">df.loc[110:112, 'title'] = \</strong></p>
			<p class="source-code">...     <strong class="bold">df.loc[110:112, 'title'].str.lower()</strong></p>
			<p class="source-code">&gt;&gt;&gt; df.loc[110:112, 'title']</p>
			<p class="source-code">110               m 1.1 - 35km s of ester, alaska</p>
			<p class="source-code">111    m 1.9 - 93km wnw of arctic village, alaska</p>
			<p class="source-code">112      m 0.9 - 20km wsw of smith valley, nevada</p>
			<p class="source-code">Name: title, dtype: object</p>
			<p>We can <a id="_idIndexMarker242"/>select all the rows (columns) if we use <strong class="source-inline">:</strong> as the row (column) indexer, just like with regular Python slicing. Let's grab all the rows of the <strong class="source-inline">title</strong> column with <strong class="source-inline">loc[]</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df.loc[:,'title']</p>
			<p class="source-code">0                  M 1.4 - 9km NE of Aguanga, CA</p>
			<p class="source-code">1                  M 1.3 - 9km NE of Aguanga, CA</p>
			<p class="source-code">2                  M 3.4 - 8km NE of Aguanga, CA</p>
			<p class="source-code">3                  M 0.4 - 9km NE of Aguanga, CA</p>
			<p class="source-code">4                  M 2.2 - 10km NW of Avenal, CA</p>
			<p class="source-code">                          ...                   </p>
			<p class="source-code">9327        M 0.6 - 9km ENE of Mammoth Lakes, CA</p>
			<p class="source-code">9328                 M 1.0 - 3km W of Julian, CA</p>
			<p class="source-code">9329    M 2.4 - 35km NNE of Hatillo, Puerto Rico</p>
			<p class="source-code">9330               M 1.1 - 9km NE of Aguanga, CA</p>
			<p class="source-code">9331               M 0.7 - 9km NE of Aguanga, CA</p>
			<p class="source-code">Name: title, Length: 9332, dtype: object</p>
			<p>We can select multiple rows and columns at the same time with <strong class="source-inline">loc[]</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df.loc[10:15, ['title', 'mag']]</p>
			<p>This leaves us with rows <strong class="source-inline">10</strong> through <strong class="source-inline">15</strong> for the <strong class="source-inline">title</strong> and <strong class="source-inline">mag</strong> columns only:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/Figure_2.21_B16834.jpg" alt="Figure 2.21 – Selecting specific rows and columns with indexing&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.21 – Selecting specific rows and columns with indexing</p>
			<p>As we <a id="_idIndexMarker243"/>have seen, when using <strong class="source-inline">loc[]</strong>, our end index is inclusive. This isn't the case with <strong class="source-inline">iloc[]</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df.iloc[10:15, [19, 8]]</p>
			<p>Observe how we had to provide a list of integers to select the same columns; these are the column numbers (starting from <strong class="source-inline">0</strong>). Using <strong class="source-inline">iloc[]</strong>, we lost the row at index <strong class="source-inline">15</strong>; this is because the integer slicing that <strong class="source-inline">iloc[]</strong> employs is exclusive of the end index, as with Python slicing syntax:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/Figure_2.22_B16834.jpg" alt="Figure 2.22 – Selecting specific rows and columns by position&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.22 – Selecting specific rows and columns by position</p>
			<p>We aren't limited to using the slicing syntax for the rows, though; columns work as well:</p>
			<p class="source-code">&gt;&gt;&gt; df.iloc[10:15, 6:10]</p>
			<p>By using slicing, we can easily grab adjacent rows and columns:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/Figure_2.23_B16834.jpg" alt="Figure 2.23 – Selecting ranges of adjacent rows and columns by position&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.23 – Selecting ranges of adjacent rows and columns by position</p>
			<p>When using <strong class="source-inline">loc[]</strong>, this slicing can be done on the column names as well. This gives us many ways to achieve the same result:</p>
			<p class="source-code">&gt;&gt;&gt; df.iloc[10:15, 6:10].equals(df.loc[10:14, 'gap':'magType'])</p>
			<p class="source-code">True</p>
			<p>To look <a id="_idIndexMarker244"/>up scalar values, we use <strong class="source-inline">at[]</strong> and <strong class="source-inline">iat[]</strong>, which are faster. Let's select the magnitude (the <strong class="source-inline">mag</strong> column) of the earthquake that was recorded in the row at index <strong class="source-inline">10</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df.at[10, 'mag']</p>
			<p class="source-code">0.5</p>
			<p>The magnitude column has a column index of <strong class="source-inline">8</strong>; therefore, we can also look up the magnitude with <strong class="source-inline">iat[]</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df.iat[10, 8]</p>
			<p class="source-code">0.5</p>
			<p>So far, we have seen how to get subsets of our data using row/column names and ranges, but how do we only take the data that meets some criteria? For this, we need to learn how to filter our data.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>Filtering</h2>
			<p>Pandas gives <a id="_idIndexMarker245"/>us a few options for filtering our data, including <strong class="bold">Boolean masks</strong> and some <a id="_idIndexMarker246"/>special methods. With Boolean masks, we test our data against some value and get a structure of the same shape back, except it is filled with <strong class="source-inline">True</strong>/<strong class="source-inline">False</strong> values; <strong class="source-inline">pandas</strong> can use this to select the appropriate rows/columns for us. There are endless possibilities for creating Boolean masks—all we need is some code that returns one Boolean value for each row. For example, we can see which entries in the <strong class="source-inline">mag</strong> column had a magnitude greater than two:</p>
			<p class="source-code">&gt;&gt;&gt; df.mag &gt; 2</p>
			<p class="source-code">0       False</p>
			<p class="source-code">1       False</p>
			<p class="source-code">2        True</p>
			<p class="source-code">3       False</p>
			<p class="source-code">        ...  </p>
			<p class="source-code">9328    False</p>
			<p class="source-code">9329     True</p>
			<p class="source-code">9330    False</p>
			<p class="source-code">9331    False</p>
			<p class="source-code">Name: mag, Length: 9332, dtype: bool</p>
			<p>While we <a id="_idIndexMarker247"/>can run this on the entire dataframe, it wouldn't be too useful with our earthquake data since we have columns of various data types. However, we can use this strategy to get the subset of the data where the magnitude of the earthquake was greater than or equal to 7.0:</p>
			<p class="source-code">&gt;&gt;&gt; df[<strong class="bold">df.mag &gt;= 7.0</strong>]</p>
			<p>Our resulting dataframe has just two rows:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/Figure_2.24_B16834.jpg" alt="Figure 2.24 – Filtering with Boolean masks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.24 – Filtering with Boolean masks</p>
			<p>We got back a lot of columns we didn't need, though. We could have chained a column selection <a id="_idIndexMarker248"/>to the end of the last code snippet; however, <strong class="source-inline">loc[]</strong> can handle Boolean masks as well:</p>
			<p class="source-code">&gt;&gt;&gt; df.<strong class="bold">loc[</strong></p>
			<p class="source-code">...     <strong class="bold">df.mag &gt;= 7.0</strong>, </p>
			<p class="source-code">...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']</p>
			<p class="source-code">... <strong class="bold">]</strong></p>
			<p>The following dataframe has been filtered so that it only contains relevant columns:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/Figure_2.25_B16834.jpg" alt="Figure 2.25 – Indexing with Boolean masks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.25 – Indexing with Boolean masks</p>
			<p>We aren't limited to just one criterion, either. Let's grab the earthquakes with a red alert and a tsunami. To combine masks, we need to surround each of our conditions with parentheses and <a id="_idIndexMarker249"/>use the <strong class="bold">bitwise AND operator</strong> (<strong class="source-inline">&amp;</strong>) to require <em class="italic">both</em> to be true:</p>
			<p class="source-code">&gt;&gt;&gt; df.loc[</p>
			<p class="source-code">...     <strong class="bold">(df.tsunami == 1) &amp; (df.alert == 'red')</strong>, </p>
			<p class="source-code">...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']</p>
			<p class="source-code">... ]</p>
			<p>There was only a single earthquake in the data that met our criteria:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/Figure_2.26_B16834.jpg" alt="Figure 2.26 – Combining filters with AND&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.26 – Combining filters with AND</p>
			<p>If, instead, we want <em class="italic">at least one</em> of our conditions <a id="_idIndexMarker250"/>to be true, we can use the <strong class="bold">bitwise OR operator</strong> (<strong class="source-inline">|</strong>):</p>
			<p class="source-code">&gt;&gt;&gt; df.loc[</p>
			<p class="source-code">...     (df.tsunami == 1) <strong class="screen-inline">|</strong> (df.alert == 'red'), </p>
			<p class="source-code">...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']</p>
			<p class="source-code">... ]</p>
			<p>Notice that <a id="_idIndexMarker251"/>this filter is much less restrictive since, while both conditions can be true, we only require that one of them is:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/Figure_2.27_B16834.jpg" alt="Figure 2.27 – Combining filters with OR&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.27 – Combining filters with OR</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">When creating Boolean masks, we must use bitwise operators (<strong class="source-inline">&amp;</strong>, <strong class="source-inline">|</strong>, <strong class="source-inline">~</strong>) instead of logical operators (<strong class="source-inline">and</strong>, <strong class="source-inline">or</strong>, <strong class="source-inline">not</strong>). A good way to remember this is that we want a Boolean for each item in the series we are testing rather than a single Boolean. For example, with the earthquake data, if we want to select the rows where the magnitude is greater than 1.5, then we want one Boolean value for each row, indicating whether the row should be selected. In cases where we want a single value for the data, perhaps to summarize it, we can use <strong class="source-inline">any()</strong>/<strong class="source-inline">all()</strong> to condense a Boolean series into a single Boolean value that can be used with logical operators. We will work with the <strong class="source-inline">any()</strong> and <strong class="source-inline">all()</strong> methods in <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>.</p>
			<p>In the previous <a id="_idIndexMarker252"/>two examples, our conditions involved equality; however, we are by no means limited to this. Let's select all the earthquakes in Alaska where we have a non-null value for the <strong class="source-inline">alert</strong> column:</p>
			<p class="source-code">&gt;&gt;&gt; df.loc[</p>
			<p class="source-code">...     (<strong class="bold">df.place.str.contains('Alaska')</strong>) </p>
			<p class="source-code">...     &amp; (<strong class="bold">df.alert.notnull()</strong>), </p>
			<p class="source-code">...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']</p>
			<p class="source-code">... ]</p>
			<p>All the earthquakes in Alaska that have a value for <strong class="source-inline">alert</strong> are <strong class="source-inline">green</strong>, and some were accompanied by tsunamis, with the highest magnitude being 5.1:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/Figure_2.28_B16834.jpg" alt="Figure 2.28 – Creating Boolean masks with non-numeric columns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.28 – Creating Boolean masks with non-numeric columns</p>
			<p>Let's break down how we got this. <strong class="source-inline">Series</strong> objects have some string methods that can be accessed via the <strong class="source-inline">str</strong> attribute. Using this, we can create a Boolean mask of all the rows where the <strong class="source-inline">place</strong> column contained the word <strong class="source-inline">Alaska</strong>:</p>
			<p class="source-code">df.place.<strong class="bold">str.contains('Alaska')</strong></p>
			<p>To get all the <a id="_idIndexMarker253"/>rows where the <strong class="source-inline">alert</strong> column was not null, we used the <strong class="source-inline">Series</strong> object's <strong class="source-inline">notnull()</strong> method (this works for <strong class="source-inline">DataFrame</strong> objects as well) to create a Boolean mask of all the rows where the <strong class="source-inline">alert</strong> column was not null:</p>
			<p class="source-code">df.alert.notnull()</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can <a id="_idIndexMarker254"/>use the <strong class="bold">bitwise negation operator</strong> (<strong class="source-inline">~</strong>), also called <strong class="bold">NOT</strong>, to negate <a id="_idIndexMarker255"/>all the Boolean values, which makes all <strong class="source-inline">True</strong> values <strong class="source-inline">False</strong> and vice versa. So, <strong class="source-inline">df.alert.notnull()</strong> and <strong class="source-inline">~df.alert.isnull()</strong>are equivalent.</p>
			<p>Then, like we did previously, we combine the two conditions with the <strong class="source-inline">&amp;</strong> operator to complete our mask:</p>
			<p class="source-code">(df.place.str.contains('Alaska')) <strong class="bold">&amp;</strong> (df.alert.notnull())</p>
			<p>Note that we aren't limited to checking if each row contains text; we can use regular expressions as well. <strong class="bold">Regular expressions</strong> (often called <em class="italic">regex</em>, for short) are very powerful <a id="_idIndexMarker256"/>because they allow us to define a search pattern rather than the exact content we want to find. This means that we can do things such as find all the words or digits in a string without having to know what all the words or digits are beforehand (or go through one character at a time). To do so, we simply pass in a string preceded by an <strong class="source-inline">r</strong> character outside the quotes; this lets Python know it is a <strong class="bold">raw string</strong>, which means <a id="_idIndexMarker257"/>that we can include backslash (<strong class="source-inline">\</strong>) characters in the string without Python thinking we are trying to escape the character immediately following it (such as when we use <strong class="source-inline">\n</strong> to mean a new line character instead of the letter <strong class="source-inline">n</strong>). This makes it perfect for use with regular expressions. The <strong class="source-inline">re</strong> module in the Python standard library (<a href="https://docs.python.org/3/library/re.html">https://docs.python.org/3/library/re.html</a>) handles regular expression operations; however, <strong class="source-inline">pandas</strong> lets us use regular expressions directly.</p>
			<p>Using a regular expression, let's select all the earthquakes in California that have magnitudes of at least 3.8. We need to select entries in the <strong class="source-inline">place</strong> column that end in <strong class="source-inline">CA</strong> or <strong class="source-inline">California</strong> because the data isn't consistent (we will look at how to fix this in the next section). The <strong class="source-inline">$</strong> character means <em class="italic">end</em> and <strong class="source-inline">'CA$'</strong> gives us entries that end in <strong class="source-inline">CA</strong>, so we can use <strong class="source-inline">'CA|California$'</strong> to get entries that end in either:</p>
			<p class="source-code">&gt;&gt;&gt; df.loc[</p>
			<p class="source-code">...     (df.place.str.contains(<strong class="bold">r'CA|California$'</strong>))</p>
			<p class="source-code">...     &amp; (df.mag &gt; 3.8),         </p>
			<p class="source-code">...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']</p>
			<p class="source-code">... ]</p>
			<p>There were <a id="_idIndexMarker258"/>only two earthquakes in California with magnitudes greater than 3.8 during the time period we are studying:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/Figure_2.29_B16834.jpg" alt="Figure 2.29 – Filtering with regular expressions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.29 – Filtering with regular expressions</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Regular expressions are extremely powerful, but unfortunately, also difficult to get right. It is often helpful to grab some sample lines for parsing and use a website to test them. Note that regular expressions come in many flavors, so be sure to select Python. This website supports Python flavor regular expressions, and also provides a nice cheat sheet on the side: https://regex101.com/.</p>
			<p>What if we want to get all earthquakes with magnitudes between 6.5 and 7.5? We could use two Boolean masks—one to check for magnitudes greater than or equal to 6.5, and another to check for magnitudes less than or equal to 7.5—and then combine them with the <strong class="source-inline">&amp;</strong> operator. Thankfully, <strong class="source-inline">pandas</strong> makes this type of mask much easier to create by providing us with the <strong class="source-inline">between()</strong> method:</p>
			<p class="source-code">&gt;&gt;&gt; df.loc[</p>
			<p class="source-code">...     <strong class="bold">df.mag.between(6.5, 7.5)</strong>, </p>
			<p class="source-code">...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']</p>
			<p class="source-code">... ]</p>
			<p>The result contains all the earthquakes with magnitudes in the range [6.5, 7.5]—it's inclusive of both ends by default, but we can pass in <strong class="source-inline">inclusive=False</strong> to change this:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/Figure_2.30_B16834.jpg" alt="Figure 2.30 – Filtering using a range of values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.30 – Filtering using a range of values</p>
			<p>We can use the <strong class="source-inline">isin()</strong> method to create a Boolean mask for values that match one of a list of values. This means that we don't have to write one mask for each of the values that we could <a id="_idIndexMarker259"/>match and then use <strong class="source-inline">|</strong> to join them. Let's utilize this to filter on the <strong class="source-inline">magType</strong> column, which indicates the measurement technique that was used to quantify the earthquake's magnitude. We will take a look at earthquakes measured with either the <strong class="source-inline">mw</strong> or <strong class="source-inline">mwb</strong> magnitude type:</p>
			<p class="source-code">&gt;&gt;&gt; df.loc[</p>
			<p class="source-code">...     df.magType.isin(['mw', 'mwb']), </p>
			<p class="source-code">...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']</p>
			<p class="source-code">... ]</p>
			<p>We have two earthquakes that were measured with the <strong class="source-inline">mwb</strong> magnitude type and four that were measured with the <strong class="source-inline">mw</strong> magnitude type:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/Figure_2.31_B16834.jpg" alt="Figure 2.31 – Filtering using membership in a list&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.31 – Filtering using membership in a list</p>
			<p>So far, we have been filtering on specific values, but suppose we wanted to see all the data for the lowest-magnitude and highest-magnitude earthquakes. Rather than finding the minimum <a id="_idIndexMarker260"/>and maximum of the <strong class="source-inline">mag</strong> column first and then creating a Boolean mask, we can ask <strong class="source-inline">pandas</strong> to give us the index where these values occur, and easily filter to grab the full rows. We can use <strong class="source-inline">idxmin()</strong> and <strong class="source-inline">idxmax()</strong> for the indices of the minimum and maximum, respectively. Let's grab the row numbers for the lowest-magnitude and highest-magnitude earthquakes:</p>
			<p class="source-code">&gt;&gt;&gt; [df.mag.idxmin(), df.mag.idxmax()]</p>
			<p class="source-code">[2409, 5263]</p>
			<p>We can use these indices to grab the rows themselves:</p>
			<p class="source-code">&gt;&gt;&gt; df.loc[</p>
			<p class="source-code">...     <strong class="bold">[df.mag.idxmin(), df.mag.idxmax()]</strong>, </p>
			<p class="source-code">...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']</p>
			<p class="source-code">... ]</p>
			<p>The minimum magnitude earthquake occurred in Alaska and the highest magnitude earthquake occurred in Indonesia, accompanied by a tsunami. We will discuss the earthquake in Indonesia in <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>, and <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/Figure_2.32_B16834.jpg" alt="Figure 2.32 – Filtering to isolate the rows containing the minimum and maximum of a column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.32 – Filtering to isolate the rows containing the minimum and maximum of a column</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Be advised that the <strong class="source-inline">filter()</strong> method does <a id="_idIndexMarker261"/>not filter the data according to its values, as we <a id="_idIndexMarker262"/>did in this section; rather, it can be used to subset rows or columns based on their names. Examples with <strong class="source-inline">DataFrame</strong> and <strong class="source-inline">Series</strong> objects can be found in the notebook.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/>Adding and removing data</h1>
			<p>In the previous sections, we frequently selected a subset of the columns, but if columns/rows aren't <a id="_idIndexMarker263"/>useful to us, we should just get rid of them. We also frequently selected data based on the value of the <strong class="source-inline">mag</strong> column; however, if we had made <a id="_idIndexMarker264"/>a new column holding the Boolean values for later selection, we would have only needed to calculate the mask once. Very rarely will we get data where we neither want to add nor remove something.</p>
			<p>Before we begin adding and removing data, it's important to understand that while most methods will return a new <strong class="source-inline">DataFrame</strong> object, some will be in-place and change our data. If we write a function where we pass in a dataframe and change it, it will change our original dataframe as well. Should we find ourselves in a situation where we don't want to change the original data, but rather want to return a new copy of the data that has been modified, we must be sure to copy our dataframe before making any changes:</p>
			<p class="source-code">df_to_modify = df.copy()</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">By default, <strong class="source-inline">df.copy()</strong> makes a <strong class="bold">deep copy</strong> of the dataframe, which allows us to make changes <a id="_idIndexMarker265"/>to either the <a id="_idIndexMarker266"/>copy or the original without repercussions. If we pass in <strong class="source-inline">deep=False</strong>, we can obtain a <strong class="bold">shallow copy</strong>—changes to the shallow copy affect the original and vice versa. We will almost always want the deep copy, since we can change it without affecting the original. More information can be found in the documentation at <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html</a>. </p>
			<p>Now, let's <a id="_idIndexMarker267"/>turn to the final notebook, <strong class="source-inline">6-adding_and_removing_data.ipynb</strong>, and get set up for the remainder of this chapter. We will <a id="_idIndexMarker268"/>once again be working with the earthquake data, but this time, we will only read in a subset of the columns:</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.read_csv(</p>
			<p class="source-code">...     'data/earthquakes.csv', </p>
			<p class="source-code">...     usecols=[</p>
			<p class="source-code">...         'time', 'title', 'place', 'magType', </p>
			<p class="source-code">...         'mag', 'alert', 'tsunami'</p>
			<p class="source-code">...     ]</p>
			<p class="source-code">... )</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor055"/>Creating new data</h2>
			<p>Creating new columns can be achieved in the same fashion as variable assignment. For example, we can <a id="_idIndexMarker269"/>create a column to indicate the source of our data; since all our data came from the same source, we can take advantage of <strong class="bold">broadcasting</strong> to set every row of this column to the same value:</p>
			<p class="source-code">&gt;&gt;&gt; df['source'] = 'USGS API'</p>
			<p class="source-code">&gt;&gt;&gt; df.head()</p>
			<p>The new column is created to the right of the original columns, with a value of <strong class="source-inline">USGS API</strong> for every row:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/Figure_2.33_B16834.jpg" alt="Figure 2.33 – Adding a new column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.33 – Adding a new column</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We cannot create the column with attribute notation (<strong class="source-inline">df.source</strong>) because the dataframe doesn't have that attribute yet, so we must use dictionary notation (<strong class="source-inline">df['source']</strong>).</p>
			<p>We aren't limited to broadcasting one value to the entire column; we can have the column hold <a id="_idIndexMarker270"/>the result of Boolean logic or a mathematical equation. For example, if we had data on distance and time, we could create a speed column that is the result of dividing the distance column by the time column. With our earthquake data, let's create a column that tells us whether the earthquake's magnitude was negative:</p>
			<p class="source-code">&gt;&gt;&gt; df['mag_negative'] = df.mag &lt; 0</p>
			<p class="source-code">&gt;&gt;&gt; df.head()</p>
			<p>Note that the new column has been added to the right:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/Figure_2.34_B16834.jpg" alt="Figure 2.34 – Storing a Boolean mask in a new column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.34 – Storing a Boolean mask in a new column</p>
			<p>In the previous section, we saw that the <strong class="source-inline">place</strong> column has some data consistency issues—we have multiple names for the same entity. In some cases, earthquakes occurring in California are marked as <strong class="source-inline">CA</strong> and as <strong class="source-inline">California</strong> in others. Needless to say, this is confusing and can easily cause issues for us if we don't carefully inspect our data beforehand. For example, by just selecting <strong class="source-inline">CA</strong>, we miss out on 124 earthquakes marked as <strong class="source-inline">California</strong>. This isn't the <a id="_idIndexMarker271"/>only place with an issue either (<strong class="source-inline">Nevada</strong> and <strong class="source-inline">NV</strong> are also both present). By using a regular expression to extract everything in the <strong class="source-inline">place</strong> column after the comma, we can see some of the issues firsthand:</p>
			<p class="source-code">&gt;&gt;&gt; df.place.str.extract(r', (.*$)')[0].sort_values().unique()</p>
			<p class="source-code">array(['Afghanistan', 'Alaska', 'Argentina', 'Arizona',</p>
			<p class="source-code">       'Arkansas', 'Australia', 'Azerbaijan', <strong class="bold">'B.C., MX'</strong>,</p>
			<p class="source-code">       'Barbuda', 'Bolivia', ..., <strong class="bold">'CA'</strong>, <strong class="bold">'California'</strong>, 'Canada',</p>
			<p class="source-code">       'Chile', ..., 'East Timor', <strong class="bold">'Ecuador'</strong>, <strong class="bold">'Ecuador region'</strong>,</p>
			<p class="source-code">       ..., <strong class="bold">'Mexico'</strong>, 'Missouri', 'Montana', <strong class="bold">'NV'</strong>, <strong class="bold">'Nevada'</strong>, </p>
			<p class="source-code">       ..., 'Yemen', <strong class="bold">nan</strong>], dtype=object)</p>
			<p>If we want to treat countries and anything near them as a single entity, we have some additional work to do (see <strong class="source-inline">Ecuador</strong> and <strong class="source-inline">Ecuador region</strong>). In addition, our naive attempt at parsing the location by looking at the information after the comma appears to have failed; this is because, in some cases, we don't have a comma. We will need to change our approach to parsing.</p>
			<p>This is an <strong class="bold">entity recognition problem</strong>, and it's not trivial to solve. With a relatively small list <a id="_idIndexMarker272"/>of unique values (which we can view with <strong class="source-inline">df.place.unique()</strong>), we can simply look through and infer how to properly match up these names. Then, we can use the <strong class="source-inline">replace()</strong> method to replace patterns in the <strong class="source-inline">place</strong> column as we see fit:</p>
			<p class="source-code">&gt;&gt;&gt; df['parsed_place'] = df.place.str.replace(</p>
			<p class="source-code">...     r'.* of ', '', regex=True # remove &lt;x&gt; of &lt;x&gt; </p>
			<p class="source-code">... ).str.replace(</p>
			<p class="source-code">...     'the ', '' # remove "the "</p>
			<p class="source-code">... ).str.replace(</p>
			<p class="source-code">...     r'CA$', 'California', regex=True # fix California</p>
			<p class="source-code">... ).str.replace(</p>
			<p class="source-code">...     r'NV$', 'Nevada', regex=True # fix Nevada</p>
			<p class="source-code">... ).str.replace(</p>
			<p class="source-code">...     r'MX$', 'Mexico', regex=True # fix Mexico</p>
			<p class="source-code">... ).str.replace(</p>
			<p class="source-code">...     r' region$', '', regex=True # fix " region" endings</p>
			<p class="source-code">... ).str.replace(</p>
			<p class="source-code">...     'northern ', '' # remove "northern "</p>
			<p class="source-code">... ).str.replace(</p>
			<p class="source-code">...     'Fiji Islands', 'Fiji' # line up the Fiji places</p>
			<p class="source-code">... ).str.replace( # remove anything else extraneous from start </p>
			<p class="source-code">...     r'^.*, ', '', regex=True </p>
			<p class="source-code">... ).str.strip() # remove any extra spaces</p>
			<p>Now, we can <a id="_idIndexMarker273"/>check the parsed places we are left with. Notice that there is arguably still more to fix here with <strong class="source-inline">South Georgia and South Sandwich Islands</strong> and <strong class="source-inline">South Sandwich Islands</strong>. We could address this with another call to <strong class="source-inline">replace()</strong>; however, this goes to show that entity recognition can be quite challenging:</p>
			<p class="source-code">&gt;&gt;&gt; df.parsed_place.sort_values().unique()</p>
			<p class="source-code">array([..., <strong class="bold">'California'</strong>, 'Canada', 'Carlsberg Ridge', ...,</p>
			<p class="source-code">       'Dominican Republic', 'East Timor', <strong class="bold">'Ecuador'</strong>,</p>
			<p class="source-code">       'El Salvador', 'Fiji', 'Greece', ...,</p>
			<p class="source-code">       <strong class="bold">'Mexico'</strong>, 'Mid-Indian Ridge', 'Missouri', 'Montana',</p>
			<p class="source-code">       <strong class="bold">'Nevada'</strong>, 'New Caledonia', ...,</p>
			<p class="source-code">       <strong class="bold">'South Georgia and South Sandwich Islands'</strong>, </p>
			<p class="source-code">       <strong class="bold">'South Sandwich Islands'</strong>, ..., 'Yemen'], dtype=object)</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In practice, entity <a id="_idIndexMarker274"/>recognition can be an extremely difficult problem, where we may look to employ <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) algorithms to help us. While this is well beyond the scope of this book, more information can be found at https://www.kdnuggets.com/2018/12/introduction-named-entity-recognition.html.</p>
			<p>Pandas also provides us with <a id="_idIndexMarker275"/>a way to make many new columns at once in one method call. With the <strong class="source-inline">assign()</strong> method, the arguments are the names of the columns we want to create (or overwrite), and the values are the data for the columns. Let's create two new columns; one will tell us if the earthquake happened in California, and the other will tell us if it happened in Alaska. Rather than just show the first five entries (which are all in California), we will use <strong class="source-inline">sample()</strong> to randomly select five rows:</p>
			<p class="source-code">&gt;&gt;&gt; df.assign(</p>
			<p class="source-code">...     in_ca=df.parsed_place.str.endswith('California'), </p>
			<p class="source-code">...     in_alaska=df.parsed_place.str.endswith('Alaska')</p>
			<p class="source-code">... ).sample(5, random_state=0)</p>
			<p>Note that <strong class="source-inline">assign()</strong> doesn't change our original dataframe; instead, it returns a new <strong class="source-inline">DataFrame</strong> object with these columns added. If we want to replace our original dataframe with this, we just use variable assignment to store the result of <strong class="source-inline">assign()</strong> in <strong class="source-inline">df</strong> (for example, <strong class="source-inline">df = df.assign(...)</strong>):</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/Figure_2.35_B16834.jpg" alt="Figure 2.35 – Creating multiple new columns at once&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.35 – Creating multiple new columns at once</p>
			<p>The <strong class="source-inline">assign()</strong> method <a id="_idIndexMarker276"/>also accepts <strong class="bold">lambda functions</strong> (anonymous functions usually defined in one line and for single use); <strong class="source-inline">assign()</strong> will pass the dataframe into the <strong class="source-inline">lambda</strong> function as <strong class="source-inline">x</strong>, and we can work from there. This makes <a id="_idIndexMarker277"/>it possible for us to use the columns we are creating in <strong class="source-inline">assign()</strong> to calculate others. For example, let's once again create the <strong class="source-inline">in_ca</strong> and <strong class="source-inline">in_alaska</strong> columns, but this time also create a new column, <strong class="source-inline">neither</strong>, which is <strong class="source-inline">True</strong> if both <strong class="source-inline">in_ca</strong> and <strong class="source-inline">in_alaska</strong> are <strong class="source-inline">False</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df.assign(</p>
			<p class="source-code">...     in_ca=df.parsed_place == 'California', </p>
			<p class="source-code">...     in_alaska=df.parsed_place == 'Alaska',</p>
			<p class="source-code">...     <strong class="bold">neither=lambda x: ~x.in_ca &amp; ~x.in_alaska</strong></p>
			<p class="source-code">... ).sample(5, random_state=0)</p>
			<p>Remember that <strong class="source-inline">~</strong> is the bitwise negation operator, so this allows us to create a column with the result of <strong class="source-inline">NOT in_ca AND NOT in_alaska</strong> per row:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/Figure_2.36_B16834.jpg" alt="Figure 2.36 – Creating multiple new columns at once with lambda functions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.36 – Creating multiple new columns at once with lambda functions</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">When working with <strong class="source-inline">pandas</strong>, it's crucial to get comfortable with <strong class="source-inline">lambda</strong> functions, as they can be used with much of the functionality available and will dramatically improve the quality and readability of the code. Throughout this book, we will see various places where <strong class="source-inline">lambda</strong> functions can be used. </p>
			<p>Now that we <a id="_idIndexMarker278"/>have seen how to add new columns, let's take a look at adding new rows. Say we were working with two separate dataframes; one with earthquakes accompanied by tsunamis and the other with earthquakes without tsunamis:</p>
			<p class="source-code">&gt;&gt;&gt; tsunami = df[df.tsunami == 1]</p>
			<p class="source-code">&gt;&gt;&gt; no_tsunami = df[df.tsunami == 0]</p>
			<p class="source-code">&gt;&gt;&gt; tsunami.shape, no_tsunami.shape</p>
			<p class="source-code">((<strong class="bold">61</strong>, 10), (<strong class="bold">9271</strong>, 10))</p>
			<p>If we wanted to look at earthquakes as a whole, we would want to concatenate the dataframes into a single one. To append rows to the bottom of our dataframe, we can either use <strong class="source-inline">pd.concat()</strong> or the <strong class="source-inline">append()</strong> method of the dataframe itself. The <strong class="source-inline">concat()</strong> function allows us to specify the axis that the operation will be performed along—<strong class="source-inline">0</strong> for appending rows to the bottom of the dataframe, and <strong class="source-inline">1</strong> for appending to the right of the last column with respect to the leftmost <strong class="source-inline">pandas</strong> object in the concatenation list. Let's use <strong class="source-inline">pd.concat()</strong> with the default <strong class="source-inline">axis</strong> of <strong class="source-inline">0</strong> for rows:</p>
			<p class="source-code">&gt;&gt;&gt; pd.concat([tsunami, no_tsunami]).shape</p>
			<p class="source-code">(<strong class="bold">9332</strong>, 10) # 61 rows + 9271 rows</p>
			<p>Note that the previous result is equivalent to running the <strong class="source-inline">append()</strong> method on the dataframe. This still returns a new <strong class="source-inline">DataFrame</strong> object, but it saves us from having to remember which axis is which, since <strong class="source-inline">append()</strong> is actually a wrapper around the <strong class="source-inline">concat()</strong> function:</p>
			<p class="source-code">&gt;&gt;&gt; tsunami.append(no_tsunami).shape</p>
			<p class="source-code">(9332, 10) # 61 rows + 9271 rows</p>
			<p>So far, we have <a id="_idIndexMarker279"/>been working with a subset of the columns from the CSV file, but suppose that we now want to work with some of the columns we ignored when we read in the data. Since we have added new columns in this notebook, we won't want to read in the file and perform those operations again. Instead, we will concatenate along the columns (<strong class="source-inline">axis=1</strong>) to add back what we are missing:</p>
			<p class="source-code">&gt;&gt;&gt; additional_columns = pd.read_csv(</p>
			<p class="source-code">...     'data/earthquakes.csv', <strong class="bold">usecols=['tz', 'felt', 'ids']</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; pd.concat([df.head(2), additional_columns.head(2)], <strong class="bold">axis=1</strong>)</p>
			<p>Since the indices of the dataframes align, the additional columns are placed to the right of our original columns:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/Figure_2.37_B16834.jpg" alt="Figure 2.37 – Concatenating columns with matching indices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.37 – Concatenating columns with matching indices</p>
			<p>The <strong class="source-inline">concat()</strong> function uses the index to determine how to concatenate the values. If they don't align, this will generate additional rows because <strong class="source-inline">pandas</strong> won't know how to align them. Say we <a id="_idIndexMarker280"/>forgot that our original dataframe had the row numbers as the index, and we read in the additional columns by setting the <strong class="source-inline">time</strong> column as the index:</p>
			<p class="source-code">&gt;&gt;&gt; additional_columns = pd.read_csv(</p>
			<p class="source-code">...     'data/earthquakes.csv',</p>
			<p class="source-code">...     usecols=['tz', 'felt', 'ids', 'time'], </p>
			<p class="source-code">...     <strong class="bold">index_col='time'</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; pd.concat([df.head(2), additional_columns.head(2)], axis=1)</p>
			<p>Despite the additional columns containing data for the first two rows, <strong class="source-inline">pandas</strong> creates a new row for them because the index doesn't match. In <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>, we will see how to reset the index and set the index, both of which could resolve this issue:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/Figure_2.38_B16834.jpg" alt="Figure 2.38 – Concatenating columns with mismatching indices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.38 – Concatenating columns with mismatching indices</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>, we will discuss merging, which will also handle some of these issues when we're augmenting the columns in the dataframe. Often, we will use <strong class="source-inline">concat()</strong> or <strong class="source-inline">append()</strong> to add rows, but <strong class="source-inline">merge()</strong> or <strong class="source-inline">join()</strong> to add columns. </p>
			<p>Say we want to concatenate the <strong class="source-inline">tsunami</strong> and <strong class="source-inline">no_tsunami</strong> dataframes, but the <strong class="source-inline">no_tsunami</strong> dataframe has an additional column (suppose we added a new column to it called <strong class="source-inline">type</strong>). The <strong class="source-inline">join</strong> parameter <a id="_idIndexMarker281"/>specifies how to handle any overlap in column names (when appending to the bottom) or in row names (when concatenating to the right). By default, this is <strong class="source-inline">outer</strong>, so we keep everything; however, if we use <strong class="source-inline">inner</strong>, we will only keep what they have in common:</p>
			<p class="source-code">&gt;&gt;&gt; pd.concat(</p>
			<p class="source-code">...     [</p>
			<p class="source-code">...         tsunami.head(2),</p>
			<p class="source-code">...         no_tsunami.head(2).assign(type='earthquake')</p>
			<p class="source-code">...     ], </p>
			<p class="source-code">...     <strong class="bold">join='inner'</strong></p>
			<p class="source-code">... )</p>
			<p>Notice that the <strong class="source-inline">type</strong> column from the <strong class="source-inline">no_tsunami</strong> dataframe doesn't show up because it wasn't present in the <strong class="source-inline">tsunami</strong> dataframe. Take a look at the index, though; these were the row numbers from the original dataframe before we divided it into <strong class="source-inline">tsunami</strong> and <strong class="source-inline">no_tsunami</strong>:</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/Figure_2.39_B16834.jpg" alt="Figure 2.39 – Appending rows and keeping only shared columns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.39 – Appending rows and keeping only shared columns</p>
			<p>If the index is not meaningful, we can also pass in <strong class="source-inline">ignore_index</strong> to get sequential values in the index:</p>
			<p class="source-code">&gt;&gt;&gt; pd.concat(</p>
			<p class="source-code">...     [</p>
			<p class="source-code">...         tsunami.head(2), </p>
			<p class="source-code">...         no_tsunami.head(2).assign(type='earthquake')</p>
			<p class="source-code">...     ],</p>
			<p class="source-code">...     join='inner', <strong class="bold">ignore_index=True</strong></p>
			<p class="source-code">... )</p>
			<p>The index is <a id="_idIndexMarker282"/>now sequential, and the row numbers no longer match the original dataframe:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/Figure_2.40_B16834.jpg" alt="Figure 2.40 – Appending rows and resetting the index&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.40 – Appending rows and resetting the index</p>
			<p>Be sure to consult the <strong class="source-inline">pandas</strong> documentation for more information on the <strong class="source-inline">concat()</strong> function and other operations for combining data, which we will discuss in <em class="italic">Chapter 4</em>, <em class="italic">Aggregating Pandas DataFrames</em>: http://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#concatenating-objects.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>Deleting unwanted data</h2>
			<p>After adding <a id="_idIndexMarker283"/>that data to our dataframe, we can see the need to delete unwanted data. We need a way to undo our mistakes and get rid of data that we aren't going to use. Like adding data, we can use dictionary syntax to delete unwanted columns, just as we would when removing keys from a dictionary. Both <strong class="source-inline">del df['&lt;column_name&gt;']</strong> and <strong class="source-inline">df.pop('&lt;column_name&gt;')</strong> will work, provided that there is indeed a column with that name; otherwise, we will get a <strong class="source-inline">KeyError</strong>. The difference here is that while <strong class="source-inline">del</strong> removes it right away, <strong class="source-inline">pop()</strong> will return the column that we are removing. Remember that both of these operations will change our original dataframe, so use them with care.</p>
			<p>Let's use <a id="_idIndexMarker284"/>dictionary notation to delete the <strong class="source-inline">source</strong> column. Notice that it no longer appears in the result of <strong class="source-inline">df.columns</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; del df['source']</p>
			<p class="source-code">&gt;&gt;&gt; df.columns</p>
			<p class="source-code">Index(['alert', 'mag', 'magType', 'place', 'time', 'title', </p>
			<p class="source-code">       'tsunami', 'mag_negative', 'parsed_place'],</p>
			<p class="source-code">      dtype='object')</p>
			<p>Note that if we aren't sure whether the column exists, we should put our column deletion code in a <strong class="source-inline">try...except</strong> block:</p>
			<p class="source-code">try:</p>
			<p class="source-code">    del df['source']</p>
			<p class="source-code">except KeyError:</p>
			<p class="source-code">    pass # handle the error here</p>
			<p>Earlier, we created the <strong class="source-inline">mag_negative</strong> column for filtering our dataframe; however, we no longer want this column as part of our dataframe. We can use <strong class="source-inline">pop()</strong> to grab the series for the <strong class="source-inline">mag_negative</strong> column, which we can use as a Boolean mask later without having it in our dataframe:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">mag_negative = df.pop('mag_negative')</strong></p>
			<p class="source-code">&gt;&gt;&gt; df.columns</p>
			<p class="source-code">Index(['alert', 'mag', 'magType', 'place', 'time', 'title', </p>
			<p class="source-code">       'tsunami', 'parsed_place'],</p>
			<p class="source-code">      dtype='object')</p>
			<p>We now have a Boolean mask in the <strong class="source-inline">mag_negative</strong> variable that used to be a column in <strong class="source-inline">df</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">mag_negative.value_counts()</strong></p>
			<p class="source-code">False    8841</p>
			<p class="source-code">True      491</p>
			<p class="source-code">Name: mag_negative, dtype: int64</p>
			<p>Since we <a id="_idIndexMarker285"/>used <strong class="source-inline">pop()</strong> to remove the <strong class="source-inline">mag_negative</strong> series rather than deleting it, we can still use it to filter our dataframe:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">df[mag_negative]</strong>.head()</p>
			<p>This leaves us with the earthquakes that had negative magnitudes. Since we also called <strong class="source-inline">head()</strong>, we get back the first five such earthquakes:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/Figure_2.41_B16834.jpg" alt="Figure 2.41 – Using a popped column as a Boolean mask&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.41 – Using a popped column as a Boolean mask</p>
			<p><strong class="source-inline">DataFrame</strong> objects have a <strong class="source-inline">drop()</strong> method for removing multiple rows or columns either in-place (overwriting the original dataframe without having to reassign it) or returning a new <strong class="source-inline">DataFrame</strong> object. To remove rows, we pass the list of indices. Let's remove the first two rows:</p>
			<p class="source-code">&gt;&gt;&gt; df.drop([0, 1]).head(2)</p>
			<p>Notice that the index starts at <strong class="source-inline">2</strong> because we dropped <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/Figure_2.42_B16834.jpg" alt="Figure 2.42 – Dropping specific rows&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.42 – Dropping specific rows</p>
			<p>By default, <strong class="source-inline">drop()</strong> assumes that we want to delete rows (<strong class="source-inline">axis=0</strong>). If we want to drop columns, we can <a id="_idIndexMarker286"/>either pass <strong class="source-inline">axis=1</strong> or specify our list of column names using the <strong class="source-inline">columns</strong> argument. Let's delete some more columns:</p>
			<p class="source-code">&gt;&gt;&gt; cols_to_drop = [</p>
			<p class="source-code">...     col for col in df.columns</p>
			<p class="source-code">...     if col not in [</p>
			<p class="source-code">...         'alert', 'mag', 'title', 'time', 'tsunami'</p>
			<p class="source-code">...     ]</p>
			<p class="source-code">... ]</p>
			<p class="source-code">&gt;&gt;&gt; df.drop(<strong class="bold">columns=cols_to_drop</strong>).head()</p>
			<p>This drops all the columns that aren't in the list we wanted to keep:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/Figure_2.43_B16834.jpg" alt="Figure 2.43 – Dropping specific columns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.43 – Dropping specific columns</p>
			<p>Whether we decide to pass <strong class="source-inline">axis=1</strong> to <strong class="source-inline">drop()</strong> or use the <strong class="source-inline">columns</strong> argument, our result will be equivalent:</p>
			<p class="source-code">&gt;&gt;&gt; df.drop(columns=cols_to_drop).equals(</p>
			<p class="source-code">...     df.drop(cols_to_drop, axis=1)</p>
			<p class="source-code">... )</p>
			<p class="source-code">True</p>
			<p>By default, <strong class="source-inline">drop()</strong> will return a new <strong class="source-inline">DataFrame</strong> object; however, if we really want to remove the data from our original dataframe, we can pass in <strong class="source-inline">inplace=True</strong>, which will save us from having to reassign the result back to our dataframe. The result is the same as in <em class="italic">Figure 2.43</em>:</p>
			<p class="source-code">&gt;&gt;&gt; df.drop(columns=cols_to_drop, <strong class="bold">inplace=True</strong>)</p>
			<p class="source-code">&gt;&gt;&gt; df.head()</p>
			<p>Always be <a id="_idIndexMarker287"/>careful with in-place operations. In some cases, it may be possible to undo them; however, in others, it may require starting over from the beginning and recreating the dataframe.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/>Summary</h1>
			<p>In this chapter, we learned how to use <strong class="source-inline">pandas</strong> for the data collection portion of data analysis and to describe our data with statistics, which will be helpful when we get to the drawing conclusions phase. We learned the main data structures of the <strong class="source-inline">pandas</strong> library, along with some of the operations we can perform on them. Next, we learned how to create <strong class="source-inline">DataFrame</strong> objects from a variety of sources, including flat files and API requests. Using earthquake data, we discussed how to summarize our data and calculate statistics from it. Subsequently, we addressed how to take subsets of data via selection, slicing, indexing, and filtering. Finally, we practiced adding and removing both columns and rows from our dataframe.</p>
			<p>These tasks also form the backbone of our <strong class="source-inline">pandas</strong> workflow and the foundation for the new topics we will cover in the next few chapters on data wrangling, aggregation, and data visualization. Be sure to complete the exercises provided in the next section before moving on.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/>Exercises</h1>
			<p>Using the <strong class="source-inline">data/parsed.csv</strong> file and the material from this chapter, complete the following exercises to practice your <strong class="source-inline">pandas</strong> skills:</p>
			<ol>
				<li>Find the 95<span class="superscript">th</span> percentile of earthquake magnitude in Japan using the <strong class="source-inline">mb</strong> magnitude type.</li>
				<li>Find the percentage of earthquakes in Indonesia that were coupled with tsunamis.</li>
				<li>Calculate summary statistics for earthquakes in Nevada.</li>
				<li>Add a column indicating whether the earthquake happened in a country or US state that is on the Ring of Fire. Use Alaska, Antarctica (look for Antarctic), Bolivia, California, Canada, Chile, Costa Rica, Ecuador, Fiji, Guatemala, Indonesia, Japan, Kermadec Islands, Mexico (be careful not to select New Mexico), New Zealand, Peru, Philippines, Russia, Taiwan, Tonga, and Washington.</li>
				<li>Calculate the number of earthquakes in the Ring of Fire locations and the number outside of them.</li>
				<li>Find the tsunami count along the Ring of Fire.</li>
			</ol>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor059"/>Further reading</h1>
			<p>Those with an R and/or SQL background may find it helpful to see how the <strong class="source-inline">pandas </strong>syntax compares:</p>
			<ul>
				<li><em class="italic">Comparison with R / R Libraries</em>: https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html</li>
				<li><em class="italic">Comparison with SQL</em>: https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html</li>
				<li><em class="italic">SQL Queries</em>: https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html</li>
			</ul>
			<p>The following are some resources on working with serialized data:</p>
			<ul>
				<li><em class="italic">Pickle in Python: Object Serialization</em>: https://www.datacamp.com/community/tutorials/pickle-python-tutorial</li>
				<li><em class="italic">Read RData/RDS files into pandas.DataFrame objects (pyreader)</em>: https://github.com/ofajardo/pyreadr</li>
			</ul>
			<p>Additional resources for working with APIs are as follows:</p>
			<ul>
				<li><em class="italic">Documentation for the requests package</em>: https://requests.readthedocs.io/en/master/</li>
				<li><em class="italic">HTTP Methods</em>: https://restfulapi.net/http-methods/</li>
				<li><em class="italic">HTTP Status Codes</em>: https://restfulapi.net/http-status-codes/</li>
			</ul>
			<p>To learn more about regular expressions, consult the following resources:</p>
			<ul>
				<li><em class="italic">Mastering Python Regular Expressions by Félix López, Víctor Romero</em>: https://www.packtpub.com/application-development/mastering-python-regular-expressions</li>
				<li><em class="italic">Regular Expression Tutorial — Learn How to Use Regular Expressions</em>: https://www.regular-expressions.info/tutorial.html</li>
			</ul>
		</div>
	</body></html>