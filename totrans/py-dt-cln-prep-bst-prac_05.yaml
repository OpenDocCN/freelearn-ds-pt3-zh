- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Transformation – Merging and Concatenating
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding how to transform and manipulate data is crucial for unlocking
    valuable insights. Techniques such as joining, merging, and appending allow us
    to blend information from various sources and organize and analyze subsets of
    data. In this chapter, we’ll learn how to merge multiple datasets into a single
    dataset and explore the various techniques that we can use. We’ll understand how
    to avoid duplicate values while merging datasets and some tricks to improve the
    process of merging datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Joining datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling duplicates when merging datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance tricks for merging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concatenating DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code for the chapter at the following link: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter05](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter05).'
  prefs: []
  type: TYPE_NORMAL
- en: Each section is followed by a script with a similar naming convention, so feel
    free to execute the scripts and/or follow along by reading the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Joining datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data analysis projects, it is common to encounter data that is spread across
    multiple sources or datasets. Each dataset may contain different pieces of information
    related to a common entity or subject. **Data merging**, also known as data joining
    or data concatenation, is the process of combining these separate datasets into
    a single cohesive dataset. In data analysis projects, it’s common to encounter
    situations where information about a particular subject or entity is spread across
    multiple datasets. For instance, imagine you’re analyzing customer data for a
    retail business. You might have one dataset containing customer demographics,
    such as names, ages, and addresses, and another dataset with their purchase history,
    such as transaction dates, items bought, and total spending. Each of these datasets
    provides valuable insights but, individually, they don’t give a complete picture
    of customer behavior. To gain a comprehensive understanding, you need to combine
    these datasets. By merging the customer demographics with their purchase history
    based on a common identifier, such as a customer ID, you create a single dataset
    that allows for richer analysis. For example, you could identify patterns such
    as which age groups are buying specific products or how spending habits vary by
    location.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the correct merge strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing the correct join type is crucial as it determines which rows from the
    input DataFrames are included in the joined output. Python’s pandas library provides
    several join types, each with different behaviors. Let’s introduce the use case
    example we are going to work on in this chapter and then expand on the different
    types of joins.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, our use case involves employee data and project assignments
    for a company managing its workforce and projects. You can execute the following
    script to see the DataFrames in more detail: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/1.use_case.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/1.use_case.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `employee_data` DataFrame represents employee details, such as their names
    and departments, as presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `project_data` DataFrame contains information about project assignments,
    including the project names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the following sections, we will discuss the different DataFrame merging options,
    starting with the inner join.
  prefs: []
  type: TYPE_NORMAL
- en: Inner merge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The inner merge returns only the rows that have matching values in both DataFrames
    for the specified join columns. It’s very important to note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Rows with non-matching keys in either DataFrame will be excluded from the merged
    output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rows with missing values in the key columns will be excluded from the merged
    result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result of an inner merge is presented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Inner merge](img/B19801_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Inner merge
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at how we can achieve the preceding result using the pandas
    `merge` function, using the example presented in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As we see in the preceding code snippet, the `pd.merge()` function is used
    to merge the two DataFrames. The `on=''employee_id''` argument specifies that
    the `employee_id` column should be used as the key on which to join the DataFrames.
    The `how=''inner''` argument specifies that an inner join should be performed.
    This type of join returns only the rows that have matching values in both DataFrames,
    which, in this case, are the rows where `employee_id` matches in both `employee_data`
    and `project_data`. In the following table, you can see the output of the inner
    join of the two DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This approach ensures that the data from both DataFrames is combined based on
    a **common key**, with rows included only when there is a match across both DataFrames,
    adhering to the principles of an inner join.
  prefs: []
  type: TYPE_NORMAL
- en: 'If this is still not clear, in the following list, we present specific examples
    from the data world where an inner merge is crucial:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Match tables**: Inner joins are ideal when you need to match data from different
    tables. For example, if you have a table of employees and another table of department
    names, you can use an inner join to match each employee with their respective
    department.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data filtering**: Inner joins can act as a filter to exclude rows that do
    not have corresponding entries in both tables. This is useful in scenarios where
    you only want to consider records that have complete data across multiple tables.
    For instance, matching customer orders with product details only where both records
    exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency in query execution**: Since inner joins only return rows with
    matching values in both tables, they can be more efficient in terms of query execution
    time compared to outer joins, which need to check for and handle non-matching
    entries as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducing data duplication**: Inner joins help in reducing data duplication
    by only returning matched rows, thus ensuring that the data in the result set
    is relevant and not redundant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplifying complex queries**: When dealing with multiple tables, inner joins
    can be used to simplify queries by reducing the number of rows to be examined
    and processed in subsequent query operations. This is particularly useful in complex
    database schemas where multiple tables are interrelated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving from an inner join to an outer join expands the scope of the merged data,
    incorporating all available rows from both datasets, even if they don’t have corresponding
    matches.
  prefs: []
  type: TYPE_NORMAL
- en: Outer merge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The outer merge (also known as full outer join) returns all the rows from both
    DataFrames, combining the matching rows as well as the non-matching rows. The
    full outer join ensures that no data is lost from either DataFrame, but it can
    introduce NaN values where there are unmatched rows in either DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of an outer merge is presented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Outer merge](img/B19801_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Outer merge
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at how we can achieve the preceding result using the pandas
    `merge` function, using the example presented in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As we see in the preceding code snippet, the `pd.merge()` function is used
    to merge the two DataFrames. The `on=''employee_id''` argument specifies that
    the `employee_id` column should be used as the key on which to merge the DataFrames.
    The `how=''outer''` argument specifies that a full outer join should be performed.
    This type of join returns all rows from both DataFrames, filling in `NaN` where
    there is no match. In the following table, you can see the output of the outer
    join of the two DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This approach ensures that the data from both DataFrames is combined, allowing
    for a comprehensive view of all available data, even if some of it is incomplete
    due to mismatches between the DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following list, we present specific examples from the data world where
    an outer merge is crucial:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Including optional data**: Outer joins are ideal when you want to include
    rows that have optional data in another table. For instance, if you have a table
    of users and a separate table of addresses, not all users might have an address.
    An outer join allows you to list all users and show addresses for those who have
    them, without excluding users without addresses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data integrity and completeness**: In scenarios where you need a comprehensive
    dataset that includes records from both tables, regardless of whether there’s
    a matching record in the joined table or not, outer joins are essential. This
    ensures that you have a complete view of the data, which is particularly important
    in reports that need to show all entities, such as a report listing all customers
    and their purchases, including those who have not made any purchases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mismatched data analysis**: Outer joins can be used to identify discrepancies
    or mismatches between tables. For example, if you are comparing a list of registered
    users against a list of participants in an event, an outer join can help identify
    users who did not participate and participants who are not registered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex data merging**: When merging data from multiple sources that do not
    perfectly align, outer joins can be used to ensure that no data is lost during
    the merging process. This is particularly useful in complex data environments
    where data integrity is critical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transitioning from an outer join to a right join narrows the focus of the merged
    data, emphasizing the inclusion of all rows from the right DataFrame while maintaining
    matches from the left DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Right merge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The right merge (also known as right outer join) returns all the rows from
    the right DataFrame and the matching rows from the left DataFrame. The result
    of a right merge is presented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Right merge](img/B19801_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Right merge
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at how we can achieve the preceding result using the pandas
    `merge` function, using the example presented in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `how=''right''` argument specifies that a right outer join should be performed.
    This type of join returns all rows from the right DataFrame (`project_data`),
    and the matched rows from the left DataFrame (`employee_data`). Where there is
    no match, the result will have `NaN` in the columns of the left DataFrame. In
    the following table, you can see the output of the preceding join of the two DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following list, we present specific examples from the data world where
    a right merge is crucial:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Completing data**: A right merge is useful when you need to ensure that all
    entries from the right DataFrame are retained in the result, which is important
    when the right DataFrame contains essential data that must not be lost'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data enrichment**: This type of join can be used to enrich a dataset (right
    DataFrame) with additional attributes from another dataset (left DataFrame) while
    ensuring that all records from the primary dataset are preserved'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mismatched data analysis**: Like outer joins, right merges can help identify
    which entries in the right DataFrame do not have corresponding entries in the
    left DataFrame, which can be critical for data cleaning and validation processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transitioning from a right to a left merge shifts the perspective of the merged
    data, prioritizing the inclusion of all rows from the left DataFrame while maintaining
    matches from the right DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Left merge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The left merge (also known as left outer join) returns all the rows from the
    left DataFrame and the matching rows from the right DataFrame. The result of a
    left merge is presented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Left merge](img/B19801_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Left merge
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at how we can achieve the preceding result using the pandas
    `merge` function, using the example presented in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `how=''left''` argument specifies that a left outer join should be performed.
    This type of join returns all rows from the left DataFrame (`employee_data`),
    and the matched rows from the right DataFrame (`project_data`). Where there is
    no match, the result will have `NaN` in the columns of the right DataFrame. In
    the following table, you can see the output of the preceding join of the two DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If you are wondering when the left merge should be used, then the considerations
    presented in the previous section about the right merge apply in this case too.
    Now that we have discussed merge operations, let’s move on to handling duplicates
    that may arise during the merging process.
  prefs: []
  type: TYPE_NORMAL
- en: Handling duplicates when merging datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Handling duplicate keys before performing merge operations is crucial because
    duplicates can lead to unexpected results, such as Cartesian products, where rows
    are multiplied by the number of matching entries. This can not only distort the
    data analysis but also significantly impact performance due to the increased size
    of the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Why handle duplication in rows and columns?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Duplicate keys can lead to a range of problems that may compromise the accuracy
    of your results and the efficiency of your data processing. Let’s explore why
    it’s a good idea to handle duplicate keys prior to merging data:'
  prefs: []
  type: TYPE_NORMAL
- en: If there are duplicate keys in either table, merging these tables can result
    in a **Cartesian product**, where each duplicate key in one table matches with
    each occurrence of the same key in the other table, leading to an exponential
    increase in the number of rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate keys might represent data errors or inconsistencies, which can lead
    to incorrect analysis or conclusions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the dataset size by removing duplicates can lead to faster processing
    times during the merge operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having understood the importance of handling duplicate keys, let’s now examine
    various strategies to effectively manage these duplicates before proceeding with
    merge operations.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping duplicate rows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dropping duplicate entries in your datasets involves identifying and removing
    any duplicate rows based on specific key columns, which ensures that each entry
    is unique. This step not only simplifies subsequent data merging but also enhances
    the reliability of the analysis by eliminating potential sources of error caused
    by duplicate data. To showcase the dropping of duplicates, we will expand the
    example we have been using to add more duplicated rows in each of the DataFrames.
    As always, you can follow the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6a.manage_duplicates.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6a.manage_duplicates.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first create the sample employee data with some duplicate keys in the
    `employee_id` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also create the sample project data with some duplicate keys in the `employee_id`
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we want to merge these datasets. But first, we’ll drop any duplicates
    so that we can make the merge operation as lightweight as possible. Dropping the
    duplicates before the merge is shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the code, `drop_duplicates()` is used to remove duplicate rows based
    on `employee_id`. The `keep='first'` parameter ensures that only the first occurrence
    is kept while others are removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'After dropping the duplicates, you can proceed with the merge operation, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The merged dataset can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `merged_data` DataFrame includes columns from both the `employee_data` and
    `project_data` DataFrames, showing the `employee_id`, `name`, `department`, and
    `project_name` values for each employee that exists in both datasets. The duplicates
    are removed, ensuring each employee appears only once in the final merged dataset.
    The `drop_duplicates` operation is crucial for avoiding data redundancy and potential
    conflicts during the merge operation. Next, we will discuss how we can guarantee
    that the merge operation respects the uniqueness of the keys and adheres to specific
    constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Validating data before merging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When merging datasets, especially large and complex ones, ensuring the integrity
    and validity of the merge operation is crucial. pandas provides the `validate`
    parameter in the `merge()` function to enforce specific conditions and relationships
    between the keys used in the merge. This helps in identifying and preventing unintended
    duplications or data mismatches that could compromise the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates how to use the `validate` parameter to enforce
    `merge()` constraints and handle exceptions when these constraints are not met.
    You can see the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6b.manage_duplicates_validate.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6b.manage_duplicates_validate.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, the merge operation is wrapped in a `try-except`
    block. This is a way to handle exceptions, which are errors occurring during a
    program’s execution. The `try` block contains the code that might raise an exception,
    in this case, the merge operation. If an exception occurs, the code execution
    moves to the `except` block.
  prefs: []
  type: TYPE_NORMAL
- en: If the merge operation fails the validation check (in our case, if there are
    duplicate keys in the left DataFrame when they are expected to be unique), `ValueError`
    exception will be raised, and the `except` block will be executed. The `except`
    block catches the `ValueError` exception and prints a `Merge failed:` message,
    followed by the error message provided by pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the preceding code, you will see the following error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `validate='one_to_many'` parameter is included in the merge operation. This
    parameter tells pandas to check that the merge operation is of the specified type.
    In this case, `one_to_many` means that the merge keys should be unique in the
    left DataFrame (`employee_data`) but can have duplicates in the right DataFrame
    (`project_data`). If the validation check fails, pandas will raise a `ValueError`
    exception.
  prefs: []
  type: TYPE_NORMAL
- en: When to use which approach
  prefs: []
  type: TYPE_NORMAL
- en: Use **manual duplicate removal** when you need fine control over how duplicates
    are identified and handled, or when duplicates require special processing (e.g.,
    aggregation or transformation based on other column values).
  prefs: []
  type: TYPE_NORMAL
- en: Use **merge validation** when you want to ensure the structural integrity of
    your data model directly within the merge operation, especially in straightforward
    cases where the relationship between the tables is well-defined and should not
    include duplicate keys according to the business logic or data model.
  prefs: []
  type: TYPE_NORMAL
- en: If there is a good reason for the existence of duplicates in the data, we can
    consider employing aggregation methods during the merge to consolidate redundant
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aggregation is a powerful technique for managing duplicates in datasets, particularly
    when dealing with key columns that should be unique but contain multiple entries.
    By grouping data on these key columns and applying aggregation functions, we can
    consolidate duplicate entries into a single, summarized record. Aggregation functions
    such as sum, average, or maximum can be used to combine or summarize the data
    in a way that aligns with the analytical goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how aggregation can be employed to effectively deal with duplicates
    before merging data. We will extend the dataset a little bit to help us with this
    example, as shown here. You can see the full example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6c.merge_and_aggregate.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/6c.merge_and_aggregate.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s perform the aggregation step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `groupby()` method is used on `employee_data` with `employee_id` as the
    key. This groups the DataFrame by `employee_id`, which is necessary because of
    the duplicate `employee_id` values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `agg()` method is then applied to perform specific aggregations on different
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''name'': ''first''` and `''department'': ''first''` ensure that the first
    encountered values for these columns are retained in the grouped data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''salary'': ''sum''` sums up the salaries for each `employee_id` value, which
    is useful if the duplicates represent split records of cumulative data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the final step, the `pd.merge()` function is used to combine `aggregated_employee_data`
    with `project_data` using an inner join on the `employee_id` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This ensures that only employees with project assignments are included in the
    result. The result after the merge is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `agg()` method in pandas is highly versatile, offering numerous options
    beyond the simple “keep first” approach demonstrated in the previous example.
    This method can apply a wide range of aggregation functions to consolidate data,
    such as summing numerical values, finding averages, or selecting maximum or minimum
    entries. We will dive deeper into the various capabilities of the `agg()` method
    in the next chapter, exploring how these different options can be applied to enhance
    data preparation and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on from using aggregation as a way to handle duplicates to concatenating
    duplicated rows when dealing with text or categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: Concatenation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concatenation of values from duplicate rows into a single row can be a useful
    technique, especially when dealing with categorical or textual data that may have
    multiple valid entries for the same key. This approach allows you to preserve
    all the information across duplicates without losing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how concatenation of rows can be employed to effectively deal with
    duplicates before merging data. To showcase this method, we will use the following
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s perform the concatenation step, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the concatenation step, the `groupby(''employee_id'')` method groups the
    data by `employee_id`. The `transform(lambda x: '', ''.join(x))` method is then
    applied to the `department` column. The `transform` function is used here with
    a `lambda` function that joins all entries of the column department for each group
    (i.e., `employee_id`) into a single string separated by commas.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of this operation replaces the original `department` column in `employee_data`,
    where each `employee_id` now has a single entry for `department` that includes
    all original department data concatenated into one string, as shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Use concatenation when you need to preserve all categorical or textual data
    across duplicate entries without preferring one entry over another.
  prefs: []
  type: TYPE_NORMAL
- en: This method is useful for summarizing textual data in a way that is still readable
    and informative, especially when dealing with attributes that can have multiple
    valid values (e.g., an employee belonging to multiple departments).
  prefs: []
  type: TYPE_NORMAL
- en: Once duplicate rows in each DataFrame are resolved, attention shifts to identifying
    and resolving duplicate columns across the DataFrames to be merged.
  prefs: []
  type: TYPE_NORMAL
- en: Handling duplication in columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When merging data from different sources, it’s not uncommon to encounter DataFrames
    with overlapping column names. This challenge often arises when combining data
    from similar datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Expanding the example data we have been using so far, we will adjust the DataFrames
    to help us showcase the options we have when dealing with common columns across
    DataFrames. The data can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see how we can merge these datasets by applying different techniques without
    breaking the merge operation.
  prefs: []
  type: TYPE_NORMAL
- en: Handling duplicate columns while merging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The columns in the two DataFrames presented previously share identical names
    and may represent the same data. However, we have decided to retain both sets
    of columns in the merged DataFrame. This decision is based on the suspicion that,
    despite having the same column names, the entries are not entirely identical,
    indicating that they may be different representations of the same data. This is
    an issue that we can address later, following the merge operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best approach to keep both sets of columns is to use the `suffixes` parameter
    in the `merge()` function. This will allow you to differentiate between the columns
    from each DataFrame without losing any data. Here’s how you can implement this
    in Python using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pd.merge()` function is used to merge the two DataFrames on `employee_id`.
    The `how=''outer''` parameter is used to ensure all records from both DataFrames
    are included in the merged DataFrame, even if there are no matching `employee_id`
    values. The `suffixes=(''_1'', ''_2'')` parameter adds suffixes to the columns
    from each DataFrame to differentiate them in the merged DataFrame. This is crucial
    when columns have the same names but come from different sources. Let’s review
    the output DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This approach is particularly useful in scenarios where merging data from different
    sources involves overlapping column names but where it’s also important to retain
    and clearly distinguish these columns in the resulting DataFrame. Another point
    to consider is that suffixes allow for identifying which DataFrame the data originated
    from, which is useful in analyses involving data from multiple sources.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain how to deal with duplicate columns by dropping
    them *before* the merge.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping duplicate columns before the merge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we find that we have copies of the same column in both DataFrames we want
    to merge and that the column in one of the DataFrames is sufficient or more reliable
    than the other, then it may be more practical to drop one of the duplicate columns
    before a merge operation instead of keeping both. This decision can be driven
    by the need to simplify the dataset, reduce redundancy, or when one of the columns
    does not provide additional value to the analysis. Let’s have a look at the data
    for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If we take a closer look at this data, we can see that the `department` column
    in the two DataFrames captures the same information but in different formats.
    For the sake of our example, let’s assume that we know the HR system tracks the
    department of each employee in the format presented in the first DataFrame. That’s
    why we will trust this column more than the one in the second DataFrame. Therefore,
    we will drop the second one before the merge operation. Here’s how you can drop
    the column before the merge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Before merging, the `department` column from `employee_data_2` is dropped because
    it’s deemed less reliable. This is done using the `drop(columns=[''department''],
    inplace=True)` method. Having dropped the required columns, we can proceed with
    the merge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The DataFrames are merged using the `employee_id` and `name` columns as keys
    with the `pd.merge()` function. The `how='inner'` parameter is used to perform
    an inner join, which includes only rows that have matching values in both DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'To optimize the merging process and improve performance, it’s often beneficial
    to drop unnecessary columns before performing the merge operation for the following
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It leads to improved performance by significantly reducing the memory footprint
    during the merge operation, as it minimizes the amount of data to be processed
    and combined, thereby expediting the process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting DataFrame becomes simpler and cleaner, facilitating easier data
    management and subsequent analysis. This reduction in complexity not only streamlines
    the merge operation but also reduces the likelihood of errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In resource-constrained environments, such as those with limited computing resources,
    minimizing the dataset before intensive operations such as merging enhances resource
    efficiency and ensures smoother execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case that we have identical columns across the DataFrames, another option
    is to consider whether we can use them as keys in the merge operation.
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate keys
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When encountering identical keys across DataFrames, a smart approach is to
    merge based on these common columns. Let’s revisit the example presented in the
    previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We can see here that we used `['employee_id', 'name']` as keys in the merge.
    If `employee_id` and `name` are reliable identifiers that ensure accurate matching
    of records across DataFrames, they should be used as keys. This ensures that the
    merged data accurately represents combined records from both sources.
  prefs: []
  type: TYPE_NORMAL
- en: As the volume and complexity of data continues to grow, it is crucial to efficiently
    combine datasets, as we will learn in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tricks for merging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with large datasets, the performance of merge operations can significantly
    impact the overall efficiency of data processing tasks. Merging is a common and
    often necessary step in data analysis, but it can be computationally intensive,
    especially when dealing with big data. Therefore, employing performance optimization
    techniques is crucial to ensure that merges are executed as quickly and efficiently
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing merge operations can lead to reduced execution time, lower memory
    consumption, and an overall smoother data-handling experience. In the following
    sections, we will explore various performance tricks that can be applied to merge
    operations in pandas, such as utilizing indexes, sorting indexes, choosing the
    right merge method, and reducing memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Set indexes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Utilizing indexes in pandas is a critical aspect of data manipulation and analysis,
    particularly when dealing with large datasets or performing frequent data retrieval
    operations. Indexes serve as a tool for both identification and efficient data
    access, providing several benefits that can significantly enhance performance.
    Specifically, when merging DataFrames, utilizing indexes can lead to performance
    improvements. Merging on indexes, rather than on columns, is generally faster
    because pandas can perform the merge operation using optimized index-based joining
    methods, which is more efficient than column-based merging. Let’s revisit the
    employee example to prove this concept. The full code for this example can be
    found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8a.perfomance_benchmark_set_index.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8a.perfomance_benchmark_set_index.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the number of rows for the benchmarking example for each DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s create the DataFrames for the example, which will have the number of
    rows as defined in the `num_rows` variable. The first employee DataFrame is defined
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The second DataFrame is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate the effectiveness of the performance tricks we applied, we will
    initially perform the merge *without utilizing the index*. We’ll calculate the
    time taken for this operation. Subsequently, we’ll set the index in both DataFrames
    and repeat the merge operation, recalculating the time. Finally, we will present
    the results. Let’s hope this approach yields the desired outcome! Let’s start
    the clock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s perform the merge operation without using indexes, just by inner joining
    on `[''``employee_id'', ''name'']`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s calculate the time it took to perform the merge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The timings may vary depending on the computer used to execute the program.
    The idea is that the optimized version takes less time than the original merge
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'By setting `employee_id` as the index for both DataFrames (`employee_data_1`
    and `employee_data_2`), we allow pandas to use optimized index-based joining methods.
    This is particularly effective because indexes in pandas are implemented via hash
    tables or B-trees, depending on the data type and the sortedness of the index,
    which facilitates faster lookups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s repeat the merge operation after setting the indexes and calculate the
    time once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now, if we calculate the percentage difference from the initial time to the
    final one, we see that we managed to drop the time by around 88.5%, just by setting
    the index. This seems impressive but let’s also discuss some considerations when
    setting indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Index considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s important to choose the right columns for indexing based on the query patterns.
    Over-indexing can lead to unnecessary use of disk space and can degrade write
    performance due to the overhead of maintaining the indexes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Rebuilding** or **reorganizing indexes** is essential for optimal performance.
    These tasks address index fragmentation and ensure consistent performance over
    time.'
  prefs: []
  type: TYPE_NORMAL
- en: While indexes can significantly improve read performance, they can also impact
    write performance. It’s crucial to find a balance between optimizing for read
    operations (such as searches and joins) and maintaining efficient write operations
    (such as inserts and updates).
  prefs: []
  type: TYPE_NORMAL
- en: Multi-column indexes, or concatenated indexes, can be beneficial when multiple
    fields are often used together in queries. However, the order of the fields in
    the index definition is important and should reflect the most common query patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Having proved the importance of setting indexes, let’s go a step further and
    discuss the option of sorting the index before merging.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting indexes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sorting the index in pandas can be particularly beneficial in scenarios where
    you are frequently merging or performing join operations on large DataFrames.
    When indexes are sorted, pandas can take advantage of more efficient algorithms
    to align and join data, which can lead to significant performance improvements.
    Let’s deep dive into this before proceeding to the code example:'
  prefs: []
  type: TYPE_NORMAL
- en: When indexes are sorted, pandas can use binary search algorithms to locate the
    matching rows between DataFrames. Binary search has a time complexity of *O(log
    n)*, which is much faster than the linear search required for unsorted indexes,
    especially as the size of the DataFrame grows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sorted indexes facilitate quicker data alignment. This is because pandas can
    make certain assumptions about the order of the data, which streamlines the process
    of finding corresponding rows in each DataFrame during a merge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With sorted indexes, pandas can avoid unnecessary comparisons that would be
    required if the indexes were unsorted. This reduces the computational overhead
    and speeds up the merging process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s go back to the code example by adding the sorting of the index. The original
    data remains the same; however, in this experiment, we are comparing the time
    it takes to perform the merge operation after the setting of the index versus
    the time it takes to perform the merge operation after the setting and sorting
    of indexes. The following code shows the main code components but, as always,
    you can follow the full example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8b.performance_benchmark_sort_indexes.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter05/8b.performance_benchmark_sort_indexes.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s perform the merge operation without sorting indexes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s repeat the merge operation after sorting the indexes and calculate the
    time once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now, if we calculate the percentage difference from the initial time to the
    final one, we see that we managed to drop the time by an extra ~22%, by sorting
    the index. This seems impressive but let’s also discuss some considerations when
    setting indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting index considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sorting a DataFrame’s index is not free of computational cost. The initial sorting
    operation itself takes time, so it’s most beneficial when the sorted DataFrame
    will be used in multiple merge or join operations, amortizing the cost of sorting
    over these operations.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting can sometimes increase the memory overhead, as pandas may create a sorted
    copy of the DataFrame’s index. This should be considered when working with very
    large datasets where memory is a constraint.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting the index is most beneficial when the key used for merging is not only
    unique but also has some logical order that can be leveraged, such as time-series
    data or ordered categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: Index management and maintenance are crucial aspects you should consider when
    working with pandas DataFrames, especially when dealing with large datasets. Maintaining
    a well-managed index requires careful consideration. For example, regularly updating
    or reindexing a DataFrame can introduce computational costs, similar to sorting
    operations. Each time you modify the index—by sorting, reindexing, or resetting—it
    can result in additional memory usage and processing time, particularly with large
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Indexes need to be maintained in a way that balances performance and resource
    usage. For instance, if you frequently merge or join DataFrames, ensuring that
    the index is properly sorted and unique can significantly speed up these operations.
    However, continuously maintaining a sorted index can be resource-intensive, so
    it’s most beneficial when the DataFrame will be involved in multiple operations
    that can leverage the sorted index.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, choosing the right index type—whether it’s a simple integer-based
    index, a datetime index for time-series data, or a multi-level index for hierarchical
    data—can influence how efficiently pandas handles your data. The choice of index
    should align with the structure and access patterns of your data to minimize unnecessary
    overhead.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss how using the `join` function instead of
    `merge` can impact performance.
  prefs: []
  type: TYPE_NORMAL
- en: Merge versus join
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While merging is a commonly used method to combine datasets based on specific
    conditions or keys, there is another approach: the `join` function. This function
    provides a streamlined way to perform merges primarily based on indexes, offering
    a simpler alternative to the more general merge function. The `join` method in
    pandas is particularly useful when the DataFrames involved have their indexes
    set up as the keys for joining, allowing for efficient and straightforward data
    combinations without the need for specifying complex join conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `join` function instead of `merge` can impact performance in several
    ways, primarily due to the underlying mechanisms and default behaviors of these
    two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: The `join` function in pandas is optimized for index-based joining, meaning
    it’s designed to be efficient when joining DataFrames on their indexes. If your
    DataFrames are already indexed by the keys you want to join on, using `join` can
    be more performance-efficient because it leverages the optimized index structures
    [2][6][7].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join is a simplified version of merge that defaults to joining on indexes. This
    simplicity can translate into performance benefits, especially for straightforward
    joining tasks where the complexity of merge is unnecessary. By avoiding the overhead
    of aligning non-index columns, join can execute more quickly in these scenarios
    [2][6].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood, join uses merge [2][6].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When joining large DataFrames, the way join and merge handle memory can impact
    performance. A join, by focusing on index-based joining, might manage memory usage
    more efficiently in certain scenarios, especially when the DataFrames have indexes
    that pandas can optimize on [1][3][4].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While merge offers greater flexibility by allowing joins on arbitrary columns,
    this flexibility comes with a performance cost, especially for complex joins involving
    multiple columns or non-index joins. Join offers a performance advantage in simpler,
    index-based joins due to its more specific use case [2][6].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, choosing between `join` and `merge` depends on the specific requirements
    of your task. If your joining operation is primarily based on indexes, join can
    offer performance benefits due to its optimization for index-based joining and
    its simpler interface. However, for more complex joining needs that involve specific
    columns or multiple keys, merge provides the necessary flexibility, albeit with
    potential impacts on performance.
  prefs: []
  type: TYPE_NORMAL
- en: Concatenating DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you have datasets spread across multiple DataFrames with similar structures
    (same columns or same rows) and you want to combine them into a single DataFrame,
    this is where concatenating shines. The concatenation process can be along a particular
    axis, either row-wise (`axis=0`) or column-wise (`axis=1`).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s deep dive into the row-wise concatenation, also known as append.
  prefs: []
  type: TYPE_NORMAL
- en: Row-wise concatenation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The row-wise concatenation is used to concatenate one DataFrame to another
    along `axis=0`. To showcase this operation, two DataFrames, `employee_data_1`
    and `employee_data_2`, created with the same structure but different data can
    be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s perform the row-wise concatenation, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The `pd.concat()` function is used to concatenate the two DataFrames. The first
    argument is a list of DataFrames to concatenate, and the `axis=0` parameter specifies
    that the concatenation should be row-wise, stacking the DataFrames on top of each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Some things you need to consider when performing row-wise concatenation are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that the columns you want to concatenate are aligned correctly. pandas
    will automatically align columns by name and fill any missing columns with `NaN`
    values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After concatenation, you may want to reset the index of the resulting DataFrame
    to avoid duplicate index values, especially if the original DataFrames had their
    own range of indices. Observe the index in the following example before performing
    a `reset` operation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s see the output once more:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Resetting the index creates a new, continuous index for the concatenated DataFrame.
    The `drop=True` parameter is used to avoid adding the old index as a column in
    the new DataFrame. This step is crucial for maintaining a clean DataFrame, especially
    when the index itself does not carry meaningful data. A continuous index is often
    easier to work with, particularly for indexing, slicing, and potential future
    merges or joins.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Concatenation can increase the memory usage of your program, especially when
    working with large DataFrames. Be mindful of the available memory resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will discuss the column-wise concatenation.
  prefs: []
  type: TYPE_NORMAL
- en: Column-wise concatenation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Concatenating DataFrames column-wise in pandas involves combining two or more
    DataFrames side by side, aligning them by their index. To showcase this operation,
    the two DataFrames we have been using so far, `employee_data_1` and `employee_data_2`,
    will be used, and the operation can be done as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pd.concat()` function is used with the `axis=1` parameter to concatenate
    the DataFrames side by side. This aligns the DataFrames by their index, effectively
    adding new columns from `employee_performance` to `employee_data_1`. This will
    display the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Some things you need to consider when performing column-wise concatenation
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The indices of the DataFrames to be concatenated are aligned properly. When
    concatenating DataFrames column-wise, each row in the resulting DataFrame should
    ideally represent data from the same entity (e.g., the same employee). Misaligned
    indexes can lead to a scenario where data from different entities is erroneously
    combined, leading to inaccurate and misleading results. For example, if the index
    represents employee IDs, misalignment could result in an employee’s details being
    incorrectly paired with another employee’s performance data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the DataFrames contain columns with the same name but are intended to be
    distinct, consider renaming these columns before concatenation to avoid confusion
    or errors in the resulting DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While column-wise concatenation typically does not increase memory usage as
    significantly as row-wise concatenation, it is still important to monitor memory
    usage, especially with large DataFrames.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join versus concatenation
  prefs: []
  type: TYPE_NORMAL
- en: '**Concatenation** is primarily used for combining DataFrames along an axis
    (either rows or columns) without considering the values within. It’s ideal for
    situations where you simply want to stack DataFrames together based on their order
    or extend them with additional columns.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Joins** are used to combine DataFrames based on one or more keys (a common
    identifier in each DataFrame). This is more about merging datasets based on shared
    data points, which allows for more complex and conditional combinations of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Having explored the nuances of concatenation in pandas, including its importance
    for aligning indexes and how it contrasts with join operations, let’s now summarize
    the key points discussed to encapsulate our understanding and highlight the critical
    takeaways from our exploration of DataFrame manipulations in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various aspects of DataFrame operations in pandas,
    focusing on concatenation, merging, and the importance of managing indexes.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed merging, which is suited for complex combinations based on shared
    keys, offering flexibility through various join types such as inner, outer, left,
    and right joins. We also discussed how concatenation is used to combine DataFrames
    along a specific axis (either row-wise or column-wise) and is particularly useful
    for appending datasets or adding new dimensions to data. The performance implications
    of these operations were discussed, highlighting that proper index management
    can significantly enhance the efficiency of these operations, especially in large
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, we will deep dive into how the `groupby` function can
    be leveraged alongside various aggregation functions to extract meaningful insights
    from complex data structures.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://github.com/pandas-dev/pandas/issues/38418](https://github.com/pandas-dev/pandas/issues/38418)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://realpython.com/pandas-merge-join-and-concat/](https://realpython.com/pandas-merge-join-and-concat/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://datascience.stackexchange.com/questions/44476/merging-dataframes-in-pandas-is-taking-a-surprisingly-long-time](https://datascience.stackexchange.com/questions/44476/merging-dataframes-in-pandas-is-taking-a-surprisingly-long-time)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://stackoverflow.com/questions/40860457/improve-pandas-merge-performance](https://stackoverflow.com/questions/40860457/improve-pandas-merge-performance)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=P6hSBrxs0Eg](https://www.youtube.com/watch?v=P6hSBrxs0Eg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://pandas.pydata.org/pandas-docs/version/1.5.1/user_guide/merging.html](https://pandas.pydata.org/pandas-docs/version/1.5.1/user_guide/merging.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://pandas.pydata.org/pandas-docs/version/0.20/merging.html](https://pandas.pydata.org/pandas-docs/version/0.20/merging.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
