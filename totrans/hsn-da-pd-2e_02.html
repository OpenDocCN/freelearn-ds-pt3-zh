<html><head></head><body>
		<div id="_idContainer045">
			<h1 id="_idParaDest-16"><em class="italic"><a id="_idTextAnchor015"/>Chapter 1</em>: Introduction to Data Analysis</h1>
			<p>Before we can begin our hands-on introduction to data analysis with <strong class="source-inline">pandas</strong>, we need to learn about the fundamentals of data analysis. Those who have ever looked at the documentation for a software library know how overwhelming it can be if you have no clue what you are looking for. Therefore, it is essential that we master not only the coding aspect but also the thought process and workflow required to analyze data, which will prove the most useful in augmenting our skill set in the future.</p>
			<p>Much like the scientific method, data science has some common workflows that we can follow when we want to conduct an analysis and present the results. The backbone of this process is <strong class="bold">statistics</strong>, which gives us ways to describe our data, make predictions, and also draw conclusions about it. Since prior knowledge of statistics is not a prerequisite, this chapter will give us exposure to the statistical concepts we will use throughout this book, as well as areas for further exploration.</p>
			<p>After covering the fundamentals, we will get our Python environment set up for the remainder of this book. Python is a powerful language, and its uses go way beyond data science: building web applications, software, and web scraping, to name a few. In order to work effectively across projects, we need to learn how to make <strong class="bold">virtual environments</strong>, which will isolate each project's dependencies. Finally, we will learn how to work with Jupyter Notebooks in order to follow along with the text.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>The fundamentals of data analysis</li>
				<li>Statistical foundations</li>
				<li>Setting up a virtual environment</li>
			</ul>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Chapter materials</h1>
			<p>All the files for this book are on GitHub at <a href="https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition">https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition</a>. While having a GitHub account isn't necessary to work through this book, it is a good idea to create one, as it will serve as a portfolio for any data/coding projects. In addition, working with Git will provide a version control system and make collaboration easy.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Check out this article to learn some Git basics: <a href="https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/">https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/</a>.</p>
			<p>In order to get a local copy of the files, we have a few options (ordered from least useful to most useful):</p>
			<ul>
				<li>Download the ZIP file and extract the files locally.</li>
				<li>Clone the repository without forking it.</li>
				<li>Fork the repository and then clone it.</li>
			</ul>
			<p>This book includes exercises for every chapter; therefore, for those who want to keep a copy of their solutions along with the original content on GitHub, it is highly recommended to <strong class="bold">fork</strong> the repository and <strong class="bold">clone</strong> the forked version. When we fork a repository, GitHub will make a repository under our own profile with the latest version of the original. Then, whenever we make changes to our version, we can push the changes back up. Note that if we simply clone, we don't get this benefit.</p>
			<p>The relevant buttons for initiating this process are circled in the following screenshot:</p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/Figure_1.1_B16834.jpg" alt="Figure 1.1 – Getting a local copy of the code for following along&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1 – Getting a local copy of the code for following along</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The cloning process will copy the files to the current working directory in a folder called <strong class="source-inline">Hands-On-Data-Analysis-with-Pandas-2nd-edition</strong>. To make a folder to put this repository in, we can use <strong class="source-inline">mkdir my_folder &amp;&amp; cd my_folder</strong>. This will create a new folder (directory) called <strong class="source-inline">my_folder</strong> and then change the current directory to that folder, after which we can clone the repository. We can chain these two commands (and any number of commands) together by adding <strong class="source-inline">&amp;&amp;</strong> in between them. This can be thought of as <em class="italic">and then</em> (provided the first command succeeds).</p>
			<p>This repository has folders for each chapter. This chapter's materials can be found at <a href="https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_01">https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_01</a>. While the bulk of this chapter doesn't involve any coding, feel free to follow along in the <strong class="source-inline">introduction_to_data_analysis.ipynb</strong> notebook on the GitHub website until we set up our environment toward the end of the chapter. After we do so, we will use the <strong class="source-inline">check_your_environment.ipynb</strong> notebook to get familiar with Jupyter Notebooks and to run some checks to make sure that everything is set up properly for the rest of this book.</p>
			<p>Since the code that's used to generate the content in these notebooks is not the main focus of this chapter, the majority of it has been separated into the <strong class="source-inline">visual_aids</strong> package, which is used to create visuals for explaining concepts throughout the book, and the <strong class="source-inline">check_environment.py</strong> file. If you choose to inspect these files, don't be overwhelmed; everything that's relevant to data science will be covered in this book.</p>
			<p>Every chapter includes exercises; however, for this chapter only, there is an <strong class="source-inline">exercises.ipynb</strong> notebook, with code to generate some initial data. Knowledge of basic Python will be necessary to complete these exercises. For those who would like to review the basics, make sure to run through the <strong class="source-inline">python_101.ipynb</strong> notebook, included in the materials for this chapter, for a crash course. The official Python tutorial is a good place to start for a more formal introduction: <a href="https://docs.python.org/3/tutorial/index.html">https://docs.python.org/3/tutorial/index.html</a>.</p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>The fundamentals of data analysis</h1>
			<p>Data analysis is <a id="_idIndexMarker000"/>a highly iterative process involving collection, preparation (wrangling), <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>), and drawing conclusions. During <a id="_idIndexMarker001"/>an analysis, we will frequently revisit each of these steps. The following diagram depicts a generalized workflow:</p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/Figure_1.2_B16834.jpg" alt="Figure 1.2 – The data analysis workflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – The data analysis workflow</p>
			<p>Over the next <a id="_idIndexMarker002"/>few sections, we will get an overview of each of these steps, starting with data collection. In practice, this process is heavily skewed toward the data preparation side. Surveys have found that although data scientists enjoy the data preparation side of their job the least, it makes up 80% of their work (<a href="https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/">https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/</a>). This data preparation step is where <strong class="source-inline">pandas</strong> really shines.</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>Data collection</h2>
			<p>Data collection <a id="_idIndexMarker003"/>is the natural first step for any data analysis—we can't <a id="_idIndexMarker004"/>analyze data we don't have. In reality, our analysis can begin even before we have the data. When we decide what we want to investigate or analyze, we have to think about what kind of data we can collect that will be useful for our analysis. While data can come from anywhere, we will explore the following sources throughout this book:</p>
			<ul>
				<li>Web scraping to extract data from a website's HTML (often with Python packages such as <strong class="source-inline">selenium</strong>, <strong class="source-inline">requests</strong>, <strong class="source-inline">scrapy</strong>, and <strong class="source-inline">beautifulsoup</strong>)</li>
				<li><strong class="bold">Application programming interfaces</strong> (<strong class="bold">APIs</strong>) for web services from which we can <a id="_idIndexMarker005"/>collect data with HTTP requests (perhaps using <strong class="source-inline">cURL</strong> or the <strong class="source-inline">requests</strong> Python package)</li>
				<li>Databases (data can be extracted with SQL or another database-querying language)</li>
				<li>Internet <a id="_idIndexMarker006"/>resources that provide data for download, such as government websites or Yahoo! Finance</li>
				<li>Log files<p class="callout-heading">Important note</p><p class="callout"><a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>, will give us the skills we need to work with the aforementioned data sources. <a href="B16834_12_Final_SK_ePub.xhtml#_idTextAnchor254"><em class="italic">Chapter 12</em></a>, <em class="italic">The Road Ahead</em>, provides numerous resources for finding data sources.</p></li>
			</ul>
			<p>We are surrounded by data, so the possibilities are limitless. It is important, however, to make sure that we are collecting data that will help us draw conclusions. For example, if we are <a id="_idIndexMarker007"/>trying to determine whether hot chocolate sales are higher when the temperature is lower, we should collect data on the amount of hot chocolate sold and the temperatures each day. While it might be interesting to see how far people traveled to get the hot chocolate, it's not relevant to our analysis.</p>
			<p>Don't worry too much about finding the perfect data before beginning an analysis. Odds are, there will always be something we want to add/remove from the initial dataset, reformat, merge with other data, or change in some way. This is where data wrangling comes into play.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Data wrangling</h2>
			<p><strong class="bold">Data wrangling</strong> is the process of preparing the data and getting it into a format that can be <a id="_idIndexMarker008"/>used for analysis. The unfortunate <a id="_idIndexMarker009"/>reality of data is that it is often dirty, meaning that it requires cleaning (preparation) before it can be used. The following are some issues we may <a id="_idIndexMarker010"/>encounter with our data:</p>
			<ul>
				<li><strong class="bold">Human errors</strong>: Data is recorded (or even collected) incorrectly, such as putting <strong class="source-inline">100</strong> instead of <strong class="source-inline">1000</strong>, or typos. In addition, there may be multiple versions of the same entry recorded, such as <strong class="source-inline">New York City</strong>, <strong class="source-inline">NYC</strong>, and <strong class="source-inline">nyc</strong>.</li>
				<li><strong class="bold">Computer error</strong>: Perhaps we weren't recording entries for a while (missing data).</li>
				<li><strong class="bold">Unexpected values</strong>: Maybe whoever was recording the data decided to use a question mark for a missing value in a numeric column, so now all the entries in the column will be treated as text instead of numeric values.</li>
				<li><strong class="bold">Incomplete information</strong>: Think of a survey with optional questions; not everyone will answer them, so we will have missing data, but not due to computer or human error.</li>
				<li><strong class="bold">Resolution</strong>: The data may have been collected per second, while we need hourly data for our analysis.</li>
				<li><strong class="bold">Relevance of the fields</strong>: Often, data is collected or generated as a product of some process rather than explicitly for our analysis. In order to get it to a usable state, we will have to clean it up.</li>
				<li><strong class="bold">Format of the data</strong>: Data may be recorded in a format that isn't conducive to analysis, which will require us to reshape it.</li>
				<li><strong class="bold">Misconfigurations in the data-recording process</strong>: Data coming from sources such as misconfigured trackers and/or webhooks may be missing fields or passed in the wrong order.</li>
			</ul>
			<p>Most of these data quality issues can be remedied, but some cannot, such as when the data is collected daily and we need it on an hourly resolution. It is our responsibility to carefully examine our data and handle any issues so that our analysis doesn't get distorted. We will cover this process in depth in <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>, and <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>.</p>
			<p>Once we <a id="_idIndexMarker011"/>have performed an initial cleaning of the data, we are ready for EDA. Note that during EDA, we may need some additional data wrangling: these two steps are highly intertwined.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Exploratory data analysis</h2>
			<p>During EDA, we use visualizations and summary statistics to get a better understanding of the data. Since <a id="_idIndexMarker012"/>the human <a id="_idIndexMarker013"/>brain excels at picking out visual patterns, data visualization is essential to any analysis. In fact, some characteristics of the data can only be observed in a plot. Depending on our data, we may create plots to see how a variable of interest has evolved over time, compare how many observations belong to each category, find outliers, look at distributions of continuous and discrete variables, and much more. In <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>, and <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>, we will learn how to create these plots for both EDA and presentation.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Data visualizations are very powerful; unfortunately, they can often be misleading. One common issue stems from the scale of the <em class="italic">y</em>-axis because most plotting tools will zoom in by default to show the pattern up close. It would be difficult for software to know what the appropriate axis limits are for every possible plot; therefore, it is our job to properly adjust the axes before presenting our results. You can read about some more ways that plots can be misleading at <a href="https://venngage.com/blog/misleading-graphs/">https://venngage.com/blog/misleading-graphs/</a>.</p>
			<p>In the workflow diagram we saw earlier (<em class="italic">Figure 1.2</em>), EDA and data wrangling shared a box. This is because they are closely tied:</p>
			<ul>
				<li>Data needs to be prepped before EDA.</li>
				<li>Visualizations that are created during EDA may indicate the need for additional data cleaning.</li>
				<li>Data wrangling uses summary statistics to look for potential data issues, while EDA uses them to understand the data. Improper cleaning will distort the findings when we're conducting EDA. In addition, data wrangling skills will be required to get summary statistics across subsets of the data.</li>
			</ul>
			<p>When calculating summary statistics, we must keep the type of data we collected in mind. Data can be <strong class="bold">quantitative</strong> (measurable quantities) or <strong class="bold">categorical</strong> (descriptions, groupings, or categories). Within these classes of data, we have further subdivisions that let us know what types of operations we can perform on them.</p>
			<p>For example, categorical data can be <strong class="bold">nominal</strong>, where we assign a numeric value to each level <a id="_idIndexMarker014"/>of the category, such as <strong class="source-inline">on = 1</strong>/<strong class="source-inline">off = 0</strong>. Note that the fact that <strong class="source-inline">on</strong> is greater than <strong class="source-inline">off</strong> is meaningless because we arbitrarily <a id="_idIndexMarker015"/>chose those numbers to represent the states <strong class="source-inline">on</strong> and <strong class="source-inline">off</strong>. When there is a ranking among the categories, they are <strong class="bold">ordinal</strong>, meaning that we can order the levels (for instance, we can have <strong class="source-inline">low &lt; medium &lt; high</strong>).</p>
			<p>Quantitative data can use an <strong class="bold">interval scale</strong> or a <strong class="bold">ratio scale</strong>. The interval scale includes things such as temperature. We can measure temperatures in Celsius and compare the temperatures of two cities, but it doesn't mean anything to say one city is twice as hot as the other. Therefore, interval scale values can be meaningfully compared using addition/subtraction, but not multiplication/division. The ratio scale, then, are those values that can be meaningfully compared with ratios (using multiplication and division). Examples of the ratio scale include prices, sizes, and counts.</p>
			<p>When we complete our EDA, we can decide on the next steps by drawing conclusions.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>Drawing conclusions</h2>
			<p>After we <a id="_idIndexMarker016"/>have collected the data for our analysis, cleaned <a id="_idIndexMarker017"/>it up, and performed some thorough EDA, it is time to draw conclusions. This is where we summarize our findings from EDA and decide the next steps:</p>
			<ul>
				<li>Did we notice any patterns or relationships when visualizing the data?</li>
				<li>Does it look like we can make accurate predictions from our data? Does it make sense to move to modeling the data?</li>
				<li>Should we handle missing data points? How?</li>
				<li>How is the data distributed?</li>
				<li>Does the data help us answer the questions we have or give insight into the problem we are investigating?</li>
				<li>Do we need to collect new or additional data?</li>
			</ul>
			<p>If we <a id="_idIndexMarker018"/>decide to model the data, this falls <a id="_idIndexMarker019"/>under machine learning and statistics. While not technically data analysis, it is usually the next step, and we will cover it in <a href="B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 9</em></a>, <em class="italic">Getting Started with Machine Learning in Python</em>, and <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>. In addition, we will see how this entire process will work in practice in <a href="B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Machine Learning Anomaly Detection</em>. As a reference, in the <em class="italic">Machine learning workflow</em> section in the <em class="italic">Appendix</em>, there is a workflow diagram depicting the full process from data analysis to machine learning. <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>, and <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>, will focus on drawing conclusions from data analysis, rather than building models.</p>
			<p>The next section will be a review of statistics; those with knowledge of statistics can skip ahead to the <em class="italic">Setting up a virtual environment</em> section.</p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>Statistical foundations</h1>
			<p>When we want to make observations about the data we are analyzing, we often, if not always, turn <a id="_idIndexMarker020"/>to statistics in some fashion. The data we have is referred to as the <strong class="bold">sample</strong>, which was observed from (and is a subset of) the <strong class="bold">population</strong>. Two broad categories of statistics are descriptive and inferential statistics. With <strong class="bold">descriptive statistics</strong>, as the name implies, we are looking to <em class="italic">describe</em> the sample. <strong class="bold">Inferential statistics</strong> involves using the sample statistics to <em class="italic">infer</em>, or deduce, something about the population, such as the underlying distribution.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Sample statistics are used as <strong class="bold">estimators</strong> of the population parameters, meaning that we have to quantify their bias and variance. There is a multitude of methods for this; some will make assumptions on the shape of the distribution (parametric) and others won't (non-parametric). This is all well beyond the scope of this book, but it is good to be aware of.</p>
			<p>Often, the goal of an analysis is to create a story for the data; unfortunately, it is very easy to misuse statistics. It's the subject of a famous quote:</p>
			<p class="author-quote">"There are three kinds of lies: lies, damned lies, and statistics."</p>
			<p class="author-quote">— Benjamin Disraeli</p>
			<p>This is <a id="_idIndexMarker021"/>especially true of inferential statistics, which is used in many scientific studies and papers to show the significance of the researchers' findings. This is a more advanced topic and, since this isn't a statistics book, we will only briefly touch upon some of the tools and principles behind inferential statistics, which can be pursued further. We will focus on descriptive statistics to help explain the data we are analyzing.</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>Sampling</h2>
			<p>There's an <a id="_idIndexMarker022"/>important thing to remember before we attempt any analysis: our sample must be a <strong class="bold">random sample</strong> that is representative of the population. This means that the data must be sampled without bias (for example, if we are asking people whether they like a certain sports team, we can't only ask fans of the team) and that we should have (ideally) members of all distinct groups from the population in our sample (in the sports team example, we can't just ask men).</p>
			<p>When we discuss machine learning in <a href="B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 9</em></a>, <em class="italic">Getting Started with Machine Learning in Python</em>, we will need to sample our data, which will be a sample to begin with. This is called <strong class="bold">resampling</strong>. Depending on the data, we will have to pick a different method of <a id="_idIndexMarker023"/>sampling. Often, our best bet is a <strong class="bold">simple random sample</strong>: we use a random number generator to pick rows at random. When we have distinct groups in the data, we want our sample to be a <strong class="bold">stratified random sample</strong>, which will preserve the proportion of the groups in the data. In some cases, we don't have enough data for the aforementioned sampling strategies, so we may turn to random sampling with replacement (<strong class="bold">bootstrapping</strong>); this is called a <strong class="bold">bootstrap sample</strong>. Note that our underlying sample needs to have been a random sample or we <a id="_idIndexMarker024"/>risk increasing the bias of the estimator (we could pick <a id="_idIndexMarker025"/>certain rows more often because they are in the data more often if it was a convenience sample, while in the true population these rows aren't as prevalent). We will see an example of bootstrapping in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">A thorough discussion of the theory behind bootstrapping and its consequences is well beyond the scope of this book, but watch this video for a primer: <a href="https://www.youtube.com/watch?v=gcPIyeqymOU">https://www.youtube.com/watch?v=gcPIyeqymOU</a>.</p>
			<p><a id="_idIndexMarker026"/>You can read more about sampling methods, along with their strengths and weaknesses, at <a href="https://www.khanacademy.org/math/statistics-probability/designing-studies/sampling-methods-stats/a/sampling-methods-review">https://www.khanacademy.org/math/statistics-probability/designing-studies/sampling-methods-stats/a/sampling-methods-review</a>.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Descriptive statistics</h2>
			<p>We will begin <a id="_idIndexMarker027"/>our discussion of descriptive statistics with <strong class="bold">univariate statistics</strong>; univariate <a id="_idIndexMarker028"/>simply means that these statistics are calculated from one (<strong class="bold">uni</strong>) variable. Everything in this section can be extended to <a id="_idIndexMarker029"/>the whole dataset, but the statistics will be calculated per variable we are recording (meaning that if we had 100 observations of speed and distance pairs, we could calculate the averages across the dataset, which would give us the average speed and average distance statistics).</p>
			<p>Descriptive statistics are used to describe and/or summarize the data we are working with. We can start our summarization of the data with a measure of <strong class="bold">central tendency</strong>, which describes where most of the data is centered around, and a measure of <strong class="bold">spread</strong> or <strong class="bold">dispersion</strong>, which indicates how far apart values are.</p>
			<h3>Measures of central tendency</h3>
			<p>Measures of central tendency describe the center of our distribution of data. There are three <a id="_idIndexMarker030"/>common statistics that are used <a id="_idIndexMarker031"/>as measures of center: mean, median, and mode. Each has its own strengths, depending on the data we are working with.</p>
			<h4>Mean</h4>
			<p>Perhaps the <a id="_idIndexMarker032"/>most common statistic for summarizing data <a id="_idIndexMarker033"/>is the average, or <strong class="bold">mean</strong>. The population mean is denoted by <em class="italic">μ</em> (the Greek letter <em class="italic">mu</em>), and the sample mean is written as <img src="image/B16834_01_001.png" alt=""/> (pronounced <em class="italic">X-bar</em>). The sample mean is calculated by summing all the values and dividing by the count of values; for example, the mean of the numbers 0, 1, 1, 2, and 9 is 2.6 (<strong class="source-inline">(0 + 1 + 1 + 2 + 9)/5</strong>):</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B16834_01_002.jpg" alt=""/>
				</div>
			</div>
			<p>We use <em class="italic">x</em><span class="subscript">i</span><span class="subscript"> </span>to represent the <em class="italic">i</em><span class="superscript">th</span><em class="italic"> </em>observation of the variable <em class="italic">X</em>. Note how the variable as a whole is represented with a capital letter, while the specific observation is lowercase. <em class="italic">Σ</em> (the Greek capital letter <em class="italic">sigma</em>) is used to represent a summation, which, in the equation for the mean, goes from <em class="italic">1</em> to <em class="italic">n</em>, which is the number of observations.</p>
			<p>One important thing to note about the mean is that it is very sensitive to <strong class="bold">outliers</strong> (values created by a different generative process than our distribution). In the previous example, we were dealing with only five values; nevertheless, the 9 is much larger than the other numbers and pulled the mean higher than all but the 9. In cases where we suspect outliers to be present in our data, we may want to instead use the median as our measure of central tendency.</p>
			<h4>Median</h4>
			<p>Unlike the <a id="_idIndexMarker034"/>mean, the <strong class="bold">median</strong> is robust to outliers. Consider income in the US; the <a id="_idIndexMarker035"/>top 1% is much higher than the rest of the population, so this will skew the mean to be higher and distort the perception of the average person's income. However, the median will be more representative of the average income because it is the 50<span class="superscript">th</span> percentile of our data; this means that 50% of the values are greater than the median and 50% are less than the median.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <em class="italic">i</em><span class="superscript">th</span> percentile is the value at which <em class="italic">i</em>% of the observations are less than that value, so the 99<span class="superscript">th</span> percentile is the value in <em class="italic">X</em> where 99% of the <em class="italic">x</em>'s are less than it.</p>
			<p>The median is <a id="_idIndexMarker036"/>calculated by taking the middle value <a id="_idIndexMarker037"/>from an ordered list of values; in cases where we have an even number of values, we take the mean of the middle two values. If we take the numbers 0, 1, 1, 2, and 9 again, our median is 1. Notice that the mean and median for this dataset are different; however, depending on the distribution of the data, they may be the same.</p>
			<h4>Mode</h4>
			<p>The <strong class="bold">mode</strong> is the <a id="_idIndexMarker038"/>most common value in the data (if we, once again, have the numbers 0, 1, 1, 2, and 9, then 1 is the mode). In practice, we will often hear things such as <em class="italic">the distribution is bimodal or multimodal</em> (as opposed to unimodal) in cases where the distribution <a id="_idIndexMarker039"/>has two or more most popular values. This doesn't necessarily mean that each of them occurred the same amount of times, but rather, they are more common than the other values by a significant amount. As shown in the following plots, a unimodal distribution has only one mode (at <strong class="bold">0</strong>), a bimodal distribution has two (at <strong class="bold">-2</strong> and <strong class="bold">3</strong>), and a multimodal distribution has many (at <strong class="bold">-2</strong>, <strong class="bold">0.4</strong>, and <strong class="bold">3</strong>):</p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/Figure_1.3_B16834.jpg" alt="Figure 1.3 – Visualizing the mode with continuous data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3 – Visualizing the mode with continuous data</p>
			<p>Understanding the concept of the mode comes in handy when describing continuous distributions; however, most of the time when we're describing our continuous data, we will use <a id="_idIndexMarker040"/>either the mean or the median as our measure <a id="_idIndexMarker041"/>of central tendency. When working with categorical data, on the other hand, we will typically use the mode.</p>
			<h3>Measures of spread</h3>
			<p>Knowing where the center of the distribution is only gets us partially to being able to summarize <a id="_idIndexMarker042"/>the distribution of our data—we need to know how <a id="_idIndexMarker043"/>values fall around the center and how far apart they are. Measures of spread tell us how the data is dispersed; this will indicate how thin (low dispersion) or wide (very spread out) our distribution is. As with measures of central tendency, we have several ways to describe the spread of a distribution, and which one we choose will depend on the situation and the data.</p>
			<h4>Range</h4>
			<p>The <strong class="bold">range</strong> is the <a id="_idIndexMarker044"/>distance between the smallest value (<strong class="bold">minimum</strong>) and the largest value (<strong class="bold">maximum</strong>). The units of the range will be the same units as <a id="_idIndexMarker045"/>our data. Therefore, unless two distributions of data are in the same units and measuring the same thing, we can't compare their ranges and say one is more dispersed than the other:</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B16834_01_003.jpg" alt=""/>
				</div>
			</div>
			<p>Just from the definition of the range, we can see why it wouldn't always be the best way to measure the spread of our data. It gives us upper and lower bounds on what we have in the data; however, if we have any outliers in our data, the range will be rendered useless.</p>
			<p>Another problem with the range is that it doesn't tell us how the data is dispersed around its center; it really only tells us how dispersed the entire dataset is. This brings us to the variance.</p>
			<h4>Variance</h4>
			<p>The <strong class="bold">variance</strong> describes how far apart observations are spread out from their average value (the mean). The population variance is denoted as <em class="italic">σ</em><span class="superscript">2</span> (pronounced <em class="italic">sigma-squared</em>), and the <a id="_idIndexMarker046"/>sample variance is written as <em class="italic">s</em><span class="superscript">2</span>. It is calculated <a id="_idIndexMarker047"/>as the average squared distance from the mean. Note that the distances must be squared so that distances below the mean don't cancel out those above the mean.</p>
			<p>If we want the sample variance to be an unbiased estimator of the population variance, we divide by <em class="italic">n - 1</em> instead of <em class="italic">n</em> to account for using the sample mean instead of the <a id="_idIndexMarker048"/>population mean; this is called Bessel's correction (<a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">https://en.wikipedia.org/wiki/Bessel%27s_correction</a>). Most statistical tools will give us the sample variance by default, since it is <em class="italic">very</em> rare that we would have data for the entire population:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B16834_01_004.jpg" alt=""/>
				</div>
			</div>
			<p>The variance gives us a statistic with <em class="italic">squared</em> units. This means that if we started with data on income in dollars ($), then our variance would be in dollars squared ($<span class="superscript">2</span>). This isn't really useful when we're trying to see how this describes the data; we can use the <strong class="bold">magnitude</strong> (size) itself to see how spread out something is (large values = large spread), but beyond that, we need a measure of spread with units that are the same as our data. For this purpose, we use the standard deviation.</p>
			<h4>Standard deviation</h4>
			<p>We can use the <strong class="bold">standard deviation</strong> to see how far from the mean data points are <em class="italic">on average</em>. A small <a id="_idIndexMarker049"/>standard deviation means that <a id="_idIndexMarker050"/>values are close to the mean, while a large standard deviation means that values are dispersed more widely. This is tied to how we would imagine the distribution curve: the smaller the standard deviation, the thinner the peak of the curve (<strong class="bold">0.5</strong>); the larger the standard deviation, the wider the peak of the curve (<strong class="bold">2</strong>):</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/Figure_1.4_B16834.jpg" alt="Figure 1.4 – Using standard deviation to quantify the spread of a distribution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4 – Using standard deviation to quantify the spread of a distribution</p>
			<p>The standard <a id="_idIndexMarker051"/>deviation is simply the square root of <a id="_idIndexMarker052"/>the variance. By performing this operation, we get a statistic in units that we can make sense of again ($ for our income example):</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B16834_01_005.jpg" alt=""/>
				</div>
			</div>
			<p>Note that the population standard deviation is represented as <em class="italic">σ</em>, and the sample standard deviation is denoted as <em class="italic">s</em>.</p>
			<h4>Coefficient of variation</h4>
			<p>When we moved from variance to standard deviation, we were looking to get to units that made sense; however, if we then want to compare the level of dispersion of one dataset <a id="_idIndexMarker053"/>to another, we would need <a id="_idIndexMarker054"/>to have the same units once again. One way around this is to calculate the <strong class="bold">coefficient of variation</strong> (<strong class="bold">CV</strong>), which is unitless. The CV is the ratio of the standard deviation to the mean:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B16834_01_006.jpg" alt=""/>
				</div>
			</div>
			<p>We will use this metric in <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>; since the CV is unitless, we can use it to compare the volatility of different assets.</p>
			<h4>Interquartile range</h4>
			<p>So far, other than the range, we have discussed mean-based measures of dispersion; now, we will <a id="_idIndexMarker055"/>look at how we can describe the spread with the <a id="_idIndexMarker056"/>median as our measure of central tendency. As mentioned earlier, the median is the 50<span class="superscript">th</span> percentile or the 2<span class="superscript">nd</span> <strong class="bold">quartile</strong> (Q<span class="subscript">2</span>). Percentiles and quartiles are both <strong class="bold">quantiles</strong>—values that divide data into equal groups each containing the same percentage of the total data. Percentiles divide the data into 100 parts, while quartiles do so into four (25%, 50%, 75%, and 100%).</p>
			<p>Since quantiles neatly divide up our data, and we know how much of the data goes in each section, they are a perfect candidate for helping us quantify the spread of our data. One common measure for this is the <strong class="bold">interquartile range</strong> (<strong class="bold">IQR</strong>), which is the distance between the 3<span class="superscript">rd</span> and 1<span class="superscript">st</span> quartiles:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B16834_01_007.jpg" alt=""/>
				</div>
			</div>
			<p>The IQR gives us the spread of data around the median <em class="italic">and</em> quantifies how much dispersion we have in the middle 50% of our distribution. It can also be useful when checking the data for outliers, which we will cover in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>. In addition, the IQR can be used to calculate a unitless measure of dispersion, which we will discuss next.</p>
			<h4>Quartile coefficient of dispersion</h4>
			<p>Just <a id="_idIndexMarker057"/>like we had the coefficient of <a id="_idIndexMarker058"/>variation when using the mean as our measure of central tendency, we have the <strong class="bold">quartile coefficient of dispersion</strong> when using the median as our measure of center. This statistic is also unitless, so it can be used to compare datasets. It is calculated by dividing the <strong class="bold">semi-quartile range</strong> (half the IQR) by the <strong class="bold">midhinge</strong> (midpoint between the first and third quartiles):</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B16834_01_008.jpg" alt=""/>
				</div>
			</div>
			<p>We will <a id="_idIndexMarker059"/>see this metric again in <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>, when we assess stock volatility. For now, let's take a look at how we can use measures of central tendency and dispersion to summarize our data.</p>
			<h3>Summarizing data</h3>
			<p>We have seen many examples of descriptive statistics that we can use to summarize our data by <a id="_idIndexMarker060"/>its center and dispersion; in practice, looking <a id="_idIndexMarker061"/>at the <strong class="bold">5-number summary</strong> and visualizing the distribution prove to be helpful <a id="_idIndexMarker062"/>first steps before diving into some of the other aforementioned metrics. The 5-number summary, as its name indicates, provides five descriptive statistics that summarize our data:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/Figure_1.5_B16834.jpg" alt="Figure 1.5 – The 5-number summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.5 – The 5-number summary</p>
			<p>A <strong class="bold">box plot</strong> (or box and whisker plot) is <a id="_idIndexMarker063"/>a visual representation of the 5-number summary. The median is denoted by a thick line in the box. The top of the box is Q<span class="subscript">3</span> and the bottom of the box is Q<span class="subscript">1</span>. Lines (whiskers) extend from both sides of the box boundaries toward the minimum and maximum. Based on the convention our plotting tool uses, though, they may only extend to a certain statistic; any values beyond these statistics are marked as outliers (using points). For this book in general, the lower bound of the <a id="_idIndexMarker064"/>whiskers will be <strong class="bold">Q</strong><span class="subscript">1</span><strong class="bold"> – 1.5 * IQR</strong> and the upper bound will be <strong class="bold">Q</strong><span class="subscript">3</span><strong class="bold"> + 1.5 * IQR</strong>, which is called the <strong class="bold">Tukey box plot</strong>:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/Figure_1.6_B16834.jpg" alt="Figure 1.6 – The Tukey box plot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6 – The Tukey box plot</p>
			<p>While the box plot is a great tool for getting an initial understanding of the distribution, we don't get <a id="_idIndexMarker065"/>to see how things are distributed inside each of the quartiles. For this purpose, we turn to <strong class="bold">histograms</strong> for <strong class="bold">discrete</strong> variables (for instance, the number of people or books) and <strong class="bold">kernel density estimates</strong> (<strong class="bold">KDEs</strong>) for <strong class="bold">continuous</strong> variables (for instance, heights or time). There is nothing stopping us from using KDEs on discrete variables, but it is easy to confuse people that way. Histograms <a id="_idIndexMarker066"/>work for both discrete and continuous <a id="_idIndexMarker067"/>variables; however, in both cases, we must keep in mind that the number of bins we choose to divide the data into can easily change the shape of the distribution we see.</p>
			<p>To make a histogram, a certain number of equal-width bins are created, and then bars with heights for the number of values we have in each bin are added. The following plot is a histogram with 10 bins, showing the three measures of central tendency for the same data that was used to generate the box plot in <em class="italic">Figure 1.6</em>:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/Figure_1.7_B16834.jpg" alt="Figure 1.7 – Example histogram&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7 – Example histogram</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In practice, we need to play around with the number of bins to find the best value. However, we have to be careful as this can misrepresent the shape of the distribution.</p>
			<p>KDEs are <a id="_idIndexMarker068"/>similar to histograms, except rather than creating bins for <a id="_idIndexMarker069"/>the data, they draw a smoothed curve, which <a id="_idIndexMarker070"/>is an estimate of the distribution's <strong class="bold">probability density function</strong> (<strong class="bold">PDF</strong>). The PDF is for continuous variables and tells us how probability is distributed over the values. Higher values for the PDF indicate higher likelihoods:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/Figure_1.8_B16834.jpg" alt="Figure 1.8 – KDE with marked measures of center&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8 – KDE with marked measures of center</p>
			<p>When the distribution starts to get a little lopsided with long tails on one side, the mean measure of center can easily get pulled to that side. Distributions that aren't symmetric have some <strong class="bold">skew</strong> to them. A <strong class="bold">left (negative) skewed distribution</strong> has a long tail on the left-hand side; a <strong class="bold">right (positive) skewed distribution</strong> has a long tail on the right-hand side. In the presence of negative skew, the mean will be less than the median, while the reverse happens with a positive skew. When there is no skew, both will be equal:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/Figure_1.9_B16834.jpg" alt="Figure 1.9 – Visualizing skew&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.9 – Visualizing skew</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">There is also <a id="_idIndexMarker071"/>another statistic called <strong class="bold">kurtosis</strong>, which compares the density of the center of the distribution with the density at the tails. Both skewness and kurtosis can be calculated with the SciPy package.</p>
			<p>Each column <a id="_idIndexMarker072"/>in our data is a <strong class="bold">random variable</strong>, because every time <a id="_idIndexMarker073"/>we observe it, we get a value according to <a id="_idIndexMarker074"/>the underlying distribution—it's not static. When we are <a id="_idIndexMarker075"/>interested in the probability of getting a value of <em class="italic">x</em> or less, we use the <strong class="bold">cumulative distribution function</strong> (<strong class="bold">CDF</strong>), which is the integral (area under the curve) of the PDF:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/Formula_01_009.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/Formula_01_010.jpg" alt=""/>
				</div>
			</div>
			<p>The probability of the random variable <em class="italic">X</em> being less than or equal to the specific value of <em class="italic">x</em> is denoted as <em class="italic">P(X ≤ x)</em>. With a continuous variable, the probability of getting exactly <em class="italic">x</em> is 0. This is because the probability will be the integral of the PDF from <em class="italic">x</em> to <em class="italic">x</em> (area under a curve with zero width), which is 0:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/Formula_01_011.jpg" alt=""/>
				</div>
			</div>
			<p>In order <a id="_idIndexMarker076"/>to visualize this, we can find an estimate of the CDF from <a id="_idIndexMarker077"/>the sample, called the <strong class="bold">empirical cumulative distribution function</strong> (<strong class="bold">ECDF</strong>). Since this is cumulative, at the point <a id="_idIndexMarker078"/>where the value on the <em class="italic">x</em>-axis is equal to <em class="italic">x</em>, the <em class="italic">y</em> value is the cumulative probability of <em class="italic">P(X ≤ x)</em>. Let's visualize <strong class="bold">P(X ≤ 50)</strong>, <strong class="bold">P(X = 50)</strong>, and <strong class="bold">P(X &gt; 50)</strong> as an example:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/Figure_1.10_B16834.jpg" alt="Figure 1.10 – Visualizing the CDF&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.10 – Visualizing the CDF</p>
			<p>In addition to examining the distribution of our data, we may find the need to utilize probability distributions for uses such as simulation (discussed in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>) or hypothesis testing (see the <em class="italic">Inferential statistics</em> section); let's take a look at a few distributions that we are likely to come across.</p>
			<h3>Common distributions</h3>
			<p>While there <a id="_idIndexMarker079"/>are many probability distributions, each <a id="_idIndexMarker080"/>with specific use cases, there are some that we will come across often. The <strong class="bold">Gaussian</strong>, or <strong class="bold">normal</strong>, looks like a bell curve and is parameterized by its mean (<em class="italic">μ</em>) and standard deviation (<em class="italic">σ</em>). The <strong class="bold">standard normal</strong> (<em class="italic">Z</em>) has a mean of 0 and a standard deviation of 1. Many things in nature happen to follow the normal distribution, such as heights. Note that testing whether a distribution is normal is not trivial—check the <em class="italic">Further reading</em> section for more information.</p>
			<p>The <strong class="bold">Poisson distribution</strong> is a discrete distribution that is often used to model arrivals. The <a id="_idIndexMarker081"/>time between arrivals can be modeled with the <strong class="bold">exponential distribution</strong>. Both are defined by their mean, lambda (<em class="italic">λ</em>). The <strong class="bold">uniform distribution</strong> places equal likelihood on each value within its bounds. We often use <a id="_idIndexMarker082"/>this for random number generation. When we generate a <a id="_idIndexMarker083"/>random number to simulate a single success/failure outcome, it is called a <strong class="bold">Bernoulli trial</strong>. This is parameterized by the probability <a id="_idIndexMarker084"/>of success (<em class="italic">p</em>). When we run the same experiment multiple times (<em class="italic">n</em>), the total number of successes is then a <strong class="bold">binomial</strong> random variable. Both the Bernoulli and binomial distributions are discrete.</p>
			<p>We can <a id="_idIndexMarker085"/>visualize both discrete and continuous distributions; however, discrete distributions give us a <strong class="bold">probability mass function</strong> (<strong class="bold">PMF</strong>) instead of a PDF:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/Figure_1.11_B16834.jpg" alt="Figure 1.11 – Visualizing some commonly used distributions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.11 – Visualizing some commonly used distributions</p>
			<p>We will <a id="_idIndexMarker086"/>use some of these distributions <a id="_idIndexMarker087"/>in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>, when we simulate some login attempt data for anomaly detection.</p>
			<h3>Scaling data</h3>
			<p>In order to <a id="_idIndexMarker088"/>compare variables from different distributions, we would have <a id="_idIndexMarker089"/>to <strong class="bold">scale</strong> the data, which we could do with the range by using <strong class="bold">min-max scaling</strong>. We <a id="_idIndexMarker090"/>take <em class="italic">each</em> data point, subtract the minimum of the dataset, then divide by the range. This <strong class="bold">normalizes</strong> our data (scales it to the range [0, 1]):</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/Formula_01_012.jpg" alt=""/>
				</div>
			</div>
			<p>This isn't the only way to scale data; we can also use the mean and standard deviation. In this case, we <a id="_idIndexMarker091"/>would subtract the mean from each observation and then divide by the standard deviation to <strong class="bold">standardize</strong> the data. This gives <a id="_idIndexMarker092"/>us what is known as a <strong class="bold">Z-score</strong>:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/Formula_01_013.jpg" alt=""/>
				</div>
			</div>
			<p>We are left <a id="_idIndexMarker093"/>with a normalized distribution with a mean of 0 and a standard deviation (and variance) of 1. The Z-score tells us how many standard deviations from the mean each observation is; the mean has a Z-score of 0, while an observation of 0.5 standard deviations below the mean will have a Z-score of -0.5.</p>
			<p>There are, of course, additional ways to scale our data, and the one we end up choosing will be dependent on our data and what we are trying to do with it. By keeping the measures of central tendency and measures of dispersion in mind, you will be able to identify how the scaling of data is being done in any other methods you come across.</p>
			<h3>Quantifying relationships between variables</h3>
			<p>In the previous sections, we were dealing with univariate statistics and were only able to say something <a id="_idIndexMarker094"/>about the variable we were looking at. With multivariate statistics, we seek to quantify relationships between variables and attempt to make predictions for future behavior.</p>
			<p>The <strong class="bold">covariance</strong> is a statistic <a id="_idIndexMarker095"/>for quantifying the relationship between variables by showing how one variable changes with respect to another (also referred to as their joint variance):</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/Formula_01_014.jpg" alt=""/>
				</div>
			</div>
			<p class="callout-heading">Important note</p>
			<p class="callout"><em class="italic">E[X]</em> is a new notation for us. It is read as <em class="italic">the expected value of X</em> or <em class="italic">the expectation of X</em>, and it is calculated by summing all the possible values of <em class="italic">X</em> multiplied by their probability—it's the long-run average of <em class="italic">X</em>.</p>
			<p>The magnitude of the covariance isn't easy to interpret, but its sign tells us whether the variables are <a id="_idIndexMarker096"/>positively or negatively <a id="_idIndexMarker097"/>correlated. However, we would also like to quantify how <em class="italic">strong</em> the relationship <a id="_idIndexMarker098"/>is between the variables, which brings us to correlation. <strong class="bold">Correlation</strong> tells us how variables change together <a id="_idIndexMarker099"/>both in direction (same or opposite) and magnitude (strength of the relationship). To find the correlation, we calculate the <strong class="bold">Pearson correlation coefficient</strong>, symbolized by <em class="italic">ρ</em> (the Greek letter <em class="italic">rho</em>), by dividing the covariance by the product of the standard deviations of the variables:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/Formula_01_015.jpg" alt=""/>
				</div>
			</div>
			<p>This normalizes the covariance and results in a statistic bounded between -1 and 1, making it easy to describe both the direction of the correlation (sign) and the strength of it (magnitude). Correlations of 1 are said to be perfect positive (linear) correlations, while those of -1 are perfect negative correlations. Values near 0 aren't correlated. If correlation coefficients are near 1 in absolute value, then the variables are said to be strongly correlated; those closer to 0.5 are said to be weakly correlated.</p>
			<p>Let's look at some examples using scatter plots. In the leftmost subplot of <em class="italic">Figure 1.12</em> (<strong class="bold">ρ = 0.11</strong>), we see that there is no correlation between the variables: they appear to be random noise with no pattern. The next plot with <strong class="bold">ρ = -0.52</strong> has a weak negative correlation: we can see that the variables appear to move together with the <em class="italic">x</em> variable increasing, while the <em class="italic">y</em> variable decreases, but there is still a bit of randomness. In the third plot from the left (<strong class="bold">ρ = 0.87</strong>), there is a strong positive correlation: <em class="italic">x</em> and <em class="italic">y</em> are increasing together. The rightmost plot with <strong class="bold">ρ = -0.99</strong> has a near-perfect negative correlation: as <em class="italic">x</em> increases, <em class="italic">y</em> decreases. We can also see how the points form a line:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/Figure_1.12_B16834.jpg" alt="Figure 1.12 – Comparing correlation coefficients&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.12 – Comparing correlation coefficients</p>
			<p>To quickly <a id="_idIndexMarker100"/>eyeball the strength <a id="_idIndexMarker101"/>and direction of the relationship between two variables (and see whether there even seems to be one), we will often use scatter plots rather than calculating the exact correlation coefficient. This is for a couple of reasons:</p>
			<ul>
				<li>It's easier to find patterns in visualizations, but it's more work to arrive at the same conclusion by looking at numbers and tables.</li>
				<li>We might see that the variables seem related, but they may not be <em class="italic">linearly</em> related. Looking at a visual representation will make it easy to see if our data is actually quadratic, exponential, logarithmic, or some other non-linear function.</li>
			</ul>
			<p>Both of the following plots depict data with strong positive correlations, but it's pretty obvious when looking at the scatter plots that these are not linear. The one on the left is logarithmic, while the one on the right is exponential:</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/Figure_1.13_B16834.jpg" alt="Figure 1.13 – The correlation coefficient can be misleading&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.13 – The correlation coefficient can be misleading</p>
			<p>It's very important to remember that while we may find a correlation between <em class="italic">X</em> and <em class="italic">Y</em>, it doesn't mean that <em class="italic">X causes Y</em> or that <em class="italic">Y causes X</em>. There could be some <em class="italic">Z</em> that actually causes both; perhaps <em class="italic">X</em> causes some intermediary event that causes <em class="italic">Y</em>, or it <a id="_idIndexMarker102"/>is actually just a coincidence. Keep in mind <a id="_idIndexMarker103"/>that we often don't have enough information to report causation—<em class="italic">correlation does not imply causation</em>.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Be sure to check out Tyler Vigen's <em class="italic">Spurious Correlations</em> blog (<a href="https://www.tylervigen.com/spurious-correlations">https://www.tylervigen.com/spurious-correlations</a>) for some interesting correlations.</p>
			<h3>Pitfalls of summary statistics</h3>
			<p>There is a very interesting dataset illustrating how careful we must be when only using summary <a id="_idIndexMarker104"/>statistics and correlation coefficients <a id="_idIndexMarker105"/>to describe our data. It also shows us that plotting is not optional. <strong class="bold">Anscombe's quartet</strong> is a collection of four different datasets <a id="_idIndexMarker106"/>that have identical summary statistics and correlation coefficients, but when plotted, it is obvious they are not similar:</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/Figure_1.14_B16834.jpg" alt="Figure 1.14 – Summary statistics can be misleading&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.14 – Summary statistics can be misleading</p>
			<p>Notice <a id="_idIndexMarker107"/>that each of the plots in <em class="italic">Figure 1.14</em> has an identical <a id="_idIndexMarker108"/>best-fit line defined by the equation <strong class="bold">y = 0.50x + 3.00</strong>. In the next section, we will discuss, at a high level, how this line is created and what it means.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Summary statistics are very helpful when we're getting to know the data, but be wary of relying exclusively on them. Remember, statistics can be misleading; be sure to also plot the data before drawing any conclusions or proceeding with the analysis. You can read more <a id="_idIndexMarker109"/>about Anscombe's quartet at <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">https://en.wikipedia.org/wiki/Anscombe%27s_quartet</a>. Also, be sure to check out the <strong class="bold">Datasaurus Dozen</strong>, which are 13 datasets that also have the same summary statistics, at <a href="https://www.autodeskresearch.com/publications/samestats">https://www.autodeskresearch.com/publications/samestats</a>.</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Prediction and forecasting</h2>
			<p>Say our favorite ice cream shop has asked us to help predict how many ice creams they can expect to sell <a id="_idIndexMarker110"/>on a given day. They are convinced that the temperature outside has a strong influence on their sales, so they have collected data on the number of ice creams sold at a given temperature. We agree to help them, and the first thing we do is make a scatter plot of the data they collected:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/Figure_1.15_B16834.jpg" alt="Figure 1.15 – Observations of ice cream sales at various temperatures&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.15 – Observations of ice cream sales at various temperatures</p>
			<p>We can observe an upward trend in the scatter plot: more ice creams are sold at higher temperatures. In order to help out the ice cream shop, though, we need to find a way to make predictions <a id="_idIndexMarker111"/>from this data. We can use a technique called <strong class="bold">regression</strong> to model the relationship between temperature and ice cream sales with an equation. Using this equation, we will be able to <strong class="bold">predict</strong> ice cream sales at a given temperature.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Remember that correlation does not imply causation. People may buy ice cream when it is warmer, but warmer temperatures don't necessarily cause people to buy ice cream.</p>
			<p>In <a href="B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 9</em></a>, <em class="italic">Getting Started with Machine Learning in Python</em>, we will go over regression in depth, so this discussion will be a high-level overview. There are many types of regression that will yield a different type of equation, such as linear (which we will use for this example) and logistic. Our first step will be to identify the <strong class="bold">dependent variable</strong>, which is <a id="_idIndexMarker112"/>the quantity we want to predict (ice cream sales), and the variables we will use to predict it, which are called <strong class="bold">independent variables</strong>. While <a id="_idIndexMarker113"/>we can have many independent variables, our ice <a id="_idIndexMarker114"/>cream sales example only has one: temperature. Therefore, we will use simple linear regression to model the relationship as a line:</p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/Figure_1.16_B16834.jpg" alt="Figure 1.16 – Fitting a line to the ice cream sales data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.16 – Fitting a line to the ice cream sales data</p>
			<p>The regression line in the previous scatter plot yields the following equation for the relationship:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/Formula_01_016.jpg" alt=""/>
				</div>
			</div>
			<p>Suppose that today the temperature is 35°C—we would plug that in for <em class="italic">temperature</em> in the equation. The result predicts that the ice cream shop will sell 24.54 ice creams. This prediction is along the red line in the previous plot. Note that the ice cream shop can't actually sell fractions of ice cream.</p>
			<p>Before leaving the model in the hands of the ice cream shop, it's important to discuss the difference between the dotted and solid portions of the regression line that we obtained. When we make predictions using the solid portion of the line, we are using <strong class="bold">interpolation</strong>, meaning <a id="_idIndexMarker115"/>that we will be predicting ice cream sales for temperatures the regression was created on. On the other hand, if we try to predict how many <a id="_idIndexMarker116"/>ice creams will be sold at 45°C, it is called <strong class="bold">extrapolation</strong> (the dotted portion of the line), since we didn't have any temperatures this high when we ran the regression. Extrapolation can be very dangerous as many trends don't continue indefinitely. People may decide not to leave their houses <a id="_idIndexMarker117"/>because it is so hot. This means that instead of selling the predicted 39.54 ice creams, they would sell zero.</p>
			<p>When working with time series, our terminology is a little different: we often look to <strong class="bold">forecast</strong> future values based on past values. Forecasting is a type of prediction for time series. Before we <a id="_idIndexMarker118"/>try to model the time series, however, we will often use a process called <strong class="bold">time series decomposition</strong> to split the time series into components, which can be combined in an additive or multiplicative fashion and may be used as parts of a model.</p>
			<p>The <strong class="bold">trend</strong> component describes the behavior of the time series in the <strong class="bold">long term</strong> without <a id="_idIndexMarker119"/>accounting for seasonal or cyclical effects. Using the trend, we can make broad statements about the time series in the long run, such as <em class="italic">the population of Earth is increasing</em> or <em class="italic">the value of a stock is stagnating</em>. The <strong class="bold">seasonality</strong> component explains <a id="_idIndexMarker120"/>the systematic and calendar-related movements of a time series. For example, the number of ice cream trucks on the streets of New York City is high in the summer and drops to nothing in the winter; this <a id="_idIndexMarker121"/>pattern repeats every year, regardless of whether the actual amount each summer is the same. Lastly, the <strong class="bold">cyclical</strong> component accounts for anything else unexplained or irregular with the time series; this could be something such as a hurricane driving the number of ice cream trucks down in the <strong class="bold">short term</strong> because it isn't safe to be outside. This component is difficult to anticipate with a forecast due to its unexpected nature.</p>
			<p>We can use Python to <strong class="bold">decompose</strong> the time series into trend, seasonality, and <strong class="bold">noise</strong> or <strong class="bold">residuals</strong>. The cyclical component is captured in the noise (random, unpredictable data); after we remove the trend and seasonality from the time series, what we are left with is the residual:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/Figure_1.17_B16834.jpg" alt="Figure 1.17 – An example of time series decomposition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.17 – An example of time series decomposition</p>
			<p>When building <a id="_idIndexMarker122"/>models to forecast <a id="_idIndexMarker123"/>time series, some common methods include exponential smoothing and ARIMA-family models. <strong class="bold">ARIMA</strong> stands for <strong class="bold">autoregressive</strong> (<strong class="bold">AR</strong>), <strong class="bold">integrated</strong> (<strong class="bold">I</strong>), <strong class="bold">moving average</strong> (<strong class="bold">MA</strong>). <strong class="bold">Autoregressive</strong> models take advantage of the fact that an observation at time <em class="italic">t</em> is <em class="italic">correlated</em> to a previous observation, for example, at time <em class="italic">t - 1</em>. In <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>, we will look at some techniques for determining whether a time series is autoregressive; note that not all time series are. The <strong class="bold">integrated</strong> component concerns the <strong class="bold">differenced</strong> data, or the <em class="italic">change</em> in the data from one time to another. For example, if we were concerned with a <strong class="bold">lag</strong> (distance between times) of 1, the differenced data would be the value at time <em class="italic">t</em> subtracted by the value at time <em class="italic">t - 1</em>. Lastly, the <strong class="bold">moving average</strong> component uses a sliding window to average the last <em class="italic">x</em> observations, where <em class="italic">x</em> is the length of the sliding window. If, for example, we have a 3-period moving average, by the time we have all of the data up to time 5, our moving average calculation only uses time periods 3, 4, and 5 to forecast time 6. We will build an ARIMA model in <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>.</p>
			<p>The moving <a id="_idIndexMarker124"/>average puts equal weight on each time period in the past involved in the calculation. In practice, this isn't always a realistic expectation of our data. Sometimes, <em class="italic">all</em> past values are important, but they <em class="italic">vary</em> in their influence on future data points. For these cases, we can use <strong class="bold">exponential smoothing</strong>, which allows us to put more weight on more recent values and less weight on values further away from what we are predicting.</p>
			<p>Note that we aren't limited to predicting numbers; in fact, depending on the data, our predictions could be categorical in nature—things such as determining which flavor of ice cream will sell the most on a given day or whether an email is spam or not. This type of prediction will be introduced in <a href="B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 9</em></a>, <em class="italic">Getting Started with Machine Learning in Python</em>.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Inferential statistics</h2>
			<p>As mentioned earlier, inferential statistics deals with inferring or deducing things from the sample <a id="_idIndexMarker125"/>data we have in order to make statements about the population as a whole. When we're looking to state our conclusions, we have to be mindful of whether <a id="_idIndexMarker126"/>we conducted an observational study or an experiment. With an <strong class="bold">observational study</strong>, the independent variable is not under the control of the researchers, and so we are <em class="italic">observing</em> those taking part in our study (think about studies on smoking—we can't force people to smoke). The fact that we can't control the independent variable means that we <em class="italic">cannot</em> conclude causation.</p>
			<p>With an <strong class="bold">experiment</strong>, we are able to directly influence the independent variable and randomly assign subjects to the control and test groups, such as A/B tests (for anything from website redesigns to ad copy). Note that the control group doesn't receive treatment; they can be given a placebo (depending on what the study is). The ideal setup for this is <strong class="bold">double-blind</strong>, where the researchers administering the treatment don't know which treatment is the placebo and also don't know which subject belongs to which group.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We can often find reference to Bayesian inference and frequentist inference. These are based on two different ways of approaching probability. Frequentist statistics focuses on the frequency of the event, while Bayesian statistics uses a degree of belief when determining the probability of an event. We will see an example of Bayesian statistics in <a href="B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Machine Learning Anomaly Detection</em>. You can read more about how these methods differ at <a href="https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/">https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/</a>.</p>
			<p>Inferential statistics gives us tools to translate our understanding of the sample data to a statement <a id="_idIndexMarker127"/>about the population. Remember that <a id="_idIndexMarker128"/>the sample statistics we discussed earlier are estimators for the population parameters. Our estimators need <strong class="bold">confidence intervals</strong>, which provide a point estimate and a margin of error around it. This is the range that the true population parameter will be in at a certain <strong class="bold">confidence level</strong>. At the 95% confidence level, 95% of the confidence intervals that are calculated from random samples of the population contain the true population parameter. Frequently, 95% is chosen for the confidence level and other purposes in statistics, although 90% and 99% are also common; the higher the confidence level, the wider the interval.</p>
			<p>Hypothesis tests allow us to test whether the true population parameter is less than, greater than, or not <a id="_idIndexMarker129"/>equal to some value at a certain <strong class="bold">significance level</strong> (called <strong class="bold">alpha</strong>). The process of performing a hypothesis test starts with stating our initial assumption or <strong class="bold">null hypothesis</strong>: for example, <em class="italic">the true population mean is 0</em>. We pick a level of statistical significance, usually 5%, which is the probability of rejecting the null hypothesis when it is true. Then, we calculate the critical value for the test statistic, which will depend on the amount of data we have and the type of statistic (such as the mean of one population or the proportion of votes for a candidate) we are testing. The critical value is compared to the test statistic from our data and, based on the result, we either reject or fail to reject the null hypothesis. Hypothesis tests are closely related to confidence intervals. The significance level is equivalent to 1 minus the confidence level. This means that a result is statistically significant if the null hypothesis value is not in the confidence interval.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">There are many things we have to be aware of when picking the method to calculate a confidence interval or the proper test statistic for a hypothesis test. This is beyond the scope of this book, but check out the link in the <em class="italic">Further reading</em> section at the end of this chapter for more information. Also, be sure to look at some of the mishaps with the p-values used in hypothesis testing, such as p-hacking, at <a href="https://en.wikipedia.org/wiki/Misuse_of_p-values">https://en.wikipedia.org/wiki/Misuse_of_p-values</a>.</p>
			<p>Now that <a id="_idIndexMarker130"/>we have an overview of statistics and <a id="_idIndexMarker131"/>data analysis, we are ready to get started with the Python portion of this book. Let's start by setting up a virtual environment.</p>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>Setting up a virtual environment</h1>
			<p>This book was written using Python 3.7.3, but the code should work for Python 3.7.1+, which is available on all major operating systems. In this section, we will go over how to set up the <a id="_idIndexMarker132"/>virtual environment in order to follow along with this book. If Python isn't already installed on your computer, read through the following sections on virtual environments first, and then decide whether to install Anaconda, since it will also install Python. To install Python without Anaconda, download it from <a href="https://www.python.org/downloads/">https://www.python.org/downloads/</a>, and then follow the <em class="italic">venv</em> section instead of the <em class="italic">conda</em> section.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">To check whether Python is already installed, run <strong class="source-inline">where python3</strong> from the command line on Windows or <strong class="source-inline">which python3</strong> from the command line on Linux/macOS. If this returns nothing, try running it with just <strong class="source-inline">python</strong> (instead of <strong class="source-inline">python3</strong>). If Python is installed, check the version by running <strong class="source-inline">python3 --version</strong>. Note that if <strong class="source-inline">python3</strong> works, then you should use that throughout the book (and conversely, use <strong class="source-inline">python</strong> if <strong class="source-inline">python3</strong> doesn't work).</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Virtual environments</h2>
			<p>Most of the time, when <a id="_idIndexMarker133"/>we want to install software on our computer, we simply download it, but the nature of programming languages where packages are constantly being updated and rely on specific versions of others means this can cause issues. We could be working on a project one day where we need a certain version of a Python package (say 0.9.1), but the next day be working on an analysis where we need the most recent version of that same package to access some newer functionality (1.1.0). Sounds like there wouldn't be an issue, right? Well, what happens if this update causes a breaking change to the first project or another package in our project that relies on this one? This is a common enough problem that a solution already exists to prevent this from being an issue: virtual environments.</p>
			<p>A <strong class="bold">virtual environment</strong> allows us to create separate environments for each of our projects. Each of our environments will only have the packages that it needs installed. This makes it <a id="_idIndexMarker134"/>easy to share our environment with others, have multiple versions of the same package installed on our machine for different projects without interfering with each other, and avoid unexpected side effects from installing packages that update or have dependencies on others. It's good practice to make a dedicated virtual environment for any projects we work on.</p>
			<p>We will discuss two common ways to achieve this setup, and you can decide which fits best. Note that all the code in this section will be executed on the command line.</p>
			<h3>venv</h3>
			<p>Python 3 comes with the <strong class="source-inline">venv</strong> module, which will create a virtual environment in the location of <a id="_idIndexMarker135"/>our choice. The process of setting up and <a id="_idIndexMarker136"/>using a development environment is as follows (after Python is installed):</p>
			<ol>
				<li>Create a folder for the project.</li>
				<li>Use <strong class="source-inline">venv</strong> to create an environment in this folder.</li>
				<li>Activate the environment.</li>
				<li>Install Python packages in the environment with <strong class="source-inline">pip</strong>.</li>
				<li>Deactivate the environment when finished.</li>
			</ol>
			<p>In practice, we will create environments for each project we work on, so our first step will be to create a directory for all of our project files. For this, we can use the <strong class="source-inline">mkdir</strong> command. Once this has been created, we will change our current directory to the newly created one using the <strong class="source-inline">cd</strong> command. Since we already obtained the project files (from the instructions in the <em class="italic">Chapter materials</em> section), the following is for reference only. To make a new directory and move to that directory, we can use the following command:</p>
			<p class="source-code">$ mkdir my_project &amp;&amp; cd my_project</p>
			<p class="callout-heading">Tip</p>
			<p class="callout"><strong class="source-inline">cd &lt;path&gt;</strong> changes the current directory to the path specified in <strong class="source-inline">&lt;path&gt;</strong>, which can be an <strong class="bold">absolute</strong> (full) path or <strong class="bold">relative</strong> (how to get there from the current directory) path.</p>
			<p>Before <a id="_idIndexMarker137"/>moving on, use <strong class="source-inline">cd</strong> to navigate to the directory containing <a id="_idIndexMarker138"/>this book's repository. Note that the path will depend on where it was cloned/downloaded:</p>
			<p class="source-code">$ cd path/to/Hands-On-Data-Analysis-with-Pandas-2nd-edition</p>
			<p>Since there are slight differences in operating systems for the remaining steps, we will go over Windows and Linux/macOS separately. Note that if you have both Python 2 and Python 3, make sure you use <strong class="source-inline">python3</strong> and not <strong class="source-inline">python</strong> in the following commands.</p>
			<h4>Windows</h4>
			<p>To create <a id="_idIndexMarker139"/>our environment for this book, we will use the <strong class="source-inline">venv</strong> module from the standard library. Note that we must provide a name for our environment (<strong class="source-inline">book_env</strong>). Remember, if your Windows setup has <strong class="source-inline">python</strong> associated with Python 3, then use <strong class="source-inline">python</strong> instead of <strong class="source-inline">python3</strong> in the following command:</p>
			<p class="source-code">C:\...&gt; python3 -m venv book_env</p>
			<p>Now, we have a folder for our virtual environment named <strong class="source-inline">book_env</strong> inside the repository folder that we cloned/downloaded earlier. In order to use the environment, we need to activate it:</p>
			<p class="source-code">C:\...&gt; %cd%\book_env\Scripts\activate.bat</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Windows replaces <strong class="source-inline">%cd%</strong> with the path to the current directory. This saves us from having to type the full path up to the <strong class="source-inline">book_env</strong> part.</p>
			<p>Note that after we activate the virtual environment, we can see <strong class="source-inline">(book_env)</strong> in front of our prompt on the command line; this lets us know we are in the environment:</p>
			<p class="source-code">(book_env) C:\...&gt; </p>
			<p>When we are finished using the environment, we simply deactivate it:</p>
			<p class="source-code">(book_env) C:\...&gt; deactivate</p>
			<p>Any packages <a id="_idIndexMarker140"/>that are installed in the environment don't exist outside the environment. Note that we no longer have <strong class="source-inline">(book_env)</strong> in front of <a id="_idIndexMarker141"/>our prompt on the command line. You can read more about <strong class="source-inline">venv</strong> in the Python documentation at <a href="https://docs.python.org/3/library/venv.html">https://docs.python.org/3/library/venv.html</a>.</p>
			<p>Now that the virtual environment is created, activate it and then head to the <em class="italic">Installing the required Python packages</em> section for the next step.</p>
			<h4>Linux/macOS</h4>
			<p>To create <a id="_idIndexMarker142"/>our environment for this book, we will use the <strong class="source-inline">venv</strong> module <a id="_idIndexMarker143"/>from the standard library. Note that we must provide a name for our environment (<strong class="source-inline">book_env</strong>):</p>
			<p class="source-code">$ python3 -m venv book_env</p>
			<p>Now, we have a folder for our virtual environment named <strong class="source-inline">book_env</strong> inside of the repository folder we cloned/downloaded earlier. In order to use the environment, we need to activate it:</p>
			<p class="source-code">$ source book_env/bin/activate</p>
			<p>Note that after we activate the virtual environment, we can see <strong class="source-inline">(book_env)</strong> in front of our prompt on the command line; this lets us know we are in the environment:</p>
			<p class="source-code">(book_env) $</p>
			<p>When we are finished using the environment, we simply deactivate it:</p>
			<p class="source-code">(book_env) $ deactivate</p>
			<p>Any packages that are installed in the environment don't exist outside the environment. Note that <a id="_idIndexMarker144"/>we no longer have <strong class="source-inline">(book_env)</strong> in front of our prompt <a id="_idIndexMarker145"/>on the command line. You can read more about <strong class="source-inline">venv</strong> in the Python documentation at <a href="https://docs.python.org/3/library/venv.html">https://docs.python.org/3/library/venv.html</a>.</p>
			<p>Now that the virtual environment is created, activate it and then head to the <em class="italic">Installing the required Python packages</em> section for the next step.</p>
			<h3>conda</h3>
			<p>Anaconda provides a way to set up a Python environment specifically for data science. It includes <a id="_idIndexMarker146"/>some of the packages we will use in this book, along with <a id="_idIndexMarker147"/>several others that may be necessary for tasks that aren't covered in this book (and also deals with dependencies outside of Python that might be tricky to install otherwise). Anaconda uses <strong class="source-inline">conda</strong> as the environment and package manager instead of <strong class="source-inline">pip</strong>, although packages can still be installed with <strong class="source-inline">pip</strong> (as long as the <strong class="source-inline">pip</strong> installed by Anaconda is called). Note that some packages may not be available with <strong class="source-inline">conda</strong>, in which case we will have to use <strong class="source-inline">pip</strong>. Consult this page in the <strong class="source-inline">conda</strong> documentation for a comparison of commands used with <strong class="source-inline">conda</strong>, <strong class="source-inline">pip</strong>, and <strong class="source-inline">venv</strong>: <a href="https://conda.io/projects/conda/en/latest/commands.html#conda-vs-pip-vs-virtualenv-commands">https://conda.io/projects/conda/en/latest/commands.html#conda-vs-pip-vs-virtualenv-commands</a>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Be warned that Anaconda is a very large install (although the Miniconda version is much lighter). Those who use Python for purposes aside from data science may prefer the <strong class="source-inline">venv</strong> method we discussed earlier in order to have more control over what gets installed.</p>
			<p>Anaconda <a id="_idIndexMarker148"/>can also be packaged with the Spyder <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) and Jupyter Notebooks, which we will discuss later. Note that we can use Jupyter with the <strong class="source-inline">venv</strong> option as well.</p>
			<p>You can read more about Anaconda and how to install it at the following pages in their official documentation:</p>
			<ul>
				<li><strong class="bold">Windows</strong>: <a href="https://docs.anaconda.com/anaconda/install/windows/">https://docs.anaconda.com/anaconda/install/windows/</a></li>
				<li><strong class="bold">macOS</strong>: <a href="https://docs.anaconda.com/anaconda/install/mac-os/">https://docs.anaconda.com/anaconda/install/mac-os/</a></li>
				<li><strong class="bold">Linux</strong>: <a href="https://docs.anaconda.com/anaconda/install/linux/">https://docs.anaconda.com/anaconda/install/linux/</a></li>
				<li><strong class="bold">User guide</strong>: <a href="https://docs.anaconda.com/anaconda/user-guide/">https://docs.anaconda.com/anaconda/user-guide/</a></li>
			</ul>
			<p>Once you <a id="_idIndexMarker149"/>have installed either Anaconda or Miniconda, confirm that it is properly installed by running <strong class="source-inline">conda -V</strong> on the command line to <a id="_idIndexMarker150"/>display the version. Note that on Windows, all <strong class="source-inline">conda</strong> commands need to be run in <strong class="bold">Anaconda Prompt</strong> (as opposed to <strong class="bold">Command Prompt</strong>).</p>
			<p>To create a new <strong class="source-inline">conda</strong> environment for this book, called <strong class="source-inline">book_env</strong>, run the following:</p>
			<p class="source-code">(base) $ conda create --name book_env</p>
			<p>Running <strong class="source-inline">conda env list</strong> will show all the <strong class="source-inline">conda</strong> environments on the system, which will now include <strong class="source-inline">book_env</strong>. The current active environment will have an asterisk (<strong class="source-inline">*</strong>) next to it—by default, <strong class="source-inline">base</strong> will be active until we activate another environment:</p>
			<p class="source-code">(base) $ conda env list</p>
			<p class="source-code"># conda environments:</p>
			<p class="source-code">#</p>
			<p class="source-code">base                  *  /miniconda3</p>
			<p class="source-code">book_env                 /miniconda3/envs/book_env</p>
			<p>To activate the <strong class="source-inline">book_env</strong> environment, we run the following command:</p>
			<p class="source-code">(base) $ conda activate book_env</p>
			<p>Note that after we activate the virtual environment, we can see <strong class="source-inline">(book_env)</strong> in front of our prompt on the command line; this lets us know we are in the environment:</p>
			<p class="source-code">(book_env) $</p>
			<p>When we are finished using the environment, we deactivate it:</p>
			<p class="source-code">(book_env) $ conda deactivate</p>
			<p>Any packages that are installed in the environment don't exist outside the environment. Note that <a id="_idIndexMarker151"/>we no longer have <strong class="source-inline">(book_env)</strong> in front of our prompt on the command line. You can read more about how <a id="_idIndexMarker152"/>to use <strong class="source-inline">conda</strong> to manage virtual environments at <a href="https://www.freecodecamp.org/news/why-you-need-python-environments-and-how-to-manage-them-with-conda-85f155f4353c/">https://www.freecodecamp.org/news/why-you-need-python-environments-and-how-to-manage-them-with-conda-85f155f4353c/</a>.</p>
			<p>In the next section, we will install the Python packages required for following along with this book, so be sure to activate the virtual environment now.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Installing the required Python packages</h2>
			<p>We can do a lot with the Python standard library; however, we will often find the need to install <a id="_idIndexMarker153"/>and use an outside package to extend functionality. The <strong class="source-inline">requirements.txt</strong> file in the repository contains <a id="_idIndexMarker154"/>all the packages we need to install to work through this book. It will be in our current directory, but it can also be found at <a href="https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/blob/master/requirements.txt">https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/blob/master/requirements.txt</a>. This file can be used to install a bunch of packages at once with the <strong class="source-inline">-r</strong> flag in the call to <strong class="source-inline">pip3 install</strong> and has the advantage of being easy to share.</p>
			<p>Before installing anything, be sure to activate the virtual environment that you created with either <strong class="source-inline">venv</strong> or <strong class="source-inline">conda</strong>. Be advised that if the environment is not activated before running the following command, the packages will be installed outside the environment:</p>
			<p class="source-code">(book_env) $ pip3 install -r requirements.txt</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If you encounter any issues, report them at <a href="https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/issues">https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/issues</a>.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Why pandas?</h2>
			<p>When it <a id="_idIndexMarker155"/>comes to data science in Python, the <strong class="source-inline">pandas</strong> library is <a id="_idIndexMarker156"/>pretty much ubiquitous. It is built on top of the NumPy library, which allows us to perform mathematical operations on arrays of single-type data efficiently. Pandas expands this to <strong class="bold">dataframes</strong>, which can be thought of as tables of data. We will get a more formal introduction to dataframes in <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>.</p>
			<p>Aside from efficient operations, <strong class="source-inline">pandas</strong> also provides <strong class="bold">wrappers</strong> around the <strong class="source-inline">matplotlib</strong> plotting library, making it very easy to create a variety of plots without needing to write many lines of <strong class="source-inline">matplotlib</strong> code. We can always tweak our plots using <strong class="source-inline">matplotlib</strong>, but for quickly visualizing our data, we only need one line of code in <strong class="source-inline">pandas</strong>. We will explore this functionality in <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>, and <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Wrapper functions wrap around code from another library, obscuring some of its complexity and leaving us with a simpler interface for repeating that functionality. This is a core principle of <strong class="bold">object-oriented programming</strong> (<strong class="bold">OOP</strong>) called <strong class="bold">abstraction</strong>, which reduces complexity and the duplication of code. We will create our own wrapper functions throughout this book.</p>
			<p>In addition <a id="_idIndexMarker157"/>to <strong class="source-inline">pandas</strong>, this book makes use of <a id="_idIndexMarker158"/>Jupyter Notebooks. While you are free to choose not to use them, it's important to be familiar with Jupyter Notebooks as they are very common in the data world. As an introduction, we will use a Jupyter Notebook to validate our setup in the next section.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Jupyter Notebooks</h2>
			<p>Each chapter of this book includes Jupyter Notebooks for following along. Jupyter Notebooks are <a id="_idIndexMarker159"/>omnipresent in Python data science <a id="_idIndexMarker160"/>because they make it very easy to write and test code in more of a discovery environment compared to writing a program. We can execute one block of code at a time and have the results printed to the notebook, directly beneath the code that generated it. In addition, we can use <strong class="bold">Markdown</strong> to add text explanations to our work. Jupyter Notebooks can be easily packaged up and shared; they can be pushed to GitHub (where they will be rendered), converted into HTML or PDF, sent to someone else, or presented.</p>
			<h3>Launching JupyterLab</h3>
			<p>JupyterLab is an IDE that allows us to create Jupyter Notebooks and Python scripts, interact with <a id="_idIndexMarker161"/>the terminal, create text documents, reference documentation, and much more from a clean web interface on our local machine. There are lots of keyboard shortcuts to master before really becoming a power user, but the interface is pretty intuitive. When we created our environment, we installed everything we needed to run JupyterLab, so let's take a quick tour of the IDE and make sure that our environment is set up properly. First, we activate our environment, and then we launch JupyterLab:</p>
			<p class="source-code">(book_env) $ jupyter lab</p>
			<p>This will then launch a window in the default browser with JupyterLab. We will be greeted with the <strong class="bold">Launcher</strong> tab and the <strong class="bold">File Browser</strong> pane to the left:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/Figure_1.18_B16834.jpg" alt="Figure 1.18 – Launching JupyterLab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.18 – Launching JupyterLab</p>
			<p>Using the <strong class="bold">File Browser</strong> pane, double-click on the <strong class="bold">ch_01</strong> folder, which contains the Jupyter Notebook that we will use to validate our setup.</p>
			<h3>Validating the virtual environment</h3>
			<p>Open the <strong class="source-inline">checking_your_setup.ipynb</strong> notebook in the <strong class="bold">ch_01</strong> folder, as shown in the <a id="_idIndexMarker162"/>following screenshot:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/Figure_1.19_B16834.jpg" alt="Figure 1.19 – Validating the virtual environment setup&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.19 – Validating the virtual environment setup</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The <strong class="bold">kernel</strong> is the <a id="_idIndexMarker163"/>process that runs and introspects our code in a Jupyter Notebook. Note that we aren't limited to running Python—we can run kernels for R, Julia, Scala, and other languages as well. By default, we will be running Python using the IPython kernel. We will learn a little more about IPython throughout the book.</p>
			<p>Click on <a id="_idIndexMarker164"/>the code cell indicated in the previous screenshot and run it by clicking the play (▶) button. If everything shows up in green, the environment is all set up. However, if this isn't the case, run the following command from the virtual environment to create a special kernel with the <strong class="source-inline">book_env</strong> virtual environment for use with Jupyter:</p>
			<p class="source-code">(book_env) $ ipython kernel install --user --name=book_env</p>
			<p>This adds an additional option in the <strong class="bold">Launcher</strong> tab, and we can now switch to the <strong class="source-inline">book_env</strong> kernel from a Jupyter Notebook as well:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/Figure_1.20_B16834.jpg" alt="Figure 1.20 – Selecting a different kernel&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.20 – Selecting a different kernel</p>
			<p>It's important <a id="_idIndexMarker165"/>to note that Jupyter Notebooks will retain the values we assign to variables while the kernel is running, and the results in the <strong class="bold">Out[#]</strong> cells will be saved when we save the file. Closing the file doesn't stop the kernel and neither does closing the JupyterLab tab in the browser.</p>
			<h3>Closing JupyterLab</h3>
			<p>Closing <a id="_idIndexMarker166"/>the browser with JupyterLab in it doesn't stop JupyterLab or the kernels it is running (we also won't get the command-line interface back). To shut down JupyterLab entirely, we need to hit <em class="italic">Ctrl</em> + <em class="italic">C</em> (which is a keyboard interrupt signal that lets JupyterLab know we want to shut it down) a couple of times in the terminal until we get the prompt back:</p>
			<p class="source-code">...</p>
			<p class="source-code">[I 17:36:53.166 LabApp] Interrupted...</p>
			<p class="source-code">[I 17:36:53.168 LabApp] Shutting down 1 kernel</p>
			<p class="source-code">[I 17:36:53.770 LabApp] Kernel shutdown: a38e1[...]b44f</p>
			<p class="source-code">(book_env) $</p>
			<p>For more <a id="_idIndexMarker167"/>information about Jupyter, including <a id="_idIndexMarker168"/>a tutorial, check <a id="_idIndexMarker169"/>out <a href="http://jupyter.org/">http://jupyter.org/</a>. Learn more about JupyterLab at <a href="https://jupyterlab.readthedocs.io/en/stable/">https://jupyterlab.readthedocs.io/en/stable/</a>.</p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Summary</h1>
			<p>In this chapter, we learned about the main processes in conducting data analysis: data collection, data wrangling, EDA, and drawing conclusions. We followed that up with an overview of descriptive statistics and learned how to describe the central tendency and spread of our data; how to summarize it both numerically and visually using the 5-number summary, box plots, histograms, and kernel density estimates; how to scale our data; and how to quantify relationships between variables in our dataset.</p>
			<p>We got an introduction to prediction and time series analysis. Then, we had a very brief overview of some core topics in inferential statistics that can be explored after mastering the contents of this book. Note that while all the examples in this chapter were of one or two variables, real-life data is often high-dimensional. <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>, will touch on some ways to address this. Lastly, we set up our virtual environment for this book and learned how to work with Jupyter Notebooks.</p>
			<p>Now that we have built a strong foundation, we will start working with data in Python in the next chapter.</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Exercises</h1>
			<p>Run through the <strong class="source-inline">introduction_to_data_analysis.ipynb</strong> notebook for a review of this chapter's content, review the <strong class="source-inline">python_101.ipynb</strong> notebook (if needed), and then complete the following exercises to practice working with JupyterLab and calculating summary statistics in Python:</p>
			<ol>
				<li value="1">Explore the JupyterLab interface and look at some of the shortcuts that are available. Don't worry about memorizing them for now (eventually, they will become second nature and save you a lot of time)—just get comfortable using Jupyter Notebooks.</li>
				<li>Is all data normally distributed? Explain why or why not.</li>
				<li>When would it make more sense to use the median instead of the mean for the measure of center?</li>
				<li>Run the code in the first cell of the <strong class="source-inline">exercises.ipynb</strong> notebook. It will give you a list of 100 values to work with for the rest of the exercises in this chapter. Be sure to treat these values as a sample of the population.</li>
				<li>Using the data from <em class="italic">exercise 4</em>, calculate the following statistics without importing anything from the <strong class="source-inline">statistics</strong> module in the standard library (<a href="https://docs.python.org/3/library/statistics.html">https://docs.python.org/3/library/statistics.html</a>), and then confirm your results match up to those that are obtained when using the <strong class="source-inline">statistics</strong> module (where possible):<p>a) Mean</p><p>b) Median</p><p>c) Mode (hint: check out the <strong class="source-inline">Counter</strong> class in the <strong class="source-inline">collections</strong> module of the standard library at <a href="https://docs.python.org/3/library/collections.html#collections.Counter">https://docs.python.org/3/library/collections.html#collections.Counter</a>)</p><p>d) Sample variance</p><p>e) Sample standard deviation</p></li>
				<li>Using the data from <em class="italic">exercise 4</em>, calculate the following statistics using the functions in the <strong class="source-inline">statistics</strong> module where appropriate:<p>a) Range</p><p>b) Coefficient of variation</p><p>c) Interquartile range</p><p>d) Quartile coefficient of dispersion</p></li>
				<li>Scale the data created in <em class="italic">exercise 4</em> using the following strategies:<p>a) Min-max scaling (normalizing)</p><p>b) Standardizing</p></li>
				<li>Using the scaled data from <em class="italic">exercise 7</em>, calculate the following:<p>a) The covariance between the standardized and normalized data</p><p>b) The Pearson correlation coefficient between the standardized and normalized data (this is actually 1, but due to rounding along the way, the result will be slightly less)</p></li>
			</ol>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Further reading</h1>
			<p>The following are some resources that you can use to become more familiar with Jupyter:</p>
			<ul>
				<li><em class="italic">Jupyter Notebook Basics</em>: <a href="https://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Notebook%20Basics.ipynb">https://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Notebook%20Basics.ipynb</a></li>
				<li><em class="italic">JupyterLab introduction</em>: <a href="https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906">https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906</a></li>
				<li><em class="italic">Learning Markdown to make your Jupyter Notebooks presentation-ready</em>: <a href="https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed">https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed</a></li>
				<li><em class="italic">28 Jupyter Notebook Tips, Tricks, and Shortcuts</em>: <a href="https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/">https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/</a></li>
			</ul>
			<p>Some resources for learning more advanced concepts of statistics (that we won't cover here) and carefully applying them are as follows:</p>
			<ul>
				<li><em class="italic">A Gentle Introduction to Normality Tests in Python</em>: <a href="https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/">https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/</a></li>
				<li><em class="italic">How Hypothesis Tests Work: Confidence Intervals and Confidence Levels</em>: <a href="https://statisticsbyjim.com/hypothesis-testing/hypothesis-tests-confidence-intervals-levels/">https://statisticsbyjim.com/hypothesis-testing/hypothesis-tests-confidence-intervals-levels/</a></li>
				<li><em class="italic">Intro to Inferential Statistics (Making Predictions with Data) on Udacity</em>: <a href="https://www.udacity.com/course/intro-to-inferential-statistics--ud201">https://www.udacity.com/course/intro-to-inferential-statistics--ud201</a></li>
				<li><em class="italic">Lesson 4: Confidence Intervals (Penn State Elementary Statistics)</em>: <a href="https://online.stat.psu.edu/stat200/lesson/4">https://online.stat.psu.edu/stat200/lesson/4</a></li>
				<li><em class="italic">Seeing Theory: A visual introduction to probability and statistics</em>: <a href="https://seeing-theory.brown.edu/index.html">https://seeing-theory.brown.edu/index.html</a></li>
				<li><em class="italic">Statistics Done Wrong: The Woefully Complete Guide by Alex Reinhart</em>: <a href="https://www.statisticsdonewrong.com/">https://www.statisticsdonewrong.com/</a> </li>
				<li><em class="italic">Survey Sampling Methods</em>: <a href="https://stattrek.com/survey-research/sampling-methods.aspx">https://stattrek.com/survey-research/sampling-methods.aspx</a></li>
			</ul>
		</div>
	</body></html>