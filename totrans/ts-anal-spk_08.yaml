- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going at Scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After building and testing models in the previous chapter, we will now address
    the requirements and considerations for scaling time-series analysis in large
    and distributed computing environments. We will cover the different ways that
    Apache Spark can be used to scale the previous examples in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133),
    starting with feature engineering and moving on to hyperparameter tuning and single-
    and multi-model training. This information is crucial as we face the requirement
    to analyze large volumes of time-series data in a timely manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need to scale time-series analysis?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling out feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling out model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before getting into the main topics, we will cover here the technical requirements
    for this chapter, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ch8` folder of the book’s GitHub repository at this URL:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch8](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch8)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Synthetic data**: We will use the Synthetic Data Vault tool, a Python library
    for creating synthetic tabular data. You can find more information on Synthetic
    Data Vault here: [https://docs.sdv.dev/sdv](https://docs.sdv.dev/sdv).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Databricks platform**: The Databricks Community Edition, while free to use,
    is limited in resources. Similarly, the resources are likely to be limited when
    using a personal computer or laptop. With the requirement to demonstrate the scaling
    of computing power in this chapter, we will be using the non-Community version
    of the Databricks platform. As discussed in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016),
    you can sign up for a 14-day free trial of Databricks, which will require you
    to first have an account with a cloud provider. Some cloud providers offer free
    credits at the start. This will provide you with more resources than on the Community
    Edition, for a limited time. Note that at the end of the trial period, the billing
    will switch to the credit card you provided at registration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Databricks compute configuration used is as per *Figure 8**.1*. The worker
    and driver types shown here are based on AWS, which is different from what is
    available on Azure and GCP. Note that the UI can be subject to change, in which
    case refer to the latest Databricks documentation here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/compute/configure.html](https://docs.databricks.com/en/compute/configure.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18568_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Databricks compute configuration'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need to scale time-series analysis?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The need to scale time-series analysis usually results from a requirement to
    perform the analysis faster or on a bigger dataset. In this chapter, we will look
    at decreasing the processing time achieved in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133)
    while increasing the dataset size fivefold. This will be possible thanks to the
    scale of processing offered by Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Scaled-up dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To exercise Spark’s scalability, we will need a more extensive dataset than
    we have used. While you may already have such a dataset, for the sake of this
    chapter, we will scale the household energy consumption dataset we used in [*Chapter
    7*](B18568_07.xhtml#_idTextAnchor133) and earlier chapters. The scaled dataset
    will be generated using the Synthetic Data Vault tool, mentioned in the *Technical*
    *requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this section is in `ts-spark_ch8_1.dbc`. We import the code into
    Databricks, similar to the approach explained for the Community Edition in the
    *Step-by-step: Loading and visualizing time series* section of [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016).'
  prefs: []
  type: TYPE_NORMAL
- en: In this code, we want to generate energy consumption data for four other families
    using one household’s data to scale it fivefold.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by capturing the metadata of `pdf_main`, which is the smaller reference
    dataset. The metadata is used as input to create a `GaussianCopulaSynthesizer`
    object named `synthesizer`, which represents a statistical model of the data.
    The synthesizer is, in turn, trained on the reference dataset (`pdf_main`) with
    the `fit` method. This model is finally used to generate synthetic data with the
    `sample` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The smaller reference dataset (`pdf_main`) is associated with customer identifier
    (`cust_id`) `1`, and the synthetic datasets are associated with identifiers `2`,
    `3`, `4`, and `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Running the code in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133) on this
    new, larger dataset is not going to be performant. We can scale up or out time-series
    analysis on larger datasets. We will explain both types of scaling next.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scaling up is the simpler way to scale and does not require us to change the
    code we wrote in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133). We improve performance
    in this way by adding more memory (RAM) and using a more powerful CPU or even
    GPU. This works to a certain point before we reach scaling limits, prohibitive
    costs, or diminishing returns. In fact, due to system bottlenecks and overheads,
    scaling up does not result in linear performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: To scale further, we need to scale out.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling out
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of making our one machine more powerful, scaling out involves adding
    more machines and parallelizing the processing. This requires a mechanism for
    the code to be distributed and executed in parallel, which is what Apache Spark
    provides.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the upcoming sections, we will cover the following different ways that Apache
    Spark can be used to scale out time-series analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark can be used to scale out feature engineering with its distributed
    computing framework. This enables parallel processing of feature engineering tasks,
    which we will demonstrate in this section.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue our discussion on data preparation in [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103)
    and improve the feature engineering done in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133).
    We will be using the pandas-based code examples from the *Development and testing*
    section of *Chapter 7* as a base for our discussion in this section. We will see
    in the following examples how non-Spark code is rewritten to be Spark compatible
    to avail the benefits of its scalability feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are many ways in which Spark can be used for feature engineering,
    we will focus on the following three related to improving [*Chapter* *7*](B18568_07.xhtml#_idTextAnchor133)’s
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: Column transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lag values calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin with column transformations in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Column transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the first code example, we will rewrite the column transformations code
    present in `ts-spark_ch7_1e_lgbm_comm.dbc`, which is used in the *Development
    and testing* section of [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133). We will
    change the code to a Spark-enabled version by using the `pyspark.sql.functions`
    library. For this, we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace the `Date` column with the concatenation (the `concat_ws` function)
    of the existing `Date` and `Time` columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert (the `to_timestamp` function) the `Date` column into timestamp format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace selectively (with the `when` and `otherwise` condition) the incorrect
    values, `?`, in `Global_active_power` to `None`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace (the `regexp_replace` function) `,` with `.` to be in the proper format
    for a `float` value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code extract demonstrates the preceding steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After leveraging Spark to parallelize the column transformations, the next code
    improvement we will be covering is for resampling the time-series data.
  prefs: []
  type: TYPE_NORMAL
- en: Resampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the second code conversion example, we will rewrite the hourly resampling
    code present in `ts-spark_ch7_1e_lgbm_comm.dbc`, which is used in the *Development
    and testing* section of [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133). We want
    to calculate the hourly mean of `Global_active_power` for each customer. For this,
    we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the `Date` column to its date and hour components using the `date_format`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resample to the hourly mean of `Global_active_power` (the `agg` and `mean` functions)
    for each customer (the `groupBy` function).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code demonstrates the preceding steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have used Spark to parallelize the resampling, the next code improvement
    we will be covering is for calculating the lag values of the time-series data.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating lag values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the third example of scaling feature engineering with Apache Spark, we will
    rewrite the lag calculation code present in `ts-spark_ch7_1e_lgbm_comm.dbc`, which
    is used in the *Development and testing* section of [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133).
    We want to calculate different lag values for each customer. For this, we need
    to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a sliding date window over which to calculate the lags for each of the
    customers (the `partitionBy` function). We have the dates ordered (the `orderBy`
    function) for each customer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the different lags over the sliding window (the `lag` and `over` functions).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that as the lag calculation is based on previous values, some of the lag
    values at the beginning of the dataset will not have enough prior values for calculation
    and will be empty. We remove the rows with these empty lag values using the `dropna`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code demonstrates the preceding steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: By using Spark functions instead of pandas, we will enable Spark to parallelize
    the lag calculations for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the different ways of leveraging Apache Spark to improve
    the feature engineering part of [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133)’s
    code, we will dive deep into the scaling out of model training.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following different ways that Apache Spark
    can be used for model training at scale:'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single model training in parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple models training in parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These approaches enable efficient model training when we have large datasets
    or many models to train.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning can be an expensive computation when the same model is
    trained repeatedly with many different hyperparameters. We want to be able to
    leverage Spark to find the best hyperparameters efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, training a single model on a large dataset can take a long time.
    In other cases, we may have many models to train for distinct time-series datasets.
    We want to speed these up by parallelizing the training on Spark clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We will go into the details of these approaches next, starting with hyperparameter
    tuning in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087), hyperparameter
    tuning in machine learning is the process of finding the best set of configurations
    for a machine learning algorithm. This search for optimal hyperparameters can
    be parallelized using libraries such as GridSearchCV, Hyperopt, and Optuna, which
    provide the framework, in conjunction with Apache Spark, for the backend processing
    parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed Spark’s processing parallelism in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063).
    Here we will focus more specifically on the use of Optuna in conjunction with
    Apache Spark for hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: If you recall, in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133), we used GridSearchCV
    on a single node to tune the hyperparameters of the LightGBM model. We will improve
    on this in the code example in this section by parallelizing the process. We will
    use Optuna with Spark to find the best hyperparameters for the LightGBM model
    we explored in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133).
  prefs: []
  type: TYPE_NORMAL
- en: 'Optuna is an open source hyperparameter optimization framework that is used
    to automate hyperparameter searches. You can find more information on Optuna here:
    [https://optuna.org/](https://optuna.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin the tuning process by defining an `objective` function (which
    we will later optimize using Optuna). This `objective` function does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the search space in `params` with the range of hyperparameter values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the LightGBM `LGBMRegressor` model with the parameters specific to
    the trial.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train (`fit`) the model on the training dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the model to predict the validation dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the model evaluation metric (`mean_absolute_percentage_error`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the evaluation metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code demonstrates the preceding steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the objective function is defined, the next steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Register Spark (the `register_spark` function) as the backend.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a study (the `create_study` function), which is a collection of trials,
    to minimize the evaluation metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the study on the Spark `parallel_backend` to optimize the `objective` function
    over `n_trials`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code demonstrates the preceding steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Retrieve the best trial from the optimization study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: trial = study2.best_trial
  prefs: []
  type: TYPE_NORMAL
- en: Print the best trial's objective function value,
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '#  typically accuracy or loss'
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Best trial accuracy: {trial.value}")'
  prefs: []
  type: TYPE_NORMAL
- en: 'print("Best trial params: ")'
  prefs: []
  type: TYPE_NORMAL
- en: Iterate through the best trial's hyperparameters and print them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'for key, value in trial.params.items():'
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"    {key}: {value}")'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: from pyspark.ml.feature import VectorAssembler
  prefs: []
  type: TYPE_NORMAL
- en: Define a list to hold the names of the lag feature columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: inputCols = []
  prefs: []
  type: TYPE_NORMAL
- en: Loop through the list of lag intervals to create feature column
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: names
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'for l in lags:'
  prefs: []
  type: TYPE_NORMAL
- en: inputCols.append('Global_active_power_lag' + str(l))
  prefs: []
  type: TYPE_NORMAL
- en: Initialize VectorAssembler with the
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: created feature column names and specify the output column name
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: assembler = VectorAssembler(
  prefs: []
  type: TYPE_NORMAL
- en: inputCols=inputCols, outputCol="features")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: from xgboost.spark import SparkXGBRegressor
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the SparkXGBRegressor for the regression task.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`num_workers` is set to the default parallelism level of -'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '#   the Spark context to utilize all available cores.'
  prefs: []
  type: TYPE_NORMAL
- en: '`label_col` specifies the target variable column name for'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: prediction.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`missing` is set to 0.0 to handle missing values in the dataset.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: xgb_model = SparkXGBRegressor(
  prefs: []
  type: TYPE_NORMAL
- en: num_workers=sc.defaultParallelism,
  prefs: []
  type: TYPE_NORMAL
- en: label_col="Global_active_power", missing=0.0
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
  prefs: []
  type: TYPE_NORMAL
- en: from pyspark.ml.evaluation import RegressionEvaluator
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the parameter grid for hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- max_depth: specifies the maximum depth of the trees in the model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- n_estimators: defines the number of trees in the ensemble'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: paramGrid = ParamGridBuilder()\
  prefs: []
  type: TYPE_NORMAL
- en: .addGrid(xgb_model.max_depth, [5, 10])\
  prefs: []
  type: TYPE_NORMAL
- en: .addGrid(xgb_model.n_estimators, [30, 100])\
  prefs: []
  type: TYPE_NORMAL
- en: .build()
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the regression evaluator for model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- metricName: specifies the metric to use for evaluation,'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '#    here RMSE (Root Mean Squared Error)'
  prefs: []
  type: TYPE_NORMAL
- en: '- labelCol: the name of the label column'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- predictionCol: the name of the prediction column'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: evaluator = RegressionEvaluator(
  prefs: []
  type: TYPE_NORMAL
- en: metricName="rmse",
  prefs: []
  type: TYPE_NORMAL
- en: LabelCol = xgb_model.getLabelCol(),
  prefs: []
  type: TYPE_NORMAL
- en: PredictionCol = xgb_model.getPredictionCol()
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the CrossValidator for hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- estimator: the model to be tuned'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- evaluator: the evaluator to be used for model evaluation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- estimatorParamMaps: the grid of parameters to be used for tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: cv = CrossValidator(
  prefs: []
  type: TYPE_NORMAL
- en: estimator = xgb_model, evaluator = evaluator,
  prefs: []
  type: TYPE_NORMAL
- en: estimatorParamMaps = paramGrid)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: from pyspark.ml import Pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize a Pipeline object with two stages:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: a feature assembler and a cross-validator for model tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'pipeline = filter function) the training data to cust_id 1. We then take all
    the records (the head function) for training except the last 48 hours as these
    will be used for testing. This results in the train_hr DataFrame with the hourly
    training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, for testing, we will filter in `cust_id` `1` and, in this case,
    use the last 48 hours. We can then apply (`transform`) the model (`pipelineModel`)
    to the test data (`test_hr`) to get the prediction of energy consumption for these
    48 hours:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the model's performance using
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Root Mean Squared Error (RMSE) metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: rmse = evaluator.evaluate(predictions)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'def train_model(df_pandas: pd.DataFrame) -> pd.DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '# Extract the customer ID for which the model is being trained'
  prefs: []
  type: TYPE_NORMAL
- en: cust_id = df_pandas["cust_id"].iloc[0]
  prefs: []
  type: TYPE_NORMAL
- en: '# Select features and target variables from the DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: X = df_pandas[[
  prefs: []
  type: TYPE_NORMAL
- en: '''Global_active_power_lag1'', ''Global_active_power_lag2'','
  prefs: []
  type: TYPE_NORMAL
- en: '''Global_active_power_lag3'', ''Global_active_power_lag4'','
  prefs: []
  type: TYPE_NORMAL
- en: '''Global_active_power_lag5'', ''Global_active_power_lag12'','
  prefs: []
  type: TYPE_NORMAL
- en: '''Global_active_power_lag24'', ''Global_active_power_lag168'''
  prefs: []
  type: TYPE_NORMAL
- en: ']]'
  prefs: []
  type: TYPE_NORMAL
- en: y = df_pandas['Global_active_power']
  prefs: []
  type: TYPE_NORMAL
- en: '# Split the dataset into training and testing sets, preserving'
  prefs: []
  type: TYPE_NORMAL
- en: '# time order'
  prefs: []
  type: TYPE_NORMAL
- en: X_train, X_test, y_train, y_test = train_test_split(
  prefs: []
  type: TYPE_NORMAL
- en: X, y, test_size=0.2, shuffle=False, random_state=12
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '# Define the hyperparameter space for LightGBM model tuning'
  prefs: []
  type: TYPE_NORMAL
- en: param_grid = {
  prefs: []
  type: TYPE_NORMAL
- en: '''num_leaves'': [30, 50, 100],'
  prefs: []
  type: TYPE_NORMAL
- en: '''learning_rate'': [0.1, 0.01, 0.001],'
  prefs: []
  type: TYPE_NORMAL
- en: '''n_estimators'': [50, 100, 200]'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '# Initialize the LightGBM regressor model'
  prefs: []
  type: TYPE_NORMAL
- en: lgbm = lgb.LGBMRegressor()
  prefs: []
  type: TYPE_NORMAL
- en: '# Initialize TimeSeriesSplit for cross-validation to'
  prefs: []
  type: TYPE_NORMAL
- en: '#  respect time series data structure'
  prefs: []
  type: TYPE_NORMAL
- en: tscv = TimeSeriesSplit(n_splits=10)
  prefs: []
  type: TYPE_NORMAL
- en: '# Perform grid search with cross-validation'
  prefs: []
  type: TYPE_NORMAL
- en: gsearch = GridSearchCV(
  prefs: []
  type: TYPE_NORMAL
- en: estimator=lgbm, param_grid=param_grid, cv=tscv)
  prefs: []
  type: TYPE_NORMAL
- en: gsearch.fit(X_train, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: '# Extract the best hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: best_params = gsearch.best_params_
  prefs: []
  type: TYPE_NORMAL
- en: '# Train the final model using the best parameters'
  prefs: []
  type: TYPE_NORMAL
- en: final_model = lgb.LGBMRegressor(**best_params)
  prefs: []
  type: TYPE_NORMAL
- en: final_model.fit(X_train, y_train)
  prefs: []
  type: TYPE_NORMAL
- en: '# Make predictions on the test set'
  prefs: []
  type: TYPE_NORMAL
- en: y_pred = final_model.predict(X_test)
  prefs: []
  type: TYPE_NORMAL
- en: '# Calculate RMSE and MAPE metrics'
  prefs: []
  type: TYPE_NORMAL
- en: rmse = np.sqrt(mean_squared_error(y_test, y_pred))
  prefs: []
  type: TYPE_NORMAL
- en: mape = mean_absolute_percentage_error(y_test, y_pred)
  prefs: []
  type: TYPE_NORMAL
- en: '# Prepare the results DataFrame to return'
  prefs: []
  type: TYPE_NORMAL
- en: return_df = pd.DataFrame(
  prefs: []
  type: TYPE_NORMAL
- en: '[[cust_id, str(best_params), rmse, mape]],'
  prefs: []
  type: TYPE_NORMAL
- en: columns=["cust_id", "best_params", "rmse", "mape"]
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: return return_df
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: from pyspark.sql.functions import lit
  prefs: []
  type: TYPE_NORMAL
- en: Group the data by customer ID and apply the
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '#  train_model function to each group using Pandas UDF'
  prefs: []
  type: TYPE_NORMAL
- en: The schema for the resulting DataFrame is defined by
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '#  train_model_result_schema'
  prefs: []
  type: TYPE_NORMAL
- en: Cache the resulting DataFrame to optimize performance for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '#  subsequent actions'
  prefs: []
  type: TYPE_NORMAL
- en: train_model_result_df = (
  prefs: []
  type: TYPE_NORMAL
- en: data_hr
  prefs: []
  type: TYPE_NORMAL
- en: .groupby("cust_id")
  prefs: []
  type: TYPE_NORMAL
- en: .applyInPandas(train_model, schema=train_model_result_schema)
  prefs: []
  type: TYPE_NORMAL
- en: .cache()
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
