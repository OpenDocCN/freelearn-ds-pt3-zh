- en: 4\. The Bias-Variance Trade-Off
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 偏差-方差权衡
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we'll cover the remaining elements of logistic regression,
    including what happens when you call `.fit` to train the model, and the statistical
    assumptions you should be aware of when using this modeling technique. You will
    learn how to use L1 and L2 regularization with logistic regression to prevent
    overfitting and how to use the practice of cross-validation to decide the regularization
    strength. After reading this chapter, you will be able to use logistic regression
    in your work and employ regularization in the model fitting process to take advantage
    of the bias-variance trade-off and improve model performance on unseen data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍逻辑回归的剩余部分，包括调用`.fit`训练模型时发生的事情，以及在使用此建模技术时应该注意的统计假设。你将学习如何在逻辑回归中使用 L1 和
    L2 正则化来防止过拟合，并了解如何使用交叉验证实践来决定正则化的强度。阅读本章后，你将能够在工作中使用逻辑回归，并在模型拟合过程中使用正则化，以利用偏差-方差权衡并提高模型在未见数据上的表现。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In this chapter, we will introduce the remaining details of logistic regression
    left over from the previous chapter. In addition to being able to use scikit-learn
    to fit logistic regression models, you will gain insight into the gradient descent
    procedure, which is similar to the processes that are used "under the hood" (invisible
    to the user) to accomplish model fitting in scikit-learn. Finally, we'll complete
    our discussion of the logistic regression model by familiarizing ourselves with
    the formal statistical assumptions of this method.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍上一章中剩余的逻辑回归细节。除了能够使用 scikit-learn 拟合逻辑回归模型外，你还将深入了解梯度下降过程，这与 scikit-learn
    中用于完成模型拟合的“幕后”过程类似。最后，我们将通过熟悉这种方法的正式统计假设，完成对逻辑回归模型的讨论。
- en: 'We begin our exploration of the foundational machine learning concepts of overfitting,
    underfitting, and the bias-variance trade-off by examining how the logistic regression
    model can be extended to address the overfitting problem. After reviewing the
    mathematical details of the regularization methods that are used to alleviate
    overfitting, you will learn a useful practice for tuning the hyperparameters of
    regularization: cross-validation. Through the methods of regularization and some
    simple feature engineering, you will gain an understanding of how to improve both
    overfitted and underfitted models.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过探讨如何扩展逻辑回归模型以解决过拟合问题，开始探索机器学习中基础概念——过拟合、欠拟合和偏差-方差权衡。在回顾用于缓解过拟合的正则化方法的数学细节后，你将学到一种调优正则化超参数的实用方法：交叉验证。通过正则化方法和一些简单的特征工程，你将理解如何改进过拟合和欠拟合的模型。
- en: Although we are focusing on logistic regression in this chapter, the concepts
    of overfitting, underfitting, regularization, and the bias-variance trade-off
    are relevant to nearly all supervised modeling techniques in machine learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章我们主要关注逻辑回归，但过拟合、欠拟合、正则化以及偏差-方差权衡的概念几乎适用于机器学习中所有监督学习建模技术。
- en: Estimating the Coefficients and Intercepts of Logistic Regression
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计逻辑回归的系数和截距
- en: 'In the previous chapter, we learned that the coefficients of a logistic regression
    model (each of which goes with a particular feature), as well as the intercept,
    are determined using the training data when the `.fit` method is called on a logistic
    regression model in scikit-learn. These numbers are called the **parameters**
    of the model, and the process of finding the best values for them is called parameter
    **estimation**. Once the parameters are found, the logistic regression model is
    essentially a finished product: with just these numbers, we can use a logistic
    regression model in any environment where we can perform common mathematical functions.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们学习了逻辑回归模型的系数（每个系数对应一个特定的特征）以及截距，这些值是在调用 scikit-learn 中逻辑回归模型的`.fit`方法时，使用训练数据来确定的。这些数值被称为模型的**参数**，而找到最佳参数值的过程称为参数**估计**。一旦参数确定，逻辑回归模型就基本完成了：只需要这些数值，我们就可以在任何可以执行常见数学函数的环境中使用逻辑回归模型。
- en: It is clear that the process of parameter estimation is important, since this
    is how we can make a predictive model from our data. So, how does parameter estimation
    work? To understand this, the first step is to familiarize ourselves with the
    concept of a **cost function**. A cost function is a way of telling how far away
    the model predictions are from perfectly describing the data. The larger the difference
    between the model predictions and the actual data, then the larger the "cost"
    returned by the cost function.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，参数估计过程是非常重要的，因为正是通过这个过程，我们能够从数据中构建预测模型。那么，参数估计是如何工作的呢？要理解这一点，第一步是熟悉**代价函数**的概念。代价函数是一种衡量模型预测与数据完美描述之间距离的方式。模型预测与实际数据之间的差异越大，代价函数返回的“代价”就越大。
- en: 'This is a straightforward concept for regression problems: the difference between
    predictions and true values can be used for the cost, after going through a transformation
    (such as absolute value or squaring) to make the value of the cost positive, and
    then averaging this over all the training samples.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，这是一个直观的概念：预测值与真实值之间的差异可以用作代价，通过某种变换（例如取绝对值或平方）将代价值转换为正数，再对所有训练样本进行平均。
- en: 'For classification problems, especially in fitting logistic regression models,
    a typical cost function is the **log-loss** function, also called cross-entropy
    loss. This is the cost function that scikit-learn uses, in a modified form, to
    fit logistic regression:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，特别是在拟合逻辑回归模型时，一个典型的代价函数是**对数损失**函数，也叫交叉熵损失。这是 scikit-learn 在拟合逻辑回归时使用的代价函数，经过修改：
- en: '![Figure 4.1: The log-loss function'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1：对数损失函数'
- en: '](img/B16925_4_1.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_1.jpg)'
- en: 'Figure 4.1: The log-loss function'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：对数损失函数
- en: 'Here, there are *n* training samples, *y*i is the true label (0 or 1) of the
    *i*th sample, *p*i is the predicted probability that the label of the *i*th sample
    equals 1, and log is the natural logarithm. The summation notation (that is, the
    uppercase Greek letter, sigma) over all the training samples and division by *n*
    serve to take the average of this cost function over all training samples. With
    this in mind, take a look at the following graph of the natural logarithm function
    and consider what the interpretation of this cost function is:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有*n*个训练样本，*y*i 是第 *i* 个样本的真实标签（0 或 1），*p*i 是第 *i* 个样本标签为 1 的预测概率，log 是自然对数。对所有训练样本求和的符号（即大写的希腊字母
    sigma）和除以 *n*，用于对所有训练样本的代价函数进行平均。考虑到这一点，看看下面的自然对数函数图像，并思考这个代价函数的解释：
- en: '![Figure 4.2: Natural logarithm on the interval (0, 1)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2：区间 (0, 1) 上的自然对数'
- en: '](img/B16925_4_2.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_2.jpg)'
- en: 'Figure 4.2: Natural logarithm on the interval (0, 1)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：区间 (0, 1) 上的自然对数
- en: To see how the log-loss cost function works, consider its value for a sample
    where the true label is 1, which is *y = 1* in this case, so the second part of
    the cost function, *(1 - y*i*)log(1 - p*i*)*, will be exactly equal to 0 and will
    not affect the value. Then the value of the cost function is *-y*i*log(p*i*) =
    -log(p*i*)* since *y*i *= 1*. So, the cost for this sample is simply the negative
    of the natural logarithm of the predicted probability. Now since the true label
    for the sample is 1, consider how the cost function should behave. We expect that
    for predicted probabilities that are close to 1, the cost function will be small,
    representing a small error for predictions that are closer to the true value.
    For predictions that are closer to 0, it will be larger, since the cost function
    is supposed to take on larger values the more "wrong" the prediction is.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解对数损失代价函数是如何工作的，考虑一个样本，其中真实标签为 1，即 *y = 1*，因此代价函数的第二部分，*(1 - y*i*)log(1 -
    p*i*)*，将完全等于 0，不会影响结果。此时，代价函数的值为 *-y*i*log(p*i*) = -log(p*i*)*，因为 *y*i *= 1*。因此，该样本的代价就是预测概率的自然对数的负值。现在，由于该样本的真实标签为
    1，考虑代价函数应该如何表现。我们期望，对于接近 1 的预测概率，代价函数会很小，表示预测值与真实值接近时的误差很小。对于接近 0 的预测，代价会更大，因为代价函数应当随着预测错误的增大而增大。
- en: From the graph of the natural logarithm in *Figure 4.2* we can see that for
    values of *p* that are closer to 0, the natural logarithm takes on increasingly
    negative values. This means the cost function will take on increasingly positive
    values, so that the cost of classifying a positive sample with a very low probability
    is relatively high, as it should be. Conversely, if the predicted probability
    is closer to 1, then the graph indicates the cost will be closer to 0 – again,
    this is as expected for a prediction that is "more correct." Therefore, the cost
    function behaves as expected for a positive sample. A similar observation can
    be made for samples where the true label is 0.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 4.2*中的自然对数图中，我们可以看到，对于更接近 0 的*p*值，自然对数的值越来越负。这意味着成本函数将变得越来越大，因此，分类一个具有非常低概率的正样本的成本相对较高，这正是我们所期望的。相反，如果预测的概率更接近
    1，则图形表明成本将接近 0——再次，这与一个“更正确”预测的期望一致。因此，成本函数在正样本的情况下表现如预期。对于标签为 0 的样本，也可以做类似的观察。
- en: Now we understand how the log-loss cost function works for logistic regression.
    But what does this have to do with how the coefficients and the intercept are
    determined? We will learn in the next section.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了对数损失成本函数在逻辑回归中的工作原理。但这与系数和截距的确定有什么关系呢？我们将在下一节学习。
- en: Note
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/NeF8P](https://packt.link/NeF8P).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成本节中展示的图表的代码可以在这里找到：[https://packt.link/NeF8P](https://packt.link/NeF8P)。
- en: Gradient Descent to Find Optimal Parameter Values
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降寻找最优参数值
- en: The problem of finding the parameter values (coefficients and intercept) for
    a logistic regression model using a log-loss cost boils down to a problem of `.fit`
    method of the logistic regression model in scikit-learn. There are different solution
    techniques for finding the set of parameters with the lowest cost, and you can
    choose which one you would like to use with the `solver` keyword when you are
    instantiating the model class. All of these methods work somewhat differently.
    However, they are all based on the concept of **gradient descent**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对数损失成本找到逻辑回归模型的参数值（系数和截距）的问题，归结为 scikit-learn 中逻辑回归模型的`.fit`方法的问题。找到具有最低成本的参数集有不同的解决技术，您可以在实例化模型类时使用`solver`关键字选择您想要使用的技术。所有这些方法都略有不同，但它们都基于**梯度下降**的概念。
- en: The gradient descent process starts with an `solver` keyword. However, for more
    advanced machine learning algorithms such as deep neural networks, selection of
    the initial guesses for parameters requires more attention.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降过程从`solver`关键字开始。然而，对于像深度神经网络这样的更高级机器学习算法，选择参数的初始猜测需要更多的关注。
- en: For the sake of illustration, we will consider a problem where there is only
    one parameter to estimate. We'll look at the value of a hypothetical cost function
    (*y = f(x) = x*2 *– 2x*) and devise a gradient descent procedure to find the value
    of the parameter, *x*, for which the cost, *y*, is the lowest. Here, we choose
    some *x* values, create a function that returns the value of the cost function,
    and look at the value of the cost function over this range of parameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明问题，我们考虑一个只需要估计一个参数的情况。我们将观察一个假设的成本函数（*y = f(x) = x*2 *– 2x*）的值，并设计一个梯度下降过程来找到使成本*y*最小的参数值*x*。在这里，我们选择一些*x*值，创建一个返回成本函数值的函数，并观察在这个参数范围内成本函数的值。
- en: 'The code to do this is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作的代码如下：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is the output of the print statement:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是打印语句的输出：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The remaining code snippet is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的代码片段如下：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The resulting plot should appear as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图应如下所示：
- en: '![Figure 4.3: A cost function plot'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3：成本函数图'
- en: '](img/B16925_4_3.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_3.jpg)'
- en: 'Figure 4.3: A cost function plot'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：成本函数图
- en: Note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'In the preceding code snippets, we assume that you would have imported the
    necessary libraries. You can refer to the following notebook for the complete
    code for the chapter including the import statement for the preceding snippets:
    [https://packt.link/A4VyF](https://packt.link/A4VyF).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们假设您已经导入了必要的库。您可以参考以下笔记本，获取包括前述代码片段导入语句的完整代码：[https://packt.link/A4VyF](https://packt.link/A4VyF)。
- en: 'Looking at the **error surface** in *Figure 4.3*, which is the plot of the
    cost function over a range of parameter values, it''s pretty evident what parameter
    value will result in the lowest value of the cost function: *x = 1*. In fact,
    with some calculus, you could easily confirm this by setting the derivative to
    zero and then solving for *x*, confirming that *x = 1* is the minimum. However,
    generally speaking, it is not always feasible to solve the problem so simply.
    In cases where it is necessary to use gradient descent, we don''t always know
    how the entire error surface looks. Rather, after we''ve chosen the initial guess
    for the parameter, all we''re able to know is the direction of the error surface
    in the immediate vicinity of that point.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient descent** is an iterative algorithm; starting from the initial guess,
    we try to find a new guess that lowers the cost function and continue with this
    until we''ve found a good solution. We are trying to move "downhill" on the error
    surface, but we only know which direction to move in and how far to move in that
    direction, based on the shape of the error surface in the immediate neighborhood
    of our current guess. In mathematical terms, we only know the value of the **derivative**
    (which is called the **gradient** in more than one dimension) at the parameter
    value of the current guess. If you have not studied calculus, you can think of
    the gradient as telling you which direction is downhill, and how steep the hill
    is from where you''re standing. We use this information to "take a step" in the
    direction of decreasing error. How big a step we decide to take depends on the
    **learning rate**. Since the gradient declines toward the direction of decreasing
    error, we want to take a step in the direction that is the negative of the gradient.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'These notions can be formalized in the following equation. To get to the new
    guess, *x*new, from the current guess, *x*old, where *f''(x*old*)* is the derivative
    (that is, the gradient) of the cost function at the current guess:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Equation to obtain the new guess from the current guess'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_4.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.4: Equation to obtain the new guess from the current guess'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following graph, we can see the results of starting a gradient descent
    procedure from *x = 4.5*, with a learning rate of 0.75, and then optimizing *x*
    to attain the lowest value of the cost function:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: The gradient descent path'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_5.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.5: The gradient descent path'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent also works in higher-dimensional spaces; in other words, with
    more than one parameter. However, you can only visualize up to a two-dimensional
    error surface (that is, two parameters at a time on a three-dimensional plot)
    on a single graph.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Having described the workings of gradient descent, let's perform an exercise
    to implement the gradient descent algorithm, expanding on the example of this
    section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/NeF8P](https://packt.link/NeF8P). If you''re reading the print
    version of this book, you can download and browse the color versions of some of
    the images in this chapter by visiting the following link: [https://packt.link/FAXBM](https://packt.link/FAXBM)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成本节所呈现图表的代码可以在这里找到：[https://packt.link/NeF8P](https://packt.link/NeF8P)。如果你正在阅读本书的印刷版，你可以通过访问以下链接下载并浏览本章一些图像的彩色版本：[https://packt.link/FAXBM](https://packt.link/FAXBM)
- en: 'Exercise 4.01: Using Gradient Descent to Minimize a Cost Function'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.01：使用梯度下降最小化代价函数
- en: 'In this exercise, our task is to find the best set of parameters in order to
    minimize the following hypothetical cost function: *y = f(x) = x*2 *– 2x*. To
    do this, we will employ gradient descent, which was described in the preceding
    section. Perform the following steps to complete the exercise:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们的任务是找到一组最佳参数，以最小化以下假设的代价函数：*y = f(x) = x*2 *– 2x*。为此，我们将采用前面部分描述的梯度下降方法。执行以下步骤以完成练习：
- en: Note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before you begin this exercise, please make sure you have executed the prerequisite
    steps of importing the necessary libraries and loading the cleaned dataframe.
    These steps along with the code for this exercise can be found at [https://packt.link/NeF8P](https://packt.link/NeF8P).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本练习之前，请确保你已执行了导入必要库和加载清理后的数据框架的先决步骤。有关这些步骤以及本练习的代码，你可以在[https://packt.link/NeF8P](https://packt.link/NeF8P)找到。
- en: 'Create a function that returns the value of the cost function and look at the
    value of the cost function over a range of parameters. You can use the following
    code to do this (note that this repeats code from the preceding section):'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个返回代价函数值的函数，并查看在一系列参数下代价函数的值。你可以使用以下代码来做到这一点（注意，这部分代码重复了前面的部分）：
- en: '[PRE3]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will obtain the following plot of the cost function:'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下的代价函数图：
- en: '![Figure 4.6: A cost function plot'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.6：代价函数图'
- en: '](img/B16925_4_6.jpg)'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_4_6.jpg)'
- en: 'Figure 4.6: A cost function plot'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.6：代价函数图
- en: 'Create a function for the value of the gradient. This is the analytical derivative
    of the cost function. Use this function to evaluate the gradient at the point
    *x = 4.5*, and then use this in combination with the learning rate to find the
    next step of the gradient descent process:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来求梯度值。这是代价函数的解析导数。使用此函数来计算在 *x = 4.5* 时的梯度，然后将其与学习率结合，找到梯度下降过程的下一步：
- en: '[PRE4]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is the next gradient descent step after *x = 4.5*.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是 *x = 4.5* 后的下一个梯度下降步骤。
- en: 'Plot the gradient descent path, from the starting point to the next point,
    using the following code:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码绘制梯度下降路径，从起点到下一个点：
- en: '[PRE5]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will obtain the following output:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 4.7: The first gradient descent path step'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.7：第一次梯度下降路径步骤'
- en: '](img/B16925_4_7.jpg)'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16925_4_7.jpg)'
- en: 'Figure 4.7: The first gradient descent path step'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.7：第一次梯度下降路径步骤
- en: Here, it appears as though we've taken a step in the right direction. However,
    it's clear that we've overshot where we want to be. It may be that our learning
    rate is too large, and consequently, we are taking steps that are too big. While
    tuning the learning rate will be a good way to converge toward an optimal solution
    more quickly, in this example, we can just continue illustrating the remainder
    of the process. Here, it looks like we may need to take a few more steps. In practice,
    gradient descent continues until the size of the steps become very small, or the
    change in the cost function becomes very small (you can specify how small by using
    the `tol` argument in the scikit-learn logistic regression), indicating that we're
    close enough to a good solution – that is, a `max_iter`).
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，看起来我们似乎朝着正确的方向迈出了第一步。然而，很明显我们已经越过了我们想要到达的位置。可能是我们的学习率过大，因此我们采取了过大的步伐。虽然调节学习率是加速收敛到最优解的好方法，但在这个例子中，我们可以继续演示过程的其余部分。这里看起来我们可能还需要再迈几步。实际上，梯度下降会一直进行，直到步伐变得非常小，或者代价函数的变化变得非常小（你可以通过使用`tol`参数在scikit-learn的逻辑回归中指定多小），这表示我们已经接近一个好的解——也就是`max_iter`。
- en: 'Perform 14 iterations to converge toward the local minimum of the cost function
    by using the following code snippet (note that `iterations = 15`, but the endpoint
    is not included in the call to `range()`):'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用以下代码片段执行14次迭代，以便向代价函数的局部最小值收敛（请注意，`iterations = 15`，但在调用 `range()` 时不包括终点）：
- en: '[PRE6]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You will obtain the following output:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This `for` loop stores the successive estimates in the `x_path` array, using
    the current estimate to calculate the derivative and find the next estimate. From
    the resulting values of the gradient descent process, it looks like we've gotten
    very close (`1.00021362`) to the optimal solution of 1.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the gradient descent path using the following code:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You will obtain the following output:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.8: The gradient descent path'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16925_4_8.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.8: The gradient descent path'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to repeat the previous procedure with different learning rates
    in order to see how they affect the gradient descent path. With the right learning
    rate, it's possible to converge on a highly accurate solution very quickly. While
    the choice of learning rate can be important in different machine learning applications,
    for logistic regression, the problem is usually pretty easy to solve and you don't
    need to select a learning rate in scikit-learn.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: As you experimented with different learning rates, did you notice what happened
    when the learning rate was greater than one? In this case, the step that we take
    in the direction of the decreasing error is too large and we actually wind up
    with a higher error. This problem can compound itself and actually lead the gradient
    descent process away from the region of minimum error. On the other hand, if the
    step size is too small, it can take a very long time to find the desired solution.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions of Logistic Regression
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since it is a classical statistical model, similar to the F-test and Pearson
    correlation we already examined, logistic regression makes certain assumptions
    about the data. While it's not necessary to follow every one of these assumptions
    in the strictest possible sense, it's good to be aware of them. That way, if a
    logistic regression model is not performing very well, you can try to investigate
    and figure out why, using your knowledge of the ideal situation that logistic
    regression is intended for. You may find slightly different lists of the specific
    assumptions from different resources. However, those that are listed here are
    widely accepted.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '**Features Are Linear in the Log Odds**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: We learned about this assumption in the previous chapter, *Chapter 3*, *Details
    of Logistic Regression and Feature Exploration*. Logistic regression is a linear
    model, so it will only work well as long as the features are effective at describing
    a linear trend in the log odds. In particular, logistic regression won't capture
    interactions, polynomial features, or the discretization of features, on its own.
    You can, however, specify all of these as "new features" – even though they may
    be engineered from existing features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Remember from the previous chapter that the most important feature from univariate
    feature exploration, `PAY_1`, was not found to be linear in the log odds.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '**No Multicollinearity of Features**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Multicollinearity means that features are correlated with each other. The worst
    violation of this assumption is when features are perfectly correlated with each
    other, such as one feature being identical to another, or when one feature equals
    another multiplied by a constant. We can investigate the correlation of features
    using the correlation plot that we''re already familiar with from univariate feature
    selection. Here is the correlation plot from the previous chapter:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 多重共线性意味着特征之间存在相关性。这个假设最严重的违反情况是特征之间完全相关，例如一个特征与另一个特征完全相同，或者一个特征等于另一个特征乘以常数。我们可以使用我们已经熟悉的相关性图来调查特征的相关性，这个图也在单变量特征选择中出现过。以下是上一章的相关性图：
- en: '![Figure 4.9: A correlation plot of features and the response'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9: 特征与响应的相关性图'
- en: '](img/B16925_4_9.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_9.jpg)'
- en: 'Figure 4.9: A correlation plot of features and the response'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.9: 特征与响应的相关性图'
- en: 'We can see from the correlation plot what perfect correlation looks like: since
    every feature and the response variable has a correlation of 1 with itself, we
    can see that a correlation of 1 is a light, cream color. From the color bar, which
    doesn''t include -1, we know there are no correlations with that value.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从相关性图中看到完美相关的样子：由于每个特征和响应变量与其自身的相关性为1，我们可以看到1的相关性是浅色的奶油色。从颜色条中，我们可以知道没有-1的相关性。
- en: Note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Jupyter notebook containing the code and the corresponding plots presented
    in this section can be found here: [https://packt.link/UOEMp](https://packt.link/UOEMp).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 包含本节中代码和相应图表的 Jupyter 笔记本可以在此找到：[https://packt.link/UOEMp](https://packt.link/UOEMp)。
- en: The clearest examples of correlated predictors in our case study data are the
    `BILL_AMT` features. It makes intuitive sense that bills might be similar from
    month to month for a given account. For instance, there may be an account that
    typically carries a balance of zero, or an account that has a large balance that
    is taking a while to pay off. Are any of the `BILL_AMT` features perfectly correlated?
    From *Figure 4.9*, it does not look like it. So, while these features may not
    contribute much independent information, we won't remove them at this point out
    of concern for multicollinearity.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例研究数据中，最明显的相关预测变量是`BILL_AMT`特征。直观来看，账单在同一个账户的每个月可能会相似。例如，可能有一个账户通常保持零余额，或者有一个账户存在大量余额，且需要较长时间才能还清。`BILL_AMT`特征之间是否存在完全相关？从*图
    4.9*来看，似乎没有。所以，虽然这些特征可能没有提供太多独立的信息，但我们目前不会出于担心多重共线性的原因而删除它们。
- en: '**The Independence of Observations**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**观察值的独立性**'
- en: This is a common assumption in classical statistical models, including linear
    regression. Here, the observations (or samples) are assumed to be independent.
    Does this make sense with the case study data? We'd want to confirm with our client
    whether the same individual can hold multiple credit accounts across the dataset
    and consider what to do depending on how common it was. Let's assume we've been
    told that in our data each credit account belongs to a unique person, so we may
    assume independence of observations in this respect.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是经典统计模型中的一个常见假设，包括线性回归。在这里，假设观察值（或样本）是独立的。这个假设在案例研究数据中是否合理？我们需要与客户确认，数据集中的同一个人是否可以拥有多个信用账户，并根据这种情况的普遍性来决定如何处理。假设我们已经被告知，在我们的数据中，每个信用账户都属于唯一的人，因此我们可以假设在这一点上观察值是独立的。
- en: 'Across different domains of data, some common violations of independence of
    observations are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的数据领域中，观察值独立性的一些常见违反情况如下：
- en: '**Spatial autocorrelation** of observations; for example, in natural phenomena
    such as soil types, where observations that are geographically close to each other
    may be similar to each other.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空间自相关**的观察值；例如，在自然现象中，如土壤类型，其中地理上彼此接近的观察值可能相似。'
- en: '**Temporal autocorrelation** of observations, which may occur in time series
    data. Observations at the current point in time are usually assumed to be correlated
    to the most recent point(s) in time.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间自相关**的观察值，通常出现在时间序列数据中。在时间序列数据中，通常假设当前时刻的观察值与最近的时刻（们）相关。'
- en: However, these issues are not relevant to our case study data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些问题与我们的案例研究数据无关。
- en: '**No Outliers**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**无异常值**'
- en: Outliers are observations where the value of the feature(s) or response are
    very far from most of the data or are different in some other way. A more appropriate
    term for an outlier observation of a feature value is a high leverage point, as
    the term "outlier" is usually applied to the response variable. However, in our
    binary classification problem, it's not possible to have an outlier value of the
    response variable, since it can only take on the values 0 and 1\. In practice,
    you may see both of these terms used to refer to features.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是指特征（或响应）的值与大多数数据的差异非常大，或者在其他方面有所不同。对于特征值的异常值，更恰当的术语是高杠杆点，因为“异常值”通常用于描述响应变量。然而，在我们的二分类问题中，不可能有响应变量的异常值，因为它只能取值0或1。在实际应用中，您可能会看到这两个术语都用于描述特征。
- en: 'To see why these kinds of points can have an adverse effect on linear models
    in general, take a look at this synthetic linear data with 100 points and the
    line of best fit that results from linear regression:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么这些类型的点通常会对线性模型产生不利影响，请看这个包含100个点的合成线性数据以及由线性回归得到的最佳拟合线：
- en: '![Figure 4.10: “Well-behaved” linear data and a regression fit'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.10：“表现良好”的线性数据和回归拟合'
- en: '](img/B16925_4_10.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_10.jpg)'
- en: 'Figure 4.10: "Well-behaved" linear data and a regression fit'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：“表现良好”的线性数据和回归拟合
- en: 'Here, the model intuitively appears to be a good fit for the data. However,
    what if an outlier feature value is added? To illustrate this, we add a point
    with an x value that is very different from most of the observations and a y value
    that is in a similar range to the other observations. We then show the resulting
    regression line:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型直观上看似与数据拟合得很好。然而，如果加入一个异常值特征值会怎样呢？为了说明这一点，我们添加了一个点，其x值与大多数观测值非常不同，而y值与其他观测值处于相似范围。然后，我们展示了结果回归线：
- en: '![Figure 4.11: A plot showing what happens when an outlier is included'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.11：显示当包含异常值时会发生什么的图表'
- en: '](img/B16925_4_11.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16925_4_11.jpg)'
- en: 'Figure 4.11: A plot showing what happens when an outlier is included'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：显示当包含异常值时会发生什么的图表
- en: Due to the presence of a single high leverage point, the regression model fit
    for all the data is no longer a very good representation of much of the data.
    This shows the potential effect of just a single data point on linear models,
    especially if that point doesn't appear to follow the same trend as the rest of
    the data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在一个高杠杆点，所有数据的回归模型拟合不再很好地代表大部分数据。这展示了单个数据点对线性模型的潜在影响，特别是当该点似乎与其余数据的趋势不一致时。
- en: There are methods for dealing with outliers. But a more fundamental question
    to ask is "Is data like this realistic?". If the data doesn't seem right, it is
    a good idea to ask the client whether the outliers are believable. If not, they
    should be excluded. However, if they do represent valid data, then non-linear
    models or other methods should be used.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 处理异常值有很多方法。但一个更根本的问题是：“这样的数据现实吗？”如果数据看起来不太对，可以询问客户这些异常值是否可信。如果不可信，应该将它们排除。但如果它们代表有效的数据，则应使用非线性模型或其他方法。
- en: With our case study data, we did not observe outliers in the histograms that
    we plotted during feature exploration. Therefore, we don't have this concern.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例研究数据中，在特征探索过程中绘制的直方图中并没有观察到异常值。因此，我们没有这个顾虑。
- en: '**How Many Features Should You Include?**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**你应该包含多少个特征？**'
- en: This is not so much an assumption as it is guidance on model building. There
    is no clear-cut law that states how many features to include in a logistic regression
    model. However, a common rule of thumb is the "rule of 10," which states that
    for every 10 occurrences of the rarest outcome class, 1 feature may be added to
    the model. So, for example, in a binary logistic regression problem with 100 samples,
    if the class balance has 20% positive outcomes and 80% negative outcomes, then
    there are only 20 positive outcomes in total, and so only 2 features should be
    used in the model. A "rule of 20" has also been suggested, which would be a more
    stringent limit on the number of features to include (1 feature in our example).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这不完全是一个假设，更像是构建模型的指导原则。没有明确的定律说明在逻辑回归模型中应该包含多少个特征。然而，一个常见的经验法则是“10的法则”，即每出现10次最稀有的结果类别，就可以在模型中添加1个特征。例如，在一个包含100个样本的二分类逻辑回归问题中，如果类别平衡是20%的正样本和80%的负样本，那么正样本总数只有20个，因此模型中应该仅使用2个特征。此外，还建议采用“20的法则”，它对包含的特征数量设定了更严格的限制（在我们的例子中为1个特征）。
- en: Another point to consider in the case of binary features, such as those that
    result from one-hot encoding, is how many samples will have a positive value for
    that feature. If the feature is very imbalanced, in other words, with very few
    samples containing either a 1 or a 0, it may not make sense to include it in the
    model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: For the case study data, we are fortunate to have a relatively large number
    of samples and relatively balanced features, so these are not concerns.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/SnX3y](https://packt.link/SnX3y).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'The Motivation for Regularization: The Bias-Variance Trade-Off'
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can extend the basic logistic regression model that we have learned about
    by using a powerful concept known as **shrinkage** or **regularization**. In fact,
    every logistic regression that you have fit so far in scikit-learn has used some
    amount of regularization. That is because it is a default option in the logistic
    regression model object. However, until now, we have ignored it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'As you learn about these concepts in greater depth, you will also become familiar
    with a few foundational concepts in machine learning: **overfitting**, **underfitting**,
    and the **bias-variance trade-off**. A model is said to overfit the training data
    if the performance of the model on the training data (for example, the ROC AUC)
    is substantially better than the performance on a held-out test set. In other
    words, good performance on the training set does not generalize to the unseen
    test set. We started to discuss these concepts in *Chapter 2*, *Introduction to
    Scikit-Learn and Model Evaluation*, when we distinguished between model training
    and test scores.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'When a model is overfitted to the training data, it is said to have high **variance**.
    In other words, whatever variability exists in the training data, the model has
    learned this very well – in fact, too well. This will be reflected in a high model
    training score. However, when such a model is used to make predictions on new
    and unseen data, the performance is lower. Overfitting is more likely in the following
    circumstances:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: There are a large number of features available in relation to the number of
    samples. In particular, there may be so many possible features that it is cumbersome
    to directly inspect all of them, like we were able to do with the case study data.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A complex model, that is, more complex than logistic regression, is used. These
    include models such as gradient boosting ensembles or neural networks.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under these circumstances, the model has an opportunity develop more complex
    **hypotheses** about the relationships between features and the response variable
    in the training data during model fitting, making overfitting more likely.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if a model is not fitting the training data very well, this is
    known as underfitting, and the model is said to have high **bias**.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'We can examine the differences between underfitting, overfitting, and the ideal
    that sits in between, by fitting polynomial models on some hypothetical data:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: Quadratic data with underfit, overfit, and ideal models'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_12.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.12: Quadratic data with underfit, overfit, and ideal models'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4.12*, we can see that including too few features, in this case,
    a linear model of *y* with just two features, a slope and an intercept, is clearly
    not a good representation of the data. This is known as an underfit model. However,
    if we include too many features, that is, many high-degree polynomial terms, such
    as *x*2, *x*3, *x*4,… *x*10, we can fit the training data almost perfectly. However,
    this is not necessarily a good thing. When we look at the results of the overfitted
    model in between the training data points, where new predictions may need to be
    made, we can see that the model is unstable and may not provide reliable predictions
    for data that was not in the training set. We can tell this just based on an intuitive
    understanding of the relationship between the features and the response variable,
    which we can get from visualizing the data.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/SnX3y](https://packt.link/SnX3y).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: The synthetic data for this example was generated by a second-degree (that is,
    quadratic) polynomial. Knowing this, we could easily find the ideal model by fitting
    a second-degree polynomial to the training data, as shown in *Figure 4.12*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In general, however, we won't know what the ideal model formulation is ahead
    of time. For this reason, we need to compare training and test scores to assess
    whether a model may be overfitting or underfitting.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, it may be desirable to introduce some bias into the model training
    process, especially if this decreases overfitting and increases model performance
    on new, unseen data. In this way, it may be possible to leverage the bias-variance
    trade-off to improve a model. We can use **regularization** methods to accomplish
    this. Additionally, we may also be able to use these methods for **variable selection**
    as part of the modeling process. Using a predictive model to select variables
    is an alternative to the univariate feature selection methods that we've already
    explored. We begin to experiment with these concepts in the following exercise.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.02: Generating and Modeling Synthetic Classification Data'
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll observe overfitting in practice by using a synthetic
    dataset. Consider yourself in the situation of having been given a binary classification
    dataset with many candidate features (200), where you don''t have time to look
    through all of them individually. It''s possible that some of these features are
    highly correlated or related in some other way. However, with this many variables,
    it can be difficult to effectively explore all of them. Additionally, the dataset
    has relatively few samples: only 1,000\. We are going to generate this challenging
    dataset by using a feature of scikit-learn that allows you to create synthetic
    datasets for making conceptual explorations such as this. Perform the following
    steps to complete the exercise:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Before you begin this exercise, please make sure you have executed the prerequisite
    steps of importing the necessary libraries. These steps along with the code for
    this exercise can be found at [https://packt.link/mIMsT](https://packt.link/mIMsT).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `make_classification`, `train_test_split`, `LogisticRegression`,
    and `roc_auc_score` classes using the following code:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Notice that we''ve imported several familiar classes from scikit-learn, in
    addition to a new one that we haven''t seen before: `make_classification`. This
    class does just what its name indicates – it makes data for a classification problem.
    Using the various keyword arguments, you can specify how many samples and features
    to include, and how many classes the response variable will have. There is also
    a range of other options that effectively control how "easy" the problem will
    be to solve.'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For more information, refer to [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html).
    Suffice to say that we've selected options here that make a reasonably easy-to-solve
    problem, with some curveballs thrown in. In other words, we expect high model
    performance, but we'll have to work a little bit to get it.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate a dataset with two variables, `x_synthetic` and `y_synthetic`. `x_synthetic`
    has the 200 candidate features, and `y_synthetic` the response variable, each
    for 1,000 samples. Use the following code:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Examine the shape of the dataset and the class fraction of the response variable
    using the following code:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You will obtain the following output:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After checking the shape of the output, note that we''ve generated an almost
    perfectly balanced dataset: close to a 50/50 class balance. It is also important
    to note that we''ve generated all the features so that they have the same `shift`
    and `scale` – that is, a mean of 0 with a standard deviation of 1\. Making sure
    that the features are on the same scale, or have roughly the same range of values,
    is a key point for using regularization methods – and we''ll see why later. If
    the features in a raw dataset are on widely different scales, it is advisable
    to normalize them so that they are on the same scale. Scikit-learn has the functionality
    to make this easy, which we''ll learn about in the activity at the end of this
    chapter.'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the first few features as histograms to show that the range of values
    is the same using the following code:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You will obtain the following output:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.13: Histograms for the first 4 of 200 synthetic features'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16925_4_13.jpg)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.13: Histograms for the first 4 of 200 synthetic features'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Because we generated this dataset, we don't need to directly examine all 200
    features to make sure that they're on the same scale. So, what are the possible
    concerns with this dataset? The data is balanced in terms of the class fractions
    of the response variable, so we don't need to undersample, oversample, or use
    other methods that are helpful for imbalanced data. What about relationships among
    the features themselves, and the features and response variable? There are a lot
    of these relationships and it is a challenge to investigate them all directly.
    Based on our rule of thumb (that is, 1 feature allowed for every 10 samples of
    the rarest class), 200 features is too many. We have 500 observations in the rarest
    class, so by that rule, we shouldn't have more than 50 features. It's possible
    that with so many features, the model training procedure will overfit. We will
    now start to learn how to use options in the scikit-learn logistic regression
    to prevent this.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into training and test sets using an 80/20 split, and then instantiate
    a logistic regression model object using the following code:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice here that we are specifying some new options in the logistic regression
    model, which, so far, we have not paid attention to. First, we specify the `penalty`
    argument to be `l1`. This means we are going to use `C` parameter to be equal
    to 1,000\. `C` is the "inverse of regularization strength," according to the scikit-learn
    documentation ([https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)).
    This means that higher values of `C` correspond to less regularization. By choosing
    a relatively large number, such as 1,000, we are using relatively little regularization.
    The default value of `C` is 1\. So, we are not really using much regularization
    here, rather, we are simply becoming familiar with the options to do so. Finally,
    we are using the `liblinear` solver, which we have used in the past.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Although we happen to be using scaled data here (all features have a mean of
    0 and standard deviation of 1), it's worth noting at this point that among the
    various options we have available for solvers, `liblinear` is "robust to unscaled
    data." Also note that `liblinear` is one of only two solver options that support
    the L1 penalty – the other option being `saga`.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can find out more information on available solvers at [https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression).
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the logistic regression model on the training data using the following
    code:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the output:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Calculate the training score using this code by first getting predicted probabilities
    and then obtaining the ROC AUC:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output should be as follows:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Calculate the test score similar to how the training score was obtained:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output should be as follows:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: From these results, it's apparent that the logistic regression model has overfit
    the data. That is, the ROC AUC score on the training data is substantially higher
    than that of the test data.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Lasso (L1) and Ridge (L2) Regularization
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before applying regularization to a logistic regression model, let's take a
    moment to understand what regularization is and how it works. The two ways of
    regularizing logistic regression models in scikit-learn are called `penalty =
    'l1'` or `'l2'`. These are called "penalties" because the effect of regularization
    is to add a penalty, or a cost, for having larger values of the coefficients in
    a fitted logistic regression model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve already learned, coefficients in a logistic regression model describe
    the relationship between the log odds of the response and each of the features.
    Therefore, if a coefficient value is particularly large, then a small change in
    that feature will have a large effect on the prediction. When a model is being
    fit and is learning the relationship between features and the response variable,
    the model can start to learn the noise in the data. We saw this previously in
    *Figure 4.12*: if there are many features available when fitting a model, and
    there are no guardrails on the values that their coefficients can take, then the
    model fitting process may try to discover relationships between the features and
    the response variable that won''t generalize to new data. In this way, the model
    becomes tuned to the unpredictable, random noise that accompanies real-world,
    imperfect data. Unfortunately, this only serves to increase the model''s skill
    at predicting the training data, which is not our ultimate goal. Therefore, we
    should seek to root out such spurious relationships from the model.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Lasso and ridge regularization use different mathematical formulations to accomplish
    this goal. These methods work by making changes to the cost function that is used
    for model fitting, which we introduced previously as the log-loss function. Lasso
    regularization uses what is called the **1-norm** (hence the term L1):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14: Log-loss equation with lasso penalty'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_14.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.14: Log-loss equation with lasso penalty'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The 1-norm, which is the first term in the equation in *Figure 4.14*, is just
    the sum of the absolute values of the coefficients of the *m* different features.
    The absolute value is used because having a coefficient that's large in either
    the positive or negative directions can contribute to overfitting. So, what else
    is different about this cost function compared to the log-loss function that we
    saw earlier? Well, now there is a *C* factor that is multiplied by the fraction
    in front of the sum of the log-loss function.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: This is the "inverse of regularization strength," as described in the scikit-learn
    documentation ([https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)).
    Since this factor is in front of the term of the cost function that calculates
    the prediction error, as opposed to the term that does regularization, then making
    it larger makes the prediction error more important in the cost function, while
    regularization is made less important. In short, *larger values of C lead to less
    regularization* in the scikit-learn implementation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'L2, or ridge regularization, is similar to L1, except that instead of the sum
    of absolute values of coefficients, ridge uses the sum of their squares, called
    the **2-norm**:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15: Log-loss equation with ridge penalty'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_15.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.15: Log-loss equation with ridge penalty'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you look at the cost functions for logistic regression in the scikit-learn
    documentation, the specific form is different than what is used here, but the
    overall idea is similar. Additionally, after you become comfortable with the concepts
    of lasso and ridge penalties, you should be aware that there is an additional
    regularization method called **elastic-net**, which is a combination lasso and
    ridge.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Are There Two Different Formulations of Regularization?**'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'It may be that one or the other will provide better out-of-sample performance,
    so you may wish to test them both. There is another key difference in these methods:
    the L1 penalty also performs feature selection, in addition to regularization.
    It does this by setting some coefficient values to exactly zero during the regularization
    process, effectively removing features from the model. L2 regularization makes
    the coefficient values smaller but does not completely eliminate them. Not all
    solver options in scikit-learn support both L1 and L2 regularization, so you will
    need to select an appropriate solver for the regularization technique you want
    to use.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical details of why L1 regularization removes features but L2 doesn't
    are beyond the scope of this book. However, for a more thorough explanation of
    this topic and further reading in general, we recommend the very readable (and
    free) resource, *An Introduction to Statistical Learning by Gareth James*, et
    al. In particular, see *page 222* of the corrected 7th printing, for a helpful
    graphic on the difference between L1 and L2 regularization.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '**Intercepts and Regularization**'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: We have not discussed intercepts very much, other than to note that we have
    been estimating them with our linear models, along with the coefficients that
    go with each feature. So, should you use an intercept? The answer is probably
    yes, until you've developed an advanced understanding of linear models and are
    certain that in a specific case you should not. However, such cases do exist,
    for example, in a linear regression where the features and the response variable
    have all been normalized to have a mean of zero.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Intercepts don't go with any particular feature. Therefore, it doesn't make
    much sense to regularize them, as they shouldn't contribute to overfitting. Notice
    that in the regularization penalty term for L1, the summation starts with *j =
    1*, and similarly for L2, we have skipped *σ*0, which is the intercept term.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the ideal situation: not regularizing the intercept. However, some
    of the solvers in scikit-learn, such as `liblinear`, actually do regularize the
    intercept. There is an `intercept_scaling` option that you can supply to the model
    class to counteract this effect. We have not illustrated this here as, although
    it is theoretically incorrect, regularizing the intercept often does not have
    much effect on the model''s predictive quality in practice.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling and Regularization**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: As noted in the previous exercise, it is best practice to `LIMIT_BAL` in our
    dataset, is much larger than other features, such as `PAY_1`, it may, in fact,
    be desirable to have a larger value for the coefficient of `PAY_1` and a smaller
    value for that of `LIMIT_BAL` in order to put their effects on the same scale
    in the linear combination of features and coefficients that are used for model
    prediction. Normalizing all the features before using regularization avoids complications
    such as this that arise simply from differences in scale.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: In fact, scaling your data may also be necessary, depending on which solver
    you are using. The different variations on the gradient descent process available
    in scikit-learn may or may not be able to work effectively with unscaled data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '**The Importance of Selecting the Right Solver**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve come to learn, the different solvers available for logistic regression
    in scikit-learn have different behaviors regarding the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Whether they support both L1 and L2 regularization
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How they treat the intercept during regularization
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How they deal with unscaled data
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are other differences as well. A helpful table comparing these and other
    traits is available at [https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression).
    You can use this table to decide which solver is appropriate for your problem.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To summarize this section, we have learned the mathematical foundations of lasso
    and ridge regularization. *These methods work by shrinking the coefficient values
    toward 0, and in the case of the lasso, setting some coefficients to exactly 0
    and thus performing feature selection*. You can imagine that in our example of
    overfitting in *Figure 4.12*, if the complex, overfitted model had some coefficients
    shrunk toward 0, it would look more like the ideal model, which has fewer coefficients.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a plot of a regularized regression model, using the same high-degree
    polynomial features as the overfitted model, but with a ridge penalty:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16: An overfit model and regularized model using the same features'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_16.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.16: An overfit model and regularized model using the same features'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: The regularized model looks similar to the ideal model, demonstrating the ability
    of regularization to correct overfitting. Note, however, that the regularized
    model should not be recommended for extrapolation. Here, we can see that the regularized
    model starts to increase toward the right side of *Figure 4.16*. This increase
    should be viewed with suspicion, as there is nothing in the training data that
    makes it clear that this would be expected. This is an example of the general
    view that the *extrapolation of model predictions outside the range of training
    data is not recommended*. However, it is clear from *Figure 4.16* that even if
    we didn't have knowledge of the model that was used to generate this synthetic
    data (as we typically don't have knowledge of the data-generating process in real-world
    predictive modeling work), we can still use regularization to reduce the effect
    of overfitting when a large number of candidate features are available.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '**Model and Feature Selection**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'L1 regularization is one way to use a model, such as logistic regression, to
    perform feature selection. Other methods include forward or backward **stepwise
    selection** from the pool of candidate features. Here is the high-level idea behind
    these methods: in the case of **forward selection**, features are added to the
    model one at a time, and the out-of-sample performance is observed along the way.
    At each iteration, the addition of all possible features from the candidate pool
    is considered, and the one resulting in the greatest increase in the out-of-sample
    performance is chosen. When adding additional features ceases to improve the model''s
    performance, no more features need to be added from the candidates. In the case
    of **backward selection**, you first start with all the features in the model
    and determine which one you should remove: the one resulting in the smallest decrease
    in the out-of-sample performance. You can continue removing features in this way
    until the performance begins to decrease appreciably.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/aUBMb](https://packt.link/aUBMb).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-Validation: Choosing the Regularization Parameter'
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you may suspect that we could use regularization in order to decrease
    the overfitting we observed when we tried to model the synthetic data in *Exercise
    4.02*, *Generating and Modeling Synthetic Classification Data*. The question is,
    how do we choose the regularization parameter, *C*? *C* is an example of a model
    **hyperparameter**. Hyperparameters are different from the parameters that are
    estimated when a model is trained, such as the coefficients and the intercept
    of a logistic regression. Rather than being estimated by an automated procedure
    like the parameters are, hyperparameters are input directly by the user as keyword
    arguments, typically when instantiating the model class. So, how do we know what
    values to choose?
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters are more difficult to estimate than parameters. This is because
    it is up to the data scientist to determine what the best value is, as opposed
    to letting an optimization algorithm find it. However, it is possible to programmatically
    choose hyperparameter values, which could be viewed as an optimization procedure
    in its own right. Practically speaking, in the case of the regularization parameter
    *C*, this is most commonly done by fitting the model on one set of data with a
    particular value of *C*, determining model training performance, and then assessing
    the out-of-sample performance on another set of data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: We are already familiar with the concept of using model training and test sets.
    However, there is a key difference here; for instance, what would happen if we
    were to use the test set multiple times in order to see the effect of different
    values of *C*?
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: It may occur to you that after the first time you use the unseen test set to
    assess the out-of-sample performance for a particular value of *C*, it is no longer
    an "unseen" test set. While only the training data was used for estimating the
    model parameters (that is, the coefficients and the intercept), now the test data
    is being used to estimate the hyperparameter *C*. Effectively, the test data has
    now become additional training data in the sense that it is being used to find
    a good value for the hyperparameter.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, it is common to divide the data into three parts: a training
    set, a test set, and a **validation set**. The validation set serves multiple
    purposes:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimating Hyperparameters**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The validation set can be repeatedly used to assess the out-of-sample performance
    with different hyperparameter values to select hyperparameters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '**A Comparison of Different Models**'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: In addition to finding hyperparameter values for a model, the validation set
    can be used to estimate the out-of-sample performance of different models; for
    example, if we wanted to compare logistic regression to random forest.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Management Best Practices**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, it's up to you to figure out how to divide up your data
    for different predictive modeling tasks. In the ideal case, you should reserve
    a portion of your data for the very end of the process, after you've already selected
    model hyperparameters and also selected the best model. This **unseen test set**
    is reserved for the last step, when it can be used to assess the endpoint of your
    model-building efforts, to see how the final model generalizes to new unseen data.
    When reserving the test set, it is good practice to make sure that the features
    and responses have similar characteristics to the rest of the data. In other words,
    the class fraction should be the same, and the distribution of features should
    be similar. This way, the test data should be representative of the data you built
    the model with.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: While model validation is a good practice, it raises the question of whether
    the particular split we choose for the training, validation, and test data has
    any effect on the outcomes that we are tracking. For example, perhaps the relationship
    between the features and the response variable is slightly different in the unseen
    test set that we have reserved, or in the validation set, versus the training
    set. It is likely impossible to eliminate all such variability, but we can use
    the method of **cross-validation** to avoid placing too much faith in one particular
    split of the data.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn provides convenient functions to facilitate cross-validation analyses.
    These functions play a similar role to `train_test_split`, which we have already
    been using, although the default behavior is somewhat different. Let''s get familiar
    with them now. First, import these two classes:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Similar to `train_test_split`, we need to specify what proportion of the dataset
    we would like to use for training versus testing. However, with cross-validation
    (specifically the **k-fold cross-validation** that was implemented in the classes
    we just imported), rather than specifying a proportion directly, we simply indicate
    how many folds we would like – that is, the "**k folds**." The idea here is that
    the data will be divided into **k** equal proportions. For example, if we specify
    4 folds, then each fold will have 25% of the data. These folds will be the test
    data in four separate instances of model training, while the remaining 75% from
    each fold will be used to train the model. In this procedure, each data point
    gets used as training data a total of *k - 1* times, and as test data only once.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'When instantiating the class, we indicate the number of folds, whether or not
    to shuffle the data before splitting, and a random seed if we want repeatable
    results across different runs:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, we''ve instantiated an object with four folds and no shuffling. The way
    in which we use the object that is returned, which we''ve called `k_folds`, is
    by passing the features and response data that we wish to use for cross-validation,
    to the `.split` method of this object. This outputs an `X_syn_train` and `y_syn_train`,
    we could loop through the splits like this:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The iterator will return the row indices of `X_syn_train` and `y_syn_train`,
    which we can use to index the data. Inside this `for` loop, we can write code
    to use these indices to select data for repeatedly training and testing a model
    object with different subsets of the data. In this way, we can get a robust indication
    of the out-of-sample performance when using one particular hyperparameter value,
    and then repeat the whole process using another hyperparameter value. Consequently,
    the cross-validation loop may sit **nested** inside an outer loop over different
    hyperparameter values. We'll illustrate this in the following exercise.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'First though, what do these splits look like? If we were to simply plot the
    indices from `train_index` and `test_index` as different colors, we would get
    something that looks like this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17: Training/test splits for k-folds with four folds and no shuffling'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_17.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.17: Training/test splits for k-folds with four folds and no shuffling'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we see that with the options we''ve indicated for the `KFold` class,
    the procedure has simply taken the first 25% of the data, according to the order
    of rows, as the first test fold, then the next 25% of data for the second fold,
    and so on. But what if we wanted stratified folds? In other words, what if we
    wanted to ensure that the class fractions of the response variable were equal
    in every fold? While `train_test_split` allows this option as a keyword argument,
    there is a separate `StratifiedKFold` class that implements this for cross-validation.
    We can illustrate how the stratified splits will appear as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Figure 4.18: Training/test splits for stratified k-folds'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_18.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.18: Training/test splits for stratified k-folds'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4.18*, we can see that there has been some amount of "shuffling"
    between the different folds. The procedure has moved samples between folds as
    necessary to ensure that the class fractions in each fold are equal.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Now, what if we want to shuffle the data to choose samples from throughout the
    range of indices for each test fold? First, why might we want to do this? Well,
    with the synthetic data that we've created for our problem, we can be certain
    that the data is in no particular order. However, in many real-world situations,
    the data we receive may be sorted in some way.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: For instance, perhaps the rows of the data have been ordered by the date an
    account was created, or by some other logic. Therefore, it can be a good idea
    to shuffle the data before splitting. This way, any traits that might have been
    used for sorting can be expected to be consistent throughout the folds. Otherwise,
    the data in different folds may have different characteristics, possibly leading
    to different relationships between features and response.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'This can lead to a situation where model performance is uneven between the
    folds. In order to "mix up" the folds throughout all the row indices of a dataset,
    all we need to do is set the `shuffle` parameter to `True`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Figure 4.19: Training/test splits for stratified k-folds with shuffling'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_19.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.19: Training/test splits for stratified k-folds with shuffling'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: With shuffling, the test folds are spread out randomly, and fairly evenly, across
    the indices of the input data.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: K-fold cross-validation is a widely used method in data science. However, the
    choice of how many folds to use depends on the particular dataset at hand. Using
    a smaller number of folds means that the amount of training data in each fold
    will be relatively small. Therefore, this increases the chances that the model
    will underfit, as models generally work better when trained on more data. It's
    a good idea to try a few different numbers of folds and see how the mean and the
    variability of the k-fold test score changes. Common numbers of folds can range
    anywhere from 4 or 5 to 10.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In the event of a very small dataset, it may be necessary to use as much data
    as possible for training in the cross-validation folds. In this scenario, you
    can use a method called **leave-one-out cross-validation** (**LOOCV**). In LOOCV,
    the test set for each fold consists of a single sample. In other words, there
    will be as many folds as there are samples in the training data. For each iteration,
    the model is trained on all but one sample, and a prediction is made for that
    sample. The accuracy, or other performance metric, can then be constructed using
    these predictions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Other concerns that relate to the creation of a test set, such as choosing an
    out-of-time test set for problems where observations from the past must be used
    to predict future events, also apply to cross-validation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: In *Exercise 4.02*, *Generating and Modeling Synthetic Classification Data*,
    we saw that fitting a logistic regression on our training data led to overfitting.
    Indeed, the test score (*ROC AUC = 0.81*) was substantially lower than the training
    score (*ROC AUC = 0.94*). We had essentially used very little or no regularization
    by setting the regularization parameter *C* to a relatively large value (1,000).
    Now we will see what happens when we vary *C* through a wide range of values.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for generating the plots presented in this section can be found here:
    [https://packt.link/37Zks](https://packt.link/37Zks).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.03: Reducing Overfitting on the Synthetic Data Classification Problem'
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise is a continuation of *Exercise 4.02*, *Generating and Modeling
    Synthetic Classification Data*. Here, we will use a cross-validation procedure
    in order to find a good value for the hyperparameter *C*. We will do this by using
    only the training data, reserving the test data for after model building is complete.
    Be prepared – this is a long exercise – but it will illustrate a general procedure
    that you will be able to use with many different kinds of machine learning models,
    so it is worth the time spent here. Perform the following steps to complete the
    exercise:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you begin this exercise, you need to execute some prerequisite steps
    that can be found in the following notebook along with the code for this exercise:
    [https://packt.link/JqbsW](https://packt.link/JqbsW).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Vary the value of the regularization parameter, *C*, to range from *C = 1000*
    to *C = 0.001*. You can use the following snippets to do this.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, define exponents, which will be powers of 10, as follows:'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is the output of the preceding code:'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, vary *C* by the powers of 10, as follows:'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output of the preceding code:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It's generally a good idea to vary the regularization parameter by powers of
    10, or by using a similar strategy, as training models can take a substantial
    amount of time, especially when using k-fold cross-validation. This gives you
    a good idea of how a wide range of *C* values impacts the bias-variance trade-off,
    without needing to train a very large number of models. In addition to the integer
    powers of 10, we also include points on the log10 scale that are about halfway
    between. If it seems like there is some interesting behavior in between these
    relatively widely spaced values, you can add more granular values for *C* in a
    smaller part of the range of possible values.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import the `roc_curve` class:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We'll continue to use the ROC AUC score for assessing, training, and testing
    performance. Now that we have several values of *C* to try and several folds (in
    this case four) for the cross-validation, we will want to store the training and
    test scores for each fold and for each value of *C*.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a function that takes the `k_folds` cross-validation splitter, the array
    of *C* values (`C_vals`), the model object (`model`), and the features and response
    variable (`X` and `Y`, respectively) as inputs, to explore different amounts of
    regularization with k-fold cross-validation. Use the following code:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The function we started in this step will return the ROC AUCs and ROC curve
    data. The return block will be written during a later step in the exercise. For
    now, you can simply write the preceding code as is, because we will be defining
    `k_folds`, `C_vals`, `model`, `X`, and `Y` as we progress in the exercise.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Within this function block, create a NumPy array to hold model performance
    data, with dimensions `n_folds` by `len(C_vals)`:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Next, we'll store the arrays of true and false positive rates and thresholds
    that go along with each of the test ROC AUC scores in a **list of lists**.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This is a convenient way to store all this model performance information, as
    a list in Python can contain any kind of data, including another list. Here, each
    item of the inner lists in the **list of lists** will be a tuple holding the arrays
    of TPR, FPR, and the thresholds for each of the folds, for each of the C values.
    Tuples are an ordered collection data type in Python, similar to lists, but unlike
    lists they are immutable: the items in a tuple can''t be changed after the tuple
    is created. When a function returns multiple values, like the roc_curve function
    of scikit-learn, these values can be output to a single variable, which will be
    a tuple of those values. This way of storing results should be more obvious when
    we access these arrays later in order to examine them.'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a list of empty lists using `[[]]` and `*len(C_vals)` as follows:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Using `*len(C_vals)` indicates that there should be a list of tuples of metrics
    (TPR, FPR, thresholds) for each value of *C*.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have learned how to loop through the different folds for cross-validation
    in the preceding section. What we need to do now is write an outer loop in which
    we will nest the cross-validation loop.
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create an outer loop for training and testing each of the k-folds for each
    value of *C*:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can reuse the same model object that we have already, and simply set a new
    value of *C* within each run of the loop. Inside the loop of *C* values, we run
    the cross-validation loop. We begin by yielding the training and test data row
    indices for each split.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Obtain the training and test indices for each fold:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Index the features and response variable to obtain the training and test data
    for this fold using the following code:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The training data for the current fold is then used to train the model.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model on the training data, as follows:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This will effectively "reset" the model from whatever the previous coefficients
    and intercept were to reflect the training on this new data.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The training and test ROC AUC scores are then obtained, as well as the arrays
    of TPRs, FPRs, and thresholds that go along with the test data.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Obtain the training ROC AUC score:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Obtain the test ROC AUC score:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Obtain the test ROC curves for each fold using the following code:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We will use a fold counter to keep track of the folds that are incremented,
    and once outside the cross-validation loop, we print a status update to standard
    output. Whenever performing long computational procedures, it's a good idea to
    periodically print the status of the job so that you can monitor its progress
    and confirm that things are still working correctly. This cross-validation procedure
    will likely take only a few seconds on your laptop, but for longer jobs this can
    be especially reassuring.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Increment the fold counter using the following code:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Write the following code to indicate the progress of execution for each value
    of *C*:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Write the code to return the ROC AUCs and ROC curve data and finish the function:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that we will continue to use the split into four folds that we illustrated
    previously, but you are encouraged to try this procedure with different numbers
    of folds to compare the effect.
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have covered a lot of material in the preceding steps. You may want to take
    a few moments to review this with your classmates in order to make sure that you
    understand each part. Running the function is comparatively simple. That is the
    beauty of a well-designed function – all the complicated parts get abstracted
    away, allowing you to concentrate on usage.
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the function we''ve designed to examine cross-validation performance, with
    the *C* values that we previously defined, and by using the model and data we
    were working with in the previous exercise. Use the following code:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'When you run this code, you should see the following output populate below
    the code cell as the cross-validation is completed for each value of *C*:'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: So, what do the results of the cross-validation look like? There are a few ways
    to examine this. It is useful to look at the performance of each fold individually,
    so that you can see how variable the results are.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This tells you how different subsets of your data perform as test sets, leading
    to a general idea of the range of performance you can expect from the unseen test
    set. What we're interested in here is whether or not we are able to use regularization
    to alleviate the overfitting that we saw. We know that using *C = 1,000* led to
    overfitting – we know this from comparing the training and test scores. But what
    about the other *C* values that we've tried? A good way to visualize this will
    be to plot the training and test scores on the *y-axis* and the values of *C*
    on the *x-axis*.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Loop over each of the folds to view their results individually by using the
    following code:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You will obtain the following output:'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.20: The training and test scores for each fold and C-value'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16925_4_20.jpg)'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.20: The training and test scores for each fold and C-value'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see that for each fold of the cross-validation, as *C* decreases, the
    training performance also decreases. However, at the same time, the test performance
    increases. For some folds and values of *C*, the test ROC AUC score actually exceeds
    that of the training data, while for others, these two metrics simply come closer
    together. In all cases, we can say that the *C* values of 10-1.5 and 10-2 appear
    to have a similar test performance, which is substantially higher than the test
    performance of *C = 10*3\. So, it appears that regularization has successfully
    addressed our overfitting problem.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: But what about the lower values of *C*? For values that are lower than 10-2,
    the ROC AUC metric suddenly drops to 0.5\. As you know, this value means that
    the classification model is essentially useless, performing no better than a coin
    flip. You are encouraged to check on this later when exploring how regularization
    affects the coefficient values; however, this is what happens when so much L1
    regularization is applied that all model coefficients shrink to 0\. Obviously,
    such models are not useful to us, as they encode no information about the relationship
    between the features and response variable.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Looking at the training and test performance of each k-fold split is helpful
    for gaining insights into the variability of model performance that may be expected
    when the model is scored on new, unseen data. But in order to summarize the results
    of the k-folds procedure, a common approach is to average the performance metric
    over the folds, for each value of the hyperparameter being considered. We'll perform
    this in the next step.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the mean of training and test ROC AUC scores for each *C* value using
    the following code:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![Figure 4.21: The average training and test scores across cross-validation
    folds'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16925_4_21.jpg)'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.21: The average training and test scores across cross-validation folds'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From this plot, it's clear that *C = 10*-1.5 and *10*-2 are the best values
    of *C*. There is little or no overfitting here, as the average training and test
    scores are nearly the same. You could search a finer grid of *C* values (that
    is *C = 10*-1.1*,* *10*-1.2, and so on) in order to more precisely locate a *C*
    value. However, from our graph, we can see that either *C = 10*-1.5 or *C = 10*-2
    will likely be good solutions. We will move forward with *C = 10*-1.5.
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Examining the summary metric of ROC AUC is a good way to get a quick idea of
    how models will perform. However, for any real-world business application, you
    will often need to choose a specific threshold, which goes along with specific
    true and false positive rates. These will be needed to use the classifier to make
    the required "yes" or "no" decision, which, in our case study, is a prediction
    of whether an account will default. For this reason, it is useful to look at the
    ROC curves across the different folds of the cross-validation. To facilitate this,
    the preceding function has been designed to return the true and false positive
    rates, and thresholds, for each test fold and value of *C*, in the `cv_test_roc`
    list of lists. First, we need to find the index of the outer list that corresponds
    to the *C* value that we've chosen, *10*-1.5.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To accomplish this, we could simply look at our list of *C* values and count
    by hand, but it's safer to do this programmatically by finding the index of the
    non-zero element of a Boolean array, as is shown in the next step.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use a Boolean array to find the index where *C = 10*-1.5 and convert it to
    an integer data type with the following code:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Here is the output of the preceding code:'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Convert the integer version of the Boolean array into a single integer index
    using the `nonzero` function with this code:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here is the output of the preceding code:'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We have now successfully located the *C* value that we wish to use.
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Access the true and false positive rates in order to plot the ROC curves for
    each fold:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'You will obtain the following output:'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.22: ROC curves for each fold'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16925_4_22.jpg)'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.22: ROC curves for each fold'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It appears that there is a fair amount of variability in the ROC curves. For
    example, if, for some reason, we want to limit the false positive rate to 40%,
    then from the plot it appears that we may be able to achieve a true positive rate
    of anywhere from approximately 60% to 80%. You can find the exact values by examining
    the arrays that we have plotted. This gives you an idea of how much variability
    in performance can be expected when deploying the model on new data. Generally,
    the more training data that is available, then the less variability there will
    be between the folds of cross-validation, so this could also be a sign that it
    would be a good idea to collect additional data, especially if the variability
    between training folds seems unacceptably high. You also may wish to try different
    numbers of folds with this procedure so as to see the effect on the variability
    of results between folds.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While normally we would try other models on our synthetic data problem, such
    as a random forest or support vector machine, if we imagine that in cross-validation,
    logistic regression proved to be the best model, we would decide to make this
    our final choice. When the final model is selected, all the training data can
    be used to fit the model, using the hyperparameters chosen with cross-validation.
    It's best to use as much data as possible in model fitting, as models typically
    work better when trained on more data.
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Train the logistic regression on all the training data from our synthetic problem
    and compare the training and test scores, using the held-out test set as shown
    in the following steps.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the final step in the model selection process. You should only use the
    unseen test set after your choice of model and hyperparameters are considered
    finished, otherwise it will not be "unseen."
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set the *C* value and train the model on all the training data with this code:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here is the output of the preceding code:'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Obtain predicted probabilities and the ROC AUC score for the training data
    with this code:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here is the output of the preceding code:'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Obtain predicted probabilities and the ROC AUC score for the test data with
    this code:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Here is the output of the preceding code:'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Here, we can see that by using regularization, the model training and test scores
    are similar, indicating that the overfitting problem has been greatly reduced.
    The training score is lower since we have introduced bias into the model at the
    expense of variance. However, this is OK, since the test score, which is the most
    important part, is higher. The out-of-sample test score is what matters for predictive
    capability. You are encouraged to check that these training and test scores are
    similar to those from the cross-validation procedure by printing the values from
    the arrays that we plotted previously; you should find that they are.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In a real-world project, before delivering this model to your client for production
    use, you may wish to train the model on all the data that you were given, including
    the unseen test set. This follows the idea that the more data a model has seen,
    the better it is likely to perform in practice. However, some practitioners prefer
    to only use models that have been tested, meaning you would deliver the model
    trained only on the training data, not including the test set.
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We know that L1 regularization works by decreasing the magnitude (that is, absolute
    value) of coefficients of the logistic regression. It can also set some coefficients
    to zero, therefore performing feature selection. In the next step, we will determine
    how many coefficients were set to zero.
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Access the coefficients of the trained model and determine how many do not
    equal zero (`!= 0`) with this code:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output should be as follows:'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This code takes the sum of a Boolean array indicating the locations of non-zero
    coefficients, so it shows how many coefficients in the model did not get set to
    zero by L1 regularization. Only 2 of the 200 features were selected!
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Examine the value of the intercept using this code:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output should be as follows:'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: This shows that the intercept was regularized to 0.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we accomplished several goals. We used the k-fold cross-validation
    procedure to tune the regularization hyperparameter. We saw the power of regularization
    for reducing overfitting, and in the case of L1 regularization in logistic regression,
    selecting features.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Many machine learning algorithms offer some type of feature selection capability.
    Many also require the tuning of hyperparameters. The function here that loops
    over hyperparameters, and performs cross-validation, is a powerful concept that
    generalizes to other models. Scikit-learn offers functionality to make this process
    easier; in particular, the `sklearn.model_selection.GridSearchCV` procedure, which
    applies cross-validation to a grid search over hyperparameters. A **grid search**
    can be helpful when there are multiple hyperparameters to tune, by looking at
    all combinations of the ranges of different hyperparameters that you specify.
    A **randomized grid search** can speed up this process by randomly choosing a
    smaller number of combinations when an exhaustive grid search would take too long.
    Once you are comfortable with the concepts illustrated here, you are encouraged
    to streamline your workflow with convenient functions like these.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Options for Logistic Regression in Scikit-Learn
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have used and discussed most of the options that you may supply to scikit-learn
    when instantiating or tuning the hyperparameters of a `LogisticRegression` model
    class. Here, we list them all and provide some general advice on their usage:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23: A complete list of options for the logistic regression model
    in scikit-learn'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_23.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.23: A complete list of options for the logistic regression model in
    scikit-learn'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: If you are in doubt regarding which option to use for logistic regression, we
    recommend you consult the scikit-learn documentation for further guidance ([https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)).
    Some options, such as the regularization parameter *C*, or the choice of a penalty
    for regularization, will need to be explored through the cross-validation process.
    Here, as with many choices to be made in data science, there is no universal approach
    that will apply to all datasets. The best way to see which options to use with
    a given dataset is to try several of them and see which gives the best out-of-sample
    performance. Cross-validation offers you a robust way to do this.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Data, Pipelines, and Interaction Features in Scikit-Learn
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Scaling Data**'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to the synthetic data we were just working with, the case study data
    is relatively large. If we want to use L1 regularization, then according to the
    official documentation ([https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)),
    we ought to use the `saga` solver. However, this solver is not robust to unscaled
    datasets. Hence, we need to be sure to scale the data. This is also a good idea
    whenever doing regularization, so all the features are on the same scale and are
    equally penalized by the regularization process. A simple way to make sure that
    all the features have the same scale is to put them all through the transformation
    of subtracting the minimum and dividing by the range from minimum to maximum.
    This transforms each feature so that it will have a minimum of 0 and a maximum
    of 1\. To instantiate the `MinMaxScaler` scaler that does this, we can use the
    following code:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '**Pipelines**'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we used a logistic regression model in the cross-validation loop.
    However, now that we''re scaling data, what new considerations are there? The
    scaling is effectively "learned" from the minimum and maximum values of the training
    data. After this, a logistic regression model would be trained on data scaled
    by the extremes of the model training data. However, we won''t know the minimum
    and maximum values of the new, unseen data. So, following the philosophy of making
    cross-validation an effective indicator of model performance on unseen data, we
    need to use the minimum and maximum values of the training data in each cross-validation
    fold in order to scale the test data in that fold, before making predictions on
    the test data. Scikit-learn has the functionality to facilitate the combination
    of several training and test steps for situations such as this: the `Pipeline`.
    Our pipeline will consist of two steps: the scaler and the logistic regression
    model. These can both be fit on the training data and then used to make predictions
    on the test data. The process of fitting a pipeline is executed as a single step
    in the code, so all the parts of the pipeline are fit at once in this sense. Here
    is how a `Pipeline` is instantiated:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '**Interaction Features**'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Considering the case study data, do you think a logistic regression model with
    all possible features would be overfit or underfit? You can think about this from
    the perspective of rules of thumb, such as the "rule of 10," and the number of
    features (17) versus samples (26,664) that we have. Alternatively, you can consider
    all the work we've done so far with this data. For instance, we've had a chance
    to visualize all the features and ensure they make sense. Since there are relatively
    few features, and we have relatively high confidence that they are high quality
    because of our data exploration work, we are in a different situation than with
    the synthetic data exercises in this chapter, where we had a large number of features
    about which we knew relatively little. So, it may be that overfitting will be
    less of an issue with our case study at this point, and the benefits of regularization
    may not be significant.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, it may be that we will underfit the model using only the 17 features
    that came with the data. One strategy to deal with this is to engineer new features.
    Some simple feature engineering techniques we''ve discussed include interaction
    and polynomial features. Polynomials may not make sense given the way in which
    some of the data has been encoded; for example, *-1*2 *= 1*, which may not be
    sensible for `PAY_1`. However, we may wish to try creating interaction features
    to capture the relationships between features. `PolynomialFeatures` can be used
    to create interaction features only, without polynomial features. The example
    code is as follows:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Here, `degree` represents the degree of the polynomial features, `interaction_only`
    takes a Boolean value (setting it to `True` indicates that only interaction features
    will be created), and so does `include_bias`, which adds an intercept to the model
    (the default value is `False`, which is correct here as the logistic regression
    model will add an intercept).
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.01: Cross-Validation and Feature Engineering with the Case Study
    Data'
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we''ll apply the knowledge of cross-validation and regularization
    that we''ve learned in this chapter to the case study data. We''ll perform basic
    feature engineering. In order to estimate parameters for the regularized logistic
    regression model for the case study data, which is larger in size than the synthetic
    data that we''ve worked with, we''ll use the `saga` solver. In order to use this
    solver, and for the purpose of regularization, we''ll need to `Pipeline` class
    in scikit-learn. Once you have completed the activity, you should obtain an improved
    cross-validation test performance with the use of interaction features, as shown
    in the following diagram:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24: Improved model test performance'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16925_4_24.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.24: Improved model test performance'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the activity:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Select the features from the DataFrame of the case study data.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can use the list of feature names that we've already created in this chapter,
    but be sure not to include the response variable, which would be a very good (but
    entirely inappropriate) feature!
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Make a training/test split using a random seed of 24.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll use this going forward and reserve this test data as the unseen test set.
    By specifying the random seed, we can easily create separate notebooks with other
    modeling approaches using the same training data.
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Instantiate `MinMaxScaler` to scale the data.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate a logistic regression model with the `saga` solver, L1 penalty,
    and set `max_iter` to `1000` as we want the solver to have enough iterations to
    find a good solution.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the `Pipeline` class and create a pipeline with the scaler and the logistic
    regression model, using the names `'scaler'` and `'model'` for the steps, respectively.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `get_params` and `set_params` methods to see how to view the parameters
    from each stage of the pipeline and change them.
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a smaller range of *C* values to test with cross-validation, as these
    models will take longer to train and test with more data than our previous exercise;
    we recommend *C = [10*2*, 10, 1, 10*-1*, 10*-2*, 10*-3*]*.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a new version of the `cross_val_C_search` function called `cross_val_C_search_pipe`.
    Instead of the `model` argument, this function will take a `pipeline` argument.
    The changes inside the function will be to set the *C* value using `set_params(model__C
    = <value you want to test>)` on the pipeline, replacing the model with the pipeline
    for the `fit` and `predict_proba` methods, and accessing the *C* value using `pipeline.get_params()['model__C']`
    for the printed status update.
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run this function as in the previous exercise, but using the new range of *C*
    values, the pipeline you created, and the features and response variable from
    the training split of the case study data.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You may see warnings here, or in later steps, regarding the non-convergence
    of the solver; you could experiment with the `tol` or `max_iter` options to try
    and achieve convergence, although the results you obtain with `max_iter = 1000`
    are likely to be sufficient.
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Plot the average training and test ROC AUC across folds for each *C* value.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create interaction features for the case study data and confirm that the number
    of new features makes sense.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the cross-validation procedure and observe the model performance when
    using interaction features.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that this will take substantially more time, due to the larger number of
    features, but it will probably take less than 10 minutes. So, does the average
    cross-validation test performance improve with the interaction features? Is regularization
    useful?
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Jupyter notebook containing the Python code for this activity can be found
    at [https://packt.link/ohGgX](https://packt.link/ohGgX). Detailed step-wise solution
    to this activity can be found via [this link](B16925_Solution_ePub.xhtml#_idTextAnchor155).
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  id: totrans-444
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the final details of logistic regression and
    continued to understand how to use scikit-learn to fit logistic regression models.
    We gained more visibility into how the model fitting process works by learning
    about the concept of a cost function, which is minimized by the gradient descent
    procedure to estimate parameters during model fitting.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: We also learned of the need for regularization by introducing the concepts of
    underfitting and overfitting. In order to reduce overfitting, we saw how to adjust
    the cost function to regularize the coefficients of a logistic regression model
    using an L1 or L2 penalty. We used cross-validation to select the amount of regularization
    by tuning the regularization hyperparameter. To reduce underfitting, we saw how
    to do some simple feature engineering with interaction features for the case study
    data.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now familiar with some of the most important concepts in machine learning.
    We have, so far, only used a very basic classification model: logistic regression.
    However, as you increase your toolbox of models that you know how to use, you
    will find that the concepts of overfitting and underfitting, the bias-variance
    trade-off, and hyperparameter tuning will come up again and again. These ideas,
    as well as convenient scikit-learn implementations of the cross-validation functions
    that we wrote in this chapter, will help us through our exploration of more advanced
    prediction methods.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about decision trees, an entirely different
    type of predictive model than logistic regression, and the random forests that
    are based on them. However, we will use the same concepts that we learned here,
    cross-validation and hyperparameter search, to tune these models.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
