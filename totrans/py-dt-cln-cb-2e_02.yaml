- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anticipating Data Cleaning Issues When Working with HTML, JSON, and Spark Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter continues our work on importing data from a variety of sources
    and the initial checks we should do on the data after importing it. Over the last
    25 years, data analysts have found that they increasingly need to work with data
    in non-tabular, semi-structured forms. Sometimes, they even create and persist
    data in those forms. We will work with a common alternative to traditional tabular
    datasets in this chapter, JSON, but the general concepts can be extended to XML
    and NoSQL data stores such as MongoDB. We will also go over common issues that
    occur when scraping data from websites.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysts have also been finding that increases in the volume of data to
    be analyzed have been even greater than improvements in machine processing power,
    at least those computing resources that are available locally. Working with big
    data sometimes requires us to rely on technology like Apache Spark, which can
    take advantage of distributed resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will work through the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Importing simple JSON data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing more complicated JSON data from an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing data from web pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Spark data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persisting JSON data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Versioning data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need pandas, NumPy, and Matplotlib to complete the recipes in this
    chapter. I used pandas 2.1.4, but the code will run on pandas 1.5.3 or later.
  prefs: []
  type: TYPE_NORMAL
- en: The code in this chapter can be downloaded from the book’s GitHub repository,
    [https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition).
  prefs: []
  type: TYPE_NORMAL
- en: Importing simple JSON data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**JavaScript Object Notation** (**JSON**) has turned out to be an incredibly
    useful standard for transferring data from one machine, process, or node to another.
    Often, a client sends a data request to a server, upon which that server queries
    the data in local storage and then converts it from something like an SQL Server,
    MySQL, or PostgreSQL table or tables into JSON, which the client can consume.
    This is sometimes complicated further by the first server (say, a web server)
    forwarding the request to a database server. JSON facilitates this, as does XML,
    by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Being readable by humans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being consumable by most client devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not being limited in structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSON is quite flexible, which means that it can accommodate just about anything,
    no matter how unwise. The structure can even change within a JSON file, so different
    keys might be present at different points. For example, the file might begin with
    some explanatory keys that have a very different structure than the remaining
    *data* keys or some keys might be present in some cases but not others. We will
    go over some approaches for dealing with that messiness (uh, flexibility).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to work with data on news stories about political candidates in
    this recipe. This data is made available for public use at [dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0ZLHOK](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0ZLHOK).
    I have combined the JSON files there into one file and randomly selected 60,000
    news stories from the combined data. This sample (`allcandidatenewssample.json`)
    is available in the GitHub repository of this book.
  prefs: []
  type: TYPE_NORMAL
- en: We will do a little work with list and dictionary comprehension in this recipe.
    *DataCamp* has good guides on list comprehension ([https://www.datacamp.com/community/tutorials/python-list-comprehension](https://www.datacamp.com/community/tutorials/python-list-comprehension))
    and dictionary comprehension ([https://www.datacamp.com/community/tutorials/python-dictionary-comprehension](https://www.datacamp.com/community/tutorials/python-dictionary-comprehension))
    if you are feeling a little rusty or have limited or no experience with list and
    dictionary comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will import a JSON file into pandas after doing some data checking and cleaning:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `json` and `pprint` libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pprint` improves the display of the lists and dictionaries that are returned
    when we load JSON data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Load the JSON data and look for potential issues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `json load` method to return data on news stories about political candidates.
    `load` returns a list of dictionaries. Use `len` to get the size of the list,
    which is the total number of news stories in this case. (Each list item is a dictionary
    with keys for the title, source, and so on, and their respective values.) Use
    `pprint` to display the first two dictionaries. Get the value from the source
    key for the first list item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Check for differences in the structure of the dictionaries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use `Counter` to check for any dictionaries in the list with fewer or more
    than the 9 keys that are normal. Look at a few of the dictionaries with almost
    no data (those with just two keys) before removing them. Confirm that the remaining
    list of dictionaries has the expected length – `60000-2382=57618`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Generate counts from the JSON data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the dictionaries just for *Politico* (a website that covers political news)
    and display a couple of dictionaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Get the `source` data and confirm that it has the anticipated length.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Show the first few items in the new `sources` list. Generate a count of news
    stories by source and display the 10 most popular sources. Notice that stories
    from *The Hill* can have `TheHill` (without a space) or `The Hill` as the value
    for `source`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Fix any errors in the values in the dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fix the `source` values for `The Hill`. Notice that `The Hill` is now the most
    frequent source for news stories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Create a pandas DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass the JSON data to the pandas `DataFrame` method. Convert the `date` column
    to a `datetime` data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Confirm that we are getting the expected values for `source`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Also, rename the `date` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We now have a pandas DataFrame with only the news stories where there is meaningful
    data and with the values for `source` fixed.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `json.load` method returns a list of dictionaries. This makes it possible
    to use a number of familiar tools when working with this data: list methods, slicing,
    list comprehensions, dictionary updates, and so on. There are times (maybe when
    you just have to populate a list or count the number of individuals in a given
    category) when there is no need to use pandas.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Steps 2* to *6*, we use list methods to do many of the same checks we have
    done with pandas in previous recipes. In *Step* *3*, we use `Counter` with a list
    comprehension (`Counter([len(item) for item in candidatenews])`) to get the number
    of keys in each dictionary. This tells us that there are 2,382 dictionaries with
    just 2 keys and 416 with 10\. We use `next` to look for an example of dictionaries
    with fewer or more than 9 keys to get a sense of the structure of those items.
    We use slicing to show 2 dictionaries with 2 keys to see if there is any data
    in those dictionaries. We then select only those dictionaries with more than 2
    keys.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we create a subset of the list of dictionaries, one that just has
    `source` equal to `Politico`, and take a look at a couple of items. We then create
    a list with just the source data and use `Counter` to list the 10 most common
    sources in *Step 5*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 6* demonstrates how to replace key values conditionally in a list of
    dictionaries. In this case, we update the key value to `The Hill` whenever `key
    (k)` is `source` and `value (v)` is `TheHill`. The `for k, v in newsdict.items()`
    section is the unsung hero of this line. It loops through all key/value pairs
    for all dictionaries in `candidatenews`.'
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to create a pandas DataFrame by passing the list of dictionaries
    to the pandas `DataFrame` method. We do this in *Step 7*. The main complication
    is that we need to convert the date column from a string to a date since dates
    are just strings in JSON.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Step* *5* and *6*, we use `item.get('source')` instead of `item['source']`.
    This is handy when there might be missing keys in a dictionary. `get` returns
    `None` when the key is missing, but we can use an optional second argument to
    specify a value to return.
  prefs: []
  type: TYPE_NORMAL
- en: I renamed the `date` column `storydate` in *Step 8*. This is not necessary but
    is a good idea. Not only does `date` not tell you anything about what the dates
    actually represent but it is also so generic a column name that it is bound to
    cause problems at some point.
  prefs: []
  type: TYPE_NORMAL
- en: The news stories data fits nicely into a tabular structure. It makes sense to
    represent each list item as one row and the key/value pairs as columns and column
    values for that row. There are no significant complications, such as key values
    that are themselves lists of dictionaries. Imagine an `authors` key for each story
    with a list item for each author as the key value, and that list item is a dictionary
    of information about the author. This is not at all unusual when working with
    JSON data in Python. The next recipe shows how to work with data structured in
    this way.
  prefs: []
  type: TYPE_NORMAL
- en: Importing more complicated JSON data from an API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we discussed one significant advantage (and challenge)
    of working with JSON data – its flexibility. A JSON file can have just about any
    structure its authors can imagine. This often means that this data does not have
    the tabular structure of the data sources we have discussed so far and that pandas
    DataFrames have. Often, analysts and application developers use JSON precisely
    because it does not insist on a tabular structure. I know I do!
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving data from multiple tables often requires us to do a one-to-many merge.
    Saving that data to one table or file means duplicating data on the “one” side
    of the one-to-many relationship. For example, student demographic data is merged
    with data on the courses studied, and the demographic data is repeated for each
    course. With JSON, duplication is not required to capture these items of data
    in one file. We can have data on the courses studied nested within the data for
    each student.
  prefs: []
  type: TYPE_NORMAL
- en: But doing analysis with JSON structured in this way will eventually require
    us to either manipulate the data in a very different way than we are used to doing
    or convert the JSON to a tabular form. We examine the first approach in the *Classes
    that handle non-tabular data structures* recipe in *Chapter 12*, *Automate Data
    Cleaning with User-Defined Functions and Classes*. This recipe takes the second
    approach. It uses a very handy tool for converting selected nodes of JSON to a
    tabular structure – `json_normalize`.
  prefs: []
  type: TYPE_NORMAL
- en: We first use an API to get JSON data because that is how JSON is frequently
    consumed. One advantage of retrieving the data with an API, rather than working
    from a file we have saved locally, is that it is easier to rerun our code when
    the source data is refreshed.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe assumes you have the `requests` and `pprint` libraries already installed.
    If they are not installed, you can install them with pip. From the terminal (or
    PowerShell in Windows), enter `pip` `install` `requests` and `pip` `install` `pprint`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the structure of the JSON file that is created when using
    the Collections API of the Cleveland Museum of Art. There is a helpful *info*
    section at the beginning, but we are interested in the *data* section. This data
    does not fit nicely into a tabular data structure. There may be several `citations`
    objects and several `creators` objects for each collection object. I have abbreviated
    the JSON file to save space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: The API used in this recipe is provided by the Cleveland Museum of Art. It is
    available for public use at [https://openaccess-api.clevelandart.org/](https://openaccess-api.clevelandart.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Since the call to the API retrieves real-time data, you may get different output
    from running the code in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a DataFrame from the museum’s collections data with one row for each
    `citation`, and the `title` and `creation_date` duplicated:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `json`, `requests`, and `pprint` libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need the `requests` library to use an API to retrieve JSON data. `pprint`
    improves the display of lists and dictionaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Use an API to load the JSON data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make a `get` request to the Collections API of the Cleveland Museum of Art.
    Use the query string to indicate that you just want collections from African-American
    artists. Display the first collection item. I have truncated the output for the
    first item to save space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Flatten the JSON data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a DataFrame from the JSON data using the `json_normalize` method. Indicate
    that the number of citations will determine the number of rows, and that `accession_number`,
    `title`, `creation_date`, `collection`, `creators`, and `type` will be repeated.
    Observe that the data has been flattened by displaying the first two observations,
    transposing them with the `.T` option to make it easier to view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Pull the `birth_year` value from `creators`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This gives us a pandas DataFrame with one row for each `citation` for each collection
    item, with the collection information (`title`, `creation_date`, and so on) duplicated.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We work with a much more *interesting* JSON file in this recipe than in the
    previous one. Each object in the JSON file is an item in the collection of the
    Cleveland Museum of Art. Nested within each collection item are one or more citations.
    The only way to capture this information in a tabular DataFrame is to flatten
    it. There are also one or more dictionaries for the creators of the collection
    item (the artist or artists). That dictionary (or dictionaries) contains the `birth_year`
    value that we want.
  prefs: []
  type: TYPE_NORMAL
- en: We want one row for every citation for all collection items. To understand this,
    imagine that we are working with relational data and have a collections table
    and a citations table and that we are doing a one-to-many merge from collections
    to citations. We do something similar with `json_normalize` by using *citations*
    as the second parameter. That tells `json_normalize` to create one row for each
    citation and use the key values in each citation dictionary – for `citation`,
    `page_number`, and `url` – as data values.
  prefs: []
  type: TYPE_NORMAL
- en: The third parameter in the call to `json_normalize` has the list of column names
    for the data that will be repeated with each citation. Notice that `access_number`,
    `title`, `creation_date`, `collection`, `creators`, and `type` are repeated in
    the first two observations. `Citation` and `page_number` change. (`url` is the
    same value for the first and second citations. Otherwise, it would also change.)
  prefs: []
  type: TYPE_NORMAL
- en: This still leaves us with the problem of the `creators` dictionaries (there
    can be more than one creator). When we ran `json_normalize`, it grabbed the value
    for each key we indicated (in the third parameter) and stored it in the data for
    that column and row, whether that value was simple text or a list of dictionaries,
    as is the case for creators. We take a look at the first (and in this case, only)
    `creators` item for the first collections row in *Step 4*, naming it `creator`.
    (Note that the `creators` list is duplicated across all `citations` for a collection
    item, just as the values for `title`, `creation_date`, and so on are.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We want the birth year of the first creator for each collection item, which
    can be found at `creator[0][''birth_year'']`. To create a `birthyear` series using
    this, we use `apply` and a `lambda` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We take a closer look at lambda functions in *Chapter 6*, *Cleaning and Exploring
    Data with Series Operations*. Here, it is helpful to think of the `x` as representing
    the `creators` series, so `x[0]` gives us the list item we want, `creators[0]`.
    We grab the value from the `birth_year` key.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that we left out some of the JSON returned by the API in
    our call to `json_normalize`. The first parameter that we passed to `json_normalize`
    was `camcollections['data']`. Effectively, we ignore the info object at the beginning
    of the JSON data. The information we want does not start until the data object.
    This is not very different conceptually from the `skiprows` parameter in the second
    recipe of the previous chapter. There is sometimes metadata like this at the beginning
    of JSON files.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding recipe demonstrates some useful techniques for doing data integrity
    checks without pandas, including list operations and comprehensions. Those are
    all relevant to the data in this recipe as well.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data from web pages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use **Beautiful Soup** in this recipe to scrape data from a web page and
    load that data into pandas. **Web scraping** is very useful when there is data
    on a website that is updated regularly but there is no API. We can rerun our code
    to generate new data whenever the page is updated.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the web scrapers we build can be broken when the structure of
    the targeted page changes. That is less likely to happen with APIs because they
    are designed for data exchange and carefully curated with that end in mind. The
    priority for most web designers is the quality of the display of information,
    not the reliability and ease of data exchange. This causes data cleaning challenges
    that are unique to web scraping, including HTML elements that house the data in
    surprising and changing locations, formatting tags that obfuscate the underlying
    data, and explanatory text that aid data interpretation being difficult to retrieve.
    In addition to these challenges, scraping presents data cleaning issues that are
    familiar, such as changing data types in columns, less-than-ideal headings, and
    missing values. We will deal with data issues that occur most frequently in this
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need Beautiful Soup installed to run the code in this recipe. You can
    install it with pip by entering `pip install beautifulsoup4` in a terminal window
    or Windows PowerShell.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will scrape data from a web page, find the following table on that page,
    and load it into a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image1956.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: COVID-19 data from countries with the lowest cases per million
    in population'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data** **note**'
  prefs: []
  type: TYPE_NORMAL
- en: I created this web page, [http://www.alrb.org/datacleaning/highlowcases.html](http://www.alrb.org/datacleaning/highlowcases.html),
    based on COVID-19 data for public use from *Our World in Data*, available at [https://ourworldindata.org/covid-cases](https://ourworldindata.org/covid-cases).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We scrape the COVID-19 data from the website and do some routine data checks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pprint`, `requests`, and `BeautifulSoup` libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Parse the web page and get the header row of the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use Beautiful Soup’s `find` method to get the table we want and then use `find_all`
    to retrieve the elements nested within the `th` elements for that table. Create
    a list of column labels based on the text of the `th` rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Get the data from the table cells.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find all of the table rows for the table we want. For each table row, find the
    `th` element and retrieve the text. We will use that text for our row labels.
    Also, for each row, find all the `td` elements (the table cells with the data)
    and save text from all of them in a list.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us `datarows`, which has all the numeric data in the table. (You
    can confirm that it matches the table from the web page.) We then insert the `labelrows`
    list (which has the row headings) at the beginning of each list in `datarows`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Load the data into pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass the `datarows` list to the `DataFrame` method of pandas. Notice that all
    data is read into pandas with the object data type and that some data has values
    that cannot be converted into numeric values in their current form (due to the
    commas):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Fix the column names and convert the data to numeric values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove spaces from the column names. Remove all non-numeric data from the first
    columns with data, including the commas (`str.replace("[^0-9]",""`). Convert to
    numeric values, handling most columns as integers, the `last_date` column as a
    datetime, `median_age` as float, and leaving `rowheadings` as an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We have now created a pandas DataFrame from an `html` table.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beautiful Soup is a very useful tool for finding specific HTML elements in a
    web page and retrieving text from them. You can get one HTML element with `find`
    and get one or more with `find_all`. The first argument for both `find` and `find_all`
    is the HTML element to get. The second argument takes a Python dictionary of attributes.
    You can retrieve text from all of the HTML elements you find with `get_text`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some amount of looping is usually necessary to process the elements and text,
    as with *Step 2* and *Step 3*. These two statements in *Step 2* are fairly typical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The first statement finds all the `th` elements we want and creates a Beautiful
    Soup result set called `theadrows` from the elements it found. The second statement
    iterates over the `theadrows` Beautiful Soup result set using the `get_text` method
    to get the text from each element and then stores it in the `labelcols` list.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3* is a little more involved but makes use of the same Beautiful Soup
    methods. We find all of the table rows (`tr`) in the target table (`rows = bs.find(''table'',
    {''id'':''tblLowCases''}).tbody.find_all(''tr'')`). We then iterate over each
    of those rows, finding the `th` element and getting the text in that element (`rowlabels
    = row.find(''th'').get_text()`). We also find all of the table cells (`td`) for
    each row (`cells = row.find_all(''td'', {''class'':''data''}`) and get the text
    from all table cells (`cellvalues = [j.get_text() for j in cells]`). Note that
    this code is dependent on the class of the `td` elements being `data`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we insert the row labels we get from the `th` elements at the beginning
    of each list in `datarows`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: In *step 4*, we use the `DataFrame` method to load the list we created in *Steps
    2* and *3* into pandas. We then do some cleaning similar to what we have done
    in previous recipes in this chapter. We use `string replace` to remove spaces
    from column names and to remove all non-numeric data, including commas, from what
    are otherwise valid numeric values. We convert all columns, except for the `rowheadings`
    column, to numeric.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our scraping code is dependent on several aspects of the web page’s structure
    not changing: the ID of the table of interest, the presence of `th` tags with
    column and row labels, and the `td` elements continuing to have their class equal
    to data. The good news is that if the structure of the web page does change, this
    will likely only affect the `find` and `find_all` calls. The rest of the code
    would not need to change.'
  prefs: []
  type: TYPE_NORMAL
- en: Working with Spark data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with large datasets, we sometimes need to rely on distributed resources
    to clean and manipulate our data. With Apache Spark, analysts can take advantage
    of the combined processing power of many machines. We will use PySpark, a Python
    API for working with Spark, in this recipe. We will also go over how to use PySpark
    tools to take a first look at our data, select parts of our data, and generate
    some simple summary statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run the code in this section, you need to get Spark running on your computer.
    If you have installed Anaconda, you can follow these steps to work with Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: Install `Java` with `conda install openjdk`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install `PySpark` with `conda install pyspark` or `conda install -c conda forge
    pyspark`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install `findspark` with `conda install -c conda-forge findspark`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Note**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Installation of PySpark can be tricky, particularly setting the necessary environment
    variables. While `findspark` helps with this, a common problem is that the Java
    installation is not recognized when running PySpark commands. If you get the dreaded
    `JAVA_HOME is not set` error when attempting to run the code in this recipe, then
    there is a good chance that that is your problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Step 3* at the following link shows you how to set the environment variables
    for Linux, macOS, and Windows machines: [https://www.dei.unipd.it/~capri/BDC/PythonInstructions.html](https://www.dei.unipd.it/~capri/BDC/PythonInstructions.html).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will work with the land temperature data from *Chapter 1*, *Anticipating
    Data Cleaning Issues When Importing Tabular Data with pandas*, and the candidate
    news data from this chapter. All data and the code we will be running in this
    recipe are available in the GitHub repository for this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data** **note**'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset, taken from the Global Historical Climatology Network integrated
    database, is made available for public use by the United States National Oceanic
    and Atmospheric Administration at [https://www.ncei.noaa.gov/data/global-historical-climatology-network-monthly/v4/](https://www.ncei.noaa.gov/data/global-historical-climatology-network-monthly/v4/).
  prefs: []
  type: TYPE_NORMAL
- en: We will use PySpark in this recipe to read data we have in local storage.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To read and explore the data, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start a Spark session and load the land temperature data. We can use
    the read method of the session object to create a Spark DataFrame. We indicate
    that the first row of the CSV file we are importing has a header:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that the `read` method returns a Spark DataFrame, not a pandas DataFrame.
    We will need to use different methods to view our data than those we have used
    so far.
  prefs: []
  type: TYPE_NORMAL
- en: We load the full dataset, not just a 100,000-row sample as we did in the first
    chapter. If your system is low on resources, you can import the `landtempssample.csv`
    file instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should take a look at the number of rows and the column names and data types
    that were imported. The `temp` column was read as a string. It should be a float.
    We will fix that in a later step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at the data for a few rows. We can choose a subset of the columns
    by using the `select` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should fix the data type of the `temp` column. We can use the `withColumn`
    function to do a range of column operations in Spark. Here, we use it to cast
    the `temp` column to `float`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can run summary statistics on the `temp` variable. We can use the `describe`
    method for that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Spark session’s read method can import a variety of different data files,
    not just CSV files. Let’s try that with the `allcandidatenews` JSON file that
    we worked with earlier in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the `count` and `printSchema` methods again to look at our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also generate some summary statistics on the `story_position` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These steps demonstrate how to import data files into a Spark DataFrame, view
    the structure of the data, and generate summary statistics.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PySpark API significantly reduces the amount of work Python programmers
    have to do to use Apache Spark to handle large data files. We get methods to work
    with that are not very different from the methods we use with pandas DataFrames.
    We can see the number of rows and columns, examine and change data types, and
    get summary statistics.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At some point in our analysis, we might want to convert the Spark DataFrame
    into a pandas DataFrame. This is a fairly expensive process, and we will lose
    the benefits of working with Spark, so we typically will not do that unless we
    are at the point of our analysis when we require the pandas library, or a library
    that depends on pandas. But when we need to move to pandas, it is very easy to
    do – though if you are working with a lot of data and your machine’s processor
    and RAM are not exactly top of the line, you might want to start the conversion
    and then go have some tea or coffee.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code converts the `allcandidatenews` Spark DataFrame that we
    created to a pandas DataFrame and displays the resulting DataFrame structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We have been largely working with non-traditional data stores in this chapter:
    JSON files, data from HTML pages, and Spark files. We often reach a point in our
    data cleaning work where it makes sense to preserve the results of that cleaning
    by persisting data. At the end of *Chapter 1*, *Anticipating Data Cleaning Issues
    When Importing Tabular Data with pandas*, we examined how to persist tabular data.
    That works fine in cases where our data can be captured well with columns and
    rows. When it cannot (say, when we are working with a JSON file that has complicated
    subdocuments), we might want to preserve that structure when persisting data.
    In the next recipe, we go over persisting JSON data.'
  prefs: []
  type: TYPE_NORMAL
- en: Persisting JSON data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several reasons why we might want to serialize a JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: We may have retrieved the data with an API but need to keep a snapshot of the
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data in the JSON file is relatively static and informs our data cleaning
    and analysis over multiple phases of a project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We might decide that the flexibility of a schema-less format such as JSON helps
    us solve many data cleaning and analysis problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is worth highlighting this last reason to use JSON – that it can solve many
    data problems. Although tabular data structures clearly have many benefits, particularly
    for operational data, they are often not the best way to store data for analysis
    purposes. In preparing data for analysis, a substantial amount of time is spent
    either merging data from different tables or dealing with data redundancy when
    working with flat files. Not only are these processes time-consuming but every
    merge or reshaping leaves the door open to a data error of broad scope. This can
    also mean that we end up paying too much attention to the mechanics of manipulating
    data and too little to the conceptual issues at the core of our work.
  prefs: []
  type: TYPE_NORMAL
- en: We return to the Cleveland Museum of Art collections data in this recipe. There
    are at least three possible units of analysis for this data file – the collection
    item level, the creator level, and the citation level. JSON allows us to nest
    citations and creators within collections. (You can examine the structure of the
    JSON file in the *Getting ready* section of this recipe.) This data cannot be
    persisted in a tabular structure without flattening the file, which we did in
    the *Importing more complicated JSON data from an API* recipe earlier in this
    chapter. In this recipe, we will use two different methods to persist JSON data,
    each with its own advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be working with data on the Cleveland Museum of Art’s collection of
    works by African-American artists. The following is the structure of the JSON
    data returned by the API. It has been abbreviated to save space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will serialize the JSON data using two different methods:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `pandas`, `json`, `pprint`, `requests`, and `msgpack` libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the JSON data from an API. I have abbreviated the JSON output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Save and reload the JSON file using Python’s `json` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Persist the JSON data in human-readable form. Reload it from the saved file
    and confirm that it worked by retrieving the `creators` data from the first `collections`
    item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Save and reload the JSON file using `msgpack`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use the Cleveland Museum of Art’s Collections API to retrieve collections
    items. The `african_american_artists` flag in the query string indicates that
    we just want collections for those creators. `json.loads` returns a dictionary
    called `info` and a list of dictionaries called `data`. We check the length of
    the `data` list. This tells us that there are 778 items in collections. We then
    display the first item of collections to get a better look at the structure of
    the data. (I have abbreviated the JSON output.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We save and then reload the data using Python’s JSON library in *Step 3*. The
    advantage of persisting the data in this way is that it keeps the data in human-readable
    form. Unfortunately, it has two disadvantages: saving takes longer than alternative
    serialization methods, and it uses more storage space.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we use `msgpack` to persist our data. This is faster than Python’s
    `json` library, and the saved file uses less space. Of course, the disadvantage
    is that the resulting JSON is binary rather than text-based.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I use both methods for persisting JSON data in my work. When I am working with
    small amounts of data, and that data is relatively static, I prefer human-readable
    JSON. A great use case for this is the recipes in the previous chapter where we
    needed to create value labels.
  prefs: []
  type: TYPE_NORMAL
- en: I use `msgpack` when I am working with large amounts of data, where that data
    changes regularly. `msgpack` files are also great when you want to take regular
    snapshots of key tables in enterprise databases.
  prefs: []
  type: TYPE_NORMAL
- en: The Cleveland Museum of Art’s collections data is similar in at least one important
    way to the data we work with every day. The unit of analysis frequently changes.
    Here, we are looking at collections, citations, and creators. In our work, we
    might have to simultaneously look at students and courses, or households and deposits.
    An enterprise database system for the museum data would likely have separate collections,
    citations, and creators tables that we would eventually need to merge. The resulting
    merged file would have data redundancy issues that we would need to account for
    whenever we changed the unit of analysis.
  prefs: []
  type: TYPE_NORMAL
- en: When we alter our data cleaning process to work directly from JSON or parts
    of it, we end up eliminating a major source of errors. We do more data cleaning
    with JSON in the *Classes that handle non-tabular data structures* recipe in *Chapter
    12*, *Automate Data Cleaning with User-Defined Functions, Classes and Pipelines*.
  prefs: []
  type: TYPE_NORMAL
- en: Versioning data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There may be times when we want to persist data without overwriting a prior
    version of the data file. This can be accomplished by appending a time stamp to
    a filename or a unique identifier. However, there are more elegant solutions available.
    One such solution is the Delta Lake library, which we will explore in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We will work with the land temperature data again in this recipe. We will load
    the data, save it to a data lake, and then save an altered version to the same
    data lake.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the Delta Lake library in this recipe, which can be installed
    with `pip install deltalake`. We will also need the `os` library so that we can
    make a directory for the data lake.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can get started with the data and version it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the Delta Lake library. We also create a folder called
    `temps_lake` for our data versions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s load the land temperature data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We save the landtemps DataFrame to the data lake:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now retrieve the data we just saved. We specify that we want the first
    version, though that is not necessary as the most recent version will be retrieved
    when the version is not indicated. This returns a `DeltaTable` type, which we
    can convert into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s persist only the first 1,000 rows of the land temperature data to the
    data lake, without replacing the existing data. We pass a value of `overwrite`
    to the `mode` parameter. This saves a new dataset to the data lake. It does not
    replace the previous one. The `overwrite` parameter value is a little confusing
    here. This will be clearer when we use the `append` parameter value later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now retrieve this latest version of our data. Notice that this only has
    1,000 rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we specify `append` instead, we add the rows of the DataFrame in the second
    argument of `write_deltalake` to the rows of the previous version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s confirm that our first version of the dataset in the data lake is still
    accessible and has the number of rows we expect:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The nomenclature regarding overwrite and append is a little confusing, but it
    might make more sense if you think of overwrite as a logical deletion of the previous
    dataset, not a physical deletion. The most recent version has all new data, but
    the previous versions are still stored.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recipes in this chapter examined importing and data preparation of non-tabular
    data in a variety of forms, including JSON and HTML. We introduced Spark for working
    with big data and discussed how to persist tabular and non-tabular data. We also
    examined how to create a data lake for versioning. We will learn how to take the
    measure of our data in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://discord.gg/p8uSgEAETX](https://discord.gg/p8uSgEAETX )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code10336218961138498953.png)'
  prefs: []
  type: TYPE_IMG
