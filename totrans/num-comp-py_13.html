<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Grouping for Aggregation, Filtration, and Transformation</h1>
                </header>
            
            <article>
                
<p>One of the most fundamental tasks during a data analysis involves splitting data into independent groups before performing a calculation on each group. This methodology has been around for quite some time but has more recently been referred to as <strong>split-apply-combine</strong>. This chapter covers the powerful <kbd>groupby</kbd> method, which allows you to group your data in any way imaginable and apply any type of function independently to each group before returning a single dataset.</p>
<div class="packt_infobox">Hadley Wickham coined the term <strong>split-apply-combine</strong> to describe the common data analysis pattern of breaking up data into independent manageable chunks, independently applying functions to these chunks, and then combining the results back together. More details can be found in his paper (<a href="http://www.stat.wvu.edu/~jharner/courses/stat623/docs/plyrJSS.pdf" target="_blank">http://bit.ly/2isFuL9</a>).</div>
<p>Before we get started with the recipes, we will need to know just a little terminology. All basic groupby operations have <strong>grouping columns</strong>, and each unique combination of values in these columns represents an independent grouping of the data. The syntax looks as follows:</p>
<pre>&gt;&gt;&gt; df.groupby(['list', 'of', 'grouping', 'columns'])<br/>&gt;&gt;&gt; df.groupby('single_column')  # when grouping by a single column</pre>
<p>The result of this operation returns a groupby object. It is this groupby object that will be the engine that drives all the calculations for this entire chapter. Pandas <span>actually</span> does <span>very little when creating this</span> groupby <span>object, merely validating that grouping is possible.</span> You will have to chain methods on this groupby object in order to unleash its powers.</p>
<p class="mce-root"/>
<div class="packt_infobox">Technically, the result of the operation will either be a <kbd>DataFrameGroupBy</kbd> or <kbd>SeriesGroupBy</kbd> but for simplicity, it will be referred to as the groupby object for the entire chapter.</div>
<div>
<p class="mce-root">In this chapter, we will cover the following topics:</p>
<ul>
<li>Defining an aggregation</li>
<li>Grouping and aggregating with multiple columns and functions</li>
<li>Removing the MultiIndex after grouping</li>
<li>Customizing an aggregation function</li>
<li>Customizing aggregating functions with<span> </span><kbd>*args</kbd><span> </span>and<span> </span><kbd>**kwargs</kbd></li>
<li>Examining the<span> </span><kbd>groupby</kbd><span> </span>object</li>
<li>Filtering for states with a minority majority</li>
<li>Transforming through a weight loss bet</li>
<li>Calculating weighted mean SAT scores per state with apply</li>
<li>Grouping by continuous variables</li>
<li>Counting the total number of flights between cities</li>
<li>Finding the longest streak of on-time flights</li>
</ul>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining an aggregation</h1>
                </header>
            
            <article>
                
<p>The most common use of the <kbd>groupby</kbd> method is to perform an aggregation. What actually is an aggregation? In our data analysis world, an aggregation takes place when a sequence of many inputs get summarized or combined into a single value output. For example, summing up all the values of a column or finding its maximum are common aggregations applied on a single sequence of data. An aggregation simply takes many values and converts them down to a single value.</p>
<p>In addition to the grouping columns defined during the introduction, most aggregations have two other components, the <strong>aggregating columns</strong> and <strong>aggregating functions</strong>. The aggregating columns are those whose values will be aggregated. The aggregating functions define how the aggregation takes place. Major aggregation functions include <kbd>sum</kbd>, <kbd>min</kbd>, <kbd>max</kbd>, <kbd>mean</kbd>, <kbd>count</kbd>, <kbd>variance</kbd>, <kbd>std</kbd>, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we examine the flights dataset and perform the simplest possible aggregation involving only a single grouping column, a single aggregating column, and a single aggregating function. We will find the average arrival delay for each airline. Pandas has quite a few different syntaxes to produce an aggregation and this recipe covers them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the flights dataset, and define the grouping columns (<kbd>AIRLINE</kbd>), aggregating columns (<kbd>ARR_DELAY</kbd>), and aggregating functions (<kbd>mean</kbd>):</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights = pd.read_csv('data/flights.csv')<br/>&gt;&gt;&gt; flights.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/50f4d53d-fcb0-4e12-9a69-0d39d5ea04d4.png" style="width:67.75em;height:12.25em;"/></div>
<ol start="2">
<li>Place the grouping column in the <kbd>groupby</kbd> method and then call the <kbd>agg</kbd> method with a dictionary pairing the aggregating column with its aggregating function:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights.groupby('AIRLINE').agg({'ARR_DELAY':'mean'}).head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fec1f726-3a08-44eb-97aa-a134f638a217.png" style="width:9.58em;height:12.17em;"/></div>
<ol start="3">
<li>Alternatively, you may place the aggregating column in the indexing operator and then pass the aggregating function as a string to <kbd>agg</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights.groupby('AIRLINE')['ARR_DELAY'].agg('mean').head()<br/>AIRLINE
AA     5.542661
AS    -0.833333
B6     8.692593
DL     0.339691
EV     7.034580
Name: ARR_DELAY, dtype: float64</pre>
<ol start="4">
<li>The string names used in the previous step are a convenience pandas offers you to refer to a particular aggregation function. You can pass any aggregating function directly to the <kbd>agg</kbd> method such as the NumPy <kbd>mean</kbd> function. The output is the same as the previous step:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights.groupby('AIRLINE')['ARR_DELAY'].agg(np.mean).head()</pre>
<ol start="5">
<li>It's possible to skip the <kbd>agg</kbd> method altogether in this case and use the <kbd>mean</kbd> method directly. This output is also the same as step 3:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights.groupby('AIRLINE')['ARR_DELAY'].mean().head()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The syntax for the <kbd>groupby</kbd> method is not as straightforward as other methods. Let's intercept the chain of methods in step 2 by storing the result of the <kbd>groupby</kbd> method as its own variable</p>
<pre style="padding-left: 30px">&gt;&gt;&gt; grouped = flights.groupby('AIRLINE')<br/>&gt;&gt;&gt; type(grouped)<br/>pandas.core.groupby.DataFrameGroupBy</pre>
<p>A completely new intermediate object is first produced with its own distinct attributes and methods. No calculations take place at this stage. Pandas merely validates the grouping columns. This groupby object has an <kbd>agg</kbd> method to perform aggregations. One of the ways to use this method is to pass it a dictionary mapping the aggregating column to the aggregating function, as done in step 2.</p>
<p>There are several different flavors of syntax that produce a similar result, with step 3 showing an alternative. Instead of identifying the aggregating column in the dictionary, place it inside the indexing operator just as if you were selecting it as a column from a DataFrame. The function string name is then passed as a scalar to the <kbd>agg</kbd> method.</p>
<p>You may pass any aggregating function to the <kbd>agg</kbd> method. Pandas allows you to use the string names for simplicity but you may also explicitly call an aggregating function as done in step 4. NumPy provides many functions that aggregate values.</p>
<p>Step 5 shows one last syntax flavor. When you are only applying a single aggregating function as in this example, you can often call it directly as a method on the groupby object itself without <kbd>agg</kbd>. Not all aggregation functions have a method equivalent but many basic ones do. The following is a list of several aggregating functions that may be passed as a string to <kbd>agg</kbd> or chained directly as a method to the groupby object:</p>
<pre><span class="go">min     max    mean    median    sum    count    std    </span><span class="go">var   <br/></span><span class="go">size    describe    nunique     idxmin     idxmax</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>If you do not use an aggregating function with <kbd>agg</kbd>, pandas raises an exception. For instance, let's see what happens when we apply the square root function to each group:</p>
<pre>&gt;&gt;&gt; flights.groupby('AIRLINE')['ARR_DELAY'].agg(np.sqrt)<br/><span class="ansi-red-fg">ValueError</span>: function does not reduce</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Pandas official documentation on <em>Aggregation</em> (<a href="http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation" target="_blank">http://bit.ly/2iuf1Nc</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grouping and aggregating with multiple columns and functions</h1>
                </header>
            
            <article>
                
<p>It is possible to do grouping and aggregating with multiple columns. The syntax is only slightly different than it is for grouping and aggregating with a single column. As usual with any kind of grouping operation, it helps to identify the three components: the grouping columns, aggregating columns, and aggregating functions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we showcase the flexibility of the <kbd>groupby</kbd> DataFrame method by answering the following queries:</p>
<ul>
<li>Finding the number of cancelled flights for every airline per weekday</li>
<li>Finding the number and percentage of cancelled and diverted flights for every airline per weekday</li>
<li>For each origin and destination, finding the total number of flights, the number and percentage of cancelled flights, and the average and variance of the airtime</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the flights dataset, and answer the first query by defining the grouping columns (<kbd>AIRLINE, WEEKDAY</kbd>), the aggregating column (<kbd>CANCELLED</kbd>), and the aggregating function (<kbd>sum</kbd>):</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights.groupby(['AIRLINE', 'WEEKDAY'])['CANCELLED'] \<br/>           .agg('sum').head(7)<br/>AIRLINE  WEEKDAY
AA       1          41
         2           9
         3          16
         4          20
         5          18
         6          21
         7          29
Name: CANCELLED, dtype: int64</pre>
<ol start="2">
<li>Answer the second query by using a list for each pair of grouping and aggregating columns. Also, use a list for the aggregating functions:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights.groupby(['AIRLINE', 'WEEKDAY']) \<br/>            ['CANCELLED', 'DIVERTED'].agg(['sum', 'mean']).head(7)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5f4fb3bf-8d5c-43ec-adfa-712255f9496e.png" style="width:18.92em;height:15.42em;"/></div>
<ol start="3">
<li>Answer the third query using a dictionary in the <kbd>agg</kbd> method to map specific aggregating columns to specific aggregating functions:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; group_cols = ['ORG_AIR', 'DEST_AIR']<br/>&gt;&gt;&gt; agg_dict = {'CANCELLED':['sum', 'mean', 'size'], <br/>                'AIR_TIME':['mean', 'var']}<br/>&gt;&gt;&gt; flights.groupby(group_cols).agg(agg_dict).head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/14de92a8-0578-44fd-93d4-149460e03eef.png" style="width:22.00em;height:11.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>To group by multiple columns as in step 1, we pass a list of the string names to the <kbd>groupby</kbd> method. Each unique combination of <kbd>AIRLINE</kbd> and <kbd>WEEKDAY</kbd> forms an independent group. Within each of these groups, the sum of the cancelled flights is found and then returned as a Series.</p>
<p>Step 2, again groups by both <kbd>AIRLINE</kbd> and <kbd>WEEKDAY</kbd>, but this time aggregates two columns. It applies each of the two aggregation functions, <kbd>sum</kbd> and <kbd>mean</kbd>, to each column resulting in four returned columns per group.</p>
<p>Step 3 goes even further, and uses a dictionary to map specific aggregating columns to different aggregating functions. Notice that the <kbd>size</kbd> aggregating function returns the total number of rows per group. This is different than the <kbd>count</kbd> aggregating function, which returns the number of non-missing values per group.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>There are a few main flavors of syntax that you will encounter when performing an aggregation. The following four blocks of pseudocode summarize the main ways you can perform an aggregation with the <kbd>groupby</kbd> method:</p>
<ol>
<li>Using <kbd>agg</kbd> with a dictionary is the most flexible and allows you to specify the aggregating function for each column:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; df.groupby(['grouping', 'columns']) \<br/>      .agg({'agg_cols1':['list', 'of', 'functions'], <br/>            'agg_cols2':['other', 'functions']})</pre>
<ol start="2">
<li>Using <kbd>agg</kbd> with a list of aggregating functions applies each of the functions to each of the aggregating columns:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; df.groupby(['grouping', 'columns'])['aggregating', 'columns'] \<br/>      .agg([aggregating, functions])</pre>
<ol start="3">
<li>Directly using a method following the aggregating columns instead of <kbd>agg</kbd>, applies just that method to each aggregating column. This way does not allow for multiple aggregating functions:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; df.groupby(['grouping', 'columns'])['aggregating', 'columns'] \<br/>      .aggregating_method()</pre>
<ol start="4">
<li>If you do not specify the aggregating columns, then the aggregating method will be applied to all the non-grouping columns:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; df.groupby(['grouping', 'columns']).aggregating_method()</pre>
<p>In the preceding four code blocks it is possible to substitute a string for any of the lists when grouping or aggregating by a single column.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing the MultiIndex after grouping</h1>
                </header>
            
            <article>
                
<p>Inevitably, when using <kbd>groupby</kbd>, you will likely create a MultiIndex in the columns or rows or both. DataFrames with MultiIndexes are more difficult to navigate and occasionally have confusing column names as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we perform an aggregation with the <kbd>groupby</kbd> method to create a DataFrame with a MultiIndex for the rows and columns and then manipulate it so that the index is a single level and the column names are descriptive.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the flights dataset; write a statement to find the total and average miles flown; and the maximum and minimum arrival delay for each airline for each weekday:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights = pd.read_csv('data/flights.csv')<br/>&gt;&gt;&gt; airline_info = flights.groupby(['AIRLINE', 'WEEKDAY'])\<br/>                          .agg({'DIST':['sum', 'mean'], <br/>                                'ARR_DELAY':['min', 'max']}) \<br/>                          .astype(int)<br/>&gt;&gt;&gt; airline_info.head(7)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/000fc6a3-3bb0-4fd1-8c48-339fdbbc16ce.png" style="width:22.67em;height:19.17em;"/></div>
<ol start="2">
<li>Both the rows and columns are labeled by a MultiIndex with two levels. Let's squash it down to just a single level. To address the columns, we use the MultiIndex method, <kbd>get_level_values</kbd>. Let's display the output of each level and then concatenate both levels before setting it as the new column values:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; level0 = airline_info.columns.get_level_values(0)<br/>Index(['DIST', 'DIST', 'ARR_DELAY', 'ARR_DELAY'], dtype='object')<br/><br/>&gt;&gt;&gt; level1 = airline_info.columns.get_level_values(1)<br/>Index(['sum', 'mean', 'min', 'max'], dtype='object')<br/><br/>&gt;&gt;&gt; airline_info.columns = level0 + '_' + level1<br/>&gt;&gt;&gt; airline_info.head(7)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/776af29d-5e5c-4c22-8d65-e75693161af6.png" style="width:36.17em;height:17.75em;"/></div>
<ol start="3">
<li>Return the row labels to a single level with <kbd>reset_index</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; airline_info.reset_index().head(7)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d2a0fb0c-945c-478e-a7a7-8835ff925b68.png" style="width:37.75em;height:15.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>When using the <kbd>agg</kbd> method to perform an aggregation on multiple columns, pandas creates an index object with two levels. The aggregating columns become the top level and the aggregating functions become the bottom level. Pandas displays MultiIndex levels differently than single-level columns. Except for the <strong>innermost</strong> levels, repeated index values do not get displayed on the screen. You can inspect the DataFrame from step 1 to verify this. For instance, the <kbd>DIST</kbd> column shows up only once but it refers to both of the first two columns.</p>
<div class="packt_infobox">The innermost MultiIndex level is the one closest to the data. This would be the bottom-most column level and the right-most index level.</div>
<p>Step 2 defines new columns by first retrieving the underlying values of each of the levels with the MultiIndex method <kbd>get_level_values.</kbd> This method accepts an integer identifying the index level. They are numbered beginning with zero from the top/left. Indexes support vectorized operations, so we concatenate both levels together with a separating underscore. We assign these new values to the <kbd>columns</kbd> attribute.</p>
<p>In step 3, we make both index levels as columns with <kbd>reset_index</kbd>. We could have concatenated the levels together like we did in step 2, but it makes more sense to keep them as separate columns.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>By default, at the end of a groupby operation, pandas puts all of the grouping columns in the index. The <kbd>as_index</kbd> <span>parameter</span> in the <kbd>groupby</kbd> method can be set to <kbd>False</kbd> to avoid this behavior. You can chain the <kbd>reset_index</kbd> method after grouping to get the same effect as done in step 3. Let's see an example of this by finding the average distance traveled per flight from each airline:</p>
<pre>&gt;&gt;&gt; flights.groupby(['AIRLINE'], as_index=False)['DIST'].agg('mean') \<br/>                                                        .round(0)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a196e45e-4c13-4ed3-9efe-5d64e7dbc121.png" style="width:9.33em;height:27.00em;"/></div>
<p>Take a look at the order of the airlines in the previous result. By default, pandas sorts the grouping columns. The <kbd>sort</kbd> parameter exists within the <kbd>groupby</kbd> method and is defaulted to <kbd>True</kbd>. You may set it to <kbd>False</kbd> to keep the order of the grouping columns the same as how they are encountered in the dataset. You also get a small performance improvement by not sorting your data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Customizing an aggregation function</h1>
                </header>
            
            <article>
                
<p>Pandas provides a number of the most common aggregation functions for you to use with the groupby object. At some point, you will need to write your own customized user-defined functions that don't exist in pandas or NumPy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we use the college dataset to calculate the mean and standard deviation of the undergraduate student population per state. We then use this information to find the maximum number of standard deviations from the mean that any single population value is per state.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the college dataset, and find the mean and standard deviation of the undergraduate population by state:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college = pd.read_csv('data/college.csv')<br/>&gt;&gt;&gt; college.groupby('STABBR')['UGDS'].agg(['mean', 'std']) \<br/>                                     .round(0).head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/da651e10-a471-434f-a5e1-e5e0f7ae4307.png" style="width:11.25em;height:12.17em;"/></div>
<ol start="2">
<li>This output isn't quite what we desire. We are not looking for the mean and standard deviations of the entire group but the maximum number of standard deviations away from the mean for any one institution. In order to calculate this, we need to subtract the mean undergraduate population by state from each institution's undergraduate population and then divide by the standard deviation. This standardizes the undergraduate population for each group. We can then take the maximum of the absolute value of these scores to find the one that is farthest away from the mean. Pandas does not provide a function capable of doing this. Instead, we will need to create a custom function:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; def max_deviation(s):<br/>        std_score = (s - s.mean()) / s.std()<br/>        return std_score.abs().max()</pre>
<ol start="3">
<li>After defining the function, pass it directly to the <kbd>agg</kbd> method to complete the aggregation:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college.groupby('STABBR')['UGDS'].agg(max_deviation) \<br/>                                     .round(1).head()<br/>STABBR
AK    2.6
AL    5.8
AR    6.3
AS    NaN
AZ    9.9
Name: UGDS, dtype: float64</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>There does not exist a predefined pandas function to calculate the maximum number of standard deviations away from the mean. We were forced to construct a customized function in step 2. Notice that this custom function <kbd>max_deviation</kbd> accepts a single parameter, <kbd>s</kbd>. Looking ahead at step 3, you will notice that the function name is placed inside the <kbd>agg</kbd> method without <span>directly being called. Nowhere is the parameter</span> <kbd>s</kbd> <span>explicitly passed to</span> <kbd>max_deviation</kbd><span>. Instead, pandas implicitly</span> passes <span>the</span> <kbd>UGDS</kbd> <span>column</span> <span>as a Series to</span> <kbd>max_deviation</kbd><span>.</span></p>
<p>The <kbd>max_deviation</kbd> function is called once for each group. <span>As <kbd>s</kbd> is a Series, all normal Series methods are available.</span> It subtracts the mean of that particular grouping from each of the values in the group before dividing by the standard deviation in a process called <strong>standardization</strong>.</p>
<div class="packt_infobox">Standardization is a common s<span>tatistical procedure to understand how greatly individual values vary from the mean. For a normal distribution, 99.7% of the data lies within three standard deviations of the mean.</span></div>
<p>As we are interested in absolute deviation from the mean, we take the absolute value from all the standardized scores and return the maximum. The <kbd>agg</kbd> method necessitates that a single scalar value must be returned from our custom function, or else an exception will be raised. Pandas defaults to using the sample standard deviation which is undefined for any groups with just a single value. For instance, the state abbreviation <em>AS</em> (American Samoa) has a missing value returned as it has only a single institution in the dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>It is possible to apply our customized function to multiple aggregating columns. We simply add more column names to the indexing operator. The <kbd>max_deviation</kbd> function only works with numeric columns:</p>
<pre>&gt;&gt;&gt; college.groupby('STABBR')['UGDS', 'SATVRMID', 'SATMTMID'] \<br/>           .agg(max_deviation).round(1).head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/aba24502-a192-411c-bc6b-61e69a37abab.png" style="width:19.25em;height:13.67em;"/></div>
<p>You can also use your customized aggregation function along with the prebuilt functions. The following does this and groups by state and religious affiliation:</p>
<pre>&gt;&gt;&gt; college.groupby(['STABBR', 'RELAFFIL']) \<br/>            ['UGDS', 'SATVRMID', 'SATMTMID'] \<br/>           .agg([max_deviation, 'mean', 'std']).round(1).head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e89efc68-2bf0-4338-9180-1c2b0efbbc56.png" style="width:51.25em;height:16.67em;"/></div>
<p class="mce-root">Notice that pandas uses the name of the function as the name for the returned column. You can change the column name directly with the rename method or you can modify the special function attribute <kbd>__name__</kbd>:</p>
<pre>&gt;&gt;&gt; max_deviation.__name__<br/>'max_deviation'<br/><br/>&gt;&gt;&gt; max_deviation.__name__ = 'Max Deviation'<br/>&gt;&gt;&gt; college.groupby(['STABBR', 'RELAFFIL']) \<br/>            ['UGDS', 'SATVRMID', 'SATMTMID'] \<br/>           .agg([max_deviation, 'mean', 'std']).round(1).head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/25d346b9-cf76-4eb2-a6db-d0fa8725a4d2.png" style="width:51.92em;height:16.67em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Customizing aggregating functions with *args and **kwargs</h1>
                </header>
            
            <article>
                
<p>When writing your own user-defined customized aggregation function, pandas implicitly passes it each of the aggregating columns one at a time as a Series. Occasionally, you will need to pass more arguments to your function than just the Series itself. To do so, you need to be aware of Python's ability to pass an arbitrary number of arguments to functions. Let's take a look at the signature of the groupby object's <kbd>agg</kbd> method with help from the <kbd>inspect</kbd> module:</p>
<pre>&gt;&gt;&gt; college = pd.read_csv('data/college.csv')<br/>&gt;&gt;&gt; grouped = college.groupby(['STABBR', 'RELAFFIL'])<br/><br/>&gt;&gt;&gt; import inspect<br/>&gt;&gt;&gt; inspect.signature(grouped.agg)<br/>&lt;Signature (arg, *args, **kwargs)&gt;</pre>
<p>The argument <kbd>*args</kbd> allow you to pass an arbitrary number of non-keyword arguments to your customized aggregation function. Similarly, <kbd>**kwargs</kbd> allows you to pass an arbitrary number of keyword arguments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we build a customized function for the college dataset that finds the percentage of schools by state and religious affiliation that have an undergraduate population between two values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Define a function that returns the percentage of schools with an undergraduate population between 1,000 and 3,000:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; def pct_between_1_3k(s):<br/>        return s.between(1000, 3000).mean()</pre>
<ol start="2">
<li>Calculate this percentage grouping by state and religious affiliation:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college.groupby(['STABBR', 'RELAFFIL'])['UGDS'] \<br/>           .agg(pct_between_1_3k).head(9)<br/>STABBR  RELAFFIL
AK      0           0.142857
        1           0.000000
AL      0           0.236111
        1           0.333333
AR      0           0.279412
        1           0.111111
AS      0           1.000000
AZ      0           0.096774
        1           0.000000
Name: UGDS, dtype: float64</pre>
<ol start="3">
<li>This function works fine but it doesn't give the user any flexibility to choose the lower and upper bound. Let's create a new function that allows the user to define these bounds:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; def pct_between(s, low, high):<br/>        return s.between(low, high).mean()</pre>
<ol start="4">
<li>Pass this new function to the <kbd>agg</kbd> method along with lower and upper bounds:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college.groupby(['STABBR', 'RELAFFIL'])['UGDS'] \<br/>           .agg(pct_between, 1000, 10000).head(9)<br/>STABBR  RELAFFIL
AK      0           0.428571
        1           0.000000
AL      0           0.458333
        1           0.375000
AR      0           0.397059
        1           0.166667
AS      0           1.000000
AZ      0           0.233871
        1           0.111111
Name: UGDS, dtype: float64</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Step 1 creates a function that doesn't accept any extra arguments. The upper and lower bounds must be hardcoded into the function itself, which isn't very flexible. Step 2 shows the results of this aggregation.</p>
<p>We create a more flexible function in step 3 that allows users to define both the lower and upper bounds dynamically. Step 4 is where the magic of <kbd>*args</kbd> and <kbd>**kwargs</kbd> come into play. In this particular example, we pass two non-keyword arguments, 1,000 and 10,000, to the <kbd>agg</kbd> method. Pandas passes these two arguments respectively to the <kbd>low</kbd> and <kbd>high</kbd> parameters of <kbd>pct_between</kbd>.</p>
<p>There are a few ways we could achieve the same result in step 4. We could have explicitly used the parameter names with the following command to produce the same result:</p>
<pre>&gt;&gt;&gt; college.groupby(['STABBR', 'RELAFFIL'])['UGDS'] \<br/>           .agg(pct_between, high=10000, low=1000).head(9)</pre>
<p>The order of the keyword arguments doesn't matter as long as they come after the function name. Further still, we can mix non-keyword and keyword arguments as long as the keyword arguments come last:</p>
<pre>&gt;&gt;&gt; college.groupby(['STABBR', 'RELAFFIL'])['UGDS'] \<br/>           .agg(pct_between, 1000, high=10000).head(9)</pre>
<p>For ease of understanding, it's probably best to include all the parameter names in the order that they are defined in the function signature.</p>
<div class="packt_infobox">Technically, when <kbd>agg</kbd> is called, all the non-keyword arguments get collected into a tuple named <kbd>args</kbd> and all the keyword arguments get collected into a dictionary named <kbd>kwargs</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span>Unfortunately, pandas does not have a direct way to use these additional arguments when using multiple aggregation functions together. For example, if you wish to aggregate using the <kbd>pct_between</kbd> and <kbd>mean</kbd> functions, you will get the following exception:</span></p>
<pre>&gt;&gt;&gt; college.groupby(['STABBR', 'RELAFFIL'])['UGDS'] \<br/>           .agg(['mean', pct_between], low=100, high=1000) <br/><span class="ansi-red-fg">TypeError</span>: pct_between() missing 2 required positional arguments: 'low' and 'high'</pre>
<p>Pandas is incapable of understanding that the extra arguments need to be passed to <kbd>pct_between</kbd>. In order to use our custom function with other built-in functions and even other custom functions, we can define a special type of nested function called a <strong>closure</strong>. We can use a generic closure to build all of our customized functions:</p>
<pre>&gt;&gt;&gt; def make_agg_func(func, name, *args, **kwargs):<br/>        def wrapper(x):<br/>            return func(x, *args, **kwargs)<br/>        wrapper.__name__ = name<br/>        return wrapper<br/><br/>&gt;&gt;&gt; my_agg1 = make_agg_func(pct_between, 'pct_1_3k', low=1000, high=3000)<br/>&gt;&gt;&gt; my_agg2 = make_agg_func(pct_between, 'pct_10_30k', 10000, 30000)<br/><br/>&gt;&gt;&gt; college.groupby(['STABBR', 'RELAFFIL'])['UGDS'] \<br/>           .agg(['mean', my_agg1, my_agg2]).head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0185d893-b8f3-47d9-847b-ea7838685247.png" style="width:24.42em;height:13.08em;"/></div>
<p>The <kbd>make_agg_func</kbd> function acts as a factory to create customized aggregation functions. It accepts the customized aggregation function that you already built (<kbd>pct_between</kbd> in this case), a <kbd>name</kbd> argument, and an arbitrary number of extra arguments. It returns a function with the extra arguments already set. For instance, <kbd>my_agg1</kbd> is a specific customized aggregating function that finds the percentage of schools with an undergraduate population between one and three thousand. <span>The extra arguments (</span><kbd>*args</kbd> <span>and</span> <kbd>**kwargs</kbd><span>) specify an exact set of parameters for your customized function (</span><kbd>pct_between</kbd> <span>in this case).</span> <span>The <kbd>name</kbd> parameter is very important and must be unique each time</span> <kbd>make_agg_func</kbd> <span>is called. It will eventually be used to rename the aggregated column.</span></p>
<div class="packt_infobox">A closure is a function that contains a function inside of it (a nested function) and returns this nested function. This nested function must refer to variables in the scope of the outer function in order to be a closure. In this example, <kbd>make_agg_func</kbd> is the outer function and returns the nested function <kbd>wrapper</kbd>, which accesses the variables <kbd>func</kbd>, <kbd>args</kbd>, and <kbd>kwargs</kbd> from the outer function.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Arbitrary Argument Lists</em> from the official Python documentation (<a href="https://docs.python.org/3/tutorial/controlflow.html#arbitrary-argument-lists" target="_blank">http://bit.ly/2vumbTE</a>)</li>
<li>A tutorial on <em>Python Closures</em> (<a href="http://www.geeksforgeeks.org/python-closures/" target="_blank">http://bit.ly/2xFdYga</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Examining the groupby object</h1>
                </header>
            
            <article>
                
<p>The immediate result from using the <kbd>groupby</kbd> method on a DataFrame will be a groupby object. Usually, we continue operating on this object to do aggregations or transformations without ever saving it to a variable. On<span>e of the primary purposes of examining this groupby object is to inspect individual groups.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we examine the groupby object itself by directly calling methods on it as well as iterating through each of its groups.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Let's get started by grouping the state and religious affiliation columns from the college dataset, saving the result to a variable and confirming its type:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college = pd.read_csv('data/college.csv')<br/>&gt;&gt;&gt; grouped = college.groupby(['STABBR', 'RELAFFIL'])<br/>&gt;&gt;&gt; type(grouped)<br/>pandas.core.groupby.DataFrameGroupBy</pre>
<ol start="2">
<li>Use the <kbd>dir</kbd> function to discover all its available functionality:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; print([attr for attr in dir(grouped) if not attr.startswith('_')])<br/>['CITY', 'CURROPER', 'DISTANCEONLY', 'GRAD_DEBT_MDN_SUPP', 'HBCU', 'INSTNM', 'MD_EARN_WNE_P10', 'MENONLY', 'PCTFLOAN', 'PCTPELL', 'PPTUG_EF', 'RELAFFIL', 'SATMTMID', 'SATVRMID', 'STABBR', 'UG25ABV', 'UGDS', 'UGDS_2MOR', 'UGDS_AIAN', 'UGDS_ASIAN', 'UGDS_BLACK', 'UGDS_HISP', 'UGDS_NHPI', 'UGDS_NRA', 'UGDS_UNKN', 'UGDS_WHITE', 'WOMENONLY', 'agg', 'aggregate', 'all', 'any', 'apply', 'backfill', 'bfill', 'boxplot', 'corr', 'corrwith', 'count', 'cov', 'cumcount', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'dtypes', 'expanding', 'ffill', 'fillna', 'filter', 'first', 'get_group', 'groups', 'head', 'hist', 'idxmax', 'idxmin', 'indices', 'last', 'mad', 'max', 'mean', 'median', 'min', 'ndim', 'ngroup', 'ngroups', 'nth', 'nunique', 'ohlc', 'pad', 'pct_change', 'plot', 'prod', 'quantile', 'rank', 'resample', 'rolling', 'sem', 'shift', 'size', 'skew', 'std', 'sum', 'tail', 'take', 'transform', 'tshift', 'var']</pre>
<ol start="3">
<li>Find the number of groups with the <kbd>ngroups</kbd> attribute:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; grouped.ngroups<br/>112</pre>
<ol start="4">
<li>To find the uniquely identifying labels for each group, look in the <kbd>groups</kbd> attribute, which contains a dictionary of each unique group mapped to all the corresponding index labels of that group:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; groups = list(grouped.groups.keys())<br/>&gt;&gt;&gt; groups[:6]<br/>[('AK', 0), ('AK', 1), ('AL', 0), ('AL', 1), ('AR', 0), ('AR', 1)]</pre>
<ol start="5">
<li>Retrieve a single group with the <kbd>get_group</kbd> method by passing it a tuple of an exact group label. For example, to get all the religiously affiliated schools in the state of Florida, do the following:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; grouped.get_group(('FL', 1)).head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/eacc01ed-8742-4326-b264-9d7ef642eb55.png" style="width:57.75em;height:19.83em;"/></div>
<ol start="6">
<li>You may want to take a peek at each individual group. This is possible because groupby objects are iterable:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; from IPython.display import display<br/>&gt;&gt;&gt; for name, group in grouped:<br/>        print(name)<br/>        display(group.head(3))</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/abafebc3-f01c-4644-8ff4-dd569a6c390b.png" style="width:52.08em;height:45.08em;"/></div>
<ol start="7">
<li>You can also call the head method on your groupby object to get the first rows of each group together in a single DataFrame.</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; grouped.head(2).head(6)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/72cd2228-11a4-4845-90d3-401a079270ee.png" style="width:52.17em;height:19.17em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Step 1 formally creates our groupby object. It is useful to display all the public attributes and methods to reveal all the possible functionality as was done in step 2. Each group is uniquely identified by a tuple containing a unique combination of the values in the grouping columns. Pandas allows you to select a specific group as a DataFrame with the <kbd>get_group</kbd> method shown in step 5.</p>
<p>It is rare that you will need to iterate through your groups and in general, you should avoid doing so if necessary, as it can be quite slow. Occasionally, you will have no other choice. <span>When iterating through a </span>groupby<span> object, you are given a tuple containing the group name and the DataFrame without the grouping columns. This tuple is unpacked into the variables <kbd>name</kbd> and <kbd>group</kbd> in the for-loop in step 6.</span></p>
<p>One interesting thing you can do while iterating through your groups is to display a few of the rows from each group directly in the notebook. To do this, you can either use the print function or the <kbd>display</kbd> function from the <kbd>IPython.display</kbd> module. Using the <kbd>print</kbd> function results in DataFrames that are in plain text without any nice HTML formatting. Using the <kbd>display</kbd> function will produce DataFrames in their normal easy-to-read format. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>There are several useful methods that were not explored from the list in step 2. Take for instance the <kbd>nth</kbd> method, which, when given a list of integers, selects those specific rows from each group. For example, the following operation selects the first and last rows from each group:</p>
<pre>&gt;&gt;&gt; grouped.nth([1, -1]).head(8)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/35964ec0-13c2-4fc8-a4e1-2c49eeeb7735.png" style="width:37.17em;height:20.00em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Official documentation of the <kbd>display</kbd> function from IPython (<a href="http://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display" target="_blank">http://bit.ly/2iAIogC</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Filtering for states with a minority majority</h1>
                </header>
            
            <article>
                
<p>In <a href="3b938362-1f65-406c-ba9d-3bf735543ca8.xhtml" target="_blank">Chapter 10</a>, <em>Selecting Subsets of Data</em>, we marked every row as <kbd>True</kbd> or <kbd>False</kbd> before filtering out the <kbd>False</kbd> rows. In a similar fashion, it is possible to mark entire groups of data as either <kbd>True</kbd> or <kbd>False</kbd> before filtering out the <kbd>False</kbd> groups. To do this, we first form groups with the <kbd>groupby</kbd> method and then apply the <kbd>filter</kbd> method. The <kbd>filter</kbd> method accepts a function that must return either <kbd>True</kbd> or <kbd>False</kbd> to indicate whether a group is kept or not.</p>
<div class="packt_infobox">This <kbd>filter</kbd> method applied after a call to the <kbd>groupby</kbd> method is completely different than the DataFrame <kbd>filter</kbd> method.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we use the college dataset to find all the states that have more non-white undergraduate students than white. As this is a dataset from the US, whites form the majority and therefore, we are looking for states with a minority majority.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the college dataset, group by state, and display the total number of groups. This should equal the number of unique states retrieved from the <kbd>nunique</kbd> Series method:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college = pd.read_csv('data/college.csv', index_col='INSTNM')<br/>&gt;&gt;&gt; grouped = college.groupby('STABBR')<br/>&gt;&gt;&gt; grouped.ngroups<br/>59<br/><br/>&gt;&gt;&gt; college['STABBR'].nunique() # verifying the same number<br/>59</pre>
<ol start="2">
<li>The <kbd>grouped</kbd> variable has a <kbd>filter</kbd> method, which accepts a custom function that determines whether a group is kept or not. The custom function gets implicitly passed a DataFrame of the current group and is required to return a boolean. Let's define a function that calculates the total percentage of minority students and returns <kbd>True</kbd> if this percentage is greater than a user-defined threshold:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; def check_minority(df, threshold):<br/>        minority_pct = 1 - df['UGDS_WHITE']<br/>        total_minority = (df['UGDS'] * minority_pct).sum()<br/>        total_ugds = df['UGDS'].sum()<br/>        total_minority_pct = total_minority / total_ugds<br/>        return total_minority_pct &gt; threshold</pre>
<ol start="3">
<li>Use the <kbd>filter</kbd> method passed with the <kbd>check_minority</kbd> function and a threshold of 50% to find all states that have a minority majority:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college_filtered = grouped.filter(check_minority, threshold=.5)<br/>&gt;&gt;&gt; college_filtered.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3f8be7b3-9a86-4710-b751-fbe63a2b8cd5.png" style="width:35.42em;height:17.50em;"/></div>
<ol start="4">
<li>Just looking at the output may not be indicative of what actually happened. The DataFrame starts with state Arizona (AZ) and not Alaska (AK) so we can visually confirm that something changed. Let's compare the <kbd>shape</kbd> of this filtered DataFrame with the original. Looking at the results, <span>about 60% of the rows have been filtered, and only 20 states remain that have a minority majority:</span></li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college.shape<br/>(7535, 26)<br/><br/>&gt;&gt;&gt; college_filtered.shape<br/>(3028, 26)<br/><br/>&gt;&gt;&gt; college_filtered['STABBR'].nunique()<br/>20</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This recipe takes a look at the total population of all the institutions on a state-by-state basis. The goal is to keep all the rows from the states, as a whole, that have a minority majority. This requires us to group our data by state, which is done in step 1. We find that there are 59 independent groups.</p>
<p>The <kbd>filter</kbd> groupby method either keeps all the rows in a group or filters them out. It does not change the number of columns. The <kbd>filter</kbd> groupby method performs this gatekeeping through a user-defined function, for example, <kbd>check_minority</kbd> in this recipe. A very important aspect to filter is that it passes the entire DataFrame for that particular group to the user-defined function and returns a single boolean for each group.</p>
<p>Inside of the <kbd>check_minority</kbd> function, the percentage and the total number of non-white students for each institution are first calculated and then the total number of all students is found. Finally, the percentage of non-white students for the entire state is checked against the given threshold, which produces a boolean.</p>
<p>The final result is a DataFrame with the same columns as the original but with the rows from the states that don't meet the threshold filtered out. As it is possible that the head of the filtered DataFrame is the same as the original, you need to do some inspection to ensure that the operation completed successfully. We verify this by checking the number of rows and number of unique states.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Our function, <kbd>check_minority</kbd>, is flexible and accepts a parameter to lower or raise the percentage of minority threshold. Let's check the shape and number of unique states for a couple of other thresholds:</p>
<pre>&gt;&gt;&gt; college_filtered_20 = grouped.filter(check_minority, threshold=.2)<br/>&gt;&gt;&gt; college_filtered_20.shape<br/>(7461, 26)<br/><br/>&gt;&gt;&gt; college_filtered_20['STABBR'].nunique()<br/>57<br/><br/>&gt;&gt;&gt; college_filtered_70 = grouped.filter(check_minority, threshold=.7)<br/>&gt;&gt;&gt; college_filtered_70.shape<br/>(957, 26)<br/><br/>&gt;&gt;&gt; college_filtered_70['STABBR'].nunique()<br/>10</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Pandas official documentation on <em>Filtration</em> (<a href="https://pandas.pydata.org/pandas-docs/stable/groupby.html#filtration" target="_blank">http://bit.ly/2xGUoA7</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming through a weight loss bet</h1>
                </header>
            
            <article>
                
<p>One method to increase motivation to lose weight is to make a bet with someone else. The scenario in this recipe will track weight loss from two individuals over the course of a four-month period and determine a winner.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we use simulated data from two individuals to track the percentage of weight loss over the course of four months. At the end of each month, a winner will be declared based on the individual who lost the highest percentage of body weight for that month. To track weight loss, we group our data by month and person, then call the <kbd>transform</kbd> method to find the percentage weight loss at each week from the start of the month.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the raw weight_loss dataset, and examine the first month of data from the two people, <kbd>Amy</kbd> and <kbd>Bob</kbd>. There are a total of four weigh-ins per month:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; weight_loss = pd.read_csv('data/weight_loss.csv')<br/>&gt;&gt;&gt; weight_loss.query('Month == "Jan"')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5efff218-dbf7-48a9-b831-f6052ddb4fef.png" style="width:16.42em;height:17.08em;"/></div>
<ol start="2">
<li>To determine the winner for each month, we only need to compare weight loss from the first week to the last week of each month. But, if we wanted to have weekly updates, we can also calculate weight loss from the current week to the first week of each month.  Let's create a function that is capable of providing weekly updates:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; def find_perc_loss(s):<br/>        return (s - s.iloc[0]) / s.iloc[0]</pre>
<ol start="3">
<li>Let's test out this function for Bob during the month of January.</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; bob_jan = weight_loss.query('Name=="Bob" and Month=="Jan"')<br/>&gt;&gt;&gt; find_perc_loss(bob_jan['Weight'])<br/>0    0.000000
2   -0.010309
4   -0.027491
6   -0.027491
Name: Weight, dtype: float64</pre>
<div class="packt_infobox">You should ignore the index values in the last output. 0, 2, 4 and 6 simply refer to the original row labels of the DataFrame and have no relation to the week.</div>
<ol start="4">
<li>After the first week, Bob lost 1% of his body weight. He continued losing weight during the second week but made no progress during the last week.  We can apply this function to every single combination of person and week to get the weight loss per week in relation to the first week of the month. To do this, we need to group our data by <kbd>Name</kbd> and <kbd>Month</kbd> , and then use the <kbd>transform</kbd> method to apply this custom function:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pcnt_loss = weight_loss.groupby(['Name', 'Month'])['Weight'] \<br/>                           .transform(find_perc_loss)<br/>&gt;&gt;&gt; pcnt_loss.head(8)<br/>0    0.000000
1    0.000000
2   -0.010309
3   -0.040609
4   -0.027491
5   -0.040609
6   -0.027491
7   -0.035533
Name: Weight, dtype: float64</pre>
<ol start="5">
<li>The <kbd>transform</kbd> method must return an object with the same number of rows as the calling DataFrame. Let's append this result to our original DataFrame as a new column. To help shorten the output, we will select Bob's first two months of data:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; weight_loss['Perc Weight Loss'] = pcnt_loss.round(3)<br/>&gt;&gt;&gt; weight_loss.query('Name=="Bob" and Month in ["Jan", "Feb"]')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/998f0234-1f22-4853-8f8c-3c1576a352ba.png" style="width:19.33em;height:14.25em;"/></div>
<ol start="6">
<li>Notice that the percentage weight loss resets after the new month. With this new column, we can manually determine a winner but let's see if we can find a way to do this automatically. As the only week that matters is the last week, let's select week 4:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; week4 = weight_loss.query('Week == "Week 4"')<br/>&gt;&gt;&gt; week4 </pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5024a731-2dbe-4fc1-9e27-d76065bb7859.png" style="width:19.50em;height:14.25em;"/></div>
<ol start="7">
<li>This narrows down the weeks but still doesn't automatically find out the winner of each month. Let's reshape this data with the <kbd>pivot</kbd> method so that Bob's and Amy's percent weight loss is side-by-side for each month:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; winner = week4.pivot(index='Month', columns='Name',<br/>                         values='Perc Weight Loss')<br/>&gt;&gt;&gt; winner</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/efdc2ed3-5b3b-4a62-b399-7da0b37d2cec.png" style="width:12.08em;height:12.33em;"/></div>
<ol start="8">
<li>This output makes it clearer who has won each month, but we can still go a couple steps farther. NumPy has a vectorized if-then-else function called <kbd>where</kbd>, which can map a Series or array of booleans to other values. Let's create a column for the name of the winner and highlight the winning percentage for each month:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; winner['Winner'] = np.where(winner['Amy'] &lt; winner['Bob'],<br/>                                'Amy', 'Bob')<br/>&gt;&gt;&gt; winner.style.highlight_min(axis=1)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/452143c1-f8f8-4aa6-a8e2-98749075c3b3.png" style="width:15.17em;height:12.00em;"/></div>
<ol start="9">
<li>Use the <kbd>value_counts</kbd> method to return the final score as the number of months won:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; winner.Winner.value_counts()<br/>Amy    3
Bob    1
Name: Winner, dtype: int64</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Throughout this recipe, the <kbd>query</kbd> method is used to filter data instead of boolean indexing. Refer to the <em>Improving readability of Boolean indexing with the query method</em> recipe from <a href="9f721370-ae04-4425-aab9-d525335b96b3.xhtml" target="_blank">Chapter 11</a>, <em>Boolean Indexing</em>, for more information<em>.</em></p>
<p>Our goal is to find the percentage weight loss for each month for each person. One way to accomplish this task is to calculate each week's weight loss relative to the start of each month. This specific task is perfectly suited to the <kbd>transform</kbd> groupby method. The <kbd>transform</kbd> method accepts a function as its one required parameter. This function gets implicitly passed each non-grouping column (or only the columns specified in the indexing operator as was done in this recipe with <kbd>Weight</kbd>). It must return a sequence of values the same length as the passed group or else an exception will be raised. In essence, all values from the original DataFrame are transforming. No aggregation or filtration takes place.</p>
<p>Step 2 creates a function that subtracts the first value of the passed Series from all of its values and then divides this result by the first value. This calculates the percent loss (or gain) relative to the first value. In step 3 we test this function on one person during one month.</p>
<p>In step 4, we use this function in the same manner over every combination of person and week. In some literal sense, we are <em>transforming</em> the <kbd>Weight</kbd> column into the percentage of weight lost for the current week. The first month of data is outputted for each person. Pandas returns the new data as a Series. This Series isn't all that useful by itself and makes more sense appended to the original DataFrame as a new column. We complete this operation in step 5.</p>
<p>To determine the winner, only week 4 of each month is necessary. We could stop here and manually determine the winner but pandas supplies us functionality to automate this. The <kbd>pivot</kbd> function in step 7 reshapes our dataset by pivoting the unique values of one column into new column names. The <kbd>index</kbd> parameter is used for the column that you do not want to pivot. The column passed to the <kbd>values</kbd> parameter gets tiled over each unique combination of the columns in the <kbd>index</kbd> and <kbd>columns</kbd> parameters.</p>
<div class="packt_infobox"><span>The <kbd>pivot</kbd> method only works if there is just a single occurrence of each unique combination of the columns in the</span> <kbd>index</kbd> <span>and</span> <kbd>columns</kbd> <span>parameters. If there is more than one unique combination, an exception will be raised. You can use the <kbd>pivot_table</kbd> method in that situation which allows you to aggregate multiple values together.</span></div>
<p>After pivoting, we utilize the highly effective and fast NumPy <kbd>where</kbd> function, whose first argument is a condition that produces a Series of booleans. <kbd>True</kbd> values get mapped to <em>Amy</em> and <kbd>False</kbd> values get mapped to <em>Bob.</em> We highlight the winner of each month and tally the final score with the <kbd>value_counts</kbd> method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Take a look at the DataFrame output from step 7. Did you notice that the months are in alphabetical and not chronological order? Pandas unfortunately, in this case at least, orders the months for us alphabetically. We can solve this issue by changing the data type of <kbd>Month</kbd> to a categorical variable. Categorical variables map all the values of each column to an integer. We can choose this mapping to be the normal chronological order for the months. Pandas uses this underlying integer mapping during the <kbd>pivot</kbd> method to order the months chronologically:</p>
<pre>&gt;&gt;&gt; week4a = week4.copy()<br/>&gt;&gt;&gt; month_chron = week4a['Month'].unique() # or use drop_duplicates<br/>&gt;&gt;&gt; month_chron<br/>array(['Jan', 'Feb', 'Mar', 'Apr'], dtype=object)<br/><br/>&gt;&gt;&gt; week4a['Month'] = pd.Categorical(week4a['Month'],<br/>                                     categories=month_chron,<br/>                                     ordered=True)<br/>&gt;&gt;&gt; week4a.pivot(index='Month', columns='Name',<br/>                 values='Perc Weight Loss')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0c4324b4-8516-4239-bba6-9cd5b45d1893.png" style="width:10.92em;height:11.67em;"/></div>
<p>To convert the <kbd>Month</kbd> column, use the <kbd>Categorical</kbd> constructor. Pass it the original column as a Series and a unique sequence of all the categories in the desired order to the <kbd>categories</kbd> parameter. As the <kbd>Month</kbd> column is already in chronological order, we can simply use the <kbd>unique</kbd> method, which preserves order to get the array that we desire. In general, to sort columns of object data type by something other than alphabetical, convert them to categorical.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Pandas official documentation on <kbd>groupby</kbd> <em>Transformation</em> (<a href="http://pandas.pydata.org/pandas-docs/stable/groupby.html#transformation" target="_blank">http://bit.ly/2vBkpA7</a>)</li>
<li>NumPy official documentation on the <kbd>where</kbd> function (<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html" target="_blank">http://bit.ly/2weT21l</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating weighted mean SAT scores per state with apply</h1>
                </header>
            
            <article>
                
<p>The groupby object has four methods that accept a function (or functions) to perform a calculation on each group. These four methods are <kbd>agg</kbd>, <kbd>filter</kbd>, <kbd>transform</kbd>, and <kbd>apply</kbd>. Each of the first three of these methods has a very specific output that the function must return. <kbd>agg</kbd> must return a scalar value, <kbd>filter</kbd> must return a boolean, and <kbd>transform</kbd> must return a Series with the same length as the passed group. The <kbd>apply</kbd> method, however, may return a scalar value, a Series, or even a DataFrame of any shape, therefore making it very flexible. It is also called <span>only</span> once per group, which contrasts with <kbd>transform</kbd> and <kbd>agg</kbd> that get called once for each non-grouping column. The <kbd>apply</kbd> method's ability to return a single object when operating on multiple columns at the same time makes the calculation in this recipe possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we calculate the weighted average of both the math and verbal SAT scores per state from the college dataset. We weight the scores by the population of undergraduate students per school.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the college dataset, and drop any rows that have missing values in either the <kbd>UGDS</kbd>, <kbd>SATMTMID</kbd>, or <kbd>SATVRMID</kbd> columns. We must have non-missing values for each of these three columns:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college = pd.read_csv('data/college.csv')<br/>&gt;&gt;&gt; subset = ['UGDS', 'SATMTMID', 'SATVRMID']<br/>&gt;&gt;&gt; college2 = college.dropna(subset=subset)<br/>&gt;&gt;&gt; college.shape<br/>(7535, 27)<br/><br/>&gt;&gt;&gt; college2.shape<br/>(1184, 27)</pre>
<ol start="2">
<li>The vast majority of institutions do not have data for our three required columns, but this is still more than enough data to continue. Next, create a user-defined function to calculate the weighted average of just the SAT math scores:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; def weighted_math_average(df):<br/>        weighted_math = df['UGDS'] * df['SATMTMID']<br/>        return int(weighted_math.sum() / df['UGDS'].sum())</pre>
<ol start="3">
<li>Group by state and pass this function to the <kbd>apply</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college2.groupby('STABBR').apply(weighted_math_average).head()<br/>STABBR
AK    503
AL    536
AR    529
AZ    569
CA    564
dtype: int64</pre>
<ol start="4">
<li><span>We successfully returned a scalar value for each group.</span> Let's take a small detour and see what the outcome would have been by passing the same function to the <kbd>agg</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college2.groupby('STABBR').agg(weighted_math_average).head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d02a23f5-aa1b-4a02-b5e6-5449ba33f6fe.png" style="width:61.42em;height:16.67em;"/></div>
<ol start="5">
<li>The <kbd>weighted_math_average</kbd> function gets applied to each non-aggregating column in the DataFrame. If you try and limit the columns to just <kbd>SATMTMID</kbd>, you will get an error as you won't have access to <kbd>UGDS</kbd>. So, the best way to complete operations that act on multiple columns is with <kbd>apply</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; college2.groupby('STABBR')['SATMTMID'] \<br/>            .agg(weighted_math_average)<br/>KeyError: 'UGDS'</pre>
<ol start="6">
<li>A nice feature of <kbd>apply</kbd> is that you can create multiple new columns by returning a Series. The index of this returned Series will be the new column names. Let's modify our function to calculate the weighted and arithmetic average for both SAT scores along with the count of the number of institutions from each group. We return these five values in a Series:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; from collections import OrderedDict<br/>&gt;&gt;&gt; def weighted_average(df):<br/>        data = OrderedDict()<br/>        weight_m = df['UGDS'] * df['SATMTMID']<br/>        weight_v = df['UGDS'] * df['SATVRMID']<br/>    <br/>        wm_avg = weight_m.sum() / df['UGDS'].sum()<br/>        wv_avg = weight_v.sum() / df['UGDS'].sum()<br/><br/>        data['weighted_math_avg'] = wm_avg<br/>        data['weighted_verbal_avg'] = wv_avg<br/>        data['math_avg'] = df['SATMTMID'].mean()<br/>        data['verbal_avg'] = df['SATVRMID'].mean()<br/>        data['count'] = len(df)<br/>        return pd.Series(data, dtype='int')<br/><br/>&gt;&gt;&gt; college2.groupby('STABBR').apply(weighted_average).head(10)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3267d64e-37a3-4e90-9c9f-2aabdf1b0d26.png" style="width:32.33em;height:20.50em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In order for this recipe to complete properly, we need to first filter for institutions that do not have missing values for <kbd>UGDS</kbd>, <kbd>SATMTMID</kbd>, and <kbd>SATVRMID</kbd>. By default, the <kbd>dropna</kbd> method drops rows that have one or more missing values. We must use the <kbd>subset</kbd> parameter to limit the columns it looks at for missing values.</p>
<p>In step 2, we define a function that calculates the weighted average for just the <kbd>SATMTMID</kbd> column. The weighted average differs from an arithmetic mean in that each value is multiplied by some weight. This quantity is then summed and divided by the sum of the weights. In this case, our weight is the undergraduate student population.</p>
<p>In step 3, we pass this function to the <kbd>apply</kbd> method. Our function <kbd>weighted_math_average</kbd> gets passed a DataFrame of all the original columns for each group. It returns a single scalar value, the weighted average of <kbd>SATMTMID</kbd>. At this point, you might think that this calculation is possible using the <kbd>agg</kbd> method. Directly replacing <kbd>apply</kbd> with <kbd>agg</kbd> does not work as <kbd>agg</kbd> returns a value for each of its aggregating columns.</p>
<div class="packt_infobox">It actually is possible to use <kbd>agg</kbd> indirectly by precomputing the multiplication of <kbd>UGDS</kbd> and <kbd>SATMTMID</kbd>.</div>
<p>Step 6 really shows the versatility of <kbd>apply</kbd>. We build a new function that calculates the weighted and arithmetic average of both SAT columns as well as the number of rows for each group. In order for <kbd>apply</kbd> to create multiple columns, you must return a Series. The index values are used as column names in the resulting DataFrame. You can return as many values as you want with this method.</p>
<p>Notice that the <kbd>OrderedDict</kbd> class was imported from the <kbd>collections</kbd> module, which is part of the standard library. This ordered dictionary is used to store the data. A normal Python dictionary could not have been used to store the data since it does not preserve insertion order.</p>
<div class="packt_infobox">The constructor, <kbd>pd.Series</kbd>, does have an index parameter that you can use to specify order but using an <kbd>OrderedDict</kbd> is cleaner.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we returned a single row as a Series for each group. It's possible to return any number of rows and columns for each group by returning a DataFrame. In addition to finding just the arithmetic and weighted means, let's also find the geometric and harmonic means of both SAT columns and return the results as a DataFrame with rows as the name of the type of mean and columns as the SAT type. To ease the burden on us, we use the NumPy function <kbd>average</kbd> to compute the weighted average and the SciPy functions <kbd>gmean</kbd> and <kbd>hmean</kbd> for geometric and harmonic means:</p>
<pre>&gt;&gt;&gt; from scipy.stats import gmean, hmean<br/>&gt;&gt;&gt; def calculate_means(df):<br/>        df_means = pd.DataFrame(index=['Arithmetic', 'Weighted',<br/>                                       'Geometric', 'Harmonic'])<br/>        cols = ['SATMTMID', 'SATVRMID']<br/>        for col in cols:<br/>            arithmetic = df[col].mean()<br/>            weighted = np.average(df[col], weights=df['UGDS'])<br/>            geometric = gmean(df[col])<br/>            harmonic = hmean(df[col])<br/>            df_means[col] = [arithmetic, weighted,<br/>                             geometric, harmonic]<br/>        <br/>        df_means['count'] = len(df)<br/>        return df_means.astype(int)<br/><br/>&gt;&gt;&gt; college2.groupby('STABBR').apply(calculate_means).head(12)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e4bf0560-11c6-4599-b1d5-e8f47cd2a215.png" style="width:25.75em;height:29.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Pandas official documentation of the <kbd>apply</kbd> groupby method (<a href="http://pandas.pydata.org/pandas-docs/stable/groupby.html#flexible-apply" target="_blank">http://bit.ly/2wmG9ki</a>)</li>
<li>Python official documentation of the <kbd>OrderedDict</kbd> class (<a href="https://docs.python.org/3/library/collections.html#collections.OrderedDict" target="_blank">http://bit.ly/2xwtUCa</a>)</li>
<li>SciPy official documentation of its stats module (<a href="https://docs.scipy.org/doc/scipy/reference/stats.html" target="_blank">http://bit.ly/2wHtQ4L</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grouping by continuous variables</h1>
                </header>
            
            <article>
                
<p>When grouping in pandas, you typically use columns with discrete repeating values. If there are no repeated values, then grouping would be pointless as there would only be one row per group. Continuous numeric columns typically have few repeated values and are generally not used to form groups. However, if we can transform columns with continuous values into a discrete column by placing each value into a bin, rounding them, or using some other mapping, then grouping with them makes sense.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we explore the flights dataset to discover the distribution of airlines for different travel distances. This allows us, for example, to find the airline that makes the most flights between 500 and 1,000 miles. To accomplish this, we use the pandas <kbd>cut</kbd> function to discretize the distance of each flight flown.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the flights dataset, and output the first five rows:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights = pd.read_csv('data/flights.csv')<br/>&gt;&gt;&gt; flights.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ed49e291-ded0-4c48-9451-d0f0cf9c47f8.png" style="width:69.33em;height:11.83em;"/></div>
<ol start="2">
<li>If we want to find the distribution of airlines over a range of distances, we need to place the values of the <kbd>DIST</kbd> column into discrete bins. Let's use the pandas <kbd>cut</kbd> function to split the data into five bins:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; bins = [-np.inf, 200, 500, 1000, 2000, np.inf]<br/>&gt;&gt;&gt; cuts = pd.cut(flights['DIST'], bins=bins)<br/>&gt;&gt;&gt; cuts.head()<br/>0     (500.0, 1000.0]
1    (1000.0, 2000.0]
2     (500.0, 1000.0]
3    (1000.0, 2000.0]
4    (1000.0, 2000.0]
Name: DIST, dtype: category
Categories (5, interval[float64]): [(-inf, 200.0] &lt; (200.0, 500.0] &lt; (500.0, 1000.0] &lt; (1000.0, 2000.0] &lt; (2000.0, inf]]</pre>
<ol start="3">
<li>An ordered categorical Series is created. To help get an idea of what happened, let's count the values of each category:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; cuts.value_counts()<br/>(500.0, 1000.0]     20659<br/>(200.0, 500.0]      15874
(1000.0, 2000.0]    14186
(2000.0, inf]        4054
(-inf, 200.0]        3719
Name: DIST, dtype: int64</pre>
<ol start="4">
<li>The <kbd>cuts</kbd> Series can now be used to form groups. Pandas allows you to form groups in any way you wish. Pass the <kbd>cuts</kbd> Series to the <kbd>groupby</kbd> method and then call the <kbd>value_counts</kbd> method on the <kbd>AIRLINE</kbd> column to find the distribution for each distance group. Notice that SkyWest (<em>OO</em>) makes up 33% of flights less than 200 miles but only 16% of those between 200 and 500 miles:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights.groupby(cuts)['AIRLINE'].value_counts(normalize=True) \<br/>                                    .round(3).head(15)<br/>DIST            AIRLINE
(-inf, 200.0]   OO         0.326
                EV         0.289
                MQ         0.211
                DL         0.086
                AA         0.052
                UA         0.027
                WN         0.009
(200.0, 500.0]  WN         0.194
                DL         0.189
                OO         0.159
                EV         0.156
                MQ         0.100
                AA         0.071
                UA         0.062
                VX         0.028
Name: AIRLINE, dtype: float64</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 2, the <kbd>cut</kbd> function places each value of the <kbd>DIST</kbd> column into one of five bins. The bins are created by a sequence of six numbers defining the edges. You always need one more edge than the number of bins. You can pass the <kbd>bins</kbd> parameter an integer, which automatically creates that number of equal-width bins. Negative infinity and positive infinity objects are available in NumPy and ensure that all values get placed in a bin. If you have values that are outside of the bin edges, they will be made missing and not be placed in a bin.</p>
<p>The <kbd>cuts</kbd> variable is now a Series of five ordered categories. It has all the normal Series methods and in step 3, the <kbd>value_counts</kbd> method is used to get a sense of its distribution.</p>
<p>Very interestingly, pandas allows you to pass the <kbd>groupby</kbd> method any object. This means that you are able to form groups from something completely unrelated to the current DataFrame. Here, we group by the values in the <kbd>cuts</kbd> variable. For each grouping, we find the percentage of flights per airline with <kbd>value_counts</kbd> by setting <kbd>normalize</kbd> to <kbd>True</kbd>.</p>
<p>Some interesting insights can be drawn from this result. Looking at the full result, SkyWest is the leading airline for under 200 miles but has no flights over 2,000 miles. In contrast, American Airlines has the fifth highest total for flights under 200 miles but has by far the most flights between 1,000 and 2,000 miles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>We can find more results when grouping by the <kbd>cuts</kbd> variable. For instance, we can find the 25th, 50th, and 75th percentile airtime for each distance grouping. As airtime is in minutes, we can divide by 60 to get hours:</p>
<pre>&gt;&gt;&gt; flights.groupby(cuts)['AIR_TIME'].quantile(q=[.25, .5, .75]) \<br/>                                     .div(60).round(2)<br/>DIST                  
(-inf, 200.0]     0.25    0.43
                  0.50    0.50
                  0.75    0.57
(200.0, 500.0]    0.25    0.77
                  0.50    0.92
                  0.75    1.05
(500.0, 1000.0]   0.25    1.43
                  0.50    1.65
                  0.75    1.92
(1000.0, 2000.0]  0.25    2.50
                  0.50    2.93
                  0.75    3.40
(2000.0, inf]     0.25    4.30
                  0.50    4.70
                  0.75    5.03
Name: AIR_TIME, dtype: float64</pre>
<p>We can use this information to create informative string labels when using the <kbd>cut</kbd> function. These labels replace the interval notation. We can also chain the <kbd>unstack</kbd> method which transposes the inner index level to column names:</p>
<pre>&gt;&gt;&gt; labels=['Under an Hour', '1 Hour', '1-2 Hours',<br/>            '2-4 Hours', '4+ Hours']<br/>&gt;&gt;&gt; cuts2 = pd.cut(flights['DIST'], bins=bins, labels=labels)<br/>&gt;&gt;&gt; flights.groupby(cuts2)['AIRLINE'].value_counts(normalize=True) \<br/>                                     .round(3) \<br/>                                     .unstack() \<br/>                                     .style.highlight_max(axis=1)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a44ef2f0-739b-4970-abba-c3e147aa0031.png" style="width:59.83em;height:16.83em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Pandas official documentation on the <kbd>cut</kbd> function (<a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html" target="_blank">http://bit.ly/2whcUkJ</a>)</li>
<li>Refer to <a href="a833ecb6-8487-4648-8632-860640490e01.xhtml" target="_blank">Chapter 14</a>, <em>Re</em><em>structuring Data into a Tidy For</em><em>m</em>, for many more recipes with unstack</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Counting the total number of flights between cities</h1>
                </header>
            
            <article>
                
<p>In the flights dataset, we have data on the origin and destination airport. It is trivial to count the number of flights originating in Houston and landing in Atlanta, for instance. What is more difficult is counting the total number of flights between the two cities, regardless of which one is the origin or destination.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we count the total number of flights between two cities regardless of which one is the origin or destination. To accomplish this, we sort the origin and destination airports alphabetically so that each combination of airports always occurs in the same order. We can then use this new column arrangement to form groups and then to count.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the flights dataset, and find the total number of flights between each origin and destination airport:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights = pd.read_csv('data/flights.csv')<br/>&gt;&gt;&gt; flights_ct = flights.groupby(['ORG_AIR', 'DEST_AIR']).size()<br/>&gt;&gt;&gt; flights_ct.head()<br/>ORG_AIR  DEST_AIR
ATL      ABE         31
         ABQ         16
         ABY         19
         ACY          6
         AEX         40
dtype: int64</pre>
<ol start="2">
<li>Select the total number of flights between Houston (<em>IAH</em>) and Atlanta (<em>ATL</em>) in both directions:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights_ct.loc[[('ATL', 'IAH'), ('IAH', 'ATL')]]<br/>ORG_AIR  DEST_AIR
ATL      IAH         121
IAH      ATL         148
dtype: int64</pre>
<ol start="3">
<li>We could simply sum these two numbers together to find the total flights between the cities but there is a more efficient and automated solution that can work for all flights. Let's independently sort the origin and destination cities for each row in alphabetical order:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights_sort = flights[['ORG_AIR', 'DEST_AIR']] \<br/>                          .apply(sorted, axis=1)<br/>&gt;&gt;&gt; flights_sort.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bacd986b-9e18-46ee-b46c-caf49b2b8385.png" style="width:11.25em;height:11.42em;"/></div>
<ol start="4">
<li>Now that each row has been independently sorted, the column names are not correct. Let's rename them to something more generic and then again find the total number of flights between all cities:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; rename_dict = {'ORG_AIR':'AIR1', 'DEST_AIR':'AIR2'}<br/>&gt;&gt;&gt; flights_sort = flights_sort.rename(columns=rename_dict)<br/>&gt;&gt;&gt; flights_ct2 = flights_sort.groupby(['AIR1', 'AIR2']).size()<br/>&gt;&gt;&gt; flights_ct2.head()<br/>AIR1  AIR2<br/>ABE   ATL     31
      ORD     24
ABI   DFW     74
ABQ   ATL     16
      DEN     46
dtype: int64</pre>
<ol start="5">
<li>Let's select all the flights between Atlanta and Houston and verify that it matches the sum of the values in step 2:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights_ct2.loc[('ATL', 'IAH')]<br/>269</pre>
<ol start="6">
<li>If we try and select flights with Houston followed by Atlanta, we get an error:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights_ct2.loc[('IAH', 'ATL')]<br/>IndexingError: Too many indexers</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we form groups by the origin and destination airport columns and then apply the <kbd>size</kbd> method to the groupby object, which simply returns the total number of rows for each group. Notice that we could have passed the string <kbd>size</kbd> to the <kbd>agg</kbd> method to achieve the same result. In step 2, the total number of flights for each direction between Atlanta and Houston are selected. The Series <kbd>flights_count</kbd> has a MultiIndex with two levels. One way to select rows from a MultiIndex is to pass the <kbd>loc</kbd> indexing operator a tuple of exact level values. Here, we actually select two different rows, <kbd>('ATL', 'HOU')</kbd> and <kbd>('HOU', 'ATL')</kbd>. We use a list of tuples to do this correctly.</p>
<p>Step 3 is the most pertinent step in the recipe. We would like to have just one label for all flights between Atlanta and Houston and so far we have two. If we alphabetically sort each combination of origin and destination airports, we would then have a single label for flights between airports. To do this, we use the DataFrame <kbd>apply</kbd> method. This is different from the groupby <kbd>apply</kbd> method. No groups are formed in step 3.</p>
<p>The DataFrame <kbd>apply</kbd> method must be passed a function. In this case, it's the built-in <kbd>sorted</kbd> function. By default, this function gets applied to each column as a Series. We can change the direction of computation by using <kbd>axis=1</kbd> (or <kbd>axis='index'</kbd>). The <kbd>sorted</kbd> function has each row of data passed to it implicitly as a Series. It returns a list of sorted airport codes. <span>Here is an example of passing the first row as a Series to the sorted function:</span></p>
<pre>&gt;&gt;&gt; sorted(flights.loc[0, ['ORG_AIR', 'DEST_AIR']])<br/>['LAX', 'SLC']</pre>
<p>The <kbd>apply</kbd> method iterates over all rows using <kbd>sorted</kbd> in this exact manner. After completion of this operation, each row is independently sorted. The column names are now meaningless. We rename the column names in the next step and then perform the same grouping and aggregating as was done in step 2. This time, all flights between Atlanta and Houston fall under the same label.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>You might be wondering why we can't use the simpler <kbd>sort_values</kbd> Series method. This method does not sort independently and instead, preserves the row or column as a single record as one would expect while doing a data analysis. Step 3 is a very expensive operation and takes several seconds to complete. There are only about 60,000 rows so this solution would not scale well to larger data. Calling the</p>
<p>Step 3 is a very expensive operation and takes several seconds to complete. There are only about 60,000 rows so this solution would not scale well to larger data. Calling the <kbd>apply</kbd> method with <kbd>axis=1</kbd> is one of the least performant operations in all of pandas. Internally, pandas loops over each row and does not provide any speed boosts from NumPy. If possible, avoid using <kbd>apply</kbd> with <kbd>axis=1</kbd>.</p>
<p>We can get a massive speed increase with the NumPy <kbd>sort</kbd> function. Let's go ahead and use this function and analyze its output. By default, it sorts each row independently:</p>
<pre>&gt;&gt;&gt; data_sorted = np.sort(flights[['ORG_AIR', 'DEST_AIR']])<br/>&gt;&gt;&gt; data_sorted[:10]<br/>array([['LAX', 'SLC'],
       ['DEN', 'IAD'],
       ['DFW', 'VPS'],
       ['DCA', 'DFW'],
       ['LAX', 'MCI'],
       ['IAH', 'SAN'],
       ['DFW', 'MSY'],
       ['PHX', 'SFO'],
       ['ORD', 'STL'],
       ['IAH', 'SJC']], dtype=object)</pre>
<p>A two-dimensional NumPy array is returned. NumPy does not easily do grouping operations so let's use the DataFrame constructor to create a new DataFrame and check whether it equals the <kbd>flights_sorted</kbd> DataFrame from step 3:</p>
<pre>&gt;&gt;&gt; flights_sort2 = pd.DataFrame(data_sorted, columns=['AIR1', 'AIR2'])<br/>&gt;&gt;&gt; fs_orig = flights_sort.rename(columns={'ORG_AIR':'AIR1',<br/>                                           'DEST_AIR':'AIR2'})<br/>&gt;&gt;&gt; flights_sort2.equals(fs_orig)<br/>True</pre>
<p>As the DataFrames are the same, you can replace step 3 with the previous faster sorting routine. Let's time the difference between each of the different sorting methods:</p>
<pre>&gt;&gt;&gt; %%timeit <br/>&gt;&gt;&gt; flights_sort = flights[['ORG_AIR', 'DEST_AIR']] \<br/>                          .apply(sorted, axis=1)<br/>7.41 s ± 189 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)<br/><br/>&gt;&gt;&gt; %%timeit<br/>&gt;&gt;&gt; data_sorted = np.sort(flights[['ORG_AIR', 'DEST_AIR']])<br/>&gt;&gt;&gt; flights_sort2 = pd.DataFrame(data_sorted,<br/>                                 columns=['AIR1', 'AIR2'])<br/>10.6 ms ± 453 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</pre>
<p>The NumPy solution is an astounding 700 times faster than using <kbd>apply</kbd> with pandas.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>NumPy official documentation on the <kbd>sort</kbd> function (<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.sort.html" target="_blank">http://bit.ly/2vtRt0M</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding the longest streak of on-time flights</h1>
                </header>
            
            <article>
                
<p>One of the most important metrics for airlines is their on-time flight performance. The Federal Aviation Administration considers a flight delayed when it arrives at least 15 minutes later than its scheduled arrival time. Pandas has direct methods to calculate the total and percentage of on-time flights per airline. While these basic summary statistics are an important metric, there are other non-trivial calculations that are interesting, such as finding the length of consecutive on-time flights for each airline at each of its origin airports.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we find the longest consecutive streak of on-time flights for each airline at each origin airport. This requires each value in a column to be aware of the value immediately following it. We make clever use of the <kbd>diff</kbd> and <kbd>cumsum</kbd> methods in order to find streaks before applying this methodology to each of the groups.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Before we get started with the actual flights dataset, let's practice counting streaks of ones with a small sample Series:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; s = pd.Series([0, 1, 1, 0, 1, 1, 1, 0])<br/>&gt;&gt;&gt; s<br/>0    0<br/>1    1<br/>2    1
3    0
4    1
5    1
6    1
7    0
dtype: int64</pre>
<ol start="2">
<li>Our final representation of the streaks of ones will be a Series of the same length as the original with an independent count beginning from one for each streak. To get started, let's use the <kbd>cumsum</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; s1 = s.cumsum()<br/>&gt;&gt;&gt; s1<br/>0    0
1    1
2    2
3    2
4    3
5    4
6    5
7    5
dtype: int64</pre>
<ol start="3">
<li>We have now accumulated all the ones going down the Series. Let's multiply this Series by the original:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; s.mul(s1)<br/>0    0
1    1
2    2
3    0
4    3
5    4
6    5
7    0
dtype: int64</pre>
<ol start="4">
<li>We have only non-zero values where we originally had ones. This result is fairly close to what we desire. We just need to restart each streak at one instead of where the cumulative sum left off. Let's chain the <kbd>diff</kbd> method, which subtracts the previous value from the current:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; s.mul(s1).diff()<br/>0    NaN
1    1.0
2    1.0
3   -2.0
4    3.0
5    1.0
6    1.0
7   -5.0
dtype: float64</pre>
<ol start="5">
<li>A negative value represents the end of a streak. We need to propagate the negative values down the Series and use them to subtract away the excess accumulation from step 2. To do this, we will make all non-negative values missing with the <kbd>where</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; s.mul(s1).diff().where(lambda x: x &lt; 0)<br/>0    NaN<br/>1    NaN
2    NaN
3   -2.0
4    NaN
5    NaN
6    NaN
7   -5.0
dtype: float64</pre>
<ol start="6">
<li>We can now propagate these values down with the <kbd>ffill</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; s.mul(s1).diff().where(lambda x: x &lt; 0).ffill()<br/>0    NaN
1    NaN
2    NaN
3   -2.0
4   -2.0
5   -2.0
6   -2.0
7   -5.0
dtype: float64</pre>
<ol start="7">
<li>Finally, we can add this Series back to <kbd>s1</kbd> to clear out the excess accumulation:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; s.mul(s1).diff().where(lambda x: x &lt; 0).ffill() \<br/>     .add(s1, fill_value=0)<br/>0    0.0
1    1.0
2    2.0
3    0.0
4    1.0
5    2.0
6    3.0
7    0.0
dtype: float64</pre>
<ol start="8">
<li>Now that we have a working consecutive streak finder, we can find the longest streak per airline and origin airport. Let's read in the flights dataset and create a column to represent on-time arrival:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights = pd.read_csv('data/flights.csv')<br/>&gt;&gt;&gt; flights['ON_TIME'] = flights['ARR_DELAY'].lt(15).astype(int)<br/>&gt;&gt;&gt; flights[['AIRLINE', 'ORG_AIR', 'ON_TIME']].head(10)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6e668d00-e72b-4851-a2ab-dfe2ecbd2739.png" style="width:8.75em;height:12.08em;"/></div>
<ol start="9">
<li>Use our logic from the first seven steps to define a function that returns the maximum streak of ones for a given Series:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; def max_streak(s):<br/>        s1 = s.cumsum()<br/>        return s.mul(s1).diff().where(lambda x: x &lt; 0) \<br/>                .ffill().add(s1, fill_value=0).max()</pre>
<ol start="10">
<li>Find the maximum streak of on-time arrivals per airline and origin airport along with the total number of flights and percentage of on-time arrivals. First, sort the day of the year and scheduled departure time:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; flights.sort_values(['MONTH', 'DAY', 'SCHED_DEP']) \<br/>           .groupby(['AIRLINE', 'ORG_AIR'])['ON_TIME'] \<br/>           .agg(['mean', 'size', max_streak]).round(2).head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3bd814fd-2141-4ccc-9b1b-334f35c85196.png" style="width:19.83em;height:13.17em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Finding streaks in the data is not a straightforward operation in pandas and requires methods that look ahead or behind, such as <kbd>diff</kbd> or <kbd>shift</kbd>, or those that remember their current state, such as <kbd>cumsum</kbd>. The final result from the first seven steps is a Series the same length as the original that keeps track of all consecutive ones. Throughout these steps, we use the <kbd>mul</kbd> and <kbd>add</kbd> <span>methods</span> instead of their operator equivalents (<kbd>*</kbd>) and (<kbd>+</kbd>). In my opinion, this allows for a slightly cleaner progression of calculations from left to right. You, of course, can replace these with the actual operators.</p>
<p>Ideally, we would like to tell pandas to apply the <kbd>cumsum</kbd> method to the start of each streak and reset itself after the end of each one. It takes many steps to convey this message to pandas. Step 2 accumulates all the ones in the Series as a whole. The rest of the steps slowly remove any excess accumulation. In order to identify this excess accumulation, we need to find the end of each streak and subtract this value from the beginning of the next streak.</p>
<p>To find the end of each streak, we cleverly make all values not part of the streak zero by multiplying <kbd>s1</kbd> by the original Series of zeros and ones in step 3. The first zero following a non-zero, marks the end of a streak. That's good, but again, we need to eliminate the excess accumulation. Knowing where the streak ends doesn't exactly get us there.</p>
<p>In step 4, we use the <kbd>diff</kbd> method to find this excess. The <kbd>diff</kbd> method takes the difference between the current value and any value located at a set number of rows away from it. By default, the difference between the current and the immediately preceding value is returned.</p>
<p>Only negative values are meaningful in step 4. Those are the ones immediately following the end of a streak. These values need to be propagated down until the end of the following streak. To eliminate (make missing) all the values we don't care about, we use the <kbd>where</kbd> method, which takes a Series of conditionals of the same size as the calling Series. By default, all the <kbd>True</kbd> values remain the same, while the <kbd>False</kbd> values become missing. The <kbd>where</kbd> method allows you to use the calling Series as part of the conditional by taking a function as its first parameter. An anonymous function is used, which gets passed the calling Series implicitly and checks whether each value is less than zero. The result of step 5 is a Series where only the negative values are preserved with the rest changed to missing.</p>
<p>The <kbd>ffill</kbd> method in step 6 replaces missing values with the last non-missing value going forward/down a Series. As the first three values don't follow a non-missing value, they remain missing. We finally have our Series that removes the excess accumulation. We add our accumulation Series to the result of step 6 to get the streaks all beginning from zero. The <kbd>add</kbd> method allows us to replace the missing values with the <kbd>fill_value</kbd> parameter. This completes the process of finding streaks of ones in the dataset. When doing complex logic like this, it is a good idea to use a small dataset where you know what the final output will be. It would be quite a difficult task to start at step 8 and build this streak-finding logic while grouping.</p>
<p>In step 8, we create the <kbd>ON_TIME</kbd> column. One item of note is that the cancelled flights have missing values for <kbd>ARR_DELAY</kbd>, which do not pass the boolean condition and therefore result in a zero for the <kbd>ON_TIME</kbd> column. Canceled flights are treated the same as delayed.</p>
<p>Step 9 turns our logic from the first seven steps into a function and chains the <kbd>max</kbd> method to return the longest streak. As our function returns a single value, it is formally an aggregating function and can be passed to the <kbd>agg</kbd> method as done in step 10. To ensure that we are looking at actual consecutive flights, we use the <kbd>sort_values</kbd> method to sort by date and scheduled departure time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Now that we have found the longest streaks of on-time arrivals, we can easily find the opposite--the longest streak of delayed arrivals. The following function returns two rows for each group passed to it. The first row is the start of the streak, and the last row is the end of the streak. Each row contains the month and day that the streak started/ended, along with the total streak length:</p>
<pre>&gt;&gt;&gt; def max_delay_streak(df):<br/>        df = df.reset_index(drop=True)<br/>        s = 1 - df['ON_TIME']<br/>        s1 = s.cumsum()<br/>        streak = s.mul(s1).diff().where(lambda x: x &lt; 0) \<br/>                  .ffill().add(s1, fill_value=0)<br/>        last_idx = streak.idxmax()<br/>        first_idx = last_idx - streak.max() + 1<br/>        df_return = df.loc[[first_idx, last_idx], ['MONTH', 'DAY']]<br/>        df_return['streak'] = streak.max()<br/>        df_return.index = ['first', 'last']<br/>        df_return.index.name='type'<br/>        return df_return<br/><br/>&gt;&gt;&gt; flights.sort_values(['MONTH', 'DAY', 'SCHED_DEP']) \<br/>           .groupby(['AIRLINE', 'ORG_AIR']) \<br/>           .apply(max_delay_streak) \<br/>           .sort_values('streak', ascending=False).head(10)</pre>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2f18b944-147e-4743-aa76-2e1ee9800842.png" style="width:19.50em;height:18.42em;"/></div>
<p>As we are using the <kbd>apply</kbd> groupby method, a DataFrame of each group is passed to the <kbd>max_delay_streak</kbd> function. Inside this function, the index of the DataFrame is dropped and replaced by a <kbd>RangeIndex</kbd> in order for us to easily find the first and last row of the streak. The <kbd>ON_TIME</kbd> column is inverted and then the same logic is used to find streaks of delayed flights. The index of the first and last rows of the streak are stored as variables. These indexes are then used to select the month and day when the streaks ended. We use a DataFrame to return our results. We label and name the index to make the final result clearer.</p>
<p>Our final results show the longest delayed streaks accompanied by the first and last date. Let's investigate to see if we can find out why these delays happened. Inclement weather is a common reason for delayed or canceled flights. Looking at the first row, American Airlines (AA) started a streak of 38 delayed flights in a row from the Dallas Fort-Worth (DFW) airport beginning February 26 until March 1 of 2015. Looking at historical weather data from February 27, 2015, two inches of snow fell, which was a record for that day (<a href="http://bit.ly/2iLGsCg" target="_blank">http://bit.ly/2iLGsCg</a>). This was a major weather event for DFW and caused massive problems for the entire city (<a href="http://bit.ly/2wmsHPj" target="_blank">http://bit.ly/2wmsHPj</a>). Notice that DFW makes another appearance as the third longest streak but this time a few days earlier and for a different airline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Pandas official documentation of <kbd>ffill</kbd> ( <a href="http://bit.ly/2gn5zGU" target="_blank">http://bit.ly/2gn5zGU</a></span></li>
</ul>


            </article>

            
        </section>
    </body></html>