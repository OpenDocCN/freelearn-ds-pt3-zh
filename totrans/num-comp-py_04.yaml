- en: Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of unsupervised learning is to discover the hidden patterns or structures
    of the data in which no target variable exists to perform either classification
    or regression methods. Unsupervised learning methods are often more challenging,
    as the outcomes are subjective and there is no simple goal for the analysis, such
    as predicting the class or continuous variable. These methods are performed as
    part of exploratory data analysis. On top of that, it can be hard to assess the
    results obtained from unsupervised learning methods, since there is no universally
    accepted mechanism for performing the validation of results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonetheless, unsupervised learning methods have growing importance in various
    fields as a trending topic nowadays, and many researchers are actively working
    on them at the moment to explore this new horizon. A few good applications are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Genomics**: Unsupervised learning applied to understanding genomic-wide biological
    insights from DNA to better understand diseases and peoples. These types of tasks
    are more exploratory in nature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search engine**: Search engines might choose which search results to display
    to a particular individual based on the click histories of other similar users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge extraction**: To extract the taxonomies of concepts from raw text
    to generate the knowledge graph to create the semantic structures in the field
    of NLP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Segmentation of customers**: In the banking industry, unsupervised learning
    like clustering is applied to group similar customers, and based on those segments,
    marketing departments design their contact strategies. For example, older, low-risk
    customers will be targeted with fixed deposit products and high-risk, younger
    customers will be targeted with credit cards or mutual funds, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Social network analysis**: To identify the cohesive groups of people in networks
    who are more connected with each other and have similar characteristics in common.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following techniques to perform unsupervised
    learning with data which is openly available:'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep auto encoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is the task of grouping observations in such a way that members of
    the same cluster are more similar to each other and members of different clusters
    are very different from each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering is commonly used to explore a dataset to either identify the underlying
    patterns in it or to create a group of characteristics. In the case of social
    networks, they can be clustered to identify communities and to suggest missing
    connections between people. Here are a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: In anti-money laundering measures, suspicious activities and individuals can
    be identified using anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In biology, clustering is used to find groups of genes with similar expression
    patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In marketing analytics, clustering is used to find segments of similar customers
    so that different marketing strategies can be applied to different customer segments
    accordingly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The k-means clustering algorithm is an iterative process of moving the centers
    of clusters or centroids to the mean position of their constituent points, and
    reassigning instances to their closest clusters iteratively until there is no
    significant change in the number of cluster centers possible or number of iterations
    reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function of k-means is determined by the Euclidean distance (square-norm)
    between the observations belonging to that cluster with its respective centroid
    value. An intuitive way to understand the equation is, if there is only one cluster
    (*k=1*), then the distances between all the observations are compared with its
    single mean. Whereas, if, number of clusters increases to *2* (*k= 2*), then two-means
    are calculated and a few of the observations are assigned to cluster *1* and other
    observations are assigned to cluster two*-*based on proximity. Subsequently, distances
    are calculated in cost functions by applying the same distance measure, but separately
    to their cluster centers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b6d065b-e4eb-4099-98ea-1bb4ce828e7a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: K-means working methodology from first principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means working methodology is illustrated in the following example in which
    12 instances are considered with their *X* and *Y* values. The task is to determine
    the optimal clusters out of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Instance** | **X** | **Y** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 7 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 6 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 6 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 5 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 3 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 1 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 5 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 7 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 7 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: After plotting the data points on a 2D chart, we can see that roughly two clusters
    are possible, where below-left is the first cluster and the top-right is another
    cluster, but in many practical cases, there would be so many variables (or dimensions)
    that, we cannot simply visualize them. Hence, we need a mathematical and algorithmic
    way to solve these types of problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0c97d44-75e6-4853-92fe-a3372e260516.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Iteration 1: Let us assume two centers from two instances out of all the *12*
    instances. Here, we have chosen instance *1* (*X = 7, Y = 8*) and instance *8*
    (*X = 1, Y = 4*), as they seem to be at both extremes. For each instance, we will
    calculate its Euclidean distances with respect to both centroids and assign it
    to the nearest cluster center.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Instance** | **X** | **Y** | **Centroid 1 distance** | **Centroid 2 distance**
    | **Assigned cluster** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 7 | 8 | 7.21 | 0.00 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | 4 | 1.00 | 6.40 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 6 | 4 | 5.00 | 4.12 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3 | 2 | 2.83 | 7.21 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 6 | 5 | 5.10 | 3.16 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 5 | 7 | 5.00 | 2.24 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 3 | 3 | 2.24 | 6.40 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 1 | 4 | 0.00 | 7.21 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 5 | 4 | 4.00 | 4.47 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 7 | 7 | 6.71 | 1.00 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 7 | 6 | 6.32 | 2.00 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 2 | 1 | 3.16 | 8.60 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| Centroid 1 | 1 | 4 | ![](img/438e74a5-745c-43f0-ad9c-2362b2b59b9c.png) |
    ![](img/cbff530d-df02-4220-9c3e-08fed5173b00.png) | ![](img/8af3b2a7-62cc-4f43-92ae-40a7b23fd7bf.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Centroid 2 | 7 | 8 | ![](img/358bb8ba-84bc-41af-adbc-ba5af7a1c240.png) |
    ![](img/4e3fe26d-4992-4faa-85bb-0f9e97e3c876.png) | ![](img/a739b078-ad01-43e3-a39d-ef9175add4bd.png)
    |'
  prefs: []
  type: TYPE_TB
- en: 'The Euclidean distance between two points *A (X1, Y1)* and *B (X2, Y2)* is
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55d4981b-74a0-49ff-847c-6979ec3f53eb.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Centroid distance calculations are performed by taking Euclidean distances.
    A sample calculation has been shown as follows. For instance, six with respect
    to both centroids (centroid 1 and centroid 2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/110a5c4f-55e5-4f06-ba05-d66bdffb1d7f.jpg)![](img/9f064a47-95f4-4124-aa3f-8a92fadc15b2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following chart describes the assignment of instances to both centroids,
    which was shown in the preceding table format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77001e36-ac36-4787-a869-233ccf9d4785.png)'
  prefs: []
  type: TYPE_IMG
- en: If we carefully observe the preceding chart, we realize that all the instances
    seem to be assigned appropriately apart from instance *9 (X =5, Y = 4)*. However,
    in later stages, it should be assigned appropriately. Let us see in the below
    steps how the assignments evolve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Iteration 2: In this iteration, new centroids are calculated from the assigned
    instances for that cluster or centroid. New centroids are calculated based on
    the simple average of the assigned points.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Instance** | **X** | **Y** | **Assigned cluster** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 7 | 8 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | 4 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 6 | 4 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3 | 2 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 6 | 5 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 5 | 7 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 3 | 3 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 1 | 4 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 5 | 4 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 7 | 7 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 7 | 6 | C2 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 2 | 1 | C1 |'
  prefs: []
  type: TYPE_TB
- en: '| Centroid 1 | 2.67 | 3 | ![](img/742e1f1f-b3d1-4f1c-9784-abc6d1a3094c.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Centroid 2 | 6.33 | 6.17 | ![](img/43b9c467-4b37-4ffa-8eaa-839f8fd21095.png)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Sample calculations for centroids 1 and 2 are shown as follows. A similar methodology
    will be applied on all subsequent iterations as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c84a254-ad28-4346-96d2-fd7375d5af80.jpg)![](img/bc01e19a-688e-441c-af12-a666e4697e47.jpg)![](img/e17ff033-4f21-4475-844f-93fa65c45e6b.jpg)![](img/f978f046-94fd-4db3-9e3b-f5e12cbe22b9.jpg)![](img/285ee94c-54a6-4438-827d-d21c75b808e6.jpg)![](img/cbd81fbc-7003-4f1e-a7f3-5577aada051c.jpg)![](img/29582c7e-f166-4f38-8d69-6369cd60f01e.png)'
  prefs: []
  type: TYPE_IMG
- en: After updating the centroids, we need to reassign the instances to the nearest
    centroids, which we will be performing in iteration 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Iteration 3: In this iteration, new assignments are calculated based on the
    Euclidean distance between instances and new centroids. In the event of any changes,
    new centroids will be calculated iteratively until no changes in assignments are
    possible or the number of iterations is reached. The following table describes
    the distance measures between new centroids and all the instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Instance** | **X** | **Y** | **Centroid 1 distance** | **Centroid 2 distance**
    | **Previously assigned cluster** | **Newly assigned cluster** | **Changed?**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 7 | 8 | 6.61 | 1.95 | C2 | C2 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | 4 | 1.20 | 4.84 | C1 | C1 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 6 | 4 | 3.48 | 2.19 | C2 | C2 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3 | 2 | 1.05 | 5.34 | C1 | C1 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 6 | 5 | 3.88 | 1.22 | C2 | C2 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 5 | 7 | 4.63 | 1.57 | C2 | C2 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 3 | 3 | 0.33 | 4.60 | C1 | C1 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 1 | 4 | 1.95 | 5.75 | C1 | C1 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 5 | 4 | 2.54 | 2.55 | C1 | C1 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 7 | 7 | 5.89 | 1.07 | C2 | C2 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 7 | 6 | 5.27 | 0.69 | C2 | C2 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 2 | 1 | 2.11 | 6.74 | C1 | C1 | No |'
  prefs: []
  type: TYPE_TB
- en: '| Centroid 1 | 2.67 | 3 | ![](img/646a430e-92f0-4fa3-aa08-984cd2bdbd34.png)
    | ![](img/09a53854-a61d-49ce-a6cb-1a001a8e975e.png) | ![](img/dd3ea496-cba7-4453-8c04-a1eea53f23db.png)
    | ![](img/c3da0bc7-e7ee-40ee-a5cb-03893bd6aedd.png) | ![](img/4ed2a736-8357-494a-ab4d-17be40cf3fc5.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Centroid 2 | 6.33 | 6.17 | ![](img/a3eb34ab-9293-4099-9786-90591849fe29.png)
    | ![](img/252f60f5-038e-411c-92cb-66d4a9d35e5c.png) | ![](img/dbf84cbb-e970-4bc6-80d0-d9c30687e625.png)
    | ![](img/e9b5dd81-94dc-4f29-8a10-dd27556a7680.png) | ![](img/06298a03-a5fa-4365-8734-12379b51a337.png)
    |'
  prefs: []
  type: TYPE_TB
- en: It seems that there are no changes registered. Hence, we can say that the solution
    is converged. One important thing to note here is that all the instances are very
    clearly classified well, apart from instance *9 (X = 5, Y = 4).* Based on instinct,
    it seems like it should be assigned to centroid 2, but after careful calculation,
    that instance is more proximate to cluster 1 than cluster 2\. However, the difference
    in distance is low (2.54 with centroid 1 and 2.55 with centroid 2).
  prefs: []
  type: TYPE_NORMAL
- en: Optimal number of clusters and cluster evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though selecting number of clusters is more of an art than science, optimal
    number of clusters are chosen where there will not be a much marginal increase
    in explanation ability by increasing number of clusters are possible. In practical
    applications, usually business should be able to provide what would be approximate
    number of clusters they are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: The elbow method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The elbow method is used to determine the optimal number of clusters in k-means
    clustering. The elbow method plots the value of the cost function produced by
    different values of *k*. As you know, if *k* increases, average distortion will
    decrease, each cluster will have fewer constituent instances, and the instances
    will be closer to their respective centroids. However, the improvements in average
    distortion will decline as *k* increases. The value of *k* at which improvement
    in distortion declines the most is called the elbow, at which we should stop dividing
    the data into further clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc03a902-fad6-4f77-8799-00a69a78965f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Evaluation of clusters with silhouette coefficient: the silhouette coefficient
    is a measure of the compactness and separation of the clusters. Higher values
    represent a better quality of cluster. The silhouette coefficient is higher for
    compact clusters that are well separated and lower for overlapping clusters. Silhouette
    coefficient values do change from -1 to +1, and the higher the value is, the better.'
  prefs: []
  type: TYPE_NORMAL
- en: The silhouette coefficient is calculated per instance. For a set of instances,
    it is calculated as the mean of the individual sample's scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2801e6f-85c6-4465-ab6c-1a32129a3059.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*a* is the mean distance between the instances in the cluster, *b* is the mean
    distance between the instance and the instances in the next closest cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering with the iris data example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The famous iris data has been used from the UCI machine learning repository
    for illustration purposes using k-means clustering. The link for downloading the
    data is here: [http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris).
    The iris data has three types of flowers: setosa, versicolor, and virginica and
    their respective measurements of sepal length, sepal width, petal length, and
    petal width. Our task is to group the flowers based on their measurements. The
    code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ef89819f-082a-43d6-8e09-9faba82ae4b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following code is used to separate `class` variable as dependent variable for
    creating colors in plot and unsupervised learning algorithm applied on given `x`
    variables without any target variable does present:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As sample metrics, three clusters have been used, but in real life, we do not
    know how many clusters data will fall under in advance, hence we need to test
    the results by trial and error. The maximum number of iterations chosen here is
    300 in the following, however, this value can also be changed and the results
    checked accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/eb623d02-bec4-4a18-b0f7-50ea36f59690.png)'
  prefs: []
  type: TYPE_IMG
- en: From the previous confusion matrix, we can see that all the setosa flowers are
    clustered correctly, whereas 2 out of 50 versicolor, and 14 out of 50 virginica
    flowers are incorrectly classified.
  prefs: []
  type: TYPE_NORMAL
- en: Again, to reiterate, in real-life examples we do not have the category names
    in advance, so we cannot measure accuracy, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following code is used to perform sensitivity analysis to check how many number
    of clusters does actually provide better explanation of segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/87ed59be-8c75-4a70-9081-061fbf072085.png)'
  prefs: []
  type: TYPE_IMG
- en: The silhouette coefficient values in the preceding results shows that `K value
    2` and `K value 3` have better scores than all the other values. As a thumb rule,
    we need to take the next `K value` of the highest silhouette coefficient. Here,
    we can say that `K value 3` is better. In addition, we also need to see the average
    within cluster variation value and elbow plot before concluding the optimal `K
    value`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/8daf39b3-428f-408c-85f1-2b5f00e27337.png)'
  prefs: []
  type: TYPE_IMG
- en: From the elbow plot, it seems that at the value of three, the slope changes
    drastically. Here, we can select the optimal k-value as three.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d33fe9d2-c24d-462e-954b-eaec1784dc56.png)'
  prefs: []
  type: TYPE_IMG
- en: Last but not least, the total percentage of variance explained value should
    be greater than 80 percent to decide the optimal number of clusters. Even here,
    a k-value of three seems to give a decent value of total variance explained. Hence,
    we can conclude from all the preceding metrics (silhouette, average within cluster
    variance, and total variance explained), that three clusters are ideal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R code for k-means clustering using iris data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Principal Component Analysis - PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**) is the dimensionality reduction
    technique which has so many utilities. PCA reduces the dimensions of a dataset
    by projecting the data onto a lower-dimensional subspace. For example, a 2D dataset
    could be reduced by projecting the points onto a line. Each instance in the dataset
    would then be represented by a single value, rather than a pair of values. In
    a similar way, a 3D dataset could be reduced to two dimensions by projecting variables
    onto a plane. PCA has the following utilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Mitigate the course of dimensionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compress the data while minimizing the information lost at the same time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal components will be further utilized in the next stage of supervised
    learning, in random forest, boosting, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the structure of data with hundreds of dimensions can be difficult,
    hence, by reducing the dimensions to 2D or 3D, observations can be visualized
    easily
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA can easily be explained with the following diagram of a mechanical bracket
    which has been drawn in the machine drawing module of a mechanical engineering
    course. The left-hand side of the diagram depicts the top view, front view, and
    side view of the component. However, on the right-hand side, an isometric view
    has been drawn, in which one single image has been used to visualize how the component
    looks. So, one can imagine that the left-hand images are the actual variables
    and the right-hand side is the first principal component, in which most variance
    has been captured.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, three images have been replaced by a single image by rotating the axis
    of direction. In fact, we replicate the same technique in PCA analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b01cbe2-ce2d-4f9c-8f32-2726291ccd9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Principal component working methodology is explained in the following sample
    example, in which actual data has been shown in a 2D space, in which *X* and *Y*
    axis are used to plot the data. Principal components are the ones in which maximum
    variation of the data is captured.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb756076-0484-4d81-91e0-7a3408877bbc.png)'
  prefs: []
  type: TYPE_IMG
- en: The following diagram illustrates how it looks after fitting the principal components.
    The first principal component covers the maximum variance in the data and the
    second principal component is orthogonal to the first principal component, as
    we know all principal components are orthogonal to each other. We can represent
    whole data with the first principal component itself. In fact, that is how it
    is advantageous to represent the data with fewer dimensions, to save space and
    also to grab maximum variance in the data, which can be utilized for supervised
    learning in the next stage. This is the core advantage of computing principal
    components.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c14a3e04-d0bd-4e75-a88c-3bc333313de5.png)'
  prefs: []
  type: TYPE_IMG
- en: Eigenvectors and eigenvalues have significant importance in the field of linear
    algebra, physics, mechanics, and so on. Refreshing, basics on eigenvectors and
    eigenvalues is necessary when studying PCAs. Eigenvectors are the axes (directions)
    along which a linear transformation acts simply by *stretching/compressing* and/or
    *flipping*; whereas, eigenvalues give you the factors by which the compression
    occurs. In another way, an eigenvector of a linear transformation is a nonzero
    vector whose direction does not change when that linear transformation is applied
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, *A* is a linear transformation from a vector space and ![](img/3db9b070-a499-4f85-adbb-f94db5952d8e.jpg) is
    a nonzero vector, then eigen vector of *A* if ![](img/3cb9f44d-af9b-458c-9ab7-30d0c07d0097.jpg) is
    a scalar multiple of ![](img/2bbd2b5e-e3f5-4a73-9fcf-e22a9fab8389.jpg). The condition
    can be written as the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f928a6c-47af-4385-a6d7-fab5714d5e3e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, ![](img/3be9b079-d8cb-429a-a0bb-846f2f1e8376.jpg) is
    an eigenvector, *A* is a square matrix, and λ is a scalar called an eigenvalue.
    The direction of an eigenvector remains the same after it has been transformed
    by *A*; only its magnitude has changed, as indicated by the eigenvalue, That is,
    multiplying a matrix by one of its eigenvectors is equal to scaling the eigenvector,
    which is a compact representation of the original matrix. The following graph
    describes eigenvectors and eigenvalues in a graphical representation in a 2D space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17245038-7b03-404a-a156-64d171bafc2b.png)'
  prefs: []
  type: TYPE_IMG
- en: The following example describes how to calculate eigenvectors and eigenvalues
    from the square matrix and its understanding. Note that eigenvectors and eigenvalues
    can be calculated only for square matrices (those with the same dimensions of
    rows and columns).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cea11ff3-de17-44c2-a6f3-3ea239962f68.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall the equation that the product of *A* and any eigenvector of *A* must
    be equal to the eigenvector multiplied by the magnitude of eigenvalue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cba22798-df2e-4158-888c-8df53ef0ade2.jpg)![](img/f58e2ccc-bbc1-416b-8a2b-0e4454dcc257.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A characteristic equation states that the determinant of the matrix, that is
    the difference between the data matrix and the product of the identity matrix
    and an eigenvalue is *0*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40df230b-283f-4970-9b17-c0d969b6a754.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Both eigenvalues for the preceding matrix are equal to *-2*. We can use eigenvalues
    to substitute for eigenvectors in an equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8910138-9f31-4248-9895-e8cb45c7cfdd.jpg)![](img/7c827736-7dc5-4c02-8f0c-0c90a94300eb.jpg)![](img/7dc3ba7d-5947-436f-a1ae-35ddcb67a9ab.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the value of eigenvalue in the preceding equation, we will obtain
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49410af9-b4e6-45af-8312-458cb9ad23e8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation can be rewritten as a system of equations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1073470-0ede-40de-beef-e5f1922bb29a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This equation indicates it can have multiple solutions of eigenvectors we can
    substitute with any values which hold the preceding equation for verification
    of equation. Here, we have used the vector *[1 1]* for verification, which seems
    to be proved.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99fb5711-0835-4d42-a455-1bfb86610443.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'PCA needs unit eigenvectors to be used in calculations, hence we need to divide
    the same with the norm or we need to normalize the eigenvector. The 2-norm equation
    is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59838abb-7a26-498f-914a-2c37a0ed985b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The norm of the output vector is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2c8bb1d-4a74-4d73-aef8-5f08fb11791f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The unit eigenvector is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0339799d-78a7-4016-accb-4d2b1b091791.jpg)'
  prefs: []
  type: TYPE_IMG
- en: PCA working methodology from first principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PCA working methodology is described in the following sample data, which has
    two dimensions for each instance or data point. The objective here is to reduce
    the 2D data into one dimension (also known as the **principal component**):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Instance** | **X** | **Y** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.72 | 0.13 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.18 | 0.23 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2.50 | 2.30 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.45 | 0.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.04 | 0.44 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.13 | 0.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.30 | 0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2.65 | 2.10 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.91 | 0.91 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.46 | 0.32 |'
  prefs: []
  type: TYPE_TB
- en: '| Column mean | 0.83 | 0.69 |'
  prefs: []
  type: TYPE_TB
- en: The first step, prior to proceeding with any analysis, is to subtract the mean
    from all the observations, which removes the scale factor of variables and makes
    them more uniform across dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '| **X** | **Y** |'
  prefs: []
  type: TYPE_TB
- en: '| *0.72 - 0.83 = -0.12* | *0.13 - 0.69 = - 0.55* |'
  prefs: []
  type: TYPE_TB
- en: '| *0.18 - 0.83 = -0.65* | *0.23 - 0.69 = - 0.46* |'
  prefs: []
  type: TYPE_TB
- en: '| *2.50 - 0.83 = 1.67* | *2.30 - 0.69 = 1.61* |'
  prefs: []
  type: TYPE_TB
- en: '| *0.45 - 0.83 = -0.38* | *0.16 - 0.69 = - 0.52* |'
  prefs: []
  type: TYPE_TB
- en: '| *0.04 - 0.83 = -0.80* | *0.44 - 0.69 = - 0.25* |'
  prefs: []
  type: TYPE_TB
- en: '| *0.13 - 0.83 = -0.71* | *0.24 - 0.69 = - 0.45* |'
  prefs: []
  type: TYPE_TB
- en: '| *0.30 - 0.83 = -0.53* | *0.03 - 0.69 = - 0.66* |'
  prefs: []
  type: TYPE_TB
- en: '| *2.65 - 0.83 = 1.82* | *2.10 - 0.69 = 1.41* |'
  prefs: []
  type: TYPE_TB
- en: '| *0.91 - 0.83 = 0.07* | *0.91 - 0.69 = 0.23* |'
  prefs: []
  type: TYPE_TB
- en: '| *0.46 - 0.83 = -0.37* | *0.32 - 0.69 = -0.36* |'
  prefs: []
  type: TYPE_TB
- en: 'Principal components are calculated using two different techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Covariance matrix of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be covering the singular value decomposition technique in the next section.
    In this section, we will solve eigenvectors and eigenvalues using covariance matrix
    methodology.
  prefs: []
  type: TYPE_NORMAL
- en: 'Covariance is a measure of how much two variables change together and it is
    a measure of the strength of the correlation between two sets of variables. If
    the covariance of two variables is zero, we can conclude that there will not be
    any correlation between two sets of the variables. The formula for covariance
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff9c2871-d222-4ea6-ad0a-7709e9255386.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A sample covariance calculation is shown for *X* and *Y* variables in the following
    formulas. However, it is a 2 x 2 matrix of an entire covariance matrix (also,
    it is a square matrix).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df0457af-fc6c-407e-9500-1a329a999902.jpg)![](img/56608b63-bc70-4109-bec9-369e7131b43c.jpg)![](img/a2526e8a-4126-408b-8c35-516225fa0e7d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since the covariance matrix is square, we can calculate eigenvectors and eigenvalues
    from it. You can refer to the methodology explained in an earlier section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbf6df6e-5d7a-4a74-8d50-4cd569311ae0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By solving the preceding equation, we can obtain eigenvectors and eigenvalues,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6786705-3ce4-498c-8a3f-1f8c54286647.jpg)![](img/aeb1ea0e-ce34-4a15-b69e-8b5d4fa6cbf5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding mentioned results can be obtained with the following Python syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/4ecbc0fb-7f02-4f7b-8c3f-450f594cf379.png)'
  prefs: []
  type: TYPE_IMG
- en: Once we obtain the eigenvectors and eigenvalues, we can project data into principal
    components. The first eigenvector has the greatest eigenvalue and is the first
    principal component, as we would like to reduce the original 2D data into 1D data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a1b86ba-2783-41a7-9b6b-c9ba58786a34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding result, we can see the 1D projection of the first principal
    component from the original 2D data. Also, the eigenvalue of 1.5725 explains the
    fact that the principal component explains variance of 57 percent more than the
    original variables. In the case of multi-dimensional data, the thumb rule is to
    select the eigenvalues or principal components with a value greater than what
    should be considered for projection.
  prefs: []
  type: TYPE_NORMAL
- en: PCA applied on handwritten digits using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The PCA example has been illustrated with the handwritten digits example from
    scikit-learn datasets, in which handwritten digits are created from 0-9 and its
    respective 64 features (8 x 8 matrix) of pixel intensities. Here, the idea is
    to represent the original features of 64 dimensions into as few as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2153e5c6-a7cd-4825-b7f0-18243c674eee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plot the graph using the `plt.show` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d5d0f2fd-2576-4511-9ec2-985333812d25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Before performing PCA, it is advisable to perform scaling of input data to
    eliminate any issues due to different dimensions of the data. For example, while
    applying PCA on customer data, their salary has larger dimensions than the customer''s
    age. Hence, if we do not put all the variables in a similar dimension, one variable
    will explain the entire variation rather than its actual impact. In the following
    code, we have applied scaling on all the columns separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the following, we have used two principal components, so that we can represent
    the performance on a 2D graph. In later sections, we have applied 3D as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following section of code, we are appending the relevant principal components
    to each digit separately so that we can create a scatter plot of all 10 digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e1250b44-8893-4b58-bfef-9856a5144b97.png)'
  prefs: []
  type: TYPE_IMG
- en: Though the preceding plot seems a bit cluttered, it does provide some idea of
    how the digits are close to and distant from each other. We get the idea that
    digits *6* and *8* are very similar and digits *4* and *7* are very distant from
    the center group, and so on. However, we should also try with a higher number
    of PCAs, as, sometimes, we might not be able to represent every variation in two
    dimensions itself.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code, we have applied three PCAs so that we can get a better
    view of the data in a 3D space. The procedure is very much similar as with two
    PCAs, except for creating one extra dimension for each digit (*X*, *Y*, and *Z*).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/558b7610-9ae7-417c-9a54-af1327c3dcd2.png)'
  prefs: []
  type: TYPE_IMG
- en: matplotlib plots have one great advantage over other software plots such as
    R plot, and so on. They are interactive, which means that we can rotate them and
    see how they look from various directions. We encourage the reader to observe
    the plot by rotating and exploring. In a 3D plot, we can see a similar story with
    more explanation. Digit *2* is at the extreme left and digit *0* is at the lower
    part of the plot. Whereas digit *4* is at the top-right end, digit *6* seems to
    be more towards the *PC 1* axis. In this way, we can visualize and see how digits
    are distributed. In the case of 4 PCAs, we need to go for subplots and visualize
    them separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the number of PCAs to be extracted is an open-ended question in unsupervised
    learning, but there are some turnarounds to get an approximated view. There are
    two ways we can determine the number of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Check where the total variance explained is diminishing marginally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total variance explained greater than 80 percent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following code does provide the total variance explained with the change
    in number of principal components. With the more number of PCs, more variance
    will be explained. But however, the challenge is to restrict it as fewer PCs possible,
    this will be achieved by restricting where the marginal increase in variance explained
    start diminishes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3cb54060-0a40-42a4-8245-5a788f0c293b.png)'
  prefs: []
  type: TYPE_IMG
- en: From the previous plot, we can see that total variance explained diminishes
    marginally at 10 PCAs; whereas, total variance explained greater than 80 percent
    is given at 21 PCAs. It is up to the business and user which value to choose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R code for PCA applied on handwritten digits data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Singular value decomposition - SVD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many implementations of PCA use singular value decomposition to calculate eigenvectors
    and eigenvalues. SVD is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bad9800-489d-425d-abdf-8e81028dabbd.jpg)![](img/34c6d790-5379-452b-a386-e8fd4a2b1359.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Columns of *U* are called left singular vectors of the data matrix, the columns
    of *V* are its right singular vectors, and the diagonal entries of ![](img/d869b053-93bb-4e03-9a0d-52edbde6c63e.png)
    are its singular values. Left singular vectors are the eigenvectors of the covariance
    matrix and the diagonal element of ![](img/c808febf-3aac-4e39-bf3d-de049bfe7bb3.png)
    are the square roots of the eigenvalues of the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before proceeding with SVD, it would be advisable to understand a few advantages
    and important points about SVD:'
  prefs: []
  type: TYPE_NORMAL
- en: SVD can be applied even on rectangular matrices; whereas, eigenvalues are defined
    only for square matrices. The equivalent of eigenvalues obtained through the SVD
    method are called singular values, and vectors obtained equivalent to eigenvectors
    are known as singular vectors. However, as they are rectangular in nature, we
    need to have left singular vectors and right singular vectors respectively for
    their dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a matrix *A* has a matrix of eigenvectors *P* that is not invertible, then
    *A* does not have an eigen decomposition. However, if *A* is *m* x *n* real matrix
    with *m* > *n*, then A can be written using a singular value decomposition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both *U* and *V* are orthogonal matrices, which means *U^T U = I* (*I* with
    *m* x *m* dimension) or *V^T V = I* (here *I* with *n* x *n* dimension), where
    two identity matrices may have different dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/beca8756-97e1-46e6-b726-4bb3bcc97c1a.png)is a non-negative diagonal
    matrix with *m* x *n* dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then computation of singular values and singular vectors is done with the following
    set of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7d80ce1-8853-409b-b9fa-b5743561ba83.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the first stage, singular values/eigenvalues are calculated with the equation.
    Once we obtain the singular/eigenvalues, we will substitute to determine the *V*
    or right singular/eigen vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbdd9880-81f0-4372-a22a-f2bd4930c904.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we obtain the right singular vectors and diagonal values, we will substitute
    to obtain the left singular vectors *U* using the equation mentioned as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/585fb802-b3e1-4703-8e0b-ce0835889d61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this way, we will calculate the singular value decompositions of the original
    system of equations matrix.
  prefs: []
  type: TYPE_NORMAL
- en: SVD applied on handwritten digits using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SVD can be applied on the same handwritten digits data to perform an apple-to-apple
    comparison of techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the following code, 15 singular vectors with 300 iterations are used, but
    we encourage the reader to change the values and check the performance of SVD.
    We have used two types of SVD functions, as a function `randomized_svd` provide
    the decomposition of the original matrix and a `TruncatedSVD` can provide total
    variance explained ratio. In practice, uses may not need to view all the decompositions
    and they can just use the `TruncatedSVD` function for their practical purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b74ae632-a243-4362-b0fd-a1f652afd617.png)'
  prefs: []
  type: TYPE_IMG
- en: By looking at the previous screenshot, we can see that the original matrix of
    dimension (1797 x 64) has been decomposed into a left singular vector (1797 x
    15), singular value (diagonal matrix of 15), and right singular vector (15 x 64).
    We can obtain the original matrix by multiplying all three matrices in order.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/63a4fdf0-fb70-4097-b069-33fa26fdefa5.png)'
  prefs: []
  type: TYPE_IMG
- en: The total variance explained for 15 singular value features is 83.4 percent.
    But the reader needs to change the different values to decide the optimum value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates the change in total variance explained with
    respective to change in number of singular values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c77ef1df-2649-4377-8f21-a37a43d75474.png)'
  prefs: []
  type: TYPE_IMG
- en: From the previous plot, we can choose either 8 or 15 singular vectors based
    on the requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R code for SVD applied on handwritten digits data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Deep auto encoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The auto encoder neural network is an unsupervised learning algorithm that applies
    backpropagation setting the target values to be equal to the inputs *y^((i)) =
    x^((i))*. Auto encoder tries to learn a function *h[w,b](x) ≈ x*, means it tries
    to learn an approximation to the identity function, so that output  ![](img/f7e07b21-0c88-4e4f-8778-a576c73e30c6.jpg) 
    that is similar to *x*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a9a491a-e5af-49d7-b618-eb7f46b02642.png)'
  prefs: []
  type: TYPE_IMG
- en: Though trying to replicate the identity function seems trivial function to learn,
    by placing the constraints on the network, such as by limiting number of hidden
    units, we can discover interesting structures about the data. Let's say input
    picture of size 10 x 10 pixels has intensity values which have, altogether, 100
    input values, the number of neurons in the second layer (hidden layer) is 50 units,
    and the output layer, finally, has 100 units of neurons as we need to pass the
    image to map it to itself and while achieving this representation in the process
    we would force the network to learn a compressed representation of the input,
    which is hidden unit activations *a^((2)) ε  R^(100)*, with which we must try
    to reconstruct the 100 pixel input *x*. If the input data is completely random
    without any correlations, and so on. it would be very difficult to compress, whereas
    if the underlying data have some correlations or detectable structures, then this
    algorithm will be able to discover the correlations and represent them compactly.
    In fact, auto encoder often ends up learning a low-dimensional representation
    very similar to PCAs.
  prefs: []
  type: TYPE_NORMAL
- en: Model building technique using encoder-decoder architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training the auto encoder model is a bit tricky, hence a detailed illustration
    has been provided for better understanding. During the training phase, the whole
    encoder-decoder section is trained against the same input as an output of decoder.
    In order to achieve the desired output, features will be compressed during the
    middle layer, as we are passing through the convergent and divergent layers. Once
    enough training has been done by reducing the error values over the number of
    iterations, we will use the trained encoder section to create the latent features
    for next stage of modeling, or for visualization, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the following diagram, a sample has been shown. The input and output layers
    have five neurons, whereas the number of neurons has been gradually decreased
    in the middle sections. The compressed layer has only two neurons, which is the
    number of latent dimensions we would like to extract from the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfec2005-fe5a-4f24-a0f8-9899219bdb98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram depicts using the trained encoder section to create latent
    features from the new input data, which will be utilized for visualization or
    for utilizing in the next stage of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/410aa78f-a964-4567-82ee-c944940b6f2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep auto encoders applied on handwritten digits using Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep auto encoders are explained with same handwritten digits data to show
    the comparison of how this non-linear method differs to linear methods like PCA
    and SVD. Non-linear methods generally perform much better, but these methods are
    kind of black-box models and we cannot determine the explanation behind that.
    Keras software has been utilized to build the deep auto encoders here, as they
    work like Lego blocks, which makes it easy for users to play around with different
    architectures and parameters of the model for better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/78b1f846-5712-45a8-b1d1-1c4e16682e72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dense neuron modules from Keras used for constructing encoder-decoder architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/73cfd2fa-5076-4ff6-85cd-5c5ab85ef714.png)'
  prefs: []
  type: TYPE_IMG
- en: GPU of NVIDIA GTX 1060 has been used here, also `cuDNN` and `CNMeM` libraries
    are installed for further enhancement of speed up to 4x-5x on the top of regular
    GPU performance. These libraries utilize 20 percent of GPU memory, which left
    the 80 percent of memory for working on the data. The user needs to be careful,
    if they have low memory GPUs like 3 GB to 4 GB, they may not be able to utilize
    these libraries.
  prefs: []
  type: TYPE_NORMAL
- en: The reader needs to consider one important point that, syntax of Keras code,
    will remain same in both CPU and GPU mode.
  prefs: []
  type: TYPE_NORMAL
- en: The following few lines of codes are the heart of the model. Input data have
    64 columns. We need to take those columns into the input of the layers, hence
    we have given the shape of 64\. Also, names have been assigned to each layer of
    the neural network, which we will explain the reason for in an upcoming section
    of the code. In the first hidden layer, 32 dense neurons are utilized, which means
    all the 64 inputs from the input layer are connected to 32 neurons in first hidden
    layer. The entire flow of dimensions are like *64, 32, 16, 2, 16, 32, 64*. We
    have compressed input to two neurons, in order to plot the components on a 2D
    plot, whereas, if we need to plot a 3D data (which we will be covering in the
    next section), we need to change the hidden three-layer number to three instead
    of two. After training is complete, we need to use encoder section and predict
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the model, we need to pass the starting and ending point of the architecture.
    In the following code, we have provided input as `input_layer` and output as `decoded`,
    which is the last layer (the name is `h6decode`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Adam optimization has been used to optimize the mean square error, as we wanted
    to reproduce the original input at the end of the output layer of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The network is trained with 100 epochs and a batch size of 256 observations
    per each batch. Validation split of 20 percent is used to check the accuracy on
    randomly selected validation data in order to ensure robustness, as if we just
    train only on the train data may create the overfitting problem, which is very
    common with highly non-linear models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/72ac3852-820e-495d-a7a8-08d87908150e.png)'
  prefs: []
  type: TYPE_IMG
- en: From the previous results, we can see that the model has been trained on 1,437
    train examples and validation on 360 examples. By looking into the loss value,
    both train and validation losses have decreased from 1.2314 to 0.9361 and 1.0451
    to 0.7326 respectively. Hence, we are moving in the right direction. However,
    readers are encouraged to try various architectures and number of iterations,
    batch sizes, and so on to see how much the accuracies can be further improved.
  prefs: []
  type: TYPE_NORMAL
- en: Once the encoder-decoder section has been trained, we need to take only the
    encoder section to compress the input features in order to obtain the compressed
    latent features, which is the core idea of dimensionality reduction altogether!
    In the following code, we have constructed another model with a trained input
    layer and a middle hidden layer (`h3latent_layer`). This is the reason behind
    assigning names for each layer of the network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Just to check the dimensions of the reduced input variables and we can see
    that for all observations, we can see two dimensions or two column vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d340c571-4e8e-4dc5-a16e-c8fda6499fe3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following section of the code is very much similar to 2D PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/969f6b4b-0500-4d46-ae3f-4d363cd134b3.png)'
  prefs: []
  type: TYPE_IMG
- en: From the previous plot we can see that data points are well separated, but the
    issue is the direction of view, as these features do not vary as per the dimensions
    perpendicular to each other, similar to principal components, which are orthogonal
    to each other. In the case of deep auto encoders, we need to change the view of
    direction from the *(0, 0)* to visualize this non-linear classification, which
    we will see in detail in the following 3D case.
  prefs: []
  type: TYPE_NORMAL
- en: The following is the code for 3D latent features. All the code remains the same
    apart from the `h3latent_layer`, in which we have to replace the value from `2`
    to `3`, as this is the end of encoder section and we will utilize it in creating
    the latent features and, eventually, it will be used for plotting purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/336bf646-4ee2-4721-8e45-ef4a33e53a74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous results we can see that, with the inclusion of three dimensions
    instead of two, loss values obtained are less than in the 2D use case. Train and
    validation losses for two latent factors after 100 epochs are 0.9061 and 0.7326,
    and for three latent factors after 100 epochs, are 0.8032 and 0.6464\. This signifies
    that, with the inclusion of one more dimension, we can reduce the errors significantly.
    This way, the reader can change various parameters to determine the ideal architecture
    for dimensionality reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d9c4ebd5-4049-4312-b0ad-120b367d964e.png)'
  prefs: []
  type: TYPE_IMG
- en: 3D plots from deep auto encoders do provide well separated classification compared
    with three PCAs. Here we have got better separation of the digits. One important
    point the reader should consider here is that the above plot is the rotated view
    from *(0, 0, 0)*, as data separation does not happen across orthogonal planes
    (like PCAs), hence we need to see the view from origin in order to see this non-linear
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned about various unsupervised learning methods
    to identify the structures and patterns within the data using k-mean clustering,
    PCA, SVD and deep auto encoders. Also, the k-means clustering algorithm explained
    with iris data. Methods were shown on how to choose the optimal k-value based
    on various performance metrics. Handwritten data from scikit-learn was been utilized
    to compare the differences between linear methods like PCA and SVD with non-linear
    techniques and deep auto encoders. The differences between PCA and SVD were given
    in detail so that the reader can understand SVD, which can be applied even on
    rectangular matrices where the number of users and number of products is not necessarily
    equal. In the end, through visualization, it has been proven that deep auto encoders
    are better at separating digits than linear unsupervised learning methods like
    PCA and SVD.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be discussing various reinforcement learning methods
    and their utilities in artificial intelligence and so on.
  prefs: []
  type: TYPE_NORMAL
