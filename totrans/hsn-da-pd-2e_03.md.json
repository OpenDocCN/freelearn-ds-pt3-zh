["```py\n>>> import numpy as np\n>>> data = np.genfromtxt(\n...     'data/example_data.csv', delimiter=';', \n...     names=True, dtype=None, encoding='UTF'\n... )\n>>> data\narray([('2018-10-13 11:10:23.560',\n'262km NW of Ozernovskiy, Russia', \n        'mww', 6.7, 'green', 1),\n('2018-10-13 04:34:15.580', \n        '25km E of Bitung, Indonesia', 'mww', 5.2, 'green', 0),\n('2018-10-13 00:13:46.220', '42km WNW of Sola, Vanuatu', \n        'mww', 5.7, 'green', 0),\n('2018-10-12 21:09:49.240', \n        '13km E of Nueva Concepcion, Guatemala',\n        'mww', 5.7, 'green', 0),\n('2018-10-12 02:52:03.620', \n        '128km SE of Kimbe, Papua New Guinea',\n        'mww', 5.6, 'green', 1)],\n      dtype=[('time', '<U23'), ('place', '<U37'),\n             ('magType', '<U3'), ('mag', '<f8'),\n             ('alert', '<U5'), ('tsunami', '<i8')])\n```", "```py\n>>> data.shape\n(5,)\n>>> data.dtype\ndtype([('time', '<U23'), ('place', '<U37'), ('magType', '<U3'), \n       ('mag', '<f8'), ('alert', '<U5'), ('tsunami', '<i8')])\n```", "```py\n>>> %%timeit\n>>> max([row[3] for row in data])\n9.74 µs ± 177 ns per loop \n(mean ± std. dev. of 7 runs, 100000 loops each)\n```", "```py\n>>> array_dict = {\n...     col: np.array([row[i] for row in data])\n...     for i, col in enumerate(data.dtype.names)\n... }\n>>> array_dict\n{'time': array(['2018-10-13 11:10:23.560',\n        '2018-10-13 04:34:15.580', '2018-10-13 00:13:46.220',\n        '2018-10-12 21:09:49.240', '2018-10-12 02:52:03.620'],\n        dtype='<U23'),\n 'place': array(['262km NW of Ozernovskiy, Russia', \n        '25km E of Bitung, Indonesia',\n        '42km WNW of Sola, Vanuatu',\n        '13km E of Nueva Concepcion, Guatemala',\n        '128km SE of Kimbe, Papua New Guinea'], dtype='<U37'),\n 'magType': array(['mww', 'mww', 'mww', 'mww', 'mww'], \n        dtype='<U3'),\n 'mag': array([6.7, 5.2, 5.7, 5.7, 5.6]),\n 'alert': array(['green', 'green', 'green', 'green', 'green'], \n        dtype='<U5'),\n 'tsunami': array([1, 0, 0, 0, 1])}\n```", "```py\n>>> %%timeit\n>>> array_dict['mag'].max()\n5.22 µs ± 100 ns per loop \n(mean ± std. dev. of 7 runs, 100000 loops each)\n```", "```py\n>>> np.array([\n...     value[array_dict['mag'].argmax()]\n...     for key, value in array_dict.items()\n... ])\narray(['2018-10-13 11:10:23.560',\n       '262km NW of Ozernovskiy, Russia',\n       'mww', '6.7', 'green', '1'], dtype='<U31')\n```", "```py\n>>> import pandas as pd\n>>> place = pd.Series(array_dict['place'], name='place')\n>>> place\n0          262km NW of Ozernovskiy, Russia\n1              25km E of Bitung, Indonesia\n2                42km WNW of Sola, Vanuatu\n3    13km E of Nueva Concepcion, Guatemala\n4      128km SE of Kimbe, Papua New Guinea\nName: place, dtype: object\n```", "```py\n>>> place_index = place.index\n>>> place_index\nRangeIndex(start=0, stop=5, step=1)\n```", "```py\n>>> place_index.values\narray([0, 1, 2, 3, 4], dtype=int64)\n```", "```py\n>>> np.array([1, 1, 1]) + np.array([-1, 0, 1])\narray([0, 1, 2])\n```", "```py\n>>> numbers = np.linspace(0, 10, num=5) # [0, 2.5, 5, 7.5, 10]\n>>> x = pd.Series(numbers) # index is [0, 1, 2, 3, 4]\n>>> y = pd.Series(numbers, index=pd.Index([1, 2, 3, 4, 5]))\n>>> x + y\n0     NaN\n1     2.5\n2     7.5\n3    12.5\n4    17.5\n5     NaN\ndtype: float64\n```", "```py\n>>> df = pd.DataFrame(array_dict) \n>>> df\n```", "```py\n>>> df.dtypes\ntime        object\nplace       object\nmagType     object\nmag        float64\nalert       object\ntsunami      int64\ndtype: object\n```", "```py\n>>> df.values\narray([['2018-10-13 11:10:23.560',\n        '262km NW of Ozernovskiy, Russia',\n        'mww', 6.7, 'green', 1],\n['2018-10-13 04:34:15.580', \n        '25km E of Bitung, Indonesia', 'mww', 5.2, 'green', 0],\n['2018-10-13 00:13:46.220', '42km WNW of Sola, Vanuatu', \n        'mww', 5.7, 'green', 0],\n       ['2018-10-12 21:09:49.240',\n        '13km E of Nueva Concepcion, Guatemala',\n        'mww', 5.7, 'green', 0],\n['2018-10-12 02:52:03.620','128 km SE of Kimbe, \nPapua New Guinea', 'mww', 5.6, 'green', 1]], \n      dtype=object)\n```", "```py\n>>> df.columns\nIndex(['time', 'place', 'magType', 'mag', 'alert', 'tsunami'], \n      dtype='object')\n```", "```py\n>>> df + df\n```", "```py\n>>> import datetime as dt\n>>> import numpy as np\n>>> import pandas as pd\n```", "```py\n>>> np.random.seed(0) # set a seed for reproducibility\n>>> pd.Series(np.random.rand(5), name='random')\n0    0.548814\n1    0.715189\n2    0.602763\n3    0.544883\n4    0.423655\nName: random, dtype: float64\n```", "```py\n>>> np.random.seed(0) # set seed so result is reproducible\n>>> pd.DataFrame(\n...     {\n...         'random': np.random.rand(5),\n...         'text': ['hot', 'warm', 'cool', 'cold', None],\n...         'truth': [np.random.choice([True, False]) \n...                   for _ in range(5)]\n...     }, \n...     index=pd.date_range(\n...         end=dt.date(2019, 4, 21),\n...         freq='1D', periods=5, name='date'\n...     )\n... )\n```", "```py\n>>> pd.DataFrame([\n...     {'mag': 5.2, 'place': 'California'},\n...     {'mag': 1.2, 'place': 'Alaska'},\n...     {'mag': 0.2, 'place': 'California'},\n... ])\n```", "```py\n>>> list_of_tuples = [(n, n**2, n**3) for n in range(5)]\n>>> list_of_tuples\n[(0, 0, 0), (1, 1, 1), (2, 4, 8), (3, 9, 27), (4, 16, 64)]\n>>> pd.DataFrame(\n...     list_of_tuples,\n...     columns=['n', 'n_squared', 'n_cubed']\n... )\n```", "```py\n>>> pd.DataFrame(\n...     np.array([\n...         [0, 0, 0],\n...         [1, 1, 1],\n...         [2, 4, 8],\n...         [3, 9, 27],\n...         [4, 16, 64]\n...     ]), columns=['n', 'n_squared', 'n_cubed']\n... )\n```", "```py\n>>> !wc -l data/earthquakes.csv\n9333 data/earthquakes.csv\n```", "```py\n>>> !ls -lh data | grep earthquakes.csv\n-rw-r--r-- 1 stefanie stefanie 3.4M ... earthquakes.csv\n```", "```py\n>>> files = !ls -lh data\n>>> [file for file in files if 'earthquake' in file]\n['-rw-r--r-- 1 stefanie stefanie 3.4M ... earthquakes.csv']\n```", "```py\n>>> !head -n 2 data/earthquakes.csv\nalert,cdi,code,detail,dmin,felt,gap,ids,mag,magType,mmi,net,nst,place,rms,sig,sources,status,time,title,tsunami,type,types,tz,updated,url\n,,37389218,https://earthquake.usgs.gov/[...],0.008693,,85.0,\",ci37389218,\",1.35,ml,,ci,26.0,\"9km NE of Aguanga, CA\",0.19,28,\",ci,\",automatic,1539475168010,\"M 1.4 - 9km NE of Aguanga, CA\",0,earthquake,\",geoserve,nearby-cities,origin,phase-data,\",-480.0,1539475395144,https://earthquake.usgs.gov/earthquakes/eventpage/ci37389218\n```", "```py\n>>> !awk -F',' '{print NF; exit}' data/earthquakes.csv\n26\n```", "```py\n>>> headers = !head -n 1 data/earthquakes.csv\n>>> len(headers[0].split(','))\n26\n```", "```py\n>>> df = pd.read_csv('earthquakes.csv')\n```", "```py\n>>> df = pd.read_csv(\n...     'https://github.com/stefmolin/'\n...     'Hands-On-Data-Analysis-with-Pandas-2nd-edition'\n...     '/blob/master/ch_02/data/earthquakes.csv?raw=True'\n... )\n```", "```py\n>>> df.to_csv('output.csv', index=False)\n```", "```py\n>>> import sqlite3\n>>> with sqlite3.connect('data/quakes.db') as connection:\n...     pd.read_csv('data/tsunamis.csv').to_sql(\n...         'tsunamis', connection, index=False,\n...         if_exists='replace'\n...     )\n```", "```py\n>>> import sqlite3\n>>> with sqlite3.connect('data/quakes.db') as connection:\n...     tsunamis = \\\n...         pd.read_sql('SELECT * FROM tsunamis', connection)\n>>> tsunamis.head()\n```", "```py\n>>> import datetime as dt\n>>> import pandas as pd\n>>> import requests\n```", "```py\n>>> yesterday = dt.date.today() - dt.timedelta(days=1)\n>>> api = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n>>> payload = {\n...     'format': 'geojson',\n...     'starttime': yesterday - dt.timedelta(days=30),\n...     'endtime': yesterday\n... }\n>>> response = requests.get(api, params=payload)\n```", "```py\n>>> response.status_code\n200\n```", "```py\n>>> earthquake_json = response.json()\n>>> earthquake_json.keys()\ndict_keys(['type', 'metadata', 'features', 'bbox'])\n```", "```py\n>>> earthquake_json['metadata']\n{'generated': 1604267813000,\n 'url': 'https://earthquake.usgs.gov/fdsnws/event/1/query?\nformat=geojson&starttime=2020-10-01&endtime=2020-10-31',\n 'title': 'USGS Earthquakes',\n 'status': 200,\n 'api': '1.10.3',\n 'count': 13706}\n```", "```py\n>>> type(earthquake_json['features'])\nlist\n```", "```py\n>>> earthquake_json['features'][0]\n{'type': 'Feature',\n 'properties': {'mag': 1,\n  'place': '50 km ENE of Susitna North, Alaska',\n  'time': 1604102395919, 'updated': 1604103325550, 'tz': None,\n  'url': 'https://earthquake.usgs.gov/earthquakes/eventpage/ak020dz5f85a',\n  'detail': 'https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=ak020dz5f85a&format=geojson',\n  'felt': None, 'cdi': None, 'mmi': None, 'alert': None,\n  'status': 'reviewed', 'tsunami': 0, 'sig': 15, 'net': 'ak',\n  'code': '020dz5f85a', 'ids': ',ak020dz5f85a,',\n  'sources': ',ak,', 'types': ',origin,phase-data,',\n  'nst': None, 'dmin': None, 'rms': 1.36, 'gap': None,\n  'magType': 'ml', 'type': 'earthquake',\n  'title': 'M 1.0 - 50 km ENE of Susitna North, Alaska'},\n 'geometry': {'type': 'Point', 'coordinates': [-148.9807, 62.3533, 5]},\n 'id': 'ak020dz5f85a'} \n```", "```py\n>>> earthquake_properties_data = [\n...     quake['properties'] \n...     for quake in earthquake_json['features']\n... ]\n```", "```py\n>>> df = pd.DataFrame(earthquake_properties_data)\n```", "```py\n>>> import numpy as np\n>>> import pandas as pd\n>>> df = pd.read_csv('data/earthquakes.csv')\n```", "```py\n>>> df.empty\nFalse\n```", "```py\n>>> df.shape\n(9332, 26)\n```", "```py\n>>> df.columns\nIndex(['alert', 'cdi', 'code', 'detail', 'dmin', 'felt', 'gap', \n       'ids', 'mag', 'magType', 'mmi', 'net', 'nst', 'place', \n       'rms', 'sig', 'sources', 'status', 'time', 'title', \n       'tsunami', 'type', 'types', 'tz', 'updated', 'url'],\n      dtype='object')\n```", "```py\n>>> df.head()\n```", "```py\n>>> df.tail(2)\n```", "```py\n>>> df.dtypes\nalert       object\n...\nmag        float64\nmagType     object\n...\ntime         int64\ntitle       object\ntsunami      int64\n...\ntz         float64\nupdated      int64\nurl         object\ndtype: object\n```", "```py\n>>> df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9332 entries, 0 to 9331\nData columns (total 26 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   alert    59 non-null     object \n ... \n 8   mag      9331 non-null   float64\n 9   magType  9331 non-null   object \n ... \n 18  time     9332 non-null   int64  \n 19  title    9332 non-null   object \n 20  tsunami  9332 non-null   int64  \n ... \n 23  tz       9331 non-null   float64\n 24  updated  9332 non-null   int64  \n 25  url      9332 non-null   object \ndtypes: float64(9), int64(4), object(13)\nmemory usage: 1.9+ MB\n```", "```py\n>>> df.describe()\n```", "```py\n>>> df.describe(include=np.object)\n```", "```py\n>>> df.alert.unique()\narray([nan, 'green', 'red'], dtype=object)\n```", "```py\n>>> df.alert.value_counts()\ngreen    58\nred       1\nName: alert, dtype: int64\n```", "```py\n>>> import pandas as pd\n>>> df = pd.read_csv('data/earthquakes.csv')\n```", "```py\n>>> df.mag\n0       1.35\n1       1.29\n2       3.42\n3       0.44\n4       2.16\n        ... \n9327    0.62\n9328    1.00\n9329    2.40\n9330    1.10\n9331    0.66\nName: mag, Length: 9332, dtype: float64\n```", "```py\n>>> df['mag']\n0       1.35\n1       1.29\n2       3.42\n3       0.44\n4       2.16\n        ... \n9327    0.62\n9328    1.00\n9329    2.40\n9330    1.10\n9331    0.66\nName: mag, Length: 9332, dtype: float64\n```", "```py\n>>> df[['mag', 'title']]\n```", "```py\n>>> df[\n...     ['title', 'time'] \n...     + [col for col in df.columns if col.startswith('mag')]\n... ]\n```", "```py\n>>> [col for col in df.columns if col.startswith('mag')]\n['mag', 'magType']\n```", "```py\n>>> ['title', 'time'] \\\n... + [col for col in df.columns if col.startswith('mag')]\n['title', 'time', 'mag', 'magType']\n```", "```py\n>>> df[\n...     ['title', 'time'] \n...     + [col for col in df.columns if col.startswith('mag')]\n... ]\n```", "```py\n>>> df[100:103]\n```", "```py\n>>> df[['title', 'time']][100:103]\n```", "```py\n>>> df[100:103][['title', 'time']].equals(\n...     df[['title', 'time']][100:103]\n... )\nTrue\n```", "```py\n>>> df[110:113]['title'] = df[110:113]['title'].str.lower()\n/.../book_env/lib/python3.7/[...]:1: SettingWithCopyWarning:  \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"Entry point for launching an IPython kernel.\n```", "```py\n>>> df[110:113]['title']\n110               m 1.1 - 35km s of ester, alaska\n111    m 1.9 - 93km wnw of arctic village, alaska\n112      m 0.9 - 20km wsw of smith valley, nevada\nName: title, dtype: object\n```", "```py\ndf.loc[row_indexer, column_indexer]\n```", "```py\n>>> df.loc[110:112, 'title'] = \\\n...     df.loc[110:112, 'title'].str.lower()\n>>> df.loc[110:112, 'title']\n110               m 1.1 - 35km s of ester, alaska\n111    m 1.9 - 93km wnw of arctic village, alaska\n112      m 0.9 - 20km wsw of smith valley, nevada\nName: title, dtype: object\n```", "```py\n>>> df.loc[:,'title']\n0                  M 1.4 - 9km NE of Aguanga, CA\n1                  M 1.3 - 9km NE of Aguanga, CA\n2                  M 3.4 - 8km NE of Aguanga, CA\n3                  M 0.4 - 9km NE of Aguanga, CA\n4                  M 2.2 - 10km NW of Avenal, CA\n                          ...                   \n9327        M 0.6 - 9km ENE of Mammoth Lakes, CA\n9328                 M 1.0 - 3km W of Julian, CA\n9329    M 2.4 - 35km NNE of Hatillo, Puerto Rico\n9330               M 1.1 - 9km NE of Aguanga, CA\n9331               M 0.7 - 9km NE of Aguanga, CA\nName: title, Length: 9332, dtype: object\n```", "```py\n>>> df.loc[10:15, ['title', 'mag']]\n```", "```py\n>>> df.iloc[10:15, [19, 8]]\n```", "```py\n>>> df.iloc[10:15, 6:10]\n```", "```py\n>>> df.iloc[10:15, 6:10].equals(df.loc[10:14, 'gap':'magType'])\nTrue\n```", "```py\n>>> df.at[10, 'mag']\n0.5\n```", "```py\n>>> df.iat[10, 8]\n0.5\n```", "```py\n>>> df.mag > 2\n0       False\n1       False\n2        True\n3       False\n        ...  \n9328    False\n9329     True\n9330    False\n9331    False\nName: mag, Length: 9332, dtype: bool\n```", "```py\n>>> df[df.mag >= 7.0]\n```", "```py\n>>> df.loc[\n...     df.mag >= 7.0, \n...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n... ]\n```", "```py\n>>> df.loc[\n...     (df.tsunami == 1) & (df.alert == 'red'), \n...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n... ]\n```", "```py\n>>> df.loc[\n...     (df.tsunami == 1) | (df.alert == 'red'), \n...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n... ]\n```", "```py\n>>> df.loc[\n...     (df.place.str.contains('Alaska')) \n...     & (df.alert.notnull()), \n...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n... ]\n```", "```py\ndf.place.str.contains('Alaska')\n```", "```py\ndf.alert.notnull()\n```", "```py\n(df.place.str.contains('Alaska')) & (df.alert.notnull())\n```", "```py\n>>> df.loc[\n...     (df.place.str.contains(r'CA|California$'))\n...     & (df.mag > 3.8),         \n...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n... ]\n```", "```py\n>>> df.loc[\n...     df.mag.between(6.5, 7.5), \n...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n... ]\n```", "```py\n>>> df.loc[\n...     df.magType.isin(['mw', 'mwb']), \n...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n... ]\n```", "```py\n>>> [df.mag.idxmin(), df.mag.idxmax()]\n[2409, 5263]\n```", "```py\n>>> df.loc[\n...     [df.mag.idxmin(), df.mag.idxmax()], \n...     ['alert', 'mag', 'magType', 'title', 'tsunami', 'type']\n... ]\n```", "```py\ndf_to_modify = df.copy()\n```", "```py\n>>> import pandas as pd\n>>> df = pd.read_csv(\n...     'data/earthquakes.csv', \n...     usecols=[\n...         'time', 'title', 'place', 'magType', \n...         'mag', 'alert', 'tsunami'\n...     ]\n... )\n```", "```py\n>>> df['source'] = 'USGS API'\n>>> df.head()\n```", "```py\n>>> df['mag_negative'] = df.mag < 0\n>>> df.head()\n```", "```py\n>>> df.place.str.extract(r', (.*$)')[0].sort_values().unique()\narray(['Afghanistan', 'Alaska', 'Argentina', 'Arizona',\n       'Arkansas', 'Australia', 'Azerbaijan', 'B.C., MX',\n       'Barbuda', 'Bolivia', ..., 'CA', 'California', 'Canada',\n       'Chile', ..., 'East Timor', 'Ecuador', 'Ecuador region',\n       ..., 'Mexico', 'Missouri', 'Montana', 'NV', 'Nevada', \n       ..., 'Yemen', nan], dtype=object)\n```", "```py\n>>> df['parsed_place'] = df.place.str.replace(\n...     r'.* of ', '', regex=True # remove <x> of <x> \n... ).str.replace(\n...     'the ', '' # remove \"the \"\n... ).str.replace(\n...     r'CA$', 'California', regex=True # fix California\n... ).str.replace(\n...     r'NV$', 'Nevada', regex=True # fix Nevada\n... ).str.replace(\n...     r'MX$', 'Mexico', regex=True # fix Mexico\n... ).str.replace(\n...     r' region$', '', regex=True # fix \" region\" endings\n... ).str.replace(\n...     'northern ', '' # remove \"northern \"\n... ).str.replace(\n...     'Fiji Islands', 'Fiji' # line up the Fiji places\n... ).str.replace( # remove anything else extraneous from start \n...     r'^.*, ', '', regex=True \n... ).str.strip() # remove any extra spaces\n```", "```py\n>>> df.parsed_place.sort_values().unique()\narray([..., 'California', 'Canada', 'Carlsberg Ridge', ...,\n       'Dominican Republic', 'East Timor', 'Ecuador',\n       'El Salvador', 'Fiji', 'Greece', ...,\n       'Mexico', 'Mid-Indian Ridge', 'Missouri', 'Montana',\n       'Nevada', 'New Caledonia', ...,\n       'South Georgia and South Sandwich Islands', \n       'South Sandwich Islands', ..., 'Yemen'], dtype=object)\n```", "```py\n>>> df.assign(\n...     in_ca=df.parsed_place.str.endswith('California'), \n...     in_alaska=df.parsed_place.str.endswith('Alaska')\n... ).sample(5, random_state=0)\n```", "```py\n>>> df.assign(\n...     in_ca=df.parsed_place == 'California', \n...     in_alaska=df.parsed_place == 'Alaska',\n...     neither=lambda x: ~x.in_ca & ~x.in_alaska\n... ).sample(5, random_state=0)\n```", "```py\n>>> tsunami = df[df.tsunami == 1]\n>>> no_tsunami = df[df.tsunami == 0]\n>>> tsunami.shape, no_tsunami.shape\n((61, 10), (9271, 10))\n```", "```py\n>>> pd.concat([tsunami, no_tsunami]).shape\n(9332, 10) # 61 rows + 9271 rows\n```", "```py\n>>> tsunami.append(no_tsunami).shape\n(9332, 10) # 61 rows + 9271 rows\n```", "```py\n>>> additional_columns = pd.read_csv(\n...     'data/earthquakes.csv', usecols=['tz', 'felt', 'ids']\n... )\n>>> pd.concat([df.head(2), additional_columns.head(2)], axis=1)\n```", "```py\n>>> additional_columns = pd.read_csv(\n...     'data/earthquakes.csv',\n...     usecols=['tz', 'felt', 'ids', 'time'], \n...     index_col='time'\n... )\n>>> pd.concat([df.head(2), additional_columns.head(2)], axis=1)\n```", "```py\n>>> pd.concat(\n...     [\n...         tsunami.head(2),\n...         no_tsunami.head(2).assign(type='earthquake')\n...     ], \n...     join='inner'\n... )\n```", "```py\n>>> pd.concat(\n...     [\n...         tsunami.head(2), \n...         no_tsunami.head(2).assign(type='earthquake')\n...     ],\n...     join='inner', ignore_index=True\n... )\n```", "```py\n>>> del df['source']\n>>> df.columns\nIndex(['alert', 'mag', 'magType', 'place', 'time', 'title', \n       'tsunami', 'mag_negative', 'parsed_place'],\n      dtype='object')\n```", "```py\ntry:\n    del df['source']\nexcept KeyError:\n    pass # handle the error here\n```", "```py\n>>> mag_negative = df.pop('mag_negative')\n>>> df.columns\nIndex(['alert', 'mag', 'magType', 'place', 'time', 'title', \n       'tsunami', 'parsed_place'],\n      dtype='object')\n```", "```py\n>>> mag_negative.value_counts()\nFalse    8841\nTrue      491\nName: mag_negative, dtype: int64\n```", "```py\n>>> df[mag_negative].head()\n```", "```py\n>>> df.drop([0, 1]).head(2)\n```", "```py\n>>> cols_to_drop = [\n...     col for col in df.columns\n...     if col not in [\n...         'alert', 'mag', 'title', 'time', 'tsunami'\n...     ]\n... ]\n>>> df.drop(columns=cols_to_drop).head()\n```", "```py\n>>> df.drop(columns=cols_to_drop).equals(\n...     df.drop(cols_to_drop, axis=1)\n... )\nTrue\n```", "```py\n>>> df.drop(columns=cols_to_drop, inplace=True)\n>>> df.head()\n```"]