- en: Chapter 7. Statistical Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a dataset with pandas and matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with statistical hypothesis testing – a simple z-test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with Bayesian methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the correlation between two variables with a contingency table and
    a chi-squared test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting a probability distribution to data with the maximum likelihood method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating a probability distribution nonparametrically with a kernel density
    estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting a Bayesian model by sampling from a posterior distribution with a Markov
    chain Monte Carlo method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing data with the R programming language in the IPython notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we reviewed technical aspects of high-performance
    interactive computing in Python. We now begin the second part of this book by
    illustrating a variety of scientific questions that can be tackled with Python.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we introduce statistical methods for data analysis. In addition
    to covering statistical packages such as pandas, statsmodels, and PyMC, we will
    explain the basics of the underlying mathematical principles. Therefore, this
    chapter will be most profitable if you have basic experience with probability
    theory and calculus.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter, [Chapter 8](ch08.html "Chapter 8. Machine Learning"), *Machine
    Learning*, is closely related; the underlying mathematics is very similar, but
    the goals are slightly different. In this chapter, we show how to gain insight
    into real-world data and how to make informed decisions in the presence of uncertainty.
    In the next chapter, the goal is to *learn from data*, that is, to generalize
    and to predict outcomes from partial observations.
  prefs: []
  type: TYPE_NORMAL
- en: In this introduction, we will give a broad, high-level overview of the methods
    we will see in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What is statistical data analysis?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of statistical data analysis is to understand a complex, real-world
    phenomenon from partial and uncertain observations. The uncertainty in the data
    results in uncertainty in the knowledge we get about the phenomenon. A major goal
    of the theory is to *quantify* this uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to make the distinction between the mathematical theory underlying
    statistical data analysis, and the decisions made after conducting an analysis.
    The former is perfectly rigorous; perhaps surprisingly, mathematicians were able
    to build an exact mathematical framework to deal with uncertainty. Nevertheless,
    there is a subjective part in the way statistical analysis yields actual human
    decisions. Understanding the risk and the uncertainty behind statistical results
    is critical in the decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will see the basic notions, principles, and theories behind
    statistical data analysis, covering in particular how to make decisions with a
    quantified risk. Of course, we will always show how to implement these methods
    with Python.
  prefs: []
  type: TYPE_NORMAL
- en: A bit of vocabulary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many terms that need introduction before we get started with the recipes.
    These notions allow us to classify statistical techniques within multiple dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration, inference, decision, and prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Exploratory methods** allow us to get a preliminary look at a dataset through
    basic statistical aggregates and interactive visualization. We covered these basic
    methods in the first chapter of this book and in the book *Learning IPython for
    Interactive Computing and Data Visualization*, *Packt Publishing*. The first recipe
    of this chapter, *Exploring a dataset with pandas and matplotlib*, shows another
    example.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical inference** consists of getting information about an unknown
    process through partial and uncertain observations. In particular, **estimation**
    entails obtaining approximate quantities for the mathematical variables describing
    this process. Three recipes in this chapter deal with statistical inference:'
  prefs: []
  type: TYPE_NORMAL
- en: The *Fitting a probability distribution to data with the maximum likelihood
    method* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Estimating a probability distribution nonparametrically with a kernel density
    estimation* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Fitting a Bayesian model by sampling from a posterior distribution with
    a Markov chain Monte Carlo method* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision theory** allows us to make decisions about an unknown process from
    random observations, with a controlled risk. The following two recipes show how
    to make statistical decisions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Getting started with statistical hypothesis testing: a simple z-test*
    recipe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Estimating the correlation between two variables with a contingency table
    and a chi-squared test* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction** consists of learning from data, that is, predicting the outcomes
    of a random process based on a limited number of observations. This is the topic
    of the next chapter, [Chapter 8](ch08.html "Chapter 8. Machine Learning"), *Machine
    Learning*.'
  prefs: []
  type: TYPE_NORMAL
- en: Univariate and multivariate methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In most cases, you can consider two dimensions in your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Observations** (or **samples**, for machine learning people)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variables** (or **features**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, observations are independent realizations of the same random process.
    Each observation is made of one or several variables. Most of the time, variables
    are either numbers, or elements belonging to a finite set (that is, taking a finite
    number of values). The first step in an analysis is to understand what your observations
    and variables are.
  prefs: []
  type: TYPE_NORMAL
- en: Your problem is **univariate** if you have one variable. It is **bivariate**
    if you have two variables and **multivariate** if you have at least two variables.
    Univariate methods are typically simpler. That being said, univariate methods
    may be used on multivariate data, using one dimension at a time. Although interactions
    between variables cannot be explored in that case, it is often an interesting
    first approach.
  prefs: []
  type: TYPE_NORMAL
- en: Frequentist and Bayesian methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are at least two different ways of considering uncertainty, resulting
    in two different classes of methods for inference, decision, and other statistical
    questions. These are called **frequentist and** **Bayesian methods**. Some people
    prefer frequentist methods, while others prefer Bayesian methods.
  prefs: []
  type: TYPE_NORMAL
- en: Frequentists interpret a probability as a **statistical average** across many
    independent realizations (law of large numbers). Bayesians interpret it as a **degree
    of belief** (no need for many realizations). The Bayesian interpretation is very
    useful when only a single trial is considered. In addition, Bayesian theory takes
    into account our **prior knowledge** about a random process. This prior probability
    distribution is updated into a posterior distribution as we get more and more
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Both frequentist and Bayesian methods have their advantages and disadvantages.
    For instance, one could say that frequentist methods might be easier to apply
    than Bayesian methods, but more difficult to interpret. For classic misuses of
    frequentist methods, see [www.refsmmat.com/statistics/](http://www.refsmmat.com/statistics/).
  prefs: []
  type: TYPE_NORMAL
- en: In any case, if you are a beginner in statistical data analysis, you probably
    want to learn the basics of both approaches before choosing sides. This chapter
    introduces you to both types of methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes are exclusively Bayesian:'
  prefs: []
  type: TYPE_NORMAL
- en: The *Getting started with Bayesian methods* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Fitting a Bayesian model by sampling from a posterior distribution with
    a Markov chain Monte Carlo method* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jake Vanderplas has written several blog posts about frequentism and Bayesianism,
    with examples in Python. The first post of the series is available at [http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/).
  prefs: []
  type: TYPE_NORMAL
- en: Parametric and nonparametric inference methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many cases, you base your analysis on a **probabilistic model**. This model
    describes how your data is generated. A probabilistic model has no reality; it
    is only a mathematical object that guides you in your analysis. A good model can
    be helpful, whereas a bad model may misguide you.
  prefs: []
  type: TYPE_NORMAL
- en: With a **parametric method**, you assume that your model belongs to a known
    family of probability distributions. The model has one or multiple numerical *parameters*
    that you can *estimate*.
  prefs: []
  type: TYPE_NORMAL
- en: With a **nonparametric model**, you do not make such an assumption in your model.
    This gives you more flexibility. However, these methods are typically more complicated
    to implement and to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes are parametric and nonparametric, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: The *Fitting a probability distribution to data with the maximum likelihood
    method* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Estimating a probability distribution nonparametrically with a kernel density
    estimation* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This chapter only gives you an idea of the wide range of possibilities that
    Python offers for statistical data analysis. You can find many books and online
    courses that cover statistical methods in much greater detail, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Statistics on WikiBooks at [http://en.wikibooks.org/wiki/Statistics](http://en.wikibooks.org/wiki/Statistics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Free statistical textbooks available at [http://stats.stackexchange.com/questions/170/free-statistical-textbooks](http://stats.stackexchange.com/questions/170/free-statistical-textbooks)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring a dataset with pandas and matplotlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this first recipe, we will show how to conduct a preliminary analysis of
    a dataset with pandas. This is typically the first step after getting access to
    the data. pandas lets us load the data very easily, explore the variables, and
    make basic plots with matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: We will take a look at a dataset containing all ATP matches played by four tennis
    players until 2012\. Here, we will focus on Roger Federer.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the *Tennis* dataset from the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data),
    and extract it to the current directory.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import NumPy, pandas, and matplotlib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The dataset is a CSV file, that is, a text file with comma-separated values.
    pandas lets us load this file with a single function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can have a first look at this dataset by just displaying it in the IPython
    notebook:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are many columns. Each row corresponds to a match played by Roger Federer.
    Let''s add a Boolean variable indicating whether he has won the match or not.
    The `tail()` method displays the last rows of the column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`df[''win'']` is a `Series` object. It is very similar to a NumPy array, except
    that each value has an index (here, the match index). This object has a few standard
    statistical functions. For example, let''s look at the proportion of matches won:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are going to look at the evolution of some variables across time. The
    `df[''start date'']` field contains the start date of the tournament as a string.
    We can convert the type to a date type using the `pd.to_datetime()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are now looking at the proportion of double faults in each match (taking
    into account that there are logically more double faults in longer matches!).
    This number is an indicator of the player's state of mind, his level of self-confidence,
    his willingness to take risks while serving, and other parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can use the `head()` and `tail()` methods to take a look at the beginning
    and the end of the column, and `describe()` to get summary statistics. In particular,
    let's note that some rows have NaN values (that is, the number of double faults
    is not available for all matches).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A very powerful feature in pandas is `groupby()`. This function allows us to
    group together rows that have the same value in a particular column. Then, we
    can aggregate this group by value to compute statistics in each group. For instance,
    here is how we can get the proportion of wins as a function of the tournament''s
    surface:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are going to display the proportion of double faults as a function
    of the tournament date, as well as the yearly average. To do this, we also use
    `groupby()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`gb` is a `GroupBy` instance. It is similar to a `DataFrame` object, but there
    are multiple rows per group (all matches played in each year). We can aggregate
    these rows using the `mean()` operation. We use the matplotlib `plot_date()` function
    because the x-axis contains dates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: pandas is an excellent tool for data wrangling and exploratory analysis. pandas
    accepts all sorts of formats (text-based, and binary files) and it lets us manipulate
    tables in many ways. In particular, the `groupby()` function is extremely powerful.
    This library is covered in much greater detail in a book by Wes McKinney, *Python
    for Data Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: What we covered here is only the first step in a data-analysis process. We need
    more advanced statistical methods to obtain reliable information about the underlying
    phenomena, make decisions and predictions, and so on. This is the topic of the
    following recipes.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, more complex datasets demand more sophisticated analysis methods.
    For example, digital recordings, images, sounds, and videos require specific signal
    processing treatments before we can apply statistical techniques. These questions
    will be covered in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with statistical hypothesis testing – a simple z-test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Statistical hypothesis testing** allows us to make decisions in the presence
    of incomplete data. By definition, these decisions are uncertain. Statisticians
    have developed rigorous methods to evaluate this risk. Nevertheless, some subjectivity
    is always involved in the decision-making process. The theory is just a tool that
    helps us make decisions in an uncertain world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we introduce the most basic ideas behind statistical hypothesis testing.
    We will follow an extremely simple example: coin tossing. More precisely, we will
    show how to perform a **z-test**, and we will briefly explain the mathematical
    ideas underlying it. This kind of method (also called the *frequentist method*),
    although widely used in science, is subject to many criticisms. We will show later
    a more modern approach based on Bayesian theory. It is very helpful to understand
    both approaches, because many studies and publications still follow frequentist
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have a basic knowledge of probability theory for this recipe (random
    variables, distributions, expectancy, variance, central limit theorem, and so
    on).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many frequentist methods for hypothesis testing roughly involve the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Writing down the hypotheses, notably the **null hypothesis**, which is the *opposite*
    of the hypothesis we want to prove (with a certain degree of confidence).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing a **test statistic**, a mathematical formula depending on the test
    type, the model, the hypotheses, and the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the computed value to accept the hypothesis, reject it, or fail to conclude.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, we flip a coin *n* times and we observe *h* heads. We want to know whether
    the coin is fair (null hypothesis). This example is extremely simple yet quite
    useful for pedagogical purposes. Besides, it is the basis of many more complex
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: We denote the Bernoulli distribution by *B(q)* with the unknown parameter *q*.
    You can refer to [http://en.wikipedia.org/wiki/Bernoulli_distribution](http://en.wikipedia.org/wiki/Bernoulli_distribution)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Bernoulli variable is:'
  prefs: []
  type: TYPE_NORMAL
- en: 0 (tail) with probability *1-q*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 (head) with probability *q*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the steps required to conduct a simple statistical *z*-test:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose that after *n=100* flips, we get *h=61* heads. We choose a significance
    level of 0.05: is the coin fair or not? Our null hypothesis is: *the coin is fair
    (q = 1/2)*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's compute the **z-score**, which is defined by the following formula (`xbar`
    is the estimated average of the distribution). We will explain this formula in
    the next section, *How it works…*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, from the z-score, we can compute the p-value as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This p-value is less than 0.05, so we reject the null hypothesis and conclude
    that *the coin is probably not fair*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The coin tossing experiment is modeled as a sequence of *n* independent random
    variables ![How it works...](img/4818OS_07_46.jpg) following the Bernoulli distribution
    *B(q)*. Each *x[i]* represents one coin flip. After our experiment, we get actual
    values (samples) for these variables. A different notation is sometimes used to
    distinguish between the random variables (probabilistic objects) and the actual
    values (samples).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following formula gives the **sample mean** (proportion of heads here):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Knowing the expectancy ![How it works...](img/4818OS_07_39.jpg) and variance
    ![How it works...](img/4818OS_07_40.jpg) of the distribution *B(q)*, we compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The z-test is the normalized version of ![How it works...](img/4818OS_07_41.jpg)
    (we remove its mean, and divide by the standard deviation, thus we get a variable
    with mean 0 and standard deviation 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Under the null hypothesis, what is the probability of obtaining a z-test higher
    than some quantity *z[0]*? This probability is called the (two-sided) **p-value**.
    According to the central limit theorem, the z-test approximately follows a standard
    Gaussian distribution *N(0,1)* for large *n*, so we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram illustrates the z-score and the p-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of the z-score and the p-value
  prefs: []
  type: TYPE_NORMAL
- en: 'In this formula, ![How it works...](img/4818OS_07_42.jpg) is the **cumulative
    distribution function** of a standard normal distribution. In SciPy, we can get
    it with `scipy.stats.norm.cdf`. So, given the z-test computed from the data, we
    compute the p-value: the probability of observing a z-test more extreme than the
    observed test, under the null hypothesis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the p-value is less than five percent (a frequently-chosen significance
    level, for arbitrary and historical reasons), we conclude that either:'
  prefs: []
  type: TYPE_NORMAL
- en: The null hypothesis is false, thus we conclude that the coin is unfair.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The null hypothesis is true, and it's just bad luck if we obtained these values.
    We cannot make a conclusion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cannot disambiguate between these two options in this framework, but typically
    the first option is chosen. We hit the limits of frequentist statistics, although
    there are ways to mitigate this problem (for example, by conducting several independent
    studies and looking at all of their conclusions).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many statistical tests following this pattern exist. Reviewing all those tests
    is largely beyond the scope of this book, but you can take a look at the reference
    at [http://en.wikipedia.org/wiki/Statistical_hypothesis_testing](http://en.wikipedia.org/wiki/Statistical_hypothesis_testing).
  prefs: []
  type: TYPE_NORMAL
- en: As a p-value is not easy to interpret, it can lead to wrong conclusions, even
    in peer-reviewed scientific publications. For an in-depth treatment of the subject,
    see [www.refsmmat.com/statistics/](http://www.refsmmat.com/statistics/).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Getting started with Bayesian methods* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with Bayesian methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last recipe, we used a frequentist method to test a hypothesis on incomplete
    data. Here, we will see an alternative approach based on **Bayesian theory**.
    The main idea is to consider that *unknown parameters are random variables*, just
    like the variables describing the experiment. Prior knowledge about the parameters
    is integrated into the model. This knowledge is updated as more and more data
    is observed.
  prefs: []
  type: TYPE_NORMAL
- en: Frequentists and Bayesians interpret probabilities differently. Frequentists
    interpret a probability as a limit of frequencies when the number of samples tends
    to infinity. Bayesians interpret it as a belief; this belief is updated as more
    and more data is observed.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we revisit the previous coin flipping example with a Bayesian approach.
    This example is sufficiently simple to permit an analytical treatment. In general,
    as we will see later in this chapter, analytical results cannot be obtained and
    numerical methods become essential.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a math-heavy recipe. Knowledge of basic probability theory (random variables,
    distributions, Bayes formula) and calculus (derivatives, integrals) is recommended.
    We use the same notations as in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let *q* be the probability of obtaining a head. Whereas *q* was just a fixed
    number in the previous recipe, we consider here that it is a **random variable**.
    Initially, this variable follows a distribution called the **prior probability
    distribution**. It represents our knowledge about *q* before we start flipping
    the coin. We will update this distribution after each trial (posterior distribution).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we assume that *q* is a *uniform* random variable in the interval [0,
    1]. That''s our prior distribution: for all *q*, *P(q)=1*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we flip our coin *n* times. We note *x[i]* the outcome of the *i*th flip
    (0 for tail and 1 for head).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the probability distribution of *q* knowing the observations *x[i]*?
    **Bayes' theorem** allows us to compute the **posterior distribution** analytically
    (see the next section for the mathematical details):![How to do it...](img/4818OS_07_07.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We define the posterior distribution according to the previous mathematical
    formula. We remark that this expression is *(n+1)* times the **probability mass
    function** (**PMF**) of the binomial distribution, which is directly available
    in `scipy.stats`. (For more information on Binomial distribution, refer to [http://en.wikipedia.org/wiki/Binomial_distribution](http://en.wikipedia.org/wiki/Binomial_distribution).)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s plot this distribution for an observation of *h=61* heads and *n=100*
    total flips:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: This curve represents our belief about the parameter *q* after we have observed
    61 heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explain Bayes' theorem, and we give the mathematical details
    underlying this example.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a very general idea in data science that consists of explaining data
    with a mathematical model. This is formalized with a one-way process, *model →
    data*.
  prefs: []
  type: TYPE_NORMAL
- en: Once this process is formalized, the task of the data scientist is to exploit
    the data to recover information about the model. In other words, we want to *invert*
    the original process and get *data → model*.
  prefs: []
  type: TYPE_NORMAL
- en: In a probabilistic setting, the direct process is represented as a **conditional
    probability distribution** *P(data|model)*. This is the probability of observing
    the data when the model is entirely specified.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the inverse process is *P(model|data)*. It gives us information about
    the model (what we're looking for), knowing the observations (what we have).
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes'' theorem is at the core of a general framework for inverting a probabilistic
    process of *model → data*. It can be stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayes'' theorem](img/4818OS_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This equation gives us information about our model, knowing the observed data.
    Bayes' equation is widely used in signal processing, statistics, machine learning,
    inverse problems, and in many other scientific applications.
  prefs: []
  type: TYPE_NORMAL
- en: In Bayes' equation, *P(model)* reflects our prior knowledge about the model.
    Also, *P(data)* is the distribution of the data. It is generally expressed as
    an integral of *P(data|model)P(model)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, Bayes'' equation gives us a general roadmap for data inference:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify a mathematical model for the direct process *model → data* (the *P(data|model)*
    term).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify a prior probability distribution for the model (*P(model) term*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform analytical or numerical calculations to solve this equation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computation of the posterior distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this recipe''s example, we found the posterior distribution with the following
    equation (deriving directly from Bayes'' theorem):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computation of the posterior distribution](img/4818OS_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Knowing that the *x[i]* are independent, we get (*h* being the number of heads):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computation of the posterior distribution](img/4818OS_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, we can compute analytically the following integral (using an integration
    by parts and an induction):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computation of the posterior distribution](img/4818OS_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computation of the posterior distribution](img/4818OS_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Maximum a posteriori estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can get a point estimate from the posterior distribution. For example, the
    **maximum a posteriori** (**MAP**) estimation consists of considering the *maximum*
    of the posterior distribution as an estimate for *q*. We can find this maximum
    analytically or numerically. For more information on MAP, refer to [http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation](http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can get this estimate analytically by deriving the posterior distribution
    with respect to *q*. We get (assuming *1* ![Maximum a posteriori estimation](img/4818OS_08_40.jpg)
    *h* ![Maximum a posteriori estimation](img/4818OS_08_40.jpg) * n-1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum a posteriori estimation](img/4818OS_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This expression is equal to zero when *q = h/n*. This is the MAP estimate of
    the parameter *q*. This value happens to be the proportion of heads obtained in
    the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we showed a few basic notions in Bayesian theory. We illustrated
    them with a simple example. The fact that we were able to derive the posterior
    distribution analytically is not very common in real-world applications. This
    example is nevertheless informative because it explains the core mathematical
    ideas behind the complex numerical methods we will see later.
  prefs: []
  type: TYPE_NORMAL
- en: Credible interval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The posterior distribution indicates the plausible values for *q* given the
    observations. We could use it to derive a **credible interval**, likely to contain
    the actual value. Credible intervals are the Bayesian analog to confidence intervals
    in frequentist statistics. For more information on credible intervals, refer to
    [http://en.wikipedia.org/wiki/Credible_interval](http://en.wikipedia.org/wiki/Credible_interval).
  prefs: []
  type: TYPE_NORMAL
- en: Conjugate distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, the prior and posterior distributions are **conjugate**, meaning
    that they belong to the same family (the beta distribution). For this reason,
    we were able to compute the posterior distribution analytically. You will find
    more details about conjugate distributions at [http://en.wikipedia.org/wiki/Conjugate_prior](http://en.wikipedia.org/wiki/Conjugate_prior).
  prefs: []
  type: TYPE_NORMAL
- en: Non-informative (objective) prior distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We chose a uniform distribution as prior distribution for the unknown parameter
    *q*. It is a simple choice and it leads to tractable computations. It reflects
    the intuitive fact that we do not favor any particular value a priori. However,
    there are rigorous ways of choosing completely uninformative priors (see [http://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors](http://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors)).
    An example is the Jeffreys prior, based on the idea that the prior distribution
    should not depend on the parameterization of the parameters. For more information
    on Jeffreys prior, refer to [http://en.wikipedia.org/wiki/Jeffreys_prior](http://en.wikipedia.org/wiki/Jeffreys_prior).
    In our example, the Jeffreys prior is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-informative (objective) prior distributions](img/4818OS_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Fitting a Bayesian model by sampling from a posterior distribution with
    a Markov chain Monte Carlo method* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the correlation between two variables with a contingency table and
    a chi-squared test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whereas univariate methods deal with single-variable observations, multivariate
    methods consider observations with several features. Multivariate datasets allow
    the study of *relations* between variables, more particularly their correlation
    or lack thereof (that is, independence).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will take a look at the same tennis dataset as in the first
    recipe of this chapter. Following a frequentist approach, we will estimate the
    correlation between the number of aces and the proportion of points won by a tennis
    player.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the *Tennis* dataset on the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data),
    and extract it in the current directory.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import NumPy, pandas, SciPy.stats, and matplotlib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the dataset corresponding to Roger Federer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each row corresponds to a match, and the 70 columns contain many player characteristics
    during that match:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we only look at the proportion of points won, and the (relative) number
    of aces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: If the two variables were independent, we would not see any trend in the cloud
    of points. On this plot, it is a bit hard to tell. Let's use pandas to compute
    a coefficient correlation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We create a new `DataFrame` object with only these fields (note that this step
    is not compulsory). We also remove the rows where one field is missing (using
    `dropna()`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s compute the Pearson''s correlation coefficient between the relative
    number of aces in the match, and the number of points won:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A correlation of ~0.26 seems to indicate a positive correlation between our
    two variables. In other words, the more aces in a match, the more points the player
    wins (which is not very surprising!).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, to determine if there is a *statistically significant* correlation between
    the variables, we use a **chi-squared test** of the independence of variables
    in a **contingency table**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we binarize our variables. Here, the value corresponding to the number
    of aces is `True` if the player is serving more aces than usual in a match, and
    `False` otherwise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create a contingency table, with the frequencies of all four possibilities
    (True and True, True and False, and so on):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we compute the chi-squared test statistic and the associated p-value.
    The null hypothesis is the independence between the variables. SciPy implements
    this test in `scipy.stats.chi2_contingency`, which returns several objects. We''re
    interested in the second result, which is the p-value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The p-value is much lower than 0.05, so we reject the null hypothesis and conclude
    that there is a statistically significant correlation between the proportion of
    aces and the proportion of points won in a match (for Roger Federer!).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As always, correlation does not imply causation. Here, it is likely that external
    factors influence both variables. See [http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation](http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We give here a few details about the statistical concepts used in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Pearson's correlation coefficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pearson''s correlation coefficient measures the linear correlation between
    two random variables, *X* and *Y*. It is a normalized version of the covariance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pearson''s correlation coefficient](img/4818OS_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It can be estimated by substituting, in this formula, the expectancy with the
    sample mean, and the variance with the sample variance. More details about its
    inference can be found at [http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient).
  prefs: []
  type: TYPE_NORMAL
- en: Contingency table and chi-squared test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The contingency table contains the frequencies *O[ij]* of all combinations
    of outcomes, when there are multiple random variables that can take a finite number
    of values. Under the null hypothesis of independence, we can compute the *expected*
    frequencies *E[ij]*, based on the marginal sums (sums in each row). The chi-squared
    statistic, by definition, is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Contingency table and chi-squared test](img/4818OS_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When there are sufficiently many observations, this variable approximately follows
    a chi-squared distribution (the distribution of the sum of normal variables squared).
    Once we get the p-value, as explained in the *Getting started with statistical
    hypothesis testing – a simple z-test* recipe, we can reject or accept the null
    hypothesis of independence. Then, we can conclude (or not) that there exists a
    significant correlation between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many other sorts of chi-squared tests, that is, tests where the test
    statistic follows a chi-squared distribution. These tests are widely used for
    testing the goodness-of-fit of a distribution, or testing the independence of
    variables. More information can be found in the following pages:'
  prefs: []
  type: TYPE_NORMAL
- en: Chi2 test in SciPy documentation available at [http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contingency table introduced at [http://en.wikipedia.org/wiki/Contingency_table](http://en.wikipedia.org/wiki/Contingency_table)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chi-squared test introduced at [http://en.wikipedia.org/wiki/Pearson's_chi-squared_test](http://en.wikipedia.org/wiki/Pearson's_chi-squared_test)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Getting started with statistical hypothesis testing – a simple z-test*
    recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting a probability distribution to data with the maximum likelihood method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A good way to explain a dataset is to apply a probabilistic model to it. Finding
    an adequate model can be a job in its own. Once a model is chosen, it is necessary
    to compare it to the data. This is what statistical estimation is about. In this
    recipe, we apply the **maximum likelihood method** on a dataset of survival times
    after heart transplant (1967-1974 study).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual in this chapter, a background in probability theory and real analysis
    is recommended. In addition, you need the statsmodels package to retrieve the
    test dataset. For more information on statsmodels, refer to [http://statsmodels.sourceforge.net](http://statsmodels.sourceforge.net).
    On Anaconda, you can install statsmodel with the `conda install statsmodels` command.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'statsmodels is a Python package for conducting statistical data analyses. It
    also contains real-world datasets that we can use when experimenting with new
    methods. Here, we load the *heart* dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at this `DataFrame`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This dataset contains censored and uncensored data: a censor of 0 means that
    the patient was alive at the end of the study, and thus we don''t know the exact
    survival time. We only know that the patient survived *at least* the indicated
    number of days. For simplicity here, we only keep uncensored data (we thereby
    introduce a bias toward patients that did not survive very long after their transplant):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at the data graphically, by plotting the raw survival data
    and the histogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: We observe that the histogram is decreasing very rapidly. Fortunately, the survival
    rates today are much higher (~70 percent after 5 years). Let's try to fit an exponential
    distribution (more information on the exponential distribution is available at
    [http://en.wikipedia.org/wiki/Exponential_distribution](http://en.wikipedia.org/wiki/Exponential_distribution))
    to the data. According to this model, *S* (number of days of survival) is an exponential
    random variable with the parameter ![How to do it...](img/4818OS_07_43.jpg), and
    the observations *s[i]* are sampled from this distribution. Let the sample mean
    be:![How to do it...](img/4818OS_07_20.jpg)The likelihood function of an exponential
    distribution is as follows, by definition (see proof in the next section):![How
    to do it...](img/4818OS_07_21.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **maximum likelihood estimate** for the rate parameter is, by definition,
    the value ![How to do it...](img/4818OS_07_43.jpg) that maximizes the likelihood
    function. In other words, it is the parameter that maximizes the probability of
    observing the data, assuming that the observations are sampled from an exponential
    distribution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here, it can be shown that the likelihood function has a maximum value when
    ![How to do it...](img/4818OS_07_44.jpg), which is the maximum likelihood estimate
    for the rate parameter. Let''s compute this parameter numerically:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To compare the fitted exponential distribution to the data, we first need to
    generate linearly spaced values for the x-axis (days):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can obtain the probability density function of the exponential distribution
    with SciPy. The parameter is the scale, the inverse of the estimated rate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s plot the histogram and the obtained distribution. We need to rescale
    the theoretical distribution to the histogram (depending on the bin size and the
    total number of data points):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The fit is far from perfect. We were able to find an analytical formula for
    the maximum likelihood estimate here. In more complex situations, that is not
    always possible. Thus we may need to resort to numerical methods. SciPy actually
    integrates numerical maximum likelihood routines for a large number of distributions.
    Here, we use this other method to estimate the parameter of the exponential distribution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can use these parameters to perform a **Kolmogorov-Smirnov test**, which
    assesses the goodness of fit of the distribution with respect to the data. This
    test is based on a distance between the **empirical distribution function** of
    the data and the **cumulative distribution function** (**CDF**) of the reference
    distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The second output value is the p-value. Here, it is very low: the null hypothesis
    (stating that the observed data stems from an exponential distribution with a
    maximum likelihood rate parameter) can be rejected with high confidence. Let''s
    try another distribution, the **Birnbaum-Sanders distribution**, which is typically
    used to model failure times. (More information on the Birnbaum-Sanders distribution
    is available at [http://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution](http://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution).)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This time, the p-value is 0.07, so that we would not reject the null hypothesis
    with a five percent confidence level. When plotting the resulting distribution,
    we observe a better fit than with the exponential distribution:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we give the calculations leading to the maximum likelihood estimation
    of the rate parameter for an exponential distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![How it works...](img/4818OS_07_45.jpg) is the sample mean. In more complex
    situations, we would require numerical optimization methods in which the principle
    is to maximize the likelihood function using a standard numerical optimization
    algorithm (see [Chapter 9](ch09.html "Chapter 9. Numerical Optimization"), *Numerical
    Optimization*).
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the maximum of this function, let''s compute its derivative function
    with respect to ![How it works...](img/4818OS_07_43.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The root of this derivative is therefore ![How it works...](img/4818OS_07_44.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood on Wikipedia, available at [http://en.wikipedia.org/wiki/Maximum_likelihood](http://en.wikipedia.org/wiki/Maximum_likelihood)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kolmogorov-Smirnov test on Wikipedia, available at [http://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test](http://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodness of fit at [http://en.wikipedia.org/wiki/Goodness_of_fit](http://en.wikipedia.org/wiki/Goodness_of_fit)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The maximum likelihood method is *parametric*: the model belongs to a prespecified
    parametric family of distributions. In the next recipe, we will see a nonparametric
    kernel-based method.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Estimating a probability distribution nonparametrically with a kernel density
    estimation* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating a probability distribution nonparametrically with a kernel density
    estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we applied a **parametric estimation method**. We had
    a statistical model (the exponential distribution) describing our data, and we
    estimated a single parameter (the rate of the distribution). **Nonparametric estimation**
    deals with statistical models that do not belong to a known family of distributions.
    The parameter space is then *infinite-dimensional* instead of finite-dimensional
    (that is, we estimate *functions* rather than *numbers*).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use a **kernel density estimation** (**KDE**) to estimate the density
    of probability of a spatial distribution. We look at the geographical locations
    of tropical cyclones from 1848 to 2013, based on data provided by the NOAA, the
    US' National Oceanic and Atmospheric Administration.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the *Storms* dataset from the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data),
    and extract it in the current directory. The data was obtained from [www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data](http://www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data).
  prefs: []
  type: TYPE_NORMAL
- en: You also need matplotlib's toolkit **basemap**, available at [http://matplotlib.org/basemap/](http://matplotlib.org/basemap/).
    With Anaconda, you can install it with conda install basemap. Windows users can
    also find an installer at [www.lfd.uci.edu/~gohlke/pythonlibs/](http://www.lfd.uci.edu/~gohlke/pythonlibs/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import the usual packages. The kernel density estimation with a Gaussian
    kernel is implemented in SciPy.stats:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s open the data with pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The dataset contains information about most storms since 1848\. A single storm
    may appear multiple times across several consecutive days.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use pandas'' `groupby()` function to obtain the average location of every
    storm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We display the storms on a map with basemap. This toolkit allows us to easily
    project the geographical coordinates on the map.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'To perform the kernel density estimation, we stack the `x` and `y` coordinates
    of the storms into a `(2, N)` array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `gaussian_kde()` routine returned a Python function. To see the results
    on a map, we need to evaluate this function on a 2D grid spanning the entire map.
    We create this grid with `meshgrid()`, and we pass the `x` and `y` values to the
    `kde` function. `kde` accepts a `(2, N)` array as input, requiring us to tweak
    the shape of the array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we display the estimated density with `imshow()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **kernel density estimator** of a set of *n* points *{x[i]}* is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *h>0* is a scaling parameter (the **bandwidth**) and *K(u)* is the **kernel**,
    a symmetric function that integrates to 1\. This estimator is to be compared with
    a classical histogram, where the kernel would be a *top-hat* function (a rectangle
    function taking its values in *{0,1}*), but the blocks would be located on a regular
    grid instead of the data points. For more information on kernel density estimator,
    refer to [http://en.wikipedia.org/wiki/Kernel_density_estimation](http://en.wikipedia.org/wiki/Kernel_density_estimation).
  prefs: []
  type: TYPE_NORMAL
- en: Multiple kernels can be chosen. Here, we chose a **Gaussian kernel**, so that
    the KDE is the superposition of Gaussian functions centered on all the data points.
    It is an estimation of the density.
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of the bandwidth is not trivial; there is a tradeoff between a too
    low value (small bias, high variance: overfitting) and a too high value (high
    bias, small variance: underfitting). We will return to this important concept
    of **bias-variance tradeoff** in the next chapter. For more information on the
    bias-variance tradeoff, refer to [http://en.wikipedia.org/wiki/Bias-variance_dilemma](http://en.wikipedia.org/wiki/Bias-variance_dilemma).'
  prefs: []
  type: TYPE_NORMAL
- en: The following figure illustrates the KDE. The dataset contains four points in
    *[0,1]* (black lines). The estimated density is a smooth curve, represented here
    with multiple bandwidth values.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Kernel density estimation
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are other KDE implementations in statsmodels and scikit-learn. You can
    find more information at [http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/](http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Fitting a probability distribution to data with the maximum likelihood
    method* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting a Bayesian model by sampling from a posterior distribution with a Markov
    chain Monte Carlo method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we illustrate a very common and useful method for characterizing
    a posterior distribution in a Bayesian model. Imagine that you have some data
    and you want to obtain information about the underlying random phenomenon. In
    a frequentist approach, you could try to fit a probability distribution within
    a given family of distributions, using a parametric method such as the maximum
    likelihood method. The optimization procedure would yield parameters that maximize
    the probability of observing the data if given the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: In a Bayesian approach, you consider the parameters themselves as random variables.
    Their prior distributions reflect your initial knowledge about these parameters.
    After the observations, your knowledge is updated, and this is reflected in the
    posterior distributions of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: A typical goal for Bayesian inference is to characterize the posterior distributions.
    Bayes' theorem gives an analytical way to do this, but it is often impractical
    in real-world problems due to the complexity of the models and the number of dimensions.
    A **Markov chain** **Monte Carlo** method, such as the **Metropolis-Hastings algorithm**,
    gives a numerical method to approximate a posterior distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we introduce the **PyMC** package, which gives an effective and natural
    interface for fitting a probabilistic model to data in a Bayesian framework. We
    will look at the annual frequency of storms in the northern Atlantic Ocean since
    the 1850s using data from NOAA, the US' National Oceanic and Atmospheric Administration.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe is largely inspired by a tutorial on PyMC's website (see the link
    in the *There's more…* section).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can find the instructions to install PyMC on the package's website. In this
    recipe, we will use PyMC2\. The new version (PyMC3) is still in development at
    the time of writing, and it is likely to be significantly different. For more
    information on PyMC, refer to [http://pymc-devs.github.io/pymc/](http://pymc-devs.github.io/pymc/).
    With Anaconda, you can try `conda install -c https://conda.binstar.org/pymc pymc`.
    Windows users can also find an installer at [www.lfd.uci.edu/~gohlke/pythonlibs/](http://www.lfd.uci.edu/~gohlke/pythonlibs/).
  prefs: []
  type: TYPE_NORMAL
- en: You also need to download the *Storms* dataset from the book's GitHub repository
    at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data)
    and extract it in the current directory.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import the standard packages and PyMC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s import the data with pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With pandas, it only takes a single line of code to get the annual number of
    storms in the North Atlantic Ocean. We first select the storms in that basin (`NA`),
    then we group the rows by year (`Season`), and then we take the number of unique
    storms (`Serial_Num`), as each storm can span several days (the `nunique()` method):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Now, we define our probabilistic model. We assume that storms arise following
    a time-dependent Poisson process with a deterministic rate. We assume that this
    rate is a piecewise-constant function that takes a first value `early_mean` before
    a switch point `switchpoint`, and a second value `late_mean` after that point.
    These three unknown parameters are treated as random variables (we will describe
    them more in the *How it works…* section).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Poisson process ([http://en.wikipedia.org/wiki/Poisson_process](http://en.wikipedia.org/wiki/Poisson_process))
    is a particular **point process**, that is, a stochastic process describing the
    random occurrence of instantaneous events. The Poisson process is fully random:
    the events occur independently at a given rate. See also [Chapter 13](ch13.html
    "Chapter 13. Stochastic Dynamical Systems"), *Stochastic Dynamical Systems*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the piecewise-constant rate as a Python function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, the observed variable is the annual number of storms. It follows a
    Poisson variable with a random mean (the rate of the underlying Poisson process).
    This fact is a known mathematical property of Poisson processes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we use the MCMC method to sample from the posterior distribution, given
    the observed data. The `sample()` method launches the fitting iterative procedure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's plot the sampled Markov chains. Their stationary distribution corresponds
    to the posterior distribution we want to characterize.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'We also plot the distribution of the samples, which correspond to the posterior
    distributions of our parameters, after the data points have been taken into account:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Taking the sample mean of these distributions, we get posterior estimates for
    the three unknown parameters, including the year where the frequency of storms
    suddenly increased:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can plot the estimated rate on top of the observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The general idea is to define a Bayesian probabilistic model and to fit it
    to the data. This model may be the starting point of an estimation or decision
    task. The model is essentially described by stochastic or deterministic variables
    linked together within a **direct acyclic graph** (**DAG**). *A* is linked to
    *B* if *B* is entirely or partially determined by *A*. The following figure shows
    the graph of the model used in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![How it works...](img/4818OS_07_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you can see, PyMC can create graph representations of the models. You need
    to install GraphViz (refer to [www.graphviz.org](http://www.graphviz.org)), pydot,
    and pyparsing. Because of an unfortunate bug, you might need to install a specific
    version of pyparsing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Stochastic variables follow distributions that can be parameterized by fixed
    numbers or other variables in the model. Parameters may be random variables themselves,
    reflecting knowledge prior to the observations. This is the core of Bayesian modeling.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the analysis is to include the observations into the model in order
    to update our knowledge as more and more data is available. Although Bayes' theorem
    gives us an exact way to compute those posterior distributions, it is rarely practical
    in real-world problems. This is notably due to the complexity of the models. Alternatively,
    numerical methods have been developed in order to tackle this problem.
  prefs: []
  type: TYPE_NORMAL
- en: The **Markov chain Monte Carlo** (**MCMC**) method used here allows us to sample
    from a complex distribution by simulating a Markov chain that has the desired
    distribution as its equilibrium distribution. The **Metropolis-Hastings algorithm**
    is a particular application of this method to our current example.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is implemented in the `MCMC` class in PyMC. The `burn` parameter
    determines how many initial iterations are thrown away. This is necessary because
    it takes a number of iterations for the Markov chain to converge to its equilibrium
    distribution. The `thin` parameter corresponds to the number of steps to skip
    in the evaluation of the distribution so as to minimize the autocorrelation of
    the samples. You will find more information at [http://pymc-devs.github.io/pymc/modelfitting.html](http://pymc-devs.github.io/pymc/modelfitting.html).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  prefs: []
  type: TYPE_NORMAL
- en: A great PyMC tutorial that we largely took inspiration from is available at
    [http://pymc-devs.github.io/pymc/tutorial.html](http://pymc-devs.github.io/pymc/tutorial.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A must-read free e-book on the subject, by Cameron Davidson-Pilon, entirely
    written in the IPython notebook, available at [http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Markov chain Monte Carlo method introduced at [http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Metropolis-Hastings algorithm introduced at [http://en.wikipedia.org/wiki/Metropolis-Hastings_algorithm](http://en.wikipedia.org/wiki/Metropolis-Hastings_algorithm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Getting started with Bayesian methods* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing data with the R programming language in the IPython notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R ([www.r-project.org](http://www.r-project.org)) is a free domain-specific
    programming language for statistics. Its syntax is well-adapted to statistical
    modeling and data analysis. By contrast, Python's syntax is typically more convenient
    for general-purpose programming. Luckily, IPython allows you to have the best
    of both worlds. For example, you can insert R code snippets anywhere in a normal
    IPython notebook. You can continue using Python and pandas for data loading and
    wrangling, and switch to R to design and fit statistical models. Using R instead
    of Python for these tasks is more than a matter of programming syntax; R comes
    with an impressive statistical toolbox that is still unmatched by Python.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will show how to use R from IPython, and we illustrate the
    most basic capabilities of R with a simple data analysis example.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need the statsmodels package for this recipe. You can find installation
    instructions in the previous recipe, *Fitting a probability distribution to data
    with the maximum likelihood method*.
  prefs: []
  type: TYPE_NORMAL
- en: You also need R. There are three steps to use R from IPython. First, install
    R and rpy2 (R to Python interface). Of course, you only need to do this step once.
    Then, to use R in an IPython session, you need to load the IPython R extension.
  prefs: []
  type: TYPE_NORMAL
- en: Download R for your operating system from [http://cran.r-project.org/mirrors.html](http://cran.r-project.org/mirrors.html)
    and install it. On Ubuntu, you can do `sudo apt-get install r-base-dev`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download rpy2 from [http://rpy.sourceforge.net/rpy2.html](http://rpy.sourceforge.net/rpy2.html)
    and install it. With Anaconda on Linux, you can try `conda install -c https://conda.binstar.org/r
    rpy2`. Alternatively, you can do `pip install rpy2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, to execute R code in an IPython notebook, execute `%load_ext rmagic` first.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: rpy2 does not appear to work well on Windows. We recommend using Linux or Mac
    OS X.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will use the following workflow: first, we load data from Python.
    Then, we use R to design and fit a model, and to make some plots in the IPython
    notebook. We could also load data from R, or design and fit a statistical model
    with Python''s statsmodels package, and so on. In particular, the analysis we
    do here could be done entirely in Python, without resorting to the R language.
    This recipe merely shows the basics of R and illustrates how R and Python can
    play together within an IPython session.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the *longley* dataset with the statsmodels package. This dataset
    contains a few economic indicators in the US from 1947 to 1962\. We also load
    the IPython R extension:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We define `x` and `y` as the exogeneous (independent) and endogenous (dependent)
    variables, respectively. The endogenous variable quantifies the total employment
    in the country.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For convenience, we add the endogenous variable to the `x` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will make a simple plot in R. First, we need to pass Python variables to
    R. We can use the `%R -i var1,var2` magic. Then, we can call R''s `plot()` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now that the data has been passed to R, we can fit a linear model to the data.
    The `lm()` function lets us perform a linear regression. Here, we want to express
    `totemp` (total employment) as a function of the country''s GNP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_07_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `-i` and `-o` options of the `%R` magic allow us to pass variables back
    and forth between IPython and R. The variable names need to be separated by commas.
    You can find more information about the `%R` magic in the documentation available
    at [http://rpy.sourceforge.net/](http://rpy.sourceforge.net/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, the tilde (`~`) expresses the dependence of a dependent variable upon
    one or several independent variables. The `lm()` function allows us to fit a simple
    linear regression model to the data. Here, `totemp` is expressed as a function
    of `gnp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *b* (intercept) and *a* are the coefficients of the linear regression
    model. These two values are returned by `fit$coefficients` in R, where `fit` is
    the fitted model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our data points do not satisfy this relation exactly, of course. The coefficients
    are chosen so as to minimize the error between this linear prediction and the
    actual values. This is typically done by minimizing the following least squares
    error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The data points are *(gnp[i]**, totemp[i])* here. The coefficients *a* and
    *b* that are returned by `lm()` make this sum minimal: they fit the data best.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Regression is an important statistical concept that we will see in greater
    detail in the next chapter. Here are a few references:'
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis on Wikipedia, available at [http://en.wikipedia.org/wiki/Regression_analysis](http://en.wikipedia.org/wiki/Regression_analysis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least squares method on Wikipedia, available at [en.wikipedia.org/wiki/Linear_least_squares_(mathematics)](http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R is an excellent platform for advanced statistics. Python has a few statistical
    packages such as pandas and statsmodels that implement many common features, but
    the number of statistical toolboxes in R remains unmatched by Python at this time.
    Yet, Python has a much wider range of possibilities outside of statistics and
    is an excellent general-purpose language that comes with an impressive number
    of various packages.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the multilanguage capabilities of IPython, you don't necessarily have
    to choose between those languages. You can keep using Python and switch to R when
    you need highly specific statistical features that are still missing in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few references about R:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to R available at [http://cran.r-project.org/doc/manuals/R-intro.html](http://cran.r-project.org/doc/manuals/R-intro.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R tutorial available at [www.cyclismo.org/tutorial/R/](http://www.cyclismo.org/tutorial/R/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRAN, or Comprehensive R Archive Network, containing many packages for R, available
    at [http://cran.r-project.org](http://cran.r-project.org)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IPython and R tutorial available at [http://nbviewer.ipython.org/github/ipython/ipython/blob/master/examples/Builtin%20Extensions/R%20Magics.ipynb](http://nbviewer.ipython.org/github/ipython/ipython/blob/master/examples/Builtin%20Extensions/R%20Magics.ipynb)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Exploring a dataset with pandas and matplotlib* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
