- en: Chapter 7. Statistical Data Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。统计数据分析
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Exploring a dataset with pandas and matplotlib
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pandas和matplotlib探索数据集
- en: Getting started with statistical hypothesis testing – a simple z-test
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始进行统计假设检验——简单的z检验
- en: Getting started with Bayesian methods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用贝叶斯方法
- en: Estimating the correlation between two variables with a contingency table and
    a chi-squared test
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用列联表和卡方检验估计两个变量之间的相关性
- en: Fitting a probability distribution to data with the maximum likelihood method
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最大似然法拟合数据的概率分布
- en: Estimating a probability distribution nonparametrically with a kernel density
    estimation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用核密度估计法非参数地估计概率分布
- en: Fitting a Bayesian model by sampling from a posterior distribution with a Markov
    chain Monte Carlo method
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过马尔可夫链蒙特卡洛方法从后验分布中抽样拟合贝叶斯模型
- en: Analyzing data with the R programming language in the IPython notebook
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在IPython笔记本中使用R编程语言分析数据
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapters, we reviewed technical aspects of high-performance
    interactive computing in Python. We now begin the second part of this book by
    illustrating a variety of scientific questions that can be tackled with Python.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们回顾了Python中高性能交互式计算的技术方面。现在，我们开始本书的第二部分，展示可以使用Python解决的各种科学问题。
- en: In this chapter, we introduce statistical methods for data analysis. In addition
    to covering statistical packages such as pandas, statsmodels, and PyMC, we will
    explain the basics of the underlying mathematical principles. Therefore, this
    chapter will be most profitable if you have basic experience with probability
    theory and calculus.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了用于数据分析的统计方法。除了涉及诸如pandas、statsmodels和PyMC等统计包外，我们还将解释这些方法背后的数学原理基础。因此，如果你有一定的概率论和微积分基础，本章将会最有益。
- en: The next chapter, [Chapter 8](ch08.html "Chapter 8. Machine Learning"), *Machine
    Learning*, is closely related; the underlying mathematics is very similar, but
    the goals are slightly different. In this chapter, we show how to gain insight
    into real-world data and how to make informed decisions in the presence of uncertainty.
    In the next chapter, the goal is to *learn from data*, that is, to generalize
    and to predict outcomes from partial observations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章，[第8章](ch08.html "第8章。机器学习")，*机器学习*，与本章密切相关；其基础数学非常相似，但目标略有不同。在本章中，我们展示了如何从现实世界数据中获得洞见，以及如何在不确定性面前做出明智的决策。而在下一章，目标是*从数据中学习*，即从部分观察中进行归纳和预测结果。
- en: In this introduction, we will give a broad, high-level overview of the methods
    we will see in this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本引言中，我们将对本章中所涉及的方法进行广泛的、高层次的概述。
- en: What is statistical data analysis?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是统计数据分析？
- en: The goal of statistical data analysis is to understand a complex, real-world
    phenomenon from partial and uncertain observations. The uncertainty in the data
    results in uncertainty in the knowledge we get about the phenomenon. A major goal
    of the theory is to *quantify* this uncertainty.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 统计数据分析的目标是从部分和不确定的观察中理解复杂的现实现象。数据中的不确定性导致我们对现象的知识也存在不确定性。理论的主要目标是*量化*这种不确定性。
- en: It is important to make the distinction between the mathematical theory underlying
    statistical data analysis, and the decisions made after conducting an analysis.
    The former is perfectly rigorous; perhaps surprisingly, mathematicians were able
    to build an exact mathematical framework to deal with uncertainty. Nevertheless,
    there is a subjective part in the way statistical analysis yields actual human
    decisions. Understanding the risk and the uncertainty behind statistical results
    is critical in the decision-making process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行统计数据分析时，重要的是区分数学理论与分析后做出的决策。前者是完美严谨的；或许令人惊讶的是，数学家们能够建立一个精确的数学框架来处理不确定性。然而，统计分析得出的实际人类决策中有主观的部分。在决策过程中，理解统计结果背后的风险和不确定性至关重要。
- en: In this chapter, we will see the basic notions, principles, and theories behind
    statistical data analysis, covering in particular how to make decisions with a
    quantified risk. Of course, we will always show how to implement these methods
    with Python.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到统计数据分析背后的基本概念、原则和理论，特别是如何在量化风险的情况下做出决策。当然，我们将始终展示如何使用Python实现这些方法。
- en: A bit of vocabulary
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些术语
- en: There are many terms that need introduction before we get started with the recipes.
    These notions allow us to classify statistical techniques within multiple dimensions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Exploration, inference, decision, and prediction
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Exploratory methods** allow us to get a preliminary look at a dataset through
    basic statistical aggregates and interactive visualization. We covered these basic
    methods in the first chapter of this book and in the book *Learning IPython for
    Interactive Computing and Data Visualization*, *Packt Publishing*. The first recipe
    of this chapter, *Exploring a dataset with pandas and matplotlib*, shows another
    example.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical inference** consists of getting information about an unknown
    process through partial and uncertain observations. In particular, **estimation**
    entails obtaining approximate quantities for the mathematical variables describing
    this process. Three recipes in this chapter deal with statistical inference:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: The *Fitting a probability distribution to data with the maximum likelihood
    method* recipe
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Estimating a probability distribution nonparametrically with a kernel density
    estimation* recipe
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Fitting a Bayesian model by sampling from a posterior distribution with
    a Markov chain Monte Carlo method* recipe
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision theory** allows us to make decisions about an unknown process from
    random observations, with a controlled risk. The following two recipes show how
    to make statistical decisions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Getting started with statistical hypothesis testing: a simple z-test*
    recipe'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Estimating the correlation between two variables with a contingency table
    and a chi-squared test* recipe
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction** consists of learning from data, that is, predicting the outcomes
    of a random process based on a limited number of observations. This is the topic
    of the next chapter, [Chapter 8](ch08.html "Chapter 8. Machine Learning"), *Machine
    Learning*.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Univariate and multivariate methods
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In most cases, you can consider two dimensions in your data:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**Observations** (or **samples**, for machine learning people)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variables** (or **features**)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, observations are independent realizations of the same random process.
    Each observation is made of one or several variables. Most of the time, variables
    are either numbers, or elements belonging to a finite set (that is, taking a finite
    number of values). The first step in an analysis is to understand what your observations
    and variables are.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Your problem is **univariate** if you have one variable. It is **bivariate**
    if you have two variables and **multivariate** if you have at least two variables.
    Univariate methods are typically simpler. That being said, univariate methods
    may be used on multivariate data, using one dimension at a time. Although interactions
    between variables cannot be explored in that case, it is often an interesting
    first approach.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Frequentist and Bayesian methods
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are at least two different ways of considering uncertainty, resulting
    in two different classes of methods for inference, decision, and other statistical
    questions. These are called **frequentist and** **Bayesian methods**. Some people
    prefer frequentist methods, while others prefer Bayesian methods.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 至少有两种不同的方式来考虑不确定性，从而产生两类用于推断、决策和其他统计问题的方法。它们分别被称为**频率派方法**和**贝叶斯方法**。一些人偏好频率派方法，而另一些人则偏好贝叶斯方法。
- en: Frequentists interpret a probability as a **statistical average** across many
    independent realizations (law of large numbers). Bayesians interpret it as a **degree
    of belief** (no need for many realizations). The Bayesian interpretation is very
    useful when only a single trial is considered. In addition, Bayesian theory takes
    into account our **prior knowledge** about a random process. This prior probability
    distribution is updated into a posterior distribution as we get more and more
    data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 频率派解释概率为多个独立试验的**统计平均**（大数法则）。贝叶斯派则解释为**信念程度**（不需要多个试验）。贝叶斯解释在只考虑单次试验时非常有用。此外，贝叶斯理论还考虑了我们对随机过程的**先验知识**。随着数据的增加，先验概率分布会被更新为后验分布。
- en: Both frequentist and Bayesian methods have their advantages and disadvantages.
    For instance, one could say that frequentist methods might be easier to apply
    than Bayesian methods, but more difficult to interpret. For classic misuses of
    frequentist methods, see [www.refsmmat.com/statistics/](http://www.refsmmat.com/statistics/).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 频率派和贝叶斯派方法各有其优缺点。例如，有人可能会说，频率派方法比贝叶斯方法更容易应用，但解释起来更为困难。有关频率派方法的经典误用，见[www.refsmmat.com/statistics/](http://www.refsmmat.com/statistics/)。
- en: In any case, if you are a beginner in statistical data analysis, you probably
    want to learn the basics of both approaches before choosing sides. This chapter
    introduces you to both types of methods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，如果你是统计数据分析的初学者，你可能希望在选择立场之前学习这两种方法的基础知识。本章将向你介绍这两种方法。
- en: 'The following recipes are exclusively Bayesian:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的配方完全是贝叶斯方法：
- en: The *Getting started with Bayesian methods* recipe
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*入门贝叶斯方法*配方'
- en: The *Fitting a Bayesian model by sampling from a posterior distribution with
    a Markov chain Monte Carlo method* recipe
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过马尔可夫链蒙特卡洛方法从后验分布中采样拟合贝叶斯模型*配方'
- en: Jake Vanderplas has written several blog posts about frequentism and Bayesianism,
    with examples in Python. The first post of the series is available at [http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Jake Vanderplas曾写过几篇关于频率派和贝叶斯派的博客文章，并且在文章中提供了Python的示例。该系列的第一篇文章可以在[http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)找到。
- en: Parametric and nonparametric inference methods
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数化和非参数化推断方法
- en: In many cases, you base your analysis on a **probabilistic model**. This model
    describes how your data is generated. A probabilistic model has no reality; it
    is only a mathematical object that guides you in your analysis. A good model can
    be helpful, whereas a bad model may misguide you.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，你会基于**概率模型**来进行分析。该模型描述了你的数据是如何生成的。概率模型并没有实际存在；它仅是一个数学对象，指导你进行分析。一个好的模型是有帮助的，而一个不好的模型可能会误导你。
- en: With a **parametric method**, you assume that your model belongs to a known
    family of probability distributions. The model has one or multiple numerical *parameters*
    that you can *estimate*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**参数化方法**时，你假设你的模型属于一个已知的概率分布族。该模型有一个或多个数值的*参数*，你可以对其进行*估计*。
- en: With a **nonparametric model**, you do not make such an assumption in your model.
    This gives you more flexibility. However, these methods are typically more complicated
    to implement and to interpret.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**非参数化模型**时，你的模型不需要做出这样的假设。这给你带来了更多的灵活性。然而，这些方法通常实现起来更为复杂，且难以解释。
- en: 'The following recipes are parametric and nonparametric, respectively:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下配方分别是参数化和非参数化的：
- en: The *Fitting a probability distribution to data with the maximum likelihood
    method* recipe
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用最大似然法拟合概率分布*配方'
- en: The *Estimating a probability distribution nonparametrically with a kernel density
    estimation* recipe
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用核密度估计非参数化估计概率分布*配方'
- en: 'This chapter only gives you an idea of the wide range of possibilities that
    Python offers for statistical data analysis. You can find many books and online
    courses that cover statistical methods in much greater detail, such as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本章仅为你提供了 Python 在统计数据分析方面广泛可能性的一个大致概念。你可以找到许多书籍和在线课程，详细讲解统计方法，例如：
- en: Statistics on WikiBooks at [http://en.wikibooks.org/wiki/Statistics](http://en.wikibooks.org/wiki/Statistics)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基教科书上的统计学内容：[http://en.wikibooks.org/wiki/Statistics](http://en.wikibooks.org/wiki/Statistics)
- en: Free statistical textbooks available at [http://stats.stackexchange.com/questions/170/free-statistical-textbooks](http://stats.stackexchange.com/questions/170/free-statistical-textbooks)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 免费的统计学教材可以在 [http://stats.stackexchange.com/questions/170/free-statistical-textbooks](http://stats.stackexchange.com/questions/170/free-statistical-textbooks)
    找到
- en: Exploring a dataset with pandas and matplotlib
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pandas 和 matplotlib 探索数据集
- en: In this first recipe, we will show how to conduct a preliminary analysis of
    a dataset with pandas. This is typically the first step after getting access to
    the data. pandas lets us load the data very easily, explore the variables, and
    make basic plots with matplotlib.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这第一个食谱中，我们将展示如何使用 pandas 对数据集进行初步分析。这通常是在获得数据后进行的第一步。pandas 让我们能够非常轻松地加载数据、探索变量，并使用
    matplotlib 创建基本的图表。
- en: We will take a look at a dataset containing all ATP matches played by four tennis
    players until 2012\. Here, we will focus on Roger Federer.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看一个数据集，该数据集包含了四名网球选手直到 2012 年为止的所有 ATP 比赛。在这里，我们将重点关注罗杰·费德勒。
- en: Getting ready
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Download the *Tennis* dataset from the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data),
    and extract it to the current directory.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从本书的 GitHub 仓库 [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data)
    下载 *Tennis* 数据集，并将其解压到当前目录。
- en: How to do it...
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We import NumPy, pandas, and matplotlib:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入 NumPy、pandas 和 matplotlib：
- en: '[PRE0]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The dataset is a CSV file, that is, a text file with comma-separated values.
    pandas lets us load this file with a single function:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集是一个 CSV 文件，即一个以逗号分隔值的文本文件。pandas 让我们能够用一个函数加载这个文件：
- en: '[PRE1]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can have a first look at this dataset by just displaying it in the IPython
    notebook:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以通过在 IPython 笔记本中直接显示数据集来进行首次查看：
- en: '[PRE2]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are many columns. Each row corresponds to a match played by Roger Federer.
    Let''s add a Boolean variable indicating whether he has won the match or not.
    The `tail()` method displays the last rows of the column:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集有很多列。每一行对应罗杰·费德勒的一场比赛。我们来添加一个布尔变量，表示他是否赢得了比赛。`tail()` 方法显示列的最后几行：
- en: '[PRE3]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`df[''win'']` is a `Series` object. It is very similar to a NumPy array, except
    that each value has an index (here, the match index). This object has a few standard
    statistical functions. For example, let''s look at the proportion of matches won:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df[''win'']` 是一个 `Series` 对象。它与 NumPy 数组非常相似，只是每个值都有一个索引（这里是比赛索引）。这个对象具有一些标准的统计函数。例如，让我们查看获胜比赛的比例：'
- en: '[PRE4]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we are going to look at the evolution of some variables across time. The
    `df[''start date'']` field contains the start date of the tournament as a string.
    We can convert the type to a date type using the `pd.to_datetime()` function:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将观察一些变量随时间的变化。`df['start date']` 字段包含了比赛的开始日期，格式为字符串。我们可以使用 `pd.to_datetime()`
    函数将其转换为日期类型：
- en: '[PRE5]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We are now looking at the proportion of double faults in each match (taking
    into account that there are logically more double faults in longer matches!).
    This number is an indicator of the player's state of mind, his level of self-confidence,
    his willingness to take risks while serving, and other parameters.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在正在查看每场比赛中的双误比例（考虑到在更长时间的比赛中，双误通常更多！）。这个数字是球员心理状态的一个指标，反映了他的自信水平、在发球时的冒险精神以及其他一些参数。
- en: '[PRE6]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can use the `head()` and `tail()` methods to take a look at the beginning
    and the end of the column, and `describe()` to get summary statistics. In particular,
    let's note that some rows have NaN values (that is, the number of double faults
    is not available for all matches).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 `head()` 和 `tail()` 方法查看列的开头和结尾，并使用 `describe()` 获取摘要统计数据。特别地，我们需要注意，有些行包含
    NaN 值（即，并不是所有比赛的双误数据都有记录）。
- en: '[PRE7]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'A very powerful feature in pandas is `groupby()`. This function allows us to
    group together rows that have the same value in a particular column. Then, we
    can aggregate this group by value to compute statistics in each group. For instance,
    here is how we can get the proportion of wins as a function of the tournament''s
    surface:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pandas中的一个非常强大的功能是`groupby()`。这个函数允许我们将具有相同列值的行组合在一起。然后，我们可以通过该值对该组进行聚合，计算每个组中的统计数据。例如，下面是我们如何根据比赛场地表面类型来获取胜利比例：
- en: '[PRE8]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we are going to display the proportion of double faults as a function
    of the tournament date, as well as the yearly average. To do this, we also use
    `groupby()`:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将显示双误差的比例与比赛日期的关系，以及每年的平均值。为此，我们也使用`groupby()`：
- en: '[PRE9]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`gb` is a `GroupBy` instance. It is similar to a `DataFrame` object, but there
    are multiple rows per group (all matches played in each year). We can aggregate
    these rows using the `mean()` operation. We use the matplotlib `plot_date()` function
    because the x-axis contains dates:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`gb`是一个`GroupBy`实例。它类似于`DataFrame`对象，但每个组中有多行（每年比赛的所有场次）。我们可以使用`mean()`操作对这些行进行聚合。我们使用matplotlib的`plot_date()`函数，因为x轴包含日期：'
- en: '[PRE10]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![How to do it...](img/4818OS_07_01.jpg)'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何实现...](img/4818OS_07_01.jpg)'
- en: There's more...
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: pandas is an excellent tool for data wrangling and exploratory analysis. pandas
    accepts all sorts of formats (text-based, and binary files) and it lets us manipulate
    tables in many ways. In particular, the `groupby()` function is extremely powerful.
    This library is covered in much greater detail in a book by Wes McKinney, *Python
    for Data Analysis*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: pandas是一个用于数据整理和探索性分析的优秀工具。pandas支持各种格式（文本格式和二进制文件），并允许我们以多种方式操作表格。特别是，`groupby()`函数非常强大。Wes
    McKinney的《*Python for Data Analysis*》一书对这个库进行了更为详细的讲解。
- en: What we covered here is only the first step in a data-analysis process. We need
    more advanced statistical methods to obtain reliable information about the underlying
    phenomena, make decisions and predictions, and so on. This is the topic of the
    following recipes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所讲述的仅仅是数据分析过程中的第一步。我们需要更高级的统计方法来获得关于基础现象的可靠信息，做出决策和预测，等等。这个内容将在接下来的章节中讨论。
- en: In addition, more complex datasets demand more sophisticated analysis methods.
    For example, digital recordings, images, sounds, and videos require specific signal
    processing treatments before we can apply statistical techniques. These questions
    will be covered in subsequent chapters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，更复杂的数据集需要更复杂的分析方法。例如，数字记录、图像、声音和视频在应用统计技术之前需要特定的信号处理处理。这些问题将在后续章节中讨论。
- en: Getting started with statistical hypothesis testing – a simple z-test
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始统计假设检验——一个简单的z检验
- en: '**Statistical hypothesis testing** allows us to make decisions in the presence
    of incomplete data. By definition, these decisions are uncertain. Statisticians
    have developed rigorous methods to evaluate this risk. Nevertheless, some subjectivity
    is always involved in the decision-making process. The theory is just a tool that
    helps us make decisions in an uncertain world.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**统计假设检验**使我们能够在数据不完全的情况下做出决策。根据定义，这些决策是有不确定性的。统计学家已经开发了严格的方法来评估这种风险。然而，决策过程中总是涉及一些主观性。理论只是帮助我们在不确定的世界中做出决策的工具。'
- en: 'Here, we introduce the most basic ideas behind statistical hypothesis testing.
    We will follow an extremely simple example: coin tossing. More precisely, we will
    show how to perform a **z-test**, and we will briefly explain the mathematical
    ideas underlying it. This kind of method (also called the *frequentist method*),
    although widely used in science, is subject to many criticisms. We will show later
    a more modern approach based on Bayesian theory. It is very helpful to understand
    both approaches, because many studies and publications still follow frequentist
    methods.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍统计假设检验背后的最基本思想。我们将跟随一个极其简单的例子：抛硬币。更准确地说，我们将展示如何进行**z检验**，并简要解释其背后的数学思想。这种方法（也称为*频率主义方法*）尽管在科学中被广泛使用，但也受到了许多批评。稍后我们将展示一种基于贝叶斯理论的更现代的方法。理解这两种方法非常有帮助，因为许多研究和出版物仍然采用频率主义方法。
- en: Getting ready
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have a basic knowledge of probability theory for this recipe (random
    variables, distributions, expectancy, variance, central limit theorem, and so
    on).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备基本的概率论知识（随机变量、分布、期望、方差、中心极限定理等）才能理解此方法。
- en: How to do it...
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Many frequentist methods for hypothesis testing roughly involve the following
    steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 许多频率主义假设检验方法大致包括以下步骤：
- en: Writing down the hypotheses, notably the **null hypothesis**, which is the *opposite*
    of the hypothesis we want to prove (with a certain degree of confidence).
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写下假设，特别是**零假设**，它是我们想要证明的假设的*对立面*（以一定的置信度）。
- en: Computing a **test statistic**, a mathematical formula depending on the test
    type, the model, the hypotheses, and the data.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算**检验统计量**，这是一个依赖于检验类型、模型、假设和数据的数学公式。
- en: Using the computed value to accept the hypothesis, reject it, or fail to conclude.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用计算得出的值来接受假设、拒绝假设或无法得出结论。
- en: Here, we flip a coin *n* times and we observe *h* heads. We want to know whether
    the coin is fair (null hypothesis). This example is extremely simple yet quite
    useful for pedagogical purposes. Besides, it is the basis of many more complex
    methods.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们抛掷硬币*n*次，并观察到*h*次正面朝上。我们想知道硬币是否公平（零假设）。这个例子非常简单，但对于教学目的非常有用。此外，它是许多更复杂方法的基础。
- en: We denote the Bernoulli distribution by *B(q)* with the unknown parameter *q*.
    You can refer to [http://en.wikipedia.org/wiki/Bernoulli_distribution](http://en.wikipedia.org/wiki/Bernoulli_distribution)
    for more information.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用*B(q)*表示伯努利分布，其中未知参数为*q*。您可以访问[http://en.wikipedia.org/wiki/Bernoulli_distribution](http://en.wikipedia.org/wiki/Bernoulli_distribution)获取更多信息。
- en: 'A Bernoulli variable is:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 伯努利变量是：
- en: 0 (tail) with probability *1-q*
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0（反面）出现的概率为*1-q*
- en: 1 (head) with probability *q*
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1（正面朝上）出现的概率为*q*
- en: 'Here are the steps required to conduct a simple statistical *z*-test:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是进行简单统计*z*-检验所需的步骤：
- en: 'Let''s suppose that after *n=100* flips, we get *h=61* heads. We choose a significance
    level of 0.05: is the coin fair or not? Our null hypothesis is: *the coin is fair
    (q = 1/2)*:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们抛掷了*n=100*次硬币，得到了*h=61*次正面朝上。我们选择显著性水平为0.05：硬币是否公平？我们的零假设是：*硬币是公平的（q = 1/2）*：
- en: '[PRE11]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let's compute the **z-score**, which is defined by the following formula (`xbar`
    is the estimated average of the distribution). We will explain this formula in
    the next section, *How it works…*.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们计算**z分数**，它由以下公式定义（`xbar`是分布的估计平均值）。我们将在下一部分*它是如何工作的...*中解释此公式。
- en: '[PRE12]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, from the z-score, we can compute the p-value as follows:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，根据z分数，我们可以按以下方式计算p值：
- en: '[PRE13]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This p-value is less than 0.05, so we reject the null hypothesis and conclude
    that *the coin is probably not fair*.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该p值小于0.05，因此我们拒绝零假设，并得出结论：*硬币可能不公平*。
- en: How it works...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The coin tossing experiment is modeled as a sequence of *n* independent random
    variables ![How it works...](img/4818OS_07_46.jpg) following the Bernoulli distribution
    *B(q)*. Each *x[i]* represents one coin flip. After our experiment, we get actual
    values (samples) for these variables. A different notation is sometimes used to
    distinguish between the random variables (probabilistic objects) and the actual
    values (samples).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 投币实验被建模为一系列*独立的随机变量*，![它是如何工作的...](img/4818OS_07_46.jpg)，遵循伯努利分布*B(q)*。每个*x[i]*代表一次投币实验。经过我们的实验后，我们获得这些变量的实际值（样本）。有时为了区分随机变量（概率对象）和实际值（样本），会使用不同的符号表示。
- en: 'The following formula gives the **sample mean** (proportion of heads here):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式给出了**样本均值**（这里是正面朝上的比例）：
- en: '![How it works...](img/4818OS_07_02.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4818OS_07_02.jpg)'
- en: 'Knowing the expectancy ![How it works...](img/4818OS_07_39.jpg) and variance
    ![How it works...](img/4818OS_07_40.jpg) of the distribution *B(q)*, we compute:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 知道分布*B(q)*的期望值![它是如何工作的...](img/4818OS_07_39.jpg)和方差![它是如何工作的...](img/4818OS_07_40.jpg)，我们计算：
- en: '![How it works...](img/4818OS_07_03.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4818OS_07_03.jpg)'
- en: 'The z-test is the normalized version of ![How it works...](img/4818OS_07_41.jpg)
    (we remove its mean, and divide by the standard deviation, thus we get a variable
    with mean 0 and standard deviation 1):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: z检验是![它是如何工作的...](img/4818OS_07_41.jpg)的标准化版本（我们去除其均值，并除以标准差，因此我们得到一个均值为0，标准差为1的变量）：
- en: '![How it works...](img/4818OS_07_04.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4818OS_07_04.jpg)'
- en: 'Under the null hypothesis, what is the probability of obtaining a z-test higher
    than some quantity *z[0]*? This probability is called the (two-sided) **p-value**.
    According to the central limit theorem, the z-test approximately follows a standard
    Gaussian distribution *N(0,1)* for large *n*, so we get:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在零假设下，获得大于某个数量*z[0]*的z检验的概率是多少？这个概率称为（双尾）**p值**。根据中心极限定理，当*n*较大时，z检验大致服从标准正态分布*N(0,1)*，因此我们得到：
- en: '![How it works...](img/4818OS_07_05.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4818OS_07_05.jpg)'
- en: 'The following diagram illustrates the z-score and the p-value:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了z分数和p值：
- en: '![How it works...](img/4818OS_07_06.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/4818OS_07_06.jpg)'
- en: Illustration of the z-score and the p-value
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: z分数和p值的示意图
- en: 'In this formula, ![How it works...](img/4818OS_07_42.jpg) is the **cumulative
    distribution function** of a standard normal distribution. In SciPy, we can get
    it with `scipy.stats.norm.cdf`. So, given the z-test computed from the data, we
    compute the p-value: the probability of observing a z-test more extreme than the
    observed test, under the null hypothesis.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![它是如何工作的...](img/4818OS_07_42.jpg) 是标准正态分布的**累积分布函数**。在SciPy中，我们可以通过
    `scipy.stats.norm.cdf` 获取它。因此，给定从数据中计算得出的z检验，我们计算p值：在原假设下，观察到比所观察到的检验值更极端的z检验的概率。
- en: 'If the p-value is less than five percent (a frequently-chosen significance
    level, for arbitrary and historical reasons), we conclude that either:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果p值小于5%（这是一个常用的显著性水平，出于任意和历史原因），我们得出结论：
- en: The null hypothesis is false, thus we conclude that the coin is unfair.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原假设为假，因此我们得出结论：硬币是不公平的。
- en: The null hypothesis is true, and it's just bad luck if we obtained these values.
    We cannot make a conclusion.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原假设为真，如果我们得到了这些值，那就是运气不好。我们无法得出结论。
- en: We cannot disambiguate between these two options in this framework, but typically
    the first option is chosen. We hit the limits of frequentist statistics, although
    there are ways to mitigate this problem (for example, by conducting several independent
    studies and looking at all of their conclusions).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架中，我们无法对这两个选项进行歧义消解，但通常选择第一个选项。我们达到了频率主义统计的极限，尽管有一些方法可以缓解这个问题（例如，通过进行几项独立研究并查看它们的所有结论）。
- en: There's more...
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Many statistical tests following this pattern exist. Reviewing all those tests
    is largely beyond the scope of this book, but you can take a look at the reference
    at [http://en.wikipedia.org/wiki/Statistical_hypothesis_testing](http://en.wikipedia.org/wiki/Statistical_hypothesis_testing).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多遵循这种模式的统计检验。回顾所有这些检验远超出本书的范围，但你可以查看 [http://en.wikipedia.org/wiki/Statistical_hypothesis_testing](http://en.wikipedia.org/wiki/Statistical_hypothesis_testing)
    中的参考资料。
- en: As a p-value is not easy to interpret, it can lead to wrong conclusions, even
    in peer-reviewed scientific publications. For an in-depth treatment of the subject,
    see [www.refsmmat.com/statistics/](http://www.refsmmat.com/statistics/).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于p值不容易解释，它可能导致错误的结论，即使是在同行评审的科学出版物中也是如此。关于该主题的深入探讨，请参见 [www.refsmmat.com/statistics/](http://www.refsmmat.com/statistics/)。
- en: See also
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The *Getting started with Bayesian methods* recipe
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*入门贝叶斯方法*部分'
- en: Getting started with Bayesian methods
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门贝叶斯方法
- en: In the last recipe, we used a frequentist method to test a hypothesis on incomplete
    data. Here, we will see an alternative approach based on **Bayesian theory**.
    The main idea is to consider that *unknown parameters are random variables*, just
    like the variables describing the experiment. Prior knowledge about the parameters
    is integrated into the model. This knowledge is updated as more and more data
    is observed.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个例子中，我们使用了频率主义方法对不完全数据进行假设检验。在这里，我们将看到一种基于**贝叶斯理论**的替代方法。主要思想是认为*未知参数是随机变量*，就像描述实验的变量一样。关于这些参数的先验知识被整合到模型中。随着数据的不断增加，这些知识会被更新。
- en: Frequentists and Bayesians interpret probabilities differently. Frequentists
    interpret a probability as a limit of frequencies when the number of samples tends
    to infinity. Bayesians interpret it as a belief; this belief is updated as more
    and more data is observed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 频率主义者和贝叶斯主义者对概率的解释不同。频率主义者将概率解释为样本数量趋于无穷大时频率的极限。而贝叶斯主义者则将其解释为一种信念；随着观察到的数据越来越多，这种信念会不断更新。
- en: Here, we revisit the previous coin flipping example with a Bayesian approach.
    This example is sufficiently simple to permit an analytical treatment. In general,
    as we will see later in this chapter, analytical results cannot be obtained and
    numerical methods become essential.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们以贝叶斯方法重新审视之前的抛硬币例子。这个例子足够简单，可以进行分析处理。通常，如我们将在本章后面看到的，无法得到解析结果，数值方法变得至关重要。
- en: Getting ready
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: This is a math-heavy recipe. Knowledge of basic probability theory (random variables,
    distributions, Bayes formula) and calculus (derivatives, integrals) is recommended.
    We use the same notations as in the previous recipe.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个数学密集的过程。建议具备基本概率论（随机变量、分布、贝叶斯公式）和微积分（导数、积分）的知识。我们使用与前一个方法相同的符号。
- en: How to do it...
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: Let *q* be the probability of obtaining a head. Whereas *q* was just a fixed
    number in the previous recipe, we consider here that it is a **random variable**.
    Initially, this variable follows a distribution called the **prior probability
    distribution**. It represents our knowledge about *q* before we start flipping
    the coin. We will update this distribution after each trial (posterior distribution).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让*q*表示获得正面的概率。而在之前的公式中，*q*只是一个固定的数字，在这里我们认为它是一个**随机变量**。最初，这个变量遵循一种被称为**先验概率分布**的分布。它表示我们在开始抛硬币之前，对*q*的知识。我们将在每次试验后更新这个分布（即后验分布）。
- en: 'First, we assume that *q* is a *uniform* random variable in the interval [0,
    1]. That''s our prior distribution: for all *q*, *P(q)=1*.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们假设*q*是区间[0, 1]上的一个*均匀*随机变量。这是我们的先验分布：对于所有*q*，*P(q)=1*。
- en: Then, we flip our coin *n* times. We note *x[i]* the outcome of the *i*th flip
    (0 for tail and 1 for head).
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们抛硬币*n*次。我们用*x[i]*表示第*i*次抛掷的结果（0代表反面，1代表正面）。
- en: What is the probability distribution of *q* knowing the observations *x[i]*?
    **Bayes' theorem** allows us to compute the **posterior distribution** analytically
    (see the next section for the mathematical details):![How to do it...](img/4818OS_07_07.jpg)
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 知道观察到的结果*x[i]*后，*q*的概率分布是多少？**贝叶斯定理**使我们能够解析地计算出**后验分布**（数学细节见下一部分）：![如何操作...](img/4818OS_07_07.jpg)
- en: We define the posterior distribution according to the previous mathematical
    formula. We remark that this expression is *(n+1)* times the **probability mass
    function** (**PMF**) of the binomial distribution, which is directly available
    in `scipy.stats`. (For more information on Binomial distribution, refer to [http://en.wikipedia.org/wiki/Binomial_distribution](http://en.wikipedia.org/wiki/Binomial_distribution).)
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据之前的数学公式定义后验分布。我们注意到，这个表达式是*(n+1)*倍的**概率质量函数**（**PMF**）二项分布的形式，二项分布可以直接通过`scipy.stats`获得。（有关二项分布的更多信息，请参见[http://en.wikipedia.org/wiki/Binomial_distribution](http://en.wikipedia.org/wiki/Binomial_distribution)。）
- en: '[PRE14]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s plot this distribution for an observation of *h=61* heads and *n=100*
    total flips:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们为观察到*h=61*次正面朝上的结果和*n=100*次抛掷总数，绘制这个分布：
- en: '[PRE15]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![How to do it...](img/4818OS_07_08.jpg)'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4818OS_07_08.jpg)'
- en: This curve represents our belief about the parameter *q* after we have observed
    61 heads.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个曲线表示我们在观察到61次正面朝上的结果后，对参数*q*的信念。
- en: How it works...
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this section, we explain Bayes' theorem, and we give the mathematical details
    underlying this example.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们解释了贝叶斯定理，并给出了这个例子背后的数学细节。
- en: Bayes' theorem
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: There is a very general idea in data science that consists of explaining data
    with a mathematical model. This is formalized with a one-way process, *model →
    data*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中有一个非常通用的思想，那就是用数学模型来解释数据。这通过一个单向过程*模型 → 数据*来形式化。
- en: Once this process is formalized, the task of the data scientist is to exploit
    the data to recover information about the model. In other words, we want to *invert*
    the original process and get *data → model*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这个过程被形式化，数据科学家的任务就是利用数据恢复关于模型的信息。换句话说，我们希望*逆转*原始过程并得到*数据 → 模型*。
- en: In a probabilistic setting, the direct process is represented as a **conditional
    probability distribution** *P(data|model)*. This is the probability of observing
    the data when the model is entirely specified.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个概率设置中，直接过程表示为**条件概率分布** *P(data|model)*。这是在模型完全指定的情况下，观察到数据的概率。
- en: Similarly, the inverse process is *P(model|data)*. It gives us information about
    the model (what we're looking for), knowing the observations (what we have).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，逆向过程是*P(model|data)*。它在知道观察结果（我们拥有的数据）的基础上，给我们关于模型（我们所寻找的）的信息。
- en: 'Bayes'' theorem is at the core of a general framework for inverting a probabilistic
    process of *model → data*. It can be stated as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理是一个通用框架的核心，用于逆转*模型 → 数据*的概率过程。它可以表述如下：
- en: '![Bayes'' theorem](img/4818OS_07_09.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯定理](img/4818OS_07_09.jpg)'
- en: This equation gives us information about our model, knowing the observed data.
    Bayes' equation is widely used in signal processing, statistics, machine learning,
    inverse problems, and in many other scientific applications.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程为我们提供了关于模型的信息，前提是我们知道观测数据。贝叶斯方程广泛应用于信号处理、统计学、机器学习、逆问题以及许多其他科学领域。
- en: In Bayes' equation, *P(model)* reflects our prior knowledge about the model.
    Also, *P(data)* is the distribution of the data. It is generally expressed as
    an integral of *P(data|model)P(model)*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, Bayes'' equation gives us a general roadmap for data inference:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Specify a mathematical model for the direct process *model → data* (the *P(data|model)*
    term).
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify a prior probability distribution for the model (*P(model) term*).
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform analytical or numerical calculations to solve this equation.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computation of the posterior distribution
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this recipe''s example, we found the posterior distribution with the following
    equation (deriving directly from Bayes'' theorem):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Computation of the posterior distribution](img/4818OS_07_10.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'Knowing that the *x[i]* are independent, we get (*h* being the number of heads):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Computation of the posterior distribution](img/4818OS_07_11.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'In addition, we can compute analytically the following integral (using an integration
    by parts and an induction):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![Computation of the posterior distribution](img/4818OS_07_12.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we get:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![Computation of the posterior distribution](img/4818OS_07_13.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: Maximum a posteriori estimation
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can get a point estimate from the posterior distribution. For example, the
    **maximum a posteriori** (**MAP**) estimation consists of considering the *maximum*
    of the posterior distribution as an estimate for *q*. We can find this maximum
    analytically or numerically. For more information on MAP, refer to [http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation](http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can get this estimate analytically by deriving the posterior distribution
    with respect to *q*. We get (assuming *1* ![Maximum a posteriori estimation](img/4818OS_08_40.jpg)
    *h* ![Maximum a posteriori estimation](img/4818OS_08_40.jpg) * n-1*):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum a posteriori estimation](img/4818OS_07_14.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: This expression is equal to zero when *q = h/n*. This is the MAP estimate of
    the parameter *q*. This value happens to be the proportion of heads obtained in
    the experiment.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we showed a few basic notions in Bayesian theory. We illustrated
    them with a simple example. The fact that we were able to derive the posterior
    distribution analytically is not very common in real-world applications. This
    example is nevertheless informative because it explains the core mathematical
    ideas behind the complex numerical methods we will see later.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Credible interval
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The posterior distribution indicates the plausible values for *q* given the
    observations. We could use it to derive a **credible interval**, likely to contain
    the actual value. Credible intervals are the Bayesian analog to confidence intervals
    in frequentist statistics. For more information on credible intervals, refer to
    [http://en.wikipedia.org/wiki/Credible_interval](http://en.wikipedia.org/wiki/Credible_interval).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Conjugate distributions
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, the prior and posterior distributions are **conjugate**, meaning
    that they belong to the same family (the beta distribution). For this reason,
    we were able to compute the posterior distribution analytically. You will find
    more details about conjugate distributions at [http://en.wikipedia.org/wiki/Conjugate_prior](http://en.wikipedia.org/wiki/Conjugate_prior).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Non-informative (objective) prior distributions
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We chose a uniform distribution as prior distribution for the unknown parameter
    *q*. It is a simple choice and it leads to tractable computations. It reflects
    the intuitive fact that we do not favor any particular value a priori. However,
    there are rigorous ways of choosing completely uninformative priors (see [http://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors](http://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors)).
    An example is the Jeffreys prior, based on the idea that the prior distribution
    should not depend on the parameterization of the parameters. For more information
    on Jeffreys prior, refer to [http://en.wikipedia.org/wiki/Jeffreys_prior](http://en.wikipedia.org/wiki/Jeffreys_prior).
    In our example, the Jeffreys prior is:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![Non-informative (objective) prior distributions](img/4818OS_07_15.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: See also
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Fitting a Bayesian model by sampling from a posterior distribution with
    a Markov chain Monte Carlo method* recipe
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the correlation between two variables with a contingency table and
    a chi-squared test
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whereas univariate methods deal with single-variable observations, multivariate
    methods consider observations with several features. Multivariate datasets allow
    the study of *relations* between variables, more particularly their correlation
    or lack thereof (that is, independence).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will take a look at the same tennis dataset as in the first
    recipe of this chapter. Following a frequentist approach, we will estimate the
    correlation between the number of aces and the proportion of points won by a tennis
    player.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the *Tennis* dataset on the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data),
    and extract it in the current directory.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import NumPy, pandas, SciPy.stats, and matplotlib:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We load the dataset corresponding to Roger Federer:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Each row corresponds to a match, and the 70 columns contain many player characteristics
    during that match:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, we only look at the proportion of points won, and the (relative) number
    of aces:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![How to do it...](img/4818OS_07_16.jpg)'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: If the two variables were independent, we would not see any trend in the cloud
    of points. On this plot, it is a bit hard to tell. Let's use pandas to compute
    a coefficient correlation.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We create a new `DataFrame` object with only these fields (note that this step
    is not compulsory). We also remove the rows where one field is missing (using
    `dropna()`):'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s compute the Pearson''s correlation coefficient between the relative
    number of aces in the match, and the number of points won:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们计算比赛中A球的相对数量与赢得的点数之间的皮尔逊相关系数：
- en: '[PRE21]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: A correlation of ~0.26 seems to indicate a positive correlation between our
    two variables. In other words, the more aces in a match, the more points the player
    wins (which is not very surprising!).
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 约为0.26的相关性似乎表明我们的两个变量之间存在正相关关系。换句话说，在一场比赛中A球越多，球员赢得的点数就越多（这并不令人惊讶！）。
- en: Now, to determine if there is a *statistically significant* correlation between
    the variables, we use a **chi-squared test** of the independence of variables
    in a **contingency table**.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了确定变量之间是否存在*统计上显著*的相关性，我们使用**卡方检验**来检验**列联表**中变量的独立性。
- en: 'First, we binarize our variables. Here, the value corresponding to the number
    of aces is `True` if the player is serving more aces than usual in a match, and
    `False` otherwise:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将变量二值化。在这里，如果球员在比赛中发球A球比平常多，值为`True`，否则为`False`：
- en: '[PRE22]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we create a contingency table, with the frequencies of all four possibilities
    (True and True, True and False, and so on):'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个列联表，其中包含所有四种可能性（真和真，真和假，依此类推）的频率：
- en: '[PRE23]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we compute the chi-squared test statistic and the associated p-value.
    The null hypothesis is the independence between the variables. SciPy implements
    this test in `scipy.stats.chi2_contingency`, which returns several objects. We''re
    interested in the second result, which is the p-value:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算卡方检验统计量和相关的P值。零假设是变量之间的独立性。SciPy在`scipy.stats.chi2_contingency`中实现了这个测试，返回了几个对象。我们对第二个结果感兴趣，即P值：
- en: '[PRE24]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The p-value is much lower than 0.05, so we reject the null hypothesis and conclude
    that there is a statistically significant correlation between the proportion of
    aces and the proportion of points won in a match (for Roger Federer!).
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: P值远低于0.05，因此我们拒绝零假设，并得出结论：在一场比赛中赢得的点数与赢得的A球比例之间存在统计学上显著的相关性（对于罗杰·费德勒！）。
- en: Tip
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: As always, correlation does not imply causation. Here, it is likely that external
    factors influence both variables. See [http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation](http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation)
    for more details.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如常，相关性并不意味着因果关系。在这里，外部因素很可能影响两个变量。有关更多细节，请参阅[http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation](http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation)。
- en: How it works...
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We give here a few details about the statistical concepts used in this recipe.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提供了一些有关本文中使用的统计概念的细节。
- en: Pearson's correlation coefficient
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数
- en: 'Pearson''s correlation coefficient measures the linear correlation between
    two random variables, *X* and *Y*. It is a normalized version of the covariance:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数衡量了两个随机变量*X*和*Y*之间的线性相关性。它是协方差的归一化版本：
- en: '![Pearson''s correlation coefficient](img/4818OS_07_17.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![皮尔逊相关系数](img/4818OS_07_17.jpg)'
- en: It can be estimated by substituting, in this formula, the expectancy with the
    sample mean, and the variance with the sample variance. More details about its
    inference can be found at [http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过将这个公式中的期望值替换为样本均值，方差替换为样本方差来估计。关于其推断的更多细节可以在[http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient)找到。
- en: Contingency table and chi-squared test
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 列联表和卡方检验
- en: 'The contingency table contains the frequencies *O[ij]* of all combinations
    of outcomes, when there are multiple random variables that can take a finite number
    of values. Under the null hypothesis of independence, we can compute the *expected*
    frequencies *E[ij]*, based on the marginal sums (sums in each row). The chi-squared
    statistic, by definition, is:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列联表包含所有组合结果的频率*O[ij]*，当存在多个随机变量可以取有限数量的值时。在独立性的零假设下，我们可以基于边际和（每行的总和）计算*期望*频率*E[ij]*。卡方统计量的定义如下：
- en: '![Contingency table and chi-squared test](img/4818OS_07_18.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![列联表和卡方检验](img/4818OS_07_18.jpg)'
- en: When there are sufficiently many observations, this variable approximately follows
    a chi-squared distribution (the distribution of the sum of normal variables squared).
    Once we get the p-value, as explained in the *Getting started with statistical
    hypothesis testing – a simple z-test* recipe, we can reject or accept the null
    hypothesis of independence. Then, we can conclude (or not) that there exists a
    significant correlation between the variables.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当观察足够多时，这个变量大致遵循卡方分布（正态变量平方和的分布）。一旦我们得到了p值，就像*开始统计假设检验 - 一个简单的z检验*中所解释的那样，我们可以拒绝或接受独立性的零假设。然后，我们可以得出（或不得出）变量之间存在显著相关性的结论。
- en: There's more...
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'There are many other sorts of chi-squared tests, that is, tests where the test
    statistic follows a chi-squared distribution. These tests are widely used for
    testing the goodness-of-fit of a distribution, or testing the independence of
    variables. More information can be found in the following pages:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他类型的卡方检验，即测试统计量遵循卡方分布的测试。这些测试广泛用于测试分布的拟合度，或测试变量的独立性。更多信息可以在以下页面找到：
- en: Chi2 test in SciPy documentation available at [http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SciPy文档中的Chi2测试可在[http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html)找到
- en: Contingency table introduced at [http://en.wikipedia.org/wiki/Contingency_table](http://en.wikipedia.org/wiki/Contingency_table)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[http://en.wikipedia.org/wiki/Contingency_table](http://en.wikipedia.org/wiki/Contingency_table)介绍的列联表
- en: Chi-squared test introduced at [http://en.wikipedia.org/wiki/Pearson's_chi-squared_test](http://en.wikipedia.org/wiki/Pearson's_chi-squared_test)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[http://en.wikipedia.org/wiki/Pearson's_chi-squared_test](http://en.wikipedia.org/wiki/Pearson's_chi-squared_test)介绍的卡方检验
- en: See also
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Getting started with statistical hypothesis testing – a simple z-test*
    recipe
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开始统计假设检验 - 一个简单的z检验*食谱'
- en: Fitting a probability distribution to data with the maximum likelihood method
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用最大似然方法将概率分布拟合到数据
- en: A good way to explain a dataset is to apply a probabilistic model to it. Finding
    an adequate model can be a job in its own. Once a model is chosen, it is necessary
    to compare it to the data. This is what statistical estimation is about. In this
    recipe, we apply the **maximum likelihood method** on a dataset of survival times
    after heart transplant (1967-1974 study).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 解释数据集的一个好方法是对其应用概率模型。找到一个合适的模型可能是一项工作。选择模型后，有必要将其与数据进行比较。这就是统计估计的内容。在这个食谱中，我们对心脏移植后存活时间（1967-1974年研究）的数据集应用**最大似然方法**。
- en: Getting ready
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As usual in this chapter, a background in probability theory and real analysis
    is recommended. In addition, you need the statsmodels package to retrieve the
    test dataset. For more information on statsmodels, refer to [http://statsmodels.sourceforge.net](http://statsmodels.sourceforge.net).
    On Anaconda, you can install statsmodel with the `conda install statsmodels` command.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章中通常一样，建议具有概率论和实分析背景。此外，您需要statsmodels包来检索测试数据集。有关statsmodels的更多信息，请参考[http://statsmodels.sourceforge.net](http://statsmodels.sourceforge.net)。在Anaconda上，您可以使用`conda
    install statsmodels`命令安装statsmodel。
- en: How to do it...
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'statsmodels is a Python package for conducting statistical data analyses. It
    also contains real-world datasets that we can use when experimenting with new
    methods. Here, we load the *heart* dataset:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: statsmodels是一个用于进行统计数据分析的Python包。它还包含我们在尝试新方法时可以使用的真实数据集。在这里，我们加载*heart*数据集：
- en: '[PRE25]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s take a look at this `DataFrame`:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看这个`DataFrame`：
- en: '[PRE26]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This dataset contains censored and uncensored data: a censor of 0 means that
    the patient was alive at the end of the study, and thus we don''t know the exact
    survival time. We only know that the patient survived *at least* the indicated
    number of days. For simplicity here, we only keep uncensored data (we thereby
    introduce a bias toward patients that did not survive very long after their transplant):'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个数据集包含被审查和未被审查的数据：0的审查意味着患者在研究结束时仍然存活，因此我们不知道确切的存活时间。我们只知道患者至少存活了指定的天数。为简单起见，我们只保留未被审查的数据（这样我们就引入了对未能在移植后存活很长时间的患者的偏见）：
- en: '[PRE27]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s take a look at the data graphically, by plotting the raw survival data
    and the histogram:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过绘制原始存活数据和直方图来以图形方式查看数据：
- en: '[PRE28]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![How to do it...](img/4818OS_07_19.jpg)'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/4818OS_07_19.jpg)'
- en: We observe that the histogram is decreasing very rapidly. Fortunately, the survival
    rates today are much higher (~70 percent after 5 years). Let's try to fit an exponential
    distribution (more information on the exponential distribution is available at
    [http://en.wikipedia.org/wiki/Exponential_distribution](http://en.wikipedia.org/wiki/Exponential_distribution))
    to the data. According to this model, *S* (number of days of survival) is an exponential
    random variable with the parameter ![How to do it...](img/4818OS_07_43.jpg), and
    the observations *s[i]* are sampled from this distribution. Let the sample mean
    be:![How to do it...](img/4818OS_07_20.jpg)The likelihood function of an exponential
    distribution is as follows, by definition (see proof in the next section):![How
    to do it...](img/4818OS_07_21.jpg)
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们观察到直方图正在迅速下降。幸运的是，今天的生存率要高得多（5年后的生存率约为70%）。让我们尝试将指数分布拟合到数据中（关于指数分布的更多信息可以参见[http://en.wikipedia.org/wiki/Exponential_distribution](http://en.wikipedia.org/wiki/Exponential_distribution)）。根据这个模型，*S*（生存天数）是一个带有参数![How
    to do it...](img/4818OS_07_43.jpg)的指数随机变量，观察值*s[i]*是从这个分布中抽样得到的。令样本均值为：![How to
    do it...](img/4818OS_07_20.jpg)指数分布的似然函数如下，按照定义（证明见下节）：![How to do it...](img/4818OS_07_21.jpg)
- en: The **maximum likelihood estimate** for the rate parameter is, by definition,
    the value ![How to do it...](img/4818OS_07_43.jpg) that maximizes the likelihood
    function. In other words, it is the parameter that maximizes the probability of
    observing the data, assuming that the observations are sampled from an exponential
    distribution.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**最大似然估计**对于速率参数的定义是：值![How to do it...](img/4818OS_07_43.jpg)，它最大化了似然函数。换句话说，这是最大化观察到数据的概率的参数，假设这些观察值是从指数分布中抽样得到的。'
- en: 'Here, it can be shown that the likelihood function has a maximum value when
    ![How to do it...](img/4818OS_07_44.jpg), which is the maximum likelihood estimate
    for the rate parameter. Let''s compute this parameter numerically:'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，可以证明当![How to do it...](img/4818OS_07_44.jpg)时，似然函数取得最大值，这就是速率参数的最大似然估计。让我们通过数值方法计算这个参数：
- en: '[PRE29]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To compare the fitted exponential distribution to the data, we first need to
    generate linearly spaced values for the x-axis (days):'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将拟合的指数分布与数据进行比较，我们首先需要为x轴（天数）生成线性间隔的值：
- en: '[PRE30]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can obtain the probability density function of the exponential distribution
    with SciPy. The parameter is the scale, the inverse of the estimated rate.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用SciPy获得指数分布的概率密度函数。参数是尺度，即估计速率的倒数。
- en: '[PRE31]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, let''s plot the histogram and the obtained distribution. We need to rescale
    the theoretical distribution to the histogram (depending on the bin size and the
    total number of data points):'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们绘制直方图和得到的分布。我们需要将理论分布重新缩放到直方图上（这取决于箱子大小和数据点的总数）：
- en: '[PRE32]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![How to do it...](img/4818OS_07_22.jpg)'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![How to do it...](img/4818OS_07_22.jpg)'
- en: The fit is far from perfect. We were able to find an analytical formula for
    the maximum likelihood estimate here. In more complex situations, that is not
    always possible. Thus we may need to resort to numerical methods. SciPy actually
    integrates numerical maximum likelihood routines for a large number of distributions.
    Here, we use this other method to estimate the parameter of the exponential distribution.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 拟合结果远非完美。我们能够找到最大似然估计的解析公式。在更复杂的情况下，这并不总是可能的。因此，我们可能需要求助于数值方法。SciPy实际上集成了针对大量分布的数值最大似然程序。在这里，我们使用这种其他方法来估计指数分布的参数。
- en: '[PRE33]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We can use these parameters to perform a **Kolmogorov-Smirnov test**, which
    assesses the goodness of fit of the distribution with respect to the data. This
    test is based on a distance between the **empirical distribution function** of
    the data and the **cumulative distribution function** (**CDF**) of the reference
    distribution.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用这些参数进行**Kolmogorov-Smirnov检验**，该检验评估分布相对于数据的拟合优度。这个检验是基于数据的**经验分布函数**与参考分布的**累积分布函数**（**CDF**）之间的距离。
- en: '[PRE34]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The second output value is the p-value. Here, it is very low: the null hypothesis
    (stating that the observed data stems from an exponential distribution with a
    maximum likelihood rate parameter) can be rejected with high confidence. Let''s
    try another distribution, the **Birnbaum-Sanders distribution**, which is typically
    used to model failure times. (More information on the Birnbaum-Sanders distribution
    is available at [http://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution](http://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution).)'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This time, the p-value is 0.07, so that we would not reject the null hypothesis
    with a five percent confidence level. When plotting the resulting distribution,
    we observe a better fit than with the exponential distribution:'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![How to do it...](img/4818OS_07_23.jpg)'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we give the calculations leading to the maximum likelihood estimation
    of the rate parameter for an exponential distribution:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_24.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: Here, ![How it works...](img/4818OS_07_45.jpg) is the sample mean. In more complex
    situations, we would require numerical optimization methods in which the principle
    is to maximize the likelihood function using a standard numerical optimization
    algorithm (see [Chapter 9](ch09.html "Chapter 9. Numerical Optimization"), *Numerical
    Optimization*).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the maximum of this function, let''s compute its derivative function
    with respect to ![How it works...](img/4818OS_07_43.jpg):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_25.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: The root of this derivative is therefore ![How it works...](img/4818OS_07_44.jpg)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood on Wikipedia, available at [http://en.wikipedia.org/wiki/Maximum_likelihood](http://en.wikipedia.org/wiki/Maximum_likelihood)
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kolmogorov-Smirnov test on Wikipedia, available at [http://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test](http://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test)
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodness of fit at [http://en.wikipedia.org/wiki/Goodness_of_fit](http://en.wikipedia.org/wiki/Goodness_of_fit)
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The maximum likelihood method is *parametric*: the model belongs to a prespecified
    parametric family of distributions. In the next recipe, we will see a nonparametric
    kernel-based method.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Estimating a probability distribution nonparametrically with a kernel density
    estimation* recipe
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating a probability distribution nonparametrically with a kernel density
    estimation
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we applied a **parametric estimation method**. We had
    a statistical model (the exponential distribution) describing our data, and we
    estimated a single parameter (the rate of the distribution). **Nonparametric estimation**
    deals with statistical models that do not belong to a known family of distributions.
    The parameter space is then *infinite-dimensional* instead of finite-dimensional
    (that is, we estimate *functions* rather than *numbers*).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use a **kernel density estimation** (**KDE**) to estimate the density
    of probability of a spatial distribution. We look at the geographical locations
    of tropical cyclones from 1848 to 2013, based on data provided by the NOAA, the
    US' National Oceanic and Atmospheric Administration.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the *Storms* dataset from the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data),
    and extract it in the current directory. The data was obtained from [www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data](http://www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: You also need matplotlib's toolkit **basemap**, available at [http://matplotlib.org/basemap/](http://matplotlib.org/basemap/).
    With Anaconda, you can install it with conda install basemap. Windows users can
    also find an installer at [www.lfd.uci.edu/~gohlke/pythonlibs/](http://www.lfd.uci.edu/~gohlke/pythonlibs/).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import the usual packages. The kernel density estimation with a Gaussian
    kernel is implemented in SciPy.stats:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s open the data with pandas:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The dataset contains information about most storms since 1848\. A single storm
    may appear multiple times across several consecutive days.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We use pandas'' `groupby()` function to obtain the average location of every
    storm:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We display the storms on a map with basemap. This toolkit allows us to easily
    project the geographical coordinates on the map.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![How to do it...](img/4818OS_07_26.jpg)'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'To perform the kernel density estimation, we stack the `x` and `y` coordinates
    of the storms into a `(2, N)` array:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `gaussian_kde()` routine returned a Python function. To see the results
    on a map, we need to evaluate this function on a 2D grid spanning the entire map.
    We create this grid with `meshgrid()`, and we pass the `x` and `y` values to the
    `kde` function. `kde` accepts a `(2, N)` array as input, requiring us to tweak
    the shape of the array:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we display the estimated density with `imshow()`:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![How to do it...](img/4818OS_07_27.jpg)'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **kernel density estimator** of a set of *n* points *{x[i]}* is given as:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_28.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: Here, *h>0* is a scaling parameter (the **bandwidth**) and *K(u)* is the **kernel**,
    a symmetric function that integrates to 1\. This estimator is to be compared with
    a classical histogram, where the kernel would be a *top-hat* function (a rectangle
    function taking its values in *{0,1}*), but the blocks would be located on a regular
    grid instead of the data points. For more information on kernel density estimator,
    refer to [http://en.wikipedia.org/wiki/Kernel_density_estimation](http://en.wikipedia.org/wiki/Kernel_density_estimation).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Multiple kernels can be chosen. Here, we chose a **Gaussian kernel**, so that
    the KDE is the superposition of Gaussian functions centered on all the data points.
    It is an estimation of the density.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of the bandwidth is not trivial; there is a tradeoff between a too
    low value (small bias, high variance: overfitting) and a too high value (high
    bias, small variance: underfitting). We will return to this important concept
    of **bias-variance tradeoff** in the next chapter. For more information on the
    bias-variance tradeoff, refer to [http://en.wikipedia.org/wiki/Bias-variance_dilemma](http://en.wikipedia.org/wiki/Bias-variance_dilemma).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The following figure illustrates the KDE. The dataset contains four points in
    *[0,1]* (black lines). The estimated density is a smooth curve, represented here
    with multiple bandwidth values.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_29.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
- en: Kernel density estimation
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are other KDE implementations in statsmodels and scikit-learn. You can
    find more information at [http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/](http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Fitting a probability distribution to data with the maximum likelihood
    method* recipe
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting a Bayesian model by sampling from a posterior distribution with a Markov
    chain Monte Carlo method
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we illustrate a very common and useful method for characterizing
    a posterior distribution in a Bayesian model. Imagine that you have some data
    and you want to obtain information about the underlying random phenomenon. In
    a frequentist approach, you could try to fit a probability distribution within
    a given family of distributions, using a parametric method such as the maximum
    likelihood method. The optimization procedure would yield parameters that maximize
    the probability of observing the data if given the null hypothesis.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: In a Bayesian approach, you consider the parameters themselves as random variables.
    Their prior distributions reflect your initial knowledge about these parameters.
    After the observations, your knowledge is updated, and this is reflected in the
    posterior distributions of the parameters.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: A typical goal for Bayesian inference is to characterize the posterior distributions.
    Bayes' theorem gives an analytical way to do this, but it is often impractical
    in real-world problems due to the complexity of the models and the number of dimensions.
    A **Markov chain** **Monte Carlo** method, such as the **Metropolis-Hastings algorithm**,
    gives a numerical method to approximate a posterior distribution.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Here, we introduce the **PyMC** package, which gives an effective and natural
    interface for fitting a probabilistic model to data in a Bayesian framework. We
    will look at the annual frequency of storms in the northern Atlantic Ocean since
    the 1850s using data from NOAA, the US' National Oceanic and Atmospheric Administration.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: This recipe is largely inspired by a tutorial on PyMC's website (see the link
    in the *There's more…* section).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can find the instructions to install PyMC on the package's website. In this
    recipe, we will use PyMC2\. The new version (PyMC3) is still in development at
    the time of writing, and it is likely to be significantly different. For more
    information on PyMC, refer to [http://pymc-devs.github.io/pymc/](http://pymc-devs.github.io/pymc/).
    With Anaconda, you can try `conda install -c https://conda.binstar.org/pymc pymc`.
    Windows users can also find an installer at [www.lfd.uci.edu/~gohlke/pythonlibs/](http://www.lfd.uci.edu/~gohlke/pythonlibs/).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: You also need to download the *Storms* dataset from the book's GitHub repository
    at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data)
    and extract it in the current directory.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import the standard packages and PyMC:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let''s import the data with pandas:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'With pandas, it only takes a single line of code to get the annual number of
    storms in the North Atlantic Ocean. We first select the storms in that basin (`NA`),
    then we group the rows by year (`Season`), and then we take the number of unique
    storms (`Serial_Num`), as each storm can span several days (the `nunique()` method):'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![How to do it...](img/4818OS_07_30.jpg)'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Now, we define our probabilistic model. We assume that storms arise following
    a time-dependent Poisson process with a deterministic rate. We assume that this
    rate is a piecewise-constant function that takes a first value `early_mean` before
    a switch point `switchpoint`, and a second value `late_mean` after that point.
    These three unknown parameters are treated as random variables (we will describe
    them more in the *How it works…* section).
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Poisson process ([http://en.wikipedia.org/wiki/Poisson_process](http://en.wikipedia.org/wiki/Poisson_process))
    is a particular **point process**, that is, a stochastic process describing the
    random occurrence of instantaneous events. The Poisson process is fully random:
    the events occur independently at a given rate. See also [Chapter 13](ch13.html
    "Chapter 13. Stochastic Dynamical Systems"), *Stochastic Dynamical Systems*.'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We define the piecewise-constant rate as a Python function:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Finally, the observed variable is the annual number of storms. It follows a
    Poisson variable with a random mean (the rate of the underlying Poisson process).
    This fact is a known mathematical property of Poisson processes.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, we use the MCMC method to sample from the posterior distribution, given
    the observed data. The `sample()` method launches the fitting iterative procedure:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Let's plot the sampled Markov chains. Their stationary distribution corresponds
    to the posterior distribution we want to characterize.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![How to do it...](img/4818OS_07_31.jpg)'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'We also plot the distribution of the samples, which correspond to the posterior
    distributions of our parameters, after the data points have been taken into account:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![How to do it...](img/4818OS_07_32.jpg)'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Taking the sample mean of these distributions, we get posterior estimates for
    the three unknown parameters, including the year where the frequency of storms
    suddenly increased:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, we can plot the estimated rate on top of the observations:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![How to do it...](img/4818OS_07_33.jpg)'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The general idea is to define a Bayesian probabilistic model and to fit it
    to the data. This model may be the starting point of an estimation or decision
    task. The model is essentially described by stochastic or deterministic variables
    linked together within a **direct acyclic graph** (**DAG**). *A* is linked to
    *B* if *B* is entirely or partially determined by *A*. The following figure shows
    the graph of the model used in this recipe:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![How it works...](img/4818OS_07_34.jpg)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
- en: Tip
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you can see, PyMC can create graph representations of the models. You need
    to install GraphViz (refer to [www.graphviz.org](http://www.graphviz.org)), pydot,
    and pyparsing. Because of an unfortunate bug, you might need to install a specific
    version of pyparsing:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Stochastic variables follow distributions that can be parameterized by fixed
    numbers or other variables in the model. Parameters may be random variables themselves,
    reflecting knowledge prior to the observations. This is the core of Bayesian modeling.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the analysis is to include the observations into the model in order
    to update our knowledge as more and more data is available. Although Bayes' theorem
    gives us an exact way to compute those posterior distributions, it is rarely practical
    in real-world problems. This is notably due to the complexity of the models. Alternatively,
    numerical methods have been developed in order to tackle this problem.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: The **Markov chain Monte Carlo** (**MCMC**) method used here allows us to sample
    from a complex distribution by simulating a Markov chain that has the desired
    distribution as its equilibrium distribution. The **Metropolis-Hastings algorithm**
    is a particular application of this method to our current example.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is implemented in the `MCMC` class in PyMC. The `burn` parameter
    determines how many initial iterations are thrown away. This is necessary because
    it takes a number of iterations for the Markov chain to converge to its equilibrium
    distribution. The `thin` parameter corresponds to the number of steps to skip
    in the evaluation of the distribution so as to minimize the autocorrelation of
    the samples. You will find more information at [http://pymc-devs.github.io/pymc/modelfitting.html](http://pymc-devs.github.io/pymc/modelfitting.html).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: A great PyMC tutorial that we largely took inspiration from is available at
    [http://pymc-devs.github.io/pymc/tutorial.html](http://pymc-devs.github.io/pymc/tutorial.html)
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A must-read free e-book on the subject, by Cameron Davidson-Pilon, entirely
    written in the IPython notebook, available at [http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Markov chain Monte Carlo method introduced at [http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Metropolis-Hastings algorithm introduced at [http://en.wikipedia.org/wiki/Metropolis-Hastings_algorithm](http://en.wikipedia.org/wiki/Metropolis-Hastings_algorithm)
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Getting started with Bayesian methods* recipe
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing data with the R programming language in the IPython notebook
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R ([www.r-project.org](http://www.r-project.org)) is a free domain-specific
    programming language for statistics. Its syntax is well-adapted to statistical
    modeling and data analysis. By contrast, Python's syntax is typically more convenient
    for general-purpose programming. Luckily, IPython allows you to have the best
    of both worlds. For example, you can insert R code snippets anywhere in a normal
    IPython notebook. You can continue using Python and pandas for data loading and
    wrangling, and switch to R to design and fit statistical models. Using R instead
    of Python for these tasks is more than a matter of programming syntax; R comes
    with an impressive statistical toolbox that is still unmatched by Python.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will show how to use R from IPython, and we illustrate the
    most basic capabilities of R with a simple data analysis example.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need the statsmodels package for this recipe. You can find installation
    instructions in the previous recipe, *Fitting a probability distribution to data
    with the maximum likelihood method*.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: You also need R. There are three steps to use R from IPython. First, install
    R and rpy2 (R to Python interface). Of course, you only need to do this step once.
    Then, to use R in an IPython session, you need to load the IPython R extension.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Download R for your operating system from [http://cran.r-project.org/mirrors.html](http://cran.r-project.org/mirrors.html)
    and install it. On Ubuntu, you can do `sudo apt-get install r-base-dev`.
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download rpy2 from [http://rpy.sourceforge.net/rpy2.html](http://rpy.sourceforge.net/rpy2.html)
    and install it. With Anaconda on Linux, you can try `conda install -c https://conda.binstar.org/r
    rpy2`. Alternatively, you can do `pip install rpy2`.
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, to execute R code in an IPython notebook, execute `%load_ext rmagic` first.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: rpy2 does not appear to work well on Windows. We recommend using Linux or Mac
    OS X.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will use the following workflow: first, we load data from Python.
    Then, we use R to design and fit a model, and to make some plots in the IPython
    notebook. We could also load data from R, or design and fit a statistical model
    with Python''s statsmodels package, and so on. In particular, the analysis we
    do here could be done entirely in Python, without resorting to the R language.
    This recipe merely shows the basics of R and illustrates how R and Python can
    play together within an IPython session.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the *longley* dataset with the statsmodels package. This dataset
    contains a few economic indicators in the US from 1947 to 1962\. We also load
    the IPython R extension:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We define `x` and `y` as the exogeneous (independent) and endogenous (dependent)
    variables, respectively. The endogenous variable quantifies the total employment
    in the country.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'For convenience, we add the endogenous variable to the `x` DataFrame:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We will make a simple plot in R. First, we need to pass Python variables to
    R. We can use the `%R -i var1,var2` magic. Then, we can call R''s `plot()` command:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![How to do it...](img/4818OS_07_35.jpg)'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now that the data has been passed to R, we can fit a linear model to the data.
    The `lm()` function lets us perform a linear regression. Here, we want to express
    `totemp` (total employment) as a function of the country''s GNP:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![How to do it...](img/4818OS_07_36.jpg)'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `-i` and `-o` options of the `%R` magic allow us to pass variables back
    and forth between IPython and R. The variable names need to be separated by commas.
    You can find more information about the `%R` magic in the documentation available
    at [http://rpy.sourceforge.net/](http://rpy.sourceforge.net/).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, the tilde (`~`) expresses the dependence of a dependent variable upon
    one or several independent variables. The `lm()` function allows us to fit a simple
    linear regression model to the data. Here, `totemp` is expressed as a function
    of `gnp`:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_37.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
- en: Here, *b* (intercept) and *a* are the coefficients of the linear regression
    model. These two values are returned by `fit$coefficients` in R, where `fit` is
    the fitted model.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'Our data points do not satisfy this relation exactly, of course. The coefficients
    are chosen so as to minimize the error between this linear prediction and the
    actual values. This is typically done by minimizing the following least squares
    error:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_07_38.jpg)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
- en: 'The data points are *(gnp[i]**, totemp[i])* here. The coefficients *a* and
    *b* that are returned by `lm()` make this sum minimal: they fit the data best.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Regression is an important statistical concept that we will see in greater
    detail in the next chapter. Here are a few references:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis on Wikipedia, available at [http://en.wikipedia.org/wiki/Regression_analysis](http://en.wikipedia.org/wiki/Regression_analysis)
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least squares method on Wikipedia, available at [en.wikipedia.org/wiki/Linear_least_squares_(mathematics)](http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics))
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R is an excellent platform for advanced statistics. Python has a few statistical
    packages such as pandas and statsmodels that implement many common features, but
    the number of statistical toolboxes in R remains unmatched by Python at this time.
    Yet, Python has a much wider range of possibilities outside of statistics and
    is an excellent general-purpose language that comes with an impressive number
    of various packages.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the multilanguage capabilities of IPython, you don't necessarily have
    to choose between those languages. You can keep using Python and switch to R when
    you need highly specific statistical features that are still missing in Python.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few references about R:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to R available at [http://cran.r-project.org/doc/manuals/R-intro.html](http://cran.r-project.org/doc/manuals/R-intro.html)
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R tutorial available at [www.cyclismo.org/tutorial/R/](http://www.cyclismo.org/tutorial/R/)
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRAN, or Comprehensive R Archive Network, containing many packages for R, available
    at [http://cran.r-project.org](http://cran.r-project.org)
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IPython and R tutorial available at [http://nbviewer.ipython.org/github/ipython/ipython/blob/master/examples/Builtin%20Extensions/R%20Magics.ipynb](http://nbviewer.ipython.org/github/ipython/ipython/blob/master/examples/Builtin%20Extensions/R%20Magics.ipynb)
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Exploring a dataset with pandas and matplotlib* recipe
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
