- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normalization and Standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature scaling, normalization, and standardization are essential preprocessing
    steps that help ensure that machine learning models can effectively learn from
    data. These techniques address issues related to numerical stability, algorithm
    convergence, model performance, and more, ultimately contributing to better, more
    reliable results in data analysis and machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will dive deep into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling features to a range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z-score scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robust scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find all the code for this chapter at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter09](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: The different code files follow the names of the different sections of the chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling features to a range
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature scaling is a preprocessing technique in machine learning that rescales
    the *range* of independent variables or features of a dataset. It’s used to ensure
    that all features contribute equally to the model training process by bringing
    them to a common scale. Feature scaling is particularly important for algorithms
    that are sensitive to the scale of input features, such as k-nearest neighbors
    and gradient descent-based optimization algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When scaling features, we’re changing the range of the distribution of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s present an example to make the concept of feature scaling easier to grasp.
    Let’s suppose you’re working on a machine learning project to predict housing
    prices based on various features of the houses, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Square footage (in square feet)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance to the nearest school (in miles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance to the nearest public transportation stop (miles)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s discuss why feature scaling is important in this context. The **Square
    footage** feature could range from hundreds to thousands of square feet. The **Distance
    to the nearest school** and **Distance to the nearest public transportation stop**
    features might range from a fraction of a mile to several miles. If you don’t
    scale these features, the algorithm may give excessive importance to the larger
    values, making square footage the dominant factor in predicting house prices.
    Features such as Distance to the nearest school might be unfairly downplayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For all the sections in this chapter, we will use the above example to showcase
    the different scaling methods. Let’s go through the data creation for this example;
    the code can be found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter09/min_max_scaling.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter09/min_max_scaling.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create a dataset with features related to housing prices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We’ll create the following features that affect the price of a house:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Square footage in square feet:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Distance to the nearest school in miles:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Commute distance to work in miles:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Traffic density (skewed feature):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create a DataFrame that holds all the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we plot the original distributions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see the original distributions of the data here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Distribution of house pricing prediction use case](img/B19801_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Distribution of house pricing prediction use case
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s display the statistics of our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will print the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Original dataset statistics](img/B19801_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Original dataset statistics
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s discuss one of the most common methods for scaling – **min-max scaling**.
  prefs: []
  type: TYPE_NORMAL
- en: Min-max scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Min-max scaling, also known as **normalization**, scales the values of a variable
    to a specific range, typically between 0 and 1\. Min-max scaling is useful when
    you want to ensure that all values in a variable fall within a standardized range,
    making them directly comparable. It is commonly employed when the distribution
    of the variable is not assumed to be normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the formula for calculating min-max scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: X _ scaled =(X − X _ min) / (X _ max − X _ min)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the formula, min-max scaling preserves the relative ordering
    of values but *compresses them into a specific range*. One thing to note here
    is that it is *not a way to deal with outliers* and if outliers exist in the data,
    these extreme values can disproportionately influence the scaling. So, it is a
    good practice to deal with outliers first and then proceed to the scaling of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scaling to a specific range is a suitable approach when the following conditions
    are satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: You have prior knowledge of the approximate upper and lower bounds of your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your data follows a relatively uniform or bell-shaped distribution across that
    range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your chosen machine learning algorithm or model benefits from having features
    within a specific range, typically `[0, 1]` or any other desired range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A classic example of this scenario is age. Age values typically span from 0
    to 90 and the entire range includes a significant number of individuals. However,
    scaling something such as income is not really recommended as there are a limited
    number of individuals with exceptionally high incomes. If you were to apply linear
    scaling to income, the upper limit of the scale would become exceedingly high,
    and most data points would be concentrated in a small segment of the scale, leading
    to loss of information and skewed representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the code for *the house pricing prediction use case* we
    discussed previously to understand how a min-max scaler can transform the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll scale the data using `MinMaxScaler()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can display the dataset statistics after scaling using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot and observe the distributions after scaling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s have a look at the modified data distributions after the scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Distribution of house pricing prediction use case after min-max
    scaling](img/B19801_09_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Distribution of house pricing prediction use case after min-max
    scaling
  prefs: []
  type: TYPE_NORMAL
- en: We applied min-max scaling to transform each column into a standardized range
    between 0 and 1\. The shape of the original feature distributions remained the
    same after scaling because min-max scaling maintains the relative distances between
    data points. The scaling had a normalization effect on the data, bringing all
    features to a common scale. This is important when features have different units
    or ranges, preventing one feature from dominating others.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If your dataset is sparse (contains many zero values), min-max scaling may not
    be appropriate, as it can result in loss of information. Alternative methods such
    as **MaxAbsScaler** or robust scalers may be considered.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will discuss z-score scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Z-score scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Z-score scaling, also known as standardization, is applied when you want to
    transform your data to have a mean of 0 and a standard deviation of 1\. Z-score
    scaling is widely used in statistical analysis and machine learning, especially
    when algorithms such as k-means clustering or **Principal Component Analysis**
    (**PCA**) are employed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the formula for z-score:'
  prefs: []
  type: TYPE_NORMAL
- en: X _ scaled =(X − mean(X)) / std(X)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue with the house pricing prediction use case to showcase the z-score
    scaling. The code can be found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter09/zscaler.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter09/zscaler.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first perform z-score scaling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we print the dataset statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we visualize the distributions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s have a look at the modified data distributions after the scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Distribution of house pricing prediction use case after Z-scaling](img/B19801_09_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Distribution of house pricing prediction use case after Z-scaling
  prefs: []
  type: TYPE_NORMAL
- en: The mean of each feature after scaling is very close to 0, which is expected
    in z-score scaling. The data is now centered around 0\. The standard deviation
    for each feature is now approximately 1, making the scales comparable. The minimum
    and maximum values have been transformed, maintaining the relative distribution
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: When to use Z-score scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s also discuss some considerations regarding when to use z-score scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: Z-score scaling assumes that the data is approximately normally distributed,
    or at least symmetrically distributed, around a central mean value. If your data
    is highly skewed or has a non-standard distribution, standardization may not be
    as effective in making the data more Gaussian. As you can see, the `Commute_Distance`
    and `Traffic_Density` features are skewed and the z-score scaling was not that
    successful as the data is not centered around the mean compared to the rest of
    the features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is most applicable to numerical features rather than categorical or ordinal
    ones. Ensure that the data you are standardizing is quantitative in nature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extreme outliers can have a significant impact on the mean and standard deviation,
    which are used in z-score scaling. It’s important to address outliers before standardization,
    as they can distort the scaling effect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z-score scaling assumes a linear relationship between variables. If the underlying
    relationship is non-linear, other scaling methods or transformations may be more
    appropriate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z-score scaling assumes that the variables are independent or at least not highly
    correlated. If variables are highly correlated, standardization may not provide
    additional benefits, and correlation structure should be considered separately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z-score scaling alters the original units of the data, which can affect the
    interpretability of results. Consider whether maintaining the original units is
    essential for your analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For small datasets, the impact of z-score scaling can be more pronounced. Be
    cautious when applying standardization to very small datasets, as it can potentially
    overemphasize the effects of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Robust scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Robust scaling, also known as **robust standardization**, is a method of feature
    scaling that is particularly useful when dealing with datasets containing outliers.
    Unlike min-max scaling and z-score scaling, which can be sensitive to outliers,
    robust scaling is designed to be robust in the presence of extreme values. It
    is especially beneficial when you want to normalize or standardize features while
    minimizing the impact of extreme values. Robust scaling is also suitable for datasets
    where the features do not follow a normal distribution and may have skewness or
    heavy tails.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the formula for robust scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: X _ scaled = (X − median) / IQR
  prefs: []
  type: TYPE_NORMAL
- en: Subtracting the median and dividing by the **Interquartile Range** (**IQR**)in
    the scaling process normalizes the data by centering it around the median and
    scaling it based on the spread represented by the IQR. This normalization helps
    to mitigate the impact of extreme values, making the scaled values more representative
    of the overall distribution.
  prefs: []
  type: TYPE_NORMAL
- en: As we have already discussed in the previous chapter, the median being the middle
    value when the data is ordered makes robust scaling less sensitive to extreme
    values or outliers than other scaling methods that rely on the mean. In addition,
    the IQR, representing the range between the first quartile (Q1) and the third
    quartile (Q3), is also robust against outliers. Unlike the full range or standard
    deviation, the IQR focuses on the middle 50% of the data, making it less affected
    by extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how to apply robust scaling using Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s have a look at the modified data distributions after the scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Distribution of house pricing prediction use case after robust
    scaling](img/B19801_09_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Distribution of house pricing prediction use case after robust
    scaling
  prefs: []
  type: TYPE_NORMAL
- en: After the robust scaling exercise, we can observe that the central tendency
    of the data has shifted, and that the mean of each feature is now closer to zero.
    This is because the robust scaling process subtracts the median. As for the spread
    of the data, it changed as a result of dividing by the IQR. The variability in
    each feature is now represented in a more consistent manner, robust to outliers.
    The range of values for each feature is now compressed, particularly for features
    with larger initial ranges, preventing dominance by features with extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: To close the chapter, we have created a summary table presenting all the techniques
    discussed so far, including information on when to use them, as well as the advantages
    and disadvantages of each.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison between methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chart provides guidance on which scaling technique is suitable for various
    data scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Scaling method** | **When** **to use** | **Pros** | **Cons** |'
  prefs: []
  type: TYPE_TB
- en: '| **Min-max scaling** | Features have a clear, known rangeNormal distribution
    is assumedData does not contain outliers | Simple and easy to understandPreserves
    relative relationshipsMemory efficient | Sensitive to outliers |'
  prefs: []
  type: TYPE_TB
- en: '| **Z-score scaling** | Data follows a normal distributionNo strong assumptions
    about the rangeHandling outliers is not a priority | Standardizes data to zero
    mean and deviation of 1 | Sensitive to outliersMay not be suitable for skewed
    data |'
  prefs: []
  type: TYPE_TB
- en: '| **Robust scaling** | Data contains outliersSkewed distributions or non-normal
    dataEqualizing feature contributionsResilience to varying feature variances |
    Less sensitive to outliersPreserves central tendency and spread | Computationally
    more expensive |'
  prefs: []
  type: TYPE_TB
- en: Table 9.1 – Comparing different scaling techniques
  prefs: []
  type: TYPE_NORMAL
- en: Computational complexity
  prefs: []
  type: TYPE_NORMAL
- en: Min-max scaling tends to be more memory-efficient, especially when dealing with
    large datasets. This is because min-max scaling only involves scaling the values
    based on the minimum and maximum values of each feature, and the computation is
    relatively straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, z-score scaling and robust scaling both require additional
    calculations such as mean, standard deviation (for z-score scaling), median, and
    interquartile range (for robust scaling), which may involve more memory usage.
    The computational complexity and memory requirements of z-score scaling and robust
    scaling can become more pronounced, particularly when dealing with large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s summarize our learning for this chapter and get inspired for
    the next one.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored three common methods for scaling numerical features:
    min-max scaling, z-score scaling, and robust scaling. Min-max scaling transforms
    data to a specific range, making it suitable for algorithms sensitive to feature
    magnitudes. Z-score scaling standardizes data to zero mean and unit variance,
    providing a standardized distribution. Robust scaling, robust to outliers, employs
    the median and interquartile range, making it suitable for datasets with skewed
    distributions or outliers. We also went through different considerations to keep
    in mind while deciding on the best approach for your use case.'
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we’ll shift our focus to handling categorical features in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
