- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reshaping DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with data is hard. Rarely, if ever, can you just collect data and have
    it immediately yield insights. Often, significant time and effort must be put
    into cleansing, transforming, and *reshaping* your data to get it into a format
    that is usable, digestible, and/or understandable.
  prefs: []
  type: TYPE_NORMAL
- en: Is your source data a collection of CSV files, where each file represents a
    different day? Proper use of `pd.concat` will help you take those files and combine
    them into one with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Does the relational database you use as a source store data in a normalized
    form, while the target columnar database would prefer to ingest data all in one
    table? `pd.merge` can help you combine your data together.
  prefs: []
  type: TYPE_NORMAL
- en: What if your boss asks you to take millions of rows of data and, from that,
    produce a nice summary report that anyone in the business can understand? `pd.pivot_table`
    is the right tool for the job here, allowing you to quickly and easily summarize
    your data.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the reasons why you need to reshape your data come from different
    places. Whether your requirements are driven by systems or people, pandas can
    help you manipulate data as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we will walk you through the functions and methods
    that pandas offers to reshape your data. Equipped with the proper knowledge and
    some creativity, reshaping with pandas can be one of the most fun and rewarding
    parts of your analytical process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Concatenating `pd.DataFrame` objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merging DataFrames with `pd.merge`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joining DataFrames with `pd.DataFrame.join`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshaping with `pd.DataFrame.stack` and `pd.DataFrame.unstack`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshaping with `pd.DataFrame.melt`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshaping with `pd.wide_to_long`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshaping with `pd.DataFrame.pivot` and `pd.pivot_table`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshaping with `pd.DataFrame.explode`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transposing with `pd.DataFrame.T`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concatenating pd.DataFrame objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term *concatenation* in pandas refers to the process of taking two or more
    `pd.DataFrame` objects and stacking them in some manner. Most commonly, users
    in pandas perform what we would consider to be *vertical* concatenation, which
    places the `pd.DataFrame` objects on top of one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Vertical concatenation of two pd.DataFrame objects'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, pandas also has the flexibility to take your `pd.DataFrame` objects
    and stack them side by side, through a process called *horizontal* concatenation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a table  Description automatically generated](img/B31091_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Vertical concatenation of two pd.DataFrame objects'
  prefs: []
  type: TYPE_NORMAL
- en: These figures may provide you with a good grasp of what concatenation is all
    about, but there are some potential issues to consider. What should happen if
    we try to concatenate vertically, but our column labels are not the same across
    all of the objects? On the flip side, what should happen if we try to concatenate
    horizontally, and not all of the row labels are the same?
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the direction along which you would like to concatenate, and regardless
    of how your labels may or may not align, concatenation in pandas is controlled
    entirely through the `pd.concat` function. This recipe will walk you through the
    basics of `pd.concat`, while showing you how you can control its behavior when
    you aren’t always working with like-labeled `pd.DataFrame` objects.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s imagine we have collected data about the stock performance of various
    companies across two different quarters. To best showcase how concatenation works,
    we have intentionally made it so that the two `pd.DataFrame` objects cover different
    time periods, show different companies, and even contain different columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The most basic call to `pd.concat` would accept both of these `pd.DataFrame`
    objects in a list. By default, this will stack the objects vertically, i.e., the
    first `pd.DataFrame` is simply stacked on top of the second.
  prefs: []
  type: TYPE_NORMAL
- en: 'While most of the columns in our `pd.DataFrame` objects overlap, `df_q1` does
    not have a `close` column, whereas `df_q2` does. To still make the concatenation
    work, pandas will include the *close* column in the result of `pd.concat`, assigning
    a missing value to the rows that came from `df_q1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You should also take note of the row index that pandas gives in the result.
    In essence, pandas takes the index values of `df_q1`, which range from 0–2, and
    then takes the index values of `df_q2`, which range from 0–3\. When creating the
    new row index, pandas simply retains those values, stacking them vertically in
    the result. If you do not care for that behavior, you can pass in `ignore_index=True`
    to `pd.concat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Another potential issue is that we can no longer see which `pd.DataFrame` our
    records originally come from. To retain that information, we can pass through
    a `keys=` argument, providing custom labels to denote the source of our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`pd.concat` also allows you to control the direction in which things are being
    concatenated. Instead of the default behavior to stack vertically, we can pass
    `axis=1` to see things stacked horizontally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: While this gave us back a result without error, a closer inspection of the result
    reveals some issues. The first two rows of data cover both `AAPL` and `MSFT`,
    respectively, so there is no cause for concern there. However, the third row of
    data shows `AMZN` as the Q1 ticker and `IBM` as the Q2 ticker – what gives?
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is that pandas is aligning on the values of the index, and not
    on any other column like `ticker`, which is what we are probably interested in.
    If we wanted `pd.concat` to align by the `ticker`, we could set that as the row
    index of the two `pd.DataFrame` objects before concatenation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'One last thing we might want to control about the alignment behavior is how
    it treats labels that appear in at least one, but not all, of the objects being
    concatenated. By default, `pd.concat` performs an “outer” join, which will take
    all of the index values (in our case, the `ticker` symbols) and show them in the
    output, using a missing value indicator where applicable. Passing `join="inner"`
    as an argument, by contrast, will only show index labels that appear in all of
    the objects being concatenated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`pd.concat` is an expensive operation, and should never be called from within
    a Python loop. If you create a bunch of `pd.DataFrame` objects within a loop and
    eventually do want to concatenate them together, you are better off storing them
    in a sequence first, only calling `pd.concat` once after the sequence has been
    fully populated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the IPython `%%time` magic function to profile the difference in
    approaches. Let’s start with the anti-pattern of using `pd.concat` within a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will yield the equivalent result but follows the practice of appending
    to a Python list during the loop, and only calling `pd.concat` once at the very
    end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Merging DataFrames with pd.merge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another common task in reshaping data is referred to as *merging*, or in some
    cases, *joining*, with the latter term being used frequently in database terminology.
    Where concatenation “stacks” objects on top of or next to one another, a *merge*
    works by finding a common key (or set of keys) between two entities and using
    that to blend other columns from the entities together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a number  Description automatically generated](img/B31091_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Merging two pd.DataFrame objects'
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used method in pandas to perform merges is `pd.merge`, whose
    functionality will be covered throughout this recipe. Another viable, though less
    commonly used, `pd.DataFrame.join` method can be used as well, although knowing
    `pd.merge` first is helpful before discussing that (we will cover `pd.DataFrame.join`
    in the next recipe).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s continue along with the stock `pd.DataFrame` objects we created in the
    *Concatenating pd.DataFrame objects* recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In one of the examples in that recipe, we saw how you could use a combination
    of `pd.concat` and `pd.DataFrame.set_index` to merge our two `pd.DataFrame` objects
    by the `ticker` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'With `pd.merge`, you can express this much more succinctly by passing an argument
    to `on=`, which clarifies the column(s) you would like pandas to use for alignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the result is not exactly the same, but we can get a little
    closer by toggling the merge behavior. By default, `pd.merge` performs an *inner*
    merge; if we wanted a result more similar to our `pd.concat` example, we could
    pass `how="outer"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'While `pd.concat` only allows you to perform *inner* or *outer* merges, `pd.merge`
    additionally supports *left* merges, which retain all data from the first `pd.DataFrame`,
    merging in data from the second `pd.DataFrame` as key fields can be matched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`how="right"` reverses that, ensuring that every row from the second `pd.DataFrame`
    is represented in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'An additional feature when using `how="outer"` is the ability to provide an
    `indicator=` argument, which will tell you where each row in the resulting `pd.DataFrame`
    was sourced from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: A value of “both” means that the key(s) used to perform the merge were found
    in both `pd.DataFrame` objects, which you can see is applicable to the `AAPL`
    and `MSFT` tickers. A value of `left_only` means the key(s) only appeared in the
    left `pd.DataFrame`, as is the case for `AMZN`. `right_only` highlights key(s)
    that only appeared in the right `pd.DataFrame`, like `GE` and `IBM`.
  prefs: []
  type: TYPE_NORMAL
- en: Another difference between our `pd.concat` output and what we get with `pd.merge`
    is that the former generates a `pd.MultiIndex` in the columns, essentially preventing
    any clashes from column labels that appear in both `pd.DataFrame` objects. `pd.merge`,
    by contrast, appends a suffix to columns that appear in both of the `pd.DataFrame`
    objects to disambiguate. The column coming from the left `pd.DataFrame` will be
    suffixed with `_x`, whereas a suffix of `_y` indicates that the column came from
    the right `pd.DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more control over this suffix, you can pass a tuple argument to `suffixes=`.
    With our sample data, this argument can be used to easily identify Q1 versus Q2
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'However, you should be aware that the suffixes are only applied if the column
    name appears in both `pd.DataFrame` objects. If a column only appears in one but
    not both objects, no suffix will be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'If our key column(s) has different names in the two `pd.DataFrame` objects,
    would that be a problem? Of course not! No need to take my word for it though
    – let’s just rename the `ticker` column in one of our `pd.DataFrame` objects to
    `SYMBOL`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'With `pd.merge`, the only thing that changes is that you now need to pass two
    different arguments to `left_on=` and `right_on=`, instead of just one argument
    to `on=`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To finish off this recipe, let’s consider a case where there are multiple columns
    that should comprise our merge key. We can start down this path by creating one
    `pd.DataFrame` that lists out the ticker, quarter, and low price:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'A second `pd.DataFrame` will also contain the ticker and quarter (albeit with
    different names), but will show the highs instead of the lows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'With the layout of these `pd.DataFrame` objects, our key field now becomes
    the combination of the ticker and the quarter. By passing the appropriate labels
    as arguments to `left_on=` and `right_on=`, pandas is still able to perform this
    merge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An extra consideration when trying to merge data is the uniqueness of the key(s)
    in both `pd.DataFrame` objects. Having a poor or incorrect understanding of this
    can lead to very hard-to-detect errors appearing in your applications. Fortunately,
    `pd.merge` can help detect these issues upfront.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate what we mean when we talk about uniqueness, highlight the issues
    it can cause, and show you how to solve them with pandas, let’s start with a small
    `pd.DataFrame` that shows hypothetical sales by salesperson over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also create a separate `pd.DataFrame` that maps each salesperson to a
    particular region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: If you have ever worked at a small company or within a small department, chances
    are you’ve seen data sources built this way. As far as employees in that space
    are concerned, everyone knows who `John` is, so they are content with the decision
    to lay out data in this fashion.
  prefs: []
  type: TYPE_NORMAL
- en: In the sales data, `John` appears multiple times, but in the regions data, `John`
    appears only once. Therefore, using `salesperson` as the merge key, the relationship
    from sales to regions is many-to-one (*n*-to-1). Conversely, the relationship
    from regions to sales is one-to-many (1-to-*n*).
  prefs: []
  type: TYPE_NORMAL
- en: 'With these types of relationships, merges do not introduce any unexpected behavior.
    A `pd.merge` between these two objects will simply display the multiple rows of
    sales data alongside the corresponding region information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were to try and sum the sales of this after the merge, we would still
    get the appropriate amount of `60`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'As the company or department grows, it becomes inevitable that another `John`
    gets hired. To accommodate this, our `regions`, `pd.DataFrame` gets updated to
    add a new `last_name` column, and add a new entry for `John Newhire`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Suddenly, the same `pd.merge` we performed before yields a different result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a definite programming mistake. If you were to try and sum the `sales`
    column from the merged `pd.DataFrame`, you would end up doubling the true amount
    of things that were actually sold. In sum, we only sold 60 units, but with the
    introduction of `John Newhire` into our `regions`, `pd.DataFrame` suddenly changed
    the relationship between the two `pd.DataFrame` objects to many-to-many (or *n*-to-*n*),
    which duplicates much of our data and yields the wrong number of sales:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'To catch these surprises upfront with pandas, you can provide a `validate=`
    argument to `pd.merge`, which establishes the expected relationship of the merge
    key between the two objects. A validation of `many_to_one` with our original `pd.DataFrame`
    objects would have been fine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Yet that same validation would have thrown an error when `John Newhire` made
    his way into our merge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: In this simplistic example, we could have avoided this issue by modeling our
    data differently upfront, by either using a natural key comprising multiple columns
    in our sales `pd.DataFrame` or by opting for surrogate keys in both `pd.DataFrame`
    objects. Because these examples were so small, we could have also visually identified
    that there was a problem with our structure.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, detecting issues like this is not so simple. You may be trying
    to merge thousands or millions of rows of data, so even if a large number of rows
    were affected by relationship issues, they could be easily overlooked. Attempting
    to detect issues like this by hand is akin to finding a needle in a haystack,
    so I strongly advise using this data validation feature to avoid surprises.
  prefs: []
  type: TYPE_NORMAL
- en: While a failure is less than ideal, in this case, you have *failed loudly* and
    can easily identify where your modeling assumptions went wrong. Without these
    checks, your users will *silently* see incorrect data, which is, more often than
    not, a worse outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Joining DataFrames with pd.DataFrame.join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While `pd.merge` is the most common approach for merging two different `pd.DataFrame`
    objects, the lesser used yet functionally similar `pd.DataFrame.join` method is
    another viable option. Stylistically, you can think of `pd.DataFrame.join` as
    a shortcut for when you want to augment an existing `pd.DataFrame` with a few
    more columns; by contrast, `pd.merge` defaults to treating both `pd.DataFrame`
    objects with equal importance.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To drive home the point about `pd.DataFrame.join` being a shortcut to augment
    an existing `pd.DataFrame`, let’s imagine a sales table where the row index corresponds
    to a salesperson but uses a surrogate key instead of a natural key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also then consider a dedicated `pd.DataFrame` that stores the metadata
    for some (but not all) of these salespeople:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the data we want to use to join these two `pd.DataFrame` objects together
    is in the row index, you would have to write out `left_index=True` and `right_index=True`
    while calling `pd.merge`. Also note that, because we have a `salesperson_id` of
    `9000` in our sales `pd.DataFrame` but no corresponding entry in `salesperson`,
    you would have to use `how="left"` to make sure records are not lost during the
    merge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'That rather lengthy call to `pd.merge` describes the default behavior of `pd.DataFrame.join`,
    so you may find it easier just to use the latter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'While `pd.DataFrame.join` defaults to a left join, you can also choose a different
    behavior through the argument to `how=`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Ultimately, there is no requirement to use `pd.DataFrame.join` over `pd.merge`.
    The former is simply a shortcut and a stylistic indication that the calling `pd.DataFrame`
    (here, `sales`) should not drop any records when being joined against another
    `pd.DataFrame`, like `salesperson`.
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping with pd.DataFrame.stack and pd.DataFrame.unstack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we jump into the terms *stacking* and *unstacking*, let’s take a step
    back and compare two tables of data. Do you notice anything different about:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | a | b | c |'
  prefs: []
  type: TYPE_TB
- en: '| x | 1 | 2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| y | 4 | 5 | 6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.1: A table in wide format'
  prefs: []
  type: TYPE_NORMAL
- en: 'compared to:'
  prefs: []
  type: TYPE_NORMAL
- en: '| x | a | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| x | b | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| x | c | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| y | a | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| y | b | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| y | c | 6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.2: A table in long format'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, visually, the tables have different shapes, but the data contained
    in each is the same. The former table would commonly be referred to as a table
    in *wide* format, as it stores data strewn across different columns. By contrast,
    in the second table, which many would say is stored in the *long* format, new
    rows are used to represent the various bits of data.
  prefs: []
  type: TYPE_NORMAL
- en: Which format is better? The answer to this is *it depends* – namely, on your
    audience and/or the systems you interact with. An executive at your company may
    prefer to see data stored in the wide format, as it is easier to read at a glance.
    A columnar database would prefer the long format, as it can better optimize for
    millions and billions of rows than it could for an equal number of columns.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing that there is no single way to store data, you will likely need to reshape
    data in and out of both of these formats, which brings us to the terms *stacking*
    and *unstacking*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Stacking* refers to the process of taking your columns and pushing them down
    into the rows, essentially, helping to move from a wide format into a long format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Stacking a pd.DataFrame from wide to long format'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unstacking* goes in the opposite direction, moving data that is stored in
    a long format into a wide format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a number and a number  Description automatically generated with
    medium confidence](img/B31091_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Unstacking a pd.DataFrame from long to wide format'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will walk you through the proper usage of the `pd.DataFrame.stack`
    and `pd.DataFrame.unstack` methods, which can be used for these reshaping purposes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with the following `pd.DataFrame`, which summarizes the amount
    of fruits being grown in different states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: In data modeling terminology, we would consider this to be a “wide” table. Each
    row represents one state with the different numbers of each crop situated in its
    own column.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wanted to convert our table to “long” form, we would essentially want
    to see each `state` and `fruit` combination as a separate row. `pd.DataFrame.stack`
    will help us do this, by taking our fruits out of the column index and forming
    a new `pd.MultiIndex` in our rows, which contains both state and fruit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'After a call to `pd.DataFrame.stack`, many users will chain in a call to `pd.Series.reset_index`
    with a `name=` argument. This converts the `pd.Series` with a `pd.MultiIndex`
    created from the `pd.DataFrame.stack` back into a `pd.DataFrame` with meaningful
    column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: This long form of storing data is preferred for storage by many databases and
    is also the expected shape of the `pd.DataFrame` to be passed to libraries like
    Seaborn, which we showcased in the *Seaborn introduction* recipe back in *Chapter
    6*, *Visualization*.
  prefs: []
  type: TYPE_NORMAL
- en: However, sometimes, you may want to go in the opposite direction, converting
    your long `pd.DataFrame` into a wider format. This can be particularly useful
    when wanting to summarize data in a compact area; utilizing both dimensions for
    display is more effective than asking your viewer to scroll through many lines
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this in action, let’s create a new `pd.Series` from one of the `pd.DataFrame.stack`
    calls we just made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'To go in the opposite direction and move one of our index levels from the rows
    to the columns, you simply need to make a call to `pd.Series.unstack`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, a call to `pd.Series.unstack` moves the innermost level of the
    row index, which, in our case, was the `fruit`. However, we could have passed
    `level=0` to have it take the very first level instead of the innermost, in the
    case that we wanted to see the states summarized across the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Because our `pd.MultiIndex` levels have names, we could also have referred
    to the level we wanted to be moved by name instead of by position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Reshaping with pd.DataFrame.melt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Reshaping with pd.DataFrame.stack and pd.DataFrame.unstack* recipe,
    we discovered that you could convert a wide `pd.DataFrame` into long form by setting
    the appropriate row and column index(es) before calling `pd.DataFrame.stack`.
    `pd.TheDataFrame.melt` function also lets you convert your `pd.DataFrame` from
    wide to long, but can do so without having to set the row and column index values
    in an intermediate step, while also offering more control over what other columns
    may or may not be included as part of the wide to long conversion.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s once again create a summary of the different fruits being grown in different
    states. However, unlike the *Reshaping with pd.DataFrame.stack and pd.DataFrame.unstack*
    recipe, we will not be setting the row index to the state values, and instead,
    just treating it as another column in our `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'To convert to long format with `pd.DataFrame.stack`, we would have to chain
    together a few calls to get back a `pd.DataFrame` without a `pd.MultiIndex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'The column name `level_1` is created by default during our `pd.DataFrame.stack`
    operation because the column index we start with is unnamed. We also see that
    we get an auto-generated column name of `0` for the newly introduced values in
    our long format, so we would still need to chain in a rename to get us a more
    readable `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '`pd.DataFrame.melt` gets us a lot closer to our desired `pd.DataFrame`, simply
    by providing an `id_vars=` argument that corresponds to the row index you would
    have used with `pd.DataFrame.stack`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'With `pd.DataFrame.melt`, the newly created column from our variables (here,
    the different fruits) is given the name `variable`, and the value column is given
    the default name of `value`. We can override these defaults through the use of
    the `var_name=` and `value_name=` arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'As an added bonus, `pd.DataFrame.melt` gives you an easy way to control which
    columns are included as part of the wide-to-long conversion. For instance, if
    we don’t care to include the `banana` values in our newly formed long table, we
    could just pass the other columns of `apple` and `orange` as arguments to `value_vars=`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Reshaping with pd.wide_to_long
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have encountered two very viable ways of converting data from wide
    to long format, whether it be through the use of the `pd.DataFrame.stack` method,
    introduced in our *Reshaping with pd.DataFrame.stack and pd.DataFrame.unstack*
    recipe, or through the use of the `pd.DataFrame.melt`, as we saw in the *Reshaping
    with pd.DataFrame.melt* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: If those aren’t enough, pandas offers the `pd.wide_to_long` function, which
    can help with that conversion given that your columns follow a particular naming
    pattern, as we will see in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s assume we have the following `pd.DataFrame`, where we have one `id` variable
    of `widget` and four columns representing sales from a business quarter. Each
    column of sales begins with `"quarter_"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Going back to our example of `pd.DataFrame.stack`, we could convert this from
    wide to long using the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'For a more succinct solution, we could use `pd.DataFrame.melt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'But there is a feature that `pd.wide_to_long` offers that neither of these
    approaches handles directly – namely, to create a new variable out of the column
    labels that are being converted into variables. So far, we see the new `quarter`
    values as `quarter_1`, `quarter_2`, `quarter_3`, and `quarter_4`, but `pd.wide_to_long`
    can extract that string out of the newly created variables, more simply leaving
    you with the digits `1`, `2`, `3`, and `4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Reshaping with pd.DataFrame.pivot and pd.pivot_table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we have seen that `pd.DataFrame.stack`, `pd.DataFrame.melt`,
    and `pd.wide_to_long` can all be used to help you convert your `pd.DataFrame`
    from a wide to a long format. On the flip side, we have only seen `pd.Series.unstack`
    helps us go from long to wide, but that method has the downside of requiring us
    to assign a proper row index before we can use it. With `pd.DataFrame.pivot`,
    you can skip any intermediate steps and go directly from a long to a wide format.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond `pd.DataFrame.pivot`, pandas offers a `pd.pivot_table` function, which
    can not only reshape from long to wide but allows you to perform aggregations
    as part of the reshape.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Using pd.pivot_table to reshape with sum aggregation'
  prefs: []
  type: TYPE_NORMAL
- en: Effective use of `pd.pivot_table` allows you to perform very complex calculations
    with a compact and concise syntax.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In many of the preceding recipes, we have started with data in wide form and
    reshaped it to long form. For this recipe, let’s start with data that appears
    in long form from the outset. We are also going to add a new column for `number_eaten`
    to showcase the aggregation capabilities when pivoting within pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'As we learned back in the *Reshaping with pd.DataFrame.stack and pd.DataFrame.unstack*
    recipe, if we wanted to convert this from long format into wide, we could do so
    with the clever use of `pd.DataFrame.set_index` paired with `pd.DataFrame.unstack`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '`pd.DataFrame.pivot` lets us tackle this in one method call. A basic usage
    of this method requires `index=` and `columns=` arguments, to dictate which column(s)
    should appear in the row and column indexes, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '`pd.DataFrame.pivot` will take any column that is not specified as an argument
    to `index=` or `columns=`, and try to convert that column into the values of the
    resulting `pd.DataFrame`. However, if you did not care for all of the remaining
    columns to be a part of the pivoted `pd.DataFrame`, you could specify what you
    want to keep with the `values=` argument. For example, if we only cared to pivot
    the `number_grown` column and ignore the `number_eaten` column, we could write
    this as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case where you only wanted to keep one value, the generated `pd.MultiIndex`
    in the columns may seem superfluous. Fortunately, this can be dropped with a simple
    call to `pd.DataFrame.droplevel`, where you indicate the `axis=` where you would
    like to drop a level (specify `1` for the columns) and the index level you would
    like to drop (here, `0` represents the first level):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'While `pd.DataFrame.pivot` is useful for reshaping, it can only help in the
    case that none of the values used to form your rows and columns are duplicated.
    To see this limitation, let’s work with a slightly modified `pd.DataFrame` that
    shows how different fruits have been consumed or grown in different states and
    years:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'We would be able to still use `pd.DataFrame.pivot` on this `pd.DataFrame` if
    we placed `state`, `fruit`, and `year` all in either the rows or the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'But what if we didn’t want to see the `year` as part of our output? Just removing
    it from our `pd.DataFrame.pivot` arguments will raise an exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'For `pd.pivot_table`, the lack of a `year` column is no problem at all:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: This works because `pd.pivot_table` aggregates the values while reshaping them
    into a wide form. Taking Arizona apples as an example, our input data showed that
    a whopping three were grown in the year 2023 before doubling to a magnificent
    six in 2024\. In our call to `pd.pivot_table`, this is shown as `4.5`. By default,
    `pd.pivot_table` will take the average of values you supply to it during a reshape.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can, of course, control the aggregation function being used. In this particular
    case, we may be more interested in knowing how many fruits were grown in each
    state in total, rather than taking an average by year. By passing a different
    aggregation function as a parameter to `aggfunc=`, you can easily get a summation
    instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'For more advanced use cases, you can even provide a dictionary of values to
    `aggfunc=`, where each key/value pair of the dictionary dictates the column and
    the type of aggregation(s) to be applied, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: Reshaping with pd.DataFrame.explode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world would be so simple if every piece of data fitted perfectly as a scalar
    into a two-dimensional `pd.DataFrame`. Alas, life is not so simple. Especially
    when working with semi-structured sources of data like JSON, it is not uncommon
    to have individual items in your `pd.DataFrame` contain non-scalar sequences like
    lists and tuples.
  prefs: []
  type: TYPE_NORMAL
- en: You may find it acceptable to leave data in that state, but other times, there
    is value to normalizing the data and potentially extracting out sequences contained
    within a column into individual elements.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Using pd.DataFrame.explode to extract list elements to individual
    rows'
  prefs: []
  type: TYPE_NORMAL
- en: To that end, `pd.DataFrame.explode` is the right tool for the job. It may not
    be a function you use every day, but when you eventually need to use it, you will
    be happy to have known about it. Attempting to replicate the same functionality
    outside of pandas can be error-prone and non-performant!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we mentioned JSON as a good source for semi-structured data in the introduction
    to this recipe, let’s start by imagining that we have to interact with a REST
    API for an HR system. The HR system should tell us who each person is in the company,
    as well as who, if anyone, reports to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hierarchy between employees is rather easy to represent in a semi-structured
    format like JSON, so the REST API might return something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'The pandas library will also let us load this data into a `pd.DataFrame`, albeit
    with the `direct_reports` column containing lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'With `pd.DataFrame.explode`, you can unpack those `direct_reports` into separate
    rows of the `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'Building off of the knowledge we picked up about merging/joining data from
    our *Merging DataFrames with pd.merge* recipe, we can very easily take our exploded
    information and merge in the names of direct reports, yielding an easy summary
    of who works at the company and who, if anyone, reports to them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While we did not introduce it in our review of types in *Chapter 3*, *Data
    Types*, PyArrow does offer a struct data type which, when used in a `pd.Series`,
    exposes a `pd.Series.struct.explode` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike `pd.DataFrame.explode`, which generates new rows of data, `pd.Series.struct.explode`
    generates new columns of data from its struct members:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: This could be particularly useful if you are dealing with a semi-structured
    data source like JSON. If you are able to fit nested data from such a source into
    the typed struct that PyArrow has to offer, `pd.Series.struct.explode` can save
    you a significant amount of trouble when trying to unnest that data.
  prefs: []
  type: TYPE_NORMAL
- en: Transposing with pd.DataFrame.T
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the final recipe in this chapter, let’s explore one of the easier reshaping
    features of pandas. *Transposition* refers to the process of inverting your `pd.DataFrame`
    so that the rows become the columns and the columns become the rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_07_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Transposing a pd.DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to transpose with the `pd.DataFrame.T` method
    while discussing how this might be useful.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transposition in pandas is straightforward. Take any `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'You can simply access the `pd.DataFrame.T` attribute and watch as your rows
    become your columns and your columns become your rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: There are an endless number of reasons why you may want to transpose, ranging
    from simply thinking *it looks better* in a given format to cases where you find
    it easier to select by a row index label instead of by a column index label.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, one of the main use cases to transpose will be to get your `pd.DataFrame`
    in an *optimal* format before applying functions. As we learned back in *Chapter
    5*, *Algorithms and How to Apply Them*, pandas has the ability to apply aggregations
    to each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'as well as to each row with the `axis=1` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, using the `axis=1` argument can drastically reduce the performance
    of your applications. If you find yourself scattering a lot of `axis=1` calls
    throughout your code, chances are you would be much better off transposing your
    data first and then applying functions with the default `axis=0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the difference, let’s look at a `pd.DataFrame` that is rather wide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'Ultimately, we will get the same result, whether we sum the rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: 'or transpose first, and then use the default summation of the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: However, if you were to repeatedly make calls using `axis=1` as an argument,
    you would find that transposing first can save significant time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure this, let’s use IPython and check how long it takes to perform our
    sum 100 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: 'Comparatively, transposing first and then performing the sum will be much faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: Overall, using `pd.DataFrame.T` to avoid subsequent calls with `axis=1` is a
    highly encouraged practice.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/pandas](https://packt.link/pandas)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5040900042138312.png)'
  prefs: []
  type: TYPE_IMG
