- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (**RL**) is the third major section of machine learning
    after supervised and unsupervised learning. These techniques have gained a lot
    of traction in recent years in the application of artificial intelligence. In
    reinforcement learning, sequential decisions are to be made rather than one shot
    decision making, which makes it difficult to train the models in a few cases.
    In this chapter, we would be covering various techniques used in reinforcement
    learning with practical examples to support with. Though covering all topics are
    beyond the scope of this book, but we did cover the most important fundamentals
    here for a reader to create enough enthusiasm on this subject. Topics discussed
    in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: Markov decision process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellman equations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temporal difference learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we deep dive into the details of reinforcement learning, I would like
    to cover some of the basics necessary for understanding the various nuts and bolts
    of RL methodologies. These basics appear across various sections of this chapter,
    which we will explain in detail whenever required:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Environment:** This is any system that has states, and mechanisms to transition
    between states. For example, the environment for a robot is the landscape or facility
    it operates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent:** This is an automated system that interacts with the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State:** The state of the environment or system is the set of variables or
    features that fully describe the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Goal or absorbing state or terminal state:** This is the state that provides
    a higher discounted cumulative reward than any other state. A high cumulative
    reward prevents the best policy from being dependent on the initial state during
    training. Whenever an agent reaches its goal, we will finish one episode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action:** This defines the transition between states. The agent is responsible
    for performing, or at least recommending an action. Upon execution of the action,
    the agent collects a reward (or punishment) from the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy:** This defines the action to be selected and executed for any state
    of the environment. In other words, policy is the agent''s behavior; it is a map
    from state to action. Policies could be either deterministic or stochastic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Best policy:** This is the policy generated through training. It defines
    the model in Q-learning and is constantly updated with any new episode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rewards**: This quantifies the positive or negative interaction of the agent
    with the environment. Rewards are usually immediate earnings made by the agent
    reaching each state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Returns or value function**: A value function (also called returns) is a
    prediction of future rewards of each state. These are used to evaluate the goodness/badness
    of the states, based on which, the agent will choose/act on for selecting the
    next best state:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/97df1bcf-51c9-4747-a6fe-23daa16edd22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Episode:** This defines the number of steps necessary to reach the goal state
    from an initial state. Episodes are also known as trials.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizon:** This is the number of future steps or actions used in the maximization
    of the reward. The horizon can be infinite, in which case, the future rewards
    are discounted in order for the value of the policy to converge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration versus Exploitation:** RL is a type of trial and error learning.
    The goal is to find the best policy; and at the same time, remain alert to explore
    some unknown policies. A classic example would be treasure hunting: if we just
    go to the locations greedily (exploitation), we fail to look for other places
    where hidden treasure might also exist (exploration). By exploring the unknown
    states, and by taking chances, even when the immediate rewards are low and without
    losing the maximum rewards, we might achieve greater goals. In other words, we
    are escaping the local optimum in order to achieve a global optimum (which is
    exploration), rather than just a short-term focus purely on the immediate rewards
    (which is exploitation). Here are a couple of examples to explain the difference:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Restaurant selection**: By exploring unknown restaurants once in a while,
    we might find a much better one than our regular favorite restaurant:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploitation**: Going to your favorite restaurant'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration**: Trying a new restaurant'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Oil drilling example:** By exploring new untapped locations, we may get newer
    insights that are more beneficial that just exploring the same place:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploitation**: Drill for oil at best known location'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration**: Drill at a new location'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State-Value versus State-Action Function:** In action-value, Q represents
    the expected return (cumulative discounted reward) an agent is to receive when
    taking Action *A* in State *S*, and behaving according to a certain policy π(a|s)
    afterwards (which is the probability of taking an action in a given state).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In state-value, the value is the expected return an agent is to receive from
    being in state *s* behaving under a policy *π(a|s)*. More specifically, the state-value
    is an expectation over the action-values under a policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b1b62a2-aae2-4d68-bea1-4ca30750e064.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**On-policy versus off-policy TD control:** An off-policy learner learns the
    value of the optimal policy independently of the agent''s actions. Q-learning
    is an off-policy learner. An on-policy learner learns the value of the policy
    being carried out by the agent, including the exploration steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction and control problems:** Prediction talks about how well I do,
    based on the given policy: meaning, if someone has given me a policy and I implement
    it, how much reward I will get for that. Whereas, in control, the problem is to
    find the best policy so that I can maximize the reward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction:** Evaluation of the values of states for a given policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the uniform random policy, what is the value function for all states?
  prefs: []
  type: TYPE_NORMAL
- en: '**Control:** Optimize the future by finding the best policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the optimal value function over all possible policies, and what is the
    optimal policy?
  prefs: []
  type: TYPE_NORMAL
- en: Usually, in reinforcement learning, we need to solve the prediction problem
    first, in order to solve the control problem after, as we need to figure out all
    the policies to figure out the best or optimal one.
  prefs: []
  type: TYPE_NORMAL
- en: '**RL Agent Taxonomy:** An RL agent includes one or more of the following components:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy:** Agent''s behavior function (map from state to action); Policies
    can be either deterministic or stochastic'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value function:** How good is each state (or) prediction of expected future
    reward for each state'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model:** Agent''s representation of the environment. A model predicts what
    the environment will do next:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transitions:** p predicts the next state (that is, dynamics):'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/88cda7e7-bfe8-482d-ad09-c50612f77308.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Rewards:** R predicts the next (immediate) reward'
  prefs:
  - PREF_UL
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/036b8b26-fd87-4bd1-8172-8231499871f4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us explain the various categories possible in RL agent taxonomy, based
    on combinations of policy and value, and model individual components with the
    following maze example. In the following maze, you have both the start and the
    goal; the agent needs to reach the goal as quickly as possible, taking a path
    to gain the total maximum reward and the minimum total negative reward. Majorly
    five categorical way this problem can be solved:'
  prefs: []
  type: TYPE_NORMAL
- en: Value based
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy based
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actor critic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model free
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model based
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/20718ee8-c5f3-4aa4-b4ed-6ff3e9ec9b60.png)'
  prefs: []
  type: TYPE_IMG
- en: Category 1 - value based
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Value function does look like the right-hand side of the image (the sum of discounted
    future rewards) where every state has some value. Let's say, the state one step
    away from the goal has a value of -1; and two steps away from the goal has a value
    of -2\. In a similar way, the starting point has a value of -16\. If the agent
    gets stuck in the wrong place, the value could be as much as -24\. In fact, the
    agent does move across the grid based on the best possible values to reach its
    goal. For example, the agent is at a state with a value of -15\. Here, it can
    choose to move either north or south, so it chooses to move north due to the high
    reward, which is -14 rather, than moving south, which has a value of -16\. In
    this way, the agent chooses its path across the grid until it reaches the goal.
  prefs: []
  type: TYPE_NORMAL
- en: '**Value Function**: Only values are defined at all states'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No Policy (Implicit)**: No exclusive policy is present; policies are chosen
    based on the values at each state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/273b4f5e-d2bc-440c-a62b-eecfd30159cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Category 2 - policy based
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The arrows in the following image represent what an agent chooses as the direction
    of the next move while in any of these states. For example, the agent first moves
    east and then north, following all the arrows until the goal has been reached.
    This is also known as mapping from states to actions. Once we have this mapping,
    an agent just needs to read it and behave accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy**: Policies or arrows that get adjusted to reach the maximum possible
    future rewards. As the name suggests, only policies are stored and optimized to
    maximize rewards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No value function**: No values exist for the states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/eba78992-d0d0-4236-8fda-8d8ee466cbc8.png)'
  prefs: []
  type: TYPE_IMG
- en: Category 3 - actor-critic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Actor-Critic, we have both policy and value functions (or a combination
    of value-based and policy-based). This method is the best of both worlds:'
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value Function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Category 4 - model-free
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In RL, a fundamental distinction is if it is model-based or model-free. In
    model-free, we do not explicitly model the environment, or we do not know the
    entire dynamics of a complete environment. Instead, we just go directly to the
    policy or value function to gain the experience and figure out how the policy
    affects the reward:'
  prefs: []
  type: TYPE_NORMAL
- en: Policy and/or value function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Category 5 - model-based
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In model-based RL, we first build the entire dynamics of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Policy and/or value function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After going through all the above categories, the following Venn diagram shows
    the entire landscape of the taxonomy of an RL agent at one single place. If you
    pick up any paper related to reinforcement learning, those methods can fit in
    within any section of this landscape.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c859d161-fdc0-4d4f-af65-fb4ddfec3627.png)'
  prefs: []
  type: TYPE_IMG
- en: Fundamental categories in sequential decision making
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two fundamental types of problems in sequential decision making:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (for example, autonomous helicopter, and so on):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment is initially unknown
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Agent interacts with the environment and obtain policies, rewards, values from
    the environment
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Agent improves its policy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Planning** (for example, chess, Atari games, and so on):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model of environment or complete dynamics of environment is known
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Agent performs computation with its model (without any external interaction)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Agent improves its policy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the type of problems also known as reasoning, searching, introspection,
    and so on
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Though the preceding two categories can be linked together as per the given
    problem, but this is basically a broad view of the two types of setups.
  prefs: []
  type: TYPE_NORMAL
- en: Markov decision processes and Bellman equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Markov decision process** (**MDP**) formally describes an environment for
    reinforcement learning. Where:'
  prefs: []
  type: TYPE_NORMAL
- en: Environment is fully observable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current state completely characterizes the process (which means the future state
    is entirely dependent on the current state rather than historic states or values)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Almost all RL problems can be formalized as MDPs (for example, optimal control
    primarily deals with continuous MDPs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Central idea of MDP:** MDP works on the simple Markovian property of a state;
    for example, *S[t+1]* is entirely dependent on latest state *S[t]* rather than
    any historic dependencies. In the following equation, the current state captures
    all the relevant information from the history, which means the current state is
    a sufficient statistic of the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/923c18bd-709d-4898-8198-f2dea3196190.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'An intuitive sense of this property can be explained with the autonomous helicopter
    example: the next step is for the helicopter to move either to the right, left,
    to pitch, or to roll, and so on, entirely dependent on the current position of
    the helicopter, rather than where it was five minutes before.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Modeling of MDP:** RL problems models the world using MDP formulation as
    a five tuple (*S, A, {P[sa]}, y, R*)'
  prefs: []
  type: TYPE_NORMAL
- en: '*S* - Set of States (set of possible orientations of the helicopter)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A* - Set of Actions (set of all possible positions that can pull the control
    stick)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P[sa]* - State transition distributions (or state transition probability distributions)
    provide transitions from one state to another and the respective probabilities
    needed for the Markov process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/db779c49-a499-4e9b-a735-39c6f3337401.jpg)![](img/4f549720-521a-45c0-9ebd-aa388abc1c26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'γ - Discount factor:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f9570224-0dcc-4670-bad4-c9a0f269444c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'R - Reward function (maps set of states to real numbers, either positive or
    negative):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/52688e9d-6663-4453-ae5b-f429b7d6eef4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Returns are calculated by discounting the future rewards until terminal state
    is reached.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bellman Equations for MDP:** Bellman equations are utilized for the mathematical
    formulation of MDP, which will be solved to obtain the optimal policies of the
    environment. Bellman equations are also known as **dynamic programming equations**
    and are a necessary condition for the optimality associated with the mathematical
    optimization method that is known as dynamic programming. Bellman equations are
    linear equations which can be solvable for the entire environment. However, the
    time complexity for solving these equations is *O (n³)*, which becomes computationally
    very expensive when the number of states in an environment is large; and sometimes,
    it is not feasible to explore all the states because the environment itself is
    very large. In those scenarios, we need to look at other ways of solving problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Bellman equations, value function can be decomposed into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Immediate reward *R[t+1]*, from the successor state you will end up with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discounted value of successor states *yv(S[t+1])* you will get from that timestep
    onwards:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/74cb4e86-c22d-4b73-9204-3fefb68e908f.jpg)![](img/b0aef024-1db2-439c-bc0c-dec7e0b7ea20.jpg)![](img/6828d717-c803-4810-8a1d-8ed77e5462d8.jpg)![](img/eee8b434-ddc4-4fe4-8231-33e4479447a7.jpg)![](img/b05a1d44-b098-45b7-96dd-b8f2a8996362.jpg)![](img/865ad437-11e1-4a35-9d51-65873a08a4d0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Grid world example of MDP:** Robot navigation tasks live in the following
    type of grid world. An obstacle is shown the cell (2,2), through which the robot
    can''t navigate. We would like the robot to move to the upper-right cell (4,3)
    and when it reaches that position, the robot will get a reward of +1\. The robot
    should avoid the cell (4,2), as, if it moved into that cell, it would receive
    a-1 reward.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01aa79ca-4cde-4eb6-bfea-81a7fc272b62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Robot can be in any of the following positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*11 States* - (except cell (2,2), in which we have got an obstacle for the
    robot)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A = {N-north, S-south, E-east, W-west}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the real world, robot movements are noisy, and a robot may not be able to
    move exactly where it has been asked to. Examples might include that some of its
    wheels slipped, its parts were loosely connected, it had incorrect actuators,
    and so on. When asked to move by 1 meter, it may not be able to move exactly 1
    meter; instead, it may move 90-105 centimeters, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a simplified grid world, stochastic dynamics of a robot can be modeled as
    follows. If we command the robot to go north, there is a 10% chance that the robot
    could drag towards the left and a 10 % chance that it could drag towards the right.
    Only 80 percent of the time it may actually go north. When a robot bounces off
    the wall (including obstacles) and just stays at the same position, nothing happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/941f1293-81d5-498e-93ef-7b181cda00b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Every state in this grid world example is represented by (x, y) coordinates.
    Let''s say it is at state (3,1) and we asked the robot to move north, then the
    state transition probability matrices are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02519596-96cd-4180-b594-c978f3eb763f.jpg)![](img/63f2607c-00dd-45aa-8fb6-c0f155aa777d.jpg)![](img/399f84ac-a979-417a-8946-c114a76ea574.jpg)![](img/6c3be965-fa7b-42ad-9a3d-39ea351993a5.jpg)![](img/09268d1d-7300-4bcb-a092-ff56615f9e03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The probability of staying in the same position is 0 for the robot.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know, that sum of all the state transition probabilities sums up to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3c6be8f-f270-454a-a54e-3baf328a9a86.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Reward function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0188acae-45ca-4cf2-81cd-cf8c22722737.jpg)![](img/71493e81-cbf5-4b56-9791-1b67a42e5382.jpg)![](img/58dac8d9-b8d7-40e4-8902-689333af3cff.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For all the other states, there are small negative reward values, which means
    it charges the robot for battery or fuel consumption when running around the grid,
    which creates solutions that do not waste moves or time while reaching the goal
    of reward +1, which encourages the robot to reach the goal as quickly as possible
    with as little fuel used as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The world ends when the robot reaches either +1 or -1 states. No more rewards
    are possible after reaching any of these states; these can be called absorbing
    states. These are zero-cost absorbing states and the robot stays there forever.
  prefs: []
  type: TYPE_NORMAL
- en: 'MDP working model:'
  prefs: []
  type: TYPE_NORMAL
- en: At state *S[0]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose *a[0]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get to *S[1] ~ P*[*s0*, a0]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose *a[1]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get to *S[2] ~ P*[*s1*, *a1*]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and so on ....
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After a while, it takes all the rewards and sums up to obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5cd5a27-28c1-47dd-9376-6c8fc379dba5.jpg)![](img/3d2fc2bd-25eb-48d6-a360-153642a335d7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Discount factor models an economic application, in which one dollar earned today
    is more valuable than one dollar earned tomorrow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The robot needs to choose actions over time (a[0], a[1], a[2, ....]) to maximize
    the expected payoff:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab87b8ba-c9da-409e-9a9f-23520ed62b1c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Over the period, a reinforcement learning algorithm learns a policy which is
    a mapping of actions for each state, which means it is a recommended action, which
    the robot needs to take based on the state in which it exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac60d23d-5e7a-4425-b650-cb68a836a197.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Optimal Policy for Grid World:** Policy maps from states to actions, which
    means that, if you are in a particular state, you need to take this particular
    action. The following policy is the optimal policy which maximizes the expected
    value of the total payoff or sum of discounted rewards. Policy always looks into
    the current state rather than previous states, which is the Markovian property:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77bc9123-6017-4a05-b603-72c8782e13be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One tricky thing to look at is at the position (3,1): optimal policy shows
    to go left (West) rather than going (north), which may have a fewer number of
    states; however, we have an even riskier state that we may step into. So, going
    left may take longer, but it safely arrives at the destination without getting
    into negative traps. These types of things can be obtained from computing, which
    do not look obvious to humans, but a computer is very good at coming up with these
    policies:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define: *V^π, V*, π**'
  prefs: []
  type: TYPE_NORMAL
- en: '*V^π* = For any given policy π, value function is *V^π : S -> R* such that
    *V^π (S)* is expected total payoff starting in state S, and execute π'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e06c006-31da-4b3f-bc50-98ac530295f7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Random policy for grid world:** The following is an example of a random policy
    and its value functions. This policy is a rather bad policy with negative values.
    For any policy, we can write down the value function for that particular policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf01ee52-fe48-4591-bc7b-097498609169.png)![](img/7e0c0c6c-5af6-495d-a087-199d2b79b60e.jpg)![](img/ca313eed-3330-4634-92bf-1a9dc2384f66.jpg)![](img/3b2db0f4-74d5-4f46-b1b5-d2ed7fa080aa.jpg)![](img/7cb27a71-2014-4494-80e6-c0643be76761.jpg)![](img/2f6ff475-d699-49c6-8e8e-f126b98603cd.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In simple English, Bellman equations illustrate that the value of the current
    state is equal to the immediate reward and discount factor applied to the expected
    total payoff of new states (*S'*) multiplied by their probability to take action
    (policy) into those states.
  prefs: []
  type: TYPE_NORMAL
- en: Bellman equations are used to solve value functions for a policy in close form,
    given fixed policy, how to solve the value function equations.
  prefs: []
  type: TYPE_NORMAL
- en: Bellman equations impose a set of linear constraints on value functions. It
    turns out that we solve the value function at any state *S* by solving a set of
    linear equations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example of Bellman equations with a grid world problem:**'
  prefs: []
  type: TYPE_NORMAL
- en: The chosen policy for cell *(3,1)* is to move north. However, we have stochasticity
    in the system that about 80 percent of the time it moves in the said direction,
    and *20%* of the time it drifts sideways, either left (10 percent) or right (10
    percent).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/994596ff-4652-4fe9-922a-feed7fd009af.jpg)![](img/4d2fdf17-c7b3-4874-8674-795915340535.png)![](img/84f35a41-5cfc-4808-bf9b-300e4c122e1e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar equations can be written for all the 11 states of the MDPs within the
    grid. We can obtain the following metrics, from which we will solve all the unknown
    values, using a system of linear equation methods:'
  prefs: []
  type: TYPE_NORMAL
- en: 11 equations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11 unknown value function variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11 constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is solving an `n` variables with `n` equations problem, for which we can
    find the exact form of a solution using a system of equations easily to get an
    exact solution for V (π) for the entire closed form of the grid, which consists
    of all the states.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dynamic programming is a sequential way of solving complex problems by breaking
    them down into sub-problems and solving each of them. Once it solves the sub-problems,
    then it puts those subproblem solutions together to solve the original complex
    problem. In the reinforcement learning world, Dynamic Programming is a solution
    methodology to compute optimal policies given a perfect model of the environment
    as a Markov Decision Process (MDP).
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic programming holds good for problems which have the following two properties.
    MDPs, in fact, satisfy both properties, which makes DP a good fit for solving
    them by solving Bellman Equations:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimal substructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principle of optimality applies
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimal solution can be decomposed into sub-problems
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Overlapping sub-problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sub-problems recur many times
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Solutions can be cached and reused
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: MDP satisfies both the properties - luckily!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellman equations have recursive decomposition of state-values
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Value function stores and reuses solutions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Though, classical DP algorithms are of limited utility in reinforcement learning,
    both because of their assumptions of a perfect model and high computational expense.
    However, it is still important, as they provide an essential foundation for understanding
    all the methods in the RL domain.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms to compute optimal policy using dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Standard algorithms to compute optimal policies for MDP utilizing Dynamic Programming
    are as follows, and we will be covering both in detail in later sections of this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Value Iteration algorithm:** An iterative algorithm, in which state values
    are iterated until it reaches optimal values; and, subsequently, optimum values
    are utilized to determine the optimal policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy Iteration algorithm:** An iterative algorithm, in which policy evaluation
    and policy improvements are utilized alternatively to reach optimal policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value Iteration algorithm:** Value Iteration algorithms are easy to compute
    for the very reason of applying iteratively on only state values. First, we will
    compute the optimal value function *V**, then plug those values into the optimal
    policy equation to determine the optimal policy. Just to give the size of the
    problem, for 11 possible states, each state can have four policies (N-north, S-south,
    E-east, W-west), which gives an overall 11⁴ possible policies. The value iteration
    algorithm consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize *V(S) = 0* for all states S
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For every S, update:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c9d080c2-be67-4ae3-af04-e0b4c8d7058b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By repeatedly computing step 2, we will eventually converge to optimal values
    for all the states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6e4c7025-ccd4-467c-83f2-acac295a8f01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are two ways of updating the values in step 2 of the algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronous update** - By performing synchronous update (or Bellman backup
    operator) we will perform RHS computing and substitute LHS of the equation represented
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ae39ce46-6137-4a13-9820-90e986fd4ab1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Asynchronous update** - Update the values of the states one at a time rather
    than updating all the states at the same time, in which states will be updated
    in a fixed order (update state number 1, followed by 2, and so on.). During convergence,
    asynchronous updates are a little faster than synchronous updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Illustration of value iteration on grid world example:** The application
    of the Value iteration on a grid world is explained in the following image, and
    the complete code for solving a real problem is provided at the end of this section.
    After applying the previous value iteration algorithm on MDP using Bellman equations,
    we''ve obtained the following optimal values V* for all the states (Gamma value
    chosen as *0.99*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/803ddf4b-5afc-4c68-a2e6-3e1f8ac21182.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we plug these values in to our policy equation, we obtain the following
    policy grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7988638f-e34f-4639-aee5-3703faad8c84.jpg)![](img/399adc98-5126-4423-b205-12f0027853fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, at position (3,1) we would like to prove mathematically why an optimal
    policy suggests taking going left (west) rather than moving up (north):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eee54233-072a-4ab6-8025-b6baced8efe0.jpg)![](img/01ddc3cb-9371-489a-af1f-d6a6aa7f7163.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Due to the wall, whenever the robot tries to move towards South (downwards side),
    it will remain in the same place, hence we assigned the value of the current position
    0.71 for a probability of 0.1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for north, we calculated the total payoff as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8aa1f41-aa36-4f09-ba60-dc1b22f0305d.jpg)![](img/7473b012-bdea-4528-a114-b53f1e4d13fc.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, it would be optimal to move towards the west rather than north, and therefore
    the optimal policy is chosen to do so.
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy Iteration Algorithm:** Policy iterations are another way of obtaining
    optimal policies for MDP in which policy evaluation and policy improvement algorithms
    are applied iteratively until the solution converges to the optimal policy. Policy
    Iteration Algorithm consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize random policy π
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeatedly do the following until convergence happens
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Solve Bellman equations for the current policy for obtaining V^π for using
    system of linear equations:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/129df1ca-f1a4-45dc-8821-555ebf29aed2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Update the policy as per the new value function to improve the policy by pretending
    the new value is an optimal value using argmax formulae:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f2cc7a6b-b385-42ac-9ff6-27e37de8abfa.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By repeating these steps, both value and policy will converge to optimal values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/98961d5f-f6e5-44f4-8614-d75b289e7716.jpg)![](img/6579aed2-6181-4017-8628-a14997334d8e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Policy iterations tend to do well with smaller problems. If an MDP has an enormous
    number of states, policy iterations will be computationally expensive. As a result,
    large MDPs tend to use value iterations rather than policy iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '**What if we don''t know exact state transition probabilities in real life
    examples** *P[s,a]* **?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to estimate the probabilities from the data by using the following
    simple formulae:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4845d43-ffbf-4f8b-bc88-89eaf86a1a7d.jpg)![](img/9ff1e5fa-788d-43b7-91b8-e7abda3a9ea5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If for some states no data is available, which leads to 0/0 problem, we can
    take a default probability from uniform distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Grid world example using value and policy iteration algorithms with basic Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The classic grid world example has been used to illustrate value and policy
    iterations with Dynamic Programming to solve MDP''s Bellman equations. In the
    following grid, the agent will start at the south-west corner of the grid in (1,1)
    position and the goal is to move towards the north-east corner, to position (4,3).
    Once it reaches the goal, the agent will get a reward of +1\. During the journey,
    it should avoid the danger zone (4,2), because this will give out a negative penalty
    of reward -1\. The agent cannot get into the position where the obstacle (2,2)
    is present from any direction. Goal and danger zones are the terminal states,
    which means the agent continues to move around until it reaches one of these two
    states. The reward for all the other states would be -0.02\. Here, the task is
    to determine the optimal policy (direction to move) for the agent at every state
    (11 states altogether), so that the agent''s total reward is the maximum, or so
    that the agent can reach the goal as quickly as possible. The agent can move in
    4 directions: north, south, east and west.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b027216-c27a-401d-88e6-e587ff48c87c.png)'
  prefs: []
  type: TYPE_IMG
- en: The complete code was written in the Python programming language with class
    implementation. For further reading, please refer to object oriented programming
    in Python to understand class, objects, constructors, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `random` package for generating moves in any of the N, E, S, W directions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `argmax` function calculated the maximum state among the given
    states, based on the value for each state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To add two vectors at component level, the following code has been utilized
    for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Orientations provide what the increment value would be, which needs to be added
    to the existing position of the agent; orientations can be applied on the *x*-axis
    or *y*-axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function is used to turn the agent in the right direction, as
    we know at every command the agent moves in that direction about 80% of the time,
    whilst 10% of the time it would move right, and 10% it would move left.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The Markov decision process is defined as a class here. Every MDP is defined
    by an initial position, state, transition model, reward function, and gamma values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Returns a numeric reward for the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Transition model with from a state and an action returns a list of (probability,
    result-state) pairs for each state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Set of actions that can be performed at a particular state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Class `GridMDP` is created for modeling a 2D grid world with grid values at
    each state, terminal positions, initial position, and gamma value (discount):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is used for reversing the grid, as we would like to see
    *row 0* at the bottom instead of at the top:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `__init__` command is a constructor used within the grid class
    for initializing parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'State transitions provide randomly 80% toward the desired direction and 10%
    for left and right. This is to model the randomness in a robot which might slip
    on the floor, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Returns the state that results from going in the direction, subject to where
    that state is in the list of valid states. If the next state is not in the list,
    like hitting the wall, then the agent should remain in the same state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert a mapping from (x, y) to v into [[..., v, ...]] grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert orientations into arrows for better graphical representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is used for solving an MDP, using value iterations, and
    returns optimum state values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Given an MDP and a utility function `STS`, determine the best policy, as a
    mapping from state to action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected utility of doing `a` in state `s`, according to the MDP and STS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is used to solve an MDP using policy iterations by alternatively
    performing policy evaluation and policy improvement steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is used to return an updated utility mapping `U` from each
    state in the MDP to its utility, using an approximation (modified policy iteration):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the input grid of a 4 x 3 grid environment that presents the
    agent with a sequential decision-making problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is for performing a value iteration on the given sequential
    decision-making environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/4daaf65e-72f1-4d5c-81a9-9411ca3e6166.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The code for policy iteration is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e01853b9-4c08-4972-a213-c7f30ddb2b43.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding output with two results, we can conclude that both value
    and policy iterations provide the same optimal policy for an agent to move across
    the grid to reach the goal state in the quickest way possible. When the problem
    size is large enough, it is computationally advisable to go for value iteration
    rather than policy iteration, as in policy iterations, we need to perform two
    steps at every iteration of the policy evaluation and policy improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using **Monte Carlo** (**MC**) methods, we will compute the value functions
    first and determine the optimal policies. In this method, we do not assume complete
    knowledge of the environment. MC require only experience, which consists of sample
    sequences of states, actions, and rewards from actual or simulated interactions
    with the environment. Learning from actual experiences is striking because it
    requires no prior knowledge of the environment's dynamics, but still attains optimal
    behavior. This is very similar to how humans or animals learn from actual experience
    rather than any mathematical model. Surprisingly, in many cases, it is easy to
    generate experience sampled according to the desired probability distributions,
    but infeasible to obtain the distributions in explicit form.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo methods solve the reinforcement learning problem based on averaging
    the sample returns over each episode. This means that we assume experience is
    divided into episodes, and that all episodes eventually terminate, no matter what
    actions are selected. Values are estimated and policies are changed only after
    the completion of each episode. MC methods are incremental in an episode-by-episode
    sense, but not in a step-by-step (which is an online learning, and which we will
    cover the same in Temporal Difference learning section) sense.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo methods sample and average returns for each state-action pair over
    the episode. However, within the same episode, the return after taking an action
    in one stage depends on the actions taken in later states. Because all the action
    selections are undergoing learning, the problem becomes non-stationary from the
    point of view of the earlier state. In order to handle this non-stationarity,
    we adapt the idea of policy iteration from dynamic programming, in which, first,
    we compute the value function for a fixed arbitrary policy; and, later, we improve
    the policy.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we know, Monte Carlo methods predict the state-value function for a given
    policy. The value of any state is the expected return or expected cumulative future
    discounted rewards starting from that state. These values are estimated in MC
    methods simply to average the returns observed after visits to that state. As
    more and more values are observed, the average should converge to the expected
    value based on the law of large numbers. In fact, this is the principle applicable
    in all Monte Carlo methods. The Monte Carlo Policy Evaluation Algorithm consist
    of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5ee2d232-f6ef-433f-8537-9f55928bdc94.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Repeat forever:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using π
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each state *s* appearing in the episode:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: G return following the first occurrence of *s*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Append *G* to Returns(s)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: V(s)  average(Returns(s))
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The suitability of Monte Carlo prediction on grid-world problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following diagram has been plotted for illustration purposes. However, practically,
    Monte Carlo methods cannot be easily used for solving grid-world type problems,
    due to the fact that termination is not guaranteed for all the policies. If a
    policy was ever found that caused the agent to stay in the same state, then the
    next episode would never end. Step-by-step learning methods like (**State-Action-Reward-State-Action**
    (**SARSA**), which we will be covering in a later part of this chapter in TD Learning
    Control) do not have this problem because they quickly learn during the episode
    that such policies are poor, and switch to something else.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03344b7c-3d21-4779-8282-22fb609f9bb7.png)'
  prefs: []
  type: TYPE_IMG
- en: Modeling Blackjack example of Monte Carlo methods using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The objective of the popular casino card game Blackjack is to obtain cards,
    the sum of whose numerical values is as great as possible, without exceeding the
    value of 21\. All face cards (king, queen, and jack) count as 10, and an ace can
    count as either 1 or as 11, depending upon the way the player wants to use it.
    Only the ace has this flexibility option. All the other cards are valued at face
    value. The game begins with two cards dealt with both dealer and players. One
    of the dealer''s cards is face up and the other is face down. If the player has
    a ''Natural 21'' from these first two cards (an ace and a 10-card), the player
    wins unless the dealer also has a Natural, in which case the game is a draw. If
    the player does not have a natural, then he can ask for additional cards, one
    by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If the
    player goes bust, he loses; if the player sticks, then it''s the dealer''s turn.
    The dealer hits or sticks according to a fixed strategy without choice: the dealer
    usually sticks on any sum of 17 or greater, and hits otherwise. If the dealer
    goes bust, then the player automatically wins. If he sticks, the outcome would
    be either win, lose, or draw, determined by whether the dealer or the player''s
    sum total is closer to 21.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2357e08-6d21-41db-b26a-257ee659ea18.png)'
  prefs: []
  type: TYPE_IMG
- en: The Blackjack problem can be formulated as an episodic finite MDP, in which
    each game of Blackjack is an episode. Rewards of +1, -1, and 0 are given for winning,
    losing, and drawing for each episode respectively at the terminal state and the
    remaining rewards within the state of game are given the value as 0 with no discount
    (gamma = 1). Therefore, the terminal rewards are also the returns for this game.
    We draw the cards from an infinite deck so that no traceable pattern exists. The
    entire game is modeled in Python in the following code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippets of code have taken inspiration from *Shangtong Zhang*''s
    Python codes for RL, and are published in this book with permission from the student
    of *Richard S. Sutton*, the famous author of *Reinforcement : Learning: An Introduction*
    (details provided in the *Further reading* section).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following package is imported for array manipulation and visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'At each turn, the player or dealer can take one of the actions possible: either
    to hit or to stand. These are the only two states possible :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The policy for player is modeled with 21 arrays of values, as the player will
    get bust after going over the value of 21:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The player has taken the policy of stick if he gets a value of either 20 or
    21, or else he will keep hitting the deck to draw a new card:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Function form of target policy of a player:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Function form of behavior policy of a player:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Fixed policy for the dealer is to keep hitting the deck until value is 17 and
    then stick between 17 to 21:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function is used for drawing a new card from the deck with replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Let's play the game!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Sum of the player, player''s trajectory and whether player uses ace as 11:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Dealer status of drawing cards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a random initial state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Initializing the player''s cards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If the sum of a player''s cards is less than 12, always hit the deck for drawing
    card:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'If the player''s sum is larger than 21, he must hold at least one ace, but
    two aces are also possible. In that case, he will use ace as 1 rather than 11\.
    If the player has only one ace, then he does not have a usable ace any more:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Initializing the dealer cards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the game state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Initializing the dealer''s sum:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The game starts from here, as the player needs to draw extra cards from here
    onwards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Get action based on the current sum of a player:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Tracking the player''s trajectory for importance sampling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Get new a card if the action is to hit the deck:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Player busts here if the total sum is greater than 21, the game ends, and he
    gets a reward of -1\. However, if he has an ace at his disposable, he can use
    it to save the game, or else he will lose.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s the dealer''s turn. He will draw cards based on a sum: if he reaches
    17, he will stop, otherwise keep on drawing cards. If the dealer also has ace,
    he can use it to achieve the bust situation, otherwise, he goes bust:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we compare the player''s sum with the dealer''s sum to decide who wins
    without going bust:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code illustrates the Monte Carlo sample with *On-Policy*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code discusses Monte Carlo with Exploring Starts, in which all
    the returns for each state-action pair are accumulated and averaged, irrespective
    of what policy was in force when they were observed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Behavior policy is greedy, which gets `argmax` of the average returns (s, a):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Play continues for several episodes and, at each episode, randomly initialized
    state, action, and update values of state-action pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Update values of state-action pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the state value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'On-Policy results with or without a usable ace for 10,000 and 500,000 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimized or Monte Carlo control of policy iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/288909aa-3398-46a7-9096-a541ae6a6d7d.png)'
  prefs: []
  type: TYPE_IMG
- en: From the previous diagram, we can conclude that a usable ace in a hand gives
    much higher rewards even at the low player sum combinations, whereas for a player
    without a usable ace, values are pretty distinguished in terms of earned reward
    if those values are less than 20.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3ae682af-301f-4a67-b74f-7b46025b5e1d.png)'
  prefs: []
  type: TYPE_IMG
- en: From the optimum policies and state values, we can conclude that, with a usable
    ace at our disposal, we can hit more than stick, and also that the state values
    for rewards are much higher compared with when there is no ace in a hand. Though
    the results we are talking about are obvious, we can see the magnitude of the
    impact of holding an ace in a hand.
  prefs: []
  type: TYPE_NORMAL
- en: Temporal difference learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Temporal Difference** (**TD**) learning is the central and novel theme of
    reinforcement learning. TD learning is the combination of both **Monte Carlo**
    (**MC**) and **Dynamic Programming** (**DP**) ideas. Like Monte Carlo methods,
    TD methods can learn directly from the experiences without the model of the environment.
    Similar to Dynamic Programming, TD methods update estimates based in part on other
    learned estimates, without waiting for a final outcome, unlike MC methods, in
    which estimates are updated after reaching the final outcome only.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4256712b-f076-4482-8960-92d182c9c460.png)'
  prefs: []
  type: TYPE_IMG
- en: TD prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both TD and MC use experience to solve *z* prediction problem. Given some policy
    π, both methods update their estimate *v* of *v*[π]  for the non-terminal states
    *S[t]* occurring in that experience. Monte Carlo methods wait until the return
    following the visit is known, then use that return as a target for *V(S[t])*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bffd9d6-a6a8-4337-b394-6d9e39c06077.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding method can be called as a constant - *α MC*, where MC must wait
    until the end of the episode to determine the increment to *V(S[t])* (only then
    is *G[t]* known).
  prefs: []
  type: TYPE_NORMAL
- en: 'TD methods need to wait only until the next timestep. At time *t+1*, they immediately
    form a target and make a useful update using the observed reward *R[t+1]* and
    the estimate *V(S[t+1])*. The simplest TD method, known as *TD(0)*, is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9301f34-ddc2-48b2-b7a5-c3cbe1febc08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Target for MC update is *G[t]*, whereas the target for the TD update is *R[t+1 ]+
    y V(S[t+1])*.
  prefs: []
  type: TYPE_NORMAL
- en: In the following diagram, a comparison has been made between TD with MC methods.
    As we've written in equation TD(0), we use one step of real data and then use
    the estimated value of the value function of next state. In a similar way, we
    can also use two steps of real data to get a better picture of the reality and
    estimate value function of the third stage. However, as we increase the steps,
    which eventually need more and more data to perform parameter updates, the more
    time it will cost.
  prefs: []
  type: TYPE_NORMAL
- en: When we take infinite steps until it touches the terminal point for updating
    parameters in each episode, TD becomes the Monte Carlo method.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/579d6e42-de4f-47c1-8fea-88eca610019f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'TD (0) for estimating *v* algorithm consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fe4f7713-158f-4708-a82a-06e72dde76f3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Repeat (for each episode):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize *S*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Repeat (for each step of episode):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A <- action given by π for S
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Take action A, observe *R,S'*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/645d3a19-7c01-4d55-9b6c-850d4cdd6a27.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/5c3bcd7e-fdba-4a49-b716-b8099ced3401.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: Until *S* is terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Driving office example for TD learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this simple example, you travel from home to the office every day and you
    try to predict how long it will take to get to the office in the morning. When
    you leave your home, you note that time, the day of the week, the weather (whether
    it is rainy, windy, and so on) any other parameter which you feel is relevant.
    For example, on Monday morning you leave at exactly 8 a.m. and you estimate it
    takes 40 minutes to reach the office. At 8:10 a.m., and you notice that a VIP
    is passing, and you need to wait until the complete convoy has moved out, so you
    re-estimate that it will take 45 minutes from then, or a total of 55 minutes.
    Fifteen minutes later you have completed the highway portion of your journey in
    good time. Now you enter a bypass road and you now reduce your estimate of total
    travel time to 50 minutes. Unfortunately, at this point, you get stuck behind
    a bunch of bullock carts and the road is too narrow to pass. You end up having
    to follow those bullock carts until you turn onto the side street where your office
    is located at 8:50\. Seven minutes later, you reach your office parking. The sequence
    of states, times, and predictions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf127b34-153f-4784-801e-809df5455da9.png)'
  prefs: []
  type: TYPE_IMG
- en: Rewards in this example are the elapsed time at each leg of the journey and
    we are using a discount factor (gamma, *v = 1*), so the return for each state
    is the actual time to go from that state to the destination (office). The value
    of each state is the predicted time to go, which is the second column in the preceding
    table, also known the current estimated value for each state encountered.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04b114de-7dbf-4065-9501-1fbcd65f3120.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous diagram, Monte Carlo is used to plot the predicted total time
    over the sequence of events. Arrows always show the change in predictions recommended
    by the constant-α MC method. These are errors between the estimated value in each
    stage and the actual return (57 minutes). In the MC method, learning happens only
    after finishing, for which it needs to wait until 57 minutes passed. However,
    in reality, you can estimate before reaching the final outcome and correct your
    estimates accordingly. TD works on the same principle, at every stage it tries
    to predict and correct the estimates accordingly. So, TD methods learn immediately
    and do not need to wait until the final outcome. In fact, that is how humans predict
    in real life. Because of these many positive properties, TD learning is considered
    as novel in reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: SARSA on-policy TD control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**State-action-reward-state-action** (**SARSA**) is an on-policy TD control
    problem, in which policy will be optimized using policy iteration (GPI), only
    time TD methods used for evaluation of predicted policy. In the first step, the
    algorithm learns a SARSA function. In particular, for an on-policy method we estimate
    *q[π] (s, a)* for the current behavior policy π and for all states (s) and actions
    (a), using the TD method for learning v[π.]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we consider transitions from state-action pair to state-action pair, and
    learn the values of state-action pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a39bee3-683a-4610-8214-ba9d78ccc614.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This update is done after every transition from a non-terminal state *S[t]*.
    If *S[t+1]* is terminal, then *Q (S[t+1,] A[t+1])* is defined as zero. This rule
    uses every element of the quintuple of events (*S[t]*, *A[t]*, *Rt*, *St[+1]*,
    *A[t+1]*), which make up a transition from one state-action pair to the next.
    This quintuple gives rise to the name SARSA for the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in all on-policy methods, we continually estimate q[π] for the behavior
    policy π, and at the same time change π toward greediness with respect to q[π.] The
    algorithm for computation of SARSA is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/dad25125-2e1c-48c5-89c9-97034a05a1ca.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Repeat (for each episode):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize S
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose A from S using policy derived from Q (for example, ε- greedy)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Repeat (for each step of episode):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Take action *A*, observe *R,S'*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose *A'* from using *S'* policy derived from Q (for example, ε - greedy)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d83543f0-d7b0-4856-b065-c1f49e9bb8ac.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/3d8f822d-6b84-44fa-8274-4fe9881c379b.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: Until *S* is terminal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Q-learning - off-policy TD control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Q-learning is the most popular method used in practical applications for many
    reinforcement learning problems. The off-policy TD control algorithm is known
    as Q-learning. In this case, the learned action-value function, Q directly approximates
    ![](img/55c095cc-5895-4255-91b6-9c22cdb1caf7.png), the optimal action-value function,
    independent of the policy being followed. This approximation simplifies the analysis
    of the algorithm and enables early convergence proofs. The policy still has an
    effect, in that it determines which state-action pairs are visited and updated.
    However, all that is required for correct convergence is that all pairs continue
    to be updated. As we know, this is a minimal requirement in the sense that any
    method guaranteed to find optimal behavior in the general case must require it.
    An algorithm of convergence is shown in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c5488bbc-43aa-4794-9ecb-fc8ce42c4efb.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Repeat (for each episode):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize S
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Repeat (for each step of episode):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose A from S using policy derived from Q (for example, ε - greedy)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Take action A, observe *R,S'*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/751ba8b1-8f2c-4399-8418-a3dd70935dc5.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/15d47def-acaf-4609-91ef-4f56531bccd4.jpg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: Until *S* is terminal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cliff walking example of on-policy and off-policy of TD control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A cliff walking grid-world example is used to compare SARSA and Q-learning,
    to highlight the differences between on-policy (SARSA) and off-policy (Q-learning)
    methods. This is a standard undiscounted, episodic task with start and end goal
    states, and with permitted movements in four directions (north, west, east and
    south). The reward of -1 is used for all transitions except the regions marked
    *The Cliff*, stepping on this region will penalize the agent with reward of -100
    and sends the agent instantly back to the start position.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f23a61ae-c0ed-4be1-bcf5-92f3becb9e2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following snippets of code have taken inspiration from Shangtong Zhang''s
    Python codes for RL and are published in this book with permission from the student
    of *Richard S. Sutton*, the famous author of *Reinforcement Learning: An Introduction*
    (details provided in the *Further reading* section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a1ed3b55-f95f-4692-9127-ac7a3bb988cb.png)'
  prefs: []
  type: TYPE_IMG
- en: After an initial transient, Q-learning learns the value of optimal policy to
    walk along the optimal path, in which the agent travels right along the edge of
    the cliff. Unfortunately, this will result in occasionally falling off the cliff
    because of ε-greedy action selection. Whereas SARSA, on the other hand, takes
    the action selection into account and learns the longer and safer path through
    the upper part of the grid. Although Q-learning learns the value of the optimal
    policy, its online performance is worse than that of the SARSA, which learns the
    roundabout and safest policy. Even if we observe the following sum of rewards
    displayed in the following diagram, SARSA has a less negative sum of rewards during
    the episode than Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c28f559-985f-438f-a861-5a1e7566ecb2.png)'
  prefs: []
  type: TYPE_IMG
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many classic resources available for reinforcement learning, and
    we encourage the reader to go through them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'R.S. Sutton and A.G. Barto, *Reinforcement Learning: An Introduction*. *MIT
    Press*, Cambridge, MA, USA, 1998'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*RL Course* by *David Silver* from YouTube: [https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning* (Stanford) by *Andrew NG* form YouTube (Lectures 16- 20): [https://www.youtube.com/watch?v=UzxYlbK2c7E&list=PLA89DCFA6ADACE599](https://www.youtube.com/watch?v=UzxYlbK2c7E&list=PLA89DCFA6ADACE599)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Algorithms for reinforcement learning* by *Csaba* from *Morgan & Claypool*
    Publishers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Artificial Intelligence: A Modern Approach* 3^(rd) Edition, by *Stuart Russell*
    and *Peter Norvig*, *Prentice Hall*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you've learned various reinforcement learning techniques, like
    Markov decision process, Bellman equations, dynamic programming, Monte Carlo methods,
    Temporal Difference learning, including both on-policy (SARSA) and off-policy
    (Q-learning), with Python examples to understand its implementation in a practical
    way. You also learned how Q-learning is being used in many practical applications
    nowadays, as this method learns from trial and error by interacting with environments.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, *Further reading* has been provided for you if you would like to pursue
    reinforcement learning full-time. We wish you all the best!
  prefs: []
  type: TYPE_NORMAL
