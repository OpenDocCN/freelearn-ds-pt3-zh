<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Combining Pandas Objects</h1>
                </header>
            
            <article>
                
<p>A wide variety of options are available to combine two or more DataFrames or Series together. The <kbd>append</kbd> method is the least flexible and only allows for new rows to be appended to a DataFrame. The <kbd>concat</kbd> method is very versatile and can combine any number of DataFrames or Series on either axis. The <kbd>join</kbd> method provides fast lookups by aligning a column of one DataFrame to the index of others. The <kbd>merge</kbd> method provides SQL-like capabilities to join two DataFrames together. </p>
<p class="mce-root">In this chapter, we will cover the following topics:</p>
<ul>
<li>Appending new rows to DataFrames</li>
<li>Concatenating multiple DataFrames together</li>
<li>Comparing President Trump's and Obama's approval ratings</li>
<li>Understanding the differences between<span> </span><kbd>concat</kbd>,<span> </span><kbd>join</kbd>, and<span> </span><kbd>merge</kbd></li>
<li>Connecting to SQL databases</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Appending new rows to DataFrames</h1>
                </header>
            
            <article>
                
<p>When performing a data analysis, it is far more common to create new columns than new rows. This is because a new row of data usually represents a new observation and, as an analyst, it is typically not your job to continually capture new data. Data capture is usually left to other platforms like relational database management systems. Nevertheless, it is a necessary feature to know as it will crop up from time to time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will begin by appending rows to a small dataset with the <kbd>.loc</kbd> indexer and then transition to using the <kbd>append</kbd> method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the names dataset, and output it:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; names = pd.read_csv('data/names.csv')<br/>&gt;&gt;&gt; names</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4954144c-e882-4317-8aea-1ed33e15dc93.png" style="width:9.67em;height:10.42em;"/></div>
<ol start="2">
<li>Let's create a list that contains some new data and use the <kbd>.loc</kbd> indexer to set a single row label equal to this new data:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; new_data_list = ['Aria', 1]<br/>&gt;&gt;&gt; names.loc[4] = new_data_list<br/>&gt;&gt;&gt; names</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1da729da-b2ad-4cb5-abc6-aa45ba1f1c00.png" style="width:9.08em;height:11.83em;"/></div>
<ol start="3">
<li>The <kbd>.loc</kbd> indexer uses labels to refer to the rows. In this case, the row labels exactly match the integer location. It is possible to append more rows with non-integer labels:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; names.loc['five'] = ['Zach', 3]<br/>&gt;&gt;&gt; names</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2a083be8-7f33-46f1-a95f-316aaa26f946.png" style="width:9.75em;height:13.42em;"/></div>
<ol start="4">
<li>To be more explicit in associating variables to values, you may use a dictionary. Also, in this step, we can dynamically choose the new index label to be the length of the DataFrame:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; names.loc[len(names)] = {'Name':'Zayd', 'Age':2}<br/>&gt;&gt;&gt; names</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8ede39cb-b883-4529-9504-08a6509b4762.png" style="width:9.92em;height:14.83em;"/></div>
<ol start="5">
<li>A Series can hold the new data as well and works exactly the same as a dictionary:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; names.loc[len(names)] = pd.Series({'Age':32,<br/>                                       'Name':'Dean'})<br/>&gt;&gt;&gt; names</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7a5690e9-355e-44db-936a-2513551b28ec.png" style="width:10.08em;height:18.08em;"/></div>
<ol start="6">
<li>The preceding operations all use the <kbd>.loc</kbd> indexing operator to make changes to the <kbd>names</kbd> DataFrame in-place. There is no separate copy of the DataFrame that is returned. In the next few steps, we will look at the <kbd>append</kbd> method, which does not modify the calling DataFrame. Instead, it returns a new copy of the DataFrame with the appended row(s). Let's begin with the original <kbd>names</kbd> DataFrame and attempt to append a row. The first argument to <kbd>append</kbd> must be either another DataFrame, Series, dictionary, or a list of these, but not a list like the one in step 2. Let's see what happens when we attempt to use a dictionary with <kbd>append</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; names = pd.read_csv('data/names.csv')<br/>&gt;&gt;&gt; names.append({'Name':'Aria', 'Age':1})<br/><span class="ansi-red-fg">TypeError</span>: Can only append a Series if ignore_index=True or if the Series has a name</pre>
<ol start="7">
<li>This error message appears to be slightly incorrect. We are passing a DataFrame and not a Series but nevertheless, it gives us instructions on how to correct it:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; names.append({'Name':'Aria', 'Age':1}, ignore_index=True)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d1209a67-6e9c-400b-878f-3b4c6454fa05.png" style="width:10.08em;height:13.08em;"/></div>
<ol start="8">
<li>This works but <kbd>ignore_index</kbd> is a sneaky parameter. When set to <kbd>True</kbd>, the old index will be removed completely and replaced with a <kbd>RangeIndex</kbd> from 0 to n-1. For instance, let's specify an index for the <kbd>names</kbd> DataFrame:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; names.index = ['Canada', 'Canada', 'USA', 'USA']<br/>&gt;&gt;&gt; names</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5ce47674-b461-431f-ac0e-0deb1442f0f1.png" style="width:12.50em;height:10.33em;"/></div>
<ol start="9">
<li>Rerun the code from step 7 and you will get the same result. The original index is completely ignored.</li>
</ol>
<p> </p>
<ol start="10">
<li>Let's continue with this <kbd>names</kbd> dataset with these country strings in the index and use a Series that has a <kbd>name</kbd> attribute with the <kbd>append</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; s = pd.Series({'Name': 'Zach', 'Age': 3}, name=len(names))<br/>&gt;&gt;&gt; s<br/>Age        3<br/>Name    Zach
Name: 4, dtype: object<br/><br/>&gt;&gt;&gt; names.append(s)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c54f0095-e1c3-4527-b793-a90c3efa3aa3.png" style="width:10.92em;height:10.58em;"/></div>
<ol start="11">
<li>The <kbd>append</kbd> method is more flexible than the <kbd>.loc</kbd> indexer. It supports appending multiple rows at the same time. One way to accomplish this is with a list of Series:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; s1 = pd.Series({'Name': 'Zach', 'Age': 3}, name=len(names))<br/>&gt;&gt;&gt; s2 = pd.Series({'Name': 'Zayd', 'Age': 2}, name='USA')<br/>&gt;&gt;&gt; names.append([s1, s2])</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/49370d22-759a-4752-8eff-b1253447829e.png" style="width:11.00em;height:13.17em;"/></div>
<ol start="12">
<li>Small DataFrames with only two columns are simple enough to manually write out all the column names and values. When they get larger, this process will be quite painful. For instance, let's take a look at the 2016 baseball dataset:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; bball_16 = pd.read_csv('data/baseball16.csv')<br/>&gt;&gt;&gt; bball_16.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5c669c06-e152-40f1-a4ce-af3f4f91803a.png" style="width:52.67em;height:11.25em;"/></div>
<ol start="13">
<li>This dataset contains 22 columns and it would be easy to mistype a column name or forget one altogether if you were manually entering new rows of data. To help protect against these mistakes, let's select a single row as a Series and chain the <kbd>to_dict</kbd> method to it to get an example row as a dictionary:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; data_dict = bball_16.iloc[0].to_dict()<br/>&gt;&gt;&gt; print(data_dict)<br/>{'playerID': 'altuvjo01', 'yearID': 2016, 'stint': 1, 'teamID': 'HOU', 'lgID': 'AL', 'G': 161, 'AB': 640, 'R': 108, 'H': 216, '2B': 42, '3B': 5, 'HR': 24, 'RBI': 96.0, 'SB': 30.0, 'CS': 10.0, 'BB': 60, 'SO': 70.0, 'IBB': 11.0, 'HBP': 7.0, 'SH': 3.0, 'SF': 7.0, 'GIDP': 15.0}</pre>
<ol start="14">
<li>Clear the old values with a dictionary comprehension assigning any previous string value as an empty string and all others, missing values. This dictionary can now serve as a template for any new data you would like to enter:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; new_data_dict = {k: '' if isinstance(v, str) else <br/>                        np.nan for k, v in data_dict.items()}<br/>&gt;&gt;&gt; print(new_data_dict)<br/>{'playerID': '', 'yearID': nan, 'stint': nan, 'teamID': '', 'lgID': '', 'G': nan, 'AB': nan, 'R': nan, 'H': nan, '2B': nan, '3B': nan, 'HR': nan, 'RBI': nan, 'SB': nan, 'CS': nan, 'BB': nan, 'SO': nan, 'IBB': nan, 'HBP': nan, 'SH': nan, 'SF': nan, 'GIDP': nan}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The <kbd>.loc</kbd> indexing operator is used to select and assign data based on the row and column labels. The first value passed to it represents the row label. In step 2, <kbd>names.loc[4]</kbd> refers to the row with a label equal to the integer 4. This label does not currently exist in the DataFrame. The assignment statement creates a new row with data provided by the list. As was mentioned in the recipe, this operation modifies the <kbd>names</kbd> DataFrame itself. If there was a previously existing row with a label equal to the integer 4, this command would have written over it. This modification in-place makes this indexing operator riskier to use than the <kbd>append</kbd> method, which never modifies the original calling DataFrame.</p>
<p>Any valid label may be used with the <kbd>.loc</kbd> indexing operator, as seen in step 3. Regardless of what the new label value actually is, the new row will always be appended at the end. Even though assigning with a list works, for clarity it's best to use a dictionary so that we know exactly which columns are associated with each value, as done in step 4.</p>
<p>Step 5 shows a little trick to dynamically set the new label to be the current number of rows in the DataFrame. Data stored in a Series will also get assigned correctly as long as the index labels match the column names.</p>
<p>The rest of the steps use the <kbd>append</kbd> method, which is a simple method that only appends new rows to DataFrames. Most DataFrame methods allow both row and column manipulation through an <kbd>axis</kbd> parameter. One exception is with <kbd>append</kbd>, which can only append rows to DataFrames.</p>
<p>Using a dictionary of column names mapped to values isn't enough information for append to work, as seen by the error message in step 6. To correctly append a dictionary without a row name, you will have to set the <kbd>ignore_index</kbd> parameter to <kbd>True</kbd>. Step 10 shows you how to keep the old index by simply converting your dictionary to a Series. Make sure to use the <kbd>name</kbd> parameter, which is then used as the new index label. Any number of rows may be added with append in this manner by passing a list of Series as the first argument.</p>
<p>When wanting to append rows in this manner with a much larger DataFrame, you can avoid lots of typing and mistakes by converting a single row to a dictionary with the <kbd>to_dict</kbd> method and then using a dictionary comprehension to clear out all the old values replacing them with some defaults.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Appending a single row to a DataFrame is a fairly expensive operation and if you find yourself writing a loop to append single rows of data to a DataFrame, then you are doing it wrong. Let's first create 1,000 rows of new data as a list of Series:</p>
<pre>&gt;&gt;&gt; random_data = []<br/>&gt;&gt;&gt; for i in range(1000):<br/>        d = dict()<br/>        for k, v in data_dict.items():<br/>            if isinstance(v, str):<br/>                d[k] = np.random.choice(list('abcde'))<br/>            else:<br/>                d[k] = np.random.randint(10)<br/>        random_data.append(pd.Series(d, name=i + len(bball_16)))<br/><br/>&gt;&gt;&gt; random_data[0].head()<br/>2B    3
3B    9
AB    3
BB    9
CS    4
Name: 16, dtype: object</pre>
<p>Let's time how long it takes to loop through each item making one append at a time:</p>
<pre>&gt;&gt;&gt; %%timeit<br/>&gt;&gt;&gt; bball_16_copy = bball_16.copy()<br/>&gt;&gt;&gt; for row in random_data:<br/>        bball_16_copy = bball_16_copy.append(row)<br/>4.88 s ± 190 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</pre>
<p>That took nearly five seconds for only 1,000 rows. If we instead pass in the entire list of Series, we get an enormous speed increase:</p>
<pre>&gt;&gt;&gt; %%timeit<br/>&gt;&gt;&gt; bball_16_copy = bball_16.copy()<br/>&gt;&gt;&gt; bball_16_copy = bball_16_copy.append(random_data)<br/>78.4 ms ± 6.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</pre>
<p>By passing in the list of Series, the time has been reduced to under one-tenth of a second. Internally, pandas converts the list of Series to a single DataFrame and then makes the append.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Concatenating multiple DataFrames together</h1>
                </header>
            
            <article>
                
<p>The versatile <kbd>concat</kbd> function enables concatenating two or more DataFrames (or Series) together, both vertically and horizontally. As per usual, when dealing with multiple pandas objects simultaneously, concatenation doesn't happen haphazardly but aligns each object by their index.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we combine DataFrames both horizontally and vertically with the <kbd>concat</kbd> function and then change the parameter values to yield different results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Read in the 2016 and 2017 stock datasets, and make their ticker symbol the index:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; stocks_2016 = pd.read_csv('data/stocks_2016.csv', <br/>                              index_col='Symbol')<br/>&gt;&gt;&gt; stocks_2017 = pd.read_csv('data/stocks_2017.csv',<br/>                              index_col='Symbol')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/78a928de-3aa5-444b-b6c3-8f74e590a2ca.png" style="width:14.33em;height:10.58em;"/>    <img src="assets/82f29089-c05d-4bf1-8208-be3212c8b259.png" style="width:10.75em;height:11.58em;"/></div>
<ol start="2">
<li>Place all the <kbd>stock</kbd> datasets into a single list, and then call the <kbd>concat</kbd> function to concatenate them together:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; s_list = [stocks_2016, stocks_2017]<br/>&gt;&gt;&gt; pd.concat(s_list)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d273561a-b8ee-4400-84c6-6fe99b2870ec.png" style="width:11.92em;height:18.50em;"/></div>
<ol start="3">
<li>By default, the <kbd>concat</kbd> function concatenates DataFrames vertically, one on top of the other. One issue with the preceding DataFrame is that there is no way to identify the year of each row. The <kbd>concat</kbd> function allows each piece of the resulting DataFrame to be labeled with the <kbd>keys</kbd> parameter. This label will appear in the outermost index level of the concatenated frame and force the creation of a MultiIndex. Also, the <kbd>names</kbd> parameter has the ability to rename each index level for clarity:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pd.concat(s_list, keys=['2016', '2017'], <br/>              names=['Year', 'Symbol'])</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/010d420f-feba-4c21-9d7c-3f60a806cf2e.png" style="width:13.75em;height:18.25em;"/></div>
<ol start="4">
<li>It is also possible to concatenate horizontally by changing the <kbd>axis</kbd> parameter to <em>columns</em> or <em>1</em>:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pd.concat(s_list, keys=['2016', '2017'],<br/>              axis='columns', names=['Year', None])</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6d248ec3-bfb5-496c-96ae-2b5d267e5c77.png" style="width:22.25em;height:17.08em;"/></div>
<ol start="5">
<li>Notice that missing values appear whenever a stock symbol is present in one year but not the other. The <kbd>concat</kbd> function, by default, uses an outer join, keeping all rows from each DataFrame in the list.  However, it gives us options to only keep rows that have the same index values in both DataFrames. This is referred to as an inner join. We set the <kbd>join</kbd> parameter to <em>inner</em> to change the behavior:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pd.concat(s_list, join='inner', keys=['2016', '2017'],<br/>              axis='columns', names=['Year', None])</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4740c498-d459-403c-bd63-17d7554797d5.png" style="width:20.42em;height:8.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>The first argument is the only argument required for the <kbd>concat</kbd> function and it must be a sequence of pandas objects, typically a list or dictionary of DataFrames or Series. By default, all these objects will be stacked vertically one on top of the other. In this recipe, only two DataFrames are concatenated, but any number of pandas objects work. When we were concatenating vertically, the DataFrames align by their column names.</span></p>
<p><span>In this dataset, all the column names were the same so each column in the 2017 data lined up precisely under the same column name in the 2016 data. However, when they were concatenated horizontally, as in step 4, only two of the index labels matched from both years--<em>AAPL</em> and <em>TSLA</em>. Therefore, these ticker symbols had no missing values for either year.</span> There are two types of alignment possible using <kbd>concat</kbd>, <em>outer</em> (the default) and <em>inner</em> referred to by the <kbd>join</kbd> parameter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The <kbd>append</kbd> method is a heavily watered down version of <kbd>concat</kbd> that can only append new rows to a DataFrame. <span>Internally, <kbd>append</kbd> just calls the <kbd>concat</kbd> function.</span> For instance, step 2 from this recipe may be duplicated with the following:</p>
<pre>&gt;&gt;&gt; stocks_2016.append(stocks_2017)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing President Trump's and Obama's approval ratings</h1>
                </header>
            
            <article>
                
<p>Public support of the current President of the United States is a topic that frequently makes it into news headlines and is formally measured through opinion polls. In recent years, there has been a rapid increase in the frequency of these polls and lots of new data rolls in each week. There are many different pollsters that each have their own questions and methodology to capture their data, and thus there exists quite a bit of variability among the data. <span>The</span> <span>American Presidency Project from the University of California, Santa Barbara, provides an aggregate approval rating down to a single data point each day.</span></p>
<p>Unlike most of the recipes in this book, the data is not readily available in a CSV file. Often, as a data analyst, you will need to find data on the web, and use a tool that can scrape it into a format that you can then parse through your local workstation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will use the <kbd>read_html</kbd> function, which comes heavily equipped to scrape data from tables online and turn them into DataFrames. You will also learn how to inspect web pages to find the underlying HTML for certain elements. I used Google Chrome as my browser and suggest you use it, or Firefox, for the web-based steps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Navigate to <em>The American Presidency Project</em> approval page for President Donald Trump (<a href="http://www.presidency.ucsb.edu/data/popularity.php?pres=45" target="_blank">http://www.presidency.ucsb.edu/data/popularity.php?pres=45</a>). You should get a page that contains a time series plot with the data in a table directly following it:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/aa1d10d8-b146-408e-a53e-c0949b87086e.png" style="width:49.58em;height:32.33em;"/></div>
<ol start="2">
<li>The <kbd>read_html</kbd> <span>function</span> is able to scrape tables off web pages and place their data into DataFrames. It works best with simple HTML tables and provides some useful parameters to select the exact table you desire in case there happen to be multiple tables on the same page. Let's go ahead and use <kbd>read_html</kbd> with its default values, which will return all the tables as DataFrames in a list:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; base_url = 'http://www.presidency.ucsb.edu/data/popularity.php?pres={}'<br/>&gt;&gt;&gt; trump_url = base_url.format(45)<br/>&gt;&gt;&gt; df_list = pd.read_html(trump_url)<br/>&gt;&gt;&gt; len(df_list)<br/>14</pre>
<ol start="3">
<li>The function has returned 14 tables, which seems preposterous at first, as the web page appears to show only a single element that most people would recognize as a table. The <kbd>read_html</kbd> function formally searches for HTML table elements that begin with <em>&lt;table</em>. Let's inspect the HTML page by right-clicking on the approval data table and selecting <span class="packt_screen">inspect</span> or <span class="packt_screen">inspect element</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e0ac10b2-f06d-41b7-8812-e98ff764ef59.png" style="width:43.50em;height:20.58em;"/></div>
<ol start="4">
<li>This opens up the console, which is a very powerful tool for web development. For this recipe, we will only need it for a few tasks. All consoles allow you to search the HTML for a specific word. Let's search for the word <kbd>table</kbd>. My browser found 15 different HTML tables, very close to the number returned by <kbd>read_html</kbd>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fa149f24-7f39-49b5-9ea0-17bb8a32f54c.png" style="width:21.25em;height:7.92em;"/></div>
<ol start="5">
<li>Let's begin inspecting the DataFrames in <kbd>df_list</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; df0 = df_list[0]<br/>&gt;&gt;&gt; df0.shape<br/>(308, 1794)<br/><br/>&gt;&gt;&gt; df0.head(7)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f2c5a7e0-0994-47a1-89a9-dc3872414d23.png" style="width:29.42em;height:16.67em;"/></div>
<ol start="6">
<li>Looking back at the web page, there is a row in the approval table for nearly each day beginning January 22, 2017, until the day the data was scraped--September 25, 2017. This is a little more than eight months or 250 rows of data, which is somewhat close to the 308 lines in that first table. Scanning through the rest of the tables, you can see that lots of empty meaningless tables were discovered, as well as tables for different parts of the web page that don't actually resemble tables. Let's use some of the parameters of the <kbd>read_html</kbd> function to help us select the table we desire. We can use the <kbd>match</kbd> parameter to search for a specific string in the table. Let's search for a table with the word <em>Start Date</em> in it:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; df_list = pd.read_html(trump_url, match='Start Date')<br/>&gt;&gt;&gt; len(df_list)<br/>3</pre>
<ol start="7">
<li>By searching for a specific string in the table, we have reduced the number of tables down to just three. Another useful parameter is <kbd>attrs</kbd>, which accepts a dictionary of HTML attributes paired with their value. We would like to find some unique attributes for our particular table. To do this, let's right-click again in our data table. This time, make sure to click at the very top in one of the table headers. For example, right click on <em>President,</em> and select <span class="packt_screen">inspect</span> or <span class="packt_screen">inspect element</span> again:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8453c5fd-4db1-4cb7-9373-7c92d335c3ce.png" style="width:24.00em;height:11.25em;"/></div>
<ol start="8">
<li>The element that you selected should be highlighted. This is actually not the element we are interested in. Keep looking until you come across an HTML tag beginning with <em>&lt;table</em>. All the words to the left of the equal signs are the attributes or <kbd>attrs</kbd> and to the right are the values. Let's use the <em>align</em> attribute with its value <em>center</em> in our search:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; df_list = pd.read_html(trump_url, match='Start Date',<br/>                           attrs={'align':'center'})<br/>&gt;&gt;&gt; len(df_list)<br/>1<br/><br/>&gt;&gt;&gt; trump = df_list[0]<br/>&gt;&gt;&gt; trump.shape<br/>(249, 19)<br/><br/>&gt;&gt;&gt; trump.head(8)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/42221dad-03c0-4e9b-9bf7-2e6aec18e4d6.png" style="width:32.42em;height:18.25em;"/></div>
<ol start="9">
<li>We only matched with one table and the number of rows is very close to the total days between the first and last dates. Looking at the data, it appears that we have indeed found the table we are looking for. The six column names appear to be on line 4. We can go even further and precisely select the rows we want to skip and which row we would like to use for the column names with the <kbd>skiprows</kbd> and <kbd>header</kbd> parameters. We can also make sure that the start and end dates are coerced correctly to the right data type with the <kbd>parse_dates</kbd> parameter:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; df_list = pd.read_html(trump_url, match='Start Date',<br/>                           attrs={'align':'center'}, <br/>                           header=0, skiprows=[0,1,2,3,5], <br/>                           parse_dates=['Start Date',<br/>                                        'End Date'])<br/>&gt;&gt;&gt; trump = df_list[0]<br/>&gt;&gt;&gt; trump.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/99e47525-f4c0-44cf-84be-62eceaa334de.png" style="width:58.17em;height:11.08em;"/></div>
<ol start="10">
<li>This is almost exactly what we want, except for the columns with missing values. Let's use the <kbd>dropna</kbd> method to drop columns with all values missing:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; trump = trump.dropna(axis=1, how='all')<br/>&gt;&gt;&gt; trump.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/907ada91-650c-4666-8dac-f7347b19b9fe.png" style="width:35.67em;height:11.00em;"/></div>
<ol start="11">
<li>Let's fill the missing values in the <kbd>President</kbd> column in a forward direction with the <kbd>ffill</kbd> method. Let's first check whether there are any missing values in the other columns:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; trump.isnull().sum()<br/>President         242
Start Date          0
End Date            0
Approving           0
Disapproving        0
unsure/no data      0
dtype: int64<br/><br/>&gt;&gt;&gt; trump = trump.ffill()<br/>trump.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/da9fc9fb-8b2e-4221-893d-105d8bca02d1.png" style="width:31.33em;height:9.58em;"/></div>
<ol start="12">
<li>Finally, it is important to check the data types to ensure they are correct:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; trump.dtypes<br/>President                 object
Start Date        datetime64[ns]
End Date          datetime64[ns]
Approving                  int64
Disapproving               int64
unsure/no data             int64
dtype: object</pre>
<ol start="13">
<li>Let's build a function with all the steps combined into one to automate the process of retrieving approval data for any President:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; def get_pres_appr(pres_num):<br/>        base_url =\<br/>'http://www.presidency.ucsb.edu/data/popularity.php?pres={}'<br/>        pres_url = base_url.format(pres_num)<br/>        df_list = pd.read_html(pres_url, match='Start Date',<br/>                               attrs={'align':'center'}, <br/>                               header=0, skiprows=[0,1,2,3,5], <br/>                               parse_dates=['Start Date',<br/>                                            'End Date'])<br/>        pres = df_list[0].copy()<br/>        pres = pres.dropna(axis=1, how='all')<br/>        pres['President'] = pres['President'].ffill()<br/>        return pres.sort_values('End Date') \<br/>                   .reset_index(drop=True)</pre>
<ol start="14">
<li>The only parameter, <kbd>pres_num</kbd>, denotes the order number of each president. Barack Obama was the 44th President of the United States; pass 44 to the <kbd>get_pres_appr</kbd> function to retrieve his approval numbers:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; obama = get_pres_appr(44)<br/>&gt;&gt;&gt; obama.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6247a2a4-700b-48c1-97ee-065dee3a30d6.png" style="width:29.25em;height:9.25em;"/></div>
<ol start="15">
<li>There is Presidential approval rating data dating back to 1941 during President Franklin Roosevelt's third term. With our custom function along with the <kbd>concat</kbd> function, it is possible to grab all the presidential approval rating data from this site. For now, let's just grab the approval rating data for the last five presidents and output the first three rows for each President:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pres_41_45 = pd.concat([get_pres_appr(x) for x in range(41,46)],<br/>                            ignore_index=True)<br/>&gt;&gt;&gt; pres_41_45.groupby('President').head(3)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b5510c6f-12c7-4faa-8711-6c8ece33933d.png" style="width:29.92em;height:23.58em;"/></div>
<ol start="16">
<li>Before continuing, let's determine if there are any dates with multiple approval ratings:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pres_41_45['End Date'].value_counts().head(8)<br/>1990-08-26    2
1990-03-11    2
1999-02-09    2
2013-10-10    2
1990-08-12    2
1992-11-22    2
1990-05-22    2
1991-09-30    1
Name: End Date, dtype: int64</pre>
<ol start="17">
<li>Only a few of the days have duplicate values. To help simplify our analysis, let's keep only the first row where the duplicate date exists:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pres_41_45 = pres_41_45.drop_duplicates(subset='End Date')</pre>
<ol start="18">
<li>Let's get a few summary statistics on the data:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pres_41_45.shape<br/>(3679, 6)<br/><br/>&gt;&gt;&gt; pres_41_45['President'].value_counts()<br/>Barack Obama          2786
George W. Bush         270
Donald J. Trump        243
William J. Clinton     227
George Bush            153
Name: President, dtype: int64<br/><br/>&gt;&gt;&gt; pres_41_45.groupby('President', sort=False) \<br/>                       .median().round(1)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/118778a4-f5d5-4d65-a816-2a35a1024802.png" style="width:27.08em;height:14.08em;"/></div>
<ol start="19">
<li>Let's plot each President's approval rating on the same chart. To do this, we will group by each President, iterate through each group, and individually plot the approval rating for each date:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; from matplotlib import cm<br/>&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(16,6))<br/><br/>&gt;&gt;&gt; styles = ['-.', '-', ':', '-', ':']<br/>&gt;&gt;&gt; colors = [.9, .3, .7, .3, .9]<br/>&gt;&gt;&gt; groups = pres_41_45.groupby('President', sort=False)<br/><br/>&gt;&gt;&gt; for style, color, (pres, df) in zip(styles, colors, groups):<br/>        df.plot('End Date', 'Approving', ax=ax,<br/>                label=pres, style=style, color=cm.Greys(color), <br/>                title='Presedential Approval Rating')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8b6bbb45-0f79-4cbd-95f5-e47498c98c9e.png" style="width:64.08em;height:24.75em;"/></div>
<ol start="20">
<li>This chart places all the Presidents sequentially one after the other. We can compare them on a simpler scale by plotting their approval rating against the number of days in office. Let's create a new variable to represent the number of days in office:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; days_func = lambda x: x - x.iloc[0]<br/>&gt;&gt;&gt; pres_41_45['Days in Office'] = pres_41_45.groupby('President') \<br/>                                             ['End Date'] \<br/>                                             .transform(days_func)<br/>&gt;&gt;&gt; pres_41_45.groupby('President').head(3)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b63c31b6-e297-40e4-b17a-49bddb24e807.png" style="width:37.25em;height:25.17em;"/></div>
<ol start="21">
<li>We have successfully given each row a relative number of days since the start of the presidency. It's interesting that the new column, <kbd>Days in Office</kbd>, has a string representation of its value. Let's check its data type:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pres_41_45.dtypes<br/>...<br/>Days in Office    timedelta64[ns]
dtype: object</pre>
<ol start="22">
<li>The <kbd>Days in Office</kbd> column is a <kbd>timedelta64</kbd> object with nanosecond precision. This is far more precision than is needed. Let's change the data type to integer by getting just the days:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pres_41_45['Days in Office'] = pres_41_45['Days in Office'] \<br/>                                             .dt.days<br/>&gt;&gt;&gt; pres_41_45['Days in Office'].head()<br/>0     0
1    32
2    35
3    43
4    46
Name: Days in Office, dtype: int64</pre>
<ol start="23">
<li>We could plot this data in a similar fashion to what we did in step 19, but there is a completely different method that doesn't involve any looping. By default, when calling the <kbd>plot</kbd> method on a DataFrame, pandas attempts to plot each column of data as a line plot and uses the index as the x-axis. Knowing this, let's pivot our data so that each President has his own column for approval rating:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pres_pivot = pres_41_45.pivot(index='Days in Office',<br/>                                  columns='President',<br/>                                  values='Approving')<br/>&gt;&gt;&gt; pres_pivot.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/16d28567-1ef0-412b-9a27-918e22994808.png" style="width:39.83em;height:13.00em;"/></div>
<ol start="24">
<li>Now that each President has his own column of approval ratings, we can plot each column directly without grouping. To reduce the clutter in the plot, we will only plot Barack Obama and Donald J. Trump:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; plot_kwargs = dict(figsize=(16,6), color=cm.gray([.3, .7]), <br/>                       style=['-', '--'], title='Approval Rating')<br/>&gt;&gt;&gt; pres_pivot.loc[:250, ['Donald J. Trump', 'Barack Obama']] \<br/>              .ffill().plot(**plot_kwargs)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/82e84686-c9f1-4e28-bdfc-393437ce7825.png" style="width:44.83em;height:18.67em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>It is typical to call <kbd>read_html</kbd> multiple times before arriving at the table (or tables) that you desire. There are two primary parameters at your disposal to specify a table, <kbd>match</kbd> and <kbd>attrs</kbd>. The string provided to <kbd>match</kbd> is used to find an exact match for the actual text in the table. This is text that will show up on the web page itself. The <kbd>attrs</kbd> parameter, on the other hand, searches for HTML table attributes found directly after the start of the table tag, <kbd>&lt;table</kbd>. To see more of the table attributes, visit this page from W3 Schools (<a href="https://www.w3schools.com/TagS/tag_table.asp" target="_blank">http://bit.ly/2hzUzdD</a>).</p>
<p>Once we find our table in step 8, we can still take advantage of some other parameters to simplify things. HTML tables don't typically translate directly to nice DataFrames. There are often missing column names, extra rows, and misaligned data. In this recipe, <kbd>skiprows</kbd> is passed a list of row numbers to skip over when reading the file. They correspond to the rows of missing values in the DataFrame output from step 8. The <kbd>header</kbd> parameter is also used to specify the location of the column names. Notice that <kbd>header</kbd> is equal to zero, which may seem wrong at first. Whenever the header parameter is used in conjunction with <kbd>skiprows</kbd>, the rows are skipped first resulting in a new integer label for each row. The correct column names are in row 4 but as we skipped rows 0 through 3, the new integer label for it is 0.</p>
<p>In step 11, the <kbd>ffill</kbd> method fills any missing values vertically, going down with the last non-missing value. This method is just a shortcut for <kbd>fillna(method='ffill')</kbd>.</p>
<p>Step 13 builds a function composed of all the previous steps to automatically get approval ratings from any President, provided you have the order number. There are a few differences in the function. Instead of applying the <kbd>ffill</kbd> method to the entire DataFrame, we only apply it to the <kbd>President</kbd> column. In Trump's DataFrame, the other columns had no missing data but this does not guarantee that all the scraped tables will have no missing data in their other columns. The last line of the function sorts the dates in a more natural way for data analysis from the oldest to newest. This changes the order of the index too, so we discard it with <kbd>reset_index</kbd> to have it begin from zero again.</p>
<p>Step 16 shows a common pandas idiom for collecting multiple, similarly indexed DataFrames into a list before combining them together with the <kbd>concat</kbd> function. After concatenation into a single DataFrame, we should visually inspect it to ensure its accuracy. One way to do this is to take a glance at the first few rows from each President's section by grouping the data and then using the <kbd>head</kbd> method on each group.</p>
<p>The summary statistics in step 18 are interesting as each successive President has had lower median approval than the last. Extrapolating the data would lead to naively predicting a negative approval rating within the next several Presidents.</p>
<p>The plotting code in step 19 is fairly complex. You might be wondering why we need to iterate through a <kbd>groupby</kbd> object, to begin with. In the DataFrame's current structure, it has no ability to plot different groups based on values in a single column. However, step 23 shows you how to set up your DataFrame so that pandas can directly plot each President's data without a loop like this.</p>
<p><span>To understand the plotting code in step 19, you must first be aware that a <kbd>groupby</kbd> object is iterable and, when iterating through, yields a tuple containing the current group (here it's just the name of the President) and the sub-DataFrame for just that group. This <kbd>groupby</kbd> object is zipped together with values controlling the color and linestyle of the plot. We import the colormap module, <kbd>cm</kbd>, from matplotlib which contains dozens of different colormaps. Passing a float between 0 and 1 chooses a specific color from that colormap and we use it in our <kbd>plot</kbd> method with the <kbd>color</kbd> parameter. It is also important to note that we had to create the figure, <kbd>fig</kbd>, along with a plotting surface, <kbd>ax</kbd>, to ensure that each approval line was placed on the same plot. At each iteration in the loop, we use the same plotting surface with the identically named parameter, <kbd>ax</kbd>.</span></p>
<p>To make a better comparison between Presidents, we create a new column equal to the number of days in office. We subtract the first date from the rest of the dates per President group. When two <kbd>datetime64</kbd> columns are subtracted, the result is a <kbd>timedelta64</kbd> object, which represents some length of time, days in this case. If we leave the column with nanosecond precision, the x-axis will similarly display too much precision by using the special <kbd>dt</kbd> accessor to return the number of days.</p>
<p>A crucial step comes in step 23. We structure the data such that each President has a unique column for their approval rating. Pandas makes a separate line for each column. Finally, in step 24, we use the <kbd>.loc</kbd> indexer to simultaneously select the first 250 days (rows) along with only the columns for just Trump and Obama. The <kbd>ffill</kbd> method is used in the rare instances that one of the Presidents has a missing value for a particular day. In Python, it is possible to pass dictionaries that contain the parameter names and their values to functions by preceding them with <kbd>**</kbd> in a process called <strong>dictionary unpacking</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The plot from step 19 shows quite a lot of noise and the data might be easier to interpret if it were smoothed. One common smoothing method is called the <strong>rolling average</strong>. Pandas offers the <kbd>rolling</kbd> method for DataFrames and <kbd>groupby</kbd> objects. It works analogously to the <kbd>groupby</kbd> method by returning an object waiting for an additional action to be performed on it. When creating it, you must pass the size of the window as the first argument, which can either be an integer or a date offset string.</p>
<p>In this example, we take a 90-day moving average with the date offset string <em>90D</em>. The <kbd>on</kbd> parameter specifies the column from which the rolling window is calculated:</p>
<pre>&gt;&gt;&gt; pres_rm = pres_41_45.groupby('President', sort=False) \<br/>                        .rolling('90D', on='End Date')['Approving'] \<br/>                        .mean()<br/>&gt;&gt;&gt; pres_rm.head()<br/>President    End Date   
George Bush  1989-01-26    51.000000
             1989-02-27    55.500000
             1989-03-02    57.666667
             1989-03-10    58.750000
             1989-03-13    58.200000
Name: Approving, dtype: float64</pre>
<p>From here, we can restructure the data so that it looks similar to the output from step 23 with the <kbd>unstack</kbd> method, and then make our plot:</p>
<pre>&gt;&gt;&gt; styles = ['-.', '-', ':', '-', ':']<br/>&gt;&gt;&gt; colors = [.9, .3, .7, .3, .9]<br/>&gt;&gt;&gt; color = cm.Greys(colors)<br/>&gt;&gt;&gt; title='90 Day Approval Rating Rolling Average'<br/>&gt;&gt;&gt; plot_kwargs = dict(figsize=(16,6), style=styles,<br/>                       color = color, title=title)<br/>&gt;&gt;&gt; correct_col_order = pres_41_45.President.unique()<br/><br/>&gt;&gt;&gt; pres_rm.unstack('President')[correct_col_order].plot(**plot_kwargs)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d120b3da-ec23-4575-b869-80eb4a9d486c.png" style="width:59.33em;height:23.00em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Colormap references for matplotlib (<a href="https://matplotlib.org/examples/color/colormaps_reference.html" target="_blank">http://bit.ly/2yJZOvt</a>)</li>
<li>A list of all the date offsets and their aliases (<a href="http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases" target="_blank">http://bit.ly/2xO5Yg0</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the differences between concat, join, and merge</h1>
                </header>
            
            <article>
                
<p>The <kbd>merge</kbd> and <kbd>join</kbd> DataFrame (and not Series) methods and the <kbd>concat</kbd> function all provide very similar functionality to combine multiple pandas objects together. As they are so similar and they can replicate each other in certain situations, it can get very confusing when and how to use them correctly. To help clarify their differences, take a look at the following outline:</p>
<ul>
<li><kbd>concat</kbd>:
<ul>
<li>Pandas function</li>
<li>Combines two or more pandas objects vertically or horizontally</li>
<li>Aligns only on the index</li>
<li>Errors whenever a duplicate appears in the index</li>
<li>Defaults to outer join with option for inner</li>
</ul>
</li>
<li><kbd>join</kbd>:
<ul>
<li>DataFrame method</li>
<li>Combines two or more pandas objects horizontally</li>
<li>Aligns the calling DataFrame's column(s) or index with the other objects' index (and not the columns)</li>
<li>Handles duplicate values on the joining columns/index by performing a cartesian product</li>
<li>Defaults to left join with options for inner, outer, and right</li>
</ul>
</li>
<li><kbd>merge</kbd>:
<ul>
<li>DataFrame method</li>
<li>Combines exactly two DataFrames horizontally</li>
<li>Aligns the calling DataFrame's column(s)/index with the other DataFrame's column(s)/index</li>
<li>Handles duplicate values on the joining columns/index by performing a cartesian product</li>
<li>Defaults to inner join with options for left, outer, and right</li>
</ul>
</li>
</ul>
<div class="packt_infobox">The first parameter to the join method is <kbd>other</kbd> which can either be a single DataFrame/Series or a list of any number of DataFrames/Series.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, <span>we will do what is </span>required to combine DataFrames. The first situation is simpler with <kbd>concat</kbd> while the second is simpler with <kbd>merge</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Let's read in stock data for 2016, 2017, and 2018 into a list of DataFrames using a loop instead of three different calls to the <kbd>read_csv</kbd> function. Jupyter notebooks currently only allow a single DataFrame to be displayed on one line. However, there is a way to customize the HTML output with help from the <kbd>IPython</kbd> library. The user-defined <kbd>display_frames</kbd> <span>function</span> accepts a list of DataFrames and outputs them all in a single row:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; from IPython.display import display_html<br/><br/>&gt;&gt;&gt; years = 2016, 2017, 2018<br/>&gt;&gt;&gt; stock_tables = [pd.read_csv('data/stocks_{}.csv'.format(year),<br/>                                index_col='Symbol') <br/>                    for year in years]<br/><br/>&gt;&gt;&gt; def display_frames(frames, num_spaces=0):<br/>        t_style = '&lt;table style="display: inline;"'<br/>        tables_html = [df.to_html().replace('&lt;table', t_style) <br/>                       for df in frames]<br/><br/>        space = '&amp;nbsp;' * num_spaces<br/>        display_html(space.join(tables_html), raw=True)<br/><br/>&gt;&gt;&gt; display_frames(stock_tables, 30)<br/>&gt;&gt;&gt; stocks_2016, stocks_2017, stocks_2018 = stock_tables</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/334685b5-02c5-4a06-a159-caccb833fc01.png" style="width:44.08em;height:12.17em;"/></div>
<ol start="2">
<li>The <kbd>concat</kbd> function is the only one able to combine DataFrames vertically. Let's do this by passing it the list <kbd>stock_tables</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pd.concat(stock_tables, keys=[2016, 2017, 2018])</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0c817ede-3963-45a8-961e-4ba60ab03d7c.png" style="width:14.25em;height:23.08em;"/></div>
<ol start="3">
<li>It can also combine DataFrames horizontally by changing the <kbd>axis</kbd> parameter to <kbd>columns</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pd.concat(dict(zip(years,stock_tables)), axis='columns')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/761c169f-57ab-4ac7-aae1-e5e83bda06a3.png" style="width:28.50em;height:16.50em;"/></div>
<ol start="4">
<li>Now that we have started combining DataFrames horizontally, we can use the <kbd>join</kbd> and <kbd>merge</kbd> methods to replicate this functionality of <kbd>concat</kbd>. Here, we use the <kbd>join</kbd> method to combine the <kbd>stock_2016</kbd> and <kbd>stock_2017</kbd> DataFrames. By default, the DataFrames align on their index. If any of the columns have the same names, then you must supply a value to the <kbd>lsuffix</kbd> or <kbd>rsuffix</kbd> parameters to distinguish them in the result:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; stocks_2016.join(stocks_2017, lsuffix='_2016',<br/>                     rsuffix='_2017', how='outer')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7003e693-6693-46b5-9880-13e184c2612d.png" style="width:31.42em;height:15.00em;"/></div>
<ol start="5">
<li>To exactly replicate the output of the <kbd>concat</kbd> function from step 3, we can pass a list of DataFrames to the <kbd>join</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; other = [stocks_2017.add_suffix('_2017'),<br/>             stocks_2018.add_suffix('_2018')]<br/>&gt;&gt;&gt; stocks_2016.add_suffix('_2016').join(other, how='outer')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/146587e7-f778-4917-8eac-06e4f8208f7a.png" style="width:46.25em;height:15.25em;"/></div>
<ol start="6">
<li>Let's check whether they actually are exactly equal:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; stock_join = stocks_2016.add_suffix('_2016').join(other, <br/>                                                      how='outer')<br/>&gt;&gt;&gt; stock_concat = pd.concat(dict(zip(years,stock_tables)),<br/>                             axis='columns')<br/>&gt;&gt;&gt; level_1 = stock_concat.columns.get_level_values(1)<br/>&gt;&gt;&gt; level_0 = stock_concat.columns.get_level_values(0).astype(str)<br/>&gt;&gt;&gt; stock_concat.columns = level_1 + '_' + level_0<br/>&gt;&gt;&gt; stock_join.equals(stock_concat)<br/>True</pre>
<ol start="7">
<li>Now, let's turn to <kbd>merge</kbd> that, unlike <kbd>concat</kbd> and <kbd>join</kbd>, can combine exactly two DataFrames together. By default, <kbd>merge</kbd> attempts to align the values in the columns that have the same name for each of the DataFrames. However, you can choose to have it align on the index by setting the boolean parameters <kbd>left_index</kbd> and <kbd>right_index</kbd> to <kbd>True</kbd>. Let's merge the 2016 and 2017 stock data together:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; stocks_2016.merge(stocks_2017, left_index=True, <br/>                      right_index=True)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/075849a5-3ab9-4b9c-a914-be9e715ea643.png" style="width:25.75em;height:7.83em;"/></div>
<ol start="8">
<li>By default, merge uses an inner join and automatically supplies suffixes for identically named columns. Let's change to an outer join and then perform another outer join of the 2018 data to exactly replicate <kbd>concat</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; step1 = stocks_2016.merge(stocks_2017, left_index=True, <br/>                              right_index=True, how='outer',<br/>                              suffixes=('_2016', '_2017'))<br/><br/>&gt;&gt;&gt; stock_merge = step1.merge(stocks_2018.add_suffix('_2018'), <br/>                              left_index=True, right_index=True,<br/>                              how='outer')<br/><br/>&gt;&gt;&gt; stock_concat.equals(stock_merge)<br/>True</pre>
<ol start="9">
<li>Now let's turn our comparison to datasets where we are interested in aligning together the values of columns and not the index or column labels themselves. The <kbd>merge</kbd> method is built exactly for this situation. Let's take a look at two new small datasets, <kbd>food_prices</kbd> and <kbd>food_transactions</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; names = ['prices', 'transactions']<br/>&gt;&gt;&gt; food_tables = [pd.read_csv('data/food_{}.csv'.format(name)) <br/>                    for name in names]<br/>&gt;&gt;&gt; food_prices, food_transactions = food_tables<br/>&gt;&gt;&gt; display_frames(food_tables, 30)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f98e02af-0bc9-4214-bff8-783bdda27391.png" style="width:37.42em;height:18.08em;"/></div>
<ol start="10">
<li>If we wanted to find the total amount of each transaction, we would need to join these tables on the <kbd>item</kbd> and <kbd>store</kbd> columns:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; food_transactions.merge(food_prices, on=['item', 'store'])</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/32a7e87a-018f-40bb-bb3e-123be98fdf88.png" style="width:22.00em;height:17.75em;"/></div>
<ol start="11">
<li>The price is now aligned correctly with its corresponding item and store, but there is a problem. Customer 2 has a total of four <kbd>steak</kbd> items. As the <kbd>steak</kbd> item appears twice in each table for store <kbd>B</kbd>, a Cartesian product takes place between them, resulting in four rows. Also, notice that the item, <kbd>coconut</kbd>, is missing because there was no corresponding price for it. Let's fix both of these issues:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; food_transactions.merge(food_prices.query('Date == 2017'),<br/>                            how='left')</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cbeec36e-7160-4664-a8a2-983ad7e70c41.png" style="width:26.00em;height:18.08em;"/></div>
<ol start="12">
<li>We can replicate this with the <kbd>join</kbd> method but we must <span>first</span> put the joining columns of the <kbd>food_prices</kbd> DataFrame into the index:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; food_prices_join = food_prices.query('Date == 2017') \<br/>                                  .set_index(['item', 'store'])<br/>&gt;&gt;&gt; food_prices_join</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a6e73c76-1791-4dd5-b906-a5f9b0f386b1.png" style="width:13.83em;height:21.75em;"/></div>
<ol start="13">
<li>The <kbd>join</kbd> method only aligns with the index of the passed DataFrame but can use the index or the columns of the calling DataFrame. To use columns for alignment on the calling DataFrame, you will need to pass them to the <kbd>on</kbd> parameter:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; food_transactions.join(food_prices_join, on=['item', 'store'])</pre>
<ol start="14">
<li>The output matches the result from step 11 exactly. To replicate this with the <kbd>concat</kbd> method, you would need to put the item and store columns into the index of both DataFrames. However, in this particular case, an error would be produced as a duplicate index value occurs in at least one of the DataFrames (with item <kbd>steak</kbd> and store <kbd>B</kbd>):</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; pd.concat([food_transactions.set_index(['item', 'store']), <br/>               food_prices.set_index(['item', 'store'])],<br/>              axis='columns')<br/><span class="ansi-red-fg">Exception</span>: cannot handle a non-unique multi-index!</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>It can get tedious to repeatedly write the <kbd>read_csv</kbd> <span>function</span> when importing many DataFrames at the same time. One way to automate this process is to put all the file names in a list and iterate through them with a for loop. This was done in step 1 with a list comprehension.</p>
<p>The rest of this step builds a function to display multiple DataFrames on the same line of output in a Jupyter notebook. All DataFrames have a <kbd>to_html</kbd> method, which returns a raw HTML string representation of the table. The CSS (cascading style sheet) of each table is changed by altering the <kbd>display</kbd> attribute to <em>inline</em> so that elements get displayed horizontally next to one another rather than vertically. To properly render the table in the notebook, you must use the helper function <kbd>read_html</kbd> provided by the IPython library.</p>
<p>At the end of step 1, we unpack the list of DataFrames into their own appropriately named variables so that each individual table may be easily and clearly referenced. The nice thing about having a list of DataFrames is that, it is the exact requirement for the <kbd>concat</kbd> function, as seen in step 2. Notice how step 2 uses the <kbd>keys</kbd> parameter to name each chunk of data. This can be also be accomplished by passing a dictionary to <kbd>concat</kbd>, as done in step 3.</p>
<p>In step 4, we must change the type of <kbd>join</kbd> to <kbd>outer</kbd> to include all of the rows in the passed DataFrame that do not have an index present in the calling DataFrame. In step 5, the passed list of DataFrames cannot have any columns in common. Although there is an <kbd>rsuffix</kbd> parameter, it only works when passing a single DataFrame and not a list of them. To work around this limitation, we change the names of the columns beforehand with the <kbd>add_suffix</kbd> method, and then call the <kbd>join</kbd> method.</p>
<p>In step 7, we use <kbd>merge</kbd>, which defaults to aligning on all column names that are the same in both DataFrames. To change this default behavior, and align on the index of either one or both, set the <kbd>left_index</kbd> or <kbd>right_index</kbd> parameters to <kbd>True</kbd>. Step 8 finishes the replication with two calls to merge. As you can see, when you are aligning multiple DataFrames on their index, <kbd>concat</kbd> is usually going to be a far better choice than merge.</p>
<p>In step 9, we switch gears to focus on a situation where <kbd>merge</kbd> has the advantage. The <kbd>merge</kbd> method is the only one capable of aligning both the calling and passed DataFrame by column values. Step 10 shows you how easy it is to merge two DataFrames. The <kbd>on</kbd> parameter is not necessary but provided for clarity.</p>
<p>Unfortunately, it is very easy to duplicate or drop data when combining DataFrames, as shown in step 10. It is vital to take some time to do some sanity checks after combining data. In this instance, the <kbd>food_prices</kbd> dataset had a duplicate price for <kbd>steak</kbd> in store <kbd>B</kbd> so we eliminated this row by querying for only the current year in step 11. We also change to a left join to ensure that each transaction is kept regardless if a price is present or not.</p>
<p>It is possible to use join in these instances but all the columns in the passed DataFrame must be moved into the index first. Finally, <kbd>concat</kbd> is going to be a poor choice whenever you intend to align data by values in their columns.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>It is possible to read all files from a particular directory into DataFrames without knowing their names. Python provides a few ways to iterate through directories, with the <kbd>glob</kbd> module being a popular choice. The gas prices directory contains five different CSV files, each having weekly prices of a particular grade of gas beginning from 2007. Each file has just two columns--the date for the week and the price. This is a perfect situation to iterate through all the files, read them into DataFrames, and combine them all together with the <kbd>concat</kbd> function. The <kbd>glob</kbd> module has the <kbd>glob</kbd> function, which takes a single parameter--the location of the directory you would like to iterate through as a string. To get all the files in the directory, use the string <em>*</em>. In this example, <em>*.csv</em> returns only files that end in <em>.csv</em>. The result from the <kbd>glob</kbd> function is a list of string filenames, which can be directly passed to the <kbd>read_csv</kbd> function:</p>
<pre>&gt;&gt;&gt; import glob<br/><br/>&gt;&gt;&gt; df_list = []<br/>&gt;&gt;&gt; for filename in glob.glob('data/gas prices/*.csv'):<br/>        df_list.append(pd.read_csv(filename, index_col='Week',<br/>                       parse_dates=['Week']))<br/><br/>&gt;&gt;&gt; gas = pd.concat(df_list, axis='columns')<br/>&gt;&gt;&gt; gas.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8a01d07f-d580-4dce-9d3c-f47cf9a4c0ea.png" style="width:27.17em;height:13.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>IPython official documentation of the <kbd>read_html</kbd> function (<a href="http://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display_html" target="_blank">http://bit.ly/2fzFRzd</a>)</li>
<li>Refer to the <em>Exploding indexes</em> recipe from <a href="a5777e1a-6de5-44f6-b291-429cbceb505f.xhtml">Chapter 12</a>, <em>Index Alignment</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connecting to SQL databases</h1>
                </header>
            
            <article>
                
<p>To become a serious data analyst, you will almost certainly have to learn some amount of SQL. Much of the world's data is stored in databases that accept SQL statements. There are many dozens of relational database management systems, with SQLite being one of the most popular and easy to use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will be exploring the Chinook sample database provided by SQLite that contains 11 tables of data for a music store. <span>One of the best things to do when first diving into a proper relational database is to study a database diagram (sometimes called an entity relationship diagram)</span> <span>to better understand how tables are related. The following diagram will be immensely helpful when navigating through this recipe:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bfb39fe0-ea36-4d28-a0d2-245d6385b744.png" style="width:43.17em;height:22.92em;"/></div>
<p>In order for this recipe to work, you will need to have the <kbd>sqlalchemy</kbd> Python package installed. If you installed the Anaconda distribution, then it should already be available to you. SQLAlchemy is the preferred pandas tool when making connections to databases. In this recipe, you will learn how to connect to a SQLite database. You will then ask two different queries, and answer them by joining together tables with the <kbd>merge</kbd> method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Before we can begin reading tables from the <kbd>chinook</kbd> database, we need to set up our SQLAlchemy engine:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; from sqlalchemy import create_engine<br/>&gt;&gt;&gt; engine = create_engine('sqlite:///data/chinook.db')</pre>
<ol start="2">
<li>We can now step back into the world of pandas and remain there for the rest of the recipe. Let's complete a simple command and read in the <kbd>tracks</kbd> table with the <kbd>read_sql_table</kbd> function. The name of the table is the first argument and the SQLAlchemy engine is the second:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; tracks = pd.read_sql_table('tracks', engine)<br/>&gt;&gt;&gt; tracks.head()<br/>&gt;&gt;&gt; <span>genres = pd.read_sql_table('genres', engine)</span></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e34917a4-21a5-4bba-b011-70d354bc6d96.png" style="width:61.75em;height:12.67em;"/></div>
<ol start="3">
<li>For the rest of the recipe, we will answer a couple of different specific queries with help from the database diagram. To begin, let's find the average length of song per genre:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; genre_track = genres.merge(tracks[['GenreId', 'Milliseconds']], <br/>                               on='GenreId', how='left') \<br/>                        .drop('GenreId', axis='columns')<br/><br/>&gt;&gt;&gt; genre_track.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/41707069-07b1-4426-a5c2-ffa53f5c92de.png" style="width:11.00em;height:11.58em;"/></div>
<ol start="4">
<li>Now we can easily find the average length of each song per genre. To help ease interpretation, we convert the <kbd>Milliseconds</kbd> column to the <kbd>timedelta</kbd> data type:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; genre_time = genre_track.groupby('Name')['Milliseconds'].mean()<br/>&gt;&gt;&gt; pd.to_timedelta(genre_time, unit='ms').dt.floor('s')<br/>                                             .sort_values()<br/>Name
Rock And Roll        00:02:14
Opera                00:02:54
Hip Hop/Rap          00:02:58
...
Drama                00:42:55
Science Fiction      00:43:45
Sci Fi &amp; Fantasy     00:48:31
Name: Milliseconds, dtype: timedelta64[ns]</pre>
<ol start="5">
<li>Now let's find the total amount spent per customer. We will need the <kbd>customers</kbd>, <kbd>invoices</kbd>, and <kbd>invoice_items</kbd> tables all connected to each other:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; cust = pd.read_sql_table('customers', engine, <br/>                             columns=['CustomerId','FirstName',<br/>                                      'LastName'])<br/>&gt;&gt;&gt; invoice = pd.read_sql_table('invoices', engine, <br/>                                 columns=['InvoiceId','CustomerId'])<br/>&gt;&gt;&gt; ii = pd.read_sql_table('invoice_items', engine, <br/>                            columns=['InvoiceId', 'UnitPrice',<br/>                                     'Quantity'])<br/><br/>&gt;&gt;&gt; cust_inv = cust.merge(invoice, on='CustomerId') \<br/>                   .merge(ii, on='InvoiceId')<br/>&gt;&gt;&gt; cust_inv.head()</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6e860d89-e3e3-4d5e-8ed2-c768bf668f23.png" style="width:30.00em;height:11.00em;"/></div>
<ol start="6">
<li>We can now multiply the quantity by the unit price and then find the total amount spent per customer:</li>
</ol>
<pre style="padding-left: 60px">&gt;&gt;&gt; total = cust_inv['Quantity'] * cust_inv['UnitPrice']<br/>&gt;&gt;&gt; cols = ['CustomerId', 'FirstName', 'LastName']<br/>&gt;&gt;&gt; cust_inv.assign(Total = total).groupby(cols)['Total'] \<br/>                                  .sum() \<br/>                                  .sort_values(ascending=False) \<br/>                                  .head()<br/>CustomerId  FirstName  LastName  
6           Helena     Holý          49.62
26          Richard    Cunningham    47.62
57          Luis       Rojas         46.62
46          Hugh       O'Reilly      45.62
45          Ladislav   Kovács        45.62
Name: Total, dtype: float64</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The <kbd>create_engine</kbd> function requires a connection string in order to work properly. The connection string for SQLite is very simple, and is just the location of the database, which is located in the data directory. Other relational database management systems have more complex connection strings. You will need to provide a username, password, hostname, port, and optionally, a database. You will also need to supply the SQL dialect and the driver. The general form for the connection string is as follows: <kbd><span class="n">dialect</span><span class="o">+</span><span class="n">driver</span><span class="p">:</span><span class="o">//</span><span class="n">username</span><span class="p">:</span><span class="n">password</span><span class="nd">@host</span><span class="p">:</span><span class="n">port</span><span class="o">/</span><span class="n">database</span></kbd><span class="n">. The driver for your particular relational database might need to be installed separately.</span></p>
<p>Once we have created the engine, selecting entire tables into DataFrames is very easy with the <kbd>read_sql_table</kbd> function in step 2. Each of the tables in the database has a primary key uniquely identifying each row. It is identified graphically with a key symbol in the diagram. In step 3, we link genres to tracks through <kbd>GenreId</kbd>. As we only care about the track length, we trim the tracks DataFrame down to just the columns we need before performing the merge. Once the tables have merged, we can answer the query with a basic <kbd>groupby</kbd> operation.</p>
<p>We go one step further and convert the integer milliseconds into a Timedelta object that is far easier to read. The key is passing in the correct unit of measurement as a string. Now that we have a Timedelta Series, we can use the <kbd>dt</kbd> attribute to access the <kbd>floor</kbd> method, which rounds the time down to the nearest second.</p>
<p>The query required to answer step 5 involves three tables. We can trim the tables down significantly to only the columns we need by passing them to the <kbd>columns</kbd> parameter. When using <kbd>merge</kbd>, the joining columns are not kept when they have the same name. In step 6, we could have assigned a column for the price times quantity with the following:</p>
<pre>cust_inv['Total'] = cust_inv['Quantity'] * cust_inv['UnitPrice']</pre>
<p>There is nothing wrong with assigning columns in this manner. We chose to dynamically create a new column with the assign method to allow a continuous chain of methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>If you are adept with SQL, you can write a SQL query as a string and pass it to the <kbd>read_sql_query</kbd> function. For example, the following will reproduce the output from step 4:</p>
<pre>&gt;&gt;&gt; sql_string1 = '''<br/>    select <br/>        Name, <br/>        time(avg(Milliseconds) / 1000, 'unixepoch') as avg_time<br/>    from (<br/>            select <br/>                g.Name, <br/>                t.Milliseconds<br/>            from <br/>                genres as g <br/>            join<br/>                tracks as t<br/>                on <br/>                    g.genreid == t.genreid<br/>         )<br/>    group by <br/>        Name<br/>    order by <br/>         avg_time<br/>'''<br/>&gt;&gt;&gt; pd.read_sql_query(sql_string1, engine)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ccb4217b-d635-47d8-bfd3-c5952e29082b.png" style="width:16.00em;height:8.75em;"/></div>
<p>To reproduce the answer from step 6, use the following SQL query:</p>
<pre>&gt;&gt;&gt; sql_string2 = '''<br/>    select <br/>          c.customerid, <br/>          c.FirstName, <br/>          c.LastName, <br/>          sum(ii.quantity * ii.unitprice) as Total<br/>    from<br/>         customers as c<br/>    join<br/>         invoices as i<br/>              on c.customerid = i.customerid<br/>    join<br/>        invoice_items as ii<br/>              on i.invoiceid = ii.invoiceid<br/>    group by<br/>        c.customerid, c.FirstName, c.LastName<br/>    order by<br/>        Total desc<br/>'''<br/>&gt;&gt;&gt; pd.read_sql_query(sql_string2, engine)</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/307645d3-c9aa-4981-8e8c-eb934a124bf9.png" style="width:17.00em;height:6.42em;"/></div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>All engine configurations for <em>SQLAlchemy</em> (<a href="http://docs.sqlalchemy.org/en/latest/core/engines.html" target="_blank">http://bit.ly/2kb07vV</a>)</span></li>
<li>Pandas official documentation on <em>SQL Queries</em> (<a href="http://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries" target="_blank">http://bit.ly/2fFsOQ8</a></li>
</ul>


            </article>

            
        </section>
    </body></html>