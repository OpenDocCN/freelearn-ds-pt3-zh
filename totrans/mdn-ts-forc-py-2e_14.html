<html><head></head><body>
  <div id="_idContainer519" class="Basic-Text-Frame">
    <h1 class="chapterNumber">11</h1>
    <h1 id="_idParaDest-245" class="chapterTitle">Introduction to Deep Learning</h1>
    <p class="normal">In the previous chapter, we learned how to use modern machine learning models to tackle time series forecasting. Now, let’s focus our attention on a subfield of machine learning that has shown a lot of promise in the last few years—<strong class="keyWord">deep learning</strong>. We will be trying to <a id="_idIndexMarker852"/>demystify deep learning and go into why it is popular nowadays. We will also break down deep learning into major components and learn about the workhorse behind deep learning—gradient descent.</p>
    <p class="normal">In this chapter, we will be covering these main topics:</p>
    <ul>
      <li class="bulletList">What is deep learning and why now?</li>
      <li class="bulletList">Components of a deep learning system</li>
      <li class="bulletList">Representation learning</li>
      <li class="bulletList">Linear layers and activation functions</li>
      <li class="bulletList">Gradient descent</li>
    </ul>
    <h1 id="_idParaDest-246" class="heading-1">Technical requirements</h1>
    <p class="normal">You will need to set up the <strong class="keyWord">Anaconda</strong> environment following the instructions in the <em class="italic">Preface</em> of the book to get a working environment with all the libraries and datasets required for the code in this book. Any additional libraries will be installed while running the notebooks.</p>
    <p class="normal">The associated code for the chapter can be found at <a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter11"><span class="url">https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter11</span></a>.</p>
    <h1 id="_idParaDest-247" class="heading-1">What is deep learning and why now?</h1>
    <p class="normal">In <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, we talked about machine learning and borrowed a definition from Arthur Samuel: “<em class="italic">Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed</em>.” And we further saw <a id="_idIndexMarker853"/>how we can learn useful functions from data using machine learning. Deep learning is a subfield of this same field of study. The objective of deep learning is also to learn useful functions from data but with a few specifications on how it does that.</p>
    <p class="normal">Before we talk about what is special about deep learning, let’s answer another question first. Why are we talking about this subfield of machine learning as a separate topic? The answer to that lies in the unreasonable effectiveness of deep learning methods in countless applications. Deep learning has taken the world of machine learning by storm, overthrowing state-of-the-art systems across types of data such as images, videos, text, and so on. If you remember the speech recognition systems on phones a decade ago, they were more meme-worthy than really useful. But today, you can say <em class="italic">Hey Google, play Pink Floyd</em>, and <em class="italic">Comfortably Numb</em> will start playing on your phone or speakers. Multiple deep learning systems made this process possible in a smooth way. The voice assistant on your phone, self-driving cars, web search, language translation—the list of applications of deep learning in our day-to-day lives just keeps on going.</p>
    <p class="normal">By now, you might be wondering what this new technology called deep learning is all about, right? Deep learning is not a new technology. The origins of deep learning can be traced way back to the late 1940s and early 1950s. It only appears to be new because of the recent surge in popularity of the field.</p>
    <p class="normal">Let’s quickly see why deep learning is suddenly popular.</p>
    <h2 id="_idParaDest-248" class="heading-2">Why now?</h2>
    <p class="normal">There are two <a id="_idIndexMarker854"/>main reasons why deep learning has gained a lot of ground in the last two decades:</p>
    <ul>
      <li class="bulletList">Increase in compute availability</li>
      <li class="bulletList">Increase in data availability</li>
    </ul>
    <p class="normal">Let’s discuss the preceding points in detail in the following sections.</p>
    <h3 id="_idParaDest-249" class="heading-3">Increase in compute availability</h3>
    <p class="normal">Back in 1960, Frank Rosenblatt wrote a paper (Reference <em class="italic">5</em>) about a three-layer <a id="_idIndexMarker855"/>neural network and stated that it went a long way in demonstrating the ability of neural networks as a pattern-recognizing device. But in the same paper, he noted that the burden on a digital computer (of the 1960s) was too great as we increased the number of connections. However, in the decades that followed, computer hardware showed close to 50,000 times more improvement, which provided a good boost to neural networks and deep learning. However, it was still not enough as neural networks were still not considered to be good enough for <em class="italic">large-scale applications</em>.</p>
    <p class="normal">This is when a particular type of hardware, which was initially developed for gaming, came to the rescue—GPUs. It’s not entirely clear who started using GPUs for deep learning. Kyoung-Su Oh and Keechul Jung published a paper titled <em class="italic">GPU implementation of neural networks</em> back in 2004, which seems to be the first to show massive speed-ups in using GPUs for deep learning. One of the earliest and more popular research papers on the topic came from Rajat Raina, Anand Madhavan, and Andrew Ng, who published a paper titled <em class="italic">Large-scale deep unsupervised learning using graphics processors</em> back in 2009. It showed the effectiveness of GPUs for deep learning.</p>
    <p class="normal">Although many groups led by LeCun, Schmidhuber, Bengio, and so on were playing around with using GPUs, the turning point came when Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton used a GPU-based deep learning system that outperformed all the other competing technologies in an image recognition contest called the <em class="italic">ImageNet Large Scale Visual Recognition Challenge 2012</em>. </p>
    <p class="normal">The introduction of GPUs provided a much-needed boost to the widespread use of deep learning and accelerated the progress in the field.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research papers <em class="italic">GPU implementation of neural networks</em>, <em class="italic">Large-scale deep unsupervised learning using graphics processors</em>, and <em class="italic">ImageNet Classification with Deep Convolutional Neural Networks</em> are cited in the <em class="italic">References</em> section under <em class="italic">1</em>, <em class="italic">2</em>, and <em class="italic">3</em>, respectively.</p>
    </div>
    <h3 id="_idParaDest-250" class="heading-3">Increase in data availability</h3>
    <p class="normal">In addition<a id="_idIndexMarker856"/> to the skyrocketing compute capability, the other main factor that helped deep learning was the sheer increase in data. As the world became more and more digitized, the amount of data that we generated increased drastically. Tables that had hundreds and thousands of rows now exploded into millions and billions of rows, and the ever-decreasing cost of storage helped this explosion of data collection.</p>
    <p class="normal">And why would an increase in data availability help deep learning? This lies in the way deep learning works. Deep learning is quite data-hungry and needs large amounts of data to learn good models. Therefore, if we keep increasing the data that we provide to a deep learning model, the model will be able to learn better and better functions. However, the same can’t be said for traditional machine learning models. Let’s cement this learning with a chart that Andrew Ng, a world-renowned machine learning educator and an adjunct professor at Stanford, popularized in his famous machine learning course—<em class="italic">Machine Learning by Stanford University</em> on Coursera (<a href="https://www.coursera.org/specializations/machine-learning-introduction"><span class="url">https://www.coursera.org/specializations/machine-learning-introduction</span></a>) (<em class="italic">Figure 11.1</em>).</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_01.png" alt="Figure 11.1 – Deep learning versus traditional machine learning as we increase the data size "/></figure>
    <p class="packt_figref">Figure 11.1: Deep learning versus traditional machine learning as we increase the data size</p>
    <p class="normal">In <em class="italic">Figure 11.1</em>, which was popularized by Andrew Ng, we can see that as we increase the data size, traditional machine learning hits a plateau and won’t improve anymore.</p>
    <p class="normal">It has been<a id="_idIndexMarker857"/> proven empirically that there are significant benefits to the overparameterization of a deep learning model. <strong class="keyWord">Overparameterization</strong> means <a id="_idIndexMarker858"/>that there are more parameters in the model than the number of data points available to train. In classical statistics, this is a big no-no because, under this scenario, the model invariably overfits. But deep learning seems to flaunt this rule with ease. One of the examples of overparameterization is the current state-of-the-art image recognition system, <strong class="keyWord">NoisyStudent</strong>. It has 480 million parameters, but it was trained on <em class="italic">ImageNet</em> with 1.2 million data points.</p>
    <p class="normal">It has been argued that the way deep learning models are trained (stochastic gradient descent, which we will be explaining soon) is the key because it has a regularizing effect. In a research paper titled <em class="italic">The Computational Limits of Deep Learning</em>, Niel C. Thompson and others tried to illustrate this using a simple experiment. They set up a dataset with 1,000 features, but only 10 of them had any signal in them. Then, they tried to learn four models based on the dataset using varying dataset sizes:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Oracle model</strong>: A model that uses the exact 10 parameters that have any signal in them.</li>
      <li class="bulletList"><strong class="keyWord">Expert model</strong>: A model that uses 9 out of 10 significant parameters.</li>
      <li class="bulletList"><strong class="keyWord">Flexible model</strong>: A model that uses all 1,000 parameters.</li>
      <li class="bulletList"><strong class="keyWord">Regularized model</strong>: A model that uses all 1,000 parameters, but is now a regularized (<strong class="keyWord">lasso</strong>) model. (We covered regularization back in <em class="chapterRef">Chapter 8</em>, <em class="italic">Forecasting Time Series with Machine Learning Models</em>.)</li>
    </ul>
    <p class="normal">Let’s see <em class="italic">Figure 11.2</em> from the research paper with the study:</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_02.png" alt="Figure 11.2 – The chart shows how different models perform under different sizes of data "/></figure>
    <p class="packt_figref">Figure 11.2: The chart shows how different models perform under different sizes of data</p>
    <p class="normal">The chart has<a id="_idIndexMarker859"/> a number of data points used on the <em class="italic">x</em>-axis and the performance (<em class="italic">-log(Mean Squared Error)</em>) on the <em class="italic">y</em>-axis. The different colored lines show the different types of models. The Oracle model sets the upper limit for learning because it has access to perfect information. </p>
    <p class="normal">The expert model plateaus because there is a definite lack of information as it doesn’t have access to one of the ten features that are important. The flexible model (which uses all 1,000 features) takes a large number of data points to start recognizing the important ones but still keeps approaching the Oracle performance as the data size gets bigger. The regularized model (which is a proxy for deep learning models) keeps improving as we give the model more and more data. This model uses regularization to figure out which of these 1,000 features is relevant to the problem and starts to leverage them with much fewer data points, and performance keeps increasing with more data points. This strengthens the concept Andrew Ng popularized once more—with more data, deep learning starts to outperform traditional machine learning.</p>
    <p class="normal">A lot more factors, apart<a id="_idIndexMarker860"/> from compute and data availability, have contributed to the success of deep learning. Sara Hooker, in her essay <em class="italic">The Hardware Lottery</em> (Reference <em class="italic">9</em>), talks about how an idea wins not necessarily because it is superior to other ideas but because it is suited to the software and hardware available at the time. And once a research direction wins the lottery, it snowballs because more funding and big research organizations get behind that idea and it eventually becomes the most prominent idea in the space of ideas.</p>
    <p class="normal">We have talked about deep learning for some time but have still not understood what it is. Let’s do that now.</p>
    <h2 id="_idParaDest-251" class="heading-2">What is deep learning?</h2>
    <p class="normal">There is no single <a id="_idIndexMarker861"/>definition of deep learning because it means slightly different things to different people. However, a large majority of people agree on one thing: a model is called deep learning when it involves automatic feature learning from raw data. As Yoshua Bengio (a Turing Award winner and one of the <em class="italic">godfathers</em> of AI) explains in his 2021 paper titled <em class="italic">Deep Learning of Representations for Unsupervised and Transfer Learning</em>:</p>
    <blockquote class="packt_quote">
      <p class="quote">”Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features.”</p>
    </blockquote>
    <p class="normal">In a 2016 presentation, <em class="italic">Deep Learning and Understandability versus Software Engineering and Verification</em>, Peter Norvig, the Director of Research at Google, had a similar but simpler definition:</p>
    <blockquote class="packt_quote">
      <p class="quote">”A kind of learning where the representation you form have (sic) several levels of abstraction, rather than a direct input to output.”</p>
    </blockquote>
    <p class="normal">Another key feature of deep learning a lot of people agree upon is compositionality. Yann LeCun, a Turing Award winner and another one of the <em class="italic">godfathers</em> of AI, has a slightly more complex but more exact definition of deep learning (tweet from <code class="inlineCode">@ylecun</code> on January 9<sup class="superscript">th</sup>, 2020):</p>
    <blockquote class="packt_quote">
      <p class="quote">”DL is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods.”</p>
    </blockquote>
    <p class="normal">The key points we would like to highlight here are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Assembling parametrized modules</strong>: This refers to the compositionality of deep learning. Deep learning systems, as we will shortly see, are composed of a few submodules with a few parameters (some without) assembled into a graph-like structure.</li>
      <li class="bulletList"><strong class="keyWord">Optimizing it with gradient-based methods</strong>: Although having gradient-based learning as a sufficient criterion for deep learning is not widely accepted, we can still see empirically that, today, most successful deep learning systems are trained using gradient-based methods. (If you are not aware of what a gradient-based optimization method is, don’t worry. We will be covering it soon in this chapter.)</li>
    </ul>
    <p class="normal">If you have read <a id="_idIndexMarker862"/>anything about deep learning before, you may have seen neural networks and deep learning used together or interchangeably. But we haven’t talked about neural networks till now. Before we do that, let’s look at a fundamental unit of any neural network.</p>
    <h2 id="_idParaDest-252" class="heading-2">Perceptron, the first neural network</h2>
    <p class="normal">A lot of <a id="_idIndexMarker863"/>what we call deep learning and neural networks are deeply influenced by the human brain and its inner workings. Although recent studies have shown very little similarity between human brains and artificial neural networks, the seed behind the idea was inspired by human biology. The human desire to create intelligent beings like themselves was manifested as early as back in Greek mythology (Galatea and Pandora). And owing to this desire, humans have studied and looked for inspiration from human anatomy for years. One of the organs of the human body that has been studied intensely is the brain because it is the center of intelligence, creativity, and everything else that makes a human.</p>
    <p class="normal">Even though we still don’t know a lot about the brain, we do know a bit about it, and we use that little information to design artificial systems. The fundamental unit of the human brain is something we <a id="_idIndexMarker864"/>call a <strong class="keyWord">neuron</strong>, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_03.png" alt="Figure 11.3 – A biological neuron "/></figure>
    <p class="packt_figref">Figure 11.3: A biological neuron</p>
    <p class="normal">Many of you <a id="_idIndexMarker865"/>might<a id="_idIndexMarker866"/> have come across this in biology or in the context of machine learning as well. But let’s refresh this anyway. The biological neuron has the following parts:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Dendrites</strong> are branched extensions of the nerve cell that collect inputs from surrounding cells or other neurons.</li>
      <li class="bulletList"><strong class="keyWord">Soma</strong>, or the cell body, collects these inputs, joins them, and is passed on.</li>
      <li class="bulletList"><strong class="keyWord">The</strong> <strong class="keyWord">axon hillock</strong> connects the soma to the axon, and it controls the firing of the neuron. If the strength of a signal exceeds a threshold, the axon hillock fires an electrical signal through the axon.</li>
      <li class="bulletList"><strong class="keyWord">Axon</strong> is the fiber that connects the soma to the nerve endings. It is the axon’s duty to pass on the electrical signal to the endpoints.</li>
      <li class="bulletList"><strong class="keyWord">Synapses</strong> are the end points of the nerve cell and transmit the signal to other nerve cells.</li>
    </ul>
    <p class="normal">McCulloch and Pitts (1943) were the first to design a mathematical model for the biological neuron. However, the McCulloch-Pitts model<a id="_idIndexMarker867"/> had a few limitations:</p>
    <ul>
      <li class="bulletList">It only accepted binary variables.</li>
      <li class="bulletList">It considered all input variables equally important.</li>
      <li class="bulletList">There was only one parameter, a threshold, which was not learnable.</li>
    </ul>
    <p class="normal">In 1957, Frank Rosenblatt generalized the McCulloch-Pitts model and made it a full model whose parameters could be learned. The similarity between modern deep learning networks and the human brain ends here. The fundamental unit of learning that started this line of research was inspired by human biology and was a rather cheap imitation of it as well.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The original research paper for Frank Rosenblatt’s <em class="italic">perceptron</em> is cited in <em class="italic">References</em> under reference <em class="italic">5</em>.</p>
    </div>
    <p class="normal">Let’s understand the<a id="_idIndexMarker868"/> perceptron in detail because it is the fundamental building block of all neural networks:</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_04.png" alt="Figure 11.4 – Perceptron "/></figure>
    <p class="packt_figref">Figure 11.4: Perceptron</p>
    <p class="normal">As we see from <em class="italic">Figure 11.4</em>, the perceptron <a id="_idIndexMarker869"/>has the following components:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Inputs</strong>: These are the real-valued inputs that are fed to a perceptron. This is like the dendrites in neurons that collect the input.</li>
      <li class="bulletList"><strong class="keyWord">Weighted sum</strong>: Each input is multiplied by a corresponding weight and summed up. The weights determine the importance of each input in determining the outcome.</li>
      <li class="bulletList"><strong class="keyWord">Non-linearity</strong>: The weighted sum goes through a non-linear function. For the original perceptron, it was a step function with a threshold activation. The output would be positive or negative based on the weighted sum and the threshold of the unit. Modern-day perceptrons and neural networks use different kinds of activation functions, but we will see that later on.</li>
    </ul>
    <p class="normal">We can write<a id="_idIndexMarker870"/> the perceptron in the mathematical form as follows:</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.5: Perceptron, a math perspective</p>
    <p class="normal">As shown in <em class="italic">Figure 11.5</em>, the perceptron output is defined by the weighted sum of inputs, which is passed in through a non-linear function. Now, we can think of this using linear algebra as well. This is an important perspective for two reasons:</p>
    <ul>
      <li class="bulletList">The linear algebra perspective will help you understand neural networks faster.</li>
      <li class="bulletList">It will also make the whole thing feasible because matrix multiplications are something that our modern-day computers and GPUs are really good at. Without linear algebra, multiplying these inputs with corresponding weights would require us to loop through the inputs, and it quickly becomes infeasible.<div class="note">
          <p class="normal"><strong class="keyWord">Linear algebra intuition recap</strong></p>
          <p class="normal">Let’s take a look at a couple of concepts as a refresher. Feel free to jump ahead if you are already aware of vectors, vector spaces, and matrix multiplication.</p>
          <p class="normal"><strong class="keyWord">Vectors and vector spaces</strong></p>
          <p class="normal">At the superficial level, a <strong class="keyWord">vector</strong> is <a id="_idIndexMarker871"/>an array of numbers. However, in linear algebra, a vector is an entity that has both magnitude and direction. Let’s take an example to elucidate:</p>
          <p class="center"><img src="../Images/B22389_11_001.png" alt=""/></p>
          <p class="normal">We can see that this is an array of numbers. But if we plot this point in the two-dimensional coordinate space, we get a point. And if we draw a line from the origin to this point, we will get an entity with direction and magnitude. This is a vector.</p>
          <p class="normal">The two-dimensional coordinate space is <a id="_idIndexMarker872"/>called a <strong class="keyWord">vector space</strong>. A two-dimensional vector space, informally, is all the possible vectors with two entries. Extending it to <em class="italic">n</em>-dimensions, an <em class="italic">n</em>-dimensional vector space is all the possible vectors with <em class="italic">n</em> entries.</p>
          <p class="normal">The final intuition I want to leave with you is this: <em class="italic">a vector is a point in the n-dimensional vector space</em>.</p>
          <p class="normal"><strong class="keyWord">Matrices and transformations</strong></p>
          <p class="normal">Again, at the <a id="_idIndexMarker873"/>superficial level, a <strong class="keyWord">matrix</strong> is a rectangular arrangement of numbers that looks like this:</p>
          <p class="center"><img src="../Images/B22389_11_002.png" alt=""/></p>
          <p class="normal">Matrices have many uses but the one intuition that is most relevant for us is that a matrix specifies a linear transformation of the vector space it resides in. When we multiply a vector with a matrix, we are essentially transforming the vector, and the values and dimensions of the matrix define the kind of transformation that happens. Depending on the content of the matrix, it does <em class="italic">rotation, reflection, scaling, shearing</em>, and so on.</p>
          <p class="normal">We have included a notebook in the <code class="inlineCode">Chapter11</code> folder titled <code class="inlineCode">01-Linear_Algebra_Intuition.ipynb</code>, which explores matrix multiplication as a transformation. We also apply these transformation matrices to vector spaces to develop intuition on how matrix multiplication can rotate and warp the vector spaces.</p>
          <p class="normal">I highly suggest heading over to the <em class="italic">Further reading</em> section, where we have given a few resources to get started and solidify necessary intuition.</p>
        </div>
      </li>
    </ul>
    <p class="normal">If we consider the inputs as vectors in the feature space (vector space with <em class="italic">m</em>-dimensions), the term <img src="../Images/B22389_11_003.png" alt=""/> is nothing but a linear combination of input vectors. We can rewrite the equation in vector form as below:</p>
    <p class="center"><img src="../Images/B22389_11_004.png" alt=""/></p>
    <p class="center">where, <img src="../Images/B22389_11_005.png" alt=""/></p>
    <p class="normal">The bias is also included here as a dummy input with a fixed value of 1 and adding <img src="../Images/B22389_11_006.png" alt=""/> to the <img src="../Images/B22389_11_007.png" alt=""/> vector.</p>
    <p class="normal">Now that we have had an introduction to deep learning, let us recall one of the aspects of deep learning we discussed earlier—compositionality—and explore it a bit more deeply in the next section.</p>
    <h1 id="_idParaDest-253" class="heading-1">Components of a deep learning system</h1>
    <p class="normal">Let us recall Yann LeCun’s definition of deep learning:</p>
    <blockquote class="packt_quote">
      <p class="quote">”Deep learning is a methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods.”</p>
    </blockquote>
    <p class="normal">The core idea<a id="_idIndexMarker874"/> here is that deep learning is an extremely modular system. Deep learning is not just one model but, rather, a language to express any model in terms of a few parametrized modules with these specific properties:</p>
    <ol>
      <li class="numberedList" value="1">It should be able to produce an output from a given input through a series of computations.</li>
      <li class="numberedList">If the desired output is given, it should be able to pass on information to its inputs on how to change, to arrive at the desired output. For instance, if the output is lower than what is desired, the module should be able to tell its inputs to change in some direction so that the output becomes closer to the desired one.</li>
    </ol>
    <p class="normal">The more mathematically inclined may have figured out the connection to the second point of differentiation. And you would be correct. To optimize these kinds of systems, we predominantly use gradient-based optimization methods. Therefore, condensing the two properties into one, we can say that these parameterized modules should be <em class="italic">differentiable functions</em>.</p>
    <p class="normal">Let’s take the help of a visual to aid further discussion.</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_06.png" alt="Figure 11.6 – A deep learning system "/></figure>
    <p class="packt_figref">Figure 11.6: A deep learning system</p>
    <p class="normal">As shown in <em class="italic">Figure 11.6</em>, deep learning <a id="_idIndexMarker875"/>can be thought of as a system that takes in raw input data through a series of linear and non-linear transforms to provide us with an output. It also can adjust its internal parameters to make the output as close as possible to the desired output through learning. To make the diagram simpler, we have chosen a paradigm that fits most of the popular deep learning systems. It all starts with raw input data. The raw input data goes through <em class="italic">N</em> blocks of linear and non-linear functions that do representation learning. Let’s explore these blocks in some detail.</p>
    <h2 id="_idParaDest-254" class="heading-2">Representation learning</h2>
    <p class="normal"><strong class="keyWord">Representation learning</strong>, informally, learns<a id="_idIndexMarker876"/> the best features by which we can make the problem<a id="_idIndexMarker877"/> linearly separable. Linearly separable means when we can separate the different classes (in a classification problem) with a straight line (<em class="italic">Figure 11.7</em>):</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_07.png" alt="Figure 11.7 – Transforming non-linearly separable data into linearly separable using a function,  "/></figure>
    <p class="packt_figref">Figure 11.7: Transforming non-linearly separable data into linearly separable using a function, <img src="../Images/B22389_07_003.png" alt=""/></p>
    <p class="normal">The <em class="italic">representation learning</em> block in <em class="italic">Figure 11.6</em> may have multiple linear and non-linear functions stacked on top of each other, and the overall function of the block is to learn a function, <img src="../Images/B22389_07_003.png" alt=""/>, which transforms the raw input into good features that make the problem linearly separable.</p>
    <p class="normal">Another way to look <a id="_idIndexMarker878"/>at this is through<a id="_idIndexMarker879"/> the lens of linear algebra. As we explored earlier in the chapter, matrix multiplication can be thought of as a linear transformation of vectors. And if we extend that intuition to the vector spaces, we can see that matrix multiplication warps the vector space in some way or another. When we stack multiple linear and non-linear transformations on top of each other, we are essentially warping, twisting, and squeezing the input vector space (with the features) into another space. When we ask a parameterized system to warp the input space (pixels of images) in such a way as to perform a particular task (such as the classification of dogs versus cats), the representation learning block learn the right transformations, which makes the task (separating cats from dogs) easier.</p>
    <p class="normal">I have created a video illustrating this because nothing establishes intuition better than a video of what is happening. I’ve taken a sample dataset that is not linearly separable, trained a neural network on the problem to classify, and then visualized how the input space was transformed by the model into a linearly separable representation. You can find the video here: <a href="https://www.youtube.com/watch?v=5xYEa9PPDTE"><span class="url">https://www.youtube.com/watch?v=5xYEa9PPDTE</span></a>.</p>
    <p class="normal">Now, let’s look inside the representation learning block. We can see there is a linear transformation and a non-linear activation.</p>
    <h2 id="_idParaDest-255" class="heading-2">Linear transformation</h2>
    <p class="normal">Linear transformations<a id="_idIndexMarker880"/> are just <a id="_idIndexMarker881"/>transformations that are applied to the vector space. When we say linear transformation in a neural network context, we actually mean affine transformations.</p>
    <p class="normal">A linear transformation fixes the origin while applying the transformation, but an affine transformation doesn’t. Rotation, reflection, scaling, and so on are purely linear transformations because the origin won’t change while we do this. But something like a translation, which moves the vector space, is an affine transformation. Therefore, <em class="italic">A</em>*<em class="italic">X</em><sup class="superscript-italic" style="font-style: italic;">T</sup> is a linear transformation, but <em class="italic">A</em>*<em class="italic">X</em><sup class="superscript-italic" style="font-style: italic;">T</sup> +<em class="italic">b</em> is an affine transformation.</p>
    <p class="normal">So, linear transformations are simply matrix multiplications that transform the input vector space, and this is at the heart of any neural network or deep learning system today.</p>
    <p class="normal">What happens if we stack<a id="_idIndexMarker882"/> linear transformations on top of each other? For instance, we first multiply the input, <em class="italic">X</em>, with a transformation matrix, <em class="italic">A</em>, and then multiply the results with another transformation matrix, <em class="italic">B</em>:</p>
    <p class="center"><img src="../Images/B22389_11_010.png" alt=""/></p>
    <p class="normal">By the <a id="_idIndexMarker883"/>associative property (which is applicable to linear algebra as well), we can rewrite this equation as follows:</p>
    <p class="center"><img src="../Images/B22389_11_011.png" alt=""/></p>
    <p class="normal">Generalizing this to a stack of <em class="italic">N</em> transformation matrices, we can see that it all works out to be a single linear transformation. This kind of defeats the purpose of stacking <em class="italic">N</em> layers, doesn’t it?</p>
    <p class="normal">This is where the non-linearity becomes essential and we introduce non-linearities by using a non-linear function, which we call activation functions.</p>
    <h2 id="_idParaDest-256" class="heading-2">Activation functions</h2>
    <p class="normal"><strong class="keyWord">Activation functions</strong> are<a id="_idIndexMarker884"/> non-linear<a id="_idIndexMarker885"/> differentiable functions. In a biological neuron, the axon hillock decides whether to fire a signal based on the inputs. The activation functions serve a similar function and are key to the neural network’s ability to model non-linear data. In other words, activation functions are key in neural networks’ ability to transform input vector space (which is linearly inseparable) to a linearly separable vector space, informally. To <em class="italic">unwarp</em> a space such that linearly inseparable points become linearly separable, we need to have non-linear transformations.</p>
    <p class="normal">We repeated the same experiment we did in the last section, where we visualized the trained transformation of a neural network on the input vector space, but this time, without any non-linearities. The resulting video can be found here: <a href="https://www.youtube.com/watch?v=z-nV8oBpH2w"><span class="url">https://www.youtube.com/watch?v=z-nV8oBpH2w</span></a>. The best transformation that the model learned is just not sufficient and the points are still linearly inseparable.</p>
    <p class="normal">Theoretically, an activation function can be any non-linear differentiable (differentiable almost everywhere, to be exact) function. However, over the course of time, there are a few non-linear functions that are popularly used as activation functions. Let’s look at a few of them.</p>
    <h3 id="_idParaDest-257" class="heading-3">Sigmoid</h3>
    <p class="normal">Sigmoid is one <a id="_idIndexMarker886"/>of the most common activation functions around, and <a id="_idIndexMarker887"/>probably one of the oldest. It is also known as the logistic function. When we discussed perceptron, we mentioned a step (also called <em class="italic">Heaviside</em> in literature) function as the activation function. The step function is not a continuous function and hence <em class="italic">is not</em> differentiable everywhere. A very close substitute is the sigmoid function.</p>
    <p class="normal">It is defined as follows:</p>
    <p class="center"><img src="../Images/B22389_11_012.png" alt=""/></p>
    <p class="normal">Where <em class="italic">g</em> is the sigmoid function and <em class="italic">x</em> is the input value.</p>
    <p class="normal">Sigmoid is a continuous function and therefore <em class="italic">is</em> differentiable everywhere. The derivative is also computationally simpler to calculate. Because of these properties of the sigmoid, it was adopted widely in the early days of deep learning as a standard activation function.</p>
    <p class="normal">Let’s see what a sigmoid function looks like and how it transforms a vector space:</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_08.png" alt="Figure 11.8 – Sigmoid activation function (left) and original, and activated vector space (middle and right) "/></figure>
    <p class="packt_figref">Figure 11.8: Sigmoid activation function (left) and original and activated vector space (middle and right)</p>
    <p class="normal">The sigmoid function squashes the input between 0 and 1, as seen in <em class="italic">Figure 11.8 (left)</em>. We can observe the same phenomenon in the vector space. One of the drawbacks of the sigmoid function is that the gradients tend to zero on the flat portions of the sigmoid. When a neuron approaches this area in the function, the gradients that it receives and propagates become negligible and the unit stops learning. We call this <em class="italic">saturating of the activation</em>. Because of this, nowadays, <em class="italic">sigmoid</em> is not typically used in deep learning, except in the output layer (we will be talking about this usage soon).</p>
    <h3 id="_idParaDest-258" class="heading-3">Hyperbolic tangent (tanh)</h3>
    <p class="normal">Hyperbolic tangents<a id="_idIndexMarker888"/> are another popular<a id="_idIndexMarker889"/> activation. They can be easily defined as follows:</p>
    <p class="center"><img src="../Images/B22389_11_013.png" alt=""/></p>
    <p class="normal">It is very similar to sigmoid. In fact, we can express <em class="italic">tanh</em> as a function of sigmoid. Let’s see what the activation function looks like:</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_09.png" alt="Figure 11.9 – TanH activation function (left) and original, and activated vector space (middle and right) "/></figure>
    <p class="packt_figref">Figure 11.9: Tanh activation function (left) and original and activated vector space (middle and right)</p>
    <p class="normal">We can see that the shape is similar to sigmoid, although a bit sharper. But the key difference is that the <em class="italic">tanh</em> function outputs a value between -1 and 1. And because of the sharpness, we can also see the vector space getting pushed out to the edges as well. The fact that the function outputs a value that is symmetrical around the origin (0) works well with the optimization of the network and hence <em class="italic">tanh</em> was preferred over <em class="italic">sigmoid</em>. But since the <em class="italic">tanh</em> function is also a saturating function, the same problem of very small gradients hampering the flow of gradients and, in turn, learning plagues <em class="italic">tanh</em> activations as well.</p>
    <h3 id="_idParaDest-259" class="heading-3">Rectified linear units and variants</h3>
    <p class="normal">As neuroscience<a id="_idIndexMarker890"/> gained more information about the human brain, researchers found out that only one to four percent of neurons in the brain are activated at any time. But with all the activation functions such as <em class="italic">sigmoid</em> or <em class="italic">tanh</em>, almost half of the neurons in a network are activated. In 2010, Vinod Nair and Geoffrey Hinton proposed <strong class="keyWord">rectified linear units</strong> (<strong class="keyWord">ReLUs</strong>) in the seminal paper <em class="italic">Rectified Linear Units Improve Restricted Boltzmann Machines</em>. Since then, ReLUs have taken over as the de facto activation functions for deep neural networks.</p>
    <h4 class="heading-4">ReLU</h4>
    <p class="normal">A ReLU is <a id="_idIndexMarker891"/>defined as follows:</p>
    <p class="center"><em class="italic">g</em>(<em class="italic">x</em>) = <em class="italic">max</em>(<em class="italic">x</em>, 0)</p>
    <p class="normal">It is just a linear function but with a kink at zero. Any value greater than zero is retained as is, but all values below zero are squashed to zero. The range of the output goes from 0 to <img src="../Images/B22389_11_014.png" alt=""/>. Let’s see how it looks visually:</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_10.png" alt="Figure 11.10 – ReLU activation function (left) and original, and activated vector space (middle and right) "/></figure>
    <p class="packt_figref">Figure 11.10: ReLU activation function (left) and original and activated vector space (middle and right)</p>
    <p class="normal">We can see that the points in the left and bottom quadrants are all pushed into the axes’ lines. This squashing is what gives the non-linearity to the activation function. Because of the way the activation sharply becomes zero and does not tend to zero like the sigmoid or tanh, ReLUs are non-saturating.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper that proposed ReLU is cited in <em class="italic">References</em> under reference <em class="italic">7</em>.</p>
    </div>
    <p class="normal">There are a few <a id="_idIndexMarker892"/>advantages to using ReLUs:</p>
    <ul>
      <li class="bulletList">The computations of the activation function as well as its gradients are really cheap.</li>
      <li class="bulletList">Training converges much faster than those with saturating activation functions.</li>
      <li class="bulletList">ReLU helps bring sparsity in the network (by having the activation as zero, a large majority of neurons in the network can be turned off) and resembles how biological neurons work.</li>
    </ul>
    <p class="normal">But ReLUs are not without problems:</p>
    <ul>
      <li class="bulletList">When <em class="italic">x</em> &lt; 0, the gradients become zero. This means a neuron that has an output &lt; 0 will have zero gradients and, therefore, the unit will not learn anymore. These are called dead ReLUs.</li>
      <li class="bulletList">Another disadvantage is that the average output of a ReLU unit is positive, and when we stack multiple layers, this might lead to a positive bias in the output.</li>
    </ul>
    <p class="normal">Let’s see a few variants that try to resolve the problems we discussed for ReLU.</p>
    <h4 class="heading-4">Leaky ReLU and parametrized ReLU</h4>
    <p class="normal">Leaky ReLU <a id="_idIndexMarker893"/>is a <a id="_idIndexMarker894"/>variant of standard ReLU that resolves the <em class="italic">dead ReLU</em> problem. It was proposed by Maas and others in 2013. A leaky ReLU can be defined as follows:</p>
    <p class="center"><img src="../Images/B22389_11_015.png" alt=""/></p>
    <p class="normal">Here, <img src="../Images/B22389_04_009.png" alt=""/> is the slope parameter (typically set to a very small value such as 0.001) and is considered a hyperparameter. This makes sure the gradients are not zero when <em class="italic">x</em> &lt; <em class="italic">0</em> and thereby ensures there are no <em class="italic">dead</em> ReLUs. But the sparsity that ReLU provides is lost here because there is no zero output that turns off a unit completely. Let’s visualize this activation function:</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_11.png" alt="Figure 10.11 – Leaky ReLU activation function (left) and original, and activated vector space (middle and right) "/></figure>
    <p class="packt_figref">Figure 11.11: Leaky ReLU activation function (left) and original and activated vector space (middle and right)</p>
    <p class="normal">In 2015, K. He and others <a id="_idIndexMarker895"/>proposed another minor modification to leaky ReLU called <strong class="keyWord">parametrized ReLU</strong>. In <a id="_idIndexMarker896"/>parametrized ReLU, instead of considering <img src="../Images/B22389_04_009.png" alt=""/> as a hyperparameter, they considered it as a learnable parameter.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Rerefence check</strong>:</p>
      <p class="normal">The research paper that proposed leaky ReLU is cited in <em class="italic">References</em> under reference <em class="italic">8</em>, and parametrized ReLU is cited under reference <em class="italic">9</em>.</p>
    </div>
    <p class="normal">There are many other activation functions that are less popularly used but still have enough use cases to be included in <em class="italic">PyTorch</em>. You can find a list of them here: <a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity"><span class="url">https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity</span></a>. We encourage you to use the notebook titled <code class="inlineCode">02-Activation_Functions.ipynb</code> in the <code class="inlineCode">Chapter 11</code> folder to try out different activation functions and see how they warp the vector space.</p>
    <p class="normal">And with that, we now have an idea of the components of the first block in <em class="italic">Figure 11.6</em>, representation learning. The next block in there is the linear classifier, which has a linear transformation and an output activation. We already know what a linear transformation is, but what is an output activation?</p>
    <h2 id="_idParaDest-260" class="heading-2">Output activation functions</h2>
    <p class="normal">Output activation functions <a id="_idIndexMarker897"/>are <a id="_idIndexMarker898"/>functions that enforce a few desirable properties to the output of the network.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Additional reading</strong>:</p>
      <p class="normal">These functions have a deeper connection with <strong class="keyWord">maximum likelihood estimation</strong> (<strong class="keyWord">MLE</strong>) and<a id="_idIndexMarker899"/> the chosen loss function, but we will not be getting into that because it is out of the scope of this book. We have linked to the book <em class="italic">Deep Learning</em> by Ian Goodfellow, Yoshua Bengio, and Aaron Courville in the <em class="italic">Further reading</em> section. If you are interested in a deeper understanding of deep learning, we suggest you use the book to that effect.</p>
    </div>
    <p class="normal">If we want the neural network to predict a continuous number in the case of regression, we just use a linear activation function (which is like saying there is no activation function). The raw output from the network is considered the prediction and fed into the loss function.</p>
    <p class="normal">But in the case of classification, the desired output is a class out of all possible classes. If there are only two classes, we can use our old friend, the <code class="inlineCode">sigmoid</code> function, which has an output between 0 and 1. We can also use <em class="italic">tanh</em> because its output is going to be between -1 and 1. The <code class="inlineCode">sigmoid</code> function is preferred because of the intuitive probabilistic interpretation that comes along with it. The closer the value is to one, the more confident the network is about that prediction.</p>
    <p class="normal">Now, <em class="italic">sigmoid</em> works for binary classification. What about multiclass classification where the possible classes are more than two?</p>
    <h3 id="_idParaDest-261" class="heading-3">Softmax</h3>
    <p class="normal"><em class="italic">Softmax</em> is a<a id="_idIndexMarker900"/> function <a id="_idIndexMarker901"/>that converts a vector of <em class="italic">K</em> real values into another <em class="italic">K</em>-positive real value, which sums up to 1. <em class="italic">Softmax</em> is defined as follows:</p>
    <p class="center"><img src="../Images/B22389_11_018.png" alt=""/></p>
    <p class="normal">This function converts the raw output from a network into something that resembles a probability across <em class="italic">K</em> classes. This has a strong relation with <em class="italic">sigmoid</em>—<em class="italic">sigmoid</em> is a special case of <em class="italic">softmax</em> when <em class="italic">K</em> = <em class="italic">2</em>. In the<a id="_idIndexMarker902"/> following figure, let’s see how a random vector of size 3 is converted into probabilities that add up to 1:</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_12.png" alt="Figure 11.12 – Raw output versus softmax output "/></figure>
    <p class="packt_figref">Figure 11.12: Raw output versus softmax output</p>
    <p class="normal">If we look closely, we <a id="_idIndexMarker903"/>can see that in addition to converting the real values into something that resembles probability, it also increases the relative gap between the maximum and the rest of the values. This activation is a standard output activation for multiclass classification problems.</p>
    <p class="normal">Now, there is only one major component left in the diagram (<em class="italic">Figure 11.6</em>)—the loss function.</p>
    <h2 id="_idParaDest-262" class="heading-2">Loss function</h2>
    <p class="normal">The <a id="_idIndexMarker904"/>loss function<a id="_idIndexMarker905"/> we touched upon in <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, translates nicely to deep learning. In deep learning also, the loss function is a way to tell how good the predictions of the model are. If the predictions are way off the target, the loss function will be higher, and as we get closer to the truth, it becomes smaller. In the deep learning paradigm, we just have one more additional requirement of the loss function—it should be differentiable.</p>
    <p class="normal">Common loss functions from classical machine learning, such as <strong class="keyWord">mean squared error</strong> or <strong class="keyWord">mean absolute error,</strong> are<a id="_idIndexMarker906"/> valid<a id="_idIndexMarker907"/> in deep learning as well. In fact, in regression tasks, they are the default choices that practitioners adopt. For classification tasks, we adopt a concept borrowed from<a id="_idIndexMarker908"/> information theory called <strong class="keyWord">cross-entropy loss</strong>. However, since deep<a id="_idIndexMarker909"/> learning is a very flexible framework, we can use any loss function as long as it is differentiable. There<a id="_idIndexMarker910"/> are a lot of loss functions people have already tried and found work in many situations. A lot of them are part of PyTorch’s API as well. You can find them here: <a href="https://pytorch.org/docs/stable/nn.html#loss-functions"><span class="url">https://pytorch.org/docs/stable/nn.html#loss-functions</span></a>.</p>
    <p class="normal">Now that we have covered all the components of a deep learning system, let’s also briefly look at how we train the whole system.</p>
    <h2 id="_idParaDest-263" class="heading-2">Forward and backward propagation</h2>
    <p class="normal">In <em class="italic">Figure 11.6</em>, we can see two sets of arrows, one going toward the desired output from the input, marked as <em class="italic">Forward Computation</em>, and another going backward to the input from the desired output, marked <em class="italic">Backward Computation</em>. These two steps are at the core of<a id="_idIndexMarker911"/> learning a deep learning system. In <em class="italic">Forward Computation</em>, popularly known as <strong class="keyWord">forward propagation</strong>, we<a id="_idIndexMarker912"/> use the series of computations that are defined in the layers and propagate the input all the way through the network to get the output. Now that we have the output, we would use the loss function to assess how close or far we are from the desired output. This information is now used in <em class="italic">Backward Computation</em>, popularly known <a id="_idIndexMarker913"/>as <strong class="keyWord">backward propagation</strong>, to<a id="_idIndexMarker914"/> calculate the gradient with respect to all the parameters.</p>
    <p class="normal">Now, what is a gradient and why do we need it? In high school math, we might have come across gradients or derivatives in another<a id="_idIndexMarker915"/> form called <strong class="keyWord">slope</strong>. It is the rate of change of a quantity when we change a variable by unit measure. Derivatives inform us of the local slope of a scalar function. While derivatives are always with respect to a single variable, gradients are a generalization of derivatives to multivariate functions. Intuitively, both gradients and derivatives inform us of the local slope of the function. And with the gradient of the loss function, we can use one of the techniques from mathematical optimization, called <strong class="keyWord">gradient descent</strong>, to <a id="_idIndexMarker916"/>optimize our loss function.</p>
    <p class="normal">Let’s see this with an example.</p>
    <h3 id="_idParaDest-264" class="heading-3">Gradient descent</h3>
    <p class="normal">Any <a id="_idIndexMarker917"/>machine<a id="_idIndexMarker918"/> learning or deep learning model can be thought of as a function that converts an input, <em class="italic">x</em>, to an output, <img src="../Images/B22389_05_001.png" alt=""/>, using a few parameters, <img src="../Images/B22389_04_016.png" alt=""/>. Here, <img src="../Images/B22389_04_016.png" alt=""/> can be the collection of all the matrix transformations that we do to the input throughout the network. But to simplify the example, let’s assume there are only two parameters, <em class="italic">a</em> and <em class="italic">b</em>. If we think about the whole process of learning a bit, we will see that by keeping the input and expected output the same, the way to change your loss would be by changing the parameters of the model. Therefore, we can postulate the loss function to be parameterized by the parameters–in this case, <em class="italic">a</em> and <em class="italic">b</em>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">03-Gradient_Descent.ipynb</code> in the <code class="inlineCode">Chapter11</code> folder and the code in the <code class="inlineCode">src</code> folder.</p>
    </div>
    <p class="normal">Let’s assume the loss function takes the following form:</p>
    <p class="center"><img src="../Images/B22389_11_022.png" alt=""/></p>
    <p class="normal">Let’s see what the function looks like. We can use a three-dimensional plot to visualize a function with two parameters, as seen in <em class="italic">Figure 11.13</em>. Two dimensions will be used to denote the two parameters and, at each point in that two-dimensional mesh, we can plot the loss value in the third dimension. </p>
    <p class="normal">This kind of plot of the loss function is also called a loss curve (in univariate settings), or loss surface (in multivariate settings).</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_13.png" alt="Figure 11.13 – Loss surface plot "/></figure>
    <p class="packt_figref">Figure 11.13: Loss surface plot</p>
    <p class="normal">The lighter<a id="_idIndexMarker919"/> portion of the 3D shape is where<a id="_idIndexMarker920"/> the loss function is less, and as we move away from there, it increases.</p>
    <p class="normal">In machine learning, our aim is to minimize the loss function, or in other words, find the parameters that make our predicted output as close as possible to the ground truth. This falls under the realm of mathematical optimization, and a particular technique lends itself suitable for this approach—<strong class="keyWord">gradient descent</strong>.</p>
    <p class="normal">Gradient descent<a id="_idIndexMarker921"/> is a mathematical optimization <a id="_idIndexMarker922"/>algorithm used to minimize a cost function by iteratively moving in the direction of the steepest descent. In a univariate function, the derivative (or the slope) gives us the direction (and magnitude) of the steepest ascent. For instance, if we know that the slope of a function is 1, we know if we move to the right, we are climbing up the slope, and moving to the left, we will be climbing down. Similarly, in the multivariate setting, the gradient of a function at any point will give us the direction (and magnitude) of the steepest ascent. And since we are concerned with minimizing a loss function, we will be using the negative gradient, which will point us in the direction of the steepest descent.</p>
    <p class="normal">So, let’s define the gradient for our loss function. We are using high school calculus, but even if you are not comfortable, you don’t need to worry:</p>
    <p class="center"><img src="../Images/B22389_11_023.png" alt=""/></p>
    <p class="normal">Now, how does the algorithm work? Very simply, as follows:</p>
    <ol>
      <li class="numberedList" value="1">Initialize the parameters to random values.</li>
      <li class="numberedList">Compute the gradient at that point.</li>
      <li class="numberedList">Make a step in the direction opposite to the gradient.</li>
      <li class="numberedList">Repeat steps 2 and 3 until it converges or we reach maximum iterations.</li>
    </ol>
    <p class="normal">There is just one more aspect that needs more clarity: how much of a step do we take in each iteration?</p>
    <p class="normal">Ideally, the magnitude of the gradient tells you how fast the function is changing in that direction, and we should just take the step equal to the gradient. But there is a property of the gradient that makes that a bad idea. The gradient only defines the direction and magnitude of the steepest ascent in the infinitesimally small locality of the current point and is blind to what happens beyond it. Therefore, we use a hyperparameter, commonly called the <strong class="keyWord">learning rate</strong>, to<a id="_idIndexMarker923"/> temper the steps we take in each iteration. Therefore, instead of taking a step equal to the gradient, we take a step equal to the learning rate multiplied by the gradient.</p>
    <p class="normal">Mathematically, if <img src="../Images/B22389_04_016.png" alt=""/> is the vector of parameters, at each iteration, we update the parameters using the following formula:</p>
    <p class="center"><img src="../Images/B22389_11_025.png" alt=""/></p>
    <p class="normal">Here, <img src="../Images/B22389_04_044.png" alt=""/> is the learning rate, and <img src="../Images/B22389_11_027.png" alt=""/> is the gradient at the point.</p>
    <p class="normal">Let’s see a very <a id="_idIndexMarker924"/>simple implementation <a id="_idIndexMarker925"/>of gradient descent (refer to the notebook in the GitHub repository for the end-to-end definition and working code). First, let’s define a function that returns the gradient at any point:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">gradient</span>(<span class="hljs-params">a, b</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span>*(a-<span class="hljs-number">8</span>), <span class="hljs-number">2</span>*(b-<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Now we define a few initial parameters such as the maximum iterations, learning rate, and initial value of <em class="italic">a</em> and <em class="italic">b</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># maximum number of iterations that can be done</span>
maximum_iterations = <span class="hljs-number">500</span>
<span class="hljs-comment"># current iteration</span>
current_iteration = <span class="hljs-number">0</span>
<span class="hljs-comment"># Learning Rate</span>
learning_rate = <span class="hljs-number">0.01</span>
<span class="hljs-comment">#Initial value of a, b</span>
current_a_value = <span class="hljs-number">28</span>
current_b_value = <span class="hljs-number">27</span>
Now, <span class="hljs-built_in">all</span> that <span class="hljs-keyword">is</span> left <span class="hljs-keyword">is</span> the actual process of gradient descent:
<span class="hljs-keyword">while</span> current_iteration &lt; maximum_iterations:
    previous_a_value = current_a_value
    previous_b_value = current_b_value
    <span class="hljs-comment"># Calculating the gradients at current values</span>
    gradient_a, gradient_b = gradient(previous_a_value, previous_b_value)
    <span class="hljs-comment"># Adjusting the parameters using the gradients</span>
    current_a_value = current_a_value - learning_rate * gradient_a * (previous_a_value)
    current_b_value = current_b_value - learning_rate * gradient_b * (previous_b_value)
    current_iteration = current_iteration + <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">We know the minimum for this function will be at <em class="italic">a</em> = <em class="italic">8</em> and <em class="italic">b</em> = <em class="italic">2</em> because that would make the loss function zero. And gradient descent finds a solution that is pretty accurate—<em class="italic">a</em> = <em class="italic">8.000000000000005</em> and <em class="italic">b</em> = <em class="italic">2.000000002230101</em>. We can also visualize the path it took to reach the minimum, as seen in <em class="italic">Figure 11.14</em>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_11_14.png" alt="Figure 11.14 – Gradient descent optimization on the loss surface "/></figure>
    <p class="packt_figref">Figure 11.14: Gradient descent optimization on the loss surface</p>
    <p class="normal">We<a id="_idIndexMarker926"/> can <a id="_idIndexMarker927"/>see that even though we initialized the parameters far from the actual origin, the optimization algorithm makes a direct path to the optimal point. At each point, the algorithm looks at the gradient of the point, moves in the opposite direction, and eventually converges on the optimum.</p>
    <p class="normal">When <a id="_idIndexMarker928"/>gradient descent is adopted in a learning task, there are a few kinks to be noted. Let’s say we have a dataset of <em class="italic">N</em> samples. There are three popular variants of gradient descent that are used in learning and each of them has its pros and cons.</p>
    <h4 class="heading-4">Batch gradient descent</h4>
    <p class="normal">We run <em class="italic">all N</em> samples<a id="_idIndexMarker929"/> through the network and average the losses across <em class="italic">all N</em> instances. Now, we use this loss to calculate the gradient, make<a id="_idIndexMarker930"/> a step in the right direction, and repeat.</p>
    <p class="normal">The pros are <a id="_idIndexMarker931"/>as follows:</p>
    <ul>
      <li class="bulletList">The optimization path is direct.</li>
      <li class="bulletList">The optimization path has guaranteed convergence.</li>
    </ul>
    <p class="normal">The cons are as <a id="_idIndexMarker932"/>follows:</p>
    <ul>
      <li class="bulletList">The entire dataset needs to be evaluated for a single step, and that is computationally expensive. The computation per optimization step becomes prohibitively high for huge datasets.</li>
      <li class="bulletList">The time taken per optimization step is high, and hence, the convergence will also be slow.</li>
    </ul>
    <h4 class="heading-4">Stochastic gradient descent (SGD)</h4>
    <p class="normal">In SGD, we<a id="_idIndexMarker933"/> randomly sample <em class="italic">one</em> instance<a id="_idIndexMarker934"/> from <em class="italic">N</em> samples, calculate the loss and gradients, and then update the parameters.</p>
    <p class="normal">The pros are as <a id="_idIndexMarker935"/>follows:</p>
    <ul>
      <li class="bulletList">Since we only use a single instance to do the optimization step, computation per optimization step is very low.</li>
      <li class="bulletList">The time taken per optimization step is also faster.</li>
      <li class="bulletList">Stochastic sampling also acts as regularization and helps to avoid overfitting.</li>
    </ul>
    <p class="normal">The cons are as follows:</p>
    <ul>
      <li class="bulletList">The gradient <a id="_idIndexMarker936"/>estimates are noisy because we make the step based on just one instance. Therefore, the path toward optimum will be choppy and noisy.</li>
      <li class="bulletList">Just because<a id="_idIndexMarker937"/> the time taken per optimization is low, it need not mean convergence is faster. We may not be taking the right step many times because of noisy gradient estimates.</li>
    </ul>
    <h4 class="heading-4">Mini-batch gradient descent</h4>
    <p class="normal">Mini-batch gradient descent <a id="_idIndexMarker938"/>is a technique that <a id="_idIndexMarker939"/>falls somewhere between the spectrum of batch gradient descent and SGD. In this variant, we have another quality called mini-batch size (or simply batch size), <em class="italic">b</em>. In each optimization step, we randomly pick <em class="italic">b </em>instances from <em class="italic">N</em> samples and calculate gradients on the average loss of all <em class="italic">b</em> instances. With <em class="italic">b</em> = <em class="italic">N</em>, we have <em class="italic">batch gradient descent</em>, and with <em class="italic">b</em> = <em class="italic">1</em>, we have <em class="italic">stochastic gradient descent</em>. This is the most popular way neural networks are trained today. By varying the batch size, we can travel between the two variants and manage the pros and cons of each option.</p>
    <p class="normal">Nothing develops intuition better than a visual playground where we can see the effects of the different components we discussed. <em class="italic">Tensorflow Playground</em> is an excellent resource (see the link in the <em class="italic">Further reading</em> section) to do just that. I strongly urge you to head over there and play with the tool, train a few neural networks right in the browser, and see in real time how the learning happens.</p>
    <h1 id="_idParaDest-265" class="heading-1">Summary</h1>
    <p class="normal">We kicked off a new part of the book with an introduction to deep learning. We started with a bit of history to understand why deep learning is so popular today and we also explored its humble beginnings in perceptron. We understood the composability of deep learning and understood and dissected the different components of deep learning, such as the representation learning block, linear layers, activation functions, and so on. Finally, we rounded off the discussion by looking at how a deep learning system uses gradient descent to learn from data. With that understanding, we are now ready to move on to the next chapter, where we will drive the narrative toward time series models.</p>
    <h1 id="_idParaDest-266" class="heading-1">References</h1>
    <p class="normal">Following is the list of references used throughout this chapter:</p>
    <ol>
      <li class="numberedList" value="1">Kyoung-Su Oh and Keechul Jung. (2004), <em class="italic">GPU implementation of neural networks</em>. Pattern Recognition, Volume 37, Issue 6, 2004: <a href="https://doi.org/10.1016/j.patcog.2004.01.013"><span class="url">https://doi.org/10.1016/j.patcog.2004.01.013</span></a>.</li>
      <li class="numberedList">Rajat Raina, Anand Madhavan, and Andrew Y. Ng. (2009), <em class="italic">Large-scale deep unsupervised learning using graphics processors</em>. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML ‘09): <a href="https://doi.org/10.1145/1553374.1553486"><span class="url">https://doi.org/10.1145/1553374.1553486</span></a>.</li>
      <li class="numberedList">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. (2012), <em class="italic">ImageNet Classification with Deep Convolutional Neural Networks</em>. Commun. ACM 60, 6 (June 2017), 84–90: <a href="https://doi.org/10.1145/3065386"><span class="url">https://doi.org/10.1145/3065386</span></a>.</li>
      <li class="numberedList">Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. (2020). <em class="italic">The Computational Limits of Deep Learning</em>. arXiv:2007.05558v1 [cs.LG]: <a href="https://arxiv.org/abs/2007.05558v1"><span class="url">https://arxiv.org/abs/2007.05558v1</span></a>.</li>
      <li class="numberedList">Frank Rosenblatt. (1957), <em class="italic">The perceptron – A perceiving and recognizing automaton</em>, Technical Report 85-460-1, Cornell Aeronautical Laboratory.</li>
      <li class="numberedList">Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Keim. (2001).<em class="italic"> On the Surprising Behavior of Distance Metrics in High Dimensional Spaces.</em> In Proceedings of the 8th International Conference on Database Theory (ICDT ‘01). Springer-Verlag, Berlin, Heidelberg, 420–434: <a href="https://dl.acm.org/doi/10.5555/645504.656414"><span class="url">https://dl.acm.org/doi/10.5555/645504.656414</span></a>.</li>
      <li class="numberedList">Nair, V. and Hinton, G.E. (2010). <em class="italic">Rectified Linear Units Improve Restricted Boltzmann Machines</em>. ICML: <a href="https://icml.cc/Conferences/2010/papers/432.pdf"><span class="url">https://icml.cc/Conferences/2010/papers/432.pdf</span></a>.</li>
      <li class="numberedList">Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. (2013). <em class="italic">Rectifier nonlinearities improve neural network acoustic models</em>. ICML Workshop on Deep Learning for Audio, Speech, and Language Processing: <a href="https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf"><span class="url">https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf</span></a>.</li>
      <li class="numberedList">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2015). <em class="italic">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</em>. 2015 IEEE International Conference on Computer Vision (ICCV), 1026-1034: <a href="https://ieeexplore.ieee.org/document/7410480"><span class="url">https://ieeexplore.ieee.org/document/7410480</span></a>.</li>
      <li class="numberedList">Sara Hooker. (2021). <em class="italic">The hardware lottery</em>. Commun. ACM, Volume 64: <a href="https://doi.org/10.1145/3467017"><span class="url">https://doi.org/10.1145/3467017</span></a>.</li>
    </ol>
    <h1 id="_idParaDest-267" class="heading-1">Further reading</h1>
    <p class="normal">You can check out the following sources if you want to read more about a few topics covered in this chapter:</p>
    <ul>
      <li class="bulletList"><em class="italic">Linear Algebra </em>course from Gilbert Strang: <a href="https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/videos/"><span class="url">https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/videos/</span></a></li>
      <li class="bulletList"><em class="italic">Essence of Linear Algebra</em> from 3Blue1Brown: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"><span class="url">https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab</span></a></li>
      <li class="bulletList"><em class="italic">Neural Networks – A Linear Algebra Perspective</em> by Manu Joseph: <a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/"><span class="url">https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/</span></a></li>
      <li class="bulletList"><em class="italic">Deep Learning</em> – Ian Goodfellow, Yoshua Bengio, Aaron Courville: <a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/"><span class="url">https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/</span></a></li>
      <li class="bulletList"><em class="italic">Tensorflow Playground</em>: <a href="https://playground.tensorflow.org/  "><span class="url">https://playground.tensorflow.org/</span></a></li>
    </ul>
    <p class="normal"><a href="https://playground.tensorflow.org/  "/></p>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
  </div>
</body></html>