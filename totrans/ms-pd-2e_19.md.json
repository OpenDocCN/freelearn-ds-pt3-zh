["```py\n    conda install scikit-learn\n```", "```py\ngit clone https://github.com/scikit-learn/scikit-learn.git cd scikit-learn python setup.py install  \n```", "```py\npip install pandas  \n```", "```py\npip install -U scikit-learn  \n```", "```py\nIn [2]: import pandas as pd\n        import numpy as np\n# For .read_csv, always use header=0 when you know row 0 is the header row\n       train_df = pd.read_csv('csv/train.csv', header=0)\nIn [3]: train_df.head(3)\n```", "```py\nIn [83]: missing_perc=train_df.apply(lambda x: 100*(1-x.count().sum()/(1.0*len(x))))\nIn [85]: sorted_missing_perc=missing_perc.order(ascending=False)\n            sorted_missing_perc\nOut[85]: Cabin          77.104377\n         Age            19.865320\n         Embarked        0.224467\n         Fare            0.000000\n         Ticket          0.000000\n         Parch           0.000000\n         SibSp           0.000000\n         Sex             0.000000\n         Name            0.000000\n         Pclass          0.000000\n         Survived        0.000000\n         PassengerId     0.000000\n         dtype: float64\n```", "```py\nIn [137]:  import random\n                   bar_width=0.1\n                   categories_map={'Pclass':{'First':1,'Second':2, 'Third':3},\n                   'Sex':{'Female':'female','Male':'male'},\n                   'Survived':{'Perished':0,'Survived':1},\n                   'Embarked':{'Cherbourg':'C','Queenstown':'Q','Southampton':'S'},\n                   'SibSp': { str(x):x for x in [0,1,2,3,4,5,8]},\n                   'Parch': {str(x):x for x in range(7)}\n                   }\n                 colors=['red','green','blue','yellow','magenta','orange']\n                 subplots=[111,211,311,411,511,611,711,811]\n                 cIdx=0\n                 fig,ax=plt.subplots(len(subplots),figsize=(10,12))\n\n                 keyorder = ['Survived','Sex','Pclass','Embarked','SibSp','Parch']\n\n    for category_key,category_items in sorted(categories_map.iteritems(),\n                                              key=lambda i:keyorder.index(i[0])):\n        num_bars=len(category_items)\n        index=np.arange(num_bars)\n        idx=0\n        for cat_name,cat_val in sorted(category_items.iteritems()):\n            ax[cIdx].bar(idx,len(train_df[train_df[category_key]==cat_val]), label=cat_name,\n                    color=np.random.rand(3,1))\n            idx+=1\n        ax[cIdx].set_title('%s Breakdown' % category_key)\n    xlabels=sorted(category_items.keys()) \n        ax[cIdx].set_xticks(index+bar_width)\n        ax[cIdx].set_xticklabels(xlabels)\n        ax[cIdx].set_ylabel('Count')\n    cIdx +=1 \n    fig.subplots_adjust(hspace=0.8)\n    for hcat in ['Age','Fare']:\n        ax[cIdx].hist(train_df[hcat].dropna(),color=np.random.rand(3,1))\n        ax[cIdx].set_title('%s Breakdown' % hcat)\n        #ax[cIdx].set_xlabel(hcat)\n        ax[cIdx].set_ylabel('Frequency')\n        cIdx +=1\n\n    fig.subplots_adjust(hspace=0.8)\n    plt.show()\n```", "```py\nIn [85]: from collections import OrderedDict\n             num_passengers=len(train_df)\n             num_men=len(train_df[train_df['Sex']=='male'])\n             men_survived=train_df[(train_df['Survived']==1 ) & (train_df['Sex']=='male')]\n             num_men_survived=len(men_survived)\n             num_men_perished=num_men-num_men_survived\n             num_women=num_passengers-num_men\n             women_survived=train_df[(train_df['Survived']==1) & (train_df['Sex']=='female')]\n             num_women_survived=len(women_survived)\n             num_women_perished=num_women-num_women_survived\n             gender_survival_dict=OrderedDict()\n             gender_survival_dict['Survived']={'Men':num_men_survived,'Women':num_women_survived}\n             gender_survival_dict['Perished']={'Men':num_men_perished,'Women':num_women_perished}\n             gender_survival_dict['Survival Rate']= {'Men' : \n            round(100.0*num_men_survived/num_men,2),\n            'Women':round(100.0*num_women_survived/num_women,2)}\n    pd.DataFrame(gender_survival_dict)\n    Out[85]:\n```", "```py\nIn [76]: #code to display survival by gender\n            fig = plt.figure()\n            ax = fig.add_subplot(111)\n            perished_data=[num_men_perished, num_women_perished]\n            survived_data=[num_men_survived, num_women_survived]\n            N=2\n            ind = np.arange(N)     # the x locations for the groups\n            width = 0.35\n\n            survived_rects = ax.barh(ind, survived_data, width,color='green')\n            perished_rects = ax.barh(ind+width, perished_data, width,color='red')\n\n            ax.set_xlabel('Count')\n            ax.set_title('Count of Survival by Gender')\n            yTickMarks = ['Men','Women']\n            ax.set_yticks(ind+width)\n            ytickNames = ax.set_yticklabels(yTickMarks)\n            plt.setp(ytickNames, rotation=45, fontsize=10)\n\n            ## add a legend\n            ax.legend((survived_rects[0], perished_rects[0]), ('Survived', 'Perished') )\n            plt.show()\n```", "```py\n    In [86]: \n    from collections import OrderedDict\n    num_passengers=len(train_df)\n    num_class1=len(train_df[train_df['Pclass']==1])\n    class1_survived=train_df[(train_df['Survived']==1 ) & (train_df['Pclass']==1)]\n    num_class1_survived=len(class1_survived)\n    num_class1_perished=num_class1-num_class1_survived\n    num_class2=len(train_df[train_df['Pclass']==2])\n    class2_survived=train_df[(train_df['Survived']==1) & (train_df['Pclass']==2)]\n    num_class2_survived=len(class2_survived)\n    num_class2_perished=num_class2-num_class2_survived\n    num_class3=num_passengers-num_class1-num_class2\n    class3_survived=train_df[(train_df['Survived']==1 ) & (train_df['Pclass']==3)]\n    num_class3_survived=len(class3_survived)\n    num_class3_perished=num_class3-num_class3_survived\n    pclass_survival_dict=OrderedDict()\n    pclass_survival_dict['Survived']={'1st Class':num_class1_survived,\n                                      '2nd Class':num_class2_survived,\n                                      '3rd Class':num_class3_survived}\n    pclass_survival_dict['Perished']={'1st Class':num_class1_perished,\n                                      '2nd Class':num_class2_perished,\n                                     '3rd Class':num_class3_perished}\n    pclass_survival_dict['Survival Rate']= {'1st Class' : round(100.0*num_class1_survived/num_class1,2),\n                   '2nd Class':round(100.0*num_class2_survived/num_class2,2),\n                   '3rd Class':round(100.0*num_class3_survived/num_class3,2),}\n    pd.DataFrame(pclass_survival_dict)\n\n    Out[86]:\n```", "```py\n    In [186]:\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    perished_data=[num_class1_perished, num_class2_perished, num_class3_perished]\n    survived_data=[num_class1_survived, num_class2_survived, num_class3_survived]\n    N=3\n    ind = np.arange(N)                # the x locations for the groups\n    width = 0.35\n    survived_rects = ax.barh(ind, survived_data, width,color='blue')\n    perished_rects = ax.barh(ind+width, perished_data, width,color='red')\n    ax.set_xlabel('Count')\n    ax.set_title('Survivor Count by Passenger class')\n    yTickMarks = ['1st Class','2nd Class', '3rd Class']\n    ax.set_yticks(ind+width)\n    ytickNames = ax.set_yticklabels(yTickMarks)\n    plt.setp(ytickNames, rotation=45, fontsize=10)\n    ## add a legend\n    ax.legend( (survived_rects[0], perished_rects[0]), ('Survived', 'Perished'),\n              loc=10 )\n    plt.show()\n```", "```py\nIn [173]: survival_counts=pd.crosstab([train_df.Pclass,train_df.Sex],train_df.Survived.astype(bool))\nsurvival_counts\nOut[173]:      Survived False  True\n    Pclass       Sex \n               1            female    3     91\n                            male     77     45\n               2            female    6     70\n                            male     91     17\n               3            female   72     72\n                            male    300     47\n```", "```py\nIn [183]: survival_counts.index=survival_counts.index.set_levels([['1st', '2nd', '3rd'], ['Women', 'Men']])\nIn [184]: survival_counts.columns=['Perished','Survived']\n```", "```py\nIn [185]: fig = plt.figure()\n              ax = fig.add_subplot(111)\n              ax.set_xlabel('Count')\n              ax.set_title('Survivor Count by Passenger class, Gender')\n              survival_counts.plot(kind='barh',ax=ax,width=0.75,\n                                   color=['red','black'], xlim=(0,400))\nOut[185]: <matplotlib.axes._subplots.AxesSubplot at 0x7f714b187e90>\n```", "```py\n         NumberOfPeople  Pclass  PriceBucket     Sex  Survived\n    0                0       1            0  female         0\n    1                1       1            0    male         0\n    2                0       1            1  female         0\n    3                0       1            1    male         0\n    4                7       1            2  female         1\n    5               34       1            2    male         0\n    6                1       1            3  female         1\n    7               19       1            3    male         0\n    8                0       2            0  female         0\n    9                0       2            0    male         0\n    10              35       2            1  female         1\n    11              63       2            1    male         0\n    12              31       2            2  female         1\n    13              25       2            2    male         0\n    14               4       2            3  female         1\n    15               6       2            3    male         0\n    16              64       3            0  female         1\n    17             256       3            0    male         0\n    18              43       3            1  female         1\n    19              38       3            1    male         0\n    20              21       3            2  female         0\n    21              24       3            2    male         0\n    22              10       3            3  female         0\n    23               5       3            3    male         0\n```", "```py\nIn [192]: test_df.head(3)[['PassengerId','Pclass','Sex','Fare']]\nOut[192]: PassengerId   Pclass  Sex     Fare\n          0        892     3       male    7.8292\n          1        893     3       female  7.0000\n          2        894     2       male    9.6875\n```", "```py\n> head -4 csv/surv_results.csv \nPassengerId,Survived\n   892,0\n   893,1\n   894,0\n```", "```py\n    In [3]: from sklearn.linear_model import LinearRegression\n\n```", "```py\n    In [4]: model = LinearRegression(normalize=True) \n    In [6]: print model\n        LinearRegression(copy_X=True, fit_intercept=True, normalize=True)\n\n```", "```py\n    In [51]: sample_size=500\n             x = []\n             y = []\n\n            for i in range(sample_size):\n                newVal = random.normalvariate(100,10)\n                x.append(newVal)\n                y.append(newVal / 2.0 + random.normalvariate(50,5))\n```", "```py\n    In [67]: X = np.array(x)[:,np.newaxis]\n             X.shape\n    Out[67]: (500, 1)\n```", "```py\n    In [71]: model.fit(X,y)\n             print \"coeff=%s, intercept=%s\" % (model.coef_,model.intercept_)\n             coeff=[ 0.47071289], intercept=52.7456611783\n```", "```py\n    In [65]: plt.title(\"Plot of linear regression line and training data\")\n             plt.xlabel('x')\n             plt.ylabel('y')\n             plt.scatter(X,y,marker='o', color='green', label='training data');\n             plt.plot(X,model.predict(X), color='red', label='regression line')\n             plt.legend(loc=2)\n\n    Out[65]: [<matplotlib.lines.Line2D at 0x7f11b0752350]\n```", "```py\n    import patsy as pts\n    pts.dmatrices(\"y ~ x + a + b + a:b\", data)\n```", "```py\n    In [17]: %cd ~/devel/Titanic\n            /home/youruser/devel/sandbox/Learning/Kaggle/Titanic\n```", "```py\n    In [18]: import matplotlib.pyplot as plt\n                 import pandas as pd\n                 import numpy as np\n                 import patsy as pt\n    In [19]: train_df = pd.read_csv('csv/train.csv', header=0)\n    test_df = pd.read_csv('csv/test.csv', header=0) \n\n```", "```py\n    In [21]: formula1 = 'C(Pclass) + C(Sex) + Fare'\n             formula2 = 'C(Pclass) + C(Sex)'\n             formula3 = 'C(Sex)'\n             formula4 = 'C(Pclass) + C(Sex) + Age + SibSp + Parch'\n    formula5 = 'C(Pclass) + C(Sex) + Age + SibSp + Parch + \n             C(Embarked)' \n             formula6 = 'C(Pclass) + C(Sex) + Age + SibSp + C(Embarked)'\n             formula7 = 'C(Pclass) + C(Sex) + SibSp + Parch + C(Embarked)'\n             formula8 = 'C(Pclass) + C(Sex) + SibSp + Parch + C(Embarked)'\n\n    In [23]: formula_map = {'PClass_Sex_Fare' : formula1,\n                            'PClass_Sex' : formula2,\n                            'Sex' : formula3,\n                            'PClass_Sex_Age_Sibsp_Parch' : formula4,\n                            'PClass_Sex_Age_Sibsp_Parch_Embarked' : \n                             formula5,\n               'PClass_Sex_Embarked' : formula6,\n               'PClass_Sex_Age_Parch_Embarked' : formula7,\n               'PClass_Sex_SibSp_Parch_Embarked' : formula8\n                  }\n\n```", "```py\n    In [24]: \n    def fill_null_vals(df,col_name):\n        null_passengers=df[df[col_name].isnull()]\n        passenger_id_list = null_passengers['PassengerId'].tolist()\n        df_filled=df.copy()\n        for pass_id in passenger_id_list:\n            idx=df[df['PassengerId']==pass_id].index[0]\n    similar_passengers = df[(df['Sex']== \n    null_passengers['Sex'][idx]) & \n            (df['Pclass']==null_passengers['Pclass'][idx])]\n            mean_val = np.mean(similar_passengers[col_name].dropna())\n            df_filled.loc[idx,col_name]=mean_val\n        return df_filled\n\n```", "```py\n    In [28]: train_df_filled=fill_null_vals(train_df,'Fare')\n             train_df_filled=fill_null_vals(train_df_filled,'Age')\n             assert len(train_df_filled)==len(train_df)\n\n             test_df_filled=fill_null_vals(test_df,'Fare')\n             test_df_filled=fill_null_vals(test_df_filled,'Age')\n             assert len(test_df_filled)==len(test_df)\n\n```", "```py\n    In [29]: \n    from sklearn import metrics,svm, tree\n    for formula_name, formula in formula_map.iteritems():\n            print \"name=%s formula=%s\" % (formula_name,formula)\n    y_train,X_train = pt.dmatrices('Survived ~ ' + formula, \n                                        train_df_filled,return_type='dataframe')\n         y_train = np.ravel(y_train)\n    model = tree.DecisionTreeClassifier(criterion='entropy', \n                 max_depth=3,min_samples_leaf=5)\n         print \"About to fit...\"\n         dt_model = model.fit(X_train, y_train)\n         print \"Training score:%s\" % dt_model.score(X_train,y_train)\n         X_test=pt.dmatrix(formula,test_df_filled)\n         predicted=dt_model.predict(X_test)\n         print \"predicted:%s\" % predicted[:5]\n         assert len(predicted)==len(test_df)\n         pred_results = pd.Series(predicted,name='Survived')\n    dt_results = pd.concat([test_df['PassengerId'], \n                      pred_results],axis=1)\n         dt_results.Survived = dt_results.Survived.astype(int)\n         results_file = 'csv/dt_%s_1.csv' % (formula_name)\n         print \"output file: %s\\n\" % results_file\n         dt_results.to_csv(results_file,index=False)\n\n```", "```py\n    from sklearn import svm\n\n```", "```py\n    model = svm.SVC(kernel=kernel)\n    svm_model = model.fit(X_train, y_train)\n    X_test = pt.dmatrix(formula, test_df_filled)\n    . . .\n\n```", "```py\n    - Does x have fur?\n    Yes: x is a mammal\n    No: Does x have feathers?\n    Yes: x is a bird\n    No: Does x have scales?\n    Yes: Does x have gills?\n    Yes: x is a fish\n    No: x is a reptile\n    No: x is an amphibian\n\n```", "```py\n    from sklearn import tree\n```", "```py\n    model = tree.DecisionTreeClassifier(criterion='entropy', \n                 max_depth=3,min_samples_leaf=5)\n    dt_model = model.fit(X_train, y_train)\n X_test = dt.dmatrix(formula, test_df_filled)\n    #. . .\n```", "```py\nfrom sklearn import RandomForestClassifier  \n```", "```py\nmodel = RandomForestClassifier(n_estimators=num_estimators, \n                                  random_state=0)\nrf_model = model.fit(X_train, y_train)\nX_test = dt.dmatrix(formula, test_df_filled)\n   . . .\n```", "```py\nfrom sklearn.datasets import load_iris \niris = load_iris()\n```", "```py\nIn [2]: iris_data.keys()\n   Out[2]: ['target_names', 'data', 'target', 'DESCR', 'feature_names']\n```", "```py\nIn [3]: iris_data.data.shape\n   Out[3]: (150, 4)\n```", "```py\nIn [4]: print iris_data.feature_names\n   ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n```", "```py\nIn [9]: print iris_data.data[:2]\n   [[ 5.1  3.5  1.4  0.2]\n    [ 4.9  3\\.   1.4  0.2]]\n```", "```py\nIn [10]: print iris_data.target_names\n            ['setosa' 'versicolor' 'virginica']\n```", "```py\n    In [118]: X, y = iris_data.data, iris_data.target\n                  from sklearn.decomposition import PCA\n                  pca = PCA(n_components=2)\n                  pca.fit(X)\n                  X_red=pca.transform(X)\n                  print \"Shape of reduced dataset:%s\" % str(X_red.shape)\n\n             Shape of reduced dataset:(150, 2)\n```", "```py\n    In [136]: figsize(8,6)\n              fig=plt.figure()\n              fig.suptitle(\"Dimensionality reduction on iris data\")\n              ax=fig.add_subplot(1,1,1)\n              colors=['red','yellow','magenta']\n              cols=[colors[i] for i in iris_data.target]\n              ax.scatter(X_red[:,0],X[:,1],c=cols)\n    Out[136]:\n    <matplotlib.collections.PathCollection at 0x7fde7fae07d0>\n```", "```py\n    In [57]:\n    print \"Dimension Composition:\"\n    idx=1\n    for comp in pca.components_:\n        print \"Dim %s\" % idx\n        print \" + \".join(\"%.2f x %s\" % (value, name)\n                         for value, name in zip(comp, iris_data.feature_names))\n        idx += 1\n\n    Dimension Composition:\n    Dim 1\n    0.36 x sepal length (cm) + -0.08 x sepal width (cm) + 0.86 x petal length (cm) + 0.36 x petal width (cm)\n    Dim 2\n    -0.66 x sepal length (cm) + -0.73 x sepal width (cm) + 0.18 x petal length (cm) + 0.07 x petal width (cm)\n```", "```py\n    In [142]: from sklearn.cluster import KMeans\n              k_means = KMeans(n_clusters=3, random_state=0)\n              k_means.fit(X_red)\n              y_pred = k_means.predict(X_red)\n```", "```py\n    In [145]: figsize(8,6)\n              fig=plt.figure()\n              fig.suptitle(\"K-Means clustering on PCA-reduced iris data, \n              K=3\")\n              ax=fig.add_subplot(1,1,1)\n              ax.scatter(X_red[:, 0], X_red[:, 1], c=y_pred);\n```", "```py\n# In[1]:\n    import os\n    import pandas as pd\n    import numpy as np\n    from sklearn import preprocessing\n    from sklearn.decomposition import PCA\n    from sklearn.preprocessing import scale\n    import matplotlib.pyplot as plt\n    import xgboost\n```", "```py\n# In[24]:\n    os.chdir('C:/')\n```", "```py\n# In[19]:\n    train = pd.read_csv('train.csv')\n    test = pd.read_csv('test.csv')\n```", "```py\n# In[31]:\n    #train.dtypes[train.dtypes=='float64'or train.dtypes == 'int64']\n    varcs = train.var(axis=0)\n    varcs = varcs[varcs == 0]\n    to_drop = list(varcs.index)\n    dt = train.drop(to_drop, axis=1)\n    print(\"The variables {} have been dropped as they have zero variance\".format(to_drop))\n```", "```py\n# In[20]:\n# drops the variables with zero variance in a given dataset\n    def drop_zerovarcs(data):\n        varcs = data.var(axis=0)\n        varcs = varcs[varcs == 0]\n        to_drop = list(varcs.index)\n        #data_new = data.drop(to_drop, axis=1, inplace=True)\n        print(\"The variables {} have zero variance\".format(to_drop))\n        return to_drop\n```", "```py\n# drops columns from test where variance is zero in test data as well as the columns for which variance is zero in train data\n# In[21]:\n    test_drops = drop_zerovarcs(test)\n    train_drops = drop_zerovarcs(train)\n    test_train_drop = [x for x in train_drops if x not in test_drops]\n\n# train and test have different columns which have zero variance\n# Hence dropping the same columns in train and test data. Dropping the columns with zero variance in train data from test data.   \ntest.drop(test_train_drop, axis=1,inplace=True)\n```", "```py\n# In[22]:\n# drop the columns in test for which variance is zero in train data    train.drop(train_drops, axis=1,inplace=True)\n    #len(list(train.drop(train_drops,axis=1).columns))\n    test.drop(train_drops,axis=1,inplace=True)\n    #len(list(test.drop(train_drops,axis=1).columns))\n```", "```py\n# In[25]:\n# Find Unique, Total Count and NAs\n    def uni_ct_na(data):\n        unique = data.apply(lambda x: x.nunique(), axis=0)\n        count = data.apply(lambda x: x.count(), axis=0)\n        null = data.isnull().sum()\n        na = data.isna().sum()\n        summary_df = pd.DataFrame([unique, count, null, na],index=['Unique', 'Count', 'Null', 'NA'])\n        summary_df.T.to_csv('summary_df.csv')\n\n# In[26]:\n    uni_ct_na(train)\n```", "```py\n# In[27]: #Finding the list of categorical variables\n    obj = list(train.dtypes[train.dtypes=='object'].index)\n```", "```py\n# In[28]: #Dummy variables using categorical variables\n   obj_dum_train = pd.get_dummies(train[obj])\n    train = pd.concat([train,obj_dum_train],axis=1).drop(obj,axis=1)\n    obj_dum_test = pd.get_dummies(test[obj])\n    test = pd.concat([test,obj_dum_test],axis=1).drop(obj,axis=1)\n```", "```py\n# In[29]: # Keeping only numeric variables to apply PCA\n    train_cols = train.columns\n    train_not_obj = [x for x in train_cols if x not in obj]\n    train = train[train_not_obj]\n\n    test_cols = test.columns\n    test_not_obj = [x for x in test_cols if x not in obj]\n    test = test[test_not_obj]\n```", "```py\n# In[30]: # Plotting Scree plot to get the number of components which will\nexplain 90% variance in data\n    X=train.iloc[:,1:].values\n    X = scale(X)\n    pca = PCA()\n    pca.fit(X)\n    var= pca.explained_variance_ratio_\n    var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n    plt.plot(var1)\n```", "```py\n# In[31]: # Performing PCA on train and test data\n    X=train.iloc[:,1:].values\n    X = scale(X)\n    pca = PCA(n_components=300)\n    pca.fit(X)\n    train_pca = pca.transform(X)\n    train_pca.shape\n\n    X_test=train.iloc[:,1:].values\n    X_test = scale(X)\n    test_pca = pca.transform(X_test)\n```", "```py\n# In[32]:\n# Separating x and y variables to be passed to xgboost\n    train_y = train_pca[:,1]\n    train_x = train_pca[:,2:]\n    test_y = test_pca[:,1]\n    test_x = test_pca[:,2:]\n```", "```py\n# In[33]: # Fitting a xgboost model with default options\n    model = xgboost.XGBRegressor() \n    model.fit(train_x, train_y)\n```", "```py\n# In[34]: # Predict from the model on test data\n    pred_y = model.predict(test_x) \n# In[189]:\n    test_y\n```", "```py\n# In[35]: # Calculating Root Mean Square Error\n    rmse = np.sqrt(np.sum((pred_y-test_y)**2)/len(pred_y))\n    rmse\n```", "```py\n import matplotlib.pyplot as plt %matplotlib inline entropies = [-(p/100)*np.log2(p/100) for p in range(1,101)] plt.plot(entropies)\n```", "```py\n se_status_u = ['Rich','Poor','Affluent'] \npurchase_u = ['yes','no','yes']\n\n# Creating the dataframe with 10,000 rows and 2 columns viz. purchase and se_status\nimport random \nimport pandas as pd \nse_status = []\npurchase = []\nfor i in range(10000):\n    se_status.append(random.choice(se_status_u))\n    purchase.append(random.choice(purchase_u))\ndf = pd.DataFrame({'se_status':se_status,'purchase':purchase})\n```", "```py\n# Function for calculating initial entropy of the dataframe\ndef int_entropy(df, ycol):\n        y_u = list(df[ycol].unique())\n        p = [df[df[ycol] == res].shape[0]/df.shape[0] for res in y_u]\n        entropy = np.sum([-(e*np.log2(e)) for e in p]) \n        return entropy\n\ndf_int_entropy = int_entropy(df,'purchase')\ndf_int_entropy\n```", "```py\n**# Function for calculating entropy of a particular column of the dataframe**  \ndef col_entropy(df,ycol,col):  \n    y_u = df[ycol].unique()  \n    col_u = df[col].unique()  \n    ent_colval = []  \n    final_ent_col = 0   \n    for colval in col_u:  \n        p = [(df[(df[ycol] == yval) & (df[col] == colval)]).shape[0]/(df[col] == colval).shape[0] for yval in y_u]  ent_colval = np.sum([-(e*np.log2(e)) for e in p])   \nfinal_ent_col += ent_colval* ((df[df[col] == colval]).shape[0]/(df.shape[0]))  return final_ent_col\n```", "```py\n    df_se_entropy = col_entropy(df,'purchase','se_status')\n    print(df_int_entropy)\n    information_gain = df_int_entropy - df_se_entropy\n    print(information_gain)\n```"]