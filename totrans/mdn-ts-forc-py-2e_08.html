<html><head></head><body>
  <div id="_idContainer223" class="Basic-Text-Frame">
    <h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-139" class="chapterTitle">Feature Engineering for Time Series Forecasting</h1>
    <p class="normal">In the previous chapter, we started looking at <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>) as a tool to solve the problem of <strong class="keyWord">time series forecasting</strong>. We also discussed a few techniques, such as <strong class="keyWord">time delay embedding</strong> and <strong class="keyWord">temporal embedding</strong>, which cast time series forecasting problems as classical regression problems from the ML paradigm. In this chapter, we’ll look at those techniques in detail and go through them in a practical sense, using the dataset we have worked with throughout this book.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Understanding feature engineering</li>
      <li class="bulletList">Avoiding data leakage</li>
      <li class="bulletList">Setting a forecast horizon</li>
      <li class="bulletList">Time delay embedding</li>
      <li class="bulletList">Temporal embedding</li>
    </ul>
    <h1 id="_idParaDest-140" class="heading-1">Technical requirements</h1>
    <p class="normal">You will need to set up the <strong class="keyWord">Anaconda</strong> environment, following the instructions in the <em class="italic">Preface</em> of the book, to get a working environment with all the libraries and datasets required for the code in this book. Any additional libraries needed will be installed while running the notebooks.</p>
    <p class="normal">You will need to run the following notebooks before using the code in this chapter:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">02-Preprocessing_London_Smart_Meter_Dataset.ipynb from Chapter02</code></li>
      <li class="bulletList"><code class="inlineCode">01-Setting_up_Experiment_Harness.ipynb from Chapter04</code></li>
    </ul>
    <p class="normal">The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter06"><span class="url">https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter06</span></a>.</p>
    <h1 id="_idParaDest-141" class="heading-1">Understanding feature engineering</h1>
    <p class="normal"><strong class="keyWord">Feature engineering</strong>, as the name suggests, is the process<a id="_idIndexMarker463"/> of engineering features from data, mostly using domain knowledge, to make the learning process smoother and more efficient. In a typical ML setting, engineering good features is essential to get good performance from any ML model. Feature engineering is a highly subjective part of ML, where each problem at hand has a different path of solution—one that is handcrafted for that problem. Suppose you have a dataset of house prices and you have a feature, <em class="italic">Year Built</em>, which tells you the year the house was built. Now, to make the information better, we can create another feature, <em class="italic">House Age</em>, from the <em class="italic">Year Built</em> feature. This may give the model better information, and this is called feature engineering.</p>
    <p class="normal">When we are casting a time series problem as a regression problem, there are a few standard techniques that we can apply. This is a key step in the process, as how well an ML model acquires an understanding of <em class="italic">time</em> is dependent on how well we engineer features to capture <em class="italic">time</em>. The baseline methods we covered in <em class="chapterRef">Chapter 4</em>, <em class="italic">Setting a Strong Baseline Forecast</em>, are the methods that are created for the specific use case of time series forecasting, and because of that, the temporal aspect of the problem is built into those models. For instance, ARIMA doesn’t need any feature engineering to understand time because it is built into the model. However, a standard regression model has no explicit understanding of time, so we need to create good features to embed the temporal aspect of the problem.</p>
    <p class="normal">In the previous chapter (<em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>), we talked about two main ways to encode time<a id="_idIndexMarker464"/> in the regression framework: <strong class="keyWord">time delay embedding</strong> and <strong class="keyWord">temporal embedding</strong>. Although we touched<a id="_idIndexMarker465"/> on these concepts at a high level, it is time to dig deeper and see them in action.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the <code class="inlineCode">01-Feature_Engineering.ipynb</code> notebook in the <code class="inlineCode">Chapter06</code> folder.</p>
    </div>
    <p class="normal">We have already split the dataset that we were working on into train, validation, and test datasets. However, since we are generating features that are based on previous observations, operationally, it is better when we have the train, validation, and test datasets combined. It will be clearer why shortly, but for now, let’s take it on faith and move ahead. Now, let’s combine the two datasets:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Reading the missing value imputed and train test split data</span>
train_df = pd.read_parquet(preprocessed / <span class="hljs-string">"selected_blocks_train_missing_imputed.parquet"</span>)
val_df = pd.read_parquet(preprocessed / <span class="hljs-string">"</span><span class="hljs-string">selected_blocks_val_missing_imputed.parquet"</span>)
test_df = pd.read_parquet(preprocessed / <span class="hljs-string">"selected_blocks_test_missing_imputed.parquet"</span>)
<span class="hljs-comment">#Adding train, validation and test tags to distinguish them before combining</span>
train_df[<span class="hljs-string">'type'</span>] = <span class="hljs-string">"train"</span>
val_df[<span class="hljs-string">'type'</span>] = <span class="hljs-string">"val"</span>
test_df[<span class="hljs-string">'type'</span>] = <span class="hljs-string">"test"</span>
full_df = pd.concat([train_df, val_df, test_df]).sort_values([<span class="hljs-string">"LCLid"</span>, <span class="hljs-string">"</span><span class="hljs-string">timestamp"</span>])
<span class="hljs-keyword">del</span> train_df, test_df, val_df
</code></pre>
    <p class="normal">Now, we have a <code class="inlineCode">full_df</code>, which combines<a id="_idIndexMarker466"/> the train, validation, and test datasets. Some of you may already have alarm bells ringing in your head at combining the train and test sets. What about <strong class="keyWord">data leakage</strong>? Let’s check it out.</p>
    <h1 id="_idParaDest-142" class="heading-1">Avoiding data leakage</h1>
    <p class="normal"><strong class="keyWord">Data leakage</strong> occurs when a model is trained<a id="_idIndexMarker467"/> with some information that would not be available at the time of prediction. Typically, this leads to high performance in the training set but very poor performance in unseen data. There are two types of data leakage:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Target leakage</strong> is when the information about<a id="_idIndexMarker468"/> the target (that we are trying to predict) leaks<a id="_idIndexMarker469"/> into some of the features in the model, leading to an overreliance of the model on those features, ultimately leading to poor generalization. This includes features<a id="_idIndexMarker470"/> that use the target<a id="_idIndexMarker471"/> in any way.</li>
      <li class="bulletList"><strong class="keyWord">Train-test contamination</strong> is when there is some information<a id="_idIndexMarker472"/> leakage between<a id="_idIndexMarker473"/> the train and test datasets. This can happen because of the careless handling and splitting of data. But it can also happen in more subtle ways, such as scaling a dataset before splitting the train and test sets.</li>
    </ul>
    <p class="normal">When we work with time series<a id="_idIndexMarker474"/> forecasting problems, the biggest and most common mistake that we can make is target leakage. We will have to think hard about each of the features to ensure that we don’t use any data that will not be available during prediction. The following diagram will help us remember and internalize this concept:</p>
    <figure class="mediaobject"><img src="../Images/B22389_06_01.png" alt="Figure 6.1 – Usable and not-usable information to avoid data leakage "/></figure>
    <p class="packt_figref">Figure 6.1: Usable and unusable information to avoid data leakage</p>
    <p class="normal">To make this concept clearer and more relevant to the time series forecasting context, let’s look at an example. Let’s say we are forecasting sales for shampoo, and we are using sales for conditioner as a feature. We developed the model, trained it on the training data, and tested it on the validation data. The model does very well. The moment we start predicting for the future, we can see a problem. We don’t know what the sales for conditioner are in the future either. While this example is pretty straightforward, there will be times when this becomes not so obvious. And that is why we need to exercise a fair amount of caution while creating features and always evaluate the features through the lens of <em class="italic">will this feature be available at the time of prediction?</em></p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practices</strong>:</p>
      <p class="normal">There are many ways of identifying<a id="_idIndexMarker475"/> target leakage, apart from thinking hard about the features:</p>
      <ul>
        <li class="bulletList">If the model you’ve built is too good to be true, you most likely have a leakage problem</li>
        <li class="bulletList">If any single feature has too much weightage in the feature importance of the model, that feature may have a problem with leakage</li>
        <li class="bulletList">Double-check the features that are highly correlated with the target</li>
      </ul>
    </div>
    <p class="normal">Although we generated forecasts<a id="_idIndexMarker476"/> earlier in this book, we never explicitly discussed <strong class="keyWord">forecast horizons</strong>. It is an important concept and essential for what we will discuss. Let’s take a bit of time to understand forecast horizons.</p>
    <h1 id="_idParaDest-143" class="heading-1">Setting a forecast horizon</h1>
    <p class="normal">A forecast horizon is the number of time steps<a id="_idIndexMarker477"/> into the future we want to forecast at any point in time. For instance, if we want to forecast the next 24 hours for the electricity consumption dataset that we have worked with, the forecast horizon becomes 48 (because the data is half-hourly). In <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, where we generated baselines, we just predicted the entire test data at once. In such cases, the forecast horizon becomes equal to the length of the test data.</p>
    <p class="normal">We never had to worry about this until now because, in the classical statistical methods of forecasting, this decision is decoupled from modeling. If we train a model, we can use it to predict any future point without retraining. But with <em class="italic">time series forecasting as regression</em>, we have a constraint on the forecast horizon, and it has its roots in data leakage. This might be unclear to you now, so we’ll revisit this point after we have learned about the feature engineering techniques. For now, let’s only look at single-step-ahead forecasting. In the context of the dataset we are working with, this means that we will be answering the question, <em class="italic">What is the energy consumption in the next half an hour?</em> We will talk about multi-step forecasting and other mechanics of forecasting in <em class="chapterRef">Part 4</em>, <em class="italic">The Mechanics of Forecasting</em>.</p>
    <p class="normal">Now that we have set some ground rules, let’s start looking at the different feature engineering techniques. To follow along with the Jupyter notebook, head over to the <code class="inlineCode">Chapter06</code> folder and use the <code class="inlineCode">01-Feature_Engineering.ipynb</code> file.</p>
    <h1 id="_idParaDest-144" class="heading-1">Time delay embedding</h1>
    <p class="normal">The basic idea behind time delay embedding<a id="_idIndexMarker478"/> is to embed time in terms of recent observations. In <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, we discussed including previous observations of a time series as <strong class="keyWord">lags</strong> (<em class="italic">Figure 5.6</em> under the subsection <em class="italic">Time delay embedding</em>).</p>
    <p class="normal">However, there are a few more ways to capture recent and seasonal information using this concept.</p>
    <ul>
      <li class="bulletList">Lags</li>
      <li class="bulletList">Rolling window aggregations</li>
      <li class="bulletList">Seasonal rolling window aggregations</li>
      <li class="bulletList">Exponentially weighted moving averages</li>
    </ul>
    <p class="normal">Let’s take a look.</p>
    <h2 id="_idParaDest-145" class="heading-2">Lags or backshift</h2>
    <p class="normal">Let’s assume we have a time series<a id="_idIndexMarker479"/> with time steps, <em class="italic">Y</em><sub class="subscript-italic" style="font-style: italic;">L</sub>. Consider that we are at time <em class="italic">T</em> and that we have a time series where the length of history is <em class="italic">L</em>. So our time series will have <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub> as the latest observation in the time series, and then <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T-1</sub>,<em class="italic"> y</em><sub class="subscript-italic" style="font-style: italic;">T-2</sub>, and so on as we move back<a id="_idIndexMarker480"/> in time. So lags, as explained in <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, are features that include the previous observations in the time series, as shown in the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B22389_06_02.png" alt="Figure 6.2 – Lag features "/></figure>
    <p class="packt_figref">Figure 6.2: Lag features</p>
    <p class="normal">We can create multiple lags<a id="_idIndexMarker481"/> by including observations<a id="_idIndexMarker482"/> that are <em class="italic">a</em> timesteps before (<em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T-a</sub>); we will call this <em class="italic">Lag a</em>. In the preceding diagram, we have shown <em class="italic">Lag 1</em>, <em class="italic">Lag 2</em>, and <em class="italic">Lag 3</em>. However, we can add any number of lags we like. Let’s learn how to do that now in code:</p>
    <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">"lag_1"</span>]=df[<span class="hljs-string">"column"</span>].shift(<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Remember when we combined the train and test datasets and I asked you to take it in good faith? It’s time to repay that faith. If we consider the lag operation (or any autoregressive feature), it relies on a continuous representation along the time axis. If we consider the test dataset, for the first few rows (or earliest dates), the lags would be missing because they are part of the training dataset. So by combining the two, we create a continuous representation along the time axis where standard functions in pandas, such as <code class="inlineCode">shift</code>, can be utilized to create these features easily and efficiently.</p>
    <p class="normal">It is as simple as that, but we need to perform the lag operation for each <code class="inlineCode">LCLid</code> separately. We have included a helpful method in <code class="inlineCode">src.feature_engineering.autoregressive_features</code> called <code class="inlineCode">add_lags</code> that adds all the lags you want for each <code class="inlineCode">LCLid</code> quickly and efficiently. Let’s see how we can use that.</p>
    <p class="normal">We are going to import the method and use a few of its parameters to configure the lag operation the way we want:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.feature_engineering.autoregressive_features <span class="hljs-keyword">import</span> add_lags
<span class="hljs-comment"># Creating first 5 lags and then same 5 lags but from previous day and previous week to capture seasonality</span>
lags = (
    (np.arange(<span class="hljs-number">5</span>) + <span class="hljs-number">1</span>).tolist()
    + (np.arange(<span class="hljs-number">5</span>) + <span class="hljs-number">46</span>).tolist()
    + (np.arange(<span class="hljs-number">5</span>) + (<span class="hljs-number">48</span> * <span class="hljs-number">7</span>) - <span class="hljs-number">2</span>).tolist()
)
full_df, added_features = add_lags(
    full_df, lags=lags, column=<span class="hljs-string">"energy_consumption"</span>, ts_id=<span class="hljs-string">"LCLid"</span>, use_32_bit=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">Now, let’s look at the parameters that we used in the previous code snippet:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">lags</code>: This parameter takes in a list of integers, denoting all the lags we need to create as features.</li>
      <li class="bulletList"><code class="inlineCode">column</code>: The name of the column to be lagged. In our case, this is <code class="inlineCode">energy_consumption</code>.</li>
      <li class="bulletList"><code class="inlineCode">ts_id</code>: The name of the column that contains the unique ID of a time series. If <code class="inlineCode">None</code>, it assumes that the DataFrame only contains a single time series. In our case, <code class="inlineCode">LCLid</code> is the name of that column.</li>
      <li class="bulletList"><code class="inlineCode">use_32_bit</code>: This parameter<a id="_idIndexMarker483"/> doesn’t do anything functionally<a id="_idIndexMarker484"/> but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</li>
    </ul>
    <p class="normal">This method returns the DataFrame with the lags added, as well as a list with the column names of the newly added features.</p>
    <h2 id="_idParaDest-146" class="heading-2">Rolling window aggregations</h2>
    <p class="normal">With lags, we connect the present<a id="_idIndexMarker485"/> points to single points in the past, but with rolling window features, we connect the present with an aggregate statistic of a window from the past. Instead of looking at the observation from previous time steps, we would look at an average of the observations from the last three timesteps. Take a look at the following<a id="_idIndexMarker486"/> diagram to understand this better:</p>
    <figure class="mediaobject"><img src="../Images/B22389_06_03.png" alt="Figure 6.3 – Rolling window aggregation features "/></figure>
    <p class="packt_figref">Figure 6.3: Rolling window aggregation features</p>
    <p class="normal">We can calculate rolling statistics with different windows, and each of them will capture slightly different aspects of the history. In the preceding diagram, we can see an example of a window of three and a window of four. When we are at timestep <em class="italic">T</em>, a rolling window of three would have <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub><sub class="subscript"> – 3</sub>, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub><sub class="subscript"> – 2</sub>, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub><sub class="subscript"> – 1</sub> as the vector of past observations. Once we have these, we can apply any aggregation functions, such as the mean, standard deviation, min, max, and so on. Once we have a scalar value after the aggregation function, we can include that as a feature for timestep <em class="italic">t</em>.</p>
    <div class="note">
      <p class="normal">We do <em class="italic">not include</em> <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub> in the vector of past observations because that leads to data leakage.</p>
    </div>
    <p class="normal">Let’s see how we can do this with pandas:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># We shift by one to make sure there is no data leakage</span>
df[<span class="hljs-string">"rolling_3_mean"</span>] = df[<span class="hljs-string">"column"</span>].shift(<span class="hljs-number">1</span>).rolling(<span class="hljs-number">3</span>).mean()
</code></pre>
    <p class="normal">Similar to the lags, we need to do this operation for each <code class="inlineCode">LCLid</code> column separately. We have included a helpful method in <code class="inlineCode">src.feature_engineering.autoregressive_features</code> called <code class="inlineCode">add_rolling_features</code> that adds all the rolling<a id="_idIndexMarker487"/> features you want for each <code class="inlineCode">LCLid</code> quickly<a id="_idIndexMarker488"/> and efficiently. Let’s see how we can use that.</p>
    <p class="normal">We are going to import this method and use a few of its parameters to configure the rolling operation the way we want:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.feature_engineering.autoregressive_features <span class="hljs-keyword">import</span> add_rolling_features
full_df, added_features = add_rolling_features(
    full_df,
    rolls=[<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">12</span>, <span class="hljs-number">48</span>],
    column=<span class="hljs-string">"energy_consumption"</span>,
    agg_funcs=[<span class="hljs-string">"mean"</span>, <span class="hljs-string">"std"</span>],
    ts_id=<span class="hljs-string">"LCLid"</span>,
    use_32_bit=<span class="hljs-literal">True</span>,
)
</code></pre>
    <p class="normal">Now, let’s look at the parameters that we used in the previous code snippet:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">rolls</code>: This parameter takes in a list of integers denoting all the windows over which we need to calculate the aggregate statistics.</li>
      <li class="bulletList"><code class="inlineCode">column</code>: The name of the column to be lagged. In our case, this is <code class="inlineCode">energy_consumption</code>.</li>
      <li class="bulletList"><code class="inlineCode">agg_funcs</code>: This is a list of aggregations that we want to do for each window we declared in <code class="inlineCode">rolls</code>. Allowable aggregation functions include <code class="inlineCode">{mean, std, max, min}</code>.</li>
      <li class="bulletList"><code class="inlineCode">n_shift</code>: This is the number of timesteps we need to shift before doing the rolling operation. This parameter avoids data leakage. Although we are shifting by one here, there are cases where we need to shift by more than one as well. This is typically used in multi-step forecasting, which we will cover in <em class="chapterRef">Part 4</em>, <em class="italic">The Mechanics of Forecasting</em>.</li>
      <li class="bulletList"><code class="inlineCode">ts_id</code>: The name of the column name that contains the unique ID of a time series. If <code class="inlineCode">None</code>, it assumes that the DataFrame only has a single time series. In our case, <code class="inlineCode">LCLid</code> is the name of that column.</li>
      <li class="bulletList"><code class="inlineCode">use_32_bit</code>: This parameter<a id="_idIndexMarker489"/> doesn’t do anything<a id="_idIndexMarker490"/> functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</li>
    </ul>
    <p class="normal">This method returns the DataFrame with the rolling features added, as well as a list with the column names of the newly added features.</p>
    <h2 id="_idParaDest-147" class="heading-2">Seasonal rolling window aggregations</h2>
    <p class="normal">Seasonal rolling window aggregations<a id="_idIndexMarker491"/> are very similar to rolling<a id="_idIndexMarker492"/> window aggregations, but instead of taking past <em class="italic">n</em> consecutive observations in the window, they take a seasonal window, skipping a constant number of timesteps between each item in a window. The following diagram will make this clearer:</p>
    <figure class="mediaobject"><img src="../Images/B22389_06_04.png" alt="Figure 6.4 – Seasonal rolling window aggregations "/></figure>
    <p class="packt_figref">Figure 6.4: Seasonal rolling window aggregations</p>
    <p class="normal">The key parameter here is the seasonality<a id="_idIndexMarker493"/> period, which is commonly<a id="_idIndexMarker494"/> referred to as <em class="italic">M</em>. This is the number of timesteps after which we expect the seasonality pattern to repeat. When we are at timestep <em class="italic">T</em>, a rolling window of three would have <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub><sub class="subscript"> – 3</sub>, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub><sub class="subscript"> – 2</sub>, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub><sub class="subscript"> – 1</sub>, as the vector of past observations. But the seasonal rolling window would skip <em class="italic">m</em> timesteps between each item in the window. This means that the observations that are there in the seasonal rolling window would be <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub><sub class="subscript"> – </sub><sub class="subscript-italic" style="font-style: italic;">M</sub>, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub><sub class="subscript"> – 2</sub><sub class="subscript-italic" style="font-style: italic;">M</sub>, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub><sub class="subscript"> – 3</sub><sub class="subscript-italic" style="font-style: italic;">M</sub>. Also, as usual, once we have the window vector, we just need to apply the aggregation function to get a scalar value and include that as a feature.</p>
    <div class="note">
      <p class="normal">We do <em class="italic">not include</em> <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">T</sub> as an element in the seasonal rolling window vector to avoid data leakage.</p>
    </div>
    <p class="normal">This is not an operation that you can do easily and efficiently with pandas. Some fancy NumPy indexing and Python loops should do the trick. We will use an implementation from <code class="inlineCode">github.com/jmoralez/window_ops/</code> that uses NumPy and Numba to make the operation fast and efficient.</p>
    <p class="normal">Just like the features we saw earlier, we need to do this operation for each <code class="inlineCode">LCLid</code> separately. We have included a helpful method in <code class="inlineCode">src.feature_engineering.autoregressive_features</code> called <code class="inlineCode">add_seasonal_rolling_features</code> that adds all the seasonal rolling features you want for each <code class="inlineCode">LCLid</code> quickly and efficiently. Let’s see how we can use that.</p>
    <p class="normal">We are going to import the method and use a few parameters of the method to configure the seasonal rolling operation the way we want:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.feature_engineering.autoregressive_features <span class="hljs-keyword">import</span> add_seasonal_rolling_features
full_df, added_features = add_seasonal_rolling_features(
    full_df,
    rolls=[<span class="hljs-number">3</span>],
    seasonal_periods=[<span class="hljs-number">48</span>, <span class="hljs-number">48</span> * <span class="hljs-number">7</span>],
    column=<span class="hljs-string">"</span><span class="hljs-string">energy_consumption"</span>,
    agg_funcs=[<span class="hljs-string">"mean"</span>, <span class="hljs-string">"std"</span>],
    ts_id=<span class="hljs-string">"LCLid"</span>,
    use_32_bit=<span class="hljs-literal">True</span>,
)
</code></pre>
    <p class="normal">Now, let’s look at the parameters<a id="_idIndexMarker495"/> that we used in the previous code<a id="_idIndexMarker496"/> snippet:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">seasonal_periods</code>: This is a list of seasonal periods that should be used in the seasonal rolling windows. In the case of multiple seasonalities, we can include the seasonal rolling features of all the seasonalities.</li>
      <li class="bulletList"><code class="inlineCode">rolls</code>: This parameter takes in a list of integers denoting all the windows over which we need to calculate aggregate statistics.</li>
      <li class="bulletList"><code class="inlineCode">column</code>: The name of the column to be lagged. In our case, this is <code class="inlineCode">energy_consumption</code>.</li>
      <li class="bulletList"><code class="inlineCode">agg_funcs</code>: This is a list of aggregations that we want to do for each window we declared in <code class="inlineCode">rolls</code>. The allowable aggregation functions are <code class="inlineCode">{mean, std, max, min}</code>.</li>
      <li class="bulletList"><code class="inlineCode">n_shift</code>: This is the number of seasonal timesteps we need to shift before doing the rolling operation. This parameter prevents data leakage.</li>
      <li class="bulletList"><code class="inlineCode">ts_id</code>: The name of the column name that contains the unique ID of a time series. If <code class="inlineCode">None</code>, it assumes that the DataFrame only contains a single time series. In our case, <code class="inlineCode">LCLid</code> is the name of that column.</li>
      <li class="bulletList"><code class="inlineCode">Use_32_bit</code>: This parameter doesn’t do anything functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</li>
    </ul>
    <p class="normal">As always, the method returns the DataFrame with seasonal rolling features and a list containing the column names of the newly added features.</p>
    <h2 id="_idParaDest-148" class="heading-2">Exponentially weighted moving average (EWMA)</h2>
    <p class="normal">With the rolling window mean<a id="_idIndexMarker497"/> operation, we calculated<a id="_idIndexMarker498"/> the average of the window, and it works<a id="_idIndexMarker499"/> synonymously with the <strong class="keyWord">moving average</strong>. EWMA is the slightly smarter cousin of the moving average. While the moving average considers a rolling window and considers each item in the window equally on the computed average, EWMA tries to do a weighted average on the window, and the weights decay at an exponential rate. There is a parameter, <img src="../Images/B22389_04_009.png" alt=""/>, that determines how fast the weights decay. Because of this, we can consider all the history available as a window and let the <img src="../Images/B22389_04_009.png" alt=""/> parameter decide how much recency is included in EWMA. This can be written simply and recursively, as follows:</p>
    <p class="center"><img src="../Images/B22389_06_003.png" alt=""/></p>
    <p class="normal">Here, we can see that the larger the value of <img src="../Images/B22389_04_009.png" alt=""/>, the more the average is skewed toward recent values (see <em class="italic">Figure 6.6</em> to get a visual impression of how the weights would be). If we expand the recursion, the weights of each term work out to be:</p>
    <p class="center"><img src="../Images/B22389_06_005.png" alt=""/></p>
    <p class="normal">where <em class="italic">k</em> is the number of timesteps<a id="_idIndexMarker500"/> behind <em class="italic">T</em>. If we plot<a id="_idIndexMarker501"/> the weights, we can see them in an exponential decay; <img src="../Images/B22389_04_009.png" alt=""/> determines how fast the decay happens. Another way to think<a id="_idIndexMarker502"/> about <img src="../Images/B22389_04_009.png" alt=""/> is in terms of <strong class="keyWord">span</strong>. Span is the number of periods at which the decayed weights approach zero (not in a strictly mathematical way but intuitively). <img src="../Images/B22389_04_009.png" alt=""/> and span are related through this equation:</p>
    <p class="center"><img src="../Images/B22389_06_009.png" alt=""/></p>
    <p class="normal">This will become clearer in the following diagram, where we have plotted how the weights decay for different values of <img src="../Images/B22389_04_009.png" alt=""/>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_06_05.png" alt="Figure 6.5 – Exponential weight decay for different values of  "/></figure>
    <p class="packt_figref">Figure 6.5: Exponential weight decay for different values of <img src="../Images/B22389_04_009.png" alt=""/></p>
    <p class="normal">Here, we can see that the weight becomes small by the time we reach the span.</p>
    <p class="normal">Intuitively, we can think of EWMA<a id="_idIndexMarker503"/> as an average<a id="_idIndexMarker504"/> of the entire history of the time series, but with parameters<a id="_idIndexMarker505"/> such as <img src="../Images/B22389_04_009.png" alt=""/> and <strong class="keyWord">span</strong>, we can make different periods of history more representative of the average. If we define a 60-period span, we can think that the last 60 time periods are what majorly drive the average. So making EWMAs with different spans or <img src="../Images/B22389_04_009.png" alt=""/> s gives us representative features that capture different periods of history.</p>
    <p class="normal">The overall process is depicted in the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B22389_06_06.png" alt="Figure 6.6 – EWMA features "/></figure>
    <p class="packt_figref">Figure 6.6: EWMA features</p>
    <p class="normal">Now, let’s see how we can do this in pandas:</p>
    <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">"ewma"</span>]=df[<span class="hljs-string">'column'</span>].shift(<span class="hljs-number">1</span>).ewm(alpha=<span class="hljs-number">0.5</span>).mean()
</code></pre>
    <p class="normal">Like the other features we discussed<a id="_idIndexMarker506"/> earlier, EWMA<a id="_idIndexMarker507"/> also needs to be done for each <code class="inlineCode">LCLid</code> separately. We have included a helpful method in <code class="inlineCode">src.feature_engineering.autoregressive_features</code> called <code class="inlineCode">add_ewma</code> that adds all the EWMA features you want for each <code class="inlineCode">LCLid</code> quickly and efficiently. Let’s see how we can use that.</p>
    <p class="normal">We are going to import the method and use a few parameters of the method to configure EWMA the way we want to:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.feature_engineering.autoregressive_features <span class="hljs-keyword">import</span> add_ewma
full_df, added_features = add_ewma(
    full_df,
    spans=[<span class="hljs-number">48</span> * <span class="hljs-number">60</span>, <span class="hljs-number">48</span> * <span class="hljs-number">7</span>, <span class="hljs-number">48</span>],
    column=<span class="hljs-string">"energy_consumption"</span>,
    ts_id=<span class="hljs-string">"LCLid"</span>,
    use_32_bit=<span class="hljs-literal">True</span>,
)
</code></pre>
    <p class="normal">Now, let’s look at the parameters that we used in the previous code snippet:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">alphas</code>: This is a list of all <img src="../Images/B22389_04_009.png" alt=""/>s we need to calculate the EWMA features for.</li>
      <li class="bulletList"><code class="inlineCode">spans</code>: Alternatively, we can use this to list all the spans we need to calculate the EWMA features for. If you use this feature, <code class="inlineCode">alphas</code> will be ignored.</li>
      <li class="bulletList"><code class="inlineCode">column</code>: The name of the column to be lagged. In our case, this is <code class="inlineCode">energy_consumption</code>.</li>
      <li class="bulletList"><code class="inlineCode">n_shift</code>: This is the number of seasonal timesteps we need to shift before doing the rolling operation. This parameter avoids data leakage.</li>
      <li class="bulletList"><code class="inlineCode">ts_id</code>: The name of the column name that has a unique ID for a time series. If <code class="inlineCode">None</code>, it assumes the DataFrame only contains a single time series. In our case, <code class="inlineCode">LCLid</code> is the name of that column.</li>
      <li class="bulletList"><code class="inlineCode">use_32_bit</code>: This parameter doesn’t do anything functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</li>
    </ul>
    <p class="normal">As always, the method returns the DataFrame containing the EWMA features, as well as a list with the column names of the newly added features.</p>
    <p class="normal">These are a few standard<a id="_idIndexMarker508"/> ways of including time delay embedding<a id="_idIndexMarker509"/> in your ML model, but you are not restricted to just these. As always, feature engineering is a space that is not bound by rules, and we can get as creative as we want and inject domain knowledge into the model. Apart from the features we have seen, we can include the difference in lag as custom lags that inject domain knowledge, and so on. In most practical cases, we end up using more than one way of time delay embedding into our models. The lag feature is the most basic and essential in most cases, but we do end up encoding more information with seasonal lags, rolling features, and so on. As with everything in ML, there is no silver bullet. Each dataset has its own intricacies, which makes feature engineering very important and different for each case.</p>
    <p class="normal">Now, let’s look at the other class of features we can add via <strong class="keyWord">temporal embedding</strong>.</p>
    <h1 id="_idParaDest-149" class="heading-1">Temporal embedding</h1>
    <p class="normal">In <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, we briefly discussed<a id="_idIndexMarker510"/> temporal embedding as a process where we try to embed <em class="italic">time</em> into features that an ML model can leverage. If we think about <em class="italic">time</em> for a second, we can see that two aspects of time are important to us in the context of time series forecasting—the <em class="italic">passage of time</em> and the <em class="italic">periodicity of time</em>.</p>
    <p class="normal">There are a few features that we can add to help us capture these aspects in an ML model:</p>
    <ul>
      <li class="bulletList">Calendar features</li>
      <li class="bulletList">Time elapsed</li>
      <li class="bulletList">Fourier terms</li>
    </ul>
    <p class="normal">Let’s look at each of them.</p>
    <h2 id="_idParaDest-150" class="heading-2">Calendar features</h2>
    <p class="normal">The first set of features<a id="_idIndexMarker511"/> that we can extract are features based on calendars. Although the strict definition of time series is a set of observations taken sequentially in time, more often than not, we will have the timestamps of these collected observations alongside the time series. We can utilize these timestamps and extract calendar features, such as the month, quarter, day of the year, hour, minutes, and so on. These features capture the periodicity of time and help an ML model capture seasonality well. Only the calendar features that are temporally higher than the frequency of the time series make sense. For instance, an hour feature in a time series with a weekly frequency doesn’t make sense, but a month feature and week feature make sense. We can utilize in-built datetime functionalities in pandas to create these features and treat them as categorical features in the model.</p>
    <h2 id="_idParaDest-151" class="heading-2">Time elapsed</h2>
    <p class="normal">This is another feature<a id="_idIndexMarker512"/> that captures the passage of time in an ML model. This feature increases monotonically as time increases, giving the ML model a sense of the passage of time. There are many ways to create this feature, but one of the easiest and most efficient ways is to use the integer representation of dates in NumPy:</p>
    <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">'</span><span class="hljs-string">time_elapsed'</span>] = df[<span class="hljs-string">'timestamp'</span>].values.astype(np.int64)/(<span class="hljs-number">10</span>**<span class="hljs-number">9</span>)
</code></pre>
    <p class="normal">We have included a helpful method in <code class="inlineCode">src.feature_engineering.temporal_features</code> called <code class="inlineCode">add_temporal_features</code> that adds all relevant temporal features automatically. Let’s see how we can use it.</p>
    <p class="normal">We are going to import the method and use a few parameters of this method to configure and create the temporal features:</p>
    <pre class="programlisting code"><code class="hljs-code">full_df, added_features = add_temporal_features(
    full_df,
    field_name=<span class="hljs-string">"timestamp"</span>,
    frequency=<span class="hljs-string">"30min"</span>,
    add_elapsed=<span class="hljs-literal">True</span>,
    drop=<span class="hljs-literal">False</span>,
    use_32_bit=<span class="hljs-literal">True</span>,
)
</code></pre>
    <p class="normal">Now, let’s look at the parameters that we used in the previous code snippet:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">field_name</code>: This is the column name that contains the datetime that should be used to create features.</li>
      <li class="bulletList"><code class="inlineCode">frequency</code>: We should provide the frequency of the time series as input so that the method automatically extracts the relevant features. These are standard pandas frequency strings.</li>
      <li class="bulletList"><code class="inlineCode">add_elapsed</code>: This flag turns the creation of the time elapsed feature on or off.</li>
      <li class="bulletList"><code class="inlineCode">use_32_bit</code>: This parameter doesn’t do anything functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</li>
    </ul>
    <p class="normal">Just like the previous methods<a id="_idIndexMarker513"/> we discussed, this also returns the new DataFrame with the temporal features added and a list containing the column names of the newly added features.</p>
    <h2 id="_idParaDest-152" class="heading-2">Fourier terms</h2>
    <p class="normal">Previously, we extracted<a id="_idIndexMarker514"/> a few calendar features such as the month, year, and so on, and we discussed<a id="_idIndexMarker515"/> using them as categorical variables in the ML model. Another way we can represent the same information, but on a continuous scale, is by using <strong class="keyWord">Fourier terms</strong>. We discussed the Fourier series in <em class="chapterRef">Chapter 3</em>, <em class="italic">Analyzing and Visualizing Time Series Data</em>. Just to reiterate, the sine-cosine form of the Fourier series is as follows:</p>
    <p class="center"><img src="../Images/B22389_06_015.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">S</em><sub class="subscript-italic" style="font-style: italic;">N</sub> is the <em class="italic">N</em>-term approximation of the signal, <em class="italic">S</em>. Theoretically, when <em class="italic">N</em> is infinite, the resulting approximation is equal to the original signal. <em class="italic">P</em> is the maximum length of the cycle, <em class="italic">a</em><sub class="subscript-italic" style="font-style: italic;">n</sub> and <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">n</sub> are the coefficients of the cosine and sine term, respectively, of the <em class="italic">n</em><sup class="superscript">th</sup> term in the expansion, and <em class="italic">a</em><sub class="subscript-italic" style="font-style: italic;">0</sub> is the intercept.</p>
    <p class="normal">We can create these cosine and sine functions as features to represent the seasonal cycle. If we encode the month, we know that it goes from 1 to 12 and then repeats itself. So <em class="italic">P</em>, in this case, will be 12, and <em class="italic">x</em> will be 1, 2, …12. Therefore, for each <em class="italic">x</em>, we can calculate the cosine and sine terms and add them as features to the ML model. Intuitively, we can think that the model will infer the coefficients based on the data and, thus, help the model predict the time series easier. </p>
    <p class="normal">The following diagram shows the difference in representations between the month on an ordinal scale and as a Fourier series:</p>
    <figure class="mediaobject"><img src="../Images/B22389_06_07.png" alt="Figure 6.7 – Month as an ordinal step function (top) versus Fourier terms (bottom) "/></figure>
    <p class="packt_figref">Figure 6.7: Month as an ordinal step function (top) versus Fourier terms (bottom)</p>
    <p class="normal">The preceding diagram<a id="_idIndexMarker516"/> shows just a single Fourier term; we can add multiple <a id="_idIndexMarker517"/>Fourier terms to help capture complex seasonality.</p>
    <p class="normal">We cannot say that continuous representation of seasonality is better than categorical because it depends on the type of model you use and the dataset. This is something we will have to find out empirically.</p>
    <p class="normal">To make the process of adding Fourier features easy, we have made some easy-to-use methods available in <code class="inlineCode">src.feature_engineering.temporal_features</code>, in a file called <code class="inlineCode">bulk_add_fourier_features</code> that adds Fourier features for all the calendar features we want automatically. Let’s see how we can use that.</p>
    <p class="normal">We are going to import the method and use a few of its parameters to configure and create the Fourier series-based features:</p>
    <pre class="programlisting code"><code class="hljs-code">full_df, added_features = bulk_add_fourier_features(
    full_df,
    [<span class="hljs-string">"timestamp_Month"</span>, <span class="hljs-string">"timestamp_Hour"</span>, <span class="hljs-string">"timestamp_Minute"</span>],
    max_values=[<span class="hljs-number">12</span>, <span class="hljs-number">24</span>, <span class="hljs-number">60</span>],
    n_fourier_terms=<span class="hljs-number">5</span>,
    use_32_bit=<span class="hljs-literal">True</span>,
)
</code></pre>
    <p class="normal">Now, let’s look at the parameters that we used in the previous code snippet:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">columns_to_encode</code>: This is the list of calendar features we need to encode using Fourier terms.</li>
      <li class="bulletList"><code class="inlineCode">max_values</code>: This is a list of max values for the seasonal cycle for the calendar features, in the same order as they are given in <code class="inlineCode">columns_to_encode</code>. For instance, for <code class="inlineCode">month</code> to encode as a column, we give <code class="inlineCode">12</code> as the corresponding <code class="inlineCode">max_value</code>. If not given, <code class="inlineCode">max_value</code> will be inferred. This is only recommended if the data you have contains at least a single complete seasonal cycle.</li>
      <li class="bulletList"><code class="inlineCode">n_fourier_terms</code>: This is the number of Fourier terms to be added. This is synonymous with <em class="italic">n</em> in the equation for the Fourier series mentioned previously.</li>
      <li class="bulletList"><code class="inlineCode">use_32_bit</code>: This parameter doesn’t do anything functionally but makes the DataFrames much smaller in memory, sacrificing the precision of the floating-point numbers.</li>
    </ul>
    <p class="normal">Just like the previous methods<a id="_idIndexMarker518"/> we’ve discussed, this also returns a new DataFrame<a id="_idIndexMarker519"/> with the Fourier features added, as well as a list with column names of the newly added features.</p>
    <p class="normal">After executing the <code class="inlineCode">01-Feature_Engineering.ipynb</code> notebook in <code class="inlineCode">Chapter06</code>, we will have the following feature-engineered files written to disk:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">selected_blocks_train_missing_imputed_feature_engg.parquet</code></li>
      <li class="bulletList"><code class="inlineCode">selected_blocks_val_missing_imputed_feature_engg.parquet</code></li>
      <li class="bulletList"><code class="inlineCode">selected_blocks_test_missing_imputed_feature_engg.parquet</code></li>
    </ul>
    <p class="normal">In this section, we looked at a few popular and effective ways to generate features for time series. But there<a id="_idIndexMarker520"/> are many more, and depending on your problem<a id="_idIndexMarker521"/> and the domain, many of them will be relevant.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Additional information</strong>:</p>
      <p class="normal">The world of feature engineering is vast, and there are a few open-source libraries that make exploring that space easier. A few of them are <a href="https://github.com/Nixtla/tsfeatures"><span class="url">https://github.com/Nixtla/tsfeatures</span></a>, <a href="https://tsfresh.readthedocs.io/en/latest/"><span class="url">https://tsfresh.readthedocs.io/en/latest/</span></a>, and <a href="https://github.com/DynamicsAndNeuralSystems/catch22"><span class="url">https://github.com/DynamicsAndNeuralSystems/catch22</span></a>. A preprint by Ben D. Fulcher titled <em class="italic">Feature-based time-series analysis</em> at <a href="https://arxiv.org/abs/1709.08055"><span class="url">https://arxiv.org/abs/1709.08055</span></a> also gives a nice summary of the space.</p>
      <p class="normal">A newer library<a id="_idIndexMarker522"/> called functime (<a href="https://github.com/functime-org/functime"><span class="url">https://github.com/functime-org/functime</span></a>) also provides fast feature engineering routines, written in Polars, and is worth checking out. A lot of the feature engineering discussed in the book can be made even faster using functime and Polars.</p>
    </div>
    <h1 id="_idParaDest-153" class="heading-1">Summary</h1>
    <p class="normal">After a brief overview of the ML for time series forecasting paradigm in the previous chapter, in this chapter, we looked at this practically and saw how we can prepare the dataset with the required features to start using these models. We reviewed a few time series-specific feature engineering techniques, such as lags, rolling, and seasonal features. All the techniques we learned in this chapter are tools with which we can quickly iterate through experiments to find out what works for our dataset. However, we only talked about feature engineering, which affects one side of the standard regression equation (<em class="italic">y</em> = <em class="italic">mX</em> + <em class="italic">c</em>). The other side, which is the target (<em class="italic">y</em>) we predict, is also equally important. In the next chapter, we’ll look at a few concepts such as stationarity and some transformations that affect the target.</p>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
  </div>
</body></html>