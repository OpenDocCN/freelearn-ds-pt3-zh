["```py\n% netstat -an | grep LISTEN\n```", "```py\n         ports:\n          - '7077:7077'\n          - '8080:8080'\n    ```", "```py\n         ports:\n          - '7077:7077'\n          - '8090:8080'\n    ```", "```py\n    git clone https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git\n    git command:\n\n    ```", "```py\n\n    In this case, you will need to reinstall the command-line tools with the following command:\n\n    ```", "```py\n\n    ```", "```py\n    make up\n    ```", "```py\nmake prep && docker-compose up -d\nsh prep-airflow.sh\n[+] Running 9/9\n✔ Container ts-spark-env-spark-master-1      Started\n✔ Container ts-spark-env-postgres-1          Healthy ✔ Container ts-spark-env-mlflow-server-1     Started ✔ Container ts-spark-env-jupyter-1           Started ✔ Container ts-spark-env-airflow-init-1      Exited ✔ Container ts-spark-env-spark-worker-1-1    Started ✔ Container ts-spark-env-airflow-scheduler-1 Running ✔ Container ts-spark-env-airflow-triggerer-1 Running ✔ Container ts-spark-env-airflow-webserver-1 Running\n```", "```py\n    open /Users/<USER_LOGIN>/.docker/buildx/current: permission denied\n    make up command.\n    ```", "```py\nmlflow.set_tracking_uri(\"http://mlflow-server:5000\")\nmlflow.set_experiment(\n    'ts-spark_ch4_data-ml-ops_time_series_prophet_notebook')\nwith mlflow.start_run():\n    model = Prophet().fit(pdf)\n…\n    mlflow.prophet.log_model(\n        model, artifact_path=ARTIFACT_DIR,\n        signature=signature)\n    mlflow.log_params(param)\n    mlflow.log_metrics(cv_metrics)\n```", "```py\n    def ingest_data():\n        sdf = spark.read.csv(\n            DATASOURCE, header=True, inferSchema=True)\n        pdf = sdf.select(\"date\", \"daily_min_temperature\").toPandas()\n        return pdf\n    ```", "```py\n    def transform_data(pdf, **kwargs):\n        pdf.columns = [\"ds\", \"y\"]\n        pdf[\"y\"] = pd.to_numeric(pdf[\"y\"], errors=\"coerce\")\n        pdf.drop(index=pdf.index[-2:], inplace=True)\n        pdf.dropna()\n        return pdf\n    ```", "```py\n    def train_and_log_model(pdf, **kwargs):\n        mlflow.set_experiment(\n            'ts-spark_ch4_data-ml-ops_time_series_prophet')\n        …\n            mlflow.prophet.log_model(\n                model, artifact_path=ARTIFACT_DIR,\n                signature=signature)\n        …\n            return model_uri\n    ```", "```py\n    def forecast(model_uri, **kwargs):\n        _model = mlflow.prophet.load_model(model_uri)\n        forecast = _model.predict(\n            _model.make_future_dataframe(30))\n        forecast[\n            ['ds', 'yhat', 'yhat_lower','yhat_upper']\n        ].to_csv('/data/ts-spark_ch4_prophet-forecast.csv')\n        return '/data/ts-spark_ch4_prophet-forecast.csv'\n    ```", "```py\ndag = DAG(\n    'ts-spark_ch4_data-ml-ops_time_series_prophet',\n    default_args=default_args,\n    description='ts-spark_ch4 - Data/MLOps pipeline example - Time series forecasting with Prophet',\n    schedule_interval=None\n)\n```", "```py\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2024, 1, 1),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1),\n}\n```", "```py\n    t1 = PythonOperator(\n        task_id='ingest_data',\n        python_callable=ingest_data,\n        dag=dag,\n    )\n    ```", "```py\n    t2 = PythonOperator(\n        task_id='transform_data',\n        python_callable=transform_data,\n        op_kwargs={'pdf': t1.output},\n        provide_context=True,\n        dag=dag,\n    )\n    ```", "```py\n    t3 = PythonOperator(\n        task_id='train_and_log_model',\n        python_callable=train_and_log_model,\n        op_kwargs={'pdf': t2.output},\n        provide_context=True,\n        dag=dag,\n    )\n    ```", "```py\n    t4 = PythonOperator(\n        task_id='forecast',\n        python_callable=forecast,\n        op_kwargs={'model_uri': t3.output},\n        provide_context=True,\n        dag=dag,\n    )\n    ```", "```py\n# Task dependencies\nt1 >> t2 >> t3 >> t4\n```", "```py\nmake down\n```", "```py\ndocker-compose down\n[+] Running 10/10\n ✔ Container ts-spark-env-spark-worker-1-1  Removed\n ✔ Container ts-spark-env-mlflow-server-1   Removed\n ✔ Container ts-spark-env-airflow-scheduler-1Removed ✔Container ts-spark-env-airflow-webserver-1Removed ✔ ontainer ts-spark-env-jupyter-1          Removed ✔ Container ts-spark-env-airflow-triggerer-1Removed ✔ Container ts-spark-env-airflow-init-1     Removed ✔ Container ts-spark-env-postgres-1         Removed ✔ Container ts-spark-env-spark-master-1     Removed ✔ Network ts-spark-env_default              Removed\n```"]