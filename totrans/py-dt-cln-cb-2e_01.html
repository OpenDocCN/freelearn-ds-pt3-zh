<html><head></head><body>
  <div id="_idContainer023" class="Basic-Text-Frame">
    <h1 class="chapterNumber">1</h1>
    <h1 id="_idParaDest-14" class="chapterTitle">Anticipating Data Cleaning Issues When Importing Tabular Data with pandas</h1>
    <p class="normal">Scientific distributions of <strong class="keyWord">Python</strong> (Anaconda, WinPython, Canopy, and so on) provide<a id="_idIndexMarker000"/> analysts with an impressive range of data manipulation, exploration, and visualization tools. One important tool is pandas. Developed by Wes McKinney in 2008, but really gaining in popularity after 2012, pandas is now an essential library for data analysis in Python. The recipes in this book demonstrate how many common data preparation tasks can be done more easily with pandas than with other tools. While we work with pandas extensively in this book, we also use other popular packages such as Numpy, matplotlib, and scipy.</p>
    <p class="normal">A key pandas object is the <strong class="keyWord">DataFrame</strong>, which represents data<a id="_idIndexMarker001"/> as a tabular structure, with rows and columns. In this way, it is similar to the other data stores we discuss in this chapter. However, a pandas DataFrame also has indexing functionality that makes selecting, combining, and transforming data relatively straightforward, as the recipes in this book will demonstrate.</p>
    <p class="normal">Before we can make use of this great functionality, we have to import our data into pandas. Data comes to us in a wide variety of formats: as CSV or Excel files, as tables from SQL databases, from statistical analysis packages such as SPSS, Stata, SAS, or R, from non-tabular sources such as JSON, and from web pages.</p>
    <p class="normal">We examine tools to import tabular data in this recipe. Specifically, we cover the following topics:</p>
    <ul>
      <li class="bulletList">Importing CSV files</li>
      <li class="bulletList">Importing Excel files</li>
      <li class="bulletList">Importing data from SQL databases</li>
      <li class="bulletList">Importing SPSS, Stata, and SAS data</li>
      <li class="bulletList">Importing R data</li>
      <li class="bulletList">Persisting tabular data</li>
    </ul>
    <h1 id="_idParaDest-15" class="heading-1">Technical requirements</h1>
    <p class="normal">The code and notebooks for this chapter are available on GitHub at <a href="https://github.com/michaelbwalker/Python-Data-Cleaning-Cookbook-Second-Edition"><span class="url">https://github.com/michaelbwalker/Python-Data-Cleaning-Cookbook-Second-Edition</span></a>. You can use any <strong class="keyWord">IDE</strong> (<strong class="keyWord">Integrated Development Environment</strong>) of your choice – IDLE, Visual Studio, Sublime, Spyder, and so on – or Jupyter Notebook to work with any of the code in this chapter, or any chapter in this book. A good guide to get started with Jupyter Notebook can be found here: <a href="https://www.dataquest.io/blog/jupyter-notebook-tutorial/"><span class="url">https://www.dataquest.io/blog/jupyter-notebook-tutorial/</span></a>. I used the Spyder IDE to write the code in this chapter.</p>
    <p class="normal">I used pandas 2.2.1 and NumPy version 1.24.3 for all of the code in this chapter and subsequent chapters. I have also tested all code with pandas 1.5.3.</p>
    <h1 id="_idParaDest-16" class="heading-1">Importing CSV files</h1>
    <p class="normal">The <code class="inlineCode">read_csv</code> method <a id="_idIndexMarker002"/>of the <code class="inlineCode">pandas</code> library<a id="_idIndexMarker003"/> can be used to read a file with <strong class="keyWord">comma separated values</strong> (<strong class="keyWord">CSV</strong>) and load it into memory as a pandas DataFrame. In this recipe, we import a CSV file and address some common issues: creating column names that make sense to us, parsing dates, and dropping rows with critical missing data.</p>
    <p class="normal">Raw data is often stored as CSV files. These files have a carriage return at the end of each line of data to demarcate a row, and a comma between each data value to delineate columns. Something other than a comma can be used as the delimiter, such as a tab. Quotation marks may be placed around values, which can be helpful when the delimiter occurs naturally within certain values, which sometimes happens with commas.</p>
    <p class="normal">All data in a CSV file are characters, regardless of the logical data type. This is why it is easy to view a CSV file, presuming it is not too large, in a text editor. The pandas <code class="inlineCode">read_csv</code> method will make an educated guess about the data type of each column, but you will need to help it along<a id="_idIndexMarker004"/> to ensure that these guesses are on the mark.</p>
    <h2 id="_idParaDest-17" class="heading-2">Getting ready</h2>
    <p class="normal">Create a folder for this chapter, and<a id="_idIndexMarker005"/> then create a new Python script or <strong class="keyWord">Jupyter Notebook</strong> file in that folder. Create a data subfolder, and then place the <code class="inlineCode">landtempssample.csv</code> file in that subfolder. Alternatively, you could retrieve all of the files from the GitHub repository, including the data files. Here is a screenshot of the beginning of the CSV file:</p>
    <figure class="mediaobject"><img src="../Images/B18596_01_01.png" alt="Screenshot from 2023-05-28 21-00-25"/></figure>
    <p class="packt_figref">Figure 1.1: Land Temperatures Data</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Data note</strong></p>
      <p class="normal">This dataset, taken from the Global Historical Climatology Network integrated database, is made available for public use<a id="_idIndexMarker006"/> by the United States National Oceanic and Atmospheric Administration at <a href="https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly"><span class="url">https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly</span></a>. I used the data from version 4. The data in this recipe uses a 100,000-row sample of the full dataset, which is also available in the repository.</p>
    </div>
    <h2 id="_idParaDest-18" class="heading-2">How to do it…</h2>
    <p class="normal">We will import a CSV file<a id="_idIndexMarker007"/> into pandas, taking advantage of some very useful <code class="inlineCode">read_csv</code> options:</p>
    <ol>
      <li class="numberedList" value="1">Import the <code class="inlineCode">pandas</code> library, and set up the environment to make viewing the output easier:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
pd.options.display.float_format = <span class="hljs-string">'{:,.2f}'</span>.<span class="hljs-built_in">format</span>
pd.set_option(<span class="hljs-string">'display.width'</span>, <span class="hljs-number">85</span>)
pd.set_option(<span class="hljs-string">'display.max_columns'</span>, <span class="hljs-number">8</span>)
</code></pre>
      </li>
      <li class="numberedList">Read the data file, set new names for the headings, and parse the date column.</li>
    </ol>
    <p class="normal-one">Pass an argument of <code class="inlineCode">1</code> to the <code class="inlineCode">skiprows</code> parameter to skip the first row, pass a list of columns to <code class="inlineCode">parse_dates</code> to create a pandas datetime column from those columns, and set <code class="inlineCode">low_memory</code> to <code class="inlineCode">False</code>. This will cause pandas to load all of the data into memory at once, rather than in chunks. We do this so that pandas can identify the data type of each column automatically. In the <em class="italic">There’s more…</em> section, we see how to set the data type for each column manually:</p>
    <pre class="programlisting code-one"><code class="hljs-code">landtemps = pd.read_csv(<span class="hljs-string">'data/landtempssample.csv'</span>,
...     names=[<span class="hljs-string">'stationid'</span>,<span class="hljs-string">'year'</span>,<span class="hljs-string">'month'</span>,<span class="hljs-string">'avgtemp'</span>,<span class="hljs-string">'latitude'</span>,
...       <span class="hljs-string">'longitude'</span>,<span class="hljs-string">'elevation'</span>,<span class="hljs-string">'station'</span>,<span class="hljs-string">'countryid'</span>,<span class="hljs-string">'country'</span>],
...     skiprows=<span class="hljs-number">1</span>,
...     parse_dates=[[<span class="hljs-string">'month'</span>,<span class="hljs-string">'year'</span>]],
...     low_memory=<span class="hljs-literal">False</span>)
<span class="hljs-built_in">type</span>(landtemps)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">&lt;class 'pandas.core.frame.DataFrame'&gt;
</code></pre>
    <div class="note-one">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">We have to use <code class="inlineCode">skiprows</code> because we are passing a list of column names to <code class="inlineCode">read_csv</code>. If we use the column names in the CSV file, we do not need to specify values for either <code class="inlineCode">names</code> or <code class="inlineCode">skiprows</code>.</p>
    </div>
    <ol>
      <li class="numberedList" value="3">Get a quick glimpse<a id="_idIndexMarker008"/> of the data.</li>
    </ol>
    <p class="normal-one">View the first few rows. Show the data type for all columns, as well as the number of rows and columns:</p>
    <pre class="programlisting code-one"><code class="hljs-code">landtemps.head(<span class="hljs-number">7</span>)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">  month_year    stationid  ...  countryid              country
0 2000-04-01  USS0010K01S  ...         US        United States
1 1940-05-01  CI000085406  ...         CI                Chile
2 2013-12-01  USC00036376  ...         US        United States
3 1963-02-01  ASN00024002  ...         AS            Australia
4 2001-11-01  ASN00028007  ...         AS            Australia
5 1991-04-01  USW00024151  ...         US        United States
6 1993-12-01  RSM00022641  ...         RS               Russia
[7 rows x 9 columns]
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">landtemps.dtypes
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">month_year       datetime64[ns]
stationed        object
avgtemp          float64
latitude         float64
longitude        float64
elevation        float64
station          object
countryid        object
country          object
dtype: object
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">landtemps.shape
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">(100000, 9)
</code></pre>
    <ol>
      <li class="numberedList" value="4">Give the date column<a id="_idIndexMarker009"/> a more appropriate name and view the summary statistics for average monthly temperature:
        <pre class="programlisting code-one"><code class="hljs-code">landtemps.rename(columns={<span class="hljs-string">'month_year'</span>:<span class="hljs-string">'measuredate'</span>}, inplace=<span class="hljs-literal">True</span>)
landtemps.dtypes
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">measuredate      datetime64[ns]
stationid        object
avgtemp          float64
latitude         float64
longitude        float64
elevation        float64
station          object
countryid        object
country          object
dtype:           object
</code></pre>
        <pre class="programlisting code-one"><code class="hljs-code">landtemps.avgtemp.describe()
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">count   85,554.00
mean    10.92
std     11.52
min     -70.70
25%     3.46
50%     12.22
75%     19.57
max     39.95
Name: avgtemp, dtype: float64
</code></pre>
      </li>
      <li class="numberedList">Look for missing values for each column.</li>
    </ol>
    <p class="normal-one">Use <code class="inlineCode">isnull</code>, which returns <code class="inlineCode">True</code> for each value that is missing for each column, and <code class="inlineCode">False</code> when not missing. Chain this with <code class="inlineCode">sum</code> to count the missing values for each column. (When working with Boolean values, <code class="inlineCode">sum</code> treats <code class="inlineCode">True</code> as <code class="inlineCode">1</code> and <code class="inlineCode">False</code> as <code class="inlineCode">0</code>. I will discuss method chaining in the <em class="italic">There’s more...</em> section of this recipe):</p>
    <pre class="programlisting code-one"><code class="hljs-code">landtemps.isnull().<span class="hljs-built_in">sum</span>()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">measuredate    0
stationed      0
avgtemp        14446
latitude       0
longitude      0
elevation      0
station        0
countryid      0
country        5
dtype: int64
</code></pre>
    <ol>
      <li class="numberedList" value="6">Remove rows with missing data for <code class="inlineCode">avgtemp</code>.</li>
    </ol>
    <p class="normal-one">Use the <code class="inlineCode">subset</code> parameter<a id="_idIndexMarker010"/> to tell <code class="inlineCode">dropna</code> to drop rows when <code class="inlineCode">avgtemp</code> is missing. Set <code class="inlineCode">inplace</code> to <code class="inlineCode">True</code>. Leaving <code class="inlineCode">inplace</code> at its default value of <code class="inlineCode">False</code> would display the DataFrame, but the changes we have made would not be retained. Use the <code class="inlineCode">shape</code> attribute of the DataFrame to get the number of rows and columns:</p>
    <pre class="programlisting code-one"><code class="hljs-code">landtemps.dropna(subset=[<span class="hljs-string">'avgtemp'</span>], inplace=<span class="hljs-literal">True</span>)
landtemps.shape
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">(85554, 9)
</code></pre>
    <p class="normal">That’s it! Importing CSV files into pandas is as simple as that.</p>
    <h2 id="_idParaDest-19" class="heading-2">How it works...</h2>
    <p class="normal">Almost all of the recipes in this book use the <code class="inlineCode">pandas</code> library. We refer to it as <code class="inlineCode">pd</code> to make it easier to reference later. This is customary. We also use <code class="inlineCode">float_format</code> to display float values in a readable way and <code class="inlineCode">set_option</code> to make the Terminal output wide enough to accommodate the number of variables.</p>
    <p class="normal">Much of the work is done by the first line in <em class="italic">Step 2</em>. We use <code class="inlineCode">read_csv</code> to load a pandas DataFrame in memory and call it <code class="inlineCode">landtemps</code>. In addition to passing a filename, we set the <code class="inlineCode">names</code> parameter to a list of our preferred column headings. We also tell <code class="inlineCode">read_csv</code> to skip the first row, by setting <code class="inlineCode">skiprows</code> to 1, since the original column headings are in the first row of the CSV file. If we do not tell it to skip the first row, <code class="inlineCode">read_csv</code> will treat the header row in the file as actual data.</p>
    <p class="normal"><code class="inlineCode">read_csv</code> also solves a date conversion issue for us. We use the <code class="inlineCode">parse_dates</code> parameter to ask it to convert the <code class="inlineCode">month</code> and <code class="inlineCode">year</code> columns to a date value.</p>
    <p class="normal"><em class="italic">Step 3</em> runs through a few standard data checks. We use <code class="inlineCode">head(7)</code> to print out all columns for the first seven rows. We use the <code class="inlineCode">dtypes</code> attribute of the DataFrame to show the data type of all columns. Each column has the expected data type. In pandas, character data has the object data type, a data type that allows for mixed values. <code class="inlineCode">shape</code> returns a tuple, whose first element is the number of rows in the DataFrame (100,000 in this case) and whose second element is the number of columns (9).</p>
    <p class="normal">When we used <code class="inlineCode">read_csv</code> to parse the <code class="inlineCode">month</code> and <code class="inlineCode">year</code> columns, it gave the resulting column the name <code class="inlineCode">month_year</code>. We used the <code class="inlineCode">rename</code> method in <em class="italic">Step 4</em> to give that column a more appropriate name. We need to specify <code class="inlineCode">inplace=True</code> to replace the old column name with the new column name in memory. The <code class="inlineCode">describe</code> method provides summary statistics on the <code class="inlineCode">avgtemp</code> column.</p>
    <p class="normal">Notice that the count for <code class="inlineCode">avgtemp</code> indicates that there are 85,554 rows that have valid values for <code class="inlineCode">avgtemp</code>. This is out of 100,000 rows for the whole DataFrame, as provided by the <code class="inlineCode">shape</code> attribute. The listing of missing values for each column in <em class="italic">Step 5</em> (<code class="inlineCode">landtemps.isnull().sum()</code>) confirms this: <em class="italic">100,000 – 85,554 = 14,446</em>.</p>
    <p class="normal"><em class="italic">Step 6</em> drops all rows where <code class="inlineCode">avgtemp</code> is <code class="inlineCode">NaN</code>. (The <code class="inlineCode">NaN</code> value, not a number, is the pandas representation of missing values.) <code class="inlineCode">subset</code> is used to indicate which column to check for missing values. The <code class="inlineCode">shape</code> attribute for <code class="inlineCode">landtemps</code> now indicates that there are 85,554 rows, which is what we would<a id="_idIndexMarker011"/> expect, given the previous count from <code class="inlineCode">describe</code>.</p>
    <h2 id="_idParaDest-20" class="heading-2">There’s more...</h2>
    <p class="normal">If the file you are reading uses a delimiter other than a comma, such as a tab, this can be specified in the <code class="inlineCode">sep</code> parameter of <code class="inlineCode">read_csv</code>. When creating the pandas DataFrame, an index was also created. The numbers to the far left of the output when <code class="inlineCode">head</code> was run are index values. Any number of rows can be specified for <code class="inlineCode">head</code>. The default value is <code class="inlineCode">5</code>.</p>
    <p class="normal">Instead of setting <code class="inlineCode">low_memory</code> to <code class="inlineCode">False</code>, to get pandas to<a id="_idIndexMarker012"/> make good guesses regarding data types, we could have set data types manually:</p>
    <pre class="programlisting code"><code class="hljs-code">landtemps = pd.read_csv(<span class="hljs-string">'</span><span class="hljs-string">data/landtempssample.csv'</span>,
    names=[<span class="hljs-string">'stationid'</span>,<span class="hljs-string">'year'</span>,<span class="hljs-string">'month'</span>,<span class="hljs-string">'avgtemp'</span>,<span class="hljs-string">'latitude'</span>,
      <span class="hljs-string">'longitude'</span>,<span class="hljs-string">'elevation'</span>,<span class="hljs-string">'station'</span>,<span class="hljs-string">'countryid'</span>,<span class="hljs-string">'</span><span class="hljs-string">country'</span>],
    skiprows=<span class="hljs-number">1</span>,
    parse_dates=[[<span class="hljs-string">'month'</span>,<span class="hljs-string">'year'</span>]],
    dtype={<span class="hljs-string">'stationid'</span>:<span class="hljs-string">'object'</span>, <span class="hljs-string">'avgtemp'</span>:<span class="hljs-string">'float64'</span>,
     <span class="hljs-string">'latitude'</span>:<span class="hljs-string">'float64'</span>,<span class="hljs-string">'</span><span class="hljs-string">longitude'</span>:<span class="hljs-string">'float64'</span>,
     <span class="hljs-string">'elevation'</span>:<span class="hljs-string">'float64'</span>,<span class="hljs-string">'station'</span>:<span class="hljs-string">'object'</span>,
     <span class="hljs-string">'countryid'</span>:<span class="hljs-string">'object'</span>,<span class="hljs-string">'country'</span>:<span class="hljs-string">'object'</span>},
    )
landtemps.info()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 100000 entries, 0 to 99999
Data columns (total 9 columns):
 #   Column      Non-Null Count   Dtype       
---  ------      --------------   -----       
 0   month_year  100000 non-null  datetime64[ns]
 1   stationid   100000 non-null  object      
 2   avgtemp     85554 non-null   float64     
 3   latitude    100000 non-null  float64     
 4   longitude   100000 non-null  float64     
 5   elevation   100000 non-null  float64     
 6   station     100000 non-null  object      
 7   countryid   100000 non-null  object      
 8   country     99995 non-null   object      
dtypes: datetime64[ns](1), float64(4), object(4)
memory usage: 6.9+ MB
</code></pre>
    <p class="normal">The <code class="inlineCode">landtemps.isnull().sum()</code> statement is an example of chaining methods. First, <code class="inlineCode">isnull</code> returns a DataFrame of <code class="inlineCode">True</code> and <code class="inlineCode">False</code> values, resulting from testing whether each column value is <code class="inlineCode">null</code>. The <code class="inlineCode">sum</code> function takes that DataFrame and sums the <code class="inlineCode">True</code> values for each column, interpreting the <code class="inlineCode">True</code> values as <code class="inlineCode">1</code> and the <code class="inlineCode">False</code> values as <code class="inlineCode">0</code>. We would have obtained the same result if we had used the following two steps:</p>
    <pre class="programlisting code"><code class="hljs-code">checknull = landtemps.isnull()
checknull.<span class="hljs-built_in">sum</span>()
</code></pre>
    <p class="normal">There is no hard and fast rule for when to chain methods and when not to do so. I find chaining<a id="_idIndexMarker013"/> helpful when the overall operation feels like a single step, even if it’s two or more steps mechanically. Chaining also has the side benefit of not creating extra objects that I might not need.</p>
    <p class="normal">The dataset used in this recipe is just a sample from the full land temperatures database, with almost 17 million records. You can run the larger file if your machine can handle it, with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">landtemps = pd.read_csv(<span class="hljs-string">'data/landtemps.zip'</span>,
...   compression=<span class="hljs-string">'zip'</span>, names=[<span class="hljs-string">'stationid'</span>,<span class="hljs-string">'year'</span>,
...     <span class="hljs-string">'month'</span>,<span class="hljs-string">'avgtemp'</span>,<span class="hljs-string">'latitude'</span>,<span class="hljs-string">'longitude'</span>,
...     <span class="hljs-string">'elevation'</span>,<span class="hljs-string">'station'</span>,<span class="hljs-string">'countryid'</span>,<span class="hljs-string">'country'</span>],
...     skiprows=<span class="hljs-number">1</span>,
...     parse_dates=[[<span class="hljs-string">'month'</span>,<span class="hljs-string">'year'</span>]],
...     low_memory=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal"><code class="inlineCode">read_csv</code> can read a compressed ZIP file. We get it to do this by passing the name of the ZIP file and the type of compression.</p>
    <h2 id="_idParaDest-21" class="heading-2">See also</h2>
    <p class="normal">Subsequent recipes in this chapter, and in other chapters, set indexes to improve navigation over rows and merging.</p>
    <p class="normal">A significant amount of reshaping of the Global Historical Climatology Network raw data was done before using it in this recipe. We demonstrate this in <em class="chapterRef">Chapter 11</em>, <em class="italic">Tidying and Reshaping Data</em>.</p>
    <h1 id="_idParaDest-22" class="heading-1">Importing Excel files</h1>
    <p class="normal">The <code class="inlineCode">read_excel</code> method<a id="_idIndexMarker014"/> of the <code class="inlineCode">pandas</code> library can be used to import data from an Excel file and load it into memory as a pandas DataFrame. In this recipe, we import an Excel file and handle some common issues when working with Excel files: extraneous header and footer information, selecting specific columns, removing rows with no data, and connecting to particular sheets.</p>
    <p class="normal">Despite the tabular structure of Excel, which invites the organization of data into rows and columns, spreadsheets are not datasets and do not require people to store data in that way. Even when some data conforms with those expectations, there is often additional information in rows or columns before or after the data to be imported. Data types are not always as clear as they are to the person who created the spreadsheet. This will be all too familiar to anyone who has ever battled with importing leading zeros. Moreover, Excel does not insist that all data in a column be of the same type, or that column headings be appropriate for use with a programming language such as Python.</p>
    <p class="normal">Fortunately, <code class="inlineCode">read_excel</code> has a number of options for handling messiness in Excel data. These options make it relatively easy to skip rows, select particular columns, and pull data from a particular sheet or sheets.</p>
    <h2 id="_idParaDest-23" class="heading-2">Getting ready</h2>
    <p class="normal">You can download<a id="_idIndexMarker015"/> the <code class="inlineCode">GDPpercapita22b.xlsx</code> file, as well as the code for this recipe, from the GitHub repository for this book. The code assumes that the Excel file is in a data subfolder. Here is a view of the beginning of the file (some columns were hidden for display purposes):</p>
    <figure class="mediaobject"><img src="../Images/B18596_01_02.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 1.2: View of the dataset</p>
    <p class="normal">And here is a view of the end of the file:</p>
    <figure class="mediaobject"><img src="../Images/B18596_01_03.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 1.3: View of the dataset</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Data note</strong></p>
      <p class="normal">This dataset, from the Organisation<a id="_idIndexMarker016"/> for Economic Co-operation and Development, is available for public use at <a href="https://stats.oecd.org/"><span class="url">https://stats.oecd.org/</span></a>.</p>
    </div>
    <h2 id="_idParaDest-24" class="heading-2">How to do it…</h2>
    <p class="normal">We import an Excel file<a id="_idIndexMarker017"/> into pandas and do some initial data cleaning:</p>
    <ol>
      <li class="numberedList" value="1">Import the <code class="inlineCode">pandas</code> library:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
</code></pre>
      </li>
      <li class="numberedList">Read the Excel per capita GDP data.</li>
    </ol>
    <p class="normal-one">Select the sheet with the data<a id="_idIndexMarker018"/> we need, but skip the columns and rows that we do not want. Use the <code class="inlineCode">sheet_name</code> parameter to specify the sheet. Set <code class="inlineCode">skiprows</code> to <code class="inlineCode">4</code> and <code class="inlineCode">skipfooter</code> to <code class="inlineCode">1</code> to skip the first four rows (the first row is hidden) and the last row. We provide values for <code class="inlineCode">usecols</code> to get data from column <code class="inlineCode">A</code> and columns <code class="inlineCode">C</code> through <code class="inlineCode">W</code> (column <code class="inlineCode">B</code> is blank). Use <code class="inlineCode">head</code> to view the first few rows and <code class="inlineCode">shape</code> to get the number of rows and columns:</p>
    <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP = pd.read_excel(<span class="hljs-string">"data/GDPpercapita22b.xlsx"</span>,
...    sheet_name=<span class="hljs-string">"OECD.Stat export"</span>,
...    skiprows=<span class="hljs-number">4</span>,
...    skipfooter=<span class="hljs-number">1</span>,
...    usecols=<span class="hljs-string">"A,C:W"</span>)
percapitaGDP.head()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">                                  Year	2000	...	2019	2020
0	Metropolitan areas	       ...	NaN 	...	NaN	NaN
1	AUS: Australia	..	       ...	...  	...	...	...
2	AUS01: Greater Sydney	       ...	... 	... 	45576	45152
3	AUS02: Greater Melbourne     ...	... 	... 	42299	40848
4	AUS03: Greater Brisbane      ...	... 	... 	42145	40741
[5 rows x 22 columns]
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.shape
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">(731, 22)
</code></pre>
    <div class="note-one">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">You may encounter a problem with <code class="inlineCode">read_excel</code> if the Excel file does not use utf-8 encoding. One way to resolve this is to save the Excel file as a CSV file, reopen it, and then save it with utf-8 encoding.</p>
    </div>
    <ol>
      <li class="numberedList" value="3">Use the <code class="inlineCode">info</code> method<a id="_idIndexMarker019"/> of the DataFrame to view data types and the <code class="inlineCode">non-null</code> count. Notice that all columns have the <code class="inlineCode">object</code> data type:
        <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.info()
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 731 entries, 0 to 730
Data columns (total 22 columns):
 #   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   Year    731 non-null    object
 1   2000    730 non-null    object
 2   2001    730 non-null    object
 3   2002    730 non-null    object
 4   2003    730 non-null    object
 5   2004    730 non-null    object
 6   2005    730 non-null    object
 7   2006    730 non-null    object
 8   2007    730 non-null    object
 9   2008    730 non-null    object
 10  2009    730 non-null    object
 11  2010    730 non-null    object
 12  2011    730 non-null    object
 13  2012    730 non-null    object
 14  2013    730 non-null    object
 15  2014    730 non-null    object
 16  2015    730 non-null    object
 17  2016    730 non-null    object
 18  2017    730 non-null    object
 19  2018    730 non-null    object
 20  2019    730 non-null    object
 21  2020    730 non-null    object
dtypes: object(22)
memory usage: 125.8+ KB
</code></pre>
      </li>
      <li class="numberedList">Rename the <code class="inlineCode">Year</code> column to <code class="inlineCode">metro</code>, and remove the leading spaces.</li>
    </ol>
    <p class="normal-one">Give an appropriate name to the metropolitan area column. There are extra spaces before the metro values in some cases. We can test for leading spaces with <code class="inlineCode">startswith(' ')</code> and then use <code class="inlineCode">any</code> to establish whether there are one or more occasions when the first character is blank. We can use <code class="inlineCode">endswith(' ')</code> to examine trailing<a id="_idIndexMarker020"/> spaces. We use strip to remove both leading and trailing spaces. When we test for trailing spaces again, we see that there are none:</p>
    <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.rename(columns={<span class="hljs-string">'Year'</span>:<span class="hljs-string">'metro'</span>}, inplace=<span class="hljs-literal">True</span>)
percapitaGDP.metro.<span class="hljs-built_in">str</span>.startswith(<span class="hljs-string">' '</span>).<span class="hljs-built_in">any</span>()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">True
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.metro.<span class="hljs-built_in">str</span>.endswith(<span class="hljs-string">' '</span>).<span class="hljs-built_in">any</span>()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">False
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.metro = percapitaGDP.metro.<span class="hljs-built_in">str</span>.strip()
percapitaGDP.metro.<span class="hljs-built_in">str</span>.startswith(<span class="hljs-string">' '</span>).<span class="hljs-built_in">any</span>()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">False
</code></pre>
    <ol>
      <li class="numberedList" value="5">Convert the data columns to numeric.</li>
    </ol>
    <p class="normal-one">Iterate over all of the GDP year columns (2000–2020) and convert the data type from <code class="inlineCode">object</code> to <code class="inlineCode">float</code>. Coerce the conversion even when there is character data – the <code class="inlineCode">..</code> in this example. We want character values in those columns to become <code class="inlineCode">missing</code>, which is what happens. Rename the year columns to better reflect the data in those columns:</p>
    <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> percapitaGDP.columns[<span class="hljs-number">1</span>:]:
...   percapitaGDP[col] = pd.to_numeric(percapitaGDP[col],
...     errors=<span class="hljs-string">'coerce'</span>)
...   percapitaGDP.rename(columns={col:<span class="hljs-string">'pcGDP'</span>+col},
...     inplace=<span class="hljs-literal">True</span>)
...
percapitaGDP.head()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">                      metro  pcGDP2000  pcGDP2001  ...  \
0        Metropolitan areas        NaN        NaN  ... 
1            AUS: Australia        NaN        NaN  ... 
2     AUS01: Greater Sydney        NaN      41091  ... 
3  AUS02: Greater Melbourne        NaN      40488  ... 
4   AUS03: Greater Brisbane        NaN      35276  ... 
   pcGDP2018  pcGDP2019  pcGDP2020
0        NaN        NaN        NaN
1        NaN        NaN        NaN
2      47171      45576      45152
3      43237      42299      40848
4      44328      42145      40741
[5 rows x 22 columns]
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.dtypes
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">metro          object
pcGDP2000      float64
pcGDP2001      float64
abbreviated to save space
pcGDP2019      float64
pcGDP2020      float64
dtype: object
</code></pre>
    <ol>
      <li class="numberedList" value="6">Use the <code class="inlineCode">describe</code> method<a id="_idIndexMarker021"/> to generate summary statistics for all numeric data in the DataFrame:
        <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.describe()
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">       pcGDP2000  pcGDP2001  pcGDP2002  ...  pcGDP2018  \
count        158        450        479  ...        692 
mean       33961      38874      39621  ...      41667 
std        15155      13194      13061  ...      17440 
min         2686       7805       7065  ...       5530 
25%        21523      30790      31064  ...      31322 
50%        35836      38078      39246  ...      41428 
75%        42804      46576      47874  ...      51130 
max        95221      96941      98929  ...     147760 
       pcGDP2019  pcGDP2020
count        596        425
mean       42709      39792
std        18893      19230
min         5698       5508
25%        29760      24142
50%        43505      41047
75%        53647      51130
max       146094     131082
[8 rows x 21 columns]
</code></pre>
      </li>
      <li class="numberedList">Remove rows where all of the per capita GDP values are missing.</li>
    </ol>
    <p class="normal-one">Use the <code class="inlineCode">subset</code> parameter<a id="_idIndexMarker022"/> of <code class="inlineCode">dropna</code> to inspect all columns, starting with the second column (it is zero-based) and going through to the last column. Use <code class="inlineCode">how</code> to specify that we want to drop rows only if all of the columns specified in <code class="inlineCode">subset</code> are missing. Use <code class="inlineCode">shape</code> to show the number of rows and columns in the resulting DataFrame:</p>
    <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.dropna(subset=percapitaGDP.columns[<span class="hljs-number">1</span>:], how=<span class="hljs-string">"all"</span>, inplace=<span class="hljs-literal">True</span>)
percapitaGDP.shape
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">(692, 22)
</code></pre>
    <ol>
      <li class="numberedList" value="8">Set the index for the DataFrame using the metropolitan area column.</li>
    </ol>
    <p class="normal-one">Confirm that there are 692 valid values for <code class="inlineCode">metro</code> and that there are 692 unique values, before setting the index:</p>
    <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.metro.count()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">692
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.metro.nunique()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">692
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">percapitaGDP.set_index(<span class="hljs-string">'metro'</span>, inplace=<span class="hljs-literal">True</span>)
percapitaGDP.head()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">                          pcGDP2000  pcGDP2001  ...  \
metro                                           ... 
AUS01: Greater Sydney           NaN      41091  ... 
AUS02: Greater Melbourne        NaN      40488  ... 
AUS03: Greater Brisbane         NaN      35276  ... 
AUS04: Greater Perth            NaN      43355  ... 
AUS05: Greater Adelaide         NaN      36081  ... 
                          pcGDP2019  pcGDP2020
metro                                         
AUS01: Greater Sydney         45576      45152
AUS02: Greater Melbourne      42299      40848
AUS03: Greater Brisbane       42145      40741
AUS04: Greater Perth          70970      78489
AUS05: Greater Adelaide       38314      39181
[5 rows x 21 columns]
percapitaGDP.loc['AUS02: Greater Melbourne']
pcGDP2000     NaN
pcGDP2001   40488
...
pcGDP2019   42299
pcGDP2020   40848
Name: AUS02: Greater Melbourne, dtype: float64
</code></pre>
    <p class="normal">We have now imported<a id="_idIndexMarker023"/> the Excel data into a pandas DataFrame and cleaned up some of the messiness in the spreadsheet.</p>
    <h2 id="_idParaDest-25" class="heading-2">How it works…</h2>
    <p class="normal">We mostly manage to get the data we want in <em class="italic">Step 2</em> by skipping rows and columns we do not want, but there are still a number of issues – <code class="inlineCode">read_excel</code> interprets all of the GDP data as character data, many rows are loaded with no useful data, and the column names do not represent the data well. In addition, the metropolitan area column might be useful as an index, but there are leading and trailing blanks, and there may be missing or duplicated values.</p>
    <p class="normal"><code class="inlineCode">read_excel</code> interprets <code class="inlineCode">Year</code> as the column name for the metropolitan area data because it looks for a header above the data for that Excel column and finds <code class="inlineCode">Year</code> there. We rename that column <code class="inlineCode">metro</code> in <em class="italic">Step 4</em>. We also use <code class="inlineCode">strip</code> to fix the problem with leading and trailing blanks. We could have just used <code class="inlineCode">lstrip</code> to remove leading blanks, or <code class="inlineCode">rstrip</code> if there had been trailing blanks. It is a good idea to assume that there might be leading or trailing blanks in any character data, cleaning that data shortly after the initial import.</p>
    <p class="normal">The spreadsheet authors used <code class="inlineCode">..</code> to represent missing data. Since this is actually valid character data, those columns get the object data type (that is how pandas treats columns with character or mixed data). We coerce a conversion to numeric type in <em class="italic">Step 5</em>. This also results in the original values of <code class="inlineCode">..</code> being replaced with <code class="inlineCode">NaN</code> (not a number), how pandas represents missing values for numbers. This is what we want.</p>
    <p class="normal">We can fix all of the per capita GDP columns with just a few lines because pandas makes it easy to iterate over the columns of a DataFrame. By specifying <code class="inlineCode">[1:]</code>, we iterate from the second column to the last column. We can then change those columns to numeric and rename them to something more appropriate.</p>
    <p class="normal">There are several reasons why it is a good idea to clean up the column headings for the annual GDP columns – it helps us to remember what the data actually is; if we merge it with other data by metropolitan area, we will not have to worry about conflicting variable names; and we can use attribute access to work with pandas Series based on those columns, which I will discuss in more detail in the <em class="italic">There’s more…</em> section of this recipe.</p>
    <p class="normal"><code class="inlineCode">describe</code> in <em class="italic">Step 6</em> shows us that fewer<a id="_idIndexMarker024"/> than 500 rows have valid data for per capita GDP for some years. When we drop all rows that have missing values for all per capita GDP columns in <em class="italic">step 7</em>, we end up with 692 rows in the DataFrame.</p>
    <h2 id="_idParaDest-26" class="heading-2">There’s more…</h2>
    <p class="normal">Once we have a pandas DataFrame, we have the ability to treat columns as more than just columns. We can use attribute access (such as <code class="inlineCode">percapitaGPA.metro</code>) or bracket notation (<code class="inlineCode">percapitaGPA['metro']</code>) to get the functionality of a pandas Series. Either method makes it possible to use Series string inspecting methods, such as <code class="inlineCode">str.startswith</code>, and counting methods, such as <code class="inlineCode">nunique</code>. Note that the original column names of <code class="inlineCode">20##</code> did not allow attribute access because they started with a number, so <code class="inlineCode">percapitaGDP.pcGDP2001.count()</code> works, but <code class="inlineCode">percapitaGDP.2001.count()</code> returns a syntax error because <code class="inlineCode">2001</code> is not a valid Python identifier (since it starts with a number).</p>
    <p class="normal">pandas is rich with features for string manipulation and for Series operations. We will try many of them out in subsequent recipes. This recipe showed those that I find most useful when importing Excel data.</p>
    <h2 id="_idParaDest-27" class="heading-2">See also</h2>
    <p class="normal">There are good reasons to consider reshaping this data. Instead of 21 columns of GDP per capita data for each metropolitan area, we should have 21 rows of data for each metropolitan area, with columns for year and GDP per capita. Recipes for reshaping data can be found in <em class="chapterRef">Chapter 11</em>, <em class="italic">Tidying and Reshaping Data</em>.</p>
    <h1 id="_idParaDest-28" class="heading-1">Importing data from SQL databases</h1>
    <p class="normal">In this recipe, we will<a id="_idIndexMarker025"/> use <code class="inlineCode">pymssql</code> and <code class="inlineCode">mysql apis</code> to read data from <strong class="keyWord">Microsoft SQL Server</strong> and <strong class="keyWord">MySQL</strong> (now owned by <strong class="keyWord">Oracle</strong>) databases, respectively. Data<a id="_idIndexMarker026"/> from sources<a id="_idIndexMarker027"/> such as these tends<a id="_idIndexMarker028"/> to be well structured, since it is designed to facilitate simultaneous transactions by members of organizations and those who interact with them. Each transaction is also likely related to some other organizational transaction.</p>
    <p class="normal">This means that although data tables from enterprise systems such as these are more reliably structured than data from CSV files and Excel files, their logic is less likely to be self-contained. You need to know how the data from one table relates to data from another table to understand its full meaning. These relationships need to be preserved, including the integrity of primary and foreign keys, when pulling data. Moreover, well-structured data tables are not necessarily uncomplicated data tables. There are often sophisticated coding schemes that determine data values, and these coding schemes can change over time. For example, codes for merchandise at a retail store chain might be different in 1998 than they are in 2024. Similarly, frequently there are codes for missing values, such as 99,999, that pandas will understand as valid values.</p>
    <p class="normal">Since much of this logic is business logic, and implemented in stored procedures or other applications, it is lost when pulled out of this larger system. Some of what is lost will eventually have to be reconstructed when preparing data for analysis. This almost always involves combining data from multiple tables, so it is important to preserve the ability to do that. However, it also may involve adding some of the coding logic back after loading the SQL table into a pandas DataFrame. We explore how to do that in this recipe.</p>
    <h2 id="_idParaDest-29" class="heading-2">Getting ready</h2>
    <p class="normal">This recipe assumes you have <code class="inlineCode">pymssql</code> and <code class="inlineCode">mysql apis</code> installed. If you do not, it is relatively straightforward to install them with <code class="inlineCode">pip</code>. From the Terminal, or <code class="inlineCode">powershell</code> (in Windows), enter <code class="inlineCode">pip install pymssql</code> or <code class="inlineCode">pip install mysql-connector-python</code>. We will work with data on educational attainment in this recipe.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Data note</strong></p>
      <p class="normal">The dataset used in this recipe is available for public use at <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip"><span class="url">https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip</span></a>.</p>
    </div>
    <h2 id="_idParaDest-30" class="heading-2">How to do it...</h2>
    <p class="normal">We import SQL Server<a id="_idIndexMarker029"/> and MySQL data tables into a pandas<a id="_idIndexMarker030"/> DataFrame, as follows:</p>
    <ol>
      <li class="numberedList" value="1">Import <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">pymssql</code>, and <code class="inlineCode">mysql</code>.</li>
    </ol>
    <p class="normal-one">This step assumes that you have installed <code class="inlineCode">pymssql</code> and <code class="inlineCode">mysql apis</code>:</p>
    <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pymssql
<span class="hljs-keyword">import</span> mysql.connector
</code></pre>
    <ol>
      <li class="numberedList" value="2">Use <code class="inlineCode">pymssql api</code> and <code class="inlineCode">read_sql</code> to retrieve and load data from a SQL Server instance.</li>
    </ol>
    <p class="normal-one">Select the columns we want from the SQL Server data, and use SQL aliases to improve column names (for example, <code class="inlineCode">fedu AS fathereducation</code>). Create a connection to the SQL Server data by passing database credentials to the <code class="inlineCode">pymssql</code> <code class="inlineCode">connect</code> function. Create a pandas DataFrame by passing the <code class="inlineCode">SELECT</code> statement and connection object to <code class="inlineCode">read_sql</code>. Use <code class="inlineCode">close</code> to return the connection to the pool on the server:</p>
    <pre class="programlisting code-one"><code class="hljs-code">sqlselect = <span class="hljs-string">"SELECT studentid, school, sex, age, famsize,\</span>
<span class="hljs-string">...   medu AS mothereducation, fedu AS fathereducation,\</span>
<span class="hljs-string">...   traveltime, studytime, failures, famrel, freetime,\</span>
<span class="hljs-string">...   goout, g1 AS gradeperiod1, g2 AS gradeperiod2,\</span>
<span class="hljs-string">...   g3 AS gradeperiod3 From studentmath"</span>
server = <span class="hljs-string">"pdcc.c9sqqzd5fulv.us-west-2.rds.amazonaws.com"</span>
user = <span class="hljs-string">"pdccuser"</span>
password = <span class="hljs-string">"</span><span class="hljs-string">pdccpass"</span>
database = <span class="hljs-string">"pdcctest"</span>
conn = pymssql.connect(server=server,
...   user=user, password=password, database=database)
studentmath = pd.read_sql(sqlselect,conn)
conn.close()
</code></pre>
    <div class="note-one">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">Although tools such as <code class="inlineCode">pymssql</code> make connecting to a SQL Server instance relatively straightforward, the syntax still might take a little time to get used to if it is unfamiliar. The previous step shows the parameter values you will typically need to pass to a connection object – the name of the server, the name of a user with credentials on the server, the password for that user, and the name of a SQL database on the server.</p>
    </div>
    <ol>
      <li class="numberedList" value="3">Check the data<a id="_idIndexMarker031"/> types and the<a id="_idIndexMarker032"/> first few rows:
        <pre class="programlisting code-one"><code class="hljs-code">studentmath.dtypes
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">studentid          object
school             object
sex                object
age                int64
famsize            object
mothereducation    int64
fathereducation    int64
traveltime         int64
studytime          int64
failures           int64
famrel             int64
freetime           int64
gout               int64
gradeperiod1       int64
gradeperiod2       int64
gradeperiod3       int64
dtype: object
</code></pre>
        <pre class="programlisting code-one"><code class="hljs-code">studentmath.head()
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">    studentid    school  ...      gradeperiod2    gradeperiod3
0	001	    GP      ...	6	        6
1	002	    GP      ...	5	        6
2	003	    GP      ...	8	        10
3	004	    GP      ...	14	        15
4	005	    GP      ...	10	        10
[5 rows x 16 columns]
</code></pre>
      </li>
      <li class="numberedList">Connecting to a MySQL server is not very different from connecting to a SQL Server instance. We can use the <code class="inlineCode">connect</code> method of the <code class="inlineCode">mysql</code> connector to do that and then use <code class="inlineCode">read_sql</code> to load the data.</li>
    </ol>
    <p class="normal-one">Create a connection<a id="_idIndexMarker033"/> to the <code class="inlineCode">mysql</code> data, pass that connection<a id="_idIndexMarker034"/> to <code class="inlineCode">read_sql</code> to retrieve the data, and load it into a pandas DataFrame (the same data file on student math scores was uploaded to SQL Server and MySQL, so we can use the same SQL <code class="inlineCode">SELECT</code> statement we used in the previous step):</p>
    <pre class="programlisting code-one"><code class="hljs-code">host = <span class="hljs-string">"pdccmysql.c9sqqzd5fulv.us-west-2.rds.amazonaws.com"</span>
user = <span class="hljs-string">"</span><span class="hljs-string">pdccuser"</span>
password = <span class="hljs-string">"pdccpass"</span>
database = <span class="hljs-string">"pdccschema"</span>
connmysql = mysql.connector.connect(host=host, \
...   database=database,user=user,password=password)
studentmath = pd.read_sql(sqlselect,connmysql)
connmysql.close()
</code></pre>
    <ol>
      <li class="numberedList" value="5">Rearrange the columns, set an index, and check for missing values.</li>
    </ol>
    <p class="normal-one">Move the grade data to the left of the DataFrame, just after <code class="inlineCode">studentid</code>. Also, move the <code class="inlineCode">freetime</code> column to the right after <code class="inlineCode">traveltime</code> and <code class="inlineCode">studytime</code>. Confirm that each row has an ID and that the IDs are unique, and set <code class="inlineCode">studentid</code> as the index:</p>
    <pre class="programlisting code-one"><code class="hljs-code">newcolorder = [<span class="hljs-string">'studentid'</span>, <span class="hljs-string">'gradeperiod1'</span>,
...   <span class="hljs-string">'gradeperiod2'</span>,<span class="hljs-string">'gradeperiod3'</span>, <span class="hljs-string">'school'</span>,
...   <span class="hljs-string">'sex'</span>, <span class="hljs-string">'age'</span>, <span class="hljs-string">'famsize'</span>,<span class="hljs-string">'mothereducation'</span>,
...   <span class="hljs-string">'fathereducation'</span>, <span class="hljs-string">'traveltime'</span>,
...   <span class="hljs-string">'studytime'</span>, <span class="hljs-string">'freetime'</span>, <span class="hljs-string">'failures'</span>,
...   <span class="hljs-string">'famrel'</span>,<span class="hljs-string">'goout'</span>]
studentmath = studentmath[newcolorder]
studentmath.studentid.count()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">395
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">studentmath.studentid.nunique()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">395
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">studentmath.set_index(<span class="hljs-string">'studentid'</span>, inplace=<span class="hljs-literal">True</span>)
</code></pre>
    <ol>
      <li class="numberedList" value="6">Use the DataFrame’s <code class="inlineCode">count</code> function<a id="_idIndexMarker035"/> to check for missing<a id="_idIndexMarker036"/> values:
        <pre class="programlisting code-one"><code class="hljs-code">studentmath.count()
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">gradeperiod1		395
gradeperiod2		395
gradeperiod3		395
school		395
sex			395
age			395
famsize		395
mothereducation	395
fathereducation	395
traveltime		395
studytime		395
freetime		395
failures		395
famrel		395
goout			395
dtype: int64
</code></pre>
      </li>
      <li class="numberedList">Replace coded data values with more informative values.</li>
    </ol>
    <p class="normal-one">Create a dictionary with the replacement values for the columns, and then use <code class="inlineCode">replace</code> to set those values:</p>
    <pre class="programlisting code-one"><code class="hljs-code">setvalues= \
...   {<span class="hljs-string">"famrel"</span>:{<span class="hljs-number">1</span>:<span class="hljs-string">"1:very bad"</span>,<span class="hljs-number">2</span>:<span class="hljs-string">"2:bad"</span>,
...     <span class="hljs-number">3</span>:<span class="hljs-string">"3:neutral"</span>,<span class="hljs-number">4</span>:<span class="hljs-string">"4:good"</span>,<span class="hljs-number">5</span>:<span class="hljs-string">"5:excellent"</span>},
...   <span class="hljs-string">"freetime"</span>:{<span class="hljs-number">1</span>:<span class="hljs-string">"1:very low"</span>,<span class="hljs-number">2</span>:<span class="hljs-string">"2:low"</span>,
...     <span class="hljs-number">3</span>:<span class="hljs-string">"3:neutral"</span>,<span class="hljs-number">4</span>:<span class="hljs-string">"4:high"</span>,<span class="hljs-number">5</span>:<span class="hljs-string">"5:very high"</span>},
...   <span class="hljs-string">"goout"</span>:{<span class="hljs-number">1</span>:<span class="hljs-string">"1:very low"</span>,<span class="hljs-number">2</span>:<span class="hljs-string">"2:low"</span>,<span class="hljs-number">3</span>:<span class="hljs-string">"3:neutral"</span>,
...     <span class="hljs-number">4</span>:<span class="hljs-string">"4:high"</span>,<span class="hljs-number">5</span>:<span class="hljs-string">"5:very high"</span>},
...   <span class="hljs-string">"mothereducation"</span>:{<span class="hljs-number">0</span>:np.nan,<span class="hljs-number">1</span>:<span class="hljs-string">"1:k-4"</span>,<span class="hljs-number">2</span>:<span class="hljs-string">"2:5-9"</span>,
...     <span class="hljs-number">3</span>:<span class="hljs-string">"3:secondary ed"</span>,<span class="hljs-number">4</span>:<span class="hljs-string">"4:higher ed"</span>},
...   <span class="hljs-string">"fathereducation"</span>:{<span class="hljs-number">0</span>:np.nan,<span class="hljs-number">1</span>:<span class="hljs-string">"1:k-4"</span>,<span class="hljs-number">2</span>:<span class="hljs-string">"2:5-9"</span>,
...     <span class="hljs-number">3</span>:<span class="hljs-string">"3:secondary ed"</span>,<span class="hljs-number">4</span>:<span class="hljs-string">"4:higher ed"</span>}}
studentmath.replace(setvalues, inplace=<span class="hljs-literal">True</span>)
</code></pre>
    <ol>
      <li class="numberedList" value="8">Change the type for columns<a id="_idIndexMarker037"/> with the changed data<a id="_idIndexMarker038"/> to <code class="inlineCode">category</code>.</li>
    </ol>
    <p class="normal-one">Check any changes in memory usage:</p>
    <pre class="programlisting code-one"><code class="hljs-code">setvalueskeys = [k <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> setvalues]
studentmath[setvalueskeys].memory_usage(index=<span class="hljs-literal">False</span>)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">famrel		3160
freetime		3160
goout			3160
mothereducation	3160
fathereducation	3160
dtype: int64
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> studentmath[setvalueskeys].columns:
...   studentmath[col] = studentmath[col]. \
...     astype(<span class="hljs-string">'category'</span>)
...
studentmath[setvalueskeys].memory_usage(index=<span class="hljs-literal">False</span>)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">famrel		607
freetime		607
goout			607
mothereducation	599
fathereducation	599
dtype: int64
</code></pre>
    <ol>
      <li class="numberedList" value="9">Calculate percentages for values in the <code class="inlineCode">famrel</code> column.</li>
    </ol>
    <p class="normal-one">Run <code class="inlineCode">value_counts</code>, and set <code class="inlineCode">normalize</code> to <code class="inlineCode">True</code> to generate percentages:</p>
    <pre class="programlisting code-one"><code class="hljs-code">studentmath[<span class="hljs-string">'famrel'</span>].value_counts(sort=<span class="hljs-literal">False</span>, normalize=<span class="hljs-literal">True</span>)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">1:very bad	0.02
2:bad		0.05
3:neutral	0.17
4:good	0.49
5:excellent	0.27
Name: famrel, dtype: float64
</code></pre>
    <ol>
      <li class="numberedList" value="10">Use <code class="inlineCode">apply</code> to calculate percentages for multiple columns:
        <pre class="programlisting code-one"><code class="hljs-code">studentmath[[<span class="hljs-string">'freetime'</span>,<span class="hljs-string">'goout'</span>]].\
...   apply(pd.Series.value_counts, sort=<span class="hljs-literal">False</span>,
...   normalize=<span class="hljs-literal">True</span>)
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">             freetime   goout
1:very low	0.05	    0.06
2:low		0.16	    0.26
3:neutral	0.40	    0.33
4:high	0.29	    0.22
5:very high	0.10	    0.13
</code></pre>
        <pre class="programlisting code-one"><code class="hljs-code">studentmath[[<span class="hljs-string">'mothereducation'</span>,<span class="hljs-string">'fathereducation'</span>]].\
...   apply(pd.Series.value_counts, sort=<span class="hljs-literal">False</span>,
...   normalize=<span class="hljs-literal">True</span>)
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">			mothereducation	fathereducation
1:k-4			0.15			0.21
2:5-9			0.26			0.29
3:secondary ed	0.25			0.25
4:higher ed		0.33			0.24
</code></pre>
      </li>
    </ol>
    <p class="normal">The preceding steps retrieved<a id="_idIndexMarker039"/> a data table from a SQL database, loaded<a id="_idIndexMarker040"/> that data into pandas, and did some initial data checking and cleaning.</p>
    <h2 id="_idParaDest-31" class="heading-2">How it works…</h2>
    <p class="normal">Since data from enterprise systems is typically better structured than CSV or Excel files, we do not need to do things such as skip rows or deal with different logical data types in a column. However, some massaging is still usually required before we can begin exploratory analysis. There are often more columns than we need, and some column names are not intuitive or not ordered in the best way for analysis. The meaningfulness of many data values is not stored in the data table to avoid entry errors and save on storage space. For example, <code class="inlineCode">3</code> is stored for mother’s education rather than secondary education. It is a good idea to reconstruct that coding as early in the cleaning process as possible.</p>
    <p class="normal">To pull data from a SQL database server, we need a connection object to authenticate us on the server, as well as a SQL select string. These can be passed to <code class="inlineCode">read_sql</code> to retrieve the data and load it into a pandas DataFrame. I usually use the SQL <code class="inlineCode">SELECT</code> statement to do a bit of cleanup of column names at this point. I sometimes also reorder columns, but I did that later in this recipe.</p>
    <p class="normal">We set the index in <em class="italic">Step 5</em>, first confirming<a id="_idIndexMarker041"/> that every row has a value<a id="_idIndexMarker042"/> for <code class="inlineCode">studentid</code> and that it is unique. This is often more important when working with enterprise data because we will almost always need to merge the retrieved data with other data files on the system. Although an index is not required for this merging, the discipline of setting one prepares us for the tricky business of merging data further down the road. It will also likely improve the speed of the merge.</p>
    <p class="normal">We use the DataFrame’s <code class="inlineCode">count</code> function to check for missing values and that there are no missing values – for non-missing values, the count is 395 (the number of rows) for every column. This is almost too good to be true. There may be values that are logically missing – that is, valid numbers that nonetheless connote missing values, such as <code class="inlineCode">-1</code>, <code class="inlineCode">0</code>, <code class="inlineCode">9</code>, or <code class="inlineCode">99</code>. We address this possibility in the next step.</p>
    <p class="normal"><em class="italic">Step 7</em> demonstrates a useful technique for replacing data values for multiple columns. We create a dictionary to map original values to new values for each column and then run it using <code class="inlineCode">replace</code>. To reduce the amount of storage space taken up by the new verbose values, we convert the data type of those columns to <code class="inlineCode">category</code>. We do this by generating a list of the keys of our <code class="inlineCode">setvalues</code> dictionary – <code class="inlineCode">setvalueskeys = [k for k in setvalues]</code> generates [<code class="inlineCode">famrel</code>, <code class="inlineCode">freetime</code>, <code class="inlineCode">goout</code>, <code class="inlineCode">mothereducation</code>, and <code class="inlineCode">fathereducation</code>]. We then iterate over those five columns and use the <code class="inlineCode">astype</code> method to change the data type to <code class="inlineCode">category</code>. Notice that the memory usage for those columns is reduced substantially.</p>
    <p class="normal">Finally, we check the assignment of new values by using <code class="inlineCode">value_counts</code> to view relative frequencies. We use <code class="inlineCode">apply</code> because we want to run <code class="inlineCode">value_counts</code> on multiple columns. To prevent <code class="inlineCode">value_counts</code> sorting by frequency, we set sort to <code class="inlineCode">False</code>.</p>
    <p class="normal">The DataFrame <code class="inlineCode">replace</code> method is also a handy tool for dealing with logical missing values that will not be recognized as missing when retrieved by <code class="inlineCode">read_sql</code>. The <code class="inlineCode">0</code> values for <code class="inlineCode">mothereducation</code> and <code class="inlineCode">fathereducation</code> seem to fall into that category. We fix this problem in the <code class="inlineCode">setvalues</code> dictionary by indicating that the <code class="inlineCode">0</code> values for <code class="inlineCode">mothereducation</code> and <code class="inlineCode">fathereducation</code> should be replaced with <code class="inlineCode">NaN</code>. It is important to address these kinds of missing values shortly after the initial import because they are not always obvious and can significantly impact all subsequent work.</p>
    <p class="normal">Users of packages such as <em class="italic">SPPS</em>, <em class="italic">SAS</em>, and <em class="italic">R</em> will notice the difference between this approach and value labels in SPSS and R, as well as the <code class="inlineCode">proc</code> format in SAS. In pandas, we need to change the actual data to get more informative values. However, we reduce how much data is actually stored<a id="_idIndexMarker043"/> by giving the column a <code class="inlineCode">category</code> data<a id="_idIndexMarker044"/> type. This is similar to <code class="inlineCode">factors</code> in R.</p>
    <h2 id="_idParaDest-32" class="heading-2">There’s more…</h2>
    <p class="normal">I moved the grade data to near the beginning of the DataFrame. I find it helpful to have potential target or dependent variables in the leftmost columns, keeping them at the forefront of your mind. It is also helpful to keep similar columns together. In this example, personal demographic variables (sex and age) are next to one another, as are family variables (<code class="inlineCode">mothereducation</code> and <code class="inlineCode">fathereducation</code>), and how students spend their time (<code class="inlineCode">traveltime</code>, <code class="inlineCode">studytime</code>, and <code class="inlineCode">freetime</code>).</p>
    <p class="normal">You could have used <code class="inlineCode">map</code> instead of <code class="inlineCode">replace</code> in <em class="italic">Step 7</em>. Prior to version 19.2 of pandas, <code class="inlineCode">map</code> was significantly more efficient. Since then, the difference in efficiency has been much smaller. If you are working with a very large dataset, the difference may still be enough to consider using <code class="inlineCode">map</code>.</p>
    <h2 id="_idParaDest-33" class="heading-2">See also</h2>
    <p class="normal">The recipes in <em class="chapterRef">Chapter 10</em>, <em class="italic">Addressing Data Issues When Combining DataFrames</em>, go into detail on merging data. We will take a closer look at bivariate and multivariate relationships between variables in <em class="chapterRef">Chapter 4</em>, <em class="italic">Identifying Outliers in Subsets of Data</em>. We will demonstrate how to use some of these same approaches in packages such as SPSS, SAS, and R in subsequent recipes in this chapter.</p>
    <h1 id="_idParaDest-34" class="heading-1">Importing SPSS, Stata, and SAS data</h1>
    <p class="normal">We will use <code class="inlineCode">pyreadstat</code> to read<a id="_idIndexMarker045"/> data from<a id="_idIndexMarker046"/> three popular<a id="_idIndexMarker047"/> statistical packages into pandas. The key advantage of <code class="inlineCode">pyreadstat</code> is that it allows data analysts to import data from these packages without losing metadata, such as variable and value labels.</p>
    <p class="normal">The SPSS, Stata, and SAS data <a id="_idIndexMarker048"/>files we receive often<a id="_idIndexMarker049"/> come to us with<a id="_idIndexMarker050"/> the data issues of CSV and Excel files and SQL databases having been resolved. We do not typically have the invalid column names, changes in data types, and unclear missing values that we can get with CSV or Excel files, nor do we usually get the detachment of data from business logic, such as the meaning of data codes, that we often get with SQL data. When someone or some organization shares a data file from one of these packages with us, they have often added variable labels and value labels for categorical data. For example, a hypothetical data column called <code class="inlineCode">presentsat</code> has the <code class="inlineCode">overall satisfaction with presentation</code> variable label and <code class="inlineCode">1</code>–<code class="inlineCode">5</code> value labels, with <code class="inlineCode">1</code> being not at all satisfied and <code class="inlineCode">5</code> being highly satisfied.</p>
    <p class="normal">The challenge is retaining that metadata when importing data from those systems into pandas. There is no precise equivalent to variable and value labels in pandas, and built-in tools for importing SAS, Stata, and SAS data lose the metadata. In this recipe, we will use <code class="inlineCode">pyreadstat</code> to load variable and value label information and use a couple of techniques to represent that information in pandas.</p>
    <h2 id="_idParaDest-35" class="heading-2">Getting ready</h2>
    <p class="normal">This recipe assumes you have installed the <code class="inlineCode">pyreadstat</code> package. If it is not installed, you can install it with <code class="inlineCode">pip</code>. From the Terminal, or Powershell (in Windows), enter <code class="inlineCode">pip install pyreadstat</code>. You will need the SPSS, Stata, and SAS data files for this recipe to run the code.</p>
    <p class="normal">We will work with data<a id="_idIndexMarker051"/> from the United States <strong class="keyWord">National Longitudinal Surveys</strong> (<strong class="keyWord">NLS</strong>) of Youth.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Data note</strong></p>
      <p class="normal">The NLS of Youth<a id="_idIndexMarker052"/> is conducted by the United States Bureau of Labor Statistics. This survey started with a cohort of individuals in 1997. Each survey respondent was high school age when they first completed the survey, having been born between 1980 and 1985. There were annual follow-up surveys each year through 2023. For this recipe, I pulled 42 variables on grades, employment, income, and attitudes toward government, from the hundreds of data items on the survey. Separate files for SPSS, Stata, and SAS can be downloaded from the repository.</p>
      <p class="normal">The original NLS data<a id="_idIndexMarker053"/> can be downloaded from <a href="https://www.nlsinfo.org/investigator/pages/search"><span class="url">https://www.nlsinfo.org/investigator/pages/search</span></a>, along with code for creating SPSS, Stata, or SAS files from the ASCII data files included in the download.</p>
    </div>
    <h2 id="_idParaDest-36" class="heading-2">How to do it...</h2>
    <p class="normal">We will import data from SPSS, Stata, and SAS, retaining metadata such as value labels:</p>
    <ol>
      <li class="numberedList" value="1">Import <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, and <code class="inlineCode">pyreadstat</code>.</li>
    </ol>
    <p class="normal-one">This step assumes that you have installed <code class="inlineCode">pyreadstat</code>:</p>
    <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pyreadstat
</code></pre>
    <ol>
      <li class="numberedList" value="2">Retrieve the SPSS data.</li>
    </ol>
    <p class="normal-one">Pass a path and filename<a id="_idIndexMarker054"/> to the <code class="inlineCode">read_sav</code> method of <code class="inlineCode">pyreadstat</code>. Display<a id="_idIndexMarker055"/> the first few rows<a id="_idIndexMarker056"/> and a frequency distribution. Note that the column names and value labels are non-descriptive, and that <code class="inlineCode">read_sav</code> returns both a pandas DataFrame and a <code class="inlineCode">meta</code> object:</p>
    <pre class="programlisting code-one"><code class="hljs-code">nls97spss, metaspss = pyreadstat.read_sav(<span class="hljs-string">'data/nls97.sav'</span>)
nls97spss.dtypes
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">R0000100	float64
R0536300	float64
R0536401	float64
...
U2962900	float64
U2963000	float64
Z9063900	float64
dtype: object
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">nls97spss.head()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">   R0000100  R0536300  ...  U2963000  Z9063900
0	1	2         ...  nan       52
1	2	1         ...  6         0
2	3	2         ...  6         0
3	4	2         ...  6         4
4	5	1         ...  5         12
[5 rows x 42 columns]
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">nls97spss[<span class="hljs-string">'R0536300'</span>].value_counts(normalize=<span class="hljs-literal">True</span>)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">1.00	0.51
2.00	0.49
Name: R0536300, dtype: float64
</code></pre>
    <ol>
      <li class="numberedList" value="3">Grab the metadata to improve column labels and value labels.</li>
    </ol>
    <p class="normal-one">The <code class="inlineCode">metaspss</code> object<a id="_idIndexMarker057"/> created when we called <code class="inlineCode">read_sav</code> has the column<a id="_idIndexMarker058"/> labels and the value<a id="_idIndexMarker059"/> labels from the SPSS file. Use the <code class="inlineCode">variable_value_labels</code> dictionary to map values to value labels for one column (<code class="inlineCode">R0536300</code>). (This does not change the data. It only improves our display when we run <code class="inlineCode">value_counts</code>.) Use the <code class="inlineCode">set_value_labels</code> method to actually apply the value labels to the DataFrame:</p>
    <pre class="programlisting code-one"><code class="hljs-code">metaspss.variable_value_labels[<span class="hljs-string">'R0536300'</span>]
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">{0.0: 'No Information', 1.0: 'Male', 2.0: 'Female'}
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">nls97spss[<span class="hljs-string">'R0536300'</span>].\
...   <span class="hljs-built_in">map</span>(metaspss.variable_value_labels[<span class="hljs-string">'R0536300'</span>]).\
...   value_counts(normalize=<span class="hljs-literal">True</span>)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">Male		0.51
Female	0.49
Name: R0536300, dtype: float64
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">nls97spss = pyreadstat.set_value_labels(nls97spss, metaspss, formats_as_category=<span class="hljs-literal">True</span>)
</code></pre>
    <ol>
      <li class="numberedList" value="4">Use column labels in the metadata to rename the columns.</li>
    </ol>
    <p class="normal-one">To use the column labels from <code class="inlineCode">metaspss</code> in our DataFrame, we can simply assign the column labels in <code class="inlineCode">metaspss</code> to our DataFrame’s column names. Clean up the column names a bit by changing them to lowercase, changing spaces to underscores, and removing all remaining non-alphanumeric characters:</p>
    <pre class="programlisting code-one"><code class="hljs-code">nls97spss.columns = metaspss.column_labels
nls97spss[<span class="hljs-string">'KEY!SEX (SYMBOL) 1997'</span>].value_counts(normalize=<span class="hljs-literal">True</span>)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">Male		0.51
Female	0.49
Name: KEY!SEX (SYMBOL) 1997, dtype: float64
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">nls97spss.dtypes
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">PUBID - YTH ID CODE 		1997	float64
KEY!SEX (SYMBOL) 			1997	category
KEY!BDATE M/Y (SYMBOL)		1997	float64
KEY!BDATE M/Y (SYMBOL) 		1997	float64
CV_SAMPLE_TYPE 			1997	category
KEY!RACE_ETHNICITY (SYMBOL) 	1997	category
"... abbreviated to save space"
HRS/WK R WATCHES TELEVISION 	2017	category
HRS/NIGHT R SLEEPS 		 	2017	float64
CVC_WKSWK_YR_ALL L99			float64
dtype: object
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">nls97spss.columns = nls97spss.columns.\
...     <span class="hljs-built_in">str</span>.lower().\
...     <span class="hljs-built_in">str</span>.replace(<span class="hljs-string">' '</span>,<span class="hljs-string">'_'</span>).\
...     <span class="hljs-built_in">str</span>.replace(<span class="hljs-string">'[^a-z0-9_]'</span>, <span class="hljs-string">''</span>, regex=<span class="hljs-literal">True</span>)
nls97spss.set_index(<span class="hljs-string">'pubid__yth_id_code_1997'</span>, inplace=<span class="hljs-literal">True</span>)
</code></pre>
    <ol>
      <li class="numberedList" value="5">Simplify the process<a id="_idIndexMarker060"/> by applying the value<a id="_idIndexMarker061"/> labels from<a id="_idIndexMarker062"/> the beginning.</li>
    </ol>
    <p class="normal-one">The data values can actually be applied in the initial call to <code class="inlineCode">read_sav</code> by setting <code class="inlineCode">apply_value_formats</code> to <code class="inlineCode">True</code>. This eliminates the need to call the <code class="inlineCode">set_value_labels</code> function later:</p>
    <pre class="programlisting code-one"><code class="hljs-code">nls97spss, metaspss = pyreadstat.read_sav(<span class="hljs-string">'data/nls97.sav'</span>, apply_value_formats=<span class="hljs-literal">True</span>, formats_as_category=<span class="hljs-literal">True</span>)
nls97spss.columns = metaspss.column_labels
nls97spss.columns = nls97spss.columns.\
...   <span class="hljs-built_in">str</span>.lower().\
...   <span class="hljs-built_in">str</span>.replace(<span class="hljs-string">' '</span>,<span class="hljs-string">'_'</span>).\
...   <span class="hljs-built_in">str</span>.replace(<span class="hljs-string">'[^a-z0-9_]'</span>, <span class="hljs-string">''</span>, regex=<span class="hljs-literal">True</span>)
</code></pre>
    <ol>
      <li class="numberedList" value="6">Show the columns and a few rows:
        <pre class="programlisting code-one"><code class="hljs-code">nls97spss.dtypes
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">pubid__yth_id_code_1997	float64
keysex_symbol_1997		category
keybdate_my_symbol_1997	float64
keybdate_my_symbol_1997	float64
hrsnight_r_sleeps_2017	float64
cvc_wkswk_yr_all_l99	float64
dtype: object
</code></pre>
        <pre class="programlisting code-one"><code class="hljs-code">nls97spss.head()
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">   pubid__yth_id_code_1997 keysex_symbol_1997  ...  \
0	1	Female  ... 
1	2	Male  ... 
2	3	Female  ... 
3	4	Female  ... 
4	5	Male  ... 
   hrsnight_r_sleeps_2017  cvc_wkswk_yr_all_l99
0	nan	52
1	6	0
2	6	0
3	6	4
4	5	12
[5 rows x 42 columns]
</code></pre>
      </li>
      <li class="numberedList">Run frequencies<a id="_idIndexMarker063"/> on one of the<a id="_idIndexMarker064"/> columns, and set<a id="_idIndexMarker065"/> the index:
        <pre class="programlisting code-one"><code class="hljs-code">nls97spss.govt_responsibility__provide_jobs_2006.\
...   value_counts(sort=<span class="hljs-literal">False</span>)
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">Definitely should be	454
Definitely should not be	300
Probably should be		617
Probably should not be	462
Name: govt_responsibility__provide_jobs_2006, dtype: int64
</code></pre>
        <pre class="programlisting code-one"><code class="hljs-code">nls97spss.set_index(<span class="hljs-string">'pubid__yth_id_code_1997'</span>, inplace=<span class="hljs-literal">True</span>)
</code></pre>
      </li>
      <li class="numberedList">That demonstrated how to convert data from SPSS. Let’s try that with Stata data.</li>
      <li class="numberedList">Import the Stata data, apply value labels, and improve the column headings.</li>
    </ol>
    <p class="normal-one">Use the same methods for the Stata data that we used for the SPSS data:</p>
    <pre class="programlisting code-one"><code class="hljs-code">nls97stata, metastata = pyreadstat.read_dta(<span class="hljs-string">'data/nls97.dta'</span>, apply_value_formats=<span class="hljs-literal">True</span>, formats_as_category=<span class="hljs-literal">True</span>)
nls97stata.columns = metastata.column_labels
nls97stata.columns = nls97stata.columns.\
...     <span class="hljs-built_in">str</span>.lower().\
...     <span class="hljs-built_in">str</span>.replace(<span class="hljs-string">'</span><span class="hljs-string"> '</span>,<span class="hljs-string">'_'</span>).\
...     <span class="hljs-built_in">str</span>.replace(<span class="hljs-string">'[^a-z0-9_]'</span>, <span class="hljs-string">''</span>, regex=<span class="hljs-literal">True</span>)
nls97stata.dtypes
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">pubid__yth_id_code_1997	float64
keysex_symbol_1997		category
keybdate_my_symbol_1997	float64
keybdate_my_symbol_1997	float64
hrsnight_r_sleeps_2017	float64
cvc_wkswk_yr_all_l99	float64
dtype: object
</code></pre>
    <ol>
      <li class="numberedList" value="10">View a few<a id="_idIndexMarker066"/> rows of<a id="_idIndexMarker067"/> the data<a id="_idIndexMarker068"/> and run frequencies:
        <pre class="programlisting code-one"><code class="hljs-code">nls97stata.head()
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">   pubid__yth_id_code_1997    keysex_symbol_1997  ...  \
0                        1                Female  ... 
1                        2                  Male  ... 
2                        3                Female  ... 
3                        4                Female  ... 
4                        5                  Male  ... 
   hrsnight_r_sleeps_2017    cvc_wkswk_yr_all_l99
0                      -5                      52
1                       6                       0
2                       6                       0
3                       6                       4
4                       5                      12
[5 rows x 42 columns]
</code></pre>
        <pre class="programlisting code-one"><code class="hljs-code">nls97stata.govt_responsibility__provide_jobs_2006.\
...   value_counts(sort=<span class="hljs-literal">False</span>)
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">-5.0	1425
-4.0	5665
-2.0	56
-1.0	5
Definitely should be	454
Definitely should not be	300
Probably should be		617
Probably should not be	462
Name: govt_responsibility__provide_jobs_2006, dtype: int64
</code></pre>
      </li>
      <li class="numberedList">Fix the logical missing<a id="_idIndexMarker069"/> values that show<a id="_idIndexMarker070"/> up with the Stata data<a id="_idIndexMarker071"/> and set an index. We can use the <code class="inlineCode">replace</code> method to set any value that is between <code class="inlineCode">–9</code> and <code class="inlineCode">–1</code> in any column to missing:
        <pre class="programlisting code-one"><code class="hljs-code">nls97stata.<span class="hljs-built_in">min</span>(numeric_only=<span class="hljs-literal">True</span>)
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">pubid__yth_id_code_1997          1
keybdate_my_symbol_1997          1
keybdate_my_symbol_1997      1,980
trans_sat_verbal_hstr           -4
trans_sat_math_hstr             -4
trans_crd_gpa_overall_hstr      -9
trans_crd_gpa_eng_hstr          -9
trans_crd_gpa_math_hstr         -9
trans_crd_gpa_lp_sci_hstr       -9
cv_ba_credits_l1_2011           -5
cv_bio_child_hh_2017            -5
cv_bio_child_nr_2017            -5
hrsnight_r_sleeps_2017          -5
cvc_wkswk_yr_all_l99            -4
dtype: float64
</code></pre>
        <pre class="programlisting code-one"><code class="hljs-code">nls97stata.replace(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(-<span class="hljs-number">9</span>,<span class="hljs-number">0</span>)), np.nan, inplace=<span class="hljs-literal">True</span>)
nls97stata.<span class="hljs-built_in">min</span>(numeric_only=<span class="hljs-literal">True</span>)
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">pubid__yth_id_code_1997          1
keybdate_my_symbol_1997          1
keybdate_my_symbol_1997      1,980
trans_sat_verbal_hstr           14
trans_sat_math_hstr              7
trans_crd_gpa_overall_hstr      10
trans_crd_gpa_eng_hstr           0
trans_crd_gpa_math_hstr          0
trans_crd_gpa_lp_sci_hstr        0
cv_ba_credits_l1_2011            0
cv_bio_child_hh_2017             0
cv_bio_child_nr_2017             0
hrsnight_r_sleeps_2017           0
cvc_wkswk_yr_all_l99             0
dtype: float64
</code></pre>
        <pre class="programlisting code-one"><code class="hljs-code">nls97stata.set_index(<span class="hljs-string">'pubid__yth_id_code_1997'</span>, inplace=<span class="hljs-literal">True</span>)
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The process is fairly similar when working with SAS data files, as the next few steps illustrate.</p>
    <ol>
      <li class="numberedList" value="12">Retrieve the SAS data, using the SAS catalog file for value labels:</li>
    </ol>
    <p class="normal-one">The data values<a id="_idIndexMarker072"/> for SAS<a id="_idIndexMarker073"/> are stored<a id="_idIndexMarker074"/> in a catalog file. Setting the catalog file path and filename retrieves the value labels and applies them:</p>
    <pre class="programlisting code-one"><code class="hljs-code">nls97sas, metasas = pyreadstat.read_sas7bdat(<span class="hljs-string">'data/nls97.sas7bdat'</span>, catalog_file=<span class="hljs-string">'data/nlsformats3.sas7bcat'</span>, formats_as_category=<span class="hljs-literal">True</span>)
nls97sas.columns = metasas.column_labels
nls97sas.columns = nls97sas.columns.\
...     <span class="hljs-built_in">str</span>.lower().\
...     <span class="hljs-built_in">str</span>.replace(<span class="hljs-string">' '</span>,<span class="hljs-string">'</span><span class="hljs-string">_'</span>).\
...     <span class="hljs-built_in">str</span>.replace(<span class="hljs-string">'[^a-z0-9_]'</span>, <span class="hljs-string">''</span>, regex=<span class="hljs-title">True</span>)
nls97sas.head()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">   pubid__yth_id_code_1997   keysex_symbol_1997    ...  \
0			     1		       Female    ... 
1			     2		         Male    ... 
2			     3		       Female    ... 
3			     4		       Female    ... 
4			     5		         Male    ... 
   hrsnight_r_sleeps_2017  cvc_wkswk_yr_all_l99
0			  nan			   52
1			    6			    0
2			    6			    0
3			    6			    4
4			    5			   12
[5 rows x 42 columns]
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">nls97sas.keysex_symbol_1997.value_counts()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">Male		4599
Female	4385
Name: keysex_symbol_1997, dtype: int64
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">nls97sas.set_index(<span class="hljs-string">'pubid__yth_id_code_1997'</span>, inplace=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">This demonstrates how to import SPSS, SAS, and Stata data without losing important metadata.</p>
    <h2 id="_idParaDest-37" class="heading-2">How it works...</h2>
    <p class="normal">The <code class="inlineCode">read_sav</code>, <code class="inlineCode">read_dta</code>, and <code class="inlineCode">read_sas7bdat</code> methods<a id="_idIndexMarker075"/> of <code class="inlineCode">Pyreadstat</code>, for SPSS, Stata, and SAS<a id="_idIndexMarker076"/> data files, respectively, work in a similar manner. Value labels<a id="_idIndexMarker077"/> can be applied when reading in the data by setting <code class="inlineCode">apply_value_formats</code> to <code class="inlineCode">True</code> for SPSS and Stata files (<em class="italic">Steps 5 and 8</em>), or by providing a catalog file path and filename for SAS (<em class="italic">Step 12</em>). </p>
    <p class="normal">We can set <code class="inlineCode">formats_as_category</code> to <code class="inlineCode">True</code> to change the data type to <code class="inlineCode">category</code> for those columns where the data values will change. The meta object has the column names and the column labels from the statistical package, so metadata column labels can be assigned to pandas DataFrame column names at any point (<code class="inlineCode">nls97spss.columns = metaspss.column_labels</code>). We can even revert to the original column headings after assigning meta column labels to them by setting pandas column names to the metadata column names (<code class="inlineCode">nls97spss.columns = metaspss.column_names</code>).</p>
    <p class="normal">In <em class="italic">Step 3</em>, we looked at some of the SPSS data before applying value labels. We looked at the dictionary for one variable (<code class="inlineCode">metaspss.variable_value_labels['R0536300']</code>), but we could have viewed it for all variables (<code class="inlineCode">metaspss.variable_value_labels</code>). When we are satisfied that the labels make sense, we can set them by calling the <code class="inlineCode">set_value_labels</code> function. This is a good approach when you do not know the data well and want to inspect the labels before applying them.</p>
    <p class="normal">The column labels from the meta object are often a better choice than the original column headings. Column headings can be quite cryptic, particularly when the SPSS, Stata, or SAS file is based on a large survey, as in this example. However, the labels are not usually ideal for column headings either. They sometimes have spaces, capitalization that is not helpful, and non-alphanumeric characters. We chain some string operations to switch to lowercase, replace spaces with underscores, and remove non-alphanumeric characters.</p>
    <p class="normal">Handling missing values is not always straightforward with these data files, since there are often many reasons why data is missing. If the file is from a survey, the missing value may be because of a survey skip pattern, or a respondent failed to respond, or the response was invalid, and so on. The NLS has nine possible values for missing, from <code class="inlineCode">–1</code> to <code class="inlineCode">–9</code>. The SPSS import automatically set those values to <code class="inlineCode">NaN</code>, while the Stata import retained the original values. (We could have gotten the SPSS import to retain those values by setting <code class="inlineCode">user_missing</code> to <code class="inlineCode">True</code>.) For the Stata data, we need to tell it to replace all values from <code class="inlineCode">–1</code> to <code class="inlineCode">–9</code> with <code class="inlineCode">NaN</code>. We do this by using<a id="_idIndexMarker078"/> the DataFrame’s <code class="inlineCode">replace</code> function and passing it a list<a id="_idIndexMarker079"/> of integers<a id="_idIndexMarker080"/> from <code class="inlineCode">–9</code> to <code class="inlineCode">–1</code> (<code class="inlineCode">list(range(-9,0))</code>).</p>
    <h2 id="_idParaDest-38" class="heading-2">There’s more…</h2>
    <p class="normal">You may have noticed similarities between this recipe and the previous one in terms of how value labels are set. The <code class="inlineCode">set_value_labels</code> function is like the DataFrame <code class="inlineCode">replace</code> operation we used to set value labels in that recipe. We passed a dictionary to <code class="inlineCode">replace</code> that mapped columns to value labels. The <code class="inlineCode">set_value_labels</code> function in this recipe essentially does the same thing, using the <code class="inlineCode">variable_value_labels</code> property of the meta object as the dictionary.</p>
    <p class="normal">Data from statistical packages is often not as well structured as SQL databases tend to be in one significant way. Since they are designed to facilitate analysis, they often violate database normalization rules. There is often an implied relational structure that might have to be <em class="italic">unflattened</em> at some point. For example, the data may combine individual and event-level data – a person and hospital visits, a brown bear and the date it emerged from hibernation. Often, this data will need to be reshaped for some aspects of the analysis.</p>
    <h2 id="_idParaDest-39" class="heading-2">See also</h2>
    <p class="normal">The <code class="inlineCode">pyreadstat</code> package is nicely documented at <a href="https://github.com/Roche/pyreadstat"><span class="url">https://github.com/Roche/pyreadstat</span></a>. The package has many useful options for selecting columns and handling missing data that space did not permit me to demonstrate in this recipe. In <em class="chapterRef">Chapter 11</em>, <em class="italic">Tidying and Reshaping Data</em>, we will examine how to normalize data that may have been flattened for analytical purposes.</p>
    <h1 id="_idParaDest-40" class="heading-1">Importing R data</h1>
    <p class="normal">We will use <code class="inlineCode">pyreadr</code> to read <a id="_idIndexMarker081"/>an R data file into pandas. Since <code class="inlineCode">pyreadr</code> cannot capture the metadata, we will write code to reconstruct value labels (analogous to R factors) and column headings. This is similar to what we did in the <em class="italic">Importing data from SQL databases</em> recipe.</p>
    <p class="normal">The R statistical package is, in many ways, similar to the combination of Python and pandas, at least in its scope. Both have strong tools across a range of data preparation and data analysis tasks. Some data scientists work with both R and Python, perhaps doing data manipulation in Python and statistical analysis in R, or vice versa, depending on their preferred packages. However, there is currently a scarcity of tools for reading data saved in R, as <code class="inlineCode">rds</code> or <code class="inlineCode">rdata</code> files, into Python. The analyst often saves the data as a CSV file first and then loads it into Python. We will use <code class="inlineCode">pyreadr</code>, from the same author as <code class="inlineCode">pyreadstat</code>, because it does not require an installation of R.</p>
    <p class="normal">When we receive an R file, or work with one we have created ourselves, we can count on it being fairly well structured, at least compared to CSV or Excel files. Each column will have only one data type, column headings will have appropriate names for Python variables, and all rows will have the same structure. However, we may need to restore some of the coding logic, as we did when working with SQL data.</p>
    <h2 id="_idParaDest-41" class="heading-2">Getting ready</h2>
    <p class="normal">This recipe assumes you have installed the <code class="inlineCode">pyreadr</code> package. If it is not installed, you can install it with <code class="inlineCode">pip</code>. From the Terminal, or Powershell (in Windows), enter <code class="inlineCode">pip install pyreadr</code>.</p>
    <p class="normal">We will again work with the NLS in this recipe. You will need to download the <code class="inlineCode">rds</code> file used in this recipe from the GitHub repository in order to run the code.</p>
    <h2 id="_idParaDest-42" class="heading-2">How to do it…</h2>
    <p class="normal">We will import data from R without losing important metadata:</p>
    <ol>
      <li class="numberedList" value="1">Load <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">pprint</code>, and the <code class="inlineCode">pyreadr</code> package:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pyreadr
<span class="hljs-keyword">import</span> pprint
</code></pre>
      </li>
      <li class="numberedList">Get the R data.</li>
    </ol>
    <p class="normal-one">Pass the path and filename<a id="_idIndexMarker082"/> to the <code class="inlineCode">read_r</code> method to retrieve the R data, and load it into memory as a pandas DataFrame. <code class="inlineCode">read_r</code> can return one or more objects. When reading an <code class="inlineCode">rds</code> file (as opposed to an <code class="inlineCode">rdata</code> file), it will return one object, having the key <code class="inlineCode">None</code>. We indicate <code class="inlineCode">None</code> to get the pandas DataFrame:</p>
    <pre class="programlisting code-one"><code class="hljs-code">nls97r = pyreadr.read_r(<span class="hljs-string">'data/nls97.rds'</span>)[<span class="hljs-literal">None</span>]
nls97r.dtypes
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">R0000100	int32
R0536300	int32
...
U2962800	int32
U2962900	int32
U2963000	int32
Z9063900	int32
dtype: object
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">nls97r.head(<span class="hljs-number">10</span>)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">     R0000100  R0536300  ...      U2963000    Z9063900
0	1	  2         ...      -5          52
1	2	  1         ...       6          0
2	3	  2         ...       6          0
3	4	  2         ...       6          4
4	5	  1         ...       5          12
5	6	  2         ...       6          6
6	7	  1         ...      -5          0
7	8	  2         ...      -5          39
8	9	  1         ...       4          0
9	10	  1         ...       6          0
[10 rows x 42 columns]
</code></pre>
    <ol>
      <li class="numberedList" value="3">Set up dictionaries for value labels and column headings.</li>
    </ol>
    <p class="normal-one">Load a dictionary that maps<a id="_idIndexMarker083"/> columns to the value labels and create a list of preferred column names as follows:</p>
    <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'data/nlscodes.txt'</span>, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> reader:
...     setvalues = <span class="hljs-built_in">eval</span>(reader.read())
...
pprint.pprint(setvalues)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">{'R0536300': {0.0: 'No Information', 1.0: 'Male', 2.0: 'Female'},
 'R1235800': {0.0: 'Oversample', 1.0: 'Cross-sectional'},
 'S8646900': {1.0: '1. Definitely',
              2.0: '2. Probably ',
              3.0: '3. Probably not',
              4.0: '4. Definitely not'}}
...abbreviated to save space
newcols = ['personid','gender','birthmonth',
...   'birthyear','sampletype','category',
...   'satverbal','satmath','gpaoverall',
...   'gpaeng','gpamath','gpascience','govjobs',
...   'govprices','govhealth','goveld','govind',
...   'govunemp','govinc','govcollege',
...   'govhousing','govenvironment','bacredits',
...   'coltype1','coltype2','coltype3','coltype4',
...   'coltype5','coltype6','highestgrade',
...   'maritalstatus','childnumhome','childnumaway',
...   'degreecol1','degreecol2','degreecol3',
...   'degreecol4','wageincome','weeklyhrscomputer',
...   'weeklyhrstv','nightlyhrssleep',
...   'weeksworkedlastyear']
</code></pre>
    <ol>
      <li class="numberedList" value="4">Set value labels and missing values, and change selected columns to the <code class="inlineCode">category</code> data type.</li>
    </ol>
    <p class="normal-one">Use the <code class="inlineCode">setvalues</code> dictionary to replace existing values with value labels. Replace all values from <code class="inlineCode">–9</code> to <code class="inlineCode">–1</code> with <code class="inlineCode">NaN</code>:</p>
    <pre class="programlisting code-one"><code class="hljs-code">nls97r.replace(setvalues, inplace=<span class="hljs-literal">True</span>)
nls97r.head()
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">     R0000100   R0536300  ...     U2963000  Z9063900
0    1          Female    ...     -5        52
1    2          Male      ...     6         0
2    3          Female    ...     6         0
3    4          Female    ...     6         4
4    5          Male      ...     5         12
[5 rows x 42 columns]
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">nls97r.replace(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(-<span class="hljs-number">9</span>,<span class="hljs-number">0</span>)), np.nan, inplace=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> nls97r[[k <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> setvalues]].columns:
...     nls97r[col] = nls97r[col].astype(<span class="hljs-string">'category'</span>)
...
nls97r.dtypes
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">R0000100	int64
R0536300	category
R0536401	int64
R0536402	int64
R1235800	category
              ... 
U2857300	category
U2962800	category
U2962900	category
U2963000	float64
Z9063900	float64
Length: 42, dtype: object
</code></pre>
    <ol>
      <li class="numberedList" value="5">Set meaningful column<a id="_idIndexMarker084"/> headings:
        <pre class="programlisting code-one"><code class="hljs-code">nls97r.columns = newcols
nls97r.dtypes
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">personid	int64
gender	category
birthmonth	int64
birthyear	int64
sampletype	category
                         ... 
wageincome	category
weeklyhrscomputer	category
weeklyhrstv	category
nightlyhrssleep	float64
weeksworkedlastyear	float64
Length: 42, dtype: object
</code></pre>
      </li>
    </ol>
    <p class="normal">This shows how R data files can be imported into pandas and value labels assigned.</p>
    <h2 id="_idParaDest-43" class="heading-2">How it works…</h2>
    <p class="normal">Reading R data into pandas<a id="_idIndexMarker085"/> with <code class="inlineCode">pyreadr</code> is fairly straightforward. Passing a filename to the <code class="inlineCode">read_r</code> function is all that is required. Since <code class="inlineCode">read_r</code> can return multiple objects with one call, we need to specify which object. When reading an <code class="inlineCode">rds</code> file (as opposed to an <code class="inlineCode">rdata</code> file), only one object is returned. It has the key <code class="inlineCode">None</code>.</p>
    <p class="normal">In <em class="italic">Step 3</em>, we loaded a dictionary that maps our variables to value labels, and a list for our preferred column headings. In <em class="italic">Step 4</em> we applied the value labels. We also changed the data type to <code class="inlineCode">category</code> for the columns where we applied the values. We did this by generating a list of the keys in our <code class="inlineCode">setvalues</code> dictionary with <code class="inlineCode">[k for k in setvalues]</code> and then iterating over those columns.</p>
    <p class="normal">We change the column headings in <em class="italic">Step 5</em> to ones that are more intuitive. Note that the order matters here. We need to set the value labels before changing the column names, since the <code class="inlineCode">setvalues</code> dictionary is based on the original column headings.</p>
    <p class="normal">The main advantage of using <code class="inlineCode">pyreadr</code> to read R files directly into pandas is that we do not have to convert the R data into a CSV file first. Once we have written our Python code to read the file, we can just rerun it whenever the R data changes. This is particularly helpful when we do not have R on the machine where we work.</p>
    <h2 id="_idParaDest-44" class="heading-2">There’s more…</h2>
    <p class="normal"><code class="inlineCode">Pyreadr</code> is able to return multiple DataFrames. This is useful when we save several data objects in R as an <code class="inlineCode">rdata</code> file. We can return all of them with one call.</p>
    <p class="normal"><code class="inlineCode">Pprint</code> is a handy tool for improving the display of Python dictionaries.</p>
    <p class="normal">We could have used <code class="inlineCode">rpy2</code> instead of <code class="inlineCode">pyreadr</code> to import R data. <code class="inlineCode">rpy2</code> requires that R also be installed, but it is more powerful than <code class="inlineCode">pyreadr</code>. It will read R factors and automatically set them to pandas DataFrame values. See the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> rpy2.robjects <span class="hljs-keyword">as</span> robjects
<span class="hljs-keyword">from</span> rpy2.robjects <span class="hljs-keyword">import</span> pandas2ri
pandas2ri.activate()
readRDS = robjects.r[<span class="hljs-string">'readRDS'</span>]
nls97withvalues = readRDS(<span class="hljs-string">'data/nls97withvalues.rds'</span>)
nls97withvalues
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">          R0000100      R0536300     ...    U2963000         Z9063900
1         1             Female       ...    -2147483648      52
2         2             Male         ...    6                0
3         3             Female       ...    6                0
4         4             Female       ...    6                4
5         5             Male         ...    5                12
...       ...           ...          ...    ...             ...
8980     9018           Female       ...    4                49
8981     9019           Male         ...    6                0
8982     9020           Male         ...    -2147483648      15
8983     9021           Male         ...    7                50
8984     9022           Female       ...    7                20
[8984 rows x 42 columns]
</code></pre>
    <p class="normal">This generates unusual <em class="italic">–2147483648</em> values. This is what happened when <code class="inlineCode">readRDS</code> interpreted missing data in numeric columns. A global replacement of that number with <code class="inlineCode">NaN</code>, after confirming that that is not a valid value, would be a good next step.</p>
    <h2 id="_idParaDest-45" class="heading-2">See also</h2>
    <p class="normal">Clear instructions and examples for <code class="inlineCode">pyreadr</code> are available at <a href="https://github.com/ofajardo/pyreadr"><span class="url">https://github.com/ofajardo/pyreadr</span></a>.</p>
    <p class="normal">Feather files, a relatively new format, can be read by both R and Python. I discuss those files in the next recipe.</p>
    <h1 id="_idParaDest-46" class="heading-1">Persisting tabular data</h1>
    <p class="normal">We persist data, copy it from memory<a id="_idIndexMarker086"/> to local or remote storage, for several reasons: to be able to access the data without having to repeat the steps we used to generate it, to share the data with others, or to make it available for use with different software. In this recipe, we save data that we have loaded into a pandas DataFrame as different file types (CSV, Excel, Pickle, and Feather).</p>
    <p class="normal">Another important, but sometimes overlooked, reason to persist data is to preserve some segment of our data that needs to be examined more closely; perhaps it needs to be scrutinized by others before our analysis can be completed. For analysts who work with operational data in medium- to large-sized organizations, this process is part of the daily data-cleaning workflow.</p>
    <p class="normal">In addition to these reasons for persisting data, our decisions about when and how to serialize data are shaped by several other factors: where we are in terms of our data analysis projects, the hardware and software resources of the machine(s) saving and reloading the data, and the size of our dataset. Analysts end up having to be much more intentional when saving data than they are when pressing <em class="keystroke">Ctrl</em> + <em class="keystroke">S</em> in their word-processing application.</p>
    <p class="normal">Once we persist data, it is stored <a id="_idIndexMarker087"/>separately from the logic that we used to create it. I find this to be one of the most important threats to the integrity of our analysis. Often, we end up loading data that we saved some time in the past (a week ago? A month ago? A year ago?) and forget how a variable was defined and how it relates to other variables. If we are in the middle of a data-cleaning task, it is best not to persist our data, so long as our workstation and network can easily handle the burden of regenerating the data. It is a good idea to persist data only once we have reached milestones in our work.</p>
    <p class="normal">Beyond the question of <em class="italic">when</em> to persist data, there is the question of <em class="italic">how</em>. If we are persisting it for our own reuse with the same software, it is best to save it in a binary format native to that software. That is pretty straightforward for tools such as SPSS, SAS, Stata, and R, but not so much for pandas. But that is good news in a way. We have lots of choices, from CSV and Excel to Pickle and Feather. We save as all these file types in this recipe.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">Pickle and Feather are binary file formats that can be used to store pandas DataFrames.</p>
    </div>
    <h2 id="_idParaDest-47" class="heading-2">Getting ready</h2>
    <p class="normal">You will need to install Feather<a id="_idIndexMarker088"/> if you do not have it on your system. You can do that by entering <code class="inlineCode">pip install pyarrow</code> in a Terminal window or <code class="inlineCode">powershell</code> (in Windows). If you do not already have a subfolder named <code class="inlineCode">Views</code> in your <code class="inlineCode">chapter 1</code> folder, you will need to create it in order to run the code for this recipe.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Data note</strong></p>
      <p class="normal">This dataset, taken from the Global Historical Climatology Network integrated database, is made available for public use by the United States National Oceanic and Atmospheric Administration at <a href="https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly"><span class="url">https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly</span></a>. I used the data from version 4. The data in this recipe uses a 100,000-row sample of the full dataset, which is also available in the repository.</p>
    </div>
    <h2 id="_idParaDest-48" class="heading-2">How to do it…</h2>
    <p class="normal">We will load a CSV file into pandas and then save it as a Pickle and a Feather file. We will also save subsets of the data to the CSV and Excel formats:</p>
    <ol>
      <li class="numberedList" value="1">Import <code class="inlineCode">pandas</code> and <code class="inlineCode">pyarrow</code>.</li>
    </ol>
    <p class="normal-one"><code class="inlineCode">pyarrow</code> needs to be imported in order to save pandas to Feather:</p>
    <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> pyarrow
</code></pre>
    <ol>
      <li class="numberedList" value="2">Load the land temperatures CSV file into pandas, drop rows with missing data, and set an index:
        <pre class="programlisting code-one"><code class="hljs-code">landtemps = \
...   pd.read_csv(<span class="hljs-string">'data/landtempssample.csv'</span>,
...     names=[<span class="hljs-string">'stationid'</span>,<span class="hljs-string">'year'</span>,<span class="hljs-string">'month'</span>,<span class="hljs-string">'avgtemp'</span>,
...       <span class="hljs-string">'latitude'</span>,<span class="hljs-string">'longitude'</span>,<span class="hljs-string">'elevation'</span>,
...       <span class="hljs-string">'station'</span>,<span class="hljs-string">'countryid'</span>,<span class="hljs-string">'country'</span>],
...     skiprows=<span class="hljs-number">1</span>,
...     parse_dates=[[<span class="hljs-string">'month'</span>,<span class="hljs-string">'year'</span>]],
...     low_memory=<span class="hljs-literal">False</span>)
landtemps.rename(columns={<span class="hljs-string">'month_year'</span>:<span class="hljs-string">'measuredate'</span>}, inplace=<span class="hljs-literal">True</span>)
landtemps.dropna(subset=[<span class="hljs-string">'avgtemp'</span>], inplace=<span class="hljs-literal">True</span>)
landtemps.dtypes
</code></pre>
        <pre class="programlisting con-one"><code class="hljs-con">measuredate	datetime64[ns]
stationid	object
avgtemp	float64
latitude	float64
longitude	float64
elevation	float64
station	object
countryid	object
country	object
dtype: object
</code></pre>
        <pre class="programlisting code-one"><code class="hljs-code">landtemps.set_index([<span class="hljs-string">'measuredate'</span>,<span class="hljs-string">'stationid'</span>], inplace=<span class="hljs-literal">True</span>)
</code></pre>
      </li>
      <li class="numberedList">Write extreme values<a id="_idIndexMarker089"/> for <code class="inlineCode">temperature</code> to CSV and Excel files.</li>
    </ol>
    <p class="normal-one">Use the <code class="inlineCode">quantile</code> method to select outlier rows, which are those at the 1 in 1,000 level at each end of the distribution:</p>
    <pre class="programlisting code-one"><code class="hljs-code">extremevals = landtemps[(landtemps.avgtemp &lt; landtemps.avgtemp.quantile(<span class="hljs-number">.001</span>)) | (landtemps.avgtemp &gt; landtemps.avgtemp.quantile(<span class="hljs-number">.999</span>))]
extremevals.shape
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">(171, 7)
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">extremevals.sample(<span class="hljs-number">7</span>)
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">                           avgtemp  ...   country
measuredate  stationid              ...       
2013-08-01	QAM00041170	35.30    ...	Qatar
2005-01-01	RSM00024966	-40.09   ...	Russia
1973-03-01	CA002401200	-40.26   ...	Canada
2007-06-01	KU000405820	37.35    ...	Kuwait
1987-07-01	SUM00062700	35.50    ...	Sudan
1998-02-01	RSM00025325	-35.71   ...	Russia
1968-12-01	RSM00024329	-43.20   ...	Russia
[7 rows x 7 columns]
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">extremevals.to_excel(<span class="hljs-string">'views/tempext.xlsx'</span>)
extremevals.to_csv(<span class="hljs-string">'views/tempext.csv'</span>)
</code></pre>
    <ol>
      <li class="numberedList" value="4">Save to Pickle and Feather files.</li>
    </ol>
    <p class="normal-one">The index needs to be reset in order to save a Feather file:</p>
    <pre class="programlisting code-one"><code class="hljs-code">landtemps.to_pickle(<span class="hljs-string">'data/landtemps.pkl'</span>)
landtemps.reset_index(inplace=<span class="hljs-literal">True</span>)
landtemps.to_feather(<span class="hljs-string">"data/landtemps.ftr"</span>)
</code></pre>
    <ol>
      <li class="numberedList" value="5">Load the Pickle and Feather files we just saved.</li>
    </ol>
    <p class="normal-one">Note that our index<a id="_idIndexMarker090"/> was preserved when saving and loading the Pickle file:</p>
    <pre class="programlisting code-one"><code class="hljs-code">landtemps = pd.read_pickle(<span class="hljs-string">'data/landtemps.pkl'</span>)
landtemps.head(<span class="hljs-number">2</span>).T
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">measuredate	2000-04-01	1940-05-01
stationid	USS0010K01S	CI000085406
avgtemp	5.27		18.04
latitude	39.90		-18.35
longitude	-110.75		-70.33
elevation	2,773.70	58.00
station	INDIAN_CANYON	ARICA
countryid	US		CI
country	United States	Chile
</code></pre>
    <pre class="programlisting code-one"><code class="hljs-code">landtemps = pd.read_feather(<span class="hljs-string">"data/landtemps.ftr"</span>)
landtemps.head(<span class="hljs-number">2</span>).T
</code></pre>
    <pre class="programlisting con-one"><code class="hljs-con">                               0                    1
measuredate	2000-04-01 00:00:00	1940-05-01 00:00:00
stationid	USS0010K01S		CI000085406
avgtemp	5.27			18.04
latitude	39.90			-18.35
longitude	-110.75			-70.33
elevation	2,773.70		58.00
station	INDIAN_CANYON		ARICA
countryid	US			CI
country	United States		Chile
</code></pre>
    <p class="normal">The previous steps demonstrated how to serialize pandas DataFrames using two different formats, Pickle and Feather.</p>
    <h2 id="_idParaDest-49" class="heading-2">How it works...</h2>
    <p class="normal">Persisting pandas data is quite<a id="_idIndexMarker091"/> straightforward. DataFrames have the <code class="inlineCode">to_csv</code>, <code class="inlineCode">to_excel</code>, <code class="inlineCode">to_pickle</code>, and <code class="inlineCode">to_feather</code> methods. Pickling preserves our index.</p>
    <h2 id="_idParaDest-50" class="heading-2">There’s more...</h2>
    <p class="normal">The advantage of storing data in CSV files is that saving it uses up very little additional memory. The disadvantage is that writing CSV files is quite slow, and we lose important metadata, such as data types. (<code class="inlineCode">read_csv</code> can often figure out the data type when we reload the file, but not always.) Pickle files keep that data but can burden a system that is low on resources when serializing. Feather is easier on resources and can be easily loaded in R as well as Python, but we have to sacrifice our index in order to serialize. Also, the authors of Feather make no promises regarding long-term support.</p>
    <p class="normal">You may have noticed that I do not make a global recommendation about what to use for data serialization – other than to limit your persistence of full datasets to project milestones. This is definitely one of those “right tools for the right job” kind of situations. I use CSV or Excel files when I want to share a segment of a file with colleagues for discussion. I use Feather for ongoing Python projects, particularly when I am using a machine with sub-par RAM and an outdated chip and also using R. When I am wrapping up a project, I pickle the DataFrames.</p>
    <h1 id="_idParaDest-51" class="heading-1">Summary</h1>
    <p class="normal">Our Python data projects typically start with raw data stored in a range of formats and exported from a variety of software tools. Among the most popular tabular formats and tools are CSV and Excel files, SQL tables, and SPSS, Stata, SAS, and R datasets. We converted data from all of these sources into a pandas DataFrame in this chapter, and addressed the most common challenges. We also explored approaches to persisting tabular data. We will work with data in other formats in the next chapter.</p>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://discord.gg/p8uSgEAETX "><span class="url">https://discord.gg/p8uSgEAETX</span></a></p>
    <p class="normal"><img src="../Images/QR_Code10336218961138498953.png" alt="" role="presentation"/></p>
  </div>
</body></html>