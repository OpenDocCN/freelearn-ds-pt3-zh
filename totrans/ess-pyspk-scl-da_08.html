<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer041">
			<h1 id="_idParaDest-107"><a id="_idTextAnchor107"/>Chapter 6: Feature Engineering – Extraction, Transformation, and Selection</h1>
			<p>In the previous chapter, you were introduced to Apache Spark's native, scalable machine learning library, called <strong class="bold">MLlib</strong>, and you were provided with an overview of its major architectural components, including transformers, estimators, and pipelines.</p>
			<p>This chapter will take you to your first stage of the <strong class="bold">scalable machine learning</strong> journey, which is <strong class="bold">feature engineering</strong>. Feature engineering deals with the process of extracting machine learning features from preprocessed and clean data in order to make it conducive for machine learning. You will learn about the concepts of <strong class="bold">feature extraction</strong>, <strong class="bold">feature transformation</strong>, <strong class="bold">feature scaling</strong>, and <strong class="bold">feature selection</strong> and implement these techniques using the algorithms that exist within Spark MLlib and some code examples. Toward the end of this chapter, you will have learned the necessary techniques to implement scalable feature engineering pipelines that convert preprocessed data into a format that is suitable and ready for the machine learning model training process.</p>
			<p>Particularly, in this chapter, you will learn the following:</p>
			<ul>
				<li>The machine learning process</li>
				<li>Feature extraction</li>
				<li>Feature transformation</li>
				<li>Feature selection</li>
				<li>Feature store as a central feature repository</li>
				<li>Delta as an offline feature store</li>
			</ul>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor108"/>Technical requirements</h1>
			<p>In this chapter, we will be using the Databricks Community Edition to run our code. This can be found at <a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a>.</p>
			<ul>
				<li>Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. The code used in this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter06">https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter06</a>.</li>
				<li>The datasets used in this chapter can be found at<a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data"> https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data</a>.</li>
			</ul>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor109"/>The machine learning process</h1>
			<p>A typical data analytics and data science process involves gathering raw data, cleaning data, consolidating data, and integrating data. Following this, we apply statistical and machine learning <a id="_idIndexMarker480"/>techniques to the preprocessed data in order to generate a machine learning model and, finally, summarize and communicate the results of the process to business stakeholders in the form of data products. A high-level overview of the machine learning process is presented in the following diagram:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="Images/B16736_06_01.jpg" alt="Figure 6.1 – The data analytics and data science process&#13;&#10;" width="767" height="135"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – The data analytics and data science process</p>
			<p>As you can see from the preceding diagram, the actual machine learning process itself is just a small portion of the entire data analytics process. Data teams spend a good amount of time curating and preprocessing data, and just a portion of that time is devoted to building actual machine learning models.</p>
			<p>The actual machine <a id="_idIndexMarker481"/>learning process involves stages that allow you to carry out steps such as data exploration, feature extraction, model training, model evaluation, and applying models for real-world business applications, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="Images/B16736_06_02.jpg" alt="Figure 6.2 – The machine learning process&#13;&#10;" width="730" height="176"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – The machine learning process</p>
			<p>In this chapter, you will learn about the <strong class="bold">Feature Engineering</strong> phase of the machine learning process. The following sections will present a few of the prominent algorithms and utilities available in the <strong class="bold">Spark MLlib</strong> library that deal with the <strong class="bold">Feature Extraction</strong>, <strong class="bold">Feature Transformation</strong>, <strong class="bold">Feature Scaling</strong>, and <strong class="bold">Feature Selection</strong> steps of the <strong class="bold">Feature Engineering</strong> process.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor110"/>Feature extraction</h1>
			<p>A machine learning model is equivalent to a function in mathematics or a method in computer programming. A machine learning model takes one or more parameters or variables as input <a id="_idIndexMarker482"/>and yields an output, called a prediction. In machine learning terminology, these input parameters or variables are called <strong class="bold">features</strong>. A feature is a column <a id="_idIndexMarker483"/>of the input dataset within a machine learning algorithm or model. A feature is a measurable data point, such as an individual's name, gender, or age, or it can be time-related data, weather, or some other piece of data that is useful for analysis.</p>
			<p>Machine learning algorithms leverage linear algebra, a field of mathematics, and make use of mathematical structures such as matrices and vectors to represent data internally and also within the code level implementation of algorithms. Real-world data, even after undergoing the <a id="_idIndexMarker484"/>data engineering process, rarely occurs in the form of matrices and vectors. Therefore, the feature engineering process is applied to preprocessed data in order to convert it into a format that is suitable for machine learning algorithms.</p>
			<p>The feature extraction process specifically deals with taking text, image, geospatial, or time series data and converting it into a feature vector. Apache Spark MLlib has a number of feature extractions available, such as <strong class="source-inline">TF-IDF</strong>, <strong class="source-inline">Word2Vec</strong>, <strong class="source-inline">CountVectorizer</strong>, and <strong class="source-inline">FeatureHasher</strong>.</p>
			<p>Let's consider an example of a group of words and convert them into a feature vector using the <strong class="source-inline">CountVectorizer</strong> algorithm. In earlier chapters of this book, we looked at sample datasets for an online retailer and applied the data engineering process on those datasets to get a clean and consolidated dataset that was ready for analytics.</p>
			<p>So, let's begin with the preprocessed and cleaned dataset produced toward the end of <a href="B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094"><em class="italic">Chapter 5</em></a>, <em class="italic">Scalable Machine Learning with PySpark</em>, named <strong class="source-inline">retail_ml.delta</strong>. This preprocessed dataset, which <a id="_idIndexMarker485"/>forms the input to the machine learning process, is also generally referred to as the <strong class="bold">training dataset</strong>:</p>
			<ol>
				<li>As a first step, let's load the data from the data lake in Delta format into a Spark DataFrame. This is shown in the following code snippet:<p class="source-code">preproc_data = spark.read.format("delta").load("dbfs:/FileStore/shared_uploads/delta/retail_ml.delta")</p><p class="source-code">preproc_data.show()</p><p>In the preceding code block, we load data stored in the data lake in Delta form into a Spark DataFrame and then display the data using the <strong class="source-inline">show()</strong> command.</p></li>
				<li>The result of the display function is shown in the following diagram:<div id="_idContainer037" class="IMG---Figure"><img src="Images/B16736_06_03.jpg" alt="Figure 6.3 – Preprocessed data&#13;&#10;" width="1412" height="420"/></div><p class="figure-caption">Figure 6.3 – Preprocessed data</p><p>In the preceding diagram, we have the preprocessed data as a result of the data engineering and data wrangling steps. Notice that there are <strong class="source-inline">11</strong> columns in the dataset with various <a id="_idIndexMarker486"/>data types, ranging from a string to a double, to a timestamp. In their current format, they are not suitable as inputs for a machine learning algorithm; therefore, we need to convert them into a suitable format via the feature engineering process.</p></li>
				<li>Let's start with the <strong class="source-inline">description</strong> column, which is of the text type, and apply the <strong class="source-inline">CountVectorizer</strong> feature extraction algorithm to it in order to convert it into a feature vector, as shown in the following code block:<p class="source-code">from pyspark.sql.functions import split, trim</p><p class="source-code">from pyspark.ml.feature import CountVectorizer</p><p class="source-code">cv_df = preproc_data.withColumn("desc_array", split(trim("description"), " ")).where("description is NOT NULL")</p><p class="source-code">cv = CountVectorizer(inputCol="desc_array", </p><p class="source-code">                     outputCol="description_vec", </p><p class="source-code">                     vocabSize=2, minDF=2.0)</p><p class="source-code">cv_model = cv.fit(cv_df)</p><p class="source-code">train_df = model.transform(cv_df)</p><p class="source-code">train_df.display()</p><p>In the previous code block, the following occurs:</p><ol><li>We import <strong class="source-inline">CountVectorizer</strong> from the <strong class="source-inline">pyspark.ml.feature</strong> library.</li><li><strong class="source-inline">CountVectorizer</strong> takes an <strong class="source-inline">Array</strong> object as input, so we use the <strong class="source-inline">split()</strong> function <a id="_idIndexMarker487"/>to split the description column into an <strong class="source-inline">Array</strong> object of words.</li><li>Then, we initialize a new <strong class="source-inline">CountVectorizer</strong> <strong class="bold">estimator</strong> by passing in the input column, defining <a id="_idIndexMarker488"/>the output column name, and then defining <a id="_idIndexMarker489"/>appropriate values for the <strong class="bold">hyperparameters</strong>.</li><li>Then, we call the <strong class="source-inline">fit()</strong> method using the previously defined estimator on the input <a id="_idIndexMarker490"/>dataset. The result is a trained model <strong class="bold">transformer</strong> object.</li><li>Finally, we call the <strong class="source-inline">transform()</strong> method on the input DataFrame, resulting in a new DataFrame with a new feature vector column for the description column.</li></ol><p>In this way, by using the <strong class="source-inline">CountVectorizer</strong> feature extractor from Spark MLlib, we are able to extract a feature vector from a text type column.</p></li>
				<li>Another feature extractor available within Spark MLlib, such as <strong class="source-inline">Word2Vec</strong>, can also be used, as shown in the following code snippet:<p class="source-code">from pyspark.ml.feature import Word2Vec</p><p class="source-code">w2v_df = preproc_data.withColumn("desc_array", split(trim("description"), "\t")).where("description is NOT NULL")</p><p class="source-code">word2vec = Word2Vec(vectorSize=2, minCount=0, </p><p class="source-code">                    inputCol="desc_array", </p><p class="source-code">                    outputCol="desc_vec")</p><p class="source-code">w2v_model = word2vec.fit(w2v_df)</p><p class="source-code">train_df = w2v_model.transform(w2v_df)</p><p>In the preceding code block, the <strong class="source-inline">Word2Vec</strong> estimator is used in a similar fashion to the previously mentioned <strong class="source-inline">CountVectorizer</strong>. Here, we use it to extract a feature vector from a text-based data column. While both <strong class="source-inline">CountVectorizer</strong> and <strong class="source-inline">Word2Vec</strong> help to convert a corpus of words into a feature vector, there <a id="_idIndexMarker491"/>are differences in the internal implementations of each algorithm. They each have different uses depending on the problem scenario and input dataset and might produce different results under different circumstances.</p></li>
			</ol>
			<p>Please note that discussing the nuances of these algorithms or making recommendations on when to use a specific feature extraction algorithm is beyond the scope of this book.</p>
			<p>Now that you have learned a few techniques of feature extraction, in the next section, let's explore a few of Spark MLlib's algorithms for <strong class="bold">feature transformation</strong>.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor111"/>Feature transformation</h1>
			<p>Feature transformation is the process of carefully reviewing the various variable types, such as categorical variables and continuous variables, present in the training data and determining the <a id="_idIndexMarker492"/>best type of transformation to achieve optimal model performance. This section will describe, with code examples, how to transform a few common types of variables found in machine learning datasets, such as text and numerical variables.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor112"/>Transforming categorical variables</h2>
			<p>Categorical variables are pieces of data that have discrete values with a limited and finite range. They are <a id="_idIndexMarker493"/>usually text-based in nature, but they can also be numerical. Examples include country codes and the month of the year. We mentioned a few techniques regarding how to extract features from text variables in the <a id="_idIndexMarker494"/>previous section. In this section, we will explore a few other algorithms to transform categorical variables.</p>
			<h3>The tokenization of text into individual terms</h3>
			<p>The <strong class="source-inline">Tokenizer</strong> class can be used to break down text into its constituent terms, as shown <a id="_idIndexMarker495"/>in the following code example:</p>
			<p class="source-code">from pyspark.ml.feature import Tokenizer</p>
			<p class="source-code">tokenizer = Tokenizer(inputCol="description", </p>
			<p class="source-code">                      outputCol="desc_terms")</p>
			<p class="source-code">tokenized_df = tokenizer.transform(preproc_data)</p>
			<p class="source-code">tokenized_df.show()</p>
			<p>In the preceding code block, we initialize the <strong class="source-inline">Tokenizer</strong> class by passing in the <strong class="source-inline">inputCol</strong> and <strong class="source-inline">outputCol</strong> parameters, which results in a transformer. Then, we transform the training dataset, resulting in a Spark DataFrame with a new column with an array of individual words from each sentence that have been converted into lowercase. This is shown in the following table:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="Images/B16736_06_04.jpg" alt="Figure 6.4 – Tokenizing the text using Tokenizer&#13;&#10;" width="837" height="302"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Tokenizing the text using Tokenizer</p>
			<p>In the preceding table, you can see from the tokenized words that there are a few unwanted words, which we need to get rid of as they do not add any value.</p>
			<h3>Removing common words using StopWordsRemover</h3>
			<p>Every language contains common and frequently occurring words such as prepositions, articles, conjunctions, and interjections. These words do not carry any meaning in terms of the machine <a id="_idIndexMarker496"/>learning process and are better removed before training a machine learning algorithm. In Spark, this process can be <a id="_idIndexMarker497"/>achieved using the <strong class="source-inline">StopWordsRemover</strong> class, as shown in the following code snippet:</p>
			<p class="source-code">from pyspark.ml.feature import StopWordsRemover</p>
			<p class="source-code">stops_remover = StopWordsRemover(inputCol="desc_terms", </p>
			<p class="source-code">                                 outputCol="desc_nostops")</p>
			<p class="source-code">stops_df = stops_remover.transform(tokenized_df)</p>
			<p class="source-code">stops_df.select("desc_terms", "desc_nostops").show()</p>
			<p>In the preceding code block, we initialize the <strong class="source-inline">StopWordsRemover</strong> class by passing in the <strong class="source-inline">inputCol</strong> and <strong class="source-inline">outputCol</strong> parameters, which results in a transformer. Then, we transform the training dataset, resulting in a Spark DataFrame with a new column that has an array of individual words with the stop words removed.</p>
			<p>Once we have an array of strings with the stop words removed, a feature extraction technique such as <strong class="source-inline">Word2Vec</strong> or <strong class="source-inline">CountVectorizer</strong> is used to build a feature vector.</p>
			<h3>Encoding discrete, categorical variables</h3>
			<p>Now we have other types of string-type columns such as country codes that need to be converted into a <a id="_idIndexMarker498"/>numerical form for consumption by a machine learning algorithm. You cannot simply assign arbitrary numerical values to such discrete, categorical variables, as this could introduce a pattern that might not necessarily <a id="_idIndexMarker499"/>exist within the data.</p>
			<p>Let's consider an example where we monotonically assign increasing values to categorical variables in alphabetical order. However, this might introduce ranking to those variables where one didn't exist in the first place. This would skew our machine learning model and is not desirable. To overcome this problem, we can use a number of Spark MLlib algorithms in which to encode these categorical variables.</p>
			<h3>Encoding string variables using StringIndexer</h3>
			<p>In our training dataset, we have string types, or categorical variables, with discrete values such as <strong class="source-inline">country_code</strong>. These variables can be assigned label indices using <strong class="source-inline">StringIndexer</strong>, as shown in the following code example:</p>
			<p class="source-code">from pyspark.ml.feature import StringIndexer</p>
			<p class="source-code">string_indexer = StringIndexer(inputCol="country_code", </p>
			<p class="source-code">                               outputCol="country_indexed", </p>
			<p class="source-code">                               handleInvalid="skip" )</p>
			<p class="source-code">indexed_df = string_indexer.fit(stops_df).transform(stops_df)</p>
			<p class="source-code">indexed_df.select("country_code", "country_indexed").show()</p>
			<p>In the <a id="_idIndexMarker500"/>preceding code snippet, we initialize the <strong class="source-inline">StringIndexer</strong> class with input and output column names. Then, we set <strong class="source-inline">handleInvalid</strong> to <strong class="source-inline">skip</strong> in order to skip <strong class="source-inline">NULLs</strong> and invalid values. This results <a id="_idIndexMarker501"/>in an estimator that can be applied to the training DataFrame, which, in turn, results in a transformer. The transformer can be applied to the training dataset. This results in a DataFrame with a new Spark DataFrame along with a new column that contains label indices for the input categorical variable.</p>
			<h3>Transforming a categorical variable into a vector using OneHotEncoder</h3>
			<p>Once <a id="_idIndexMarker502"/>we have our categorical variables encoded into label indices, they can finally be converted into <a id="_idIndexMarker503"/>a binary vector, using the <strong class="source-inline">OneHotEncoder</strong> class, as shown in the following code snippet:</p>
			<p class="source-code">from pyspark.ml.feature import OneHotEncoder</p>
			<p class="source-code">ohe = OneHotEncoder(inputCol="country_indexed", </p>
			<p class="source-code">                    outputCol="country_ohe")</p>
			<p class="source-code">ohe_df = ohe.fit(indexed_df).transform(indexed_df)</p>
			<p class="source-code">ohe_df.select("country_code", "country_ohe").show()</p>
			<p>In the preceding code snippet, we initialize the <strong class="source-inline">OneHotEncoder</strong> class with input and output <a id="_idIndexMarker504"/>column names. This results in an estimator that can be applied to the training DataFrame, which, in turn, results in a transformer. The transformer can be applied to the training <a id="_idIndexMarker505"/>dataset. This results in a DataFrame with a new Spark DataFrame along with a new column that contains a feature vector representing the original categorical variable.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor113"/>Transforming continuous variables</h2>
			<p>Continuous variables represent data in the form of measurements or observations. Typically, they are numerical in nature and can virtually have an infinite range. Here, the data is continuous <a id="_idIndexMarker506"/>and not discrete, and a few examples include age, quantity, and unit price. They seem straightforward enough and can be directly fed into a machine learning algorithm. However, they still need to be engineered into features, as continuous variables might have just far too many values to be handled by the machine learning algorithm. There are multiple ways in which to handle continuous variables, such as binning, normalization, applying custom business logic, and more, and an appropriate method should be chosen depending on the problem being solved and the business domain.</p>
			<p>One such technique to feature engineer continuous variables is binarization, where the continuous numerical values are converted into binary values based on a user-defined threshold, as shown in the following code example:</p>
			<p class="source-code">from pyspark.ml.feature import Binarizer</p>
			<p class="source-code">binarizer = Binarizer(threshold=10, inputCol="unit_price", </p>
			<p class="source-code">                      outputCol="binarized_price")</p>
			<p class="source-code">binarized_df = binarizer.transform(ohe_df)</p>
			<p class="source-code">binarized_df.select("quantity", "binarized_price").show()</p>
			<p>In the preceding code block, we initialize the <strong class="source-inline">Binarizer</strong> class with the input and output column parameters, which results in a transformer. The transformer can then be applied to the training DataFrame, which, in turn, results in a new DataFrame along with a new column representing the binary values for the continuous variable.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor114"/>Transforming the date and time variables</h2>
			<p>A date or timestamp type of column in itself doesn't add much value to a machine learning model <a id="_idIndexMarker507"/>training process. However, there might be patterns <a id="_idIndexMarker508"/>within the components of a date such as month, year, or day of the week. Therefore, it would be useful to choose a part of the datetime column and transform it into an appropriate feature.</p>
			<p>In the following code example, we extract the month value from a datetime column and transform it into a feature, treating it like a categorical variable:</p>
			<p class="source-code">from pyspark.sql.functions import month</p>
			<p class="source-code">month_df = binarized_df.withColumn("invoice_month", </p>
			<p class="source-code">                                   month("invoice_time"))</p>
			<p class="source-code">month_indexer = StringIndexer(inputCol="invoice_month", </p>
			<p class="source-code">                              outputCol="month_indexed", </p>
			<p class="source-code">                              handleInvalid="skip" )</p>
			<p class="source-code">month_df = month_indexer.fit(month_df).transform(month_df)</p>
			<p class="source-code">month_df.select("invoice_month", "month_indexed").show()</p>
			<p>In the preceding code block, first, we extract the month from the timestamp column using the <strong class="source-inline">month()</strong> function and append it to the DataFrame. Then, we run the new column through the <strong class="source-inline">StringIndexer</strong> estimator and transform the month numeric column into a label index.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor115"/>Assembling individual features into a feature vector</h2>
			<p>Most machine learning algorithms accept a single feature vector as input. Therefore, it would be useful to <a id="_idIndexMarker509"/>combine the individual features that you have extracted and transformed into a single feature vector. This can be accomplished using Spark MLlib's <strong class="source-inline">VectorAssembler</strong> transformer, as shown in the following code example:</p>
			<p class="source-code">from pyspark.ml.feature import VectorAssembler</p>
			<p class="source-code">vec_assembler = VectorAssembler(</p>
			<p class="source-code">    inputCols=["desc_vec", "country_ohe", </p>
			<p class="source-code">               "binarized_price", "month_indexed", </p>
			<p class="source-code">               "quantity_indexed"],</p>
			<p class="source-code">    outputCol="features")</p>
			<p class="source-code">features_df = vec_assembler.transform(month_df)</p>
			<p class="source-code">features_df.select("features").show()</p>
			<p>In the preceding block of code, we initialize the <strong class="source-inline">VectorAssembler</strong> class with input and output <a id="_idIndexMarker510"/>parameters, which results in a transformer object. We make use of the transformer to combine the individual features into a single feature vector. This results in a new column of the vector type being appended to the training DataFrame.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor116"/>Feature scaling</h2>
			<p>It is common for training datasets to have columns with different units of measurements. For instance, while one column uses the metric system of measurement, another column might be using the imperial system. It is also possible for certain columns <a id="_idIndexMarker511"/>to have a high range, such as a column representing dollar amounts than another column representing quantities, for instance. These differences might cause a machine learning model to unduly assign more weightage to a certain value compared to others, which is undesirable and might introduce bias or skew into the model. To overcome this issue, a technique called feature scaling can be utilized. Spark MLlib comes with a few feature scaler algorithms, such as <strong class="source-inline">Normalizer</strong>, <strong class="source-inline">StandardScaler</strong>, <strong class="source-inline">RobustScaler</strong>, <strong class="source-inline">MinMaxScaler</strong>, and <strong class="source-inline">MaxAbsScaler</strong>, built in.</p>
			<p>In the following code example, we will make use of <strong class="source-inline">StandardScaler</strong> to demonstrate how feature scaling can be applied in Apache Spark. <strong class="source-inline">StandardScaler</strong> transforms a feature vector and normalizes each vector to have a unit of standard deviation:</p>
			<p class="source-code">from pyspark.ml.feature import StandardScaler</p>
			<p class="source-code">std_scaler = StandardScaler(inputCol="features", </p>
			<p class="source-code">                            outputCol="scaled_features")</p>
			<p class="source-code">scaled_df = std_scaler.fit(features_df).transform(features_df)</p>
			<p class="source-code">scaled_df.select("scaled_features").show()</p>
			<p>In the preceding block of code, the <strong class="source-inline">StandardScaler</strong> class is initialized with the input and output <a id="_idIndexMarker512"/>column parameters. Then, the <strong class="source-inline">StandardScaler</strong> estimator is applied to the training dataset, resulting in a <strong class="source-inline">StandardScaler</strong> model transformer object. This, in turn, can be applied to the training DataFrame to yield a new DataFrame a new column that contains the normalized features.</p>
			<p>So far, in this section, you have learned how to extract machine learning features from dataset columns. Additionally, you have learned a feature extraction technique to convert text-based columns into feature vectors. Feature transformation techniques for converting categorical, continuous, and date- and time-based variables were also explored. Techniques for combing multiple individual features into a single feature vector were introduced, and, finally, you were also introduced to a feature scaling technique to normalize features.</p>
			<p>In the following section, you will learn techniques in which to reduce the number of features; this is referred to as <strong class="bold">feature selection</strong>.</p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor117"/>Feature selection</h1>
			<p>Feature selection is a technique that involves reducing the number of features in the machine <a id="_idIndexMarker513"/>learning process while leveraging lesser data and also improving the accuracy of the trained model. Feature selection is the process of either automatically or manually selecting only those features that contribute the most to the prediction variable that you are interested in. Feature selection is an important aspect of machine learning, as irrelevant or semi-relevant features can gravely impact model accuracy.</p>
			<p>Apache Spark MLlib comes packaged with a few feature selectors, including <strong class="source-inline">VectorSlicer</strong>, <strong class="source-inline">ChiSqSelector</strong>, <strong class="source-inline">UnivariateFeatureSelector</strong>, and <strong class="source-inline">VarianceThresholdSelector</strong>. Let's explore how to implement feature selection within Apache Spark using the following code example that utilizes <strong class="source-inline">ChiSqSelector</strong> to select the optimal features given the label column that we are trying to predict:</p>
			<p class="source-code">from pyspark.ml.feature import ChiSqSelector</p>
			<p class="source-code">chisq_selector=ChiSqSelector(numTopFeatures=1, </p>
			<p class="source-code">                             featuresCol="scaled_features", </p>
			<p class="source-code">                             outputCol="selected_features", </p>
			<p class="source-code">                             labelCol="cust_age")</p>
			<p class="source-code">result_df = chisq_selector.fit(scaled_df).transform(scaled_df)</p>
			<p class="source-code">result_df.select("selected_features").show()</p>
			<p>In the preceding code block, we initialize <strong class="source-inline">ChiSqSelector</strong> using the input and the output columns. We also specify the label column, as <strong class="source-inline">ChiSqSelector</strong> chooses the optimal features best suited <a id="_idIndexMarker514"/>for predicting the label columns. Then, the <strong class="source-inline">ChiSqSelector</strong> estimator is applied to the training dataset, resulting in a <strong class="source-inline">ChiSqSelector</strong> model transformer object. This, in turn, can be applied to the training DataFrame to yield a new DataFrame column that contains the newly selected features.</p>
			<p>Similarly, we can also leverage <strong class="source-inline">VectorSlicer</strong> to select a subset of features from a given feature vector, as shown in the following code snippet:</p>
			<p class="source-code">from pyspark.ml.feature import VectorSlicer</p>
			<p class="source-code">vec_slicer = VectorSlicer(inputCol="scaled_features", </p>
			<p class="source-code">                          outputCol="selected_features", </p>
			<p class="source-code">                          indices=[1])</p>
			<p class="source-code">result_df = vec_slicer.transform(scaled_df)</p>
			<p class="source-code">result_df.select("scaled_features", </p>
			<p class="source-code">                 "selected_features").display()</p>
			<p>The preceding code block also performs feature selection. However, unlike <strong class="source-inline">ChiSqSelector</strong>, <strong class="source-inline">VectorSlicer</strong> doesn't optimize feature selection for a given variable. Instead, <strong class="source-inline">VectorSlicer</strong> takes a vector column with specified indices. This results in a new vector column whose values are selected through the specified indices. Each feature selector has its own way of making feature selections, and the appropriate feature selector should be used for the given scenario and the problem being solved.</p>
			<p>So far, you have learned how to perform feature extraction from text-based variables and how to perform feature transformation on categorical and continuous types of variables. Additionally, you have <a id="_idIndexMarker515"/>explored the techniques for feature slicing along with feature selection. You have acquired techniques to transform preprocessed raw data into feature vectors that are ready to be fed into a machine learning algorithm in order to build machine learning models.</p>
			<p>However, it seems redundant and time-consuming to perform feature engineering for each and every machine learning problem. So, can you not just use some previously built features for a new model? The answer is yes, and you should reuse some of your previously built features for new machine learning problems. You should also be able to make use of the features of some of your other team members. This can be accomplished via a centralized feature store. We will explore this topic further in the following section.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor118"/>Feature store as a central feature repository</h1>
			<p>A large percentage of the time spent on any machine learning problem is on data cleansing and data wrangling to ensure we build our models on clean and meaningful data. Feature <a id="_idIndexMarker516"/>engineering is another critical process of the machine learning process where data scientists spend a huge chunk of their time curating machine learning features, which happens to be a complex and time-consuming process. It appears counter-intuitive to have to create features again and again for each new machine learning problem.</p>
			<p>Typically, feature engineering takes place on already existing historic data, and new features are perfectly reusable in different machine learning problems. In fact, data scientists spend a good amount of time searching for the right features for the problem at hand. So, it would be tremendously beneficial to have a centralized repository of features that is also <a id="_idIndexMarker517"/>searchable and has metadata to identify features. This central repository of searchable features is generally termed a <strong class="bold">feature store</strong>. A typical feature store architecture is depicted in the following diagram:</p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="Images/B16736_06_05.jpg" alt="Figure 6.5 – The feature store architecture&#13;&#10;" width="1265" height="460"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – The feature store architecture</p>
			<p>Features are useful <a id="_idIndexMarker518"/>not only during the model training phase of the machine learning process, but they are also required during model inferencing. <strong class="bold">Inferencing</strong>, which is also referred to as <strong class="bold">model scoring</strong>, is the process <a id="_idIndexMarker519"/>of feeding an already built model with new and unseen features in order to generate predictions on the new data. Depending on whether the inferencing <a id="_idIndexMarker520"/>process takes place in batch mode or a streaming, real-time fashion, features can be very broadly classified into offline features and online features.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor119"/>Batch inferencing using the offline feature store</h2>
			<p>Offline features, as the name suggests, are generated offline using a batch job. Their consumption also happens offline using either the model training process or model inferencing in a batch <a id="_idIndexMarker521"/>fashion, that is, using scheduled batch machine learning pipelines. These features can be time-consuming to create <a id="_idIndexMarker522"/>and are typically created using big data frameworks, such as Apache Spark, or by running scheduled queries off of a database or a data warehouse.</p>
			<p>The storage mechanism used to generate offline features is referred to as an offline feature store. Historical datastores, RDBMS databases, data warehouse systems, and data lakes <a id="_idIndexMarker523"/>all make good candidates for offline feature stores. It is desirable for an offline feature store to be strongly typed, have a schema enforcement mechanism, and have the ability to store metadata along with the actual <a id="_idIndexMarker524"/>features. Any database or a data warehouse is adequate for an offline feature store; however, in the next section, we will explore Delta Lake as an offline feature store.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor120"/>Delta Lake as an offline feature store</h1>
			<p>In <a href="B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Cleansing and Integration</em>, we established data lakes as the scalable and relatively inexpensive choice for the long-term storage of historical data. Some challenges with <a id="_idIndexMarker525"/>reliability and cloud-based data lakes were presented, and you learned how Delta Lake has been designed to overcome these challenges. The benefits of Delta Lake as an abstraction layer on top of cloud-based data lakes extend beyond just data engineering workloads to data science workloads as well, and we will explore those benefits in this section.</p>
			<p>Delta Lake makes for an ideal candidate for an offline feature store on cloud-based data lakes because of the data reliability features and the novel time travel features that Delta Lake has to offer. We will discuss these in the following sections.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor121"/>Structure and metadata with Delta tables</h2>
			<p>Delta Lake supports structured data with well-defined data types for columns. This makes Delta tables strongly typed, ensuring that all kinds of features of various data types can <a id="_idIndexMarker526"/>be stored in Delta tables. In comparison, the actual storage happens on relatively inexpensive and infinitely scalable cloud-based data lakes. This makes <a id="_idIndexMarker527"/>Delta Lake an ideal candidate offline feature store in the cloud.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor122"/>Schema enforcement and evolution with Delta Lake</h2>
			<p>Delta Lake fully <a id="_idIndexMarker528"/>supports schema enforcement, which means that the <a id="_idIndexMarker529"/>data integrity of features inserted into a Delta Lake feature store is well maintained. This will help to ensure that only the correct data with proper data types will be used for the machine learning model building <a id="_idIndexMarker530"/>process, ensuring model performance. Delta <a id="_idIndexMarker531"/>Lake's support for schema evolution also means that new features could be easily added to a Delta Lake-based feature store.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor123"/>Support for simultaneous batch and streaming workloads</h2>
			<p>Since Delta Lake fully supports unified batch and streaming workloads, data scientists can build near <a id="_idIndexMarker532"/>real-time, streaming feature engineering pipelines in addition to batch pipelines. This will help to train machine learning <a id="_idIndexMarker533"/>models with the freshest features and also generate predictions in a near real-time fashion. This will help to eliminate any operational overhead for use cases with relatively higher latency inferencing requirements by just leveraging Apache Spark's unified analytics engine.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor124"/>Delta Lake time travel</h2>
			<p>Often, data scientists experiment with slight variations of data to improve model accuracy, and they often <a id="_idIndexMarker534"/>maintain several versions of the same physical data for this purpose. With Delta Lake's time travel functionality, a single Delta table can easily support multiple versions of data, thus eliminating the overhead for data scientists in maintaining several physical versions of data.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor125"/>Integration with machine learning operations tools</h2>
			<p>Delta Lake also <a id="_idIndexMarker535"/>supports integration with the <a id="_idIndexMarker536"/>popular machine learning operations and workflow <a id="_idIndexMarker537"/>management tool called <strong class="bold">MLflow</strong>. We will explore MLOps and MLflow in <a href="B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164"><em class="italic">Chapter 9</em></a><em class="italic">, Machine Learning Life Cycle Management</em>.</p>
			<p>A code example of leveraging Delta Lake as an offline feature store is presented here:</p>
			<p class="source-code">spark.sql("CREATE DATABASE feature_store")</p>
			<p class="source-code">(result_df</p>
			<p class="source-code">   .write</p>
			<p class="source-code">   .format("delta")</p>
			<p class="source-code">   .mode("append")</p>
			<p class="source-code">   .option("location", "/FileStore/shared_uploads/delta/retail_features.delta")</p>
			<p class="source-code">   .saveAsTable("feature_store.retail_features"))</p>
			<p>First, we create a database named <strong class="source-inline">feature_store</strong>. Then, we save the DataFrame as a result of the <em class="italic">Feature selection</em> step in the previous section as a Delta table.</p>
			<p>In this way, features can be searched for using simple SQL commands and can also be shared and used <a id="_idIndexMarker538"/>for other machine learning use cases via the shared Hive metastore. Delta Lake also supports common <a id="_idIndexMarker539"/>metadata such as column names and data types, and other metadata such as user notes and comments can be also included to add more context to the features in the feature store.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Most big data platforms, including Databricks, support the built-in Hive metastore for storing table metadata. Additionally, these platforms come with security mechanisms such as databases, tables, and, sometimes, even row- and column-level access control mechanisms. In this way, the feature store can be secured, and features can be selectively shared among data teams.</p>
			<p>In this way, using Delta Lake can serve as an offline feature store on top of cloud-based data lakes. Once features are stored in Delta tables, they are accessible from all of Spark's APIs, including DataFrames and SQL APIs.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">All of the functions and methods present inside Spark MLlib have been designed to be natively scalable. Therefore, any machine learning operation performed using Spark MLlib is inherently scalable and can run Spark jobs with parallel and distributed tasks underneath.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor126"/>Online feature store for real-time inferencing</h2>
			<p>Features that are used in online machine learning inferencing are called online features. Usually, these features <a id="_idIndexMarker540"/>have an ultra-low latency requirement, ranging from milliseconds to mere seconds. Some use cases of online features include real-time predictions in end user applications.</p>
			<p>Let's consider the example of a customer browsing an e-tailer's web app. The customer adds a product to their cart, and based on the customer's zip code, the web app needs to provide an estimated delivery time within seconds. The machine learning model involved here requires a few features to estimate the delivery lead time, such as warehouse location, product availability, historical delivery times from this warehouse, and maybe even the weather and seasonal conditions, but most importantly, it needs the customer zip code. Most of the features could already be precalculated and available in an offline feature store. However, given the low latency requirement for this use case, the feature store must be able to deliver the features with the lowest latency possible. A data lake, or a database or data warehouse, is not an ideal candidate for this use case and requires an ultra-low latency, online feature store.</p>
			<p>From the preceding example, we can conclude that online inferencing in real time requires a few critical components:</p>
			<ul>
				<li>An ultra-low <a id="_idIndexMarker541"/>latency, preferably, in-memory feature store.</li>
				<li>An event processing, low latency streaming engine</li>
				<li>RESTful APIs for integration with the end user web and mobile applications</li>
			</ul>
			<p>An example of a real-time inferencing pipeline is presented in the following diagram:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="Images/B16736_06_06.jpg" alt="Figure 6.6 – A real-time machine learning inferencing pipeline&#13;&#10;" width="1163" height="174"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – A real-time machine learning inferencing pipeline</p>
			<p>In the preceding diagram, data arrives from the web or mobile apps onto a message queue such as Apache Kafka in real time. A low-latency event processing engine such as Apache Flink <a id="_idIndexMarker542"/>processes incoming features and stores them onto a NoSQL database such as Apache Cassandra or in-memory databases such as Redis for online feature stores. A machine learning inference engine fetches the features from the online feature store, generates predictions, and pushes them back to the web or mobile apps via REST APIs.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Neither Apache Spark nor Delta Lake on cloud-based data lakes makes a good candidate for an online <a id="_idIndexMarker543"/>feature store. Spark's Structured Streaming has been designed to handle <a id="_idIndexMarker544"/>high throughput in favor of low latency or processing. Structured Streaming's micro-batch is not suitable to process an event as it arrives at the source. In general, cloud-based data lakes are designed for scalability and have latency specifications, and, therefore, Delta Lake cannot support the ultra-low latency requirements of online feature stores.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor127"/>Summary</h1>
			<p>In this chapter, you learned about the concept of feature engineering and why it is an important part of the whole machine learning process. Additionally, you learned why it is required to create features and train machine learning models.</p>
			<p>You explored various feature engineering techniques such as feature extraction and how they can be used to convert text-based data into features. Feature transformation techniques useful in dealing with categorical and continuous variables were introduced, and examples of how to convert them into features were presented. You also explored feature scaling techniques that are useful for normalizing features to help prevent some features from unduly biasing the trained model.</p>
			<p>Finally, you were introduced to techniques for selecting the right features to optimize the model performance for the label being predicted via feature selection techniques. The skills learned in this chapter will help you to implement scalable and performant feature engineering pipelines using Apache Spark and leveraging Delta Lake as a central, sharable repository of features.</p>
			<p>In the following chapter, you will learn about the various machine learning training algorithms that fall under the supervised learning category. Additionally, you will implement code examples that make use of the features generated in this chapter in order to train actual machine learning models using Apache Spark MLlib.</p>
		</div>
	</div></body></html>