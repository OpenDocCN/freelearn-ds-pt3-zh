["```py\n>>> import random,operator\n```", "```py\n>>> def argmax(seq, fn):\n...     best = seq[0]; best_score = fn(best)\n...     for x in seq:\n...         x_score = fn(x)\n...     if x_score > best_score:\n...         best, best_score = x, x_score\n...     return best\n```", "```py\n>>> def vector_add(a, b):\n...     return tuple(map(operator.add, a, b))\n```", "```py\n>>> orientations = [(1,0), (0, 1), (-1, 0), (0, -1)]\n```", "```py\n>>> def turn_right(orientation):\n...     return orientations[orientations.index(orientation)-1]\n>>> def turn_left(orientation):\n...     return orientations[(orientations.index(orientation)+1) % len(orientations)]\n>>> def isnumber(x):\n...     return hasattr(x, '__int__')\n```", "```py\n>>> class MDP:\n... def __init__(self, init_pos, actlist, terminals, transitions={}, states=None, gamma=0.99):\n...     if not (0 < gamma <= 1):\n...         raise ValueError(\"MDP should have 0 < gamma <= 1 values\")\n...     if states:\n...         self.states = states\n...     else:\n...         self.states = set()\n...         self.init_pos = init_pos\n...         self.actlist = actlist\n...         self.terminals = terminals\n...         self.transitions = transitions\n...         self.gamma = gamma\n...         self.reward = {}\n```", "```py\n... def R(self, state):\n...     return self.reward[state]\n```", "```py\n... def T(self, state, action):\n...     if(self.transitions == {}):\n...         raise ValueError(\"Transition model is missing\")\n...     else:\n...         return self.transitions[state][action]\n```", "```py\n... def actions(self, state):\n...     if state in self.terminals:\n...         return [None]\n...     else:\n...         return self.actlist\n```", "```py\n>>> class GridMDP(MDP):\n... def __init__(self, grid, terminals, init_pos=(0, 0), gamma=0.99):\n```", "```py\n... grid.reverse()\n```", "```py\n... MDP.__init__(self, init_pos, actlist=orientations,\nterminals=terminals, gamma=gamma)\n... self.grid = grid\n... self.rows = len(grid)\n... self.cols = len(grid[0])\n... for x in range(self.cols):\n...     for y in range(self.rows):\n...         self.reward[x, y] = grid[y][x]\n...         if grid[y][x] is not None:\n...             self.states.add((x, y))\n```", "```py\n... def T(self, state, action):\n...     if action is None:\n...         return [(0.0, state)]\n...     else:\n...         return [(0.8, self.go(state, action)),\n...                (0.1, self.go(state, turn_right(action))),\n...                (0.1, self.go(state, turn_left(action)))]\n```", "```py\n... def go(self, state, direction):\n...     state1 = vector_add(state, direction)\n...     return state1 if state1 in self.states else state\n```", "```py\n... def to_grid(self, mapping):\n...     return list(reversed([[mapping.get((x, y), None)\n...                         for x in range(self.cols)]\n...                         for y in range(self.rows)]))\n```", "```py\n... def to_arrows(self, policy):\n...     chars = {(1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1):\n 'v', None: '.'}\n...     return self.to_grid({s: chars[a] for (s, a) in policy.items()})\n```", "```py\n>>> def value_iteration(mdp, epsilon=0.001):\n...     STSN = {s: 0 for s in mdp.states}\n...     R, T, gamma = mdp.R, mdp.T, mdp.gamma\n...     while True:\n...         STS = STSN.copy()\n...         delta = 0\n...         for s in mdp.states:\n...             STSN[s] = R(s) + gamma * max([sum([p * STS[s1] for \n...             (p, s1) in T(s,a)]) for a in mdp.actions(s)])\n...             delta = max(delta, abs(STSN[s] - STS[s]))\n...         if delta < epsilon * (1 - gamma) / gamma:\n...             return STS\n```", "```py\n>>> def best_policy(mdp, STS):\n...     pi = {}\n...     for s in mdp.states:\n...         pi[s] = argmax(mdp.actions(s), lambda a: expected_utility(a, s, STS, mdp))\n...     return pi\n```", "```py\n>>> def expected_utility(a, s, STS, mdp):\n...     return sum([p * STS[s1] for (p, s1) in mdp.T(s, a)])\n```", "```py\n>>> def policy_iteration(mdp):\n...     STS = {s: 0 for s in mdp.states}\n...     pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n...     while True:\n...         STS = policy_evaluation(pi, STS, mdp)\n...         unchanged = True\n...         for s in mdp.states:\n...             a = argmax(mdp.actions(s),lambda a: expected_utility(a, s, STS, mdp))\n...             if a != pi[s]:\n...                 pi[s] = a\n...                 unchanged = False\n...         if unchanged:\n...             return pi\n```", "```py\n>>> def policy_evaluation(pi, STS, mdp, k=20):\n...     R, T, gamma = mdp.R, mdp.T, mdp.gamma\n ..     for i in range(k):\n...     for s in mdp.states:\n...         STS[s] = R(s) + gamma * sum([p * STS[s1] for (p, s1) in T(s, pi[s])])\n...     return STS\n\n>>> def print_table(table, header=None, sep=' ', numfmt='{}'):\n...     justs = ['rjust' if isnumber(x) else 'ljust' for x in table[0]]\n...     if header:\n...         table.insert(0, header)\n...     table = [[numfmt.format(x) if isnumber(x) else x for x in row]\n...             for row in table]\n...     sizes = list(map(lambda seq: max(map(len, seq)),\n...                      list(zip(*[map(str, row) for row in table]))))\n...     for row in table:\n...         print(sep.join(getattr(str(x), j)(size) for (j, size, x)\n...             in zip(justs, sizes, row)))\n```", "```py\n>>> sequential_decision_environment = GridMDP([[-0.02, -0.02, -0.02, +1],\n...                                           [-0.02, None, -0.02, -1],\n...                                           [-0.02, -0.02, -0.02, -0.02]],\n...                                           terminals=[(3, 2), (3, 1)])\n```", "```py\n>>> value_iter = best_policy(sequential_decision_environment,value_iteration (sequential_decision_environment, .01))\n>>> print(\"\\n Optimal Policy based on Value Iteration\\n\")\n>>> print_table(sequential_decision_environment.to_arrows(value_iter))\n```", "```py\n>>> policy_iter = policy_iteration(sequential_decision_environment)\n>>> print(\"\\n Optimal Policy based on Policy Iteration & Evaluation\\n\")\n>>> print_table(sequential_decision_environment.to_arrows(policy_iter))\n```", "```py\n>>> from __future__ import print_function \n>>> import numpy as np \n>>> import matplotlib.pyplot as plt \n>>> from mpl_toolkits.mplot3d import Axes3D \n```", "```py\n>>> ACTION_HIT = 0 \n>>> ACTION_STAND = 1   \n>>> actions = [ACTION_HIT, ACTION_STAND] \n```", "```py\n>>> policyPlayer = np.zeros(22) \n\n>>> for i in range(12, 20): \n...     policyPlayer[i] = ACTION_HIT \n```", "```py\n>>> policyPlayer[20] = ACTION_STAND \n>>> policyPlayer[21] = ACTION_STAND \n```", "```py\n>>> def targetPolicyPlayer(usableAcePlayer, playerSum, dealerCard): \n...     return policyPlayer[playerSum] \n```", "```py\n>>> def behaviorPolicyPlayer(usableAcePlayer, playerSum, dealerCard): \n...     if np.random.binomial(1, 0.5) == 1: \n...         return ACTION_STAND \n...     return ACTION_HIT \n```", "```py\n>>> policyDealer = np.zeros(22) \n>>> for i in range(12, 17): \n...     policyDealer[i] = ACTION_HIT \n>>> for i in range(17, 22): \n...     policyDealer[i] = ACTION_STAND \n```", "```py\n>>> def getCard(): \n...     card = np.random.randint(1, 14) \n...     card = min(card, 10) \n...     return card \n```", "```py\n>>> def play(policyPlayerFn, initialState=None, initialAction=None): \n```", "```py\n...     playerSum = 0 \n...     playerTrajectory = [] \n...     usableAcePlayer = False \n```", "```py\n...     dealerCard1 = 0 \n...     dealerCard2 = 0 \n...     usableAceDealer = False \n\n...     if initialState is None: \n```", "```py\n...         numOfAce = 0 \n```", "```py\n...         while playerSum < 12: \n```", "```py\n...             card = getCard() \n...             if card == 1: \n...                 numOfAce += 1 \n...                 card = 11 \n...                 usableAcePlayer = True \n...             playerSum += card \n```", "```py\n...         if playerSum > 21: \n...             playerSum -= 10 \n...             if numOfAce == 1: \n...                 usableAcePlayer = False \n```", "```py\n...         dealerCard1 = getCard() \n...         dealerCard2 = getCard() \n\n...     else: \n...         usableAcePlayer = initialState[0] \n...         playerSum = initialState[1] \n...         dealerCard1 = initialState[2] \n...         dealerCard2 = getCard() \n```", "```py\n...     state = [usableAcePlayer, playerSum, dealerCard1] \n```", "```py\n...     dealerSum = 0 \n...     if dealerCard1 == 1 and dealerCard2 != 1: \n...         dealerSum += 11 + dealerCard2 \n...         usableAceDealer = True \n...     elif dealerCard1 != 1 and dealerCard2 == 1: \n...         dealerSum += dealerCard1 + 11 \n...         usableAceDealer = True \n...     elif dealerCard1 == 1 and dealerCard2 == 1: \n...         dealerSum += 1 + 11 \n...         usableAceDealer = True \n...     else: \n...         dealerSum += dealerCard1 + dealerCard2 \n```", "```py\n...     while True: \n...         if initialAction is not None: \n...             action = initialAction \n...             initialAction = None \n...         else: \n```", "```py\n...             action = policyPlayerFn(usableAcePlayer, playerSum, dealerCard1) \n```", "```py\n...         playerTrajectory.append([action, (usableAcePlayer, playerSum, dealerCard1)]) \n\n...         if action == ACTION_STAND: \n...             break \n```", "```py\n...         playerSum += getCard() \n```", "```py\n...         if playerSum > 21: \n...             if usableAcePlayer == True: \n...                 playerSum -= 10 \n...                 usableAcePlayer = False \n...             else: \n...                 return state, -1, playerTrajectory \n```", "```py\n...     while True: \n...         action = policyDealer[dealerSum] \n...         if action == ACTION_STAND: \n...             break \n...         dealerSum += getCard() \n...         if dealerSum > 21: \n...             if usableAceDealer == True: \n...                 dealerSum -= 10 \n...                 usableAceDealer = False \n...             else: \n...                 return state, 1, playerTrajectory \n```", "```py\n...     if playerSum > dealerSum: \n...         return state, 1, playerTrajectory \n...     elif playerSum == dealerSum: \n...         return state, 0, playerTrajectory \n...     else: \n...         return state, -1, playerTrajectory \n```", "```py\n>>> def monteCarloOnPolicy(nEpisodes): \n...     statesUsableAce = np.zeros((10, 10)) \n...     statesUsableAceCount = np.ones((10, 10)) \n...     statesNoUsableAce = np.zeros((10, 10)) \n...     statesNoUsableAceCount = np.ones((10, 10)) \n...     for i in range(0, nEpisodes): \n...         state, reward, _ = play(targetPolicyPlayer) \n...         state[1] -= 12 \n...         state[2] -= 1 \n...         if state[0]: \n...             statesUsableAceCount[state[1], state[2]] += 1 \n...             statesUsableAce[state[1], state[2]] += reward \n...         else: \n...             statesNoUsableAceCount[state[1], state[2]] += 1 \n...             statesNoUsableAce[state[1], state[2]] += reward \n...     return statesUsableAce / statesUsableAceCount, statesNoUsableAce / statesNoUsableAceCount \n```", "```py\n>>> def monteCarloES(nEpisodes): \n...     stateActionValues = np.zeros((10, 10, 2, 2)) \n...     stateActionPairCount = np.ones((10, 10, 2, 2)) \n```", "```py\n...     def behaviorPolicy(usableAce, playerSum, dealerCard): \n...         usableAce = int(usableAce) \n...         playerSum -= 12 \n...         dealerCard -= 1 \n...         return np.argmax(stateActionValues[playerSum, dealerCard, usableAce, :] \n                      / stateActionPairCount[playerSum, dealerCard, usableAce, :]) \n```", "```py\n...     for episode in range(nEpisodes): \n...         if episode % 1000 == 0: \n...             print('episode:', episode) \n...         initialState = [bool(np.random.choice([0, 1])), \n...                        np.random.choice(range(12, 22)), \n...                        np.random.choice(range(1, 11))] \n...         initialAction = np.random.choice(actions) \n...         _, reward, trajectory = play(behaviorPolicy, initialState, initialAction) \n...         for action, (usableAce, playerSum, dealerCard) in trajectory: \n...             usableAce = int(usableAce) \n...             playerSum -= 12 \n...             dealerCard -= 1 \n```", "```py\n...             stateActionValues[playerSum, dealerCard, usableAce, action] += reward \n...             stateActionPairCount[playerSum, dealerCard, usableAce, action] += 1 \n...     return stateActionValues / stateActionPairCount \n```", "```py\n>>> figureIndex = 0 \n>>> def prettyPrint(data, tile, zlabel='reward'): \n...     global figureIndex \n...     fig = plt.figure(figureIndex) \n...     figureIndex += 1 \n...     fig.suptitle(tile) \n...     ax = fig.add_subplot(111, projection='3d') \n...     x_axis = [] \n...     y_axis = [] \n...     z_axis = [] \n...     for i in range(12, 22): \n...         for j in range(1, 11): \n...             x_axis.append(i) \n...             y_axis.append(j) \n...             z_axis.append(data[i - 12, j - 1]) \n...     ax.scatter(x_axis, y_axis, z_axis,c='red') \n...     ax.set_xlabel('player sum') \n...     ax.set_ylabel('dealer showing') \n...     ax.set_zlabel(zlabel) \n```", "```py\n>>> def onPolicy(): \n...     statesUsableAce1, statesNoUsableAce1 = monteCarloOnPolicy(10000) \n...     statesUsableAce2, statesNoUsableAce2 = monteCarloOnPolicy(500000) \n...     prettyPrint(statesUsableAce1, 'Usable Ace & 10000 Episodes') \n...     prettyPrint(statesNoUsableAce1, 'No Usable Ace & 10000 Episodes') \n...     prettyPrint(statesUsableAce2, 'Usable Ace & 500000 Episodes') \n...     prettyPrint(statesNoUsableAce2, 'No Usable Ace & 500000 Episodes') \n...     plt.show() \n\n```", "```py\n>>> def MC_ES_optimalPolicy(): \n...     stateActionValues = monteCarloES(500000) \n...     stateValueUsableAce = np.zeros((10, 10)) \n...     stateValueNoUsableAce = np.zeros((10, 10)) \n    # get the optimal policy \n...     actionUsableAce = np.zeros((10, 10), dtype='int') \n...     actionNoUsableAce = np.zeros((10, 10), dtype='int') \n...     for i in range(10): \n...         for j in range(10): \n...             stateValueNoUsableAce[i, j] = np.max(stateActionValues[i, j, 0, :]) \n...             stateValueUsableAce[i, j] = np.max(stateActionValues[i, j, 1, :]) \n...             actionNoUsableAce[i, j] = np.argmax(stateActionValues[i, j, 0, :]) \n...             actionUsableAce[i, j] = np.argmax(stateActionValues[i, j, 1, :]) \n...     prettyPrint(stateValueUsableAce, 'Optimal state value with usable Ace') \n...     prettyPrint(stateValueNoUsableAce, 'Optimal state value with no usable Ace') \n...     prettyPrint(actionUsableAce, 'Optimal policy with usable Ace', 'Action (0 Hit, 1 Stick)') \n...     prettyPrint(actionNoUsableAce, 'Optimal policy with no usable Ace', 'Action (0 Hit, 1 Stick)') \n...     plt.show() \n\n# Run on-policy function \n>>> onPolicy()\n```", "```py\n# Run Monte Carlo Control or Explored starts \n>>> MC_ES_optimalPolicy() \n```", "```py\n# Cliff-Walking - TD learning - SARSA & Q-learning \n>>> from __future__ import print_function \n>>> import numpy as np \n>>> import matplotlib.pyplot as plt \n\n# Grid dimensions \n>>> GRID_HEIGHT = 4 \n>>> GRID_WIDTH = 12 \n\n# probability for exploration, step size,gamma  \n>>> EPSILON = 0.1 \n>>> ALPHA = 0.5 \n>>> GAMMA = 1 \n\n# all possible actions \n>>> ACTION_UP = 0; ACTION_DOWN = 1;ACTION_LEFT = 2;ACTION_RIGHT = 3 \n>>> actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT] \n\n# initial state action pair values \n>>> stateActionValues = np.zeros((GRID_HEIGHT, GRID_WIDTH, 4)) \n>>> startState = [3, 0] \n>>> goalState = [3, 11] \n\n# reward for each action in each state \n>>> actionRewards = np.zeros((GRID_HEIGHT, GRID_WIDTH, 4)) \n>>> actionRewards[:, :, :] = -1.0 \n>>> actionRewards[2, 1:11, ACTION_DOWN] = -100.0 \n>>> actionRewards[3, 0, ACTION_RIGHT] = -100.0 \n\n# set up destinations for each action in each state \n>>> actionDestination = [] \n>>> for i in range(0, GRID_HEIGHT): \n...     actionDestination.append([]) \n...     for j in range(0, GRID_WIDTH): \n...         destinaion = dict() \n...         destinaion[ACTION_UP] = [max(i - 1, 0), j] \n...         destinaion[ACTION_LEFT] = [i, max(j - 1, 0)] \n...         destinaion[ACTION_RIGHT] = [i, min(j + 1, GRID_WIDTH - 1)] \n...         if i == 2 and 1 <= j <= 10: \n...             destinaion[ACTION_DOWN] = startState \n...         else: \n...             destinaion[ACTION_DOWN] = [min(i + 1, GRID_HEIGHT - 1), j] \n...         actionDestination[-1].append(destinaion) \n>>> actionDestination[3][0][ACTION_RIGHT] = startState \n\n# choose an action based on epsilon greedy algorithm \n>>> def chooseAction(state, stateActionValues): \n...     if np.random.binomial(1, EPSILON) == 1: \n...         return np.random.choice(actions) \n...     else: \n...         return np.argmax(stateActionValues[state[0], state[1], :]) \n\n# SARSA update \n\n>>> def sarsa(stateActionValues, expected=False, stepSize=ALPHA): \n...     currentState = startState \n...     currentAction = chooseAction(currentState, stateActionValues) \n...     rewards = 0.0 \n...     while currentState != goalState: \n\n...         newState = actionDestination[currentState[0]][currentState[1]] [currentAction] \n\n...         newAction = chooseAction(newState, stateActionValues) \n...         reward = actionRewards[currentState[0], currentState[1], currentAction] \n...         rewards += reward \n...         if not expected: \n...             valueTarget = stateActionValues[newState[0], newState[1], newAction] \n...         else: \n...             valueTarget = 0.0 \n...             actionValues = stateActionValues[newState[0], newState[1], :] \n...             bestActions = np.argwhere(actionValues == np.max(actionValues)) \n...             for action in actions: \n...                 if action in bestActions: \n\n...                     valueTarget += ((1.0 - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[0], newState[1], action] \n\n...                 else: \n...                     valueTarget += EPSILON / len(actions) * stateActionValues[newState[0], newState[1], action] \n...         valueTarget *= GAMMA \n...         stateActionValues[currentState[0], currentState[1], currentAction] += stepSize * (reward+ valueTarget - stateActionValues[currentState[0], currentState[1], currentAction]) \n...         currentState = newState \n...         currentAction = newAction \n...     return rewards \n\n# Q-learning update \n>>> def qlearning(stateActionValues, stepSize=ALPHA): \n...     currentState = startState \n...     rewards = 0.0 \n...     while currentState != goalState: \n...         currentAction = chooseAction(currentState, stateActionValues) \n...         reward = actionRewards[currentState[0], currentState[1], currentAction] \n...         rewards += reward \n...         newState = actionDestination[currentState[0]][currentState[1]] [currentAction] \n...         stateActionValues[currentState[0], currentState[1], currentAction] += stepSize * (reward + GAMMA * np.max(stateActionValues[newState[0], newState[1], :]) - \n...             stateActionValues[currentState[0], currentState[1], currentAction]) \n...         currentState = newState \n...     return rewards \n\n# print optimal policy \n>>> def printOptimalPolicy(stateActionValues): \n...     optimalPolicy = [] \n...     for i in range(0, GRID_HEIGHT): \n...         optimalPolicy.append([]) \n...         for j in range(0, GRID_WIDTH): \n...             if [i, j] == goalState: \n...                 optimalPolicy[-1].append('G') \n...                 continue \n...             bestAction = np.argmax(stateActionValues[i, j, :]) \n...             if bestAction == ACTION_UP: \n...                 optimalPolicy[-1].append('U') \n...             elif bestAction == ACTION_DOWN: \n...                 optimalPolicy[-1].append('D') \n...             elif bestAction == ACTION_LEFT: \n...                 optimalPolicy[-1].append('L') \n...             elif bestAction == ACTION_RIGHT: \n...                 optimalPolicy[-1].append('R') \n...     for row in optimalPolicy: \n...         print(row) \n\n>>> def SARSAnQLPlot(): \n    # averaging the reward sums from 10 successive episodes \n...     averageRange = 10 \n\n    # episodes of each run \n...     nEpisodes = 500 \n\n    # perform 20 independent runs \n...     runs = 20 \n\n...     rewardsSarsa = np.zeros(nEpisodes) \n...     rewardsQlearning = np.zeros(nEpisodes) \n...     for run in range(0, runs): \n...         stateActionValuesSarsa = np.copy(stateActionValues) \n...         stateActionValuesQlearning = np.copy(stateActionValues) \n...         for i in range(0, nEpisodes): \n            # cut off the value by -100 to draw the figure more elegantly \n...             rewardsSarsa[i] += max(sarsa(stateActionValuesSarsa), -100) \n...             rewardsQlearning[i] += max(qlearning(stateActionValuesQlearning), -100) \n\n    # averaging over independent runs \n...     rewardsSarsa /= runs \n...     rewardsQlearning /= runs \n\n    # averaging over successive episodes \n...     smoothedRewardsSarsa = np.copy(rewardsSarsa) \n...     smoothedRewardsQlearning = np.copy(rewardsQlearning) \n...     for i in range(averageRange, nEpisodes): \n...         smoothedRewardsSarsa[i] = np.mean(rewardsSarsa[i - averageRange: i + 1]) \n...         smoothedRewardsQlearning[i] = np.mean(rewardsQlearning[i - averageRange: i + 1]) \n\n    # display optimal policy \n...     print('Sarsa Optimal Policy:') \n...     printOptimalPolicy(stateActionValuesSarsa) \n...     print('Q-learning Optimal Policy:') \n...     printOptimalPolicy(stateActionValuesQlearning) \n\n    # draw reward curves \n...     plt.figure(1) \n...     plt.plot(smoothedRewardsSarsa, label='Sarsa') \n...     plt.plot(smoothedRewardsQlearning, label='Q-learning') \n...     plt.xlabel('Episodes') \n...     plt.ylabel('Sum of rewards during episode') \n...     plt.legend() \n\n# Sum of Rewards for SARSA versus Qlearning \n>>> SARSAnQLPlot() \n```"]