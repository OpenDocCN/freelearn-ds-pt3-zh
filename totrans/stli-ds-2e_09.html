<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer127">
    <h1 class="chapterNumber">9</h1>
    <h1 class="chapterTitle" id="_idParaDest-102">Connecting to Databases</h1>
    <p class="normal">In the previous chapters we focused entirely on data stored in individual files, but most of the real-world, work-based applications center around data stored in databases. Companies tend to store their data in the cloud, and therefore, being able to perform analyses on this data is a critical skill. In this chapter, we will explore how to access and use data stored in popular databases such as Snowflake and BigQuery. For each database, we’ll connect to the database, write SQL queries, and then make an example app.</p>
    <p class="normal">Whether you are looking to perform ad hoc analysis on large datasets or build data-driven applications, the ability to efficiently retrieve and manipulate data from databases is essential. By the end of this chapter, you will have a strong understanding of how to use Streamlit to connect to and interact with databases, empowering you to extract insights and make data-driven decisions with confidence.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Connecting to Snowflake with Streamlit</li>
      <li class="bulletList">Connecting to BigQuery with Streamlit</li>
      <li class="bulletList">Adding user input to queries</li>
      <li class="bulletList">Organizing queries</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-103">Technical requirements</h1>
    <p class="normal">The following is a list of software and hardware installations that are required for this chapter:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Snowflake account</strong>: To get a <a id="_idIndexMarker338"/>Snowflake account, go to (<a href="https://signup.snowflake.com/"><span class="url">https://signup.snowflake.com/</span></a>) and start a free trial.</li>
      <li class="bulletList"><strong class="keyWord">Snowflake Python Connector</strong>: The Snowflake Python Connector<a id="_idIndexMarker339"/> allows you to run queries from Python. If you installed the requirements for this book, then you already have the library. If not, <code class="inlineCode">pip install</code> <code class="inlineCode">snowflake-connector-python</code> to get started.</li>
      <li class="bulletList"><strong class="keyWord">BigQuery account</strong>: To get a<a id="_idIndexMarker340"/> BigQuery account, go to (<a href="https://console.cloud.google.com/bigquery"><span class="url">https://console.cloud.google.com/bigquery</span></a>) and start a free trial.</li>
      <li class="bulletList"><strong class="keyWord">BigQuery Python Connector</strong>: BigQuery<a id="_idIndexMarker341"/> also has a Python Connector that works the same way as the Snowflake Python Connector does! It also is in the requirements file that you installed at the beginning of the book, but you can also pip install <code class="inlineCode">google-cloud-bigquery</code> if you do not have the library yet.</li>
    </ul>
    <p class="normal">Now that we have everything set up, let’s begin!</p>
    <h1 class="heading-1" id="_idParaDest-104">Connecting to Snowflake with Streamlit</h1>
    <p class="normal">To connect <a id="_idIndexMarker342"/>to any database within Streamlit, we mostly need to think about how to connect to that service <strong class="keyWord">in</strong> Python and then add some Streamlit-specific functions (like caching!) to improve the user experience. Luckily, Snowflake has invested a lot of time in making it incredibly easy to connect to Snowflake from Python; all you need to do is specify your account info and the Snowflake Python connector does the rest.</p>
    <p class="normal">In this chapter, we’ll create and work in a new folder called <code class="inlineCode">database_examples</code> and add a <code class="inlineCode">streamlit_app.py</code> file, along with a Streamlit <code class="inlineCode">secrets</code> file to get started:</p>
    <pre class="programlisting con"><code class="hljs-con">mkdir database_examples
cd database_examples
touch streamlit_app.py
mkdir .streamlit
touch .streamlit/secrets.toml
</code></pre>
    <p class="normal">Within the <code class="inlineCode">secrets.toml</code> file, we need to add our username, password, account, and warehouse. Our username and password are the ones we added when we signed up for our Snowflake account, the warehouse is the virtual computer that Snowflake uses to run the query (the default one is called <code class="inlineCode">COMPUTE_WH</code>), and your account identifier is the only one left! To find your account identifier, the easiest way to find up to date info is through this link (<a href="https://docs.snowflake.com/en/user-guide/admin-account-identifier"><span class="url">https://docs.snowflake.com/en/user-guide/admin-account-identifier</span></a>). Now that we have all the info we need, we can add them to our secrets file! Our file should look like the following, with your info instead of mine.</p>
    <p class="normal">Now that we have<a id="_idIndexMarker343"/> the account info from the result of the SQL query above, we have all the info we need, and we can add it to our <code class="inlineCode">secrets</code> file! Our file should look like the following, with your info instead of mine:</p>
    <pre class="programlisting code"><code class="hljs-code">[snowflake]
user = <span class="hljs-string">"streamlitfordatascience"</span>
password = <span class="hljs-string">"my_password"</span>
account = <span class="hljs-string">"gfa95012"</span>
warehouse = <span class="hljs-string">"COMPUTE_WH"</span>
</code></pre>
    <p class="normal">Now we can start making our Streamlit app. Our first step is going to create our Snowflake connection, run a basic SQL query, and then output that to our Streamlit app:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> snowflake.connector
<span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st
session = snowflake.connector.connect(
    **st.secrets[<span class="hljs-string">"snowflake"</span>], client_session_keep_alive=<span class="hljs-literal">True</span>
)
 
sql_query = <span class="hljs-string">"select 1"</span>
st.write(<span class="hljs-string">"Snowflake Query Result"</span>)
df = session.cursor().execute(sql_query).fetch_pandas_all()
st.write(df)
</code></pre>
    <p class="normal">This code does a few things; first, it uses the Snowflake Python Connector to programmatically connect to our Snowflake account using the secrets in our <code class="inlineCode">secrets</code> file, then it runs the SQL query that just returns <code class="inlineCode">1</code>, and finally, it shows that output in our app.</p>
    <p class="normal">Our app should now look like the following:</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B18444_09_01.png"/></figure>
    <p class="packt_figref">Figure 9.1: Snowflake Query Result</p>
    <p class="normal">Every time we <a id="_idIndexMarker344"/>run this app it will reconnect to Snowflake. This isn’t a great user experience, as it will make our app slower. In the past we would have cached this by wrapping it in a function and caching it with <code class="inlineCode">st.cache_data</code>, but that will actually not work here as the connection is not data. Instead, we should cache it with <code class="inlineCode">st.cache_resource</code>, similar to how we dealt with the HuggingFace model earlier in this book. Our session initialization code should now look like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@st.cache_resource</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">initialize_snowflake_connection</span>():
    session = snowflake.connector.connect(
        **st.secrets[<span class="hljs-string">"snowflake"</span>], client_session_keep_alive=<span class="hljs-literal">True</span>
    )
    <span class="hljs-keyword">return</span> session
 
session = initialize_snowflake_connection()
sql_query = <span class="hljs-string">"select 1"</span>
</code></pre>
    <p class="normal">Now, this connection will run at the beginning of running your app and any subsequent runs will use the cached connection. As a side note, in later versions of Streamlit, you can use the experimental method <code class="inlineCode">st.experimental_connection</code> (<a href="https://docs.streamlit.io/library/api-reference/connections/st.experimental_connection"><span class="url">https://docs.streamlit.io/library/api-reference/connections/st.experimental_connection</span></a>) instead of the earlier code snippet The next improvement will be on the SQL query, which is a test query at the moment. Instead, we can query a dataset called TCP-H, which is included in all new Snowflake accounts by default. It is not particularly important for you to understand how this database works, just for you to understand how you would write a query on your own data. This is a great time to use your own data in Snowflake, if you already have some for a <a id="_idIndexMarker345"/>personal project or for your company! A sample query for us to use looks like this:</p>
    <pre class="programlisting code"><code class="hljs-code">sql_query = <span class="hljs-string">"""</span>
<span class="hljs-string">    SELECT</span>
<span class="hljs-string">    l_returnflag,</span>
<span class="hljs-string">    sum(l_quantity) as sum_qty,</span>
<span class="hljs-string">    sum(l_extendedprice) as sum_base_price</span>
<span class="hljs-string">    FROM</span>
<span class="hljs-string">    snowflake_sample_data.tpch_sf1.lineitem</span>
<span class="hljs-string">    WHERE</span>
<span class="hljs-string">    l_shipdate &lt;= dateadd(day, -90, to_date('1998-12-01'))</span>
<span class="hljs-string">    GROUP BY 1</span>
<span class="hljs-string">"""</span>
</code></pre>
    <p class="normal">Now, our app should look like this: </p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B18444_09_02.png"/></figure>
    <p class="packt_figref">Figure 9.2: SQL GROUPBY</p>
    <p class="normal">Now, we also want to cache the result of the data to speed up our app and reduce the cost. This is something we’ve done before; we can wrap the query call in a function and use <code class="inlineCode">st.cache_data</code> to cache it! It should look like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@st.cache_data</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">run_query</span>(<span class="hljs-params">session, sql_query</span>):
    df = session.cursor().execute(sql_query).fetch_pandas_all()
    <span class="hljs-keyword">return</span> df
df = run_query(session, sql_query)
</code></pre>
    <p class="normal">Our last step for this<a id="_idIndexMarker346"/> app is to dress up the appearance a bit. Right now it’s fairly basic, so we can add a graph, a title, and also what column we should use to graph as the user. Also, we will make sure our results are of the type <code class="inlineCode">float</code> (which is roughly a non-integer number), as a good, general practice:</p>
    <pre class="programlisting code"><code class="hljs-code">df = run_query(session, sql_query)
 
st.title(<span class="hljs-string">"Snowflake TPC-H Explorer"</span>)
col_to_graph = st.selectbox(
    <span class="hljs-string">"Select a column to graph"</span>, [<span class="hljs-string">"Order Quantity"</span>, <span class="hljs-string">"Base Price"</span>]
)
df[<span class="hljs-string">"SUM_QTY"</span>] = df[<span class="hljs-string">"SUM_QTY"</span>].astype(<span class="hljs-built_in">float</span>)
df[<span class="hljs-string">"SUM_BASE_PRICE"</span>] = df[<span class="hljs-string">"SUM_BASE_PRICE"</span>].astype(<span class="hljs-built_in">float</span>)
 
<span class="hljs-keyword">if</span> col_to_graph == <span class="hljs-string">"Order Quantity"</span>:
    st.bar_chart(data=df, 
                 x=<span class="hljs-string">"L_RETURNFLAG"</span>, 
                 y=<span class="hljs-string">"SUM_QTY"</span>)
<span class="hljs-keyword">else</span>:
    st.bar_chart(data=df,
                 x=<span class="hljs-string">"L_RETURNFLAG"</span>, 
                 y=<span class="hljs-string">"SUM_BASE_PRICE"</span>)
</code></pre>
    <p class="normal">Now our app is interactive, and it shows a great graph! It will look like the following:</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B18444_09_03.png"/></figure>
    <p class="packt_figref">Figure 9.3: The TCP-H final app</p>
    <p class="normal">That is it for <a id="_idIndexMarker347"/>our section on connecting to Snowflake with Streamlit! There are currently Snowflake products in preview that let you create Streamlit apps directly inside of Snowflake. If you want access to products like these, reach out to your Snowflake admin and they should be able to help you get access!</p>
    <p class="normal">Now, on to BigQuery!</p>
    <h1 class="heading-1" id="_idParaDest-105">Connecting to BigQuery with Streamlit</h1>
    <p class="normal">The first step to getting <a id="_idIndexMarker348"/>BigQuery connected to your Streamlit app is to gather the authentication information necessary from BigQuery. There is a wonderful Quickstart doc that Google keeps (and maintains!) that you should follow, which can be found here: <a href="https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries"><span class="url">https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries</span></a>. This link will help you sign up for a free account, and create a project. After you create your project, you need to create a service account (<a href="https://console.cloud.google.com/apis/credentials"><span class="url">https://console.cloud.google.com/apis/credentials</span></a>) and download the credentials as a JSON file. Once you have this file, you have all the data needed and can return to this chapter. </p>
    <p class="normal">For this section, we<a id="_idIndexMarker349"/> will create a new file in our <code class="inlineCode">database_example</code> folder called <code class="inlineCode">bigquery_app.py</code>, and we will add a new section to the <code class="inlineCode">secrets.toml</code> file we already created. First, we can add to the <code class="inlineCode">secrets.toml</code> file and finally, let you create and view your service account credentials using this link (<a href="https://console.cloud.google.com/apis/credentials"><span class="url">https://console.cloud.google.com/apis/credentials</span></a>). Go ahead and paste your service account credentials into a new section of your <code class="inlineCode">secrets.toml</code> file like so:</p>
    <pre class="programlisting code"><code class="hljs-code">[bigquery_service_account]
<span class="hljs-built_in">type</span> = <span class="hljs-string">"service_account"</span>
project_id = <span class="hljs-string">"xxx"</span>
private_key_id = <span class="hljs-string">"xxx"</span>
private_key = <span class="hljs-string">"xxx"</span>
client_email = <span class="hljs-string">"xxx"</span>
client_id = <span class="hljs-string">"xxx"</span>
auth_uri = <span class="hljs-string">"https://accounts.google.com/o/oauth2/auth"</span>
token_uri = <span class="hljs-string">"https://oauth2.googleapis.com/token"</span>
auth_provider_x509_cert_url = <span class="hljs-string">"https://www.googleapis.com/oauth2/v1/certs"</span>
client_x509_cert_url = <span class="hljs-string">"</span><span class="hljs-string">xxx"</span>
</code></pre>
    <p class="normal">Now we need to create and open our new app file, called <code class="inlineCode">bigquery_app.py</code>, and connect to BigQuery from there:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st
<span class="hljs-keyword">from</span> google.oauth2 <span class="hljs-keyword">import</span> service_account 
<span class="hljs-keyword">from</span> google.cloud <span class="hljs-keyword">import</span> bigquery 
 
 
credentials = service_account.Credentials.from_service_account_info( 
    st.secrets[<span class="hljs-string">"bigquery_service_account"</span>] 
) 
client = bigquery.Client(credentials=credentials)
</code></pre>
    <p class="normal">Now, when we want to run a query, we can use the client variable that we created with our authentication to run it! To show an example, Google kindly provides a free dataset that stores how often people download Python libraries. We can write a quick query of that dataset that counts the last 5 days of Streamlit downloads in our app, as shown below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st
<span class="hljs-keyword">from</span> google.cloud <span class="hljs-keyword">import</span> bigquery
<span class="hljs-keyword">from</span> google.oauth2 <span class="hljs-keyword">import</span> service_account
 
credentials = service_account.Credentials.from_service_account_info(
    st.secrets[<span class="hljs-string">"bigquery_service_account"</span>]
)
client = bigquery.Client(credentials=credentials)
 
st.title(<span class="hljs-string">"BigQuery App"</span>)
my_first_query = <span class="hljs-string">"""</span>
<span class="hljs-string">    SELECT</span>
<span class="hljs-string">    CAST(file_downloads.timestamp  AS DATE) AS file_downloads_timestamp_date,</span>
<span class="hljs-string">    file_downloads.file.project AS file_downloads_file__project,</span>
<span class="hljs-string">    COUNT(*) AS file_downloads_count</span>
<span class="hljs-string">    FROM 'bigquery-public-data.pypi.file_downloads'</span>
<span class="hljs-string">    	    AS file_downloads</span>
<span class="hljs-string">    WHERE (file_downloads.file.project = 'streamlit')</span>
<span class="hljs-string">AND (file_downloads.timestamp &gt;= timestamp_add(current_timestamp(), INTERVAL -(5) DAY))</span>
<span class="hljs-string">    GROUP BY 1,2</span>
<span class="hljs-string">    """</span>
 
downloads_df = client.query(my_first_query).to_dataframe()
st.write(downloads_df)
</code></pre>
    <p class="normal">When we run<a id="_idIndexMarker350"/> this app, we get the following result:</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B18444_09_04.png"/></figure>
    <p class="packt_figref">Figure 9.4: The BigQuery result</p>
    <p class="normal">In this case, I ran the <a id="_idIndexMarker351"/>query around 8pm PST on March 29<sup class="superscript">th</sup>, which means that parts of the world had already moved on to March 30<sup class="superscript">th</sup> and started downloading libraries. This is the reason for the big drop on the 30<sup class="superscript">th</sup>! Next, as an improvement, we can graph the downloads over time with <code class="inlineCode">st.line_chart()</code>, as we have done quite a few times in this book:</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B18444_09_05.png"/></figure>
    <p class="packt_figref">Figure 9.5: The BigQuery graph</p>
    <p class="normal">As you will notice, it takes quite a while to run these queries. This is because we are caching neither the result nor the connection. Let’s add some functions to do that into our app:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> google.oauth2 <span class="hljs-keyword">import</span> service_account 
<span class="hljs-meta">@st.cache_resource </span>
<span class="hljs-keyword">def</span> <span class="hljs-title">get_bigquery_client</span>(): 
credentials = service_account.Credentials.from_service_account_info(st.secrets[<span class="hljs-string">"bigquery_service_account"</span>])
<span class="hljs-keyword">return</span> bigquery.Client(credentials=credentials) 
client = get_bigquery_client() 
<span class="hljs-meta">@st.cache_data  </span>
<span class="hljs-keyword">def</span> <span class="hljs-title">get_dataframe_from_sql</span>(<span class="hljs-params">query</span>):  
df = client.query(query).to_dataframe() 
    <span class="hljs-keyword">return</span> df
</code></pre>
    <p class="normal">And the bottom of our app will use the new <code class="inlineCode">get_dataframe_from_sql</code> that we’ve just created:</p>
    <pre class="programlisting code"><code class="hljs-code">Downloads_df = get_dataframe_from_sql(my_first_query)
st.line_chart(downloads_df,
x="file_downloads_timestamp_date",
y="file_downloads_count)	
</code></pre>
    <p class="normal">And that is it! Now <a id="_idIndexMarker352"/>you know how to get data from BigQuery and cache the results and the authentication process. This will be extremely useful as you start using Streamlit in work environments, as data rarely lives entirely in .<code class="inlineCode">csv</code> files and instead exists in cloud databases. This next section will cover a couple more strategies to work with queries and databases in Streamlit.</p>
    <h2 class="heading-2" id="_idParaDest-106">Adding user input to queries</h2>
    <p class="normal">One of the major <a id="_idIndexMarker353"/>benefits of using Streamlit is making user<a id="_idIndexMarker354"/> interactivity extremely easy, and we want to enable this while we write the apps that connect to databases. So far, we have written queries that we convert into DataFrames, and on top of these DataFrames, we can add our typical Streamlit widgets to further filter, group by, and then graph our data. However, this situation will only truly work on relatively small datasets, and often, we will have to change the underlying query for better performance in our apps. Let’s prove this point with an example.</p>
    <p class="normal">Let us return to our Streamlit app in <code class="inlineCode">bigquery_app.py</code>. We had a relatively arbitrary lookback period for our app, where we simply pulled the last 5 days in our query. What if we wanted to let the user define the lookback period? If we insisted on not changing the query and filtering after the query ran, then we would have to pull all the data from the <code class="inlineCode">bigquery-public-data.pypi.file_downloads</code> table, which would be extremely slow and cost a huge amount of money. Instead, we can do the following to add a slider that changes the underlying query:</p>
    <pre class="programlisting code"><code class="hljs-code">st.title(<span class="hljs-string">"BigQuery App"</span>)
days_lookback = st.slider(<span class="hljs-string">'How many days of data do you want to see?'</span>, min_value=<span class="hljs-number">1</span>, max_value=<span class="hljs-number">30</span>, value=<span class="hljs-number">5</span>)
my_first_query = <span class="hljs-string">f"""</span>
<span class="hljs-string">    SELECT</span>
<span class="hljs-string">    CAST(file_downloads.timestamp  AS DATE) AS file_downloads_timestamp_date,</span>
<span class="hljs-string">    file_downloads.file.project AS file_downloads_file__project,</span>
<span class="hljs-string">    COUNT(*) AS file_downloads_count</span>
<span class="hljs-string">    FROM 'bigquery-public-data.pypi.file_downloads'</span>
<span class="hljs-string">    AS file_downloads</span>
<span class="hljs-string">    WHERE (file_downloads.file.project = 'streamlit')</span>
<span class="hljs-string">        AND (file_downloads.timestamp &gt;=</span>
<span class="hljs-string">        timestamp_add(current_timestamp(), </span>
<span class="hljs-string">INTERVAL -(</span><span class="hljs-subst">{days_lookback}</span><span class="hljs-string">) DAY))</span>
<span class="hljs-string">    GROUP BY 1,2</span>
<span class="hljs-string">    """</span>
</code></pre>
    <p class="normal">In this situation, we<a id="_idIndexMarker355"/> added a slider that has appropriate minimum and <a id="_idIndexMarker356"/>maximum values, inputting the result of the slider into our query. This will cause the query to rerun every time the slider is moved, but it is still much more efficient than pulling the entire dataset. Now our app should look like this:</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B18444_09_06.png"/></figure>
    <p class="packt_figref">Figure 9.6: Dynamic SQL</p>
    <p class="normal">We could have just as easily added dynamic SQL to our Snowflake queries with the same method, but this shows a wonderful example of it with BigQuery.</p>
    <p class="normal">One word of <a id="_idIndexMarker357"/>warning here is to <strong class="keyWord">never</strong> use text input as input into a <a id="_idIndexMarker358"/>database query. If you allow freeform text as an input and put that into your queries, you functionally give your users the same access to your database that you have. You can use any of the other Streamlit widgets you would like without the same ramification because you have a guarantee of the output of widgets like <code class="inlineCode">st.slider</code>, which will always return a number and never a malicious query.</p>
    <p class="normal">Now that we have learned about adding user input to our queries, we can head over to our last section, organizing queries in Streamlit apps.</p>
    <h2 class="heading-2" id="_idParaDest-107">Organizing queries</h2>
    <p class="normal">As you create more and more <a id="_idIndexMarker359"/>Streamlit apps that rely on database queries, your Streamlit apps often tend to get extremely long and will include long queries stored as strings. This tends to make apps harder to read, and less understandable when collaborating with others. It is not uncommon for the Streamlit Data Team to have half a dozen 30-line queries powering one Streamlit app that we created! There are two strategies to improve this setup:</p>
    <ul>
      <li class="bulletList">Creating downstream tables with a tool like <code class="inlineCode">dbt</code></li>
      <li class="bulletList">Storing queries in separate files </li>
    </ul>
    <p class="normal">We will really only<a id="_idIndexMarker360"/> cover the first of these, creating downstream tables, briefly. As we noticed in the last example, every time the user changed the slider, the query would rerun in the app. This can get rather inefficient! We could use a tool like dbt, which is a very popular tool that lets us schedule SQL queries, to create a smaller table that already had the larger table filtered down to contain only the last 30 days of Streamlit data inside <code class="inlineCode">bigquery-public-data.pypi.file_downloads</code>. This way, our query would be fewer lines and would not crowd our app, and it would also be more cost-effective and fast! We use this tip very often in the Streamlit Data Team, and we often have smaller downstream tables created in dbt that power our Streamlit apps.</p>
    <p class="normal">The second option is to store our queries in entirely separate files, and then import them into our apps. To do this, create a new file called <code class="inlineCode">queries.py</code> in the same directory as our Streamlit app. Inside this file, we want to create a function that returns the <code class="inlineCode">pypi</code> data query that we have already created, with the input to the function being the day filter we need for our app. It should look like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_streamlit_pypi_data</span>(<span class="hljs-params">day_filter</span>):
    streamlit_pypy_query = <span class="hljs-string">f"""</span>
<span class="hljs-string">    SELECT</span>
<span class="hljs-string">    CAST(file_downloads.timestamp  AS DATE) </span>
<span class="hljs-string">        AS file_downloads_timestamp_date,</span>
<span class="hljs-string">    file_downloads.file.project AS</span>
<span class="hljs-string">   file_downloads_file__project,</span>
<span class="hljs-string">    COUNT(*) AS file_downloads_count</span>
<span class="hljs-string">    FROM 'bigquery-public-data.pypi.file_downloads'</span>
<span class="hljs-string">    AS file_downloads</span>
<span class="hljs-string">    WHERE (file_downloads.file.project = 'streamlit')</span>
<span class="hljs-string">        AND (file_downloads.timestamp &gt;=</span>
<span class="hljs-string">        timestamp_add(current_timestamp(), </span>
<span class="hljs-string">        INTERVAL -(</span><span class="hljs-subst">{day_filter}</span><span class="hljs-string">) DAY))</span>
<span class="hljs-string">    GROUP BY 1,2</span>
<span class="hljs-string">    """</span>
    <span class="hljs-keyword">return</span> streamlit_pypy_query
</code></pre>
    <p class="normal">Now, inside our <a id="_idIndexMarker361"/>Streamlit app file, we can import this function from our file and use it like so (I omitted the two cached functions for brevity):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st
<span class="hljs-keyword">from</span> google.cloud <span class="hljs-keyword">import</span> bigquery
<span class="hljs-keyword">from</span> google.oauth2 <span class="hljs-keyword">import</span> service_account
<span class="hljs-keyword">from</span> queries <span class="hljs-keyword">import</span> get_streamlit_pypi_data
...
st.title(<span class="hljs-string">"BigQuery App"</span>)
days_lookback = st.slider(<span class="hljs-string">'How many days of data do you want to see?'</span>, min_value=<span class="hljs-number">1</span>, max_value=<span class="hljs-number">30</span>, value=<span class="hljs-number">5</span>)
pypi_query = get_streamlit_pypi_data(days_lookback)
 
downloads_df = get_dataframe_from_sql(pypi_query)
st.line_chart(downloads_df, x=<span class="hljs-string">"file_downloads_timestamp_date"</span>, y=<span class="hljs-string">"file_downloads_count"</span>)
</code></pre>
    <p class="normal">Perfect! Now our app is much smaller, and the Streamlit sections are logically separated from the query sections of our app. We consistently use strategies like this on the Streamlit Data Team, and we recommend strategies like this to folks who develop Streamlit apps in production.</p>
    <h1 class="heading-1" id="_idParaDest-108">Summary</h1>
    <p class="normal">This concludes <em class="chapterRef">Chapter 9</em>, <em class="italic">Connecting to Databases</em>. In this chapter, we learned a whole host of things, from connecting to Snowflake and BigQuery data in Streamlit to how to cache our queries and our database connections, saving us money and improving the user experience. In the next chapter, we will focus on improving job applications in Streamlit.</p>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask questions to the author, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="https://packt.link/sl"><span class="url">https://packt.link/sl</span></a></p>
    <p class="normal"><img alt="" role="presentation" src="../Images/QR_Code13440134443835796.png"/></p>
  </div>
</body></html>