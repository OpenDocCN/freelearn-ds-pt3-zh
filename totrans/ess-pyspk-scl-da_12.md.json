["```py\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nboston_data = datasets.load_boston()\nboston_pd = pd.DataFrame(boston_data.data, \n                         columns=boston_data.feature_names)\nboston_pd.info()\nboston_pd.head()\nboston_pd.shape\nboston_pd.isnull().sum()\nboston_pd.describe()\n```", "```py\nboston_df = spark.createDataFrame(boston_pd)\nboston_df.show()\nprint((boston_df.count(), len(boston_df.columns)))\nboston_df.where(boston_df.AGE.isNull()).count()\nboston_df.describe().display()\n```", "```py\nimport mlflow\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nX = boston_pd[features]\ny = boston_pd['MEDV']\nwith mlflow.start_run() as run1:\n  lr = LinearRegression()\n  lr_model = lr.fit(X_train,y_train)\n  mlflow.sklearn.log_model(lr_model, \"model\")\n```", "```py\nimport mlflow.pyfunc\nfrom pyspark.sql.functions import struct\nmodel_uri = \"runs:/\" + run1.info.run_id + \"/model\"\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri)\npredicted_df = boston_df.withColumn(\"prediction\", pyfunc_udf(struct('CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO', 'B', 'LSTAT')))\npredicted_df.show()\n```", "```py\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\ndigits_pd = load_digits()\nX = digits_pd.data \ny = digits_pd.target\nparameter_grid = {\"max_depth\": [2, None],\n              \"max_features\": [1, 2, 5],\n              \"min_samples_split\": [2, 3, 5],\n              \"min_samples_leaf\": [1, 2, 5],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"],\n              \"n_estimators\": [5, 10, 15, 20]}\ngrid_search = GridSearchCV(RandomForestClassifier(), \n                           param_grid=parameter_grid)\ngrid_search.fit(X, y) \n```", "```py\nfrom sklearn import grid_search\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\nfrom spark_sklearn import GridSearchCV\ndigits_pd = load_digits()\nX = digits_pd.data \ny = digits_pd.target\nparameter_grid = {\"max_depth\": [2, None],\n              \"max_features\": [1, 2, 5],\n              \"min_samples_split\": [2, 3, 5],\n              \"min_samples_leaf\": [1, 2, 5],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"],\n              \"n_estimators\": [5, 10, 15, 20]}\ngrid_search = grid_search.GridSearchCV(RandomForestClassifier(), \n             param_grid=parameter_grid)\ngrid_search.fit(X, y)\n```", "```py\nimport koalas as ks\nboston_data = load_boston()\nboston_pd = ks.DataFrame(boston_data.data, columns=boston_data.feature_names)\nfeatures = boston_data.feature_names\nboston_pd['MEDV'] = boston_data.target\nboston_pd.info()\nboston_pd.head()\nboston_pd.isnull().sum()\nboston_pd.describe()\n```"]