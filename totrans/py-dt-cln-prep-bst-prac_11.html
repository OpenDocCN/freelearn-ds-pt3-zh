<html><head></head><body>
		<div id="_idContainer114">
			<h1 id="_idParaDest-209" class="chapter-number"><a id="_idTextAnchor246"/><span class="koboSpan" id="kobo.1.1">11</span></h1>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor247"/><span class="koboSpan" id="kobo.2.1">Consuming Time Series Data</span></h1>
			<p><span class="koboSpan" id="kobo.3.1">In this chapter about time series analysis, we will explore the fundamental concepts, methodologies, and practical applications of time series across various industries. </span><span class="koboSpan" id="kobo.3.2">Time series analysis involves studying data points collected over time to identify patterns and trends and </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">make predictions.</span></span></p>
			<p><span class="koboSpan" id="kobo.5.1">In this chapter, we will deep dive into the </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">following topics:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.7.1">Understanding the components of time </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">series data</span></span></li>
				<li><span class="koboSpan" id="kobo.9.1">Types of time </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">series data</span></span></li>
				<li><span class="koboSpan" id="kobo.11.1">Identifying missing values in time </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">series data</span></span></li>
				<li><span class="koboSpan" id="kobo.13.1">Handling missing values in time </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">series data</span></span></li>
				<li><span class="koboSpan" id="kobo.15.1">Analyzing time </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">series data</span></span></li>
				<li><span class="koboSpan" id="kobo.17.1">Dealing </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">with outliers</span></span></li>
				<li><span class="koboSpan" id="kobo.19.1">Feature engineering with time </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">series data</span></span></li>
				<li><span class="koboSpan" id="kobo.21.1">Applying time series techniques in </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">different industries</span></span></li>
			</ul>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor248"/><span class="koboSpan" id="kobo.23.1">Technical requirements</span></h1>
			<p><span class="koboSpan" id="kobo.24.1">The complete code for this chapter can be found in this book’s GitHub repository </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter11"><span class="No-Break"><span class="koboSpan" id="kobo.26.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter11</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.27.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.28.1">Run the following code to install all the necessary libraries we will use in </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">this chapter:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.30.1">
pip install pandas
pip install numpy
pip install matplotlib
pip install statsmodels</span></pre>			<h1 id="_idParaDest-212"><a id="_idTextAnchor249"/><span class="koboSpan" id="kobo.31.1">Understanding the components of time series data</span></h1>
			<p><span class="koboSpan" id="kobo.32.1">Time series data</span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.33.1"> refers to a sequence of observations or measurements that are collected and recorded </span><em class="italic"><span class="koboSpan" id="kobo.34.1">over time</span></em><span class="koboSpan" id="kobo.35.1">. </span><span class="koboSpan" id="kobo.35.2">Unlike non-sequential data, where observations are taken at a single point in time, time series data captures information at multiple points in a sequential order. </span><span class="koboSpan" id="kobo.35.3">Each data point in a time series is associated with a specific timestamp, creating a temporal structure that allows trends, patterns, and dependencies to be analyzed over time. </span><span class="koboSpan" id="kobo.35.4">Let’s discuss the different components of the time series data, starting with </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">the trend.</span></span></p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor250"/><span class="koboSpan" id="kobo.37.1">Trend</span></h2>
			<p><span class="koboSpan" id="kobo.38.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.39.1">trend</span></strong><span class="koboSpan" id="kobo.40.1"> component </span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.41.1">represents the long-term</span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.42.1"> movement or direction in the data. </span><span class="koboSpan" id="kobo.42.2">It reflects the overall pattern that persists over an extended period, indicating whether the values are generally increasing, decreasing, or remaining </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">relatively constant.</span></span></p>
			<p><span class="koboSpan" id="kobo.44.1">Trends have the </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">following characteristics:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.46.1">Upward trend</span></strong><span class="koboSpan" id="kobo.47.1">: Values </span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.48.1">systematically increase </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">over time</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.50.1">Downward trend</span></strong><span class="koboSpan" id="kobo.51.1">: Values</span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.52.1"> systematically decrease </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">over time</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.54.1">Flat trend</span></strong><span class="koboSpan" id="kobo.55.1">: Values </span><a id="_idIndexMarker785"/><span class="koboSpan" id="kobo.56.1">remain relatively constant </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">over time</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.58.1">Identifying the trend is essential for making informed decisions about the long-term behavior of the phenomenon being observed. </span><span class="koboSpan" id="kobo.58.2">It provides insights into the overall direction and can be valuable for forecasting future trends. </span><span class="koboSpan" id="kobo.58.3">In the following section, we will present a use case inspired by the data world that focuses on the </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">trend component.</span></span></p>
			<h3><span class="koboSpan" id="kobo.60.1">Analyzing long-term sales trends</span></h3>
			<p><span class="koboSpan" id="kobo.61.1">In this use case, we aim to analyze a decade-long sales trend to understand the growth pattern of a</span><a id="_idIndexMarker786"/><span class="koboSpan" id="kobo.62.1"> business from 2010 to 2020. </span><span class="koboSpan" id="kobo.62.2">You can find the code for this example in this book’s GitHub repository at </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/trend.py"><span class="koboSpan" id="kobo.63.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/trend.py</span></a><span class="koboSpan" id="kobo.64.1">. </span><span class="koboSpan" id="kobo.64.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.66.1">We will start by generating a date range and corresponding sales data for each month over </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">10 years:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.68.1">
date_rng = pd.date_range(start='2010-01-01', end='2020-12-31', freq='M')
sales_data = pd.Series(range(1, len(date_rng) + 1), index=date_rng)</span></pre></li>				<li><span class="koboSpan" id="kobo.69.1">Then, we must plot the data to visualize the </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">upward trend:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.71.1">
plt.figure(figsize=(10, 5))
plt.plot(sales_data, label='Sales Data')
plt.title('Time Series Data with Trend')
plt.xlabel('Time')
plt.ylabel('Sales')
plt.legend()
plt.show()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.72.1">This will result in the </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">following graph:</span></span></p></li>			</ol>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<span class="koboSpan" id="kobo.74.1"><img src="image/B19801_11_1.jpg" alt="Figure 11.1 – Monthly sales data with upward trend"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.75.1">Figure 11.1 – Monthly sales data with upward trend</span></p>
			<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.76.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.77.1">.1</span></em><span class="koboSpan" id="kobo.78.1"> shows a </span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.79.1">consistent upward trend in sales over the decade. </span><span class="koboSpan" id="kobo.79.2">This indicates that the business has been </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">growing steadily.</span></span></p>
			<p><span class="koboSpan" id="kobo.81.1">In our initial analysis, we focused on understanding the overall upward trend in sales data over a decade. </span><span class="koboSpan" id="kobo.81.2">This provided us with valuable insights into the long-term growth of </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">the business.</span></span></p>
			<p><span class="koboSpan" id="kobo.83.1">Often, businesses experience fluctuations that recur regularly within specific periods, such as months or quarters. </span><span class="koboSpan" id="kobo.83.2">This is known as seasonality. </span><span class="koboSpan" id="kobo.83.3">Recognizing these seasonal patterns can be just as crucial as understanding the overall trend as it helps businesses anticipate and prepare for periods of high or low demand. </span><span class="koboSpan" id="kobo.83.4">To illustrate this, let’s extend our analysis so that it includes seasonality in the </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">sales data.</span></span></p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor251"/><span class="koboSpan" id="kobo.85.1">Seasonality</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.86.1">Seasonality</span></strong><span class="koboSpan" id="kobo.87.1"> refers </span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.88.1">to the repetitive and predictable patterns</span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.89.1"> that occur at regular intervals within the time series. </span><span class="koboSpan" id="kobo.89.2">These patterns often correspond to specific time frames, such as days, months, or seasons, and can be influenced by external factors such as weather, holidays, or </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">cultural events.</span></span></p>
			<p><span class="koboSpan" id="kobo.91.1">Unlike long-term trends, seasonality spans shorter time frames, exerting a short-term influence on the data. </span><span class="koboSpan" id="kobo.91.2">This recurring nature of seasonality allows businesses to anticipate and plan for fluctuations in demand, thereby optimizing their operations </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">and strategies.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.93.1">Important note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.94.1">Understanding seasonality helps in identifying recurring patterns and predicting when certain behaviors or events are likely to occur. </span><span class="koboSpan" id="kobo.94.2">This information is crucial for accurate forecasting </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">and planning.</span></span></p>
			<p><span class="koboSpan" id="kobo.96.1">In the following section, we will extend the sales use case presented previously while focusing on the </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">seasonal component.</span></span></p>
			<h3><span class="koboSpan" id="kobo.98.1">Analyzing long-term sales trends with seasonality</span></h3>
			<p><span class="koboSpan" id="kobo.99.1">In </span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.100.1">this part of the use case, we aim to analyze a decade-long sales trend that includes seasonal variations. </span><span class="koboSpan" id="kobo.100.2">You can find the full code example here: </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/seasonality.py"><span class="koboSpan" id="kobo.101.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/seasonality.py</span></a><span class="koboSpan" id="kobo.102.1">. </span><span class="koboSpan" id="kobo.102.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.104.1">We will start by generating a date range and corresponding sales data for each month over </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">10 years:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.106.1">
date_rng = pd.date_range(start='2010-01-01', end='2020-12-31', freq='M')
seasonal_data = pd.Series([10, 12, 15, 22, 30, 35, 40, 38, 30, 22, 15, 12] * 11, index=date_rng)</span></pre></li>				<li><span class="koboSpan" id="kobo.107.1">Then, we must plot the data to visualize the </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">seasonal component:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<span class="koboSpan" id="kobo.109.1"><img src="image/B19801_11_2.jpg" alt="Figure 11.2 – Monthly sales data with seasonality"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.110.1">Figure 11.2 – Monthly sales data with seasonality</span></p>
			<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.111.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.112.1">.2</span></em><span class="koboSpan" id="kobo.113.1"> reveals a</span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.114.1"> repeating pattern every 12 months, indicating a clear seasonality in sales. </span><span class="koboSpan" id="kobo.114.2">Sales peak around mid-year and drop toward the end and beginning of the year, suggesting higher sales in summer and lower sales in winter. </span><span class="koboSpan" id="kobo.114.3">This pattern’s consistency over the years can aid in predicting future sales cycles. </span><span class="koboSpan" id="kobo.114.4">Understanding these seasonal trends is valuable for inventory management, marketing campaigns, and resource allocation during peak sales periods, allowing businesses to optimize their </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">strategies accordingly.</span></span></p>
			<p><span class="koboSpan" id="kobo.116.1">While identifying trends and seasonality provides valuable insights into sales patterns, real-world data often contains another critical component: noise. </span><span class="koboSpan" id="kobo.116.2">In the following section, we will deep dive into noise and extend the sales use case so that it explores how noise </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">affects sales.</span></span></p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor252"/><span class="koboSpan" id="kobo.118.1">Noise</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.119.1">Noise</span></strong><span class="koboSpan" id="kobo.120.1">, also</span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.121.1"> known</span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.122.1"> as residuals or errors, represents the random fluctuations or irregularities in the time series data that cannot be attributed to the trend or seasonality. </span><span class="koboSpan" id="kobo.122.2">It reflects the variability in the data that is not explained by the </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">underlying patterns.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.124.1">Important note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.125.1">While noise is often considered unwanted, it is a natural part of any real-world data. </span><span class="koboSpan" id="kobo.125.2">Recognizing</span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.126.1"> and isolating noise is essential for building accurate models and understanding the inherent uncertainty in the </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">time series.</span></span></p>
			<p><span class="koboSpan" id="kobo.128.1">In the following section, we will extend the sales use case presented previously while focusing </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">on noise.</span></span></p>
			<h3><span class="koboSpan" id="kobo.130.1">Analyzing sales data with noise</span></h3>
			<p><span class="koboSpan" id="kobo.131.1">In this </span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.132.1">use case, we aim to analyze sales data that includes noise, in addition to trends and seasonality. </span><span class="koboSpan" id="kobo.132.2">This will help us understand how random fluctuations impact our ability to identify underlying patterns. </span><span class="koboSpan" id="kobo.132.3">To follow along with this example, take a look at the following code: </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/noise.py"><span class="koboSpan" id="kobo.133.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/noise.py</span></a><span class="koboSpan" id="kobo.134.1">. </span><span class="koboSpan" id="kobo.134.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.136.1">Let’s import the </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">required libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.138.1">
import pandas as pd
import matplotlib.pyplot as plt</span></pre></li>				<li><span class="koboSpan" id="kobo.139.1">We will start by generating a date range for each month over </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">10 years:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.141.1">
date_rng = pd.date_range(start='2010-01-01', end='2020-12-31', freq='M')</span></pre></li>				<li><span class="koboSpan" id="kobo.142.1">Then, we must create sales data </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">with noise:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.144.1">
np.random.seed(42)
noise_data = pd.Series(np.random.normal(0, 2, len(date_rng)), index=date_rng)</span></pre></li>				<li><span class="koboSpan" id="kobo.145.1"> Now, we must plot the data to visualize </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">the noise:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<span class="koboSpan" id="kobo.147.1"><img src="image/B19801_11_3.jpg" alt="Figure 11.3 – Monthly sales data with noise"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.148.1">Figure 11.3 – Monthly sales data with noise</span></p>
			<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.149.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.150.1">.3</span></em><span class="koboSpan" id="kobo.151.1"> shows</span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.152.1"> random, unpredictable variations that do not follow any specific pattern. </span><span class="koboSpan" id="kobo.152.2">These fluctuations occur over short timeframes, creating instability in the data and making it harder to see </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">any patterns.</span></span></p>
			<p><span class="koboSpan" id="kobo.154.1">Now that we can identify the different time series components, let’s have a look at the different types of </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">time series.</span></span></p>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor253"/><span class="koboSpan" id="kobo.156.1">Types of time series data</span></h1>
			<p><span class="koboSpan" id="kobo.157.1">In this section, we will quickly revise the types of time series data – univariate and multivariate – while clarifying their distinctions and showcasing </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">their applications.</span></span></p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor254"/><span class="koboSpan" id="kobo.159.1">Univariate time series data</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.160.1">Univariate time series data</span></strong><span class="koboSpan" id="kobo.161.1"> consists </span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.162.1">of a single</span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.163.1"> variable or observation recorded over time. </span><span class="koboSpan" id="kobo.163.2">It is a one-dimensional time-ordered sequence, making it simpler to analyze compared to multivariate time </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">series data.</span></span></p>
			<p><span class="koboSpan" id="kobo.165.1">Consider a univariate time series representing the monthly average temperature in a city over several years. </span><span class="koboSpan" id="kobo.165.2">You can find the full code </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/2.types/univariate.py"><span class="No-Break"><span class="koboSpan" id="kobo.167.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/2.types/univariate.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.168.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.169.1">Let’s </span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.170.1">generate </span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.171.1">our univariate time </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">series data:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.173.1">First, we will create the data range we want, in this case from </span><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">2010-01-01</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.175.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">2020-12-31</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.178.1">
date_rng = pd.date_range(start='2010-01-01', end='2020-12-31', freq='M')</span></pre></li>				<li><span class="koboSpan" id="kobo.179.1">Then, we must create the corresponding values for the temperatures by adding noise</span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.180.1"> using a </span><strong class="bold"><span class="koboSpan" id="kobo.181.1">normal distribution</span></strong><span class="koboSpan" id="kobo.182.1"> (also known as a </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">Gaussian distribution):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.184.1">
temperature_data = pd.Series(np.random.normal(20, 5, len(date_rng)), index=date_rng)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.185.1">Let’s understand the </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">value parameters:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">20</span></strong><span class="koboSpan" id="kobo.188.1">: This is the </span><strong class="bold"><span class="koboSpan" id="kobo.189.1">mean</span></strong><span class="koboSpan" id="kobo.190.1"> of the normal distribution. </span><span class="koboSpan" id="kobo.190.2">The noise that’s generated will have an average value </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">of 20.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.192.1">5</span></strong><span class="koboSpan" id="kobo.193.1">: This is</span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.194.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.195.1">standard deviation</span></strong><span class="koboSpan" id="kobo.196.1"> of the normal distribution. </span><span class="koboSpan" id="kobo.196.2">The noise values will typically vary by about ±5 units from the mean. </span><span class="koboSpan" id="kobo.196.3">A larger standard deviation means the noise will be more spread out, while a smaller standard deviation means the noise values are closer to </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">the mean.</span></span></li><li><span class="koboSpan" id="kobo.198.1">The date range we created previously is passed as an index to </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">the DataFrame.</span></span></li></ul></li>				<li><span class="koboSpan" id="kobo.200.1">Now, let’s plot the univariate time </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">series data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.202.1">
plt.figure(figsize=(10, 5))
plt.plot(temperature_data, label='Temperature Data')
plt.title('Univariate Time Series Data')
plt.xlabel('Time')
plt.ylabel('Temperature (°C)')
plt.legend()
plt.show()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.203.1">This will output the </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">following plot:</span></span></p></li>			</ol>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<span class="koboSpan" id="kobo.205.1"><img src="image/B19801_11_4.jpg" alt="Figure 11.4 – Univariate temperature data"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.206.1">Figure 11.4 – Univariate temperature data</span></p>
			<p><span class="koboSpan" id="kobo.207.1">In this</span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.208.1"> example, the </span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.209.1">univariate time series represents the monthly average temperature. </span><span class="koboSpan" id="kobo.209.2">Since the data is randomly generated with a mean of 20°C and some variation (a standard deviation of 5°C), the plot will exhibit random fluctuations around this </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">average temperature.</span></span></p>
			<p><span class="koboSpan" id="kobo.211.1">Understanding the complexities of univariate time series data lays a solid foundation for delving into multivariate time series analysis. </span><span class="koboSpan" id="kobo.211.2">Unlike univariate data, which involves observing a single variable over time, multivariate time series data involves monitoring multiple interrelated </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">variables simultaneously.</span></span></p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor255"/><span class="koboSpan" id="kobo.213.1">Multivariate time series data</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.214.1">Multivariate time series data</span></strong><span class="koboSpan" id="kobo.215.1"> involves </span><a id="_idIndexMarker805"/><span class="koboSpan" id="kobo.216.1">multiple</span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.217.1"> variables or observations recorded over time. </span><span class="koboSpan" id="kobo.217.2">Each variable is a time-ordered sequence, and the variables may be interdependent, capturing more </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">complex relationships.</span></span></p>
			<p><span class="koboSpan" id="kobo.219.1">Consider a multivariate time series representing both the monthly average temperature and monthly rainfall in a city over several years. </span><span class="koboSpan" id="kobo.219.2">You can find the code for this example at </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/2.types/multivariate.py"><span class="koboSpan" id="kobo.220.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/2.types/multivariate.py</span></a><span class="koboSpan" id="kobo.221.1">. </span><span class="koboSpan" id="kobo.221.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.223.1">Let’s add </span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.224.1">the required libraries for </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">this example:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.226.1">
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np</span></pre></li>				<li><span class="koboSpan" id="kobo.227.1">Now, let’s </span><a id="_idIndexMarker808"/><span class="koboSpan" id="kobo.228.1">generate an example of multivariate time series data by using the temperature data we created previously and adding a new time series in the same DataFrame representing the rainfall data with different mean and standard </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">deviation values:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.230.1">
date_rng = pd.date_range(start='2010-01-01', end='2020-12-31', freq='M')
temperature_data = pd.Series(np.random.normal(20, 5, len(date_rng)), index=date_rng)
</span><strong class="bold"><span class="koboSpan" id="kobo.231.1">rainfall_data = pd.Series(np.random.normal(50, 20, len(date_rng)), index=date_rng)</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.232.1">Combine all the time series into the same DataFrame, making sure to include both temperature and </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">rainfall data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.234.1">
multivariate_data = pd.DataFrame({'Temperature': temperature_data, 'Rainfall': rainfall_data})
print(multivariate_data.head())</span></pre></li>				<li><span class="koboSpan" id="kobo.235.1">The combined time series DataFrame is </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">shown here:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.237.1">            Temperature   Rainfall</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.238.1">2010-01-31    19.132623  56.621393</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.239.1">2010-02-28    18.551274  51.249927</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.240.1">2010-03-31    24.502358  65.679049</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.241.1">2010-04-30</span></strong><strong class="bold"><span class="koboSpan" id="kobo.242.1">    27.069077  73.044307</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.243.1">2010-05-31    21.176376  41.317497</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.244.1">Finally, let’s plot the multivariate time </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">series data:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<span class="koboSpan" id="kobo.246.1"><img src="image/B19801_11_5.jpg" alt="Figure 11.5 – Multivariate data"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.247.1">Figure 11.5 – Multivariate data</span></p>
			<p><span class="koboSpan" id="kobo.248.1">In this </span><a id="_idIndexMarker809"/><span class="koboSpan" id="kobo.249.1">example, the multivariate time series includes </span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.250.1">both temperature and rainfall data, providing a more comprehensive view of </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">environmental conditions.</span></span></p>
			<p><span class="koboSpan" id="kobo.252.1">Overall, univariate data is simpler to work with, while multivariate data allows us to capture more complex relationships and dependencies between variables over time. </span><span class="koboSpan" id="kobo.252.2">Multivariate analysis is essential for tackling real-world challenges in diverse fields such as economics, finance, environmental science, and healthcare, where understanding multifaceted relationships among variables </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">is crucial.</span></span></p>
			<p><span class="koboSpan" id="kobo.254.1">Now that we have a strong understanding of time series data, we can explore methods for cleaning and managing this type of </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">data effectively.</span></span></p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor256"/><span class="koboSpan" id="kobo.256.1">Identifying missing values in time series data</span></h1>
			<p><span class="koboSpan" id="kobo.257.1">Identifying </span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.258.1">missing values in time series data </span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.259.1">is somewhat like identifying missing values in other types of data, but there are specific considerations due to the temporal nature of time series. </span><span class="koboSpan" id="kobo.259.2">Since we covered some of these techniques in </span><a href="B19801_08.xhtml#_idTextAnchor195"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.260.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.261.1">, </span><em class="italic"><span class="koboSpan" id="kobo.262.1">Detecting and Handling Missing Values and Outliers</span></em><span class="koboSpan" id="kobo.263.1">, let’s summarize them here and highlight their specific adaptations for analyzing time series data using a stock market analysis </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">use case.</span></span></p>
			<p><span class="koboSpan" id="kobo.265.1">Let’s consider a use case where we have daily stock prices (open, high, low, and close) for a particular company over several years. </span><span class="koboSpan" id="kobo.265.2">Our goal is to identify missing data in this time series to ensure the integrity of the dataset. </span><span class="koboSpan" id="kobo.265.3">You can find the code for this example </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/1.identify_missing_values.py"><span class="No-Break"><span class="koboSpan" id="kobo.267.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/1.identify_missing_values.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.268.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.269.1">Let’s start by generating </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">the data:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.271.1">First, we will generate a date range for business days from January 1, 2020, to December 31, 2023. </span><span class="koboSpan" id="kobo.271.2">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.272.1">freq='B'</span></strong><span class="koboSpan" id="kobo.273.1"> is used to generate a range of dates that includes only </span><em class="italic"><span class="koboSpan" id="kobo.274.1">business days</span></em><span class="koboSpan" id="kobo.275.1"> (weekdays, </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">excluding weekends):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.277.1">
date_range = pd.date_range(start='2020-01-01', end='2023-12-31', </span><strong class="source-inline"><span class="koboSpan" id="kobo.278.1">freq='B'</span></strong><span class="koboSpan" id="kobo.279.1">)  # Business days</span></pre></li>				<li><span class="koboSpan" id="kobo.280.1">Next, we must generate random stock prices for the date range with a length </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">of </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.282.1">n</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.284.1">
n = len(date_range)
data = {
    'open': np.random.uniform(100, 200, n),
    'high': np.random.uniform(200, 300, n),
    'low': np.random.uniform(50, 100, n),
    'close': np.random.uniform(100, 200, n)
}</span></pre></li>				<li><span class="koboSpan" id="kobo.285.1">Next, we must create a DataFrame by passing all the separate data points that were created in the </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">previous step:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.287.1">
df = pd.DataFrame(data, index=date_range)</span></pre></li>				<li><span class="koboSpan" id="kobo.288.1">Now, let’s introduce random NaN values to simulate some missing values in </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">the data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.290.1">
nan_indices = np.random.choice(n, size=100, replace=False)
df.iloc[nan_indices] = np.nan</span></pre></li>				<li><span class="koboSpan" id="kobo.291.1">Next, drop </span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.292.1">random dates to simulate</span><a id="_idIndexMarker814"/> <span class="No-Break"><span class="koboSpan" id="kobo.293.1">missing timestamps:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.294.1">
missing_dates = np.random.choice(date_range, size=50, replace=False)</span></pre></li>				<li><span class="koboSpan" id="kobo.295.1">Finally, display the first few rows of </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">the DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.297.1">
                  open        high        low       close
2020-01-01  137.454012  262.589138  55.273685  183.849183
2020-01-02  195.071431  288.597775  82.839005  180.509032
2020-01-03  173.199394  261.586319  91.105158  182.298381
2020-01-06  159.865848  223.295947  69.021000  193.271051
2020-01-07         NaN         NaN        NaN         NaN</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.298.1">The key thing to notice here is that we have two kinds of </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">missing data:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.300.1">Complete rows missing, so one full date index is </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">not available</span></span></li>
				<li><span class="koboSpan" id="kobo.302.1">Some observations in some of the columns are missing for the </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">current date</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.304.1">We will mainly address the first case here as we covered the second case in </span><a href="B19801_08.xhtml#_idTextAnchor195"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.305.1">Chapter 8</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.306.1">, Detecting and Handling Missing Values and Outliers</span></em><span class="koboSpan" id="kobo.307.1">. </span><span class="koboSpan" id="kobo.307.2">Let’s start with the simple but effective </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.308.1">isnull()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.309.1"> method.</span></span></p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor257"/><span class="koboSpan" id="kobo.310.1">Checking for NaNs or null values</span></h2>
			<p><span class="koboSpan" id="kobo.311.1">Unlike regular datasets, time </span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.312.1">series data points are ordered in time. </span><span class="koboSpan" id="kobo.312.2">Missing values can break the continuity and affect the analysis of trends and seasonal patterns. </span><span class="koboSpan" id="kobo.312.3">Let’s use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.313.1">isnull()</span></strong><span class="koboSpan" id="kobo.314.1"> method to identify missing timestamps. </span><span class="koboSpan" id="kobo.314.2">Here, we are looking to find complete rows that are missing from </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">the dataset:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.316.1">To check which dates are missing from a time series DataFrame, we need to create a full date range (with no missing values) in the frequency of our current DataFrame index and compare it against the date range we have in our current DataFrame. </span><span class="koboSpan" id="kobo.316.2">Here, we are creating a complete date range for </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">business days:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.318.1">
complete_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='B')</span></pre></li>				<li><span class="koboSpan" id="kobo.319.1">To quickly see the missing index points, the DataFrame must be reindexed to this complete date range so that we can identify any </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">missing timestamps:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.321.1">
df_reindexed = df.reindex(complete_index)</span></pre></li>				<li><span class="koboSpan" id="kobo.322.1">Now, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.323.1">isnull()</span></strong><span class="koboSpan" id="kobo.324.1"> method to identify any </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">missing timestamps:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.326.1">
missing_timestamps = df_reindexed[df_reindexed.isnull().any(axis=1)]</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.327.1">Here, we can see that there are some missing timestamps in </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">the data:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.329.1">
print(f"\nPercentage of Missing Timestamps: {missing_timestamps_percentage:.2f}%")
Percentage of Missing Timestamps: </span><strong class="bold"><span class="koboSpan" id="kobo.330.1">14.09%</span></strong></pre>			<p><span class="koboSpan" id="kobo.331.1">The analysis so far tells us that we have complete dates missing from the dataset. </span><span class="koboSpan" id="kobo.331.2">Now, let’s add some visual plots to help us better see the gaps in </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">the data.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.333.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.334.1">As presented in </span><a href="B19801_08.xhtml#_idTextAnchor195"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.335.1">Chapter 8</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.336.1">, Detecting and Handling Missing Values and Outliers</span></em><span class="koboSpan" id="kobo.337.1">, you can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.338.1">isnull()</span></strong><span class="koboSpan" id="kobo.339.1"> method to see how many nulls we have in each column – for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">missing_values = </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.341.1">df.isnull().sum()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">.</span></span></p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor258"/><span class="koboSpan" id="kobo.343.1">Visual inspection</span></h2>
			<p><span class="koboSpan" id="kobo.344.1">Visualizing the data </span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.345.1">can help us identify missing values and patterns of missingness. </span><span class="koboSpan" id="kobo.345.2">Plots can reveal gaps in data that are not immediately obvious from a </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">tabular inspection.</span></span></p>
			<p><span class="koboSpan" id="kobo.347.1">Continuing with the example from the previous section, let’s plot our time series data and mark any missing values on </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">our graph:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.349.1">Plot the </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">closing prices:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.351.1">
plt.figure(figsize=(14, 7))
plt.plot(df.index, df['close'], linestyle='-', label='Closing Price', color='blue')</span></pre></li>				<li><span class="koboSpan" id="kobo.352.1">Mark missing timestamps with </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">vertical lines:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.354.1">
for date in missing_dates:
    plt.axvline(x=date, color='red', linestyle='--', linewidth=1)
plt.title('Daily Closing Prices with Missing Timestamps and NaN Values Highlighted')
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.legend()
plt.grid(True)
plt.show()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.355.1">This will result in the </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">following plot:</span></span></p></li>			</ol>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<span class="koboSpan" id="kobo.357.1"><img src="image/B19801_11_6.jpg" alt="Figure 11.6 – Daily closing prices with missing timestamps highlighted"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.358.1">Figure 11.6 – Daily closing prices with missing timestamps highlighted</span></p>
			<p><span class="koboSpan" id="kobo.359.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.360.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.361.1">.6</span></em><span class="koboSpan" id="kobo.362.1">, the </span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.363.1">closing prices are plotted in blue with markers, while missing timestamps are highlighted with dotted lines, making it easy to see gaps in the data. </span><span class="koboSpan" id="kobo.363.2">Now, let’s explore the final method, known as lagged analysis. </span><span class="koboSpan" id="kobo.363.3">In this method, we create a lagged version of the series and compare it with the original to </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">detect inconsistencies.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.365.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.366.1">In </span><a href="B19801_03.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.367.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.368.1">, </span><em class="italic"><span class="koboSpan" id="kobo.369.1">Data Profiling – Understanding Data Structure, Quality, and Distribution</span></em><span class="koboSpan" id="kobo.370.1"> we demonstrated various data profiling methods. </span><span class="koboSpan" id="kobo.370.2">You can apply similar techniques to time series data by using the built-in gap analysis feature. </span><span class="koboSpan" id="kobo.370.3">Simply pass </span><strong class="source-inline"><span class="koboSpan" id="kobo.371.1">tsmode=True</span></strong><span class="koboSpan" id="kobo.372.1"> when creating the profile report – for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.373.1">profile = </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">ProfileReport(df, tsmode=True)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.376.1">As we move forward, it’s essential to explore effective strategies for handling missing data in </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">time series.</span></span></p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor259"/><span class="koboSpan" id="kobo.378.1">Handling missing values in time series data</span></h1>
			<p><span class="koboSpan" id="kobo.379.1">Missing values </span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.380.1">are a common challenge in time series data and can arise due to various reasons, such as sensor failures, data transmission issues, or simply the absence of recorded observations. </span><span class="koboSpan" id="kobo.380.2">As we’ve discussed, two main scenarios </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">often arise:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.382.1">Some null values in features</span></strong><span class="koboSpan" id="kobo.383.1">: Imagine a stock market analysis where daily trading data is collected. </span><span class="koboSpan" id="kobo.383.2">While all trading days are accounted for, the volume of shares traded on certain days may be missing due to reporting errors. </span><span class="koboSpan" id="kobo.383.3">This scenario presents a challenge: how do you maintain the integrity of the dataset while ensuring that analyses </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">remain robust?</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.385.1">Complete rows are missing</span></strong><span class="koboSpan" id="kobo.386.1">: Conversely, consider a weather monitoring system that records daily temperatures. </span><span class="koboSpan" id="kobo.386.2">If entire days of data are missing – perhaps due to sensor failures – this poses a significant issue. </span><span class="koboSpan" id="kobo.386.3">Missing timestamps means you cannot simply fill in values; the absence of data for those days disrupts the entire </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">time series.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.388.1">In the next section, we will focus on solving the first scenario and consider the existence of null values in some features. </span><span class="koboSpan" id="kobo.388.2">Once we have done this, we can adjust it for the </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">second one.</span></span></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor260"/><span class="koboSpan" id="kobo.390.1">Removing missing data</span></h2>
			<p><span class="koboSpan" id="kobo.391.1">Removing missing data</span><a id="_idIndexMarker819"/><span class="koboSpan" id="kobo.392.1"> is a straightforward approach, but it should be done with caution while considering the impact on the overall dataset. </span><span class="koboSpan" id="kobo.392.2">Here are some scenarios where removal might </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">be appropriate:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.394.1">If the missing values constitute a small percentage of the dataset (for example, less than 5%), removing them might be feasible. </span><span class="koboSpan" id="kobo.394.2">This approach works well if the data loss does not significantly impact the analysis or the conclusions drawn from the dataset. </span><span class="koboSpan" id="kobo.394.3">For example, in a dataset with 10,000 time points, if 50 points are missing, removing these 50 points (0.5% of the data) might not significantly affect the </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">overall analysis.</span></span></li>
				<li><span class="koboSpan" id="kobo.396.1">If imputing missing values would introduce too much uncertainty, especially if the values are critical and cannot be accurately estimated. </span><span class="koboSpan" id="kobo.396.2">This scenario is common when the missing values are highly unpredictable data, making </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">imputation unreliable.</span></span></li>
				<li><span class="koboSpan" id="kobo.398.1">If missing </span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.399.1">values occur completely at random and do not follow any systematic pattern. </span><span class="koboSpan" id="kobo.399.2">An example of this is sensor data where occasional random failures cause missing readings, but there is no underlying pattern to </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">these failures.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.401.1">Let’s revisit the stock market use case and see how we can drop the null values to see what effect this has on </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">the dataset.</span></span></p>
			<h3><span class="koboSpan" id="kobo.403.1">Removing missing data in the stock market use case</span></h3>
			<p><span class="koboSpan" id="kobo.404.1">In our </span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.405.1">stock prices data scenario, we will add some NaN values and evaluate the impact of removing them. </span><span class="koboSpan" id="kobo.405.2">You can find the full code here: </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/2.remove_missing_values.py"><span class="koboSpan" id="kobo.406.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/2.remove_missing_values.py</span></a><span class="koboSpan" id="kobo.407.1">. </span><span class="koboSpan" id="kobo.407.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.409.1">Continuing with the example from the previous section, we will create the stock data with the different features. </span><span class="koboSpan" id="kobo.409.2">Then, we will randomly select some indexes from specific features (for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.410.1">close</span></strong><span class="koboSpan" id="kobo.411.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">open</span></strong><span class="koboSpan" id="kobo.413.1">) so that we can map the values for each feature of that index to a </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">NaN value:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.415.1">
nan_indices_close = np.random.choice(df.index, size=50, replace=False)
nan_indices_open = np.random.choice(df.index, size=50, replace=False)</span></pre></li>				<li><span class="koboSpan" id="kobo.416.1">Then, we will map the indices that were randomly selected before to </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">NaN values:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.418.1">
df.loc[nan_indices_close, 'close'] = np.nan
df.loc[nan_indices_open, 'open'] = np.nan</span></pre></li>				<li><span class="koboSpan" id="kobo.419.1">Let’s check how many NaNs or null values are available in </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">the data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.421.1">
missing_values = df.isnull().sum()
Percentage of Missing Values in Each Column:
open     4.793864
high     0.000000
low      0.000000
close    4.793864</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.422.1">As expected, some null values were introduced on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.423.1">open</span></strong><span class="koboSpan" id="kobo.424.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">close</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.426.1"> features.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.427.1">Let’s check</span><a id="_idIndexMarker822"/><span class="koboSpan" id="kobo.428.1"> how many rows we have before removing any based on the nulls we have in </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">the dataset:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.430.1">print(f"\nNumber of rows before dropping NaN values: {len(df)}")
Number of rows before dropping NaN values: </span><strong class="source-inline"><span class="koboSpan" id="kobo.431.1">1043</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.432.1">At this stage, we will drop any rows that have NaN values in either the </span><strong class="source-inline"><span class="koboSpan" id="kobo.433.1">close</span></strong><span class="koboSpan" id="kobo.434.1"> or </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.435.1">low</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.436.1"> columns:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.437.1">
df_cleaned = df.dropna()
print(f"\nNumber of rows after dropping NaN values: {len(df_cleaned)}")
----
Number of rows after dropping NaN values: </span><strong class="bold"><span class="koboSpan" id="kobo.438.1">945</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.439.1">Let’s plot the time </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">series data:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<span class="koboSpan" id="kobo.441.1"><img src="image/B19801_11_7.jpg" alt="Figure 11.7 – Daily closing prices with missing data to be dropped/flagged"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.442.1">Figure 11.7 – Daily closing prices with missing data to be dropped/flagged</span></p>
			<p><span class="koboSpan" id="kobo.443.1">As shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.444.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.445.1">.7</span></em><span class="koboSpan" id="kobo.446.1">, the original closing prices are plotted, and points that were dropped </span><a id="_idIndexMarker823"/><span class="koboSpan" id="kobo.447.1">due to missing values are highlighted with red “x” markers. </span><span class="koboSpan" id="kobo.447.2">Remember that even with selective dropping, removing rows can lead to a loss of useful information as it reduces the sample size, which can decrease the statistical power of the analysis and affect the generalizability of </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">the results.</span></span></p>
			<p><span class="koboSpan" id="kobo.449.1">In scenarios where retaining every timestamp is crucial but missing values within features need to be addressed, forward and backward filling offer practical solutions. </span><span class="koboSpan" id="kobo.449.2">These methods allow us to maintain the chronological integrity of time series data while efficiently filling in missing values based on adjacent observations. </span><span class="koboSpan" id="kobo.449.3">Let’s explore how forward and backward filling can effectively handle missing data in time </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">series analyses.</span></span></p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor261"/><span class="koboSpan" id="kobo.451.1">Forward and backward fill</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.452.1">Forward fill</span></strong><span class="koboSpan" id="kobo.453.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.454.1">ffill</span></strong><span class="koboSpan" id="kobo.455.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.456.1">backward fill</span></strong><span class="koboSpan" id="kobo.457.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.458.1">bfill</span></strong><span class="koboSpan" id="kobo.459.1">) are </span><a id="_idIndexMarker824"/><span class="koboSpan" id="kobo.460.1">methods of </span><a id="_idIndexMarker825"/><span class="koboSpan" id="kobo.461.1">imputing missing values by propagating the last</span><a id="_idIndexMarker826"/><span class="koboSpan" id="kobo.462.1"> known value forward or the </span><a id="_idIndexMarker827"/><span class="koboSpan" id="kobo.463.1">next known value backward in the time </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">series, respectively.</span></span></p>
			<p><span class="koboSpan" id="kobo.465.1">When dealing with time series backfilling, the choice between ffill and bfill depends on several factors and use cases. </span><span class="koboSpan" id="kobo.465.2">Here’s an overview of when to use each approach and the thought process behind</span><a id="_idIndexMarker828"/> <span class="No-Break"><span class="koboSpan" id="kobo.466.1">these decisions:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.467.1">Ffill</span></strong><span class="koboSpan" id="kobo.468.1">: Forward filling, also </span><a id="_idIndexMarker829"/><span class="koboSpan" id="kobo.469.1">known as </span><strong class="bold"><span class="koboSpan" id="kobo.470.1">last observation carried forward</span></strong><span class="koboSpan" id="kobo.471.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.472.1">LOCF</span></strong><span class="koboSpan" id="kobo.473.1">), propagates</span><a id="_idIndexMarker830"/><span class="koboSpan" id="kobo.474.1"> the last known value forward to fill in missing </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">data points.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.476.1">Here’s when you should </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">use it:</span></span></p><ul><li><span class="koboSpan" id="kobo.478.1">When you believe the most recent known value is the best predictor of missing </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">future values</span></span></li><li><span class="koboSpan" id="kobo.480.1">In financial time series, where carrying forward the last known price is often a </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">reasonable assumption</span></span></li><li><span class="koboSpan" id="kobo.482.1">When dealing with slowly changing variables where persistence is a </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">good assumption</span></span></li><li><span class="koboSpan" id="kobo.484.1">In scenarios where you want to maintain the most recent state until new information </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">becomes available</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.486.1">If you’re still uncertain or find yourself pondering which method to use, answering “yes” to at least two of the following three questions can guide you in the </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">right direction:</span></span></p><ul><li><span class="koboSpan" id="kobo.488.1">Is the variable likely to remain relatively stable over </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">short periods?</span></span></li><li><span class="koboSpan" id="kobo.490.1">Would using the last known value be a reasonable assumption for the </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">missing data?</span></span></li><li><span class="koboSpan" id="kobo.492.1">Is it more important to reflect the most recent known state rather than potential </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">future changes?</span></span></li></ul></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.494.1">Bfill</span></strong><span class="koboSpan" id="kobo.495.1">, on the </span><a id="_idIndexMarker831"/><span class="koboSpan" id="kobo.496.1">other </span><a id="_idIndexMarker832"/><span class="koboSpan" id="kobo.497.1">hand, propagates the next known value backward to fill in missing </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">data points.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.499.1">Here’s when you should </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">use it:</span></span></p><ul><li><span class="koboSpan" id="kobo.501.1">When you</span><a id="_idIndexMarker833"/><span class="koboSpan" id="kobo.502.1"> have more confidence in future values than </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">past values</span></span></li><li><span class="koboSpan" id="kobo.504.1">In scenarios where you want to retroactively apply known outcomes to previous </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">missing periods</span></span></li><li><span class="koboSpan" id="kobo.506.1">When you’re dealing with lagged effects where future events influence past </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">missing data</span></span></li><li><span class="koboSpan" id="kobo.508.1">In cases where you want to align data with the next known state rather than the </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">previous one</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.510.1">If you’re still </span><a id="_idIndexMarker834"/><span class="koboSpan" id="kobo.511.1">uncertain or find yourself pondering which method to use, answering “yes” to the following questions will guide you in the </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">right direction:</span></span></p><ul><li><span class="koboSpan" id="kobo.513.1">Is the next known value likely to be more representative of the missing data than the previous </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">known value?</span></span></li><li><span class="koboSpan" id="kobo.515.1">Are you dealing with a scenario where future information should inform past </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">missing values?</span></span></li><li><span class="koboSpan" id="kobo.517.1">Would aligning with the next known state provide more meaningful insights for </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">your analysis?</span></span></li></ul></li>
			</ul>
			<p><span class="koboSpan" id="kobo.519.1">In practice, choosing between ffill and bfill often requires a combination of domain expertise, understanding of the data generation process, and consideration of the specific analytical goals. </span><span class="koboSpan" id="kobo.519.2">It’s also worth experimenting with both methods and comparing the results to see which provides more meaningful and accurate insights for your particular </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">use case.</span></span></p>
			<p><span class="koboSpan" id="kobo.521.1">As always, there are some</span><a id="_idIndexMarker835"/><span class="koboSpan" id="kobo.522.1"> important considerations when using ffill and bfill in time series data. </span><span class="koboSpan" id="kobo.522.2">Let’s expand </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">on those:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.524.1">Sequential nature</span></strong><span class="koboSpan" id="kobo.525.1">: The </span><a id="_idIndexMarker836"/><span class="koboSpan" id="kobo.526.1">sequential nature of time series data is indeed crucial for both ffill and bfill methods. </span><span class="koboSpan" id="kobo.526.2">Both methods rely on the assumption that adjacent data points are related, which is fundamental to time </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">series analysis.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.528.1">Ffill and increasing trends</span></strong><span class="koboSpan" id="kobo.529.1">: Ffill can be appropriate for increasing trends as it carries forward the last known value, potentially underestimating the true value in an upward trend. </span><span class="koboSpan" id="kobo.529.2">However, it may lead to a “staircase” effect in strongly increasing trends, potentially understating the rate </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">of increase.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.531.1">Bfill and decreasing trends</span></strong><span class="koboSpan" id="kobo.532.1">: Bfill can be suitable for decreasing trends as it pulls back future </span><a id="_idIndexMarker837"/><span class="koboSpan" id="kobo.533.1">lower values, potentially</span><a id="_idIndexMarker838"/><span class="koboSpan" id="kobo.534.1"> overestimating the true value in a downward trend. </span><span class="koboSpan" id="kobo.534.2">It may create a similar “staircase” effect in strongly decreasing trends, potentially overstating the rate </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">of decrease.</span></span></li>
				<li><span class="koboSpan" id="kobo.536.1">The choice </span><a id="_idIndexMarker839"/><span class="koboSpan" id="kobo.537.1">between ffill and bfill should consider not just the direction of the trend, but also its </span><em class="italic"><span class="koboSpan" id="kobo.538.1">strength</span></em><span class="koboSpan" id="kobo.539.1"> and the </span><em class="italic"><span class="koboSpan" id="kobo.540.1">length</span></em><span class="koboSpan" id="kobo.541.1"> of missing data periods. </span><span class="koboSpan" id="kobo.541.2">For subtle trends, either method might be appropriate, and the choice may depend more on other factors, such as the nature of the data or the specific </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">analysis goals.</span></span></li>
				<li><span class="koboSpan" id="kobo.543.1">Both methods </span><a id="_idIndexMarker840"/><span class="koboSpan" id="kobo.544.1">can indeed propagate errors if the missing values are inconsistent with surrounding data points. </span><span class="koboSpan" id="kobo.544.2">This is particularly problematic for long stretches of missing data, where the filled values may significantly deviate from the true </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">underlying pattern.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.546.1">Handling outliers</span></strong><span class="koboSpan" id="kobo.547.1">: If an outlier precedes or follows a stretch of missing data, ffill or bfill can propagate this anomalous value, distorting </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">the series.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.549.1">Assumption of data continuity</span></strong><span class="koboSpan" id="kobo.550.1">: Both methods assume that the missing data can be </span><strong class="bold"><span class="koboSpan" id="kobo.551.1">reasonably approximated</span></strong><span class="koboSpan" id="kobo.552.1"> by adjacent known values, which may not always be true. </span><span class="koboSpan" id="kobo.552.2">For variables that can change abruptly or have discontinuities, these methods may </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">be inappropriate.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.554.1">Let’s revisit the stock prices example and see how we can fill the missing values on the columns </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">with nulls.</span></span></p>
			<h3><span class="koboSpan" id="kobo.556.1">Filling nulls in the stock market use case</span></h3>
			<p><span class="koboSpan" id="kobo.557.1">In this example, we</span><a id="_idIndexMarker841"/><span class="koboSpan" id="kobo.558.1"> will not be focusing on missing indexes, just on the missing data in some of the features available. </span><span class="koboSpan" id="kobo.558.2">Let’s deep dive into the code – as always, you can find the full end-to-end code </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/3.back_forward_fill.py"><span class="No-Break"><span class="koboSpan" id="kobo.560.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/3.back_forward_fill.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.561.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.562.1">This code introduces random missing values into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">close</span></strong><span class="koboSpan" id="kobo.564.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.565.1">open</span></strong><span class="koboSpan" id="kobo.566.1"> columns of a DataFrame (</span><strong class="source-inline"><span class="koboSpan" id="kobo.567.1">df</span></strong><span class="koboSpan" id="kobo.568.1">). </span><span class="koboSpan" id="kobo.568.2">It begins by randomly selecting 50 indices from the DataFrame’s index using </span><strong class="source-inline"><span class="koboSpan" id="kobo.569.1">np.random.choice</span></strong><span class="koboSpan" id="kobo.570.1">. </span><span class="koboSpan" id="kobo.570.2">The selected indices are stored in two variables, </span><strong class="source-inline"><span class="koboSpan" id="kobo.571.1">nan_indices_close</span></strong><span class="koboSpan" id="kobo.572.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.573.1">nan_indices_open</span></strong><span class="koboSpan" id="kobo.574.1">, which correspond to the rows where missing values will </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">be inserted:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.576.1">
nan_indices_close = np.random.choice(df.index, size=50, replace=False)
nan_indices_open = np.random.choice(df.index, size=50, replace=False)</span></pre></li>				<li><span class="koboSpan" id="kobo.577.1">The following code uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.578.1">.loc</span></strong><span class="koboSpan" id="kobo.579.1"> accessor to assign NaN to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.580.1">close</span></strong><span class="koboSpan" id="kobo.581.1"> column at the indices specified by </span><strong class="source-inline"><span class="koboSpan" id="kobo.582.1">nan_indices_close</span></strong><span class="koboSpan" id="kobo.583.1">, and similarly to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.584.1">open</span></strong><span class="koboSpan" id="kobo.585.1"> column at the indices specified by </span><strong class="source-inline"><span class="koboSpan" id="kobo.586.1">nan_indices_open</span></strong><span class="koboSpan" id="kobo.587.1">. </span><span class="koboSpan" id="kobo.587.2">Effectively, this creates 50 random missing values in each of these columns, which can be useful for simulating real-world data scenarios or testing data </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">handling techniques:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.589.1">
df.loc[nan_indices_close, 'close'] = np.nan
df.loc[nan_indices_open, 'open'] = np.nan</span></pre></li>				<li><span class="koboSpan" id="kobo.590.1">Fill NaN values using ffill </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">and bfill:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.592.1">
df['close_ffill'] = df['close'].ffill() # Forward Fill
df['close_bfill'] = df['close'].bfill() # Backward Fill</span></pre></li>				<li><span class="koboSpan" id="kobo.593.1">Let’s see </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">the result:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.595.1">
print(df[['open', 'close', 'close_ffill', 'close_bfill']].head(20)) # Show first 20 rows</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.596.1">This will </span><a id="_idIndexMarker842"/><span class="koboSpan" id="kobo.597.1">display the </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">following output:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.599.1">                  open       close  close_ffill  close_bfill
2020-01-01  137.454012  183.849183   183.849183   183.849183
2020-01-02  195.071431  180.509032   180.509032   180.509032
2020-01-03  173.199394  182.298381   182.298381   182.298381
2020-01-06  159.865848  193.271051   193.271051   193.271051
</span><strong class="bold"><span class="koboSpan" id="kobo.600.1">2020-01-07  115.601864         NaN   193.271051   120.028202</span></strong><span class="koboSpan" id="kobo.601.1">
2020-01-08  115.599452  120.028202   120.028202   120.028202
2020-01-09  105.808361  161.678361   161.678361   161.678361
2020-01-10  186.617615  174.288149   174.288149   174.288149
2020-01-13  160.111501  173.791739   173.791739   173.791739
2020-01-14  170.807258  152.144902   152.144902   152.144902
</span><strong class="bold"><span class="koboSpan" id="kobo.602.1">2020-01-15  102.058449         NaN   152.144902   137.111294</span></strong><span class="koboSpan" id="kobo.603.1">
2020-01-16  196.990985  137.111294   137.111294   137.111294</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.604.1">As we can see, on </span><strong class="source-inline"><span class="koboSpan" id="kobo.605.1">2020-01-07</span></strong><span class="koboSpan" id="kobo.606.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.607.1">2020-01-15</span></strong><span class="koboSpan" id="kobo.608.1">, there are missing values (NaN) in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.609.1">close</span></strong><span class="koboSpan" id="kobo.610.1"> column. </span><span class="koboSpan" id="kobo.610.2">This indicates that the closing prices for these dates were not recorded or are </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">otherwise unavailable.</span></span></p>
			<p><span class="koboSpan" id="kobo.612.1">As we’ve learned, the ffill method (</span><strong class="source-inline"><span class="koboSpan" id="kobo.613.1">close_ffill</span></strong><span class="koboSpan" id="kobo.614.1">) fills missing values with the last </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">valid observation:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.616.1">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.617.1">2020-01-07</span></strong><span class="koboSpan" id="kobo.618.1">, the closing price is filled with the last known value from </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.619.1">2020-01-06</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.620.1"> (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.621.1">193.27</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">)</span></span></li>
				<li><span class="koboSpan" id="kobo.623.1">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.624.1">2020-01-15</span></strong><span class="koboSpan" id="kobo.625.1">, the missing value is filled with the last valid price from </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.626.1">2020-01-14</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.627.1"> (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.628.1">152.14</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">)</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.630.1">On the other hand, the</span><a id="_idIndexMarker843"/><span class="koboSpan" id="kobo.631.1"> bfill method (</span><strong class="source-inline"><span class="koboSpan" id="kobo.632.1">close_bfill</span></strong><span class="koboSpan" id="kobo.633.1">) fills missing values with the next </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">valid observation:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.635.1">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.636.1">2020-01-07</span></strong><span class="koboSpan" id="kobo.637.1">, since no subsequent valid price is recorded immediately, it takes the closing price from </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.638.1">2020-01-08</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.639.1"> (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.640.1">120.03</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">)</span></span></li>
				<li><span class="koboSpan" id="kobo.642.1">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.643.1">2020-01-15</span></strong><span class="koboSpan" id="kobo.644.1">, the value is filled with the next known price </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">from </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.646.1">2020-01-16</span></strong></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.647.1">Let’s take a closer look at what happened in the data after we performed the different </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">filling methods:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.649.1">On </span><strong class="source-inline"><span class="koboSpan" id="kobo.650.1">2020-01-07</span></strong><span class="koboSpan" id="kobo.651.1">, ffill overestimates the missing value compared to bfill, which aligns more closely with the next </span><span class="No-Break"><span class="koboSpan" id="kobo.652.1">known value</span></span></li>
				<li><span class="koboSpan" id="kobo.653.1">On </span><strong class="source-inline"><span class="koboSpan" id="kobo.654.1">2020-01-15</span></strong><span class="koboSpan" id="kobo.655.1">, ffill and bfill provide different estimates, with ffill potentially overestimating the value compared </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">to bfill</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.657.1">As a general recommendation, we need to investigate the pattern of missing values. </span><span class="koboSpan" id="kobo.657.2">If the missing values are </span><em class="italic"><span class="koboSpan" id="kobo.658.1">random and sparse</span></em><span class="koboSpan" id="kobo.659.1">, either method might be appropriate. </span><span class="koboSpan" id="kobo.659.2">However, if there is a systematic pattern, more sophisticated imputation methods might be needed, such as </span><em class="italic"><span class="koboSpan" id="kobo.660.1">interpolation</span></em><span class="koboSpan" id="kobo.661.1">. </span><span class="koboSpan" id="kobo.661.2">Interpolation allows us to estimate missing data points by leveraging the existing values in the dataset, providing a more nuanced approach that can capture trends and patterns over time. </span><span class="koboSpan" id="kobo.661.3">We’ll discuss this in more </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">detail next.</span></span></p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor262"/><span class="koboSpan" id="kobo.663.1">Interpolation</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.664.1">Interpolation</span></strong><span class="koboSpan" id="kobo.665.1"> is a </span><a id="_idIndexMarker844"/><span class="koboSpan" id="kobo.666.1">method </span><a id="_idIndexMarker845"/><span class="koboSpan" id="kobo.667.1">for estimating missing values by filling in the gaps based on the surrounding data points. </span><span class="koboSpan" id="kobo.667.2">Unlike ffill and bfill, which copy existing values, interpolation uses mathematical techniques to estimate missing values. </span><span class="koboSpan" id="kobo.667.3">There are different interpolation techniques and applications. </span><span class="koboSpan" id="kobo.667.4">So, let’s </span><a id="_idIndexMarker846"/><span class="koboSpan" id="kobo.668.1">have a look at the available options, along with </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">their considerations:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.670.1">Linear interpolation</span></strong><span class="koboSpan" id="kobo.671.1">: Linear interpolation</span><a id="_idIndexMarker847"/><span class="koboSpan" id="kobo.672.1"> connects</span><a id="_idIndexMarker848"/><span class="koboSpan" id="kobo.673.1"> two adjacent known data points with a straight line and estimates the missing values along this line. </span><span class="koboSpan" id="kobo.673.2">It is the simplest form of interpolation and assumes a linear relationship between the data points. </span><span class="koboSpan" id="kobo.673.3">It is suitable for datasets where changes between data points are expected to be linear or nearly linear. </span><span class="koboSpan" id="kobo.673.4">It is commonly used in financial data, temperature readings, and other environmental data where gradual changes </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">are expected.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.675.1">Polynomial interpolation</span></strong><span class="koboSpan" id="kobo.676.1">: Polynomial interpolation</span><a id="_idIndexMarker849"/><span class="koboSpan" id="kobo.677.1"> fits a polynomial function to the known data </span><a id="_idIndexMarker850"/><span class="koboSpan" id="kobo.678.1">points and uses this function to estimate missing values. </span><span class="koboSpan" id="kobo.678.2">Higher-order polynomials can capture more complex relationships between data points. </span><span class="koboSpan" id="kobo.678.3">It is suitable for datasets with non-linear trends, and it is usually used in scientific and engineering applications, where data follows a </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">polynomial trend.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.680.1">Spline interpolation</span></strong><span class="koboSpan" id="kobo.681.1">: Spline interpolation</span><a id="_idIndexMarker851"/><span class="koboSpan" id="kobo.682.1"> uses</span><a id="_idIndexMarker852"/><span class="koboSpan" id="kobo.683.1"> piecewise polynomials, typically cubic splines, to fit the data points, ensuring smoothness at the data points and providing a smooth curve through the data. </span><span class="koboSpan" id="kobo.683.2">It is suitable for datasets requiring smooth transitions between data points and is commonly used in computer graphics, signal processing, and </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">environmental data.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.685.1">Let’s use interpolation in our </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">use case.</span></span></p>
			<h3><span class="koboSpan" id="kobo.687.1">Interpolation in the stock market use case</span></h3>
			<p><span class="koboSpan" id="kobo.688.1">Consider</span><a id="_idIndexMarker853"/><span class="koboSpan" id="kobo.689.1"> the same time series dataset presented previously with missing values. </span><span class="koboSpan" id="kobo.689.2">In this case, we want to impute these missing values using different interpolation methods. </span><span class="koboSpan" id="kobo.689.3">You can find the full code examples in this book’s GitHub repository: </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/4.interpolation.py"><span class="koboSpan" id="kobo.690.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/4.interpolation.py</span></a><span class="koboSpan" id="kobo.691.1">. </span><span class="koboSpan" id="kobo.691.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.693.1">The following code introduces random missing values into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.694.1">close</span></strong><span class="koboSpan" id="kobo.695.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.696.1">open</span></strong><span class="koboSpan" id="kobo.697.1"> columns of our DataFrame (</span><strong class="source-inline"><span class="koboSpan" id="kobo.698.1">df</span></strong><span class="koboSpan" id="kobo.699.1">), as we did in the </span><span class="No-Break"><span class="koboSpan" id="kobo.700.1">previous section:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.701.1">
nan_indices_close = np.random.choice(df.index, size=50, replace=False)
nan_indices_open = np.random.choice(df.index, size=50, replace=False)
df.loc[nan_indices_close, 'close'] = np.nan
df.loc[nan_indices_open, 'open'] = np.nan</span></pre></li>				<li><span class="koboSpan" id="kobo.702.1">The following line of code is used to fill in missing values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.703.1">close</span></strong><span class="koboSpan" id="kobo.704.1"> column of our DataFrame (</span><strong class="source-inline"><span class="koboSpan" id="kobo.705.1">df</span></strong><span class="koboSpan" id="kobo.706.1">) using linear interpolation. </span><span class="koboSpan" id="kobo.706.2">The code specifically employs </span><strong class="bold"><span class="koboSpan" id="kobo.707.1">linear interpolation</span></strong><span class="koboSpan" id="kobo.708.1">, where</span><a id="_idIndexMarker854"/><span class="koboSpan" id="kobo.709.1"> the missing values are estimated by drawing a straight line between the nearest known data points before and after the </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">missing value:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.711.1">
df['close_linear'] = df['close'].interpolate(method='linear')</span></pre></li>				<li><span class="koboSpan" id="kobo.712.1">We can interpolate missing values using polynomial interpolation by changing the method argument to </span><strong class="source-inline"><span class="koboSpan" id="kobo.713.1">method='polynomial'</span></strong><span class="koboSpan" id="kobo.714.1">. </span><span class="koboSpan" id="kobo.714.2">This specifies that the interpolation should be done using a polynomial function of </span><strong class="source-inline"><span class="koboSpan" id="kobo.715.1">order=3</span></strong><span class="koboSpan" id="kobo.716.1">. </span><span class="koboSpan" id="kobo.716.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.717.1">order</span></strong><span class="koboSpan" id="kobo.718.1"> argument indicates the degree of the polynomial to be used. </span><span class="koboSpan" id="kobo.718.2">In this case, a cubic polynomial (third degree) is used, which means the function that estimates the missing values will be a curve, potentially providing a better fit for more complex data trends compared to a simple straight line (as in </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">linear interpolation):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.720.1">
df['close_poly'] = df['close'].interpolate(method='polynomial', </span><strong class="source-inline"><span class="koboSpan" id="kobo.721.1">order=3</span></strong><span class="koboSpan" id="kobo.722.1">)</span></pre></li>				<li><span class="koboSpan" id="kobo.723.1">We can interpolate missing values using spline interpolation by changing the method to </span><strong class="source-inline"><span class="koboSpan" id="kobo.724.1">method='spline'</span></strong><span class="koboSpan" id="kobo.725.1">. </span><span class="koboSpan" id="kobo.725.2">This specifies that the interpolation should be done using spline interpolation, which is a piecewise polynomial function that ensures smoothness at the data points. </span><span class="koboSpan" id="kobo.725.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.726.1">order=3</span></strong><span class="koboSpan" id="kobo.727.1"> argument indicates the degree of the</span><a id="_idIndexMarker855"/><span class="koboSpan" id="kobo.728.1"> polynomial used in each piece of the spline. </span><span class="koboSpan" id="kobo.728.2">In this case, a cubic spline (third-degree polynomial) is used, meaning that the interpolation will involve fitting cubic polynomials to segments of </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">the data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.730.1">
df['close_spline'] = df['close'].interpolate(method='spline', order=3)</span></pre></li>				<li><span class="koboSpan" id="kobo.731.1">Now, let’s plot the </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">interpolated data:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<span class="koboSpan" id="kobo.733.1"><img src="image/B19801_11_8.jpg" alt="Figure 11.8 – Daily closing prices interpolated"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.734.1">Figure 11.8 – Daily closing prices interpolated</span></p>
			<p><span class="koboSpan" id="kobo.735.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.736.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.737.1">.8</span></em><span class="koboSpan" id="kobo.738.1">, we can see how the data changes with the different interpolation methods. </span><span class="koboSpan" id="kobo.738.2">To better grasp the differences, let’s have a look at the actual data after interpolation, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.739.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.740.1">.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">:</span></span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<span class="koboSpan" id="kobo.742.1"><img src="image/B19801_11_9.jpg" alt="Figure 11.9 – Table of the daily closing prices interpolated"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.743.1">Figure 11.9 – Table of the daily closing prices interpolated</span></p>
			<p><span class="koboSpan" id="kobo.744.1">Let’s compare </span><a id="_idIndexMarker856"/><span class="koboSpan" id="kobo.745.1">the different interpolation methods and come to </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">some conclusions:</span></span></p>
			<p><span class="koboSpan" id="kobo.747.1">On 2020-01-07, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">the following:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.749.1">Linear </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.750.1">interpolation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.751.1">: 156.649626</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.752.1">Polynomial </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.753.1">interpolation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.754.1">: 142.704592</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.755.1">Spline </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.756.1">interpolation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">: 143.173016</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.758.1">On 2020-01-15, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.759.1">the following:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.760.1">Linear </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.761.1">interpolation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">: 144.628098</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.763.1">Polynomial </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.764.1">interpolation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.765.1">: 127.403857</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.766.1">Spline </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.767.1">interpolation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.768.1">: 128.666028</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.769.1">Based on this data, linear interpolation seems to provide higher estimates compared to polynomial and spline interpolation. </span><span class="koboSpan" id="kobo.769.2">It assumes a linear trend between data points, which might not be accurate for non-linear data. </span><span class="koboSpan" id="kobo.769.3">Polynomial interpolation seems to provide lower estimates and capture more complex relationships but can be prone to overfitting. </span><span class="koboSpan" id="kobo.769.4">Finally, spline interpolation provides smooth estimates that are intermediate between linear and polynomial interpolation, offering a balance between simplicity and accuracy. </span><span class="koboSpan" id="kobo.769.5">In this specific case, we would go with spline interpolation as it provides a smooth curve that avoids abrupt changes, and the results are more realistic and closely aligned with the expected trends in the data. </span><span class="koboSpan" id="kobo.769.6">While spline interpolation is recommended based on the provided data, it is essential to validate the interpolated values against known data points or </span><span class="No-Break"><span class="koboSpan" id="kobo.770.1">domain knowledge.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.771.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.772.1">Interpolation methods such as linear, polynomial, and spline interpolation can also be used to deal with </span><em class="italic"><span class="koboSpan" id="kobo.773.1">outliers</span></em><span class="koboSpan" id="kobo.774.1"> in time </span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">series data.</span></span></p>
			<p><span class="koboSpan" id="kobo.776.1">Choosing</span><a id="_idIndexMarker857"/><span class="koboSpan" id="kobo.777.1"> and tuning interpolation arguments for filling missing values involves understanding the characteristics of your data and the specific needs of your analysis. </span><span class="koboSpan" id="kobo.777.2">For straightforward data with a linear trend, linear interpolation is efficient and effective. </span><span class="koboSpan" id="kobo.777.3">However, if your data exhibits non-linear patterns, polynomial interpolation can provide a better fit, with the degree of the polynomial (</span><strong class="source-inline"><span class="koboSpan" id="kobo.778.1">order</span></strong><span class="koboSpan" id="kobo.779.1">) influencing the complexity of the curve; lower orders work well for simpler trends, while higher orders may capture more detail but risk overfitting. </span><span class="koboSpan" id="kobo.779.2">Spline interpolation offers a smooth and flexible approach, with cubic splines (</span><strong class="source-inline"><span class="koboSpan" id="kobo.780.1">order=3</span></strong><span class="koboSpan" id="kobo.781.1">) being commonly used for their balance of smoothness and flexibility. </span><span class="koboSpan" id="kobo.781.2">To tune these methods, start with simpler approaches and test more complex ones progressively while monitoring for overfitting and ensuring the fit aligns with the data’s underlying trends. </span><span class="koboSpan" id="kobo.781.3">Employ cross-validation, visual inspection, and statistical metrics to evaluate and refine your </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">interpolation choices.</span></span></p>
			<p><span class="koboSpan" id="kobo.783.1">Now that we have explored the various techniques for handling missing data in time series, it’s essential to summarize the different filling methods to understand their unique applications </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">and effectiveness.</span></span></p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor263"/><span class="koboSpan" id="kobo.785.1">Comparing the different methods for missing values</span></h2>
			<p><span class="koboSpan" id="kobo.786.1">Handling </span><a id="_idIndexMarker858"/><span class="koboSpan" id="kobo.787.1">missing values in time series data is an involved process that requires thoughtful consideration of the specific context and characteristics of the dataset. </span><span class="koboSpan" id="kobo.787.2">The decision to drop values, use bfill, or apply interpolation should be guided by a careful assessment of the impact on subsequent analyses and the preservation of critical information within the time series. </span><span class="koboSpan" id="kobo.787.3">The </span><a id="_idIndexMarker859"/><span class="koboSpan" id="kobo.788.1">following table summarizes the different techniques and can be used as </span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">a guide:</span></span></p>
			<table id="table001-8" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.790.1">Method</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.791.1">When </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.792.1">to Use</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.793.1">Pros</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.794.1">Cons</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.795.1">Dropping </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">missing values</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.797.1">A small percentage of </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">missing values</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.799.1">- </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">Simplicity</span></span></p>
							<p><span class="koboSpan" id="kobo.801.1">- Avoids </span><span class="No-Break"><span class="koboSpan" id="kobo.802.1">imputation uncertainty</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.803.1">- </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">Information loss</span></span></p>
							<p><span class="koboSpan" id="kobo.805.1">- </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">Potential bias</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.807.1">Bfill</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.808.1">Missing values are expected to precede </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">consistent values</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.810.1">- Preserves the </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">general trend</span></span></p>
							<p><span class="koboSpan" id="kobo.812.1">- Suitable for </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">increasing trends</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.814.1">- Propagates errors if missing values are not similar to the </span><span class="No-Break"><span class="koboSpan" id="kobo.815.1">following values</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.816.1">Ffill</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.817.1">Missing values are expected to follow </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">consistent values</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.819.1">- Simple </span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">to implement</span></span></p>
							<p><span class="koboSpan" id="kobo.821.1">- Maintains recent state until new data </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">is available</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.823.1">- Can misrepresent data if </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">trends change</span></span></p>
							<p><span class="koboSpan" id="kobo.825.1">- Propagates errors if missing values differ from </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">previous values</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.827.1">Linear </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">interpolation</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.829.1">Missing values need to be estimated based on neighboring </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">data points</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.831.1">- Simple and easy </span><span class="No-Break"><span class="koboSpan" id="kobo.832.1">to implement</span></span></p>
							<p><span class="koboSpan" id="kobo.833.1">- Preserves </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">overall trend</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.835.1">- May not capture </span><span class="No-Break"><span class="koboSpan" id="kobo.836.1">non-linear trends</span></span></p>
							<p><span class="koboSpan" id="kobo.837.1">- Sensitive </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">to outliers</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.839.1">Polynomial </span><span class="No-Break"><span class="koboSpan" id="kobo.840.1">interpolation</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.841.1">Missing values need to be estimated with more complex </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">relationships</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.843.1">- Captures </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">complex relationships</span></span></p>
							<p><span class="koboSpan" id="kobo.845.1">- Flexible with </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">polynomial order</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.847.1">- Can lead to overfitting </span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">and oscillations</span></span></p>
							<p><span class="koboSpan" id="kobo.849.1">- </span><span class="No-Break"><span class="koboSpan" id="kobo.850.1">Computationally intensive</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.851.1">Spline </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">interpolation</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.853.1">Missing values need to be estimated with </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">smooth transitions</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.855.1">- Provides a </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">smooth curve</span></span></p>
							<p><span class="koboSpan" id="kobo.857.1">- Avoids oscillations of </span><span class="No-Break"><span class="koboSpan" id="kobo.858.1">high-order polynomials</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.859.1">- More complex to </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">implement</span></span></p>
							<p><span class="koboSpan" id="kobo.861.1">- Computationally </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">intensive</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.863.1">Table 11.1 – Comparison of the different methods to handle missing data in time series</span></p>
			<p><span class="koboSpan" id="kobo.864.1">Having examined the</span><a id="_idIndexMarker860"/><span class="koboSpan" id="kobo.865.1"> various methods for filling missing values in time series data, it is equally important to address another critical aspect: the correlation of a time series with its own </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">lagged values.</span></span></p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor264"/><span class="koboSpan" id="kobo.867.1">Analyzing time series data</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.868.1">Autocorrelation</span></strong><span class="koboSpan" id="kobo.869.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.870.1">partial autocorrelation</span></strong><span class="koboSpan" id="kobo.871.1"> are crucial tools in time series analysis that </span><a id="_idIndexMarker861"/><span class="koboSpan" id="kobo.872.1">provide insights into data patterns and guide model selection. </span><span class="koboSpan" id="kobo.872.2">For outlier detection, they help distinguish between genuine anomalies and expected variations, leading to more accurate and context-aware </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">outlier identification.</span></span></p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor265"/><span class="koboSpan" id="kobo.874.1">Autocorrelation and partial autocorrelation</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.875.1">Autocorrelation</span></strong><span class="koboSpan" id="kobo.876.1"> refers</span><a id="_idIndexMarker862"/><span class="koboSpan" id="kobo.877.1"> to correlating a time series with its </span><a id="_idIndexMarker863"/><span class="koboSpan" id="kobo.878.1">own lagged values. </span><span class="koboSpan" id="kobo.878.2">Simply put, it measures how each observation in a time series is related to </span><a id="_idIndexMarker864"/><span class="koboSpan" id="kobo.879.1">its past observations. </span><span class="koboSpan" id="kobo.879.2">Autocorrelation is a crucial concept in understanding the temporal dependencies and patterns present in time </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">series data.</span></span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.881.1">Partial autocorrelation function</span></strong><span class="koboSpan" id="kobo.882.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.883.1">PACF</span></strong><span class="koboSpan" id="kobo.884.1">), on the other hand, is a statistical tool that’s used </span><a id="_idIndexMarker865"/><span class="koboSpan" id="kobo.885.1">in time series analysis to measure the correlation </span><em class="italic"><span class="koboSpan" id="kobo.886.1">between a time series and its lagged values after removing the effects of intermediate lags</span></em><span class="koboSpan" id="kobo.887.1">. </span><span class="koboSpan" id="kobo.887.2">It provides a more direct measure of the relationship between observations at different time points, excluding the indirect effects of </span><span class="No-Break"><span class="koboSpan" id="kobo.888.1">shorter lags.</span></span></p>
			<p><span class="koboSpan" id="kobo.889.1">Both autocorrelation and partial autocorrelation help in the </span><span class="No-Break"><span class="koboSpan" id="kobo.890.1">following cases:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.891.1">Temporal patterns</span></strong><span class="koboSpan" id="kobo.892.1">: They help identify patterns that repeat over time. </span><span class="koboSpan" id="kobo.892.2">This is crucial for understanding the inherent structure of the time </span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">series data.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.894.1">Stationarity assessment</span></strong><span class="koboSpan" id="kobo.895.1">: They help in assessing the stationarity of a time series. </span><span class="koboSpan" id="kobo.895.2">A lack of stationarity can impact the reliability of statistical analyses and </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">model predictions.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.897.1">Lag selection for models</span></strong><span class="koboSpan" id="kobo.898.1">: They guide the selection of appropriate lags for time series models, such</span><a id="_idIndexMarker866"/><span class="koboSpan" id="kobo.899.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.900.1">autoregressive</span></strong><span class="koboSpan" id="kobo.901.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.902.1">AR</span></strong><span class="koboSpan" id="kobo.903.1">) components </span><a id="_idIndexMarker867"/><span class="koboSpan" id="kobo.904.1">in </span><strong class="bold"><span class="koboSpan" id="kobo.905.1">autoregressive moving average</span></strong><span class="koboSpan" id="kobo.906.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.907.1">ARIMA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">) models.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.909.1">Seasonality detection</span></strong><span class="koboSpan" id="kobo.910.1">: Significant</span><a id="_idIndexMarker868"/><span class="koboSpan" id="kobo.911.1"> peaks in the </span><strong class="bold"><span class="koboSpan" id="kobo.912.1">autocorrelation function</span></strong><span class="koboSpan" id="kobo.913.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.914.1">ACF</span></strong><span class="koboSpan" id="kobo.915.1">) plot at specific lags indicate the presence of seasonality, providing insights for </span><span class="No-Break"><span class="koboSpan" id="kobo.916.1">further analysis.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.917.1">Anomaly detection</span></strong><span class="koboSpan" id="kobo.918.1">: Unusual patterns in the autocorrelation function may suggest anomalies or outliers in the data, prompting investigation </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">and cleaning.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.920.1">Now, let’s </span><a id="_idIndexMarker869"/><span class="koboSpan" id="kobo.921.1">perform an ACF and PACF analysis</span><a id="_idIndexMarker870"/><span class="koboSpan" id="kobo.922.1"> on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.923.1">close_filled</span></strong><span class="koboSpan" id="kobo.924.1"> series from our stock price dataset. </span><span class="koboSpan" id="kobo.924.2">This analysis will help us determine the appropriate parameters (</span><strong class="source-inline"><span class="koboSpan" id="kobo.925.1">p</span></strong><span class="koboSpan" id="kobo.926.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.927.1">q</span></strong><span class="koboSpan" id="kobo.928.1">) for the ARIMA modeling we will perform in the </span><span class="No-Break"><span class="koboSpan" id="kobo.929.1">following section.</span></span></p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor266"/><span class="koboSpan" id="kobo.930.1">ACT and PACF in the stock market use case</span></h2>
			<p><span class="koboSpan" id="kobo.931.1">We </span><a id="_idIndexMarker871"/><span class="koboSpan" id="kobo.932.1">will continue with the example we’ve used so far and </span><a id="_idIndexMarker872"/><span class="koboSpan" id="kobo.933.1">add the ACT and PACF charts. </span><span class="koboSpan" id="kobo.933.2">As always, you can have a look at the full code here: </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/4.analisis/autocorrelation.py"><span class="koboSpan" id="kobo.934.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/4.analisis/autocorrelation.py</span></a><span class="koboSpan" id="kobo.935.1">. </span><span class="koboSpan" id="kobo.935.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.937.1">Create an </span><span class="No-Break"><span class="koboSpan" id="kobo.938.1">autocorrelation plot:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.939.1">
plot_acf(df['close'].dropna(), lags=40, ax=plt.gca())</span></pre></li>				<li><span class="koboSpan" id="kobo.940.1">Create a partial </span><span class="No-Break"><span class="koboSpan" id="kobo.941.1">autocorrelation plot:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.942.1">
plot_pacf(df['close'].dropna(), lags=40, ax=plt.gca())</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.943.1">The resultant plots are </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">shown here:</span></span></p></li>			</ol>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<span class="koboSpan" id="kobo.945.1"><img src="image/B19801_11_10.jpg" alt="Figure 11.10 – ACF and PACF plots"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.946.1">Figure 11.10 – ACF and PACF plots</span></p>
			<p><span class="koboSpan" id="kobo.947.1">Let’s </span><a id="_idIndexMarker873"/><span class="koboSpan" id="kobo.948.1">explain </span><a id="_idIndexMarker874"/><span class="koboSpan" id="kobo.949.1">what we can see in the preceding charts. </span><span class="koboSpan" id="kobo.949.2">Here’s what we can see </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">for ACF:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.951.1">The ACF plot shows the correlation between the series and its lagged values at various lags (</span><strong class="source-inline"><span class="koboSpan" id="kobo.952.1">lags=40</span></strong><span class="koboSpan" id="kobo.953.1">, in this example). </span><span class="koboSpan" id="kobo.953.2">The </span><em class="italic"><span class="koboSpan" id="kobo.954.1">X</span></em><span class="koboSpan" id="kobo.955.1">-axis of the ACF plot represents the number of lags, indicating how many points back in time the correlation is </span><span class="No-Break"><span class="koboSpan" id="kobo.956.1">being calculated.</span></span></li>
				<li><span class="koboSpan" id="kobo.957.1">The </span><em class="italic"><span class="koboSpan" id="kobo.958.1">Y</span></em><span class="koboSpan" id="kobo.959.1">-axis of the ACF plot represents the correlation coefficients between the original time series and its lagged values. </span><span class="koboSpan" id="kobo.959.2">The correlation values range from -1 </span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">to 1.</span></span></li>
				<li><span class="koboSpan" id="kobo.961.1">The blue shaded area represents the confidence interval. </span><span class="koboSpan" id="kobo.961.2">Bars that extend beyond the shaded area are considered statistically significant and indicate strong autocorrelation at those lags, which suggest potential values for the </span><em class="italic"><span class="koboSpan" id="kobo.962.1">q parameter in the ARIMA model</span></em><span class="koboSpan" id="kobo.963.1"> (MA order), as we will see in the </span><span class="No-Break"><span class="koboSpan" id="kobo.964.1">following section.</span></span></li>
				<li><span class="koboSpan" id="kobo.965.1">Significant peaks at regular intervals indicate the presence of </span><em class="italic"><span class="koboSpan" id="kobo.966.1">seasonality</span></em><span class="koboSpan" id="kobo.967.1"> in the time </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">series data.</span></span></li>
				<li><span class="koboSpan" id="kobo.969.1">If there is a significant autocorrelation at lag 1 in the ACF plot (a spike that goes beyond the blue-shaded region, as in our case), it suggests that the series has a strong correlation </span><em class="italic"><span class="koboSpan" id="kobo.970.1">with its immediate previous value</span></em><span class="koboSpan" id="kobo.971.1">. </span><span class="koboSpan" id="kobo.971.2">This might indicate that the series is </span><em class="italic"><span class="koboSpan" id="kobo.972.1">not stationary</span></em><span class="koboSpan" id="kobo.973.1"> and may need differencing (d &gt; </span><span class="No-Break"><span class="koboSpan" id="kobo.974.1">0).</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.975.1">Here’s what </span><a id="_idIndexMarker875"/><span class="koboSpan" id="kobo.976.1">we can </span><a id="_idIndexMarker876"/><span class="koboSpan" id="kobo.977.1">see </span><span class="No-Break"><span class="koboSpan" id="kobo.978.1">for PACF:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.979.1">The PACF plot shows the correlation between the series and its lagged values after removing the effects explained by </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">shorter lags.</span></span></li>
				<li><span class="koboSpan" id="kobo.981.1">Significant spikes in the PACF plot indicate that lag 1 and potentially lag 2 could be good candidates for the </span><em class="italic"><span class="koboSpan" id="kobo.982.1">p parameter in the ARIMA model</span></em><span class="koboSpan" id="kobo.983.1"> (</span><span class="No-Break"><span class="koboSpan" id="kobo.984.1">AR order).</span></span></li>
			</ul>
			<p class="callout-heading"><span class="koboSpan" id="kobo.985.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.986.1">When we specify </span><strong class="source-inline"><span class="koboSpan" id="kobo.987.1">lags=40</span></strong><span class="koboSpan" id="kobo.988.1"> in the context of ACF and PACF plots, we are examining the autocorrelation and partial autocorrelation of the time series at 40 different lag intervals. </span><span class="koboSpan" id="kobo.988.2">This means we will see how the series is correlated with itself from </span><em class="italic"><span class="koboSpan" id="kobo.989.1">lag 1 up to </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.990.1">lag 40</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.992.1">ACF and PACF plots are crucial for identifying the underlying structure in a time series. </span><span class="koboSpan" id="kobo.992.2">In the next section, we will link the ACF and PACF analysis to outlier detection and handling to ensure our time series model captures the underlying </span><span class="No-Break"><span class="koboSpan" id="kobo.993.1">patterns accurately.</span></span></p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor267"/><span class="koboSpan" id="kobo.994.1">Dealing with outliers</span></h1>
			<p><span class="koboSpan" id="kobo.995.1">Time series data </span><a id="_idIndexMarker877"/><span class="koboSpan" id="kobo.996.1">often exhibit seasonal patterns (for example, sales spikes during holidays) and trends (for example, gradual growth over the years). </span><span class="koboSpan" id="kobo.996.2">An outlier in this context might not be an anomaly; rather, it could reflect a normal seasonal effect or a change in the underlying trend. </span><span class="koboSpan" id="kobo.996.3">For example, a sudden spike in retail sales during Black Friday is expected and should not be treated as an outlier. </span><span class="koboSpan" id="kobo.996.4">Techniques such</span><a id="_idIndexMarker878"/><span class="koboSpan" id="kobo.997.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.998.1">seasonal decomposition of time series</span></strong><span class="koboSpan" id="kobo.999.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1000.1">STL</span></strong><span class="koboSpan" id="kobo.1001.1">), autocorrelation, and seasonal indices can aid in understanding the expected behavior of the data, thus providing a clearer basis for </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">identifying outliers.</span></span></p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor268"/><span class="koboSpan" id="kobo.1003.1">Identifying outliers with seasonal decomposition</span></h2>
			<p><span class="koboSpan" id="kobo.1004.1">One way </span><a id="_idIndexMarker879"/><span class="koboSpan" id="kobo.1005.1">to identify outliers in time series is to decompose the series into trend, seasonality, and residual components, as outliers are often identified in the residual component. </span><span class="koboSpan" id="kobo.1005.2">To decompose the series into trend, seasonality, and residual components, we can use the STL method. </span><span class="koboSpan" id="kobo.1005.3">This method helps in identifying and handling outliers by analyzing the residual component, which ideally should be white noise. </span><span class="koboSpan" id="kobo.1005.4">Let’s see how we can do this using the stock market data. </span><span class="koboSpan" id="kobo.1005.5">You can find the full code example </span><span class="No-Break"><span class="koboSpan" id="kobo.1006.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/5.outliers/1.seasonal_decomposition.py"><span class="No-Break"><span class="koboSpan" id="kobo.1007.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/5.outliers/1.seasonal_decomposition.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1009.1">
result = seasonal_decompose(df['close'], model='additive', period=252)</span></pre>			<p><span class="koboSpan" id="kobo.1010.1">In this code snippet, we decompose the time series while assuming there are 252 business days in a year. </span><span class="koboSpan" id="kobo.1010.2">We will also calculate the Z-scores of residuals to identify outliers using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">following code:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1012.1">
df['resid_z'] = zscore(df['residual'].dropna())</span></pre>			<p><span class="koboSpan" id="kobo.1013.1">Finally, let’s plot the </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">decomposed series:</span></span></p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1015.1"><img src="image/B19801_11_11.jpg" alt="Figure 11.11 – Decomposed time series"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1016.1">Figure 11.11 – Decomposed time series</span></p>
			<p><span class="koboSpan" id="kobo.1017.1">Outliers can be detected by analyzing the residual component. </span><span class="koboSpan" id="kobo.1017.2">Significant deviations from zero or sudden spikes in the residual component indicate </span><span class="No-Break"><span class="koboSpan" id="kobo.1018.1">potential outliers:</span></span></p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1019.1"><img src="image/B19801_11_12.jpg" alt="Figure 11.12 – Table of decomposed values"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1020.1">Figure 11.12 – Table of decomposed values</span></p>
			<p><span class="koboSpan" id="kobo.1021.1">Based on the</span><a id="_idIndexMarker880"/><span class="koboSpan" id="kobo.1022.1"> decomposed time series in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1023.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1024.1">.11</span></em><span class="koboSpan" id="kobo.1025.1">, we can analyze the outliers by examining the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1026.1">residual</span></strong><span class="koboSpan" id="kobo.1027.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1028.1">resid_z</span></strong><span class="koboSpan" id="kobo.1029.1"> columns. </span><span class="koboSpan" id="kobo.1029.2">Typically, Z-scores with an absolute value greater than 2 or 3 are considered potential outliers. </span><span class="koboSpan" id="kobo.1029.3">In this dataset, the largest positive residuals are observed on </span><strong class="source-inline"><span class="koboSpan" id="kobo.1030.1">2020-01-06</span></strong><span class="koboSpan" id="kobo.1031.1"> (Z-score: 1.468043), </span><strong class="source-inline"><span class="koboSpan" id="kobo.1032.1">2020-01-17</span></strong><span class="koboSpan" id="kobo.1033.1"> (Z-score: 1.300488), and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1034.1">2020-01-27</span></strong><span class="koboSpan" id="kobo.1035.1"> (Z-score: 1.172529), while the largest negative residuals are on </span><strong class="source-inline"><span class="koboSpan" id="kobo.1036.1">2020-01-15</span></strong><span class="koboSpan" id="kobo.1037.1"> (Z-score: -1.721474) and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1038.1">2020-01-22</span></strong><span class="koboSpan" id="kobo.1039.1"> (Z-score: -1.082559). </span><span class="koboSpan" id="kobo.1039.2">Although these values indicate some deviations from the trend and seasonal components, none of the Z-scores exceed the typical threshold of ±2 or ±3, suggesting that there are no extreme outliers in this dataset. </span><span class="koboSpan" id="kobo.1039.3">The residuals appear to be relatively well-distributed around zero, indicating a good fit for the decomposition model. </span><span class="koboSpan" id="kobo.1039.4">However, the dates with the largest deviations (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1040.1">2020-01-06</span></strong><span class="koboSpan" id="kobo.1041.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1042.1">2020-01-15</span></strong><span class="koboSpan" id="kobo.1043.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1044.1">2020-01-17</span></strong><span class="koboSpan" id="kobo.1045.1">) might be worth investigating further for any unusual events or factors that could explain their deviation from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1046.1">expected values.</span></span></p>
			<p><span class="koboSpan" id="kobo.1047.1">On digging deeper into this data to understand the reasons behind the fluctuations and upon closer inspection, we can see that the deviations on these dates were due to specific events and </span><span class="No-Break"><span class="koboSpan" id="kobo.1048.1">system issues:</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1049.1">Disclaimer!</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1050.1">The following events correspond to </span><span class="No-Break"><span class="koboSpan" id="kobo.1051.1">made-up events!</span></span></p>
			<ul>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.1052.1">2020-01-06</span></strong><span class="koboSpan" id="kobo.1053.1">: A technical glitch in the stock exchange’s trading system caused a temporary spike </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">in prices</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.1055.1">2020-01-15</span></strong><span class="koboSpan" id="kobo.1056.1">: An erroneous trade input led to a sudden drop in prices, which was </span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">later corrected</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.1058.1">2020-01-17</span></strong><span class="koboSpan" id="kobo.1059.1">: A major economic announcement led to increased volatility and a brief surge in </span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1">stock prices</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.1061.1">2020-01-22</span></strong><span class="koboSpan" id="kobo.1062.1">: A miscommunication about quarterly earnings results caused temporary </span><span class="No-Break"><span class="koboSpan" id="kobo.1063.1">panic selling</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.1064.1">2020-01-27</span></strong><span class="koboSpan" id="kobo.1065.1">: Rumors of a merger and acquisition led to speculative buying, temporarily inflating </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">the prices</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1067.1">These findings </span><a id="_idIndexMarker881"/><span class="koboSpan" id="kobo.1068.1">helped us understand that the residuals’ deviations were not random but were due to specific, identifiable events. </span><span class="koboSpan" id="kobo.1068.2">While these events did not qualify as significant outliers statistically, they highlighted the inherent volatility and noise in stock price data. </span><span class="koboSpan" id="kobo.1068.3">Given the noisy nature of stock prices, even without significant outliers, smoothing techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.1069.1">become crucial!</span></span></p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor269"/><span class="koboSpan" id="kobo.1070.1">Handling outliers – model-based approaches – ARIMA</span></h2>
			<p><span class="koboSpan" id="kobo.1071.1">ARIMA</span><a id="_idIndexMarker882"/><span class="koboSpan" id="kobo.1072.1"> models</span><a id="_idIndexMarker883"/><span class="koboSpan" id="kobo.1073.1"> are widely used for forecasting time series data. </span><span class="koboSpan" id="kobo.1073.2">They predict future values based on past observations, making them effective tools for identifying outliers by comparing actual values against predicted values. </span><span class="koboSpan" id="kobo.1073.3">The ARIMA model consists of three </span><span class="No-Break"><span class="koboSpan" id="kobo.1074.1">main components:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1075.1">Autoregressive</span></strong><span class="koboSpan" id="kobo.1076.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1077.1">AR</span></strong><span class="koboSpan" id="kobo.1078.1">): Uses the dependency between an observation and several lagged </span><span class="No-Break"><span class="koboSpan" id="kobo.1079.1">observations (p)</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1080.1">Integrated</span></strong><span class="koboSpan" id="kobo.1081.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1082.1">I</span></strong><span class="koboSpan" id="kobo.1083.1">): Uses differencing of observations to make the time series </span><span class="No-Break"><span class="koboSpan" id="kobo.1084.1">stationary (d)</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1085.1">Moving average</span></strong><span class="koboSpan" id="kobo.1086.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1087.1">MA</span></strong><span class="koboSpan" id="kobo.1088.1">): Uses dependency between an observation and a residual error from a moving average model applied to lagged </span><span class="No-Break"><span class="koboSpan" id="kobo.1089.1">observations (q)</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1090.1">ARIMA models are effective in handling the </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">following outliers:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1092.1">Additive outliers</span></strong><span class="koboSpan" id="kobo.1093.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1094.1">AO</span></strong><span class="koboSpan" id="kobo.1095.1">): Sudden spikes or drops in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1096.1">time series</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1097.1">Innovative outliers</span></strong><span class="koboSpan" id="kobo.1098.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1099.1">IO</span></strong><span class="koboSpan" id="kobo.1100.1">): Changes that affect the entire series from the point of </span><span class="No-Break"><span class="koboSpan" id="kobo.1101.1">occurrence onwards</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1102.1">Let’s discuss </span><a id="_idIndexMarker884"/><span class="koboSpan" id="kobo.1103.1">how the ARIMA model can be used for outlier detection and smoothing in the context of the stock price data example we’ve been working with. </span><span class="koboSpan" id="kobo.1103.2">You can find the full example </span><span class="No-Break"><span class="koboSpan" id="kobo.1104.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/5.outliers/3.arima.py"><span class="No-Break"><span class="koboSpan" id="kobo.1105.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/5.outliers/3.arima.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1106.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1107.1">Fit the ARIMA model to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1108.1">close_filled</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1109.1"> series:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1110.1">
model = ARIMA(df['close_filled'], order=(2,1,1))
results = model.fit()</span></pre></li>				<li><span class="koboSpan" id="kobo.1111.1">Calculate the residuals </span><span class="No-Break"><span class="koboSpan" id="kobo.1112.1">and Z-scores:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1113.1">
df['residuals'] = results.resid
df['residuals_z'] = zscore(df['residuals'].dropna())</span></pre></li>				<li><span class="koboSpan" id="kobo.1114.1">Identify any outliers based on the Z-score threshold (for </span><span class="No-Break"><span class="koboSpan" id="kobo.1115.1">example, ±3):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1116.1">
outliers_arima = df[np.abs(df['residuals_z']) &gt; 3]</span></pre></li>				<li><span class="koboSpan" id="kobo.1117.1">Visualize the original </span><strong class="source-inline"><span class="koboSpan" id="kobo.1118.1">close_filled</span></strong><span class="koboSpan" id="kobo.1119.1"> series and the smoothed series that was obtained from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1120.1">ARIMA model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1121.1">
df['arima_smooth'] = results.fittedvalues</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1122.1">Here’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1123.1">the output:</span></span></p></li>			</ol>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1124.1"><img src="image/B19801_11_13.jpg" alt="Figure 11.13 – ARIMA smoothing and outlier detection"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1125.1">Figure 11.13 – ARIMA smoothing and outlier detection</span></p>
			<ol>
				<li value="5"><span class="koboSpan" id="kobo.1126.1">Generate</span><a id="_idIndexMarker885"/><span class="koboSpan" id="kobo.1127.1"> diagnostic plots to evaluate the model fit, including residual</span><a id="_idIndexMarker886"/><span class="koboSpan" id="kobo.1128.1"> analysis, a </span><strong class="bold"><span class="koboSpan" id="kobo.1129.1">Quantile-Quantile</span></strong><span class="koboSpan" id="kobo.1130.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1131.1">Q-Q</span></strong><span class="koboSpan" id="kobo.1132.1">) plot, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1133.1">standardized residuals:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1134.1">
results.plot_diagnostics(figsize=(14,8))</span></pre></li>				<li><span class="koboSpan" id="kobo.1135.1">The resulting plots are </span><span class="No-Break"><span class="koboSpan" id="kobo.1136.1">shown here:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1137.1"><img src="image/B19801_11_14.jpg" alt="Figure 11.14 – Residual analysis, Q-Q plot, and standardized residuals"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1138.1">Figure 11.14 – Residual analysis, Q-Q plot, and standardized residuals</span></p>
			<p><span class="koboSpan" id="kobo.1139.1">Let’s dive a bit</span><a id="_idIndexMarker887"/><span class="koboSpan" id="kobo.1140.1"> deeper into the diagnostic plots shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1141.1">Figure 11</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1142.1">.14</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1143.1">:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1144.1">Standardized residuals</span></strong><span class="koboSpan" id="kobo.1145.1">: Standardized residuals </span><a id="_idIndexMarker888"/><span class="koboSpan" id="kobo.1146.1">are the residuals from the ARIMA model scaled by their standard deviation. </span><span class="koboSpan" id="kobo.1146.2">For the ARIMA model to be considered a good fit, the standardized residuals </span><em class="italic"><span class="koboSpan" id="kobo.1147.1">should resemble white noise, meaning they should have no discernible pattern</span></em><span class="koboSpan" id="kobo.1148.1">. </span><span class="koboSpan" id="kobo.1148.2">This implies that the residuals are randomly distributed with a mean of zero and constant variance. </span><span class="koboSpan" id="kobo.1148.3">If a pattern is visible in the residuals, it suggests that the model has not captured some underlying structure in the data, and further refinement may be necessary. </span><span class="koboSpan" id="kobo.1148.4">In our case, the </span><em class="italic"><span class="koboSpan" id="kobo.1149.1">residuals</span></em><span class="koboSpan" id="kobo.1150.1"> look like </span><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">white noise.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1152.1">Histogram plus KDE</span></strong><span class="koboSpan" id="kobo.1153.1">: The </span><a id="_idIndexMarker889"/><span class="koboSpan" id="kobo.1154.1">histogram, combined with the </span><strong class="bold"><span class="koboSpan" id="kobo.1155.1">kernel density estimate</span></strong><span class="koboSpan" id="kobo.1156.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1157.1">KDE</span></strong><span class="koboSpan" id="kobo.1158.1">) plot of the residuals, provides</span><a id="_idIndexMarker890"/><span class="koboSpan" id="kobo.1159.1"> a visual assessment of their distribution. </span><span class="koboSpan" id="kobo.1159.2">For a well-fitted ARIMA model, the residuals should follow a normal distribution. </span><span class="koboSpan" id="kobo.1159.3">The histogram should resemble the familiar bell curve, and the KDE plot should overlay a smooth curve that matches this shape. </span><span class="koboSpan" id="kobo.1159.4">Deviations from the normal distribution, such as skewness or heavy tails, indicate that the residuals are not normally distributed, suggesting potential issues with the model. </span><span class="koboSpan" id="kobo.1159.5">In our case, we don’t see any significant skewness or tails in </span><span class="No-Break"><span class="koboSpan" id="kobo.1160.1">the residuals.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1161.1">Normal Q-Q plot</span></strong><span class="koboSpan" id="kobo.1162.1">: The Q-Q plot </span><a id="_idIndexMarker891"/><span class="koboSpan" id="kobo.1163.1">compares the quantiles of the residuals to the quantiles of a normal distribution. </span><span class="koboSpan" id="kobo.1163.2">If the residuals are normally distributed, the points on the Q-Q plot will lie along the 45-degree line. </span><span class="koboSpan" id="kobo.1163.3">Significant deviations from this line indicate departures from normality. </span><span class="koboSpan" id="kobo.1163.4">In our case, we don’t see any </span><span class="No-Break"><span class="koboSpan" id="kobo.1164.1">significant deviations.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1165.1">Correlogram</span></strong><span class="koboSpan" id="kobo.1166.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1167.1">ACF of residuals</span></strong><span class="koboSpan" id="kobo.1168.1">): The correlogram</span><a id="_idIndexMarker892"/><span class="koboSpan" id="kobo.1169.1"> displays the ACF of the residuals. </span><span class="koboSpan" id="kobo.1169.2">For a </span><a id="_idIndexMarker893"/><span class="koboSpan" id="kobo.1170.1">properly specified ARIMA model, the residuals should show no significant autocorrelation. </span><span class="koboSpan" id="kobo.1170.2">This means that none of the lags should have statistically significant correlation coefficients. </span><span class="koboSpan" id="kobo.1170.3">Significant spikes in the ACF plot indicate that the residuals are still correlated with their past values, suggesting that the model has not fully captured the time series’ structure. </span><span class="koboSpan" id="kobo.1170.4">This can guide further model refinement, such as increasing the order of the AR or MA components. </span><span class="koboSpan" id="kobo.1170.5">In our case, everything </span><span class="No-Break"><span class="koboSpan" id="kobo.1171.1">looks good.</span></span></li>
			</ul>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1172.1">What is lag 0?</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1173.1">In the correlogram (ACF plot), the term </span><strong class="bold"><span class="koboSpan" id="kobo.1174.1">lag 0</span></strong><span class="koboSpan" id="kobo.1175.1"> refers to the autocorrelation of the time series with itself at lag 0, which is essentially the correlation of the time series with itself at the same time point. </span><span class="koboSpan" id="kobo.1175.2">By definition, this correlation is always 1, because any time series is perfectly correlated with itself at lag 0. </span><span class="koboSpan" id="kobo.1175.3">This means the autocorrelation value at lag 0 is always 1, which is why you see a spike at lag 0 in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1176.1">ACF plot.</span></span></p>
			<p><span class="koboSpan" id="kobo.1177.1">It is a good idea to go and play with the different settings and see their effect on the ARIMA model and </span><span class="No-Break"><span class="koboSpan" id="kobo.1178.1">the residuals.</span></span></p>
			<p><span class="koboSpan" id="kobo.1179.1">Having explored the ARIMA method to detect and handle outliers in our stock price dataset, we have seen that outliers can significantly affect the accuracy and reliability of our time series model. </span><span class="koboSpan" id="kobo.1179.2">While the ARIMA method helps in identifying and adjusting for these sudden changes, it’s also valuable to consider other approaches to robust outlier detection and handling. </span><span class="koboSpan" id="kobo.1179.3">One such approach involves using moving window techniques, as we will see in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1180.1">next section.</span></span></p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor270"/><span class="koboSpan" id="kobo.1181.1">Moving window techniques</span></h2>
			<p><span class="koboSpan" id="kobo.1182.1">Moving </span><a id="_idIndexMarker894"/><span class="koboSpan" id="kobo.1183.1">window techniques, also known as rolling or sliding window methods, involve analyzing a fixed-size subset or “window” of data that </span><a id="_idIndexMarker895"/><span class="koboSpan" id="kobo.1184.1">moves sequentially across a larger dataset. </span><span class="koboSpan" id="kobo.1184.2">At each position of the window, a specific calculation or function is applied, such as computing the mean, median, sum, or more complex statistical measures. </span><span class="koboSpan" id="kobo.1184.3">As the window shifts by one or more data points, the calculation is updated with the new subset of data. </span><span class="koboSpan" id="kobo.1184.4">This method is particularly robust in time series analysis, where it is often used for smoothing data, identifying trends, or detecting anomalies </span><span class="No-Break"><span class="koboSpan" id="kobo.1185.1">over time.</span></span></p>
			<p><span class="koboSpan" id="kobo.1186.1">The robustness of moving window techniques lies in their ability to provide localized analysis while maintaining a connection to the broader dataset. </span><span class="koboSpan" id="kobo.1186.2">For example, when smoothing a time series, a moving average can reduce noise and highlight underlying trends without distorting the overall signal. </span><span class="koboSpan" id="kobo.1186.3">Similarly, in financial data, moving windows can be used to compute rolling averages or volatilities, offering a real-time view of </span><span class="No-Break"><span class="koboSpan" id="kobo.1187.1">market conditions.</span></span></p>
			<p><span class="koboSpan" id="kobo.1188.1">In this section, we will focus on two primary methods: </span><strong class="bold"><span class="koboSpan" id="kobo.1189.1">simple moving average</span></strong><span class="koboSpan" id="kobo.1190.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1191.1">SMA</span></strong><span class="koboSpan" id="kobo.1192.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.1193.1">exponential moving average</span></strong><span class="koboSpan" id="kobo.1194.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1195.1">EMA</span></strong><span class="koboSpan" id="kobo.1196.1">). </span><span class="koboSpan" id="kobo.1196.2">Both can act as a basis that you can adjust with other statistics such as the </span><span class="No-Break"><span class="koboSpan" id="kobo.1197.1">median later.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1198.1">SMA</span></h3>
			<p><span class="koboSpan" id="kobo.1199.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.1200.1">SMA</span></strong><span class="koboSpan" id="kobo.1201.1"> is a </span><a id="_idIndexMarker896"/><span class="koboSpan" id="kobo.1202.1">commonly </span><a id="_idIndexMarker897"/><span class="koboSpan" id="kobo.1203.1">used statistical calculation that represents the average of a set of data points over a specified time. </span><span class="koboSpan" id="kobo.1203.2">It is a type of moving average that smoothens out fluctuations in data to identify trends more easily. </span><span class="koboSpan" id="kobo.1203.3">The SMA is calculated by summing up a set of values and dividing the sum by the number of data points. </span><span class="koboSpan" id="kobo.1203.4">More advanced methods such as Kalman smoothing can estimate missing values by modeling the </span><span class="No-Break"><span class="koboSpan" id="kobo.1204.1">underlying process:</span></span></p>
			<p><em class="italic"><span class="koboSpan" id="kobo.1205.1">SMA</span></em><span class="subscript"><span class="koboSpan" id="kobo.1206.1">t</span></span><span class="koboSpan" id="kobo.1207.1"> = (</span><em class="italic"><span class="koboSpan" id="kobo.1208.1">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.1209.1">t  </span></span><span class="koboSpan" id="kobo.1210.1">+ </span><em class="italic"><span class="koboSpan" id="kobo.1211.1">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.1212.1">t–1 </span></span><span class="koboSpan" id="kobo.1213.1">+ </span><em class="italic"><span class="koboSpan" id="kobo.1214.1">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.1215.1">t–2 </span></span><span class="koboSpan" id="kobo.1216.1">+  ...+ </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1217.1">X</span></em></span><span class="No-Break"><span class="subscript"><span class="koboSpan" id="kobo.1218.1">t–</span></span></span><span class="No-Break"><span class="subscript"><span class="koboSpan" id="kobo.1219.1">n+1</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1220.1">)/</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1221.1">n</span></em></span></p>
			<p><span class="koboSpan" id="kobo.1222.1">Here, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1223.1">the following:</span></span></p>
			<ul>
				<li><em class="italic"><span class="koboSpan" id="kobo.1224.1">SMA</span></em><span class="subscript"><span class="koboSpan" id="kobo.1225.1">t</span></span><span class="koboSpan" id="kobo.1226.1"> is the SMA at </span><span class="No-Break"><span class="koboSpan" id="kobo.1227.1">time </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1228.1">t</span></em></span></li>
				<li><em class="italic"><span class="koboSpan" id="kobo.1229.1">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.1230.1">t  </span></span><span class="koboSpan" id="kobo.1231.1">+ </span><em class="italic"><span class="koboSpan" id="kobo.1232.1">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.1233.1">t–1 </span></span><span class="koboSpan" id="kobo.1234.1">+ </span><em class="italic"><span class="koboSpan" id="kobo.1235.1">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.1236.1">t–2 </span></span><span class="koboSpan" id="kobo.1237.1">+  ...+ </span><em class="italic"><span class="koboSpan" id="kobo.1238.1">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.1239.1">t–</span></span><span class="subscript"><span class="koboSpan" id="kobo.1240.1">n+1</span></span><span class="koboSpan" id="kobo.1241.1"> are the values for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1242.1">time period</span></span></li>
				<li><em class="italic"><span class="koboSpan" id="kobo.1243.1">n</span></em><span class="koboSpan" id="kobo.1244.1"> is the number </span><a id="_idIndexMarker898"/><span class="koboSpan" id="kobo.1245.1">of periods included in </span><span class="No-Break"><span class="koboSpan" id="kobo.1246.1">the calculations</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1247.1">Now, let’s introduce the exponential moving average so that we can compare </span><span class="No-Break"><span class="koboSpan" id="kobo.1248.1">the two.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1249.1">EMA</span></h3>
			<p><span class="koboSpan" id="kobo.1250.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.1251.1">EMA</span></strong><span class="koboSpan" id="kobo.1252.1"> gives </span><a id="_idIndexMarker899"/><span class="koboSpan" id="kobo.1253.1">more weight</span><a id="_idIndexMarker900"/><span class="koboSpan" id="kobo.1254.1"> to recent data points and less weight to older data points. </span><span class="koboSpan" id="kobo.1254.2">It uses an exponential </span><span class="No-Break"><span class="koboSpan" id="kobo.1255.1">decay formula:</span></span></p>
			<p><em class="italic"><span class="koboSpan" id="kobo.1256.1">EM</span></em><em class="italic"><span class="koboSpan" id="kobo.1257.1">A</span></em><span class="subscript"><span class="koboSpan" id="kobo.1258.1">t</span></span><span class="koboSpan" id="kobo.1259.1"> = </span><em class="italic"><span class="koboSpan" id="kobo.1260.1">α</span></em><span class="koboSpan" id="kobo.1261.1"> • </span><em class="italic"><span class="koboSpan" id="kobo.1262.1">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.1263.1">t  </span></span><span class="koboSpan" id="kobo.1264.1">+ (1 – </span><em class="italic"><span class="koboSpan" id="kobo.1265.1">α</span></em><span class="koboSpan" id="kobo.1266.1">) • </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1267.1">EM</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1268.1">A</span></em></span><span class="No-Break"><span class="subscript"><span class="koboSpan" id="kobo.1269.1">t–</span></span></span><span class="No-Break"><span class="subscript"><span class="koboSpan" id="kobo.1270.1">1</span></span></span></p>
			<p><span class="koboSpan" id="kobo.1271.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.1272.1">α</span></em><span class="koboSpan" id="kobo.1273.1"> is the </span><span class="No-Break"><span class="koboSpan" id="kobo.1274.1">smoothing factor.</span></span></p>
			<p><span class="koboSpan" id="kobo.1275.1">Now, let’s discuss how the SMA and EMA can be used for outlier detection and smoothing in the context of the stock price data example we’ve been </span><span class="No-Break"><span class="koboSpan" id="kobo.1276.1">working with.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1277.1">Smoothing with SMA and EMA on the stock price use case</span></h3>
			<p><span class="koboSpan" id="kobo.1278.1">Continuing </span><a id="_idIndexMarker901"/><span class="koboSpan" id="kobo.1279.1">with the stock price data </span><a id="_idIndexMarker902"/><span class="koboSpan" id="kobo.1280.1">example we presented previously, let’s see the effect that SMA and EMA have on </span><span class="No-Break"><span class="koboSpan" id="kobo.1281.1">the data:</span></span></p>
			<p><span class="koboSpan" id="kobo.1282.1">First, let’s calculate the SMA with a window of </span><span class="No-Break"><span class="koboSpan" id="kobo.1283.1">12 months:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1284.1">Define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1285.1">window</span></strong><span class="koboSpan" id="kobo.1286.1"> size for SMA and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1287.1">span</span></strong><span class="koboSpan" id="kobo.1288.1"> size </span><span class="No-Break"><span class="koboSpan" id="kobo.1289.1">for EMA:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1290.1">
window_size = 20
span = 20</span></pre></li>				<li><span class="koboSpan" id="kobo.1291.1">Calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.1292.1">the SMA:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1293.1">
df['SMA'] = df['close'].rolling(window=window_size, min_periods=1).mean()</span></pre></li>				<li><span class="koboSpan" id="kobo.1294.1">Calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.1295.1">the EMA:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1296.1">
df['EMA'] = df['close'].ewm(span=span, adjust=False).mean()</span></pre></li>				<li><span class="koboSpan" id="kobo.1297.1">Calculate</span><a id="_idIndexMarker903"/><span class="koboSpan" id="kobo.1298.1"> the residuals for the SMA </span><span class="No-Break"><span class="koboSpan" id="kobo.1299.1">and EMA:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1300.1">
df['SMA_residuals'] = df['close'] - df['SMA'] df['EMA_residuals'] = df['close'] - df['EMA'] sma_window = 12
data['SMA'] = data['Passengers'].rolling(window=sma_window).mean()</span></pre></li>				<li><span class="koboSpan" id="kobo.1301.1">Plot the </span><a id="_idIndexMarker904"/><span class="koboSpan" id="kobo.1302.1">original time series and </span><span class="No-Break"><span class="koboSpan" id="kobo.1303.1">the SMA:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1304.1"><img src="image/B19801_11_15.jpg" alt="Figure 11.15 – SMA and EMA"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1305.1">Figure 11.15 – SMA and EMA</span></p>
			<p><span class="koboSpan" id="kobo.1306.1">In this example, we calculated the SMA and EMA using a window size of 20 and a span of 20, respectively. </span><span class="koboSpan" id="kobo.1306.2">The window size for SMA determines how many previous data points are included in calculating the average at each point in time. </span><span class="koboSpan" id="kobo.1306.3">Just like SMA, the frequency of your data points influences the choice of span. </span><span class="koboSpan" id="kobo.1306.4">If your data is daily, a span zone of 20 might represent roughly 20 days of </span><span class="No-Break"><span class="koboSpan" id="kobo.1307.1">historical data.</span></span></p>
			<p><span class="koboSpan" id="kobo.1308.1">Let’s discuss the generated plot a </span><span class="No-Break"><span class="koboSpan" id="kobo.1309.1">little more:</span></span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1310.1">SMA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1311.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1312.1">Smoothing effect</span></strong><span class="koboSpan" id="kobo.1313.1">: The SMA smooths the time series data by averaging the values within the</span><a id="_idIndexMarker905"/><span class="koboSpan" id="kobo.1314.1"> window, reducing noise, and highlighting the </span><span class="No-Break"><span class="koboSpan" id="kobo.1315.1">underlying trend</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1316.1">Outlier impact</span></strong><span class="koboSpan" id="kobo.1317.1">: While SMA reduces the impact of outliers, it can still be influenced by them since it considers all values within the </span><span class="No-Break"><span class="koboSpan" id="kobo.1318.1">window equally</span></span></li></ul></li>
				<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1319.1">EMA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1320.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1321.1">Smoothing effect</span></strong><span class="koboSpan" id="kobo.1322.1">: The </span><a id="_idIndexMarker906"/><span class="koboSpan" id="kobo.1323.1">EMA also smooths the data but gives more weight to recent observations, making it more responsive to </span><span class="No-Break"><span class="koboSpan" id="kobo.1324.1">recent changes</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1325.1">Outlier impact</span></strong><span class="koboSpan" id="kobo.1326.1">: EMA is less influenced by older outliers but can be more affected by recent ones due to its </span><span class="No-Break"><span class="koboSpan" id="kobo.1327.1">weighting scheme</span></span></li></ul></li>
			</ul>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1328.1">Finding a balance between smoothness and responsiveness</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1329.1">Larger window sizes result in smoother moving averages but may lag behind changes in the data. </span><span class="koboSpan" id="kobo.1329.2">Smaller window sizes make the moving average more responsive to short-term fluctuations but might introduce </span><span class="No-Break"><span class="koboSpan" id="kobo.1330.1">more noise.</span></span></p>
			<p><span class="koboSpan" id="kobo.1331.1">Remember the autocorrelation plot we created in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1332.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1333.1">.10</span></em><span class="koboSpan" id="kobo.1334.1">? </span><span class="koboSpan" id="kobo.1334.2">We can use the analysis to adjust the span or window size based on the observed autocorrelation patterns. </span><span class="koboSpan" id="kobo.1334.3">The following points will help you guide the selection of the window size </span><span class="No-Break"><span class="koboSpan" id="kobo.1335.1">and span:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.1336.1">Consider the frequency of your data points (daily, </span><span class="No-Break"><span class="koboSpan" id="kobo.1337.1">weekly, monthly).</span></span></li>
				<li><span class="koboSpan" id="kobo.1338.1">If the autocorrelation plot shows significant autocorrelation at shorter lags, a smaller span in EMA or a smaller window size in SMA can help maintain responsiveness to recent changes while mitigating the influence of </span><span class="No-Break"><span class="koboSpan" id="kobo.1339.1">short-term noise.</span></span></li>
				<li><span class="koboSpan" id="kobo.1340.1">If your data exhibits seasonal patterns, you might choose a window size or span that aligns with the seasonal cycle. </span><span class="koboSpan" id="kobo.1340.2">For example, if there’s a weekly seasonality, you might consider a window size of 5 or 7. </span><span class="koboSpan" id="kobo.1340.3">Use the autocorrelation chart to figure </span><span class="No-Break"><span class="koboSpan" id="kobo.1341.1">this out.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1342.1">To evaluate how well the</span><a id="_idIndexMarker907"/><span class="koboSpan" id="kobo.1343.1"> window models perform, we can use the </span><strong class="bold"><span class="koboSpan" id="kobo.1344.1">mean absolute error</span></strong><span class="koboSpan" id="kobo.1345.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1346.1">MAE</span></strong><span class="koboSpan" id="kobo.1347.1">), as well </span><a id="_idIndexMarker908"/><span class="koboSpan" id="kobo.1348.1">as the </span><strong class="bold"><span class="koboSpan" id="kobo.1349.1">mean squared error</span></strong><span class="koboSpan" id="kobo.1350.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1351.1">MSE</span></strong><span class="koboSpan" id="kobo.1352.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.1353.1">root mean squared error</span></strong><span class="koboSpan" id="kobo.1354.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1355.1">RMSE</span></strong><span class="koboSpan" id="kobo.1356.1">). </span><span class="koboSpan" id="kobo.1356.2">We can compare the errors between the original</span><a id="_idIndexMarker909"/><span class="koboSpan" id="kobo.1357.1"> data and the smoothed values</span><a id="_idIndexMarker910"/><span class="koboSpan" id="kobo.1358.1"> generated by these models, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1359.1">following figure:</span></span></p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1360.1"><img src="image/B19801_11_16.jpg" alt="Figure 11.16 – Performance metrics for SMA and EMA"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1361.1">Figure 11.16 – Performance metrics for SMA and EMA</span></p>
			<p><span class="koboSpan" id="kobo.1362.1">To make sure we have a clear</span><a id="_idIndexMarker911"/><span class="koboSpan" id="kobo.1363.1"> understanding of the different metrics presented in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1364.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1365.1">.16</span></em><span class="koboSpan" id="kobo.1366.1">, let’s look at this in </span><span class="No-Break"><span class="koboSpan" id="kobo.1367.1">more detail:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1368.1">MAE</span></strong><span class="koboSpan" id="kobo.1369.1">: This</span><a id="_idIndexMarker912"/><span class="koboSpan" id="kobo.1370.1"> represents the average magnitude of the errors in a set of predictions, providing a simple average of the absolute differences between predicted and </span><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">actual values</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1372.1">MSE</span></strong><span class="koboSpan" id="kobo.1373.1">: This </span><a id="_idIndexMarker913"/><span class="koboSpan" id="kobo.1374.1">measures the average squared differences between predicted and actual values, penalizing larger errors more heavily </span><span class="No-Break"><span class="koboSpan" id="kobo.1375.1">than MAE</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1376.1">RMSE</span></strong><span class="koboSpan" id="kobo.1377.1">: RMSE is </span><a id="_idIndexMarker914"/><span class="koboSpan" id="kobo.1378.1">the square root of MSE, offering an interpretable measure of the average magnitude of error, aligning with the scale of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1379.1">original data</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1380.1">Now that we </span><a id="_idIndexMarker915"/><span class="koboSpan" id="kobo.1381.1">know what these terms represent, let’s unpick what they show for our stock prices use case. </span><span class="koboSpan" id="kobo.1381.2">Lower MAE, MSE, and RMSE values indicate better performance of the smoothing method. </span><span class="koboSpan" id="kobo.1381.3">While MAE and RMSE are very close for </span><a id="_idIndexMarker916"/><span class="koboSpan" id="kobo.1382.1">SMA and EMA, the MSE is lower for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1383.1">exponential method.</span></span></p>
			<p><span class="koboSpan" id="kobo.1384.1">The following table compares and summarizes when to use the SMA </span><span class="No-Break"><span class="koboSpan" id="kobo.1385.1">and EMA:</span></span></p>
			<table id="table002-5" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1386.1">Criteria</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1387.1">SMA</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1388.1">EMA</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1389.1">Type </span><span class="No-Break"><span class="koboSpan" id="kobo.1390.1">of smoothing</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1391.1">Simple and uniform smoothing of data points over </span><span class="No-Break"><span class="koboSpan" id="kobo.1392.1">a window</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1393.1">More responsive and adaptable, giving more weight to recent </span><span class="No-Break"><span class="koboSpan" id="kobo.1394.1">data points</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1395.1">Weighing </span><span class="No-Break"><span class="koboSpan" id="kobo.1396.1">data points</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1397.1">Equal weight to all data points in </span><span class="No-Break"><span class="koboSpan" id="kobo.1398.1">the window</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1399.1">More weight to recent observations; older observations receive exponentially </span><span class="No-Break"><span class="koboSpan" id="kobo.1400.1">decreasing weights</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1401.1">Responsiveness </span><span class="No-Break"><span class="koboSpan" id="kobo.1402.1">to changes</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1403.1">Lagging indicator; slower to respond to </span><span class="No-Break"><span class="koboSpan" id="kobo.1404.1">recent changes</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1405.1">More responsive to recent changes; adapts quickly to shifts in </span><span class="No-Break"><span class="koboSpan" id="kobo.1406.1">the data</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1407.1">Suitability </span><span class="No-Break"><span class="koboSpan" id="kobo.1408.1">for stability</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1409.1">Suitable for stable and less volatile </span><span class="No-Break"><span class="koboSpan" id="kobo.1410.1">time series</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1411.1">Suitable for volatile or rapidly changing </span><span class="No-Break"><span class="koboSpan" id="kobo.1412.1">time series</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1413.1">Adaptability </span><span class="No-Break"><span class="koboSpan" id="kobo.1414.1">to trends</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1415.1">Smooths out long-term trends, suitable for identifying </span><span class="No-Break"><span class="koboSpan" id="kobo.1416.1">overall patterns</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1417.1">Adapts quickly to shifting trends, suitable for capturing </span><span class="No-Break"><span class="koboSpan" id="kobo.1418.1">recent changes</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1419.1">Use case </span><span class="No-Break"><span class="koboSpan" id="kobo.1420.1">example</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1421.1">Analyzing long-term trends and identifying </span><span class="No-Break"><span class="koboSpan" id="kobo.1422.1">seasonality patterns</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1423.1">Capturing short-term fluctuations and reacting to </span><span class="No-Break"><span class="koboSpan" id="kobo.1424.1">market volatility</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1425.1">Calculation </span><span class="No-Break"><span class="koboSpan" id="kobo.1426.1">complexity</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1427.1">Simpler calculation and easier to understand </span><span class="No-Break"><span class="koboSpan" id="kobo.1428.1">and implement</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1429.1">More complex calculations involve a </span><span class="No-Break"><span class="koboSpan" id="kobo.1430.1">smoothing factor</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1431.1">Table 11.2 – Comparison between SMA and EMA</span></p>
			<p><span class="koboSpan" id="kobo.1432.1">Beyond </span><a id="_idIndexMarker917"/><span class="koboSpan" id="kobo.1433.1">moving</span><a id="_idIndexMarker918"/><span class="koboSpan" id="kobo.1434.1"> average techniques, exploring advanced feature engineering steps such as lags and differencing can significantly enrich our understanding and predictive capabilities. </span><span class="koboSpan" id="kobo.1434.2">We’ll explore those in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1435.1">next section.</span></span></p>
			<h1 id="_idParaDest-234"><a id="_idTextAnchor271"/><span class="koboSpan" id="kobo.1436.1">Feature engineering for time series data</span></h1>
			<p><span class="koboSpan" id="kobo.1437.1">Effective</span><a id="_idIndexMarker919"/><span class="koboSpan" id="kobo.1438.1"> feature engineering is essential in time series analysis to uncover meaningful patterns and enhance predictive accuracy. </span><span class="koboSpan" id="kobo.1438.2">It involves transforming </span><a id="_idIndexMarker920"/><span class="koboSpan" id="kobo.1439.1">raw data into informative features that capture temporal dependencies, seasonal variations, and other relevant aspects of the time series. </span><span class="koboSpan" id="kobo.1439.2">The first technique we are going to explore is creating lags </span><span class="No-Break"><span class="koboSpan" id="kobo.1440.1">of features.</span></span></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor272"/><span class="koboSpan" id="kobo.1441.1">Lag features and their importance</span></h2>
			<p><span class="koboSpan" id="kobo.1442.1">Lag features</span><a id="_idIndexMarker921"/><span class="koboSpan" id="kobo.1443.1"> are a crucial aspect of time series feature engineering as they allow us to transform time series data into a format suitable for supervised learning models. </span><span class="koboSpan" id="kobo.1443.2">Lag features involve creating new variables that represent past observations of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1444.1">target variable:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1445.1">Lag 1</span></strong><span class="koboSpan" id="kobo.1446.1">: The value from the previous </span><span class="No-Break"><span class="koboSpan" id="kobo.1447.1">time step</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1448.1">Lag 2</span></strong><span class="koboSpan" id="kobo.1449.1">: The value from two time </span><span class="No-Break"><span class="koboSpan" id="kobo.1450.1">steps ago</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1451.1">Lag k</span></strong><span class="koboSpan" id="kobo.1452.1">: The value from k time </span><span class="No-Break"><span class="koboSpan" id="kobo.1453.1">steps ago</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1454.1">By shifting the time series data by a specified number of time steps (referred to as the lag), these past values are included as features in the model at the current timestamp. </span><span class="koboSpan" id="kobo.1454.2">As we know, time series data often exhibits temporal dependencies, where the current value is related to </span><a id="_idIndexMarker922"/><span class="koboSpan" id="kobo.1455.1">past observations. </span><span class="koboSpan" id="kobo.1455.2">Lag features help capture these dependencies, allowing the model to learn from </span><span class="No-Break"><span class="koboSpan" id="kobo.1456.1">historical patterns.</span></span></p>
			<p><span class="koboSpan" id="kobo.1457.1">Now, let’s discuss how the lag features can be used in the context of the stock price data example we’ve been </span><span class="No-Break"><span class="koboSpan" id="kobo.1458.1">working with.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1459.1">Creating lag features in the stock price use case</span></h3>
			<p><span class="koboSpan" id="kobo.1460.1">Continuing </span><a id="_idIndexMarker923"/><span class="koboSpan" id="kobo.1461.1">with the stock price data example we presented previously, let’s see the effect lag features have on </span><span class="No-Break"><span class="koboSpan" id="kobo.1462.1">the data:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1463.1">First, introduce more aggressive outliers in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1464.1">close</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1465.1"> column:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1466.1">
outlier_indices = np.random.choice(df.index, size=10, replace=False)
df.loc[outlier_indices[:5], 'close'] = df['close'] * 1.5 # Increase by 50%
df.loc[outlier_indices[5:], 'close'] = df['close'] * 0.5 # Decrease by 50%</span></pre></li>				<li><span class="koboSpan" id="kobo.1467.1">Use the following function to create </span><span class="No-Break"><span class="koboSpan" id="kobo.1468.1">lagged features:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1469.1">
def create_lagged_features(df, column, lags):
    for lag in lags:
        df[f'{column}_lag_{lag}'] =</span><strong class="source-inline"> </strong><strong class="bold"><span class="koboSpan" id="kobo.1470.1">df[column].shift(lag)</span></strong><span class="koboSpan" id="kobo.1471.1">
    return df # Define the lags to create lags = [1, 5, 10, 20]</span></pre></li>				<li><span class="koboSpan" id="kobo.1472.1"> Create lagged features for the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1473.1">close</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1474.1"> column:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1475.1">
df = create_lagged_features(df, 'close', lags)</span></pre></li>				<li><span class="koboSpan" id="kobo.1476.1">Plot the original time series and </span><span class="No-Break"><span class="koboSpan" id="kobo.1477.1">lagged datasets:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1478.1"><img src="image/B19801_11_17.jpg" alt="Figure 11.17 – Original versus lagged features"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1479.1">Figure 11.17 – Original versus lagged features</span></p>
			<p><span class="koboSpan" id="kobo.1480.1">As we</span><a id="_idIndexMarker924"/><span class="koboSpan" id="kobo.1481.1"> can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1482.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1483.1">.17</span></em><span class="koboSpan" id="kobo.1484.1">, lag 1 (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1485.1">close_lag_1</span></strong><span class="koboSpan" id="kobo.1486.1">) represents the closing price from the previous day, lag 5 (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1487.1">close_lag_5</span></strong><span class="koboSpan" id="kobo.1488.1">) represents the closing price from 5 days ago, and so on. </span><span class="koboSpan" id="kobo.1488.2">You can observe how each lag captures the historical values of the target variable. </span><span class="koboSpan" id="kobo.1488.3">When adding lagged features to a time series, the start date of the data shifts forward because the first few data points cannot be used until the specified lag period is complete. </span><span class="koboSpan" id="kobo.1488.4">This shift means that if you add more lags, the number of initial data points that lack complete lagged data increases, effectively pushing the start </span><span class="No-Break"><span class="koboSpan" id="kobo.1489.1">date forward.</span></span></p>
			<p><span class="koboSpan" id="kobo.1490.1">Feel free to experiment with different lag values and see the effect on the dataset. </span><span class="koboSpan" id="kobo.1490.2">Adjusting the lag values allows you to capture different degrees of temporal dependencies and trends in </span><span class="No-Break"><span class="koboSpan" id="kobo.1491.1">the data.</span></span></p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor273"/><span class="koboSpan" id="kobo.1492.1">Differencing time series</span></h2>
			<p><span class="koboSpan" id="kobo.1493.1">In </span><a href="B19801_04.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1494.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.1495.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1496.1">Cleaning Messy Data and Data Manipulation</span></em><span class="koboSpan" id="kobo.1497.1">, we discussed </span><a id="_idIndexMarker925"/><span class="koboSpan" id="kobo.1498.1">how calculating the time difference between two datetime objects using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1499.1">diff()</span></strong><span class="koboSpan" id="kobo.1500.1"> function can help us measure the time elapsed between consecutive events. </span><span class="koboSpan" id="kobo.1500.2">This technique is useful for understanding the temporal gaps in a sequence of timestamps. </span><span class="koboSpan" id="kobo.1500.3">Similarly, in time series analysis, differencing is a powerful technique that’s used to stabilize the </span><a id="_idIndexMarker926"/><span class="koboSpan" id="kobo.1501.1">mean of a time series by removing changes in the level of a time series, thus eliminating trend and seasonality. </span><span class="koboSpan" id="kobo.1501.2">Just as we calculated the time elapsed in the previous chapter, we can apply differencing to our stock market data to highlight changes over time. </span><span class="koboSpan" id="kobo.1501.3">However, we will also introduce a new term – </span><span class="No-Break"><span class="koboSpan" id="kobo.1502.1">seasonal differencing.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1503.1">Seasonal differencing</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.1504.1">Seasonal differencing</span></strong><span class="koboSpan" id="kobo.1505.1"> is a</span><a id="_idIndexMarker927"/><span class="koboSpan" id="kobo.1506.1"> technique that’s used to remove seasonal patterns from time series data, making it more stationary and suitable for analysis and forecasting. </span><span class="koboSpan" id="kobo.1506.2">Seasonal differencing involves subtracting the value of an observation from a previous observation </span><a id="_idIndexMarker928"/><span class="koboSpan" id="kobo.1507.1">at a lag equal to the </span><strong class="bold"><span class="koboSpan" id="kobo.1508.1">seasonal</span></strong><span class="koboSpan" id="kobo.1509.1"> period. </span><span class="koboSpan" id="kobo.1509.2">So, we need to identify the seasonal period with all the tools we provided previously and then take the seasonal period and difference the data </span><span class="No-Break"><span class="koboSpan" id="kobo.1510.1">on that.</span></span></p>
			<p><span class="koboSpan" id="kobo.1511.1">For monthly data with an annual seasonal pattern, we can use the following formula: </span></p>
			<p><em class="italic"><span class="koboSpan" id="kobo.1512.1">y</span></em><span class="koboSpan" id="kobo.1513.1">'</span><span class="subscript"><span class="koboSpan" id="kobo.1514.1">t</span></span><span class="koboSpan" id="kobo.1515.1"> =</span><em class="italic"><span class="koboSpan" id="kobo.1516.1">y</span></em><span class="subscript"><span class="koboSpan" id="kobo.1517.1">t</span></span><span class="koboSpan" id="kobo.1518.1"> – </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1519.1">y</span></em></span><span class="No-Break"><span class="subscript"><span class="koboSpan" id="kobo.1520.1">t–</span></span></span><span class="No-Break"><span class="subscript"><span class="koboSpan" id="kobo.1521.1">12</span></span></span></p>
			<p><span class="koboSpan" id="kobo.1522.1">For quarterly data, we can use the following formula: </span></p>
			<p><em class="italic"><span class="koboSpan" id="kobo.1523.1">y</span></em><span class="koboSpan" id="kobo.1524.1">'</span><span class="subscript"><span class="koboSpan" id="kobo.1525.1">t</span></span><span class="koboSpan" id="kobo.1526.1"> =</span><em class="italic"><span class="koboSpan" id="kobo.1527.1">y</span></em><span class="subscript"><span class="koboSpan" id="kobo.1528.1">t</span></span><span class="koboSpan" id="kobo.1529.1"> – </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1530.1">y</span></em></span><span class="No-Break"><span class="subscript"><span class="koboSpan" id="kobo.1531.1">t–</span></span></span><span class="No-Break"><span class="subscript"><span class="koboSpan" id="kobo.1532.1">4</span></span></span></p>
			<p><span class="koboSpan" id="kobo.1533.1">Here,  is the seasonally differenced series and  is the </span><span class="No-Break"><span class="koboSpan" id="kobo.1534.1">original series.</span></span></p>
			<p><span class="koboSpan" id="kobo.1535.1">Now, let’s discuss how the difference can be used in the context of the stock price data example we’ve been </span><span class="No-Break"><span class="koboSpan" id="kobo.1536.1">working with.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1537.1">Differencing the stock price data</span></h3>
			<p><span class="koboSpan" id="kobo.1538.1">To showcase</span><a id="_idIndexMarker929"/><span class="koboSpan" id="kobo.1539.1"> the seasonal differencing, we will introduce some seasonality in the stock market data. </span><span class="koboSpan" id="kobo.1539.2">Based on the analysis we’ve done so far, there is not a big seasonal component in the data. </span><span class="koboSpan" id="kobo.1539.3">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1540.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1541.1">Create a seasonal component (weekly seasonality with </span><span class="No-Break"><span class="koboSpan" id="kobo.1542.1">higher amplitude):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1543.1">
seasonal_component = 50 * np.sin(2 * np.pi * np.arange(n) / 5) # 5-day seasonality</span></pre></li>				<li><span class="koboSpan" id="kobo.1544.1">Generate </span><a id="_idIndexMarker930"/><span class="koboSpan" id="kobo.1545.1">random stock prices with </span><span class="No-Break"><span class="koboSpan" id="kobo.1546.1">added seasonality:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1547.1">
data = {
'open': np.random.uniform(100, 200, n) + seasonal_component,
'high': np.random.uniform(200, 300, n) + seasonal_component,
'low': np.random.uniform(50, 100, n) + seasonal_component,
'close': np.random.uniform(100, 200, n) + seasonal_component
}
df = pd.DataFrame(data, index=date_range)</span></pre></li>				<li><span class="koboSpan" id="kobo.1548.1">Calculate the </span><span class="No-Break"><span class="koboSpan" id="kobo.1549.1">first difference:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1550.1">
df['First Difference'] = </span><strong class="bold"><span class="koboSpan" id="kobo.1551.1">df['close'].diff()</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.1552.1">Calculate the </span><span class="No-Break"><span class="koboSpan" id="kobo.1553.1">second difference:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1554.1">
df['Second Difference'] = </span><strong class="bold"><span class="koboSpan" id="kobo.1555.1">df['First Difference'].diff()</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.1556.1">Finally, calculate the seasonal difference (</span><span class="No-Break"><span class="koboSpan" id="kobo.1557.1">weekly seasonality):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1558.1">
df['Seasonal Difference'] = df['close']</span><strong class="bold"><span class="koboSpan" id="kobo.1559.1">.diff(5)</span></strong></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1560.1">Let’s demonstrate differencing by plotting the first, second, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1561.1">seasonal differences:</span></span></p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1562.1"><img src="image/B19801_11_18.jpg" alt="Figure 11.18 – Original versus differenced series"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1563.1">Figure 11.18 – Original versus differenced series</span></p>
			<p><span class="koboSpan" id="kobo.1564.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1565.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1566.1">.18</span></em><span class="koboSpan" id="kobo.1567.1">, we can </span><a id="_idIndexMarker931"/><span class="koboSpan" id="kobo.1568.1">observe the first, second, and seasonal differencing. </span><span class="koboSpan" id="kobo.1568.2">We can see that in the original plot, there is some seasonality, but after the first difference, we can see that the seasonal component is minimized. </span><span class="koboSpan" id="kobo.1568.3">But how can we evaluate this more statistically? </span><span class="koboSpan" id="kobo.1568.4">Let’s perform some statistical tests to check for stationarity in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1569.1">time series.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1570.1">The Augmented Dickey-Fuller (ADF) test</span></h3>
			<p><span class="koboSpan" id="kobo.1571.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.1572.1">ADF</span></strong><span class="koboSpan" id="kobo.1573.1"> test is </span><a id="_idIndexMarker932"/><span class="koboSpan" id="kobo.1574.1">a statistical </span><a id="_idIndexMarker933"/><span class="koboSpan" id="kobo.1575.1">test that’s used to determine whether a time series is stationary or not. </span><span class="koboSpan" id="kobo.1575.2">The ADF test examines the null hypothesis that a unit root is present in a time series sample. </span><span class="koboSpan" id="kobo.1575.3">The presence of a unit root indicates that the time series is non-stationary. </span><span class="koboSpan" id="kobo.1575.4">The alternative hypothesis is that the time series is stationary. </span><span class="koboSpan" id="kobo.1575.5">For the ADF test, a more negative value indicates stronger evidence against the </span><span class="No-Break"><span class="koboSpan" id="kobo.1576.1">null hypothesis.</span></span></p>
			<p><span class="koboSpan" id="kobo.1577.1">The p-value represents the probability of obtaining test results at least as extreme as the observed results, assuming that the null hypothesis is true. </span><span class="koboSpan" id="kobo.1577.2">In the case of the ADF test, we want to </span><a id="_idIndexMarker934"/><span class="koboSpan" id="kobo.1578.1">see </span><em class="italic"><span class="koboSpan" id="kobo.1579.1">a small p-value to reject the null hypothesis </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1580.1">of non-stationarity</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1581.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.1582.1">To conclude that</span><a id="_idIndexMarker935"/><span class="koboSpan" id="kobo.1583.1"> a time series is stationary, we typically want to see </span><span class="No-Break"><span class="koboSpan" id="kobo.1584.1">the following:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1585.1">p-value &lt; 0.05</span></strong><span class="koboSpan" id="kobo.1586.1">: This is the most common threshold that’s used in statistical testing. </span><span class="koboSpan" id="kobo.1586.2">If p &lt; 0.05, we reject the null hypothesis at the 5% significance level. </span><span class="koboSpan" id="kobo.1586.3">This means we have strong evidence to conclude the series </span><span class="No-Break"><span class="koboSpan" id="kobo.1587.1">is stationary.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1588.1">Even smaller p-values</span></strong><span class="koboSpan" id="kobo.1589.1">: p &lt; 0.01 (1% significance level) and p &lt; 0.001 (0.1% significance level) provide even stronger evidence </span><span class="No-Break"><span class="koboSpan" id="kobo.1590.1">of stationarity.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1591.1">Let’s code up </span><span class="No-Break"><span class="koboSpan" id="kobo.1592.1">this test:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1593.1">
def adf_test(series, title=''):
    </span><strong class="bold"><span class="koboSpan" id="kobo.1594.1">result = adfuller(series.dropna(), autolag='AIC')</span></strong><span class="koboSpan" id="kobo.1595.1">
    print(f'Augmented Dickey-Fuller Test: {title}')
    print(f'ADF Statistic: {result[0]}')
    print(f'p-value: {result[1]}')
    for key, value in result[4].items():
        print(f' {key}: {value}')
    print('\n')</span></pre>			<p><span class="koboSpan" id="kobo.1596.1">Now, it’s time for the results! </span><span class="koboSpan" id="kobo.1596.2">We will run the test for the original time series (to check if it is stationary or not) and then for each of the differenced time series. </span><span class="koboSpan" id="kobo.1596.3">Let’s explain </span><span class="No-Break"><span class="koboSpan" id="kobo.1597.1">the results:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1598.1">
Augmented Dickey-Fuller Test: </span><strong class="bold"><span class="koboSpan" id="kobo.1599.1">Original Series</span></strong><span class="koboSpan" id="kobo.1600.1">
ADF Statistic: -3.5898552445987595
p-value: 0.005957961883734467
   1%: -3.4367333690404767
   5%: -2.8643583648001925
   10%: -2.568270618452702</span></pre>			<p><span class="koboSpan" id="kobo.1601.1">The</span><a id="_idIndexMarker936"/><span class="koboSpan" id="kobo.1602.1"> ADF statistic of -3.5899 is less than the 5% critical value of -2.8644, and the p-value is below 0.05. </span><span class="koboSpan" id="kobo.1602.2">This</span><a id="_idIndexMarker937"/><span class="koboSpan" id="kobo.1603.1"> indicates that we can reject the null hypothesis of the presence of a unit root, suggesting that </span><em class="italic"><span class="koboSpan" id="kobo.1604.1">the original series is likely stationary</span></em><span class="koboSpan" id="kobo.1605.1">. </span><span class="koboSpan" id="kobo.1605.2">However, the result is relatively close to the critical value, indicating </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1606.1">borderline stationarity</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1607.1">:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1608.1">
Augmented Dickey-Fuller Test: </span><strong class="bold"><span class="koboSpan" id="kobo.1609.1">First Difference</span></strong><span class="koboSpan" id="kobo.1610.1">
ADF Statistic: -11.786384523171499
p-value: 1.0064914317100746e-21
   1%: -3.4367709764382024
   5%: -2.8643749513463637
   10%: -2.568279452717228</span></pre>			<p><span class="koboSpan" id="kobo.1611.1">The ADF statistic of -11.7864 is well below the 5% critical value of -2.8644, and the p-value is extremely small. </span><span class="koboSpan" id="kobo.1611.2">This strongly suggests that the first-differenced series is stationary. </span><span class="koboSpan" id="kobo.1611.3">The significant drop in the ADF statistic compared to the original series indicates that first differencing has effectively removed any remaining trends or </span><span class="No-Break"><span class="koboSpan" id="kobo.1612.1">unit roots:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1613.1">
Augmented Dickey-Fuller Test: </span><strong class="bold"><span class="koboSpan" id="kobo.1614.1">Second Difference</span></strong><span class="koboSpan" id="kobo.1615.1">
ADF Statistic: -14.95687341689794
p-value: 1.2562905072914351e-27
   1%: -3.4367899468008916
   5%: -2.8643833180472744
   10%: -2.5682839089705536</span></pre>			<p><span class="koboSpan" id="kobo.1616.1">The ADF statistic of -14.9569 is much lower than the 5% critical value, and the p-value is extremely small. </span><span class="koboSpan" id="kobo.1616.2">This result suggests that the second-differenced series is also stationary. </span><span class="koboSpan" id="kobo.1616.3">However, </span><em class="italic"><span class="koboSpan" id="kobo.1617.1">over-differencing can lead to loss of meaningful patterns and increase noise</span></em><span class="koboSpan" id="kobo.1618.1">, so it’s essential to balance between achieving stationarity and maintaining the integrity of </span><span class="No-Break"><span class="koboSpan" id="kobo.1619.1">the series:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1620.1">
Augmented Dickey-Fuller Test: </span><strong class="bold"><span class="koboSpan" id="kobo.1621.1">Seasonal Differencing</span></strong><span class="koboSpan" id="kobo.1622.1">
ADF Statistic: -11.48334880444129
p-value: 4.933051350797084e-21
   1%: -3.4367899468008916
   5%: -2.8643833180472744
   10%: -2.5682839089705536</span></pre>			<p><span class="koboSpan" id="kobo.1623.1">Finally, the </span><a id="_idIndexMarker938"/><span class="koboSpan" id="kobo.1624.1">ADF statistic of -11.4833 is well below the 5% critical value, and the p-value is very small. </span><span class="koboSpan" id="kobo.1624.2">This </span><a id="_idIndexMarker939"/><span class="koboSpan" id="kobo.1625.1">indicates that seasonal differencing has successfully made the series stationary. </span><span class="koboSpan" id="kobo.1625.2">Seasonal differencing is particularly useful if the series exhibits periodic patterns at </span><span class="No-Break"><span class="koboSpan" id="kobo.1626.1">specific intervals.</span></span></p>
			<p><span class="koboSpan" id="kobo.1627.1">Given these results, the first difference appears to be the most appropriate choice for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1628.1">following reasons:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.1629.1">The original series is already stationary at the 1% level, but first differencing significantly </span><span class="No-Break"><span class="koboSpan" id="kobo.1630.1">improves stationarity</span></span></li>
				<li><span class="koboSpan" id="kobo.1631.1">The first difference yields a highly significant result (p-value: 1.006e-21) without risking </span><span class="No-Break"><span class="koboSpan" id="kobo.1632.1">over-differencing</span></span></li>
				<li><span class="koboSpan" id="kobo.1633.1">While the second difference shows an even more significant result, it may lead to over-differencing, which can introduce unnecessary complexity and potentially remove important information from </span><span class="No-Break"><span class="koboSpan" id="kobo.1634.1">the series</span></span></li>
				<li><span class="koboSpan" id="kobo.1635.1">Seasonal differencing also shows strong results, but unless there’s a clear seasonal pattern in your data, the simpler first difference method is </span><span class="No-Break"><span class="koboSpan" id="kobo.1636.1">generally preferred</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1637.1">In conclusion, first difference strikes a good balance between achieving stationarity and avoiding over-differencing. </span><span class="koboSpan" id="kobo.1637.2">Now, it’s time to discuss some of the most common use cases in the time </span><span class="No-Break"><span class="koboSpan" id="kobo.1638.1">series space.</span></span></p>
			<h1 id="_idParaDest-237"><a id="_idTextAnchor274"/><span class="koboSpan" id="kobo.1639.1">Applying time series techniques in different industries</span></h1>
			<p><span class="koboSpan" id="kobo.1640.1">The ability </span><a id="_idIndexMarker940"/><span class="koboSpan" id="kobo.1641.1">to analyze temporal patterns provides a competitive advantage in today’s data-driven world across various industries. </span><span class="koboSpan" id="kobo.1641.2">Here are some popular use cases across </span><span class="No-Break"><span class="koboSpan" id="kobo.1642.1">various industries:</span></span></p>
			<table id="table003-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1643.1">Sector</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1644.1">Use Case</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1645.1">Explanation</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="3">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1646.1">Finance</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1647.1">Stock </span><span class="No-Break"><span class="koboSpan" id="kobo.1648.1">market analysis</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1649.1">Analyzing historical stock prices and trading volumes to make informed </span><span class="No-Break"><span class="koboSpan" id="kobo.1650.1">investment decisions</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1651.1">Portfolio management</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1652.1">Assessing the performance of investment portfolios over time to optimize </span><span class="No-Break"><span class="koboSpan" id="kobo.1653.1">asset allocation</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1654.1">Risk assessment</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1655.1">Modeling and forecasting financial risks such as market volatility and </span><span class="No-Break"><span class="koboSpan" id="kobo.1656.1">credit defaults</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="3">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1657.1">Healthcare</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1658.1">Patient monitoring</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1659.1">Continuously tracking vital signs and health metrics for early detection </span><span class="No-Break"><span class="koboSpan" id="kobo.1660.1">of abnormalities</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1661.1">Epidemiology</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1662.1">Analyzing temporal patterns of disease spread and </span><span class="No-Break"><span class="koboSpan" id="kobo.1663.1">predicting outbreaks</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1664.1">Treatment effectiveness</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1665.1">Assessing the impact of medical interventions </span><span class="No-Break"><span class="koboSpan" id="kobo.1666.1">over time</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="3">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1667.1">Meteorology</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1668.1">Weather forecasting</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1669.1">Analyzing historical weather patterns to predict </span><span class="No-Break"><span class="koboSpan" id="kobo.1670.1">future conditions</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1671.1">Climate </span><span class="No-Break"><span class="koboSpan" id="kobo.1672.1">change studies</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1673.1">Monitoring long-term trends and variations in </span><span class="No-Break"><span class="koboSpan" id="kobo.1674.1">climate data</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1675.1">Natural </span><span class="No-Break"><span class="koboSpan" id="kobo.1676.1">disaster prediction</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1677.1">Early detection of potential disasters such as hurricanes, floods, </span><span class="No-Break"><span class="koboSpan" id="kobo.1678.1">and droughts</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="3">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1679.1">Manufacturing</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1680.1">Production planning</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1681.1">Forecasting demand and optimizing </span><span class="No-Break"><span class="koboSpan" id="kobo.1682.1">production schedules</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1683.1">Quality control</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1684.1">Monitoring and ensuring product quality </span><span class="No-Break"><span class="koboSpan" id="kobo.1685.1">over time</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1686.1">Equipment maintenance</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1687.1">Predictive maintenance based on the performance history </span><span class="No-Break"><span class="koboSpan" id="kobo.1688.1">of machinery</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="3">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1689.1">Marketing</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1690.1">Sales forecasting</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1691.1">Predicting future sales based on </span><span class="No-Break"><span class="koboSpan" id="kobo.1692.1">historical data</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1693.1">Customer engagement</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1694.1">Analyzing patterns of customer interaction with products </span><span class="No-Break"><span class="koboSpan" id="kobo.1695.1">and services</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1696.1">Campaign optimization</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1697.1">Evaluating the impact of marketing initiatives </span><span class="No-Break"><span class="koboSpan" id="kobo.1698.1">over time</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1699.1">Sector</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1700.1">Use Case</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1701.1">Explanation</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="3">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1702.1">Transportation</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1703.1">Traffic </span><span class="No-Break"><span class="koboSpan" id="kobo.1704.1">flow analysis</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1705.1">Monitoring and optimizing traffic patterns in </span><span class="No-Break"><span class="koboSpan" id="kobo.1706.1">urban areas</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1707.1">Vehicle tracking</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1708.1">Tracking the movement and efficiency of </span><span class="No-Break"><span class="koboSpan" id="kobo.1709.1">transportation fleets</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1710.1">Supply </span><span class="No-Break"><span class="koboSpan" id="kobo.1711.1">chain optimization</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1712.1">Forecasting demand and optimizing the movement of goods </span><span class="No-Break"><span class="koboSpan" id="kobo.1713.1">over time</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1714.1">Table 11.3 – Time series techniques use cases</span></p>
			<p><span class="koboSpan" id="kobo.1715.1">With that, we can</span><a id="_idIndexMarker941"/><span class="koboSpan" id="kobo.1716.1"> summarize </span><span class="No-Break"><span class="koboSpan" id="kobo.1717.1">this chapter.</span></span></p>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor275"/><span class="koboSpan" id="kobo.1718.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.1719.1">Time series analysis plays a pivotal role in extracting meaningful insights and making informed decisions in a wide range of industries. </span><span class="koboSpan" id="kobo.1719.2">As technology advances, sophisticated time series techniques will become increasingly integral to understanding complex temporal patterns and trends. </span><span class="koboSpan" id="kobo.1719.3">Whether in finance, healthcare, or transportation, the ability to analyze and forecast time-dependent data empowers organizations to adapt, optimize, and make strategic decisions in an </span><span class="No-Break"><span class="koboSpan" id="kobo.1720.1">ever-evolving landscape.</span></span></p>
			<p><span class="koboSpan" id="kobo.1721.1">In this chapter, we covered techniques for handling missing values and outliers, differencing methods, and feature engineering in time series analysis. </span><span class="koboSpan" id="kobo.1721.2">We learned how to use ffill and bfill for missing values and compared their effects on stock price data. </span><span class="koboSpan" id="kobo.1721.3">Differencing techniques, including first, second, and seasonal differencing, were applied to achieve stationarity and were evaluated using ADF tests. </span><span class="koboSpan" id="kobo.1721.4">We also explored lagged features for capturing temporal dependencies and assessed model performance using metrics such as MAE, MSE, and RMSE. </span><span class="koboSpan" id="kobo.1721.5">These skills will prepare you so that you can manage and analyze time series </span><span class="No-Break"><span class="koboSpan" id="kobo.1722.1">data effectively.</span></span></p>
			<p><span class="koboSpan" id="kobo.1723.1">In the next chapter, we will pivot to a different type of data – text. </span><span class="koboSpan" id="kobo.1723.2">Analyzing text data involves unique challenges and methodologies distinct from those used for numerical time series. </span><span class="koboSpan" id="kobo.1723.3">We will deep dive into text preprocessing and cover text cleaning techniques, tokenization strategies, and spelling correction approaches, all of which are essential for any </span><strong class="bold"><span class="koboSpan" id="kobo.1724.1">natural language processing</span></strong><span class="koboSpan" id="kobo.1725.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1726.1">NLP</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1727.1">) task.</span></span></p>
		</div>
	

		<div id="_idContainer115" class="Content">
			<h1 id="_idParaDest-239" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor276"/><span class="koboSpan" id="kobo.1.1">Part 3: Downstream Data Cleaning – Consuming Unstructured Data</span></h1>
			<p><span class="koboSpan" id="kobo.2.1">This part focuses on the challenges and techniques involved in processing unstructured data, such as text, images, and audio, in the context of modern machine learning, particularly </span><strong class="bold"><span class="koboSpan" id="kobo.3.1">large language models</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.5.1">LLMs</span></strong><span class="koboSpan" id="kobo.6.1">). </span><span class="koboSpan" id="kobo.6.2">It provides a comprehensive overview of how to prepare unstructured data types for machine learning applications, ensuring that the data is properly preprocessed for analysis and model training. </span><span class="koboSpan" id="kobo.6.3">The chapters cover essential preprocessing methods for text, as well as image and audio data, offering readers the tools to work with more complex and varied datasets in today’s </span><span class="No-Break"><span class="koboSpan" id="kobo.7.1">AI-driven landscape.</span></span></p>
			<p><span class="koboSpan" id="kobo.8.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">following chapters:</span></span></p>
			<ul>
				<li><a href="B19801_12.xhtml#_idTextAnchor277"><em class="italic"><span class="koboSpan" id="kobo.10.1">Chapter 12</span></em></a><em class="italic"><span class="koboSpan" id="kobo.11.1">, Text Preprocessing in the Era of LLMs</span></em></li>
				<li><a href="B19801_13.xhtml#_idTextAnchor302"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 13</span></em></a><em class="italic"><span class="koboSpan" id="kobo.13.1">, Image and Audio Preprocessing with LLMs</span></em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer116" class="Basic-Graphics-Frame">
			</div>
		</div>
	</body></html>