- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text Preprocessing in the Era of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the era of **Large Language Models** (**LLMs**), mastering text preprocessing
    is more crucial than ever. As LLMs grow in complexity and capability, the foundation
    of successful **Natural Language Processing** (**NLP**) tasks still lies in how
    well the text data is prepared. In this chapter, we will discuss text preprocessing,
    the foundation for any NLP Task. We will also explore essential preprocessing
    techniques, focusing on adapting them to maximize the potential of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Relearning text preprocessing in the era of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text cleaning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling rare words and spelling variations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turning tokens into embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The complete code for this chapter can be found in the following GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter12](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter12)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s install the necessary libraries we will use in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Relearning text preprocessing in the era of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Text preprocessing** involves the application of various techniques to raw
    textual data with the aim of cleaning, organizing, and transforming it into a
    format suitable for analysis or modeling. The primary goal is to enhance the quality
    of the data by addressing common challenges associated with unstructured text.
    This entails tasks such as cleaning irrelevant characters, handling variations,
    and preparing the data for downstream NLP tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: With the rapid advancements in LLMs, the landscape of NLP has evolved significantly.
    However, fundamental preprocessing techniques such as text cleaning and tokenization
    remain crucial, albeit with some shifts in approach and importance.
  prefs: []
  type: TYPE_NORMAL
- en: Staring with text cleaning, while LLMs have shown remarkable robustness to noise
    in input text, clean data still yields better results and is especially important
    for fine-tuning tasks. Basic cleaning techniques such as removing HTML tags, handling
    special characters, and normalizing text are still relevant. However, more advanced
    techniques such as spelling correction may be less critical for LLMs, as they
    can often handle minor spelling errors. Domain-specific cleaning remains important,
    especially when dealing with specialized vocabulary or jargon.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization has evolved with the advent of subword tokenization methods used
    by most modern LLMs such as **Byte-Pair Encoding** (**BPE**) or WordPiece. Traditional
    word-level tokenization is less common in LLM contexts. Some traditional NLP preprocessing
    steps such as stopword removal, stemming, and lemmatization have become less critical.
    Stopword removal, which involves eliminating common words such as “and” or “the,”
    is less necessary because LLMs can understand their contextual importance and
    how they contribute to the meaning of a sentence. Similarly, stemming and lemmatization,
    which reduce words to their base forms (e.g., “running” to “run”), are less frequently
    used because LLMs can interpret different word forms accurately and understand
    their relationships within the text. This shift allows for a more nuanced understanding
    of language, capturing subtleties that rigid preprocessing might miss.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key message is that while LLMs can handle raw text impressively, preprocessing
    remains crucial in certain scenarios as it can improve model performance on specific
    tasks. Remember: **garbage in, garbage out**. Cleaning and standardizing text
    can also reduce the number of tokens processed by an LLM, potentially lowering
    computational costs. New approaches are emerging that blend traditional preprocessing
    with LLM capabilities, using LLMs themselves for data cleaning and preprocessing
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, while LLMs have reduced the need for extensive preprocessing
    in many NLP tasks, understanding and judiciously applying these fundamental techniques
    remains valuable. In the following sections, we will focus on the text preprocessing
    techniques that remain relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Text cleaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary goal of text cleaning is to transform unstructured textual information
    into a standardized and more manageable form. While cleaning text, several operations
    are commonly performed, such as the removal of HTML tags, special characters,
    and numerical values, as well as the standardization of letter cases and the handling
    of whitespaces and formatting issues. These operations collectively contribute
    to refining the quality of textual data and reducing its ambiguity. Let’s deep
    dive into these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Removing HTML tags and special characters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HTML tags are often present due to the extraction of content from web pages.
    These tags, such as `<p>`, `<a>`, or `<div>`, carry *no semantic meaning* in the
    context of NLP and must be removed. The cleaning process involves the identification
    and stripping of HTML tags, leaving behind only the actual words.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, let’s consider a scenario where we have a dataset of user
    reviews for a product and want to prepare the text data for sentiment analysis.
    You can find the code for this section in the GitHub repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/1.text_cleaning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/1.text_cleaning.py).
    In this script, the data generation is also available for you, and you can follow
    the example step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we’ve included key code snippets to illustrate the
    most important concepts. However, to see the complete code, including the libraries
    used, and to run the full end-to-end examples, please visit the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first text preprocessing step that we will execute is the removal of HTML
    tags. Let’s have a look at the code step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the libraries for this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The sample user reviews are shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a function that uses `BeautifulSoup` to parse the HTML content
    and extract only the text, removing any HTML tags:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we preprocess all the reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we get the preprocessed reviews as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we can see, all the HTML tags have been removed and the text is clean. We
    will continue enhancing this example by adding another common preprocessing step:
    handling the capitalization of text.'
  prefs: []
  type: TYPE_NORMAL
- en: Handling capitalization and letter case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text data often comes in various cases—uppercase, lowercase, or a mix of both.
    Inconsistent capitalization can lead to ambiguity in language processing tasks.
    Therefore, one common text-cleaning practice is to standardize the letter case
    throughout the corpus. This not only aids in maintaining consistency but also
    ensures that the model generalizes well across different cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building on the previous example, we are going to expand the preprocessing
    function to add one extra step: that of letter standardization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first remind ourselves what the reviews looked like after the removal
    of HTML tags from the previous preprocessing step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following function will convert all characters into lowercase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will expand the `preprocess_text` function we presented in the previous
    example to convert all characters in the text to lowercase, making the text case
    insensitive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s print the preprocessed reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The lower-cased reviews are presented here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice how all the letters have turned to lower case! Go ahead and update the
    capitalization function as follows to turn everything to upper case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The upper case reviews are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In case you are wondering whether you should use lower or upper case, we’ve
    got you covered.
  prefs: []
  type: TYPE_NORMAL
- en: Lower or upper case?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The choice between using lowercase or uppercase text depends on the specific
    requirements of the NLP task. For instance, tasks such as sentiment analysis typically
    benefit from lowercasing, as it simplifies the text and reduces variability. Conversely,
    tasks such as **Named Entity Recognition** (**NER**) may require preserving case
    information to accurately identify and differentiate entities.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in German, all nouns are capitalized, so maintaining the case is
    crucial for correct language representation. In contrast, English typically does
    not use capitalization to convey meaning, so lowercasing might be more appropriate
    for general text analysis.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with text data from user inputs, such as social media posts or
    reviews, it’s important to consider the role of case variations. For instance,
    a tweet may use mixed case for emphasis or tone, which could be relevant for sentiment
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Modern LLMs such as **Bidirectional Encoder Representations from Transformers**
    (**BERT**) and GPT-3 are trained on mixed-case text and handle both uppercase
    and lowercase effectively. These models utilize case information to enhance context
    and understanding. Their tokenizers are designed to manage case sensitivity inherently,
    processing text without needing explicit conversion.
  prefs: []
  type: TYPE_NORMAL
- en: If your task requires distinguishing between different cases (e.g., recognizing
    proper nouns or acronyms), it is better to preserve the original casing. However,
    always consult the documentation and best practices for the specific model you
    are using. Some models might be optimized for lowercased input and could perform
    better if the text is converted to lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to learn how we can deal with numerical values and symbols
    in the text.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with numerical values and symbols
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Numerical values, symbols, and mathematical expressions may be present in text
    data but may not always contribute meaningfully to the context. Cleaning them
    involves deciding whether to retain, replace, or remove these elements based on
    the specific requirements of the task.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in sentiment analysis, numerical values might be less relevant,
    and their presence could be distracting. In contrast, for tasks related to quantitative
    analysis or financial sentiment, preserving numerical information becomes crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building on the previous example, we are going to remove all the numbers and
    symbols in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s review how the data looked like after the previous preprocessing step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let’s add a function that removes all characters from the text *except
    alphabetic characters* *and spaces*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the text preprocessing pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s have a look at the preprocessed reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, after this preprocessing step, all the punctuations and symbols
    have been removed from the text. The decision to retain, replace, or remove symbols
    and punctuation during text preprocessing depends on the specific goals of your
    NLP task and the characteristics of your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Retaining symbols and punctuation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the advancements in LLMs, the approach to handling punctuation and symbols
    during text preprocessing has evolved significantly. Modern LLMs benefit from
    retaining punctuation and symbols due to their extensive training on diverse datasets.
    This retention helps these models understand context more accurately by capturing
    nuances such as emotions, emphasis, and sentence boundaries. For instance, punctuation
    marks such as exclamation points and question marks play a crucial role in sentiment
    analysis by conveying strong emotions, which improves the model’s performance.
    Similarly, in tasks such as text generation, punctuation maintains readability
    and structure, while in NER and translation, it aids in identifying proper nouns
    and sentence boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there are scenarios where removing punctuation and symbols
    can be advantageous. Modern LLMs are robust enough to handle noisy data, but in
    certain applications, simplifying text by removing punctuation can streamline
    preprocessing and *reduce the number of unique tokens*. This approach is beneficial
    for tasks such as topic modeling and clustering, where the focus is on content
    rather than structural elements. For example, removing punctuation can help identify
    core topics by eliminating distractions from sentence structure, and in text classification,
    it can standardize input data when punctuation does not add significant value.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is replacing punctuation and symbols with spaces or specific
    tokens, which helps in normalizing text while preserving some level of separation
    between tokens. This method can be particularly useful for custom tokenization
    strategies. In specialized NLP pipelines, replacing punctuation with specific
    tokens can retain important distinctions without adding unnecessary clutter to
    the text, facilitating more effective tokenization and preprocessing for downstream
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see a quick example on how to remove or replace symbols and punctuation.
    You can find the code for this section at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/2.punctuation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/2.punctuation.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the sample text:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Option 1: replace symbols and punctuation with spaces:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will print the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Option 2: remove symbols and punctuation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will print the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Removing symbols and numbers is a crucial preprocessing step in text analysis
    that simplifies text by eliminating non-alphanumeric characters. The last thing
    we will discuss in this section is addressing whitespace issues to enhance text
    readability and ensure consistent formatting.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing whitespace and formatting issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whitespaces and formatting inconsistencies can be prevalent in text data, especially
    when it originates from diverse sources. Cleaning involves addressing issues such
    as multiple consecutive spaces, leading or trailing whitespaces, and variations
    in formatting styles. Regularization of whitespace ensures a standardized text
    representation, reducing the risk of misinterpretation by downstream models.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing whitespace and formatting issues remains crucial in the world of
    LLMs. Although modern LLMs exhibit robustness to various formatting inconsistencies,
    managing whitespace and formatting effectively can still enhance model performance
    and ensure data consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing whitespace and formatting creates a uniform dataset, which facilitates
    model training and analysis by minimizing noise and focusing attention on the
    content rather than formatting discrepancies. Enhanced readability, achieved through
    proper whitespace management, aids both human and machine learning interpretation
    by clearly delineating text elements. Furthermore, consistent whitespace handling
    is essential for accurate tokenization—a fundamental process in many NLP tasks—as
    it ensures precise identification and processing of words and phrases.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s go back to the review example and add another step in the pipeline
    to remove whitespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by addressing whitespace and formatting issues. This function removes
    extra spaces and ensures that there is only one space between words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we’ll add this to our text preprocessing pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s have a look at the reviews before applying the new step and focus on
    the whitespaces marked here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s review the clean dataset, after having applied the whitespace
    removal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Let’s move from pure text cleaning to focusing on safeguarding the data.
  prefs: []
  type: TYPE_NORMAL
- en: Removing personally identifiable information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When preprocessing text data, removing **Personally Identifiable Information**
    (**PII**) is crucial for maintaining privacy, ensuring compliance with regulations,
    and improving data quality. For instance, consider a dataset of user reviews that
    includes names, email addresses, and phone numbers. If this sensitive information
    is not anonymized or removed, it poses significant risks such as privacy violations
    and potential misuse. Regulations such as the **General Data Protection Regulation**
    (**GDPR**), **California Consumer Privacy Act** (**CCPA**), and **Health Insurance
    Portability and Accountability Act** (**HIPAA**) mandate that personal data must
    be handled carefully. Failing to remove PII can lead to legal penalties and loss
    of trust. Moreover, including identifiable details can introduce bias into machine
    learning models and compromise their generalization. Removing PII is essential
    for responsible AI development, as it allows for the creation and use of datasets
    that maintain individual privacy while still providing valuable insights for research
    and analysis
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates how to use the presidio-analyzer and
    presidio-anonymizer libraries to detect and anonymize PII. Let’s have a look at
    the code step by step. The full code can be accessed at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/3.pii_detection.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/3.pii_detection.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the required libraries for this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a sample DataFrame with one column named `text` containing sentences
    with different types of PII (e.g., names, email addresses, and phone numbers):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize `AnalyzerEngine` for *detecting* PII entities and `AnonymizerEngine`
    for *anonymizing* the detected PII entities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we’ll define an anonymization function that detects PII in the text and
    applies masking rules based on the entity type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `anonymize_text` function is designed to protect sensitive information within
    a given text by anonymizing specific types of entities. It first analyzes the
    text to identify entities such as names (`PERSON`), email addresses (`EMAIL_ADDRESS`),
    and phone numbers (`PHONE_NUMBER`) using an analyzer. For each entity type, it
    then applies a masking operation to conceal part of the information. Specifically,
    it masks the last four characters of a person’s name, the last five characters
    of an email address, and the last six characters of a phone number. The function
    returns the text with these sensitive entities anonymized, ensuring that personal
    information is obscured while retaining the overall structure of the text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Apply the anonymization function to the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By using these configurations, you can tailor the anonymization process to meet
    specific requirements and ensure that sensitive information is properly protected.
    This approach helps you comply with privacy regulations and protect sensitive
    information in your datasets.
  prefs: []
  type: TYPE_NORMAL
- en: While removing PII is essential for protecting privacy and ensuring data compliance,
    another critical aspect of text preprocessing is handling rare words and spelling
    variations.
  prefs: []
  type: TYPE_NORMAL
- en: Handling rare words and spelling variations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rise of LLMs has revolutionized how we interact with technology and process
    information, particularly in the world of handling spelling variations and rare
    words. Before the emergence of LLMs, managing these linguistic challenges required
    extensive manual effort, often involving specialized knowledge and painstakingly
    crafted algorithms. Traditional spell-checkers and language processors struggled
    with rare words and variations, leading to frequent errors and inefficiencies.
    Today, LLMs such as GPT-4, Lllama3, and others have transformed this landscape
    by leveraging vast datasets and sophisticated machine-learning techniques to understand
    and generate text that accommodates a wide range of spelling variations and uncommon
    terminology. These models can recognize and correct misspellings, provide contextually
    appropriate suggestions, and accurately interpret rare words, enhancing the precision
    and reliability of text processing.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with rare words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the era of LLMs such as GPT-3 and GPT-4, handling rare words has become less
    of a challenge compared to traditional NLP methods. These models have been trained
    on vast and diverse datasets, enabling them to understand and generate text with
    rare or even unseen words. However, there are still some considerations for text
    preprocessing and handling rare words effectively.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we handle rare words with LLMs? There are some key concepts we need
    to understand, starting with tokenization. We won’t explore tokenization in detail
    here as we have a dedicated section later on; for now, let’s say that LLMs use
    **subword tokenization** methods that break down rare words into more common subword
    units. This helps in managing **Out-of-Vocabulary** (**OOV**) words by decomposing
    them into familiar components. The other interesting thing about LLMs is that
    even if they don’t know the word per se, they have contextual understanding capabilities,
    meaning that LLMs leverage context to infer the meaning of rare words.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code example, we will test GPT-2 to see if it can handle rare
    words. You can find the code in the repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/4.rare_words.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/4.rare_words.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the GPT-2 tokenizer and model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a text prompt with a rare word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Encode the input text to tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate text until the output length reaches 50 tokens. The model generates
    text based on the input prompt, leveraging its understanding of the context to
    handle the rare word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `generate` function in the given code snippet is used to produce text output
    from a model based on the input tokens provided. The parameters used in this function
    call control various aspects of the text generation process:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`indexed_tokens`: This represents the input sequence that the model will use
    to start generating text. It consists of tokenized text that serves as the starting
    point for generation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length=50`: This parameter sets the maximum length of the generated text.
    The model will generate up to 50 tokens, including the input tokens, ensuring
    that the output doesn’t exceed this length.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beams=5`: This controls the beam search process, where the model keeps
    track of the top five most likely sequences during generation. Beam search helps
    improve the quality of the generated text by exploring multiple possible outcomes
    simultaneously and selecting the most likely one.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_repeat_ngram_size=2`: This prevents the model from repeating any sequence
    of two tokens (bigrams) within the generated text. It helps produce more coherent
    and less repetitive output by ensuring that the same phrases don’t appear multiple
    times.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`early_stopping=True`: This parameter allows the generation process to stop
    early if all beams have reached the end of the text sequence (e.g., a sentence-ending
    token). This can make the generation process more efficient by avoiding unnecessary
    continuation when a complete and sensible output has already been produced.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These parameters can be adjusted depending on the desired output. For instance,
    increasing `max_length` generates longer text, while modifying `num_beams` can
    balance quality and computational cost. Adjusting `no_repeat_ngram_size` changes
    the strictness of repetition prevention, and toggling `early_stopping` can affect
    the efficiency and length of the generated text. *I would advise that you go and
    play with these configurations to see how their output* *is affected*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated tokens are decoded back into human-readable text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the decoded text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the model understood the meaning of *quokka* and created a sequence
    of words, additional text that continues from the prompt, showcasing the language
    generation capabilities of LLMs. This is possible because LLMs turn the tokens
    into a numerical representation called **embeddings**, as we will see later on,
    that capture the meaning of words.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the use of rare words in text preprocessing. Let’s now move to
    another challenge—spelling errors and typos.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing spelling variations and typos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The challenge with spelling variations and typos is that it can lead to *different
    tokenizations for similar words*. In the era of LLMs, handling spelling and typos
    has become more sophisticated. LLMs can understand contexts and generate text
    that often corrects such errors implicitly. However, explicit preprocessing to
    correct spelling mistakes can still enhance the performance of these models, especially
    in applications where accuracy is critical. There are different ways to address
    spelling variations and mistakes, as we will see in the following section, starting
    with spelling correction.
  prefs: []
  type: TYPE_NORMAL
- en: Spelling correction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s create an example of fixing spelling mistakes using an LLM with Hugging
    Face Transformers. We’ll use the experimental `oliverguhr/spelling-correction-english-base`
    spelling correction model for this demonstration. You can find the full code at
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/5.spelling_checker.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/5.spelling_checker.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the spelling function pipeline. Inside this function, we initialize
    the spelling correction pipeline using the `oliverguhr/spelling-correction-english-base`
    model. This model is specifically trained for spelling correction tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the pipeline to generate the corrected text. The `max_length` parameter
    is set to `2048` to allow for longer input texts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the function with some sample text containing spelling mistakes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It’s important to note that this is an experimental model, and its performance
    may vary depending on the complexity and context of the input text. For more robust
    spelling and grammar correction, you might consider using more advanced models;
    however, some of them need authentication to download or sign agreements. So,
    for simplicity, we used an experimental model here. You can replace it with any
    model you have access to, from Llama3 to GPT4 and others.
  prefs: []
  type: TYPE_NORMAL
- en: The significance of spelling correction in text preprocessing tasks takes us
    nicely to the concept of fuzzy matching, a technique that further enhances the
    accuracy and relevance of generated content by accommodating minor errors and
    variations in input text.
  prefs: []
  type: TYPE_NORMAL
- en: Fuzzy matching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fuzzy matching is a technique used to compare strings for similarity, even when
    they are not exactly the same. It’s like finding words that are “kind of similar”
    or “close enough.” So, we can use fuzzy matching algorithms to identify and map
    similar words, as well as to solve for variations and minor misspellings. We can
    enhance the spelling correction function by adding fuzzy matching using the `TheFuzz`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the code that you can find at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/6.fuzzy_matching.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/6.fuzzy_matching.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by installing the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the spelling correction pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `oliverguhr/spelling-correction-english-base` model is specifically fine-tuned
    for the task of spelling correction, making it a highly effective and efficient
    tool for spelling correction. This model has been trained to recognize and correct
    common spelling errors in English text, leading to greater accuracy. It is optimized
    for text-to-text generation, which allows it to efficiently generate corrected
    versions of input text with minimal computational overhead. Additionally, its
    training likely involved exposure to datasets containing spelling errors and their
    corrections, enabling it to make informed and contextually appropriate corrections.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the corrected text as in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the original and corrected texts into words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a dictionary of common English words (you can expand this list):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fuzzy match each word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the function with some sample text containing spelling mistakes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, as you can see, not all the spelling mistakes are corrected. We could get
    some better performance by fine-tuning the model on the examples it usually misses.
    However, there is good news! The rise of LLMs has made it less critical to correct
    spelling mistakes because these models are designed to understand and process
    text contextually. Even when words are misspelled, LLMs can infer the intended
    meaning by analyzing the surrounding words and overall sentence structure. This
    ability reduces the need for perfect spelling, as the primary focus shifts to
    conveying the message rather than ensuring every word is spelled correctly.
  prefs: []
  type: TYPE_NORMAL
- en: After completing the initial text preprocessing steps, the next critical phase
    is **chunking**. This process involves breaking the cleaned text into smaller,
    meaningful units. Let’s discuss that in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chunking is an essential preprocessing step in NLP that involves breaking down
    text into smaller, manageable units, or “chunks.” This process is crucial for
    various applications, including text summarization, sentiment analysis, information
    extraction, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Why is chunking becoming more and more important? By breaking down large documents,
    chunking enhances manageability and efficiency, particularly for models with *token
    limits*, preventing overload and enabling smoother processing. It also improves
    accuracy by allowing models to *focus on smaller, coherent segments of text*,
    which reduces noise and complexity compared to analyzing entire documents. Additionally,
    chunking helps maintain context within each segment, which is essential for tasks
    such as machine translation and text generation, ensuring that the model comprehends
    and processes the text effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking can be implemented in many different ways; for instance, summarization
    may benefit from paragraph-level chunks, whereas sentiment analysis might use
    sentence-level chunks to capture nuanced emotional tones. In the following sections,
    we will focus on fixed-length, recursive, and semantic chunking as we see them
    more often in the data world.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing fixed-length chunking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Fixed-length chunking** involves breaking text into chunks of a *predefined
    length*, either by character count or token count. It is usually preferred because
    it is very simple to implement and ensures uniform chunk sizes. However, as the
    split is random, it may split sentences or semantic units, leading to a loss of
    context. It is suitable for tasks where uniform chunk sizes are needed, such as
    certain types of text classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To showcase fixed-length chunking, we are going to work with review data again,
    but we will include a few lengthier reviews. You can see the full example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/7.fixed_chunking.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/7.fixed_chunking.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by loading the example data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `TokenTextSplitter` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the `TokenTextSplitter` class with a chunk size of `50` tokens and
    no overlap:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Combine the reviews into a single text block for chunking:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the text into token-based chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To understand how varying chunk sizes affect the output, you can modify the
    `chunk_size` parameter. For instance, you might try chunk sizes of `20`, `70`,
    and `150` tokens. Here, you can see how you can adapt the code to test different
    chunk sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We successfully divided our review into the required chunks, but before moving
    forward, it’s crucial to understand the significance of the `chunk_overlap=0`parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Chunk overlap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Chunk overlap** refers to the number of characters or tokens that are *shared*
    between adjacent chunks when splitting a text. It’s the amount of text that “overlaps”
    between one chunk and the next.'
  prefs: []
  type: TYPE_NORMAL
- en: Chunk overlap is crucial as it helps preserve context and enhance the coherence
    of the text. By ensuring that adjacent chunks share some common content, overlap
    *maintains continuity* and prevents important information from being lost at the
    boundaries. For instance, if a document is divided into chunks without overlap,
    a critical piece of information could be split between two chunks, potentially
    rendering it inaccessible or causing a loss of meaning. In retrieval tasks, such
    as searching or question-answering, overlap ensures that relevant details are
    captured even if they fall across chunk boundaries, thereby improving the effectiveness
    of the retrieval process. For example, if a chunk ends mid-sentence, the overlap
    ensures that the entire sentence is considered, which is essential for accurate
    comprehension and response generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider a simple example to illustrate chunk overlap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'With a chunk size of five words and an overlap of one word, we’ll get the following
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, each chunk overlaps with the next by *two words*, helping to
    maintain context and prevent loss of meaning at chunk boundaries. Fixed-length
    chunking divides text into segments of a uniform size, but this method can sometimes
    fail to capture meaningful units of text, especially when dealing with natural
    language’s inherent variability. Transitioning to paragraph chunking, on the other
    hand, allows for a more contextually coherent approach by segmenting text based
    on its natural structure.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing RecursiveCharacter chunking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`RecursiveCharacterTextSplitter` is a sophisticated text-splitting tool designed
    to handle more complex text segmentation tasks, especially when dealing with lengthy
    documents that need to be broken down into smaller, meaningful chunks. Unlike
    basic text splitters that simply cut text into fixed or variable-sized chunks,
    `RecursiveCharacterTextSplitter` uses a recursive approach to divide text, ensuring
    that each chunk is both contextually coherent and appropriately sized for processing
    by natural language models. Continuing from the review example, we will now demonstrate
    how to split a document into paragraphs using `RecursiveCharacterTextSplitter`.
    You can find the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/8.paragraph_chunking.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/8.paragraph_chunking.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a `RecursiveCharacterTextSplitter` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `RecursiveCharacterTextSplitter` instance is instantiated with specific
    parameters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`separators`: This is a list of separators used to split the text. Here, it
    includes double newlines (`\n\n`), single newlines (`\n`), spaces (), and empty
    strings (`""`). This helps the splitter to use natural text boundaries and whitespace
    for chunking.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_size`: This is the maximum size of each chunk, set to 200 characters.
    This means each chunk will be *up to* 200 characters long.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_overlap`: This is the number of characters overlapping between adjacent
    chunks, set to 0\. This means there is no overlap between chunks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length_function`: This is a function used to measure the length of the text,
    set to `len`, which calculates the number of characters in a string.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Split the text into chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the chunks. In this first one, the user is very satisfied with the smartphone
    camera, praising the sharpness and vibrant colors of the photos. However, the
    user is disappointed with the laptop’s performance, citing frequent lags:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The user is pleased with the blender, noting its effectiveness in making smoothies,
    its power, and its ease of cleaning. They consider it a good value for the price:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The user had a negative experience with customer support, mentioning long wait
    times and unresolved issues. The user finds the book to be a fascinating read
    with an engaging storyline and well-developed characters, and they highly recommend
    it to readers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are left with one remaining word:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, these chunks are not perfect, but let’s understand how `RecursiveCharacterTextSplitter`
    works so that you can adjust it to your use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chunk size target**: The splitter aims for chunks of about 200 characters,
    but this is a maximum rather than a strict requirement. It will try to create
    chunks as close to 200 characters as possible without exceeding this limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recursive approach**: The recursive nature means it will apply these rules
    repeatedly, working its way through the separator list until it finds an appropriate
    split point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preserving semantic meaning**: By using this approach, the splitter attempts
    to keep semantically related content together. For example, it will try to avoid
    splitting in the middle of a paragraph or sentence if possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunk_overlap` set to `0`, there’s no repetition of content between chunks.
    Each chunk is distinct.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`len` function is used to measure chunk size, meaning it’s counting characters
    rather than tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The length_function parameter
  prefs: []
  type: TYPE_NORMAL
- en: The `length_function` parameter in `RecursiveCharacterTextSplitter` is a flexible
    option that allows you to define *how the length of text chunks is measured*.
    While `len` is the default and most common choice, there are many other options,
    from token-based to word-based to custom implementations.
  prefs: []
  type: TYPE_NORMAL
- en: While recursive chunking focuses on creating chunks based on fixed sizes and
    natural separators, semantic chunking takes this a step further by grouping text
    based on its meaning and context. This method ensures that chunks are not only
    coherent in length but also semantically meaningful, improving the relevance and
    accuracy of downstream NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing semantic chunking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Semantic chunking involves breaking text into chunks based on semantic meaning
    rather than just syntactic rules or fixed lengths. Behind the scenes, they use
    *embeddings* to group related sentences together (we will deep dive into embeddings
    in [*Chapter 13*](B19801_13.xhtml#_idTextAnchor302)*, Image and Audio Preprocessing
    with LLMs*). We usually use semantic chunking for tasks requiring a deep understanding
    of context, such as question answering and thematic analysis. Let’s deep dive
    into the process behind semantic chunking:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text input**: The process begins with a text input, which could be a document,
    a collection of sentences, or any textual data that needs to be processed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Embedding generation**: Each segment of the text (typically sentences or
    small groups of sentences (chunks)) is converted into a high-dimensional vector
    representation using embeddings. These embeddings are generated by pre-trained
    language models and the key to understand here is that these embeddings capture
    the semantic meaning of the text. In other words, we convert text into a numerical
    representation *that encodes* *its meaning*!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Similarity measurement**: The embeddings are then compared to measure the
    semantic similarity between different parts of the text. Techniques such as cosine
    similarity are often used to quantify how closely related different segments are.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Clustering**: Based on the similarity scores, sentences or text segments
    are clustered together. The clustering algorithm groups sentences that are semantically
    similar into the same chunk. This ensures that each chunk maintains semantic coherence
    and context.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chunk creation**: The clustered sentences are then combined to form chunks.
    These chunks are designed to be semantically meaningful units of text, which can
    be more effectively processed by NLP models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s go back to the product review example and see what chunk we generate
    with semantic chunking. You can find the code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/9.semantic_chunking.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/9.semantic_chunking.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize `SemanticChunker` with `HuggingFaceEmbeddings`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the text into chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each chunk contains related sentences that form a coherent segment. For example,
    chunk 1 discusses various product performances, while chunk 2 includes customer
    support experience and a book review. The chunks also maintain context within
    each segment, ensuring that related information is grouped together. A point for
    improvement is that chunk 1 includes reviews of different products (smartphone,
    laptop, and blender), and chunk 2 mixes a customer support experience with a book
    review, which could be seen as semantically unrelated. In this case, we could
    further split the text into smaller, more focused chunks to make it more coherent
    or/and tweak the parameters of the sematic chunker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find more details about these parameters in the documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html](https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the steps to improve chunking in our case could look something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: Use different embedding models to see which provides the best embeddings for
    your text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tweak the buffer size to find the right balance between chunk size and coherence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjust the threshold type and amount to optimize where chunks are split based
    on semantic breaks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customize the regular expression for sentence splitting to better fit the structure
    of your text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transitioning from chunking to tokenization involves moving from a process where
    text is divided into larger, often syntactically significant segments (chunks)
    to a process where text is divided into smaller, more granular units (tokens).
    Let’s take a look at how **tokenization** works.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenization is the process of breaking down a sequence of text into smaller
    units, or tokens, which can be words, subwords, or characters. This process is
    essential for converting text into a format suitable for *computational processing*,
    enabling models to learn patterns at a finer granularity.
  prefs: []
  type: TYPE_NORMAL
- en: Some key terms in the Tokenization phase are `[CLS]` for classification, `[SEP]`
    for separation, etc.). Each token in the vocabulary is assigned an ID, which the
    model uses to represent the token internally. These IDs are integers and typically
    range from 0 to the size of the vocabulary minus one.
  prefs: []
  type: TYPE_NORMAL
- en: Can all the words in the world fit into a vocabulary? The answer is *no*! OOV
    words are words that are not present in the model’s vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the main terms that are used, let’s explore the different types
    of tokenization and the challenges associated with them.
  prefs: []
  type: TYPE_NORMAL
- en: Word tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word tokenization involves splitting text into individual words.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the sentence “Tokenization is crucial in NLP!” would be tokenized
    into `["Tokenization", "is", "crucial", "in", "``NLP", "!"]`.
  prefs: []
  type: TYPE_NORMAL
- en: Word tokenization preserves whole words, which can be beneficial for tasks requiring
    word-level understanding. It works well for languages with clear word boundaries.
    It is a simple solution but can lead to problems with OOV words, especially in
    specialized domains such as medical texts and texts with a lot of misspellings.
    You can find the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/10.word_tokenisation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/10.word_tokenisation.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see a code example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the necessary NLTK data (run this once):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take the following as sample text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform word tokenization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This type of word tokenization is useful when dealing with simple, well-formed
    text where each word is clearly separated by spaces and punctuation. It’s an easy
    method that aligns well with how humans perceive words. However, different forms
    of the same word (e.g., “run”, “running”, “ran”) are treated as separate tokens,
    which can dilute the model’s understanding. It can also result in a large vocabulary,
    especially for languages with rich morphology or many unique words. Finally, these
    models struggle with words not seen during training, leading to OOV tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Given the limitations of word tokenization, subword tokenization methods have
    become popular. Subword tokenization strikes a balance between word-level and
    character-level tokenization, addressing many of the shortcomings of both.
  prefs: []
  type: TYPE_NORMAL
- en: Subword tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Subword tokenization** splits text into smaller units than words, typically
    subwords. By breaking the words into known subwords, it can handle OOV words.
    It reduces the vocabulary size and parameter count significantly. Let’s see the
    different options in the subword tokenization in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Byte Pair Encoding (BPE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BPE starts with individual characters and iteratively merges the most frequent
    pairs of tokens to create subwords. It was originally developed as a data compression
    algorithm but has been adapted for tokenization in NLP tasks. The process looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a vocabulary of individual characters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the frequency of all character pairs in the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge the most frequent pair of characters to form a new token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the process until the desired vocabulary size is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This frequency-based merging strategy can be useful for languages with simpler
    morphological structures (e.g., English) or when a straightforward yet robust
    tokenization is needed. It is simple and computationally efficient due to the
    frequency-based merging. Let’s demonstrate an example of how to implement BPE
    for tokenization. You can find the full example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/11.bpe_tokeniser.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/11.bpe_tokeniser.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the pre-trained tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the sample text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert tokens to input IDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the tokens and the IDs, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The special character “Ġ” in the tokenization output has a specific meaning
    in the context of BPE tokenization. It indicates that the token following it originally
    had a preceding space or was at the beginning of the text, so it allows for preserving
    information about word boundaries and spacing in the original text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explain the output we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Token` and `ization`: These are subwords of “Tokenization”, split without
    a “Ġ” because *they’re part of the* *same word*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`in`, `medical`, `texts`, and so on: These tokens start with **Ġ**, indicating
    they were *separate* words in the original text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hyper`, `lip`, `id`, and `emia`: These are subwords of “hyperlipidemia”, a
    medical term. The `hyper` shows it’s a new word, while the subsequent subwords
    don’t have `hyperlipidemia` is broken into `hyper` (prefix meaning *excessive*),
    `lip` (related to fats), `id` (connecting element), and `emia` (suffix meaning
    *blood condition*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having explored BPE tokenization and its impact on text processing, we now turn
    our attention to WordPiece tokenization, another powerful method that further
    refines the handling of subword units in NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: WordPiece tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'WordPiece, used by BERT, starts with a base vocabulary of characters, and iteratively
    adds the most frequent subword units. The process looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a base vocabulary of individual characters and a special token for
    unknown words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iteratively merge the most frequent pairs of tokens (starting with characters)
    to form new tokens until a predefined vocabulary size is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For any given word, the longest matching subword units from the vocabulary are
    used. This process is known as **maximum matching**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'WordPiece tokenization is effective for languages with complex word structures
    (e.g., Korean and Japanese) and when handling a diverse vocabulary efficiently
    is crucial. Its effectiveness comes from the fact that the merges are chosen based
    on maximizing the likelihood, leading to potentially more meaningful subwords.
    However, everything comes with a cost, and in this case, it is more computationally
    intensive due to the likelihood maximization step. Let’s have a look at a code
    example of how to perform WordPiece tokenization using the BERT tokenizer. You
    can find the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/12.tokenisation_wordpiece.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/12.tokenisation_wordpiece.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the pre-trained tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take some sample text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the text. This method splits the input text into WordPiece tokens.
    For example, `unaffordable` is broken down into `un`, `##``afford`, `##able`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert tokens to input IDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `##` prefix is used to denote that the token is a continuation of the previous
    token. Thus, it helps in reconstructing the original word by indicating that the
    token should be appended to the preceding token without a space.
  prefs: []
  type: TYPE_NORMAL
- en: After examining tokenization methods such as BPE and WordPiece, it is crucial
    to consider how tokenizers can be tailored to handle specialized data, such as
    medical texts, to ensure precise and contextually relevant processing in these
    specific domains.
  prefs: []
  type: TYPE_NORMAL
- en: Domain-specific data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When working with domain-specific data such as medical texts, it’s crucial
    to ensure that the tokenizer can handle specialized vocabulary effectively. When
    a domain has a high frequency of unique terms or specialized vocabulary, standard
    tokenizers may not perform optimally. In this case, domain-specific tokenizers
    can better capture the nuances and terminology of the field, leading to improved
    model performance. When you are faced with this challenge, there are some options
    available:'
  prefs: []
  type: TYPE_NORMAL
- en: Train a tokenizer on a corpus of domain-specific texts to create a vocabulary
    that includes specialized terms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider extending existing tokenizers with domain-specific tokens instead of
    training from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, how can you know that you need to go the extra mile and tune the tokenizer
    on the dataset? Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the need for specialized tokenizers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we explained, when working with domain-specific data, such as medical texts,
    it’s essential to evaluate whether a specialized tokenizer is needed to ensure
    accurate and contextually relevant processing. Let’s have a look at several key
    factors to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Analyze OOV rate**: Determine the percentage of words in your domain-specific
    corpus that are not included in the standard tokenizer’s vocabulary. A high OOV
    rate suggests that many important terms in your domain are not being recognized,
    highlighting the need for a specialized tokenizer to better handle the unique
    vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Examine tokenization quality**: Check how standard tokenizers split domain-specific
    terms by manually reviewing sample tokenizations. If crucial terms, such as medical
    terminology, are frequently broken into meaningless subwords, this indicates that
    the tokenizer is not well-suited for the domain and may require customization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compression ratio**: Measure the average number of tokens per sentence using
    both standard and domain-specific tokenizers. A significantly lower ratio with
    a domain-specific tokenizer suggests it is more efficient at compressing and representing
    domain knowledge, reducing redundancy, and improving performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, in a medical corpus, terms such as *myocardial infarction* might
    be tokenized as `myo`, `cardial`, and `infarction` by a standard tokenizer, leading
    to a loss of meaningful context. A specialized medical tokenizer, however, might
    recognize *myocardial infarction* as a single term, preserving its meaning and
    improving the quality of downstream tasks such as entity recognition and text
    generation. Similarly, if a standard tokenizer results in an OOV rate of 15% compared
    to just 3% with a specialized tokenizer, it clearly indicates the need for a tailored
    approach. Lastly, if the compression ratio using a standard tokenizer is 1.8 tokens
    per sentence versus 1.2 tokens with a specialized tokenizer, it shows that the
    specialized tokenizer is more efficient and effective in capturing the domain-specific
    nuances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement a small application to evaluate the tokenizers for different
    medical data. The code for the example is available at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/13.specialised_tokenisers.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/13.specialised_tokenisers.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize Stanza for biomedical text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the standard GPT-2 tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set `pad_token` to `eos_token`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set `pad_token_id` for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A sample medical corpus consisting of sentences related to myocardial infarction
    and heart conditions is defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following `stanza_tokenize` function uses the Stanza pipeline to tokenize
    the text and return a list of tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `calculate_oov_and_compression` function tokenizes each sentence in the
    corpus and calculates the OOV rate, as well as the average tokens per sentence,
    and returns all tokens. For standard tokenizers, it checks whether tokens are
    in the vocabulary, while for Stanza, it does not check OOV tokens explicitly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `analyze_token_utilization` function calculates the frequency of each token
    in the corpus and returns a dictionary of token utilization percentages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `calculate_perplexity` function calculates the perplexity of the model
    on the given text, which is a measure of how well the model predicts the sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following script evaluates both the standard GPT-2 tokenizer and the Stanza
    medical tokenizer by calculating the OOV rate, average tokens per sentence, token
    utilization, and perplexity. Finally, it prints the results for each tokenizer
    and compares their performance on the `myocardial` `infarction` term:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s see the results of the two tokenizers presented in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Standard** **GPT-2 tokenizer** | **Stanza** **Medical tokenizer**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **OOV rate** | 0.00% | 0.00% |'
  prefs: []
  type: TYPE_TB
- en: '| **Average tokens** **per sentence** | 10.80 | 7.60 |'
  prefs: []
  type: TYPE_TB
- en: '| **Top five most** **used tokens** | . : 9.26% | . : 13.16% |'
  prefs: []
  type: TYPE_TB
- en: '| ocard : 5.56% | infarction : 7.89% |'
  prefs: []
  type: TYPE_TB
- en: '| ial : 5.56% | myocardial : 5.26% |'
  prefs: []
  type: TYPE_TB
- en: '| Ġinf : 5.56% | heart : 5.26% |'
  prefs: []
  type: TYPE_TB
- en: '| ar : 5.56% | The : 2.63% |'
  prefs: []
  type: TYPE_TB
- en: Table 12.1 – Comparison of GPT-2 tokenizer and specialized medical tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: 'As we see in the table, both tokenizers show an OOV rate of 0.00%, indicating
    that all tokens in the corpus are recognized by both tokenizers. The Stanza Medical
    tokenizer has a lower average rate of tokens per sentence (7.60) compared to the
    Standard GPT-2 tokenizer (10.80). This suggests that the Stanza Medical tokenizer
    is more efficient at compressing domain-specific terms into fewer tokens. The
    Standard GPT-2 tokenizer splits meaningful medical terms into smaller subwords,
    leading to less meaningful token utilization (e.g., `ocard`, `ial`, `inf`, and
    `ar`). However, the Stanza Medical tokenizer maintains the integrity of medical
    terms (e.g., `infarction` and `myocardial`), making the tokens more meaningful
    and contextually relevant. Based on the analysis, the Stanza Medical tokenizer
    should be preferred for medical text processing because of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It efficiently tokenizes domain-specific terms into fewer tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It preserves the integrity and meaning of medical terms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides more meaningful and contextually relevant tokens, which is crucial
    for tasks such as entity recognition and text generation in the medical domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Standard GPT-2 tokenizer, while useful for general text, splits medical
    terms into subwords, leading to potential loss of context and meaning, making
    it less suitable for specialized medical text.
  prefs: []
  type: TYPE_NORMAL
- en: Vocabulary size trade-offs
  prefs: []
  type: TYPE_NORMAL
- en: Larger vocabularies can capture more domain-specific terms but increase the
    model size and computational requirements. Find a balance that adequately covers
    domain terminology without excessive growth.
  prefs: []
  type: TYPE_NORMAL
- en: Having evaluated the performance of different tokenization methods, including
    their handling of OOV terms and their efficiency in compressing domain-specific
    knowledge, the next logical step is to explore how these tokenized outputs are
    transformed into meaningful numerical representations through embedding techniques.
    This transition is crucial, as embeddings form the foundation of how models understand
    and process the tokenized text.
  prefs: []
  type: TYPE_NORMAL
- en: Turning tokens into embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embeddings are numerical representations of words, phrases, or entire documents
    in a high-dimensional vector space. Essentially, we represent words as arrays
    of numbers to capture their semantic meaning. These numerical arrays aim to encode
    the underlying significance of words and sentences, allowing models to understand
    and process text in a meaningful way. Let’s explore the process from tokenization
    to embedding.
  prefs: []
  type: TYPE_NORMAL
- en: The process starts with tokenization, whereby text is split into manageable
    units called tokens. For instance, the sentence “The cat sat on the mat” might
    be tokenized into individual words or subword units such as [“The”, “cat”, “sat”,
    “on”, “the”, “mat”]. Once the text is tokenized, each token is mapped to an embedding
    vector using an embedding layer or lookup table. This table is often initialized
    with random values and then trained to capture meaningful relationships between
    words. For instance, `cat` might be represented as a 300-dimensional vector.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced models such as transformers (e.g., BERT or GPT) generate contextual
    embeddings, where the vector representation of a word is influenced by its surrounding
    words. This allows the model to understand nuances and context, such as distinguishing
    between “bank” in “river bank” versus “financial bank.”
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go and have a look at these models in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: BERT – Contextualized Embedding Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BERT is a powerful NLP model developed by Google. It belongs to the family of
    transformer-based models and is pre-trained on massive amounts of text data to
    learn contextualized representations of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The BERT embedding model is a *component* of the BERT architecture that generates
    contextualized word embeddings. Unlike traditional word embeddings that assign
    a fixed vector representation to each word, BERT embeddings are context-dependent,
    capturing the meaning of words in the context of the entire sentence. Here’s an
    explanation of how to use the BERT embedding model [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/14.embedding_bert.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/14.embedding_bert.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load a pre-trained BERT model and tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Encode the input text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain BERT embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The shape of the embeddings tensor will be (`1`, `sequence_length`, `hidden_size`).
    Here, `sequence_length` is the number of tokens in the input sentence, and `hidden_size`
    is the size of the hidden states in the BERT model (`768` for `bert-base-uncased`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `[CLS]` token embedding represents the entire input sentence and is often
    used for classification tasks. It’s the first token in the output tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'The embedding for the first actual word in the sentence represents the contextualized
    embedding for that specific word. The embedding associated with the first actual
    word in a sentence is not just a static or isolated representation of that word.
    Instead, it’s a “context-aware” or “contextualized” embedding, meaning it reflects
    how the word’s meaning is influenced by the surrounding words in the sentence.
    In simpler terms, this embedding captures not only the word’s intrinsic meaning
    but also how that meaning changes based on the context provided by the other words
    around it. This is a key feature of models such as BERT, which generate different
    embeddings for the same word depending on its usage in different contexts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: The key thing to understand here is that we started with text and we have vectors
    or embeddings as outputs. The tokenization step is happening behind the scenes
    when we use the tokenizer provided by the `transformers` library. The tokenizer
    converts the input sentence into tokens and their corresponding token IDs, which
    are then passed to the BERT model. Remember that each word in the sentence has
    its own embedding that reflects its meaning within the context of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: BERT’s versatility has enabled it to perform exceptionally well across a wide
    array of NLP tasks. However, the need for more efficient and task-specific embeddings
    has led to the development of models such as **BAAI General Embedding** (**BGE**).
    BGE is designed to be smaller and faster, providing high-quality embeddings optimized
    for tasks such as semantic similarity and information retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: BGE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The BAAI/bge-small-en model is part of a series of BGE models developed by the
    **Beijing Academy of Artificial Intelligence** (**BAAI**). These models are designed
    for generating high-quality embeddings for text, typically used in various NLP
    tasks such as text classification, semantic search, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'These models generate embeddings (vector representations) for text. The embeddings
    capture the semantic meaning of the text, making them useful for tasks such as
    similarity search, clustering, and classification. The `bge-small-en` model is
    a smaller, English-specific model in this series. Let’s see an example. The full
    code for this example is available at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/15.embedding_bge.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/15.embedding_bge.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model name and parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the embeddings model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We sample a few sentences to embed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate embeddings for each sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: BGE models, such as bge-small-en, are designed to be smaller and more efficient
    for embedding generation tasks compared to larger, more general models such as
    BERT. This efficiency translates to reduced memory usage and faster inference
    times, making BGE models particularly suitable for applications where computational
    resources are limited or where real-time processing is crucial. While BERT is
    a versatile, general-purpose model capable of handling a wide range of NLP tasks,
    BGE models are specifically optimized for generating high-quality embeddings.
    This optimization allows BGE models to provide comparable or even superior performance
    for specific tasks, such as semantic search and information retrieval, where the
    quality of embeddings is paramount. By focusing on the precision and semantic
    richness of embeddings, BGE models leverage advanced techniques such as learned
    sparse embeddings, which combine the benefits of both dense and sparse representations.
    This targeted optimization enables BGE models to excel in scenarios that demand
    nuanced text representation and efficient processing, making them a better choice
    for embedding-centric applications compared to the more generalized BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: Building on the success of both BERT and BGE, the introduction of **General
    Text Embeddings** (**GTEs**) marks another significant step forward. GTE models
    are specifically fine-tuned to deliver robust and efficient embeddings tailored
    for various text-related applications.
  prefs: []
  type: TYPE_NORMAL
- en: GTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GTEs represent the next generation of embedding models, designed to address
    the growing demand for specialized and efficient text representations. GTE models
    excel in providing high-quality embeddings for specific tasks such as semantic
    similarity, clustering, and information retrieval. Let’s see them in action. The
    full code is available at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/16.embedding_gte.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/16.embedding_gte.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the GTE-base model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We sample a few random texts to embed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the shape of the embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the first few values of the first embedding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: One of the standout features of GTE is its efficiency. By maintaining a smaller
    model size and faster inference times, GTE is well-suited for real-time applications
    and environments with constrained computational resources. This efficiency does
    not come at the cost of performance; GTE models continue to deliver exceptional
    results across various text-processing tasks. However, their reduced complexity
    handling can be a limitation, as the smaller model size may impede their ability
    to process highly intricate or nuanced texts effectively. This could result in
    the less accurate capture of subtle contextual details, affecting performance
    in more complex scenarios. Additionally, a GTE’s focus on efficiency might lead
    to diminished generalization capabilities; although it excels in specific tasks,
    it may struggle to adapt to a wide variety of diverse or less common language
    inputs. Furthermore, the model’s smaller size may constrain its fine-tuning flexibility,
    potentially limiting its ability to adapt to specialized tasks or domains due
    to a reduced capacity for learning and storing intricate patterns specific to
    niche applications.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the right embedding model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When selecting a model for your application, start by identifying your specific
    use case and domain. Whether you need a model for classification, clustering,
    retrieval, or summarization, and whether your domain is legal, medical, or general
    text, will significantly influence your choice.
  prefs: []
  type: TYPE_NORMAL
- en: Next, evaluate the model’s *size and memory usage*. Larger models generally
    provide better performance but come with increased computational requirements
    and higher latency. Begin with a smaller model for initial prototyping and consider
    transitioning to a larger one if your needs evolve. Pay attention to the embedding
    dimensions, as larger dimensions offer a richer representation of the data but
    are also more computationally intensive. Striking a balance between capturing
    detailed information and maintaining operational efficiency is key.
  prefs: []
  type: TYPE_NORMAL
- en: Assess *inference time carefully*, particularly if you have real-time application
    requirements; models with higher latency might necessitate GPU acceleration to
    meet performance standards. Finally, evaluate the model’s performance using benchmarks
    such as the **Massive Text Embedding Benchmark** (**MTEB**) to compare across
    various metrics. Consider both intrinsic evaluations, which examine the model’s
    understanding of semantic and syntactic relationships, and extrinsic evaluations,
    which assess performance on specific downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Solving real problems with embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the advancements in embedding models such as BERT, BGE, and GTE, we can
    tackle a wide range of challenges across various domains. These models enable
    us to solve different problems, as presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic search**: Embeddings improve search relevance by capturing the contextual
    meaning of queries and documents, enhancing the accuracy of search results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation systems**: They facilitate personalized content suggestions
    based on user preferences and behaviors, tailoring recommendations to individual
    needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text classification**: Embeddings enable accurate categorization of documents
    into predefined classes, such as for sentiment analysis or topic identification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information retrieval**: They enhance the accuracy of retrieving relevant
    documents from extensive datasets, improving the efficiency of information retrieval
    systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural language understanding**: Embeddings support tasks such as NER, helping
    systems identify and classify key entities within text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering techniques**: They improve the organization of similar documents
    or topics in large datasets, aiding in better clustering and data management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multimodal data processing**: Embeddings are essential for integrating and
    analyzing text, image, and audio data, leading to more comprehensive insights
    and enhanced decision-making capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s summarize the learnings from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we had a look at text preprocessing, which is an essential
    step in NLP. We saw different text cleaning techniques, from handling HTML tags
    and capitalization to addressing numerical values and whitespace challenges. We
    deep-dived into tokenization, examining word and subword tokenization, with practical
    Python examples. Finally, we explored various methods for embedding documents
    and introduced some of the most popular embedding models available today.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue our journey with unstructured data, delving
    into image and audio preprocessing.
  prefs: []
  type: TYPE_NORMAL
