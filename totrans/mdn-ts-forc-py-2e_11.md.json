["```py\n# Finding the lowest metric for each LCLid\nbest_alg = metrics_combined_df.idxmin(axis=1)\n#Initialize two columns in the dataframe\npred_wide_test[\"best_fit\"] = np.nan\npred_wide_test[\"best_fit_alg\"] = \"\"\n#For each LCL id\nfor lcl_id in tqdm(pred_wide_test.index.get_level_values(0).unique()):\n    # pick the best algorithm\n    alg = best_alg[lcl_id]\n    # and store the forecast in the best_fit column\n    pred_wide_test.loc[lcl_id, \"best_fit\"] = pred_wide_test.loc[lcl_id, alg].values\n    # also store which model was chosen for traceability\n    pred_wide_test.loc[lcl_id, \"best_fit_alg\"] = alg \n```", "```py\n# ensemble_forecasts is a list of column names(forecast) we want to combine\npred_wide_test[\"average_ensemble\"] = pred_wide_test[ensemble_forecasts].mean(axis=1)\npred_wide_test[\"median_ensemble\"] = pred_wide_test[ensemble_forecasts].median(axis=1) \n```", "```py\n    # Used to partially construct a function call\n    from functools import partial\n    # calculate_performance is a custom method we defined to calculate the MAE provided a list of candidates and prediction dataframe\n    from src.forecasting.ensembling import calculate_performance, greedy_optimization \n    ```", "```py\n    # We partially construct the function call by passing the necessary parameters\n    objective = partial(\n        calculate_performance, pred_wide=pred_wide_val, target=\"energy_consumption\"\n    )\n    # ensemble forecasts is the list of candidates\n    solution, best_score = greedy_optimization(objective, ensemble_forecasts) \n    ```", "```py\n    pred_wide_test[\"greedy_ensemble\"] = pred_wide_test[solution].mean(axis=1) \n    ```", "```py\nfrom src.forecasting.ensembling import stochastic_hillclimbing\n# ensemble forecasts is the list of candidates\nsolution, best_score = stochastic_hillclimbing(\n    objective, ensemble_forecasts, n_iterations=10, init=\"best\", random_state=9\n) \n```", "```py\nfrom src.forecasting.ensembling import simulated_annealing\n# ensemble forecasts is the list of candidates\nsolution, best_score = simulated_annealing(\n    objective,\n    ensemble_forecasts,\n    p_range=(0.5, 0.0001),\n    n_iterations=50,\n    init=\"best\",\n    temperature_decay=\"geometric\",\n    random_state=42,\n) \n```", "```py\ndef loss_function(weights):\n        # Calculating the weighted average\n        fc = np.sum(pred_wide[candidates].values * np.array(weights), axis=1)\n        # Using any metric function to calculate the metric\n        return metric_fn(pred_wide[target].values, fc) \n```", "```py\nfrom scipy import optimize\nopt_weights = optimize.minimize(\n        loss_function,\n        # set x0 as initial values, which is a uniform distribution over all the candidates\n        x0=[1 / len(candidates)] * len(candidates),\n        # Set the constraint so that the weights sum to one\n        constraints=({\"type\": \"eq\", \"fun\": lambda w: 1 - sum(w)}),\n        # Choose the optimization technique. Should be gradient-free and bounded.\n        method=\"SLSQP\",\n        # Set the lower and upper bound as a tuple for each element in the candidate list.\n        # We set the maximum values between 1 and 0\n        bounds=[(0.0, 1.0)] * len(candidates),\n        # Set the tolerance for termination\n        options={\"ftol\": 1e-10},\n    )[\"x\"] \n```", "```py\nfrom sklearn.linear_model import LinearRegression\nstacking_model = LinearRegression()\n# ensemble_forecasts is the list of candidates\nstacking_model.fit(\n    pred_wide_val[ensemble_forecasts], pred_wide_val[\"energy_consumption\"]\n)\npred_wide_test[\"linear_reg_blending\"] = stacking_model.predict(\n    pred_wide_test[ensemble_forecasts]\n) \n```"]