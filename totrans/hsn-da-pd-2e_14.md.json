["```py\n>>> %matplotlib inline\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> import pandas as pd\n>>> import seaborn as sns\n>>> planets = pd.read_csv('data/planets.csv') \n>>> red_wine = pd.read_csv('data/winequality-red.csv')\n>>> white_wine = \\\n...     pd.read_csv('data/winequality-white.csv', sep=';') \n>>> wine = pd.concat([\n...     white_wine.assign(kind='white'),\n...     red_wine.assign(kind='red')\n... ])\n>>> red_wine['high_quality'] = pd.cut(\n...     red_wine.quality, bins=[0, 6, 10], labels=[0, 1]\n... )\n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> red_y = red_wine.pop('high_quality')\n>>> red_X = red_wine.drop(columns='quality')\n>>> r_X_train, r_X_test, \\\n... r_y_train, r_y_test = train_test_split(\n...     red_X, red_y, test_size=0.1, random_state=0,\n...     stratify=red_y\n... )\n>>> wine_y = np.where(wine.kind == 'red', 1, 0)\n>>> wine_X = wine.drop(columns=['quality', 'kind'])\n>>> w_X_train, w_X_test, \\\n... w_y_train, w_y_test = train_test_split(\n...     wine_X, wine_y, test_size=0.25, \n...     random_state=0, stratify=wine_y\n... )\n>>> data = planets[\n...     ['semimajoraxis', 'period', 'mass', 'eccentricity']\n... ].dropna()\n>>> planets_X = data[\n...     ['semimajoraxis', 'mass', 'eccentricity']\n... ]\n>>> planets_y = data.period\n>>> pl_X_train, pl_X_test, \\\n... pl_y_train, pl_y_test = train_test_split(\n...     planets_X, planets_y, test_size=0.25, random_state=0\n... )\n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> r_X_train_new, r_X_validate,\\\n... r_y_train_new, r_y_validate = train_test_split(\n...     r_X_train, r_y_train, test_size=0.3, \n...     random_state=0, stratify=r_y_train\n... )\n```", "```py\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.metrics import f1_score\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import MinMaxScaler\n# we will try 10 values from 10^-1 to 10^1 for C\n>>> inv_regularization_strengths = \\\n...     np.logspace(-1, 1, num=10)\n>>> scores = []\n>>> for inv_reg_strength in inv_regularization_strengths:\n...     pipeline = Pipeline([\n...         ('scale', MinMaxScaler()),\n...         ('lr', LogisticRegression(\n...             class_weight='balanced', random_state=0,\n...             C=inv_reg_strength\n...         ))\n...     ]).fit(r_X_train_new, r_y_train_new)\n...     scores.append(f1_score(\n...         pipeline.predict(r_X_validate), r_y_validate\n...     ))\n```", "```py\n>>> plt.plot(inv_regularization_strengths, scores, 'o-')\n>>> plt.xscale('log')\n>>> plt.xlabel('inverse of regularization strength (C)')\n>>> plt.ylabel(r'$F_1$ score')\n>>> plt.title(\n...     r'$F_1$ score vs. '\n...     'Inverse of Regularization Strength'\n... )\n```", "```py\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.model_selection import GridSearchCV\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> pipeline = Pipeline([\n...     ('scale', MinMaxScaler()),\n...     ('lr', LogisticRegression(class_weight='balanced',\n...                               random_state=0))\n... ])\n>>> search_space = {\n...     'lr__C': np.logspace(-1, 1, num=10),\n...     'lr__fit_intercept': [True, False]\n... }\n>>> lr_grid = GridSearchCV(\n...     pipeline, search_space, scoring='f1_macro', cv=5\n... ).fit(r_X_train, r_y_train)\n```", "```py\n# best values of `C` and `fit_intercept` in search space\n>>> lr_grid.best_params_\n{'lr__C': 3.593813663804626, 'lr__fit_intercept': True}\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> print(classification_report(\n...     r_y_test, lr_grid.predict(r_X_test)\n... ))\n              precision    recall  f1-score   support\n           0       0.94      0.80      0.87       138\n           1       0.36      0.68      0.47        22\n    accuracy                           0.79       160\n   macro avg       0.65      0.74      0.67       160\nweighted avg       0.86      0.79      0.81       160\n```", "```py\n>>> from sklearn.model_selection import RepeatedStratifiedKFold\n>>> lr_grid = GridSearchCV(\n...     pipeline, search_space, scoring='f1_macro', \n...     cv=RepeatedStratifiedKFold(random_state=0)\n... ).fit(r_X_train, r_y_train)\n>>> print('Best parameters (CV score=%.2f):\\n    %s' % (\n...     lr_grid.best_score_, lr_grid.best_params_\n... )) # f1 macro score\nBest parameters (CV score=0.69): \n    {'lr__C': 5.994842503189409, 'lr__fit_intercept': True}\n```", "```py\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.metrics import \\\n...     make_scorer, mean_squared_error\n>>> from sklearn.model_selection import GridSearchCV\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> model_pipeline = Pipeline([\n...     ('scale', StandardScaler()),\n...     ('lr', LinearRegression())\n... ])\n>>> search_space = {\n...     'scale__with_mean': [True, False],\n...     'scale__with_std': [True, False],\n...     'lr__fit_intercept': [True, False], \n...     'lr__normalize': [True, False]\n... }\n>>> grid = GridSearchCV(\n...     model_pipeline, search_space, cv=5,\n...     scoring={\n...         'r_squared': 'r2',\n...         'mse': 'neg_mean_squared_error',\n...         'mae': 'neg_mean_absolute_error',\n...         'rmse': make_scorer(\n...             lambda x, y: \\\n...                 -np.sqrt(mean_squared_error(x, y))\n...         )\n...     }, refit='mae'\n... ).fit(pl_X_train, pl_y_train)\n```", "```py\n>>> print('Best parameters (CV score=%.2f):\\n%s' % (\n...     grid.best_score_, grid.best_params_\n... )) # MAE score * -1\nBest parameters (CV score=-1215.99):\n{'lr__fit_intercept': False, 'lr__normalize': True, \n 'scale__with_mean': False, 'scale__with_std': True}\n```", "```py\n>>> from sklearn.metrics import mean_absolute_error\n>>> mean_absolute_error(pl_y_test, grid.predict(pl_X_test))\n1248.3690943844194\n```", "```py\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> PolynomialFeatures(degree=2).fit_transform(\n...     r_X_train[['citric acid', 'fixed acidity']]\n... )\narray([[1.000e+00, 5.500e-01, 9.900e+00, 3.025e-01, \n        5.445e+00, 9.801e+01],\n       [1.000e+00, 4.600e-01, 7.400e+00, 2.116e-01, \n        3.404e+00, 5.476e+01],\n       [1.000e+00, 4.100e-01, 8.900e+00, 1.681e-01, \n        3.649e+00, 7.921e+01],\n       ...,\n       [1.000e+00, 1.200e-01, 7.000e+00, 1.440e-02, \n        8.400e-01, 4.900e+01],\n       [1.000e+00, 3.100e-01, 7.600e+00, 9.610e-02, \n        2.356e+00, 5.776e+01],\n       [1.000e+00, 2.600e-01, 7.700e+00, 6.760e-02, \n        2.002e+00, 5.929e+01]])\n```", "```py\n>>> PolynomialFeatures(\n...     degree=2, include_bias=False, interaction_only=True\n... ).fit_transform(\n...     r_X_train[['citric acid', 'fixed acidity']]\n... )\narray([[0.55 , 9.9  , 5.445],\n       [0.46 , 7.4  , 3.404],\n       [0.41 , 8.9  , 3.649],\n       ...,\n       [0.12 , 7.   , 0.84 ],\n       [0.31 , 7.6  , 2.356],\n       [0.26 , 7.7  , 2.002]])\n```", "```py\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.model_selection import GridSearchCV\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import (\n...     MinMaxScaler, PolynomialFeatures\n... )\n>>> pipeline = Pipeline([\n...     ('poly', PolynomialFeatures(degree=2)),\n...     ('scale', MinMaxScaler()),\n...     ('lr', LogisticRegression(\n...         class_weight='balanced', random_state=0\n...     ))\n... ]).fit(r_X_train, r_y_train)\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> preds = pipeline.predict(r_X_test)\n>>> print(classification_report(r_y_test, preds))\n              precision    recall  f1-score   support\n           0       0.95      0.79      0.86       138\n           1       0.36      0.73      0.48        22\n    accuracy                           0.78       160\n   macro avg       0.65      0.76      0.67       160\nweighted avg       0.87      0.78      0.81       160\n```", "```py\n>>> from sklearn.feature_selection import VarianceThreshold\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> white_or_red_min_var = Pipeline([\n...     ('feature_selection',\n...VarianceThreshold(threshold=0.01)), \n...     ('scale', StandardScaler()), \n...     ('lr', LogisticRegression(random_state=0))\n... ]).fit(w_X_train, w_y_train)\n```", "```py\n>>> w_X_train.columns[\n...     ~white_or_red_min_var.named_steps[\n...         'feature_selection'\n...     ].get_support()\n... ]\nIndex(['chlorides', 'density'], dtype='object')\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> print(classification_report(\n...     w_y_test, white_or_red_min_var.predict(w_X_test)\n... ))\n              precision    recall  f1-score   support\n           0       0.98      0.99      0.99      1225\n           1       0.98      0.95      0.96       400\n    accuracy                           0.98      1625\n   macro avg       0.98      0.97      0.97      1625\nweighted avg       0.98      0.98      0.98      1625\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\ndef pca_scatter(X, labels, cbar_label, cmap='brg'):\n    \"\"\"\n    Create a 2D scatter plot from 2 PCA components of X\n    Parameters:\n        - X: The X data for PCA\n        - labels: The y values\n        - cbar_label: The label for the colorbar\n        - cmap: Name of the colormap to use.\n    Returns:\n        Matplotlib `Axes` object\n    \"\"\"\n    pca = Pipeline([\n        ('scale', MinMaxScaler()),\n        ('pca', PCA(2, random_state=0))\n    ]).fit(X)\n    data, classes = pca.transform(X), np.unique(labels)\n    ax = plt.scatter(\n        data[:, 0], data[:, 1],\n        c=labels, edgecolor='none', alpha=0.5,\n        cmap=plt.cm.get_cmap(cmap, classes.shape[0])\n    )\n    plt.xlabel('component 1')\n    plt.ylabel('component 2')\n    cbar = plt.colorbar()\n    cbar.set_label(cbar_label)\n    cbar.set_ticks(classes)\n    plt.legend([\n        'explained variance\\n'\n        'comp. 1: {:.3}\\ncomp. 2: {:.3}'.format(\n           *pca.named_steps['pca'].explained_variance_ratio_\n        ) \n    ])\n    return ax\n```", "```py\n>>> from ml_utils.pca import pca_scatter\n>>> pca_scatter(wine_X, wine_y, 'wine is red?')\n>>> plt.title('Wine Kind PCA (2 components)')\n```", "```py\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\ndef pca_scatter_3d(X, labels, cbar_label, cmap='brg', \n                   elev=10, azim=15):\n    \"\"\"\n    Create a 3D scatter plot from 3 PCA components of X\n    Parameters:\n        - X: The X data for PCA\n        - labels: The y values\n        - cbar_label: The label for the colorbar\n        - cmap: Name of the colormap to use.\n        - elev: The degrees of elevation to view the plot from. \n        - azim: The azimuth angle on the xy plane (rotation \n                around the z-axis).\n    Returns:\n        Matplotlib `Axes` object\n    \"\"\"\n    pca = Pipeline([\n        ('scale', MinMaxScaler()),\n        ('pca', PCA(3, random_state=0))\n    ]).fit(X)\n    data, classes = pca.transform(X), np.unique(labels)\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    p = ax.scatter3D(\n        data[:, 0], data[:, 1], data[:, 2],\n        alpha=0.5, c=labels,\n        cmap=plt.cm.get_cmap(cmap, classes.shape[0])\n    )\n    ax.view_init(elev=elev, azim=azim)\n    ax.set_xlabel('component 1')\n    ax.set_ylabel('component 2')\n    ax.set_zlabel('component 3')\n    cbar = fig.colorbar(p, pad=0.1)\n    cbar.set_ticks(classes)\n    cbar.set_label(cbar_label)\n    plt.legend([\n        'explained variance\\ncomp. 1: {:.3}\\n'\n        'comp. 2: {:.3}\\ncomp. 3: {:.3}'.format(\n            *pca.named_steps['pca'].explained_variance_ratio_\n        ) \n    ])\n    return ax\n```", "```py\n>>> from ml_utils.pca import pca_scatter_3d\n>>> pca_scatter_3d(\n...     wine_X, wine_y, 'wine is red?', elev=20, azim=-10\n... )\n>>> plt.suptitle('Wine Type PCA (3 components)')\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef pca_explained_variance_plot(pca_model, ax=None):\n    \"\"\"\n    Plot the cumulative explained variance of PCA components.\n    Parameters:\n        - pca_model: The PCA model that has been fit already\n        - ax: Matplotlib `Axes` object to plot on.\n    Returns:\n        A matplotlib `Axes` object\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots()\n    ax.plot(\n        np.append(\n            0, pca_model.explained_variance_ratio_.cumsum()\n        ), 'o-'\n    )\n    ax.set_title(\n        'Total Explained Variance Ratio for PCA Components'\n    )\n    ax.set_xlabel('PCA components used')\n    ax.set_ylabel('cumulative explained variance ratio')\n    return ax\n```", "```py\n>>> from sklearn.decomposition import PCA\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> from ml_utils.pca import pca_explained_variance_plot\n>>> pipeline = Pipeline([\n...     ('normalize', MinMaxScaler()),\n...     ('pca', PCA(8, random_state=0))\n... ]).fit(w_X_train, w_y_train) \n>>> pca_explained_variance_plot(pipeline.named_steps['pca'])\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef pca_scree_plot(pca_model, ax=None):\n    \"\"\"\n    Plot explained variance of each consecutive PCA component.\n    Parameters:\n        - pca_model: The PCA model that has been fit already\n        - ax: Matplotlib `Axes` object to plot on.\n    Returns: A matplotlib `Axes` object\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots()\n    values = pca_model.explained_variance_\n    ax.plot(np.arange(1, values.size + 1), values, 'o-')\n    ax.set_title('Scree Plot for PCA Components')\n    ax.set_xlabel('component')\n    ax.set_ylabel('explained variance')\n    return ax\n```", "```py\n>>> from sklearn.decomposition import PCA\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> from ml_utils.pca import pca_scree_plot\n>>> pipeline = Pipeline([\n...     ('normalize', MinMaxScaler()),\n...     ('pca', PCA(8, random_state=0))\n... ]).fit(w_X_train, w_y_train)\n>>> pca_scree_plot(pipeline.named_steps['pca'])\n```", "```py\n>>> from sklearn.decomposition import PCA\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> from sklearn.linear_model import LogisticRegression\n>>> pipeline = Pipeline([\n...     ('normalize', MinMaxScaler()),\n...     ('pca', PCA(4, random_state=0)),\n...     ('lr', LogisticRegression(\n...         class_weight='balanced', random_state=0\n...     ))\n... ]).fit(w_X_train, w_y_train)\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> preds = pipeline.predict(w_X_test)\n>>> print(classification_report(w_y_test, preds))\n              precision    recall  f1-score   support\n           0       0.99      0.99      0.99      1225\n           1       0.96      0.96      0.96       400\n    accuracy                           0.98      1625\n   macro avg       0.98      0.98      0.98      1625\nweighted avg       0.98      0.98      0.98      1625\n```", "```py\n>>> from sklearn.feature_selection import VarianceThreshold\n>>> from sklearn.pipeline import FeatureUnion, Pipeline\n>>> from sklearn.preprocessing import (\n...     MinMaxScaler, PolynomialFeatures\n... )\n>>> from sklearn.linear_model import LogisticRegression\n>>> combined_features = FeatureUnion([\n...     ('variance', VarianceThreshold(threshold=0.01)),\n...     ('poly', PolynomialFeatures(\n...         degree=2, include_bias=False, interaction_only=True\n...     ))\n... ])\n>>> pipeline = Pipeline([\n...     ('normalize', MinMaxScaler()),\n...     ('feature_union', combined_features),\n...     ('lr', LogisticRegression(\n...         class_weight='balanced', random_state=0\n...     ))\n... ]).fit(r_X_train, r_y_train)\n```", "```py\n>>> pipeline.named_steps['feature_union']\\\n...     .transform(r_X_train)[0]\narray([9.900000e+00, 3.500000e-01, 5.500000e-01, 5.000000e+00,\n       1.400000e+01, 9.971000e-01, 3.260000e+00, 1.060000e+01,\n       9.900000e+00, 3.500000e-01, 5.500000e-01, 2.100000e+00,\n       6.200000e-02, 5.000000e+00, 1.400000e+01, 9.971000e-01,\n       ..., 3.455600e+01, 8.374000e+00])\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> preds = pipeline.predict(r_X_test)\n>>> print(classification_report(r_y_test, preds))\n              precision    recall  f1-score   support\n           0       0.94      0.80      0.87       138\n           1       0.36      0.68      0.47        22\n    accuracy                           0.79       160\n   macro avg       0.65      0.74      0.67       160\nweighted avg       0.86      0.79      0.81       160\n```", "```py\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> dt = DecisionTreeClassifier(random_state=0).fit(\n...     w_X_train, w_y_train\n... )\n>>> pd.DataFrame([(col, coef) for col, coef in zip(\n...     w_X_train.columns, dt.feature_importances_\n... )], columns=['feature', 'importance']\n... ).set_index('feature').sort_values(\n...     'importance', ascending=False\n... ).T\n```", "```py\n>>> from sklearn.tree import export_graphviz\n>>> import graphviz\n>>> graphviz.Source(export_graphviz(\n...     DecisionTreeClassifier(\n...         max_depth=2, random_state=0\n...     ).fit(w_X_train, w_y_train),\n...     feature_names=w_X_train.columns\n... ))\n```", "```py\n>>> from sklearn.tree import DecisionTreeRegressor\n>>> dt = DecisionTreeRegressor(random_state=0).fit(\n...     pl_X_train, pl_y_train\n... )\n>>> [(col, coef) for col, coef in zip(\n...     pl_X_train.columns, dt.feature_importances_\n... )]\n[('semimajoraxis', 0.9969449557611615),\n ('mass', 0.0015380986260574154),\n ('eccentricity', 0.0015169456127809738)]\n```", "```py\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.model_selection import GridSearchCV\n>>> rf = RandomForestClassifier(\n...     n_estimators=100, random_state=0\n... )\n>>> search_space = {\n...     'max_depth': [4, 8], # keep trees small\n...     'min_samples_leaf': [4, 6]\n... }\n>>> rf_grid = GridSearchCV(\n...     rf, search_space, cv=5, scoring='precision'\n... ).fit(r_X_train, r_y_train)\n>>> rf_grid.score(r_X_test, r_y_test)\n0.6\n```", "```py\n>>> from sklearn.ensemble import GradientBoostingClassifier\n>>> from sklearn.model_selection import GridSearchCV\n>>> gb = GradientBoostingClassifier(\n...     n_estimators=100, random_state=0\n... )\n>>> search_space = {\n...     'max_depth': [4, 8], # keep trees small\n...     'min_samples_leaf': [4, 6],\n...     'learning_rate': [0.1, 0.5, 1]\n... }\n>>> gb_grid = GridSearchCV(\n...     gb, search_space, cv=5, scoring='f1_macro'\n... ).fit(r_X_train, r_y_train)\n```", "```py\n>>> gb_grid.score(r_X_test, r_y_test)\n0.7226024272287617\n```", "```py\n>>> from sklearn.metrics import cohen_kappa_score\n>>> cohen_kappa_score(\n...     rf_grid.predict(r_X_test), gb_grid.predict(r_X_test)\n... )\n0.7185929648241206\n```", "```py\n>>> from sklearn.ensemble import VotingClassifier\n>>> majority_rules = VotingClassifier(\n...     [('lr', lr_grid.best_estimator_),\n...      ('rf', rf_grid.best_estimator_), \n...      ('gb', gb_grid.best_estimator_)],\n...     voting='hard'\n... ).fit(r_X_train, r_y_train)\n>>> max_probabilities = VotingClassifier(\n...     [('lr', lr_grid.best_estimator_), \n...      ('rf', rf_grid.best_estimator_), \n...      ('gb', gb_grid.best_estimator_)],\n...     voting='soft'\n... ).fit(r_X_train, r_y_train)\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> print(classification_report(\n...     r_y_test, majority_rules.predict(r_X_test)\n... ))\n              precision    recall  f1-score   support\n           0       0.92      0.95      0.93       138\n           1       0.59      0.45      0.51        22\n    accuracy                           0.88       160\n   macro avg       0.75      0.70      0.72       160\nweighted avg       0.87      0.88      0.87       160\n>>> print(classification_report(\n...     r_y_test, max_probabilities.predict(r_X_test)\n... ))\n              precision    recall  f1-score   support\n           0       0.92      0.93      0.92       138\n           1       0.52      0.50      0.51        22\n    accuracy                           0.87       160\n   macro avg       0.72      0.71      0.72       160\nweighted avg       0.87      0.87      0.87       160\n```", "```py\n>>> prediction_probabilities = pd.DataFrame(\n...     white_or_red.predict_proba(w_X_test), \n...     columns=['prob_white', 'prob_red']\n... ).assign(\n...     is_red=w_y_test == 1,\n...     pred_white=lambda x: x.prob_white >= 0.5,\n...     pred_red=lambda x: np.invert(x.pred_white),\n...     correct=lambda x: (np.invert(x.is_red) & x.pred_white)\n...                        | (x.is_red & x.pred_red)\n... )\n```", "```py\n>>> g = sns.displot(\n...     data=prediction_probabilities, x='prob_red', \n...     rug=True, kde=True, bins=20, col='correct',\n...     facet_kws={'sharey': True} \n... )\n>>> g.set_axis_labels('probability wine is red', None) \n>>> plt.suptitle('Prediction Confidence', y=1.05)\n```", "```py\n>>> incorrect = w_X_test.assign(is_red=w_y_test).iloc[\n...     prediction_probabilities.query('not correct').index\n... ]\n```", "```py\n>>> import math\n>>> chemical_properties = [col for col in wine.columns\n...                        if col not in ['quality', 'kind']]\n>>> melted = \\\n...     wine.drop(columns='quality').melt(id_vars=['kind'])\n>>> fig, axes = plt.subplots(\n...     math.ceil(len(chemical_properties) / 4), 4,\n...     figsize=(15, 10)\n... )\n>>> axes = axes.flatten()\n>>> for prop, ax in zip(chemical_properties, axes):\n...     sns.boxplot(\n...         data=melted[melted.variable.isin([prop])], \n...         x='variable', y='value', hue='kind', ax=ax,\n...         palette={'white': 'lightyellow', 'red': 'orchid'},\n...         saturation=0.5, fliersize=2\n...     ).set_xlabel('')\n...     for _, wrong in incorrect.iterrows():\n...         # _ is convention for collecting info we won't use\n...         x_coord = -0.2 if not wrong['is_red'] else 0.2\n...         ax.scatter(\n...             x_coord, wrong[prop], marker='x',\n...             color='red', s=50\n...         )\n>>> for ax in axes[len(chemical_properties):]:\n...     ax.remove()\n>>> plt.suptitle(\n...     'Comparing Chemical Properties of Red and White Wines'\n...     '\\n(classification errors are red x\\'s)'\n... )\n>>> plt.tight_layout() # clean up layout\n```", "```py\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> knn = KNeighborsClassifier(n_neighbors=5).fit(\n...     r_X_train, r_y_train\n... )\n>>> knn_preds = knn.predict(r_X_test)\n```", "```py\n>>> %%timeit\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> knn = KNeighborsClassifier(n_neighbors=5).fit(\n...     r_X_train, r_y_train\n... )\n3.24 ms ± 599 µs per loop \n(mean ± std. dev. of 7 runs, 100 loops each)\n```", "```py\n>>> %%timeit\n>>> from sklearn.svm import SVC\n>>> svc = SVC(gamma='auto').fit(r_X_train, r_y_train)\n153 ms ± 6.7 ms per loop \n(mean ± std. dev. of 7 runs, 1 loop each)\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> print(classification_report(r_y_test, knn_preds))\n              precision    recall  f1-score   support\n           0       0.91      0.93      0.92       138\n           1       0.50      0.41      0.45        22\n    accuracy                           0.86       160\n   macro avg       0.70      0.67      0.69       160\nweighted avg       0.85      0.86      0.86       160\n```", "```py\n>>> from imblearn.under_sampling import RandomUnderSampler\n>>> X_train_undersampled, y_train_undersampled = \\\n...     RandomUnderSampler(random_state=0)\\\n...         .fit_resample(r_X_train, r_y_train)\n```", "```py\n# before\n>>> r_y_train.value_counts() \n0    1244\n1     195\nName: high_quality, dtype: int64\n# after\n>>> pd.Series(y_train_undersampled).value_counts().sort_index()\n0    195\n1    195\ndtype: int64\n```", "```py\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> knn_undersampled = KNeighborsClassifier(n_neighbors=5)\\\n...     .fit(X_train_undersampled, y_train_undersampled)\n>>> knn_undersampled_preds = knn_undersampled.predict(r_X_test)\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> print(\n...     classification_report(r_y_test, knn_undersampled_preds)\n... )\n              precision    recall  f1-score   support\n           0       0.93      0.65      0.77       138\n           1       0.24      0.68      0.35        22\n    accuracy                           0.66       160\n   macro avg       0.58      0.67      0.56       160\nweighted avg       0.83      0.66      0.71       160\n```", "```py\n>>> from imblearn.over_sampling import SMOTE\n>>> X_train_oversampled, y_train_oversampled = SMOTE(\n...     k_neighbors=5, random_state=0\n... ).fit_resample(r_X_train, r_y_train)\n```", "```py\n# before\n>>> r_y_train.value_counts()\n0    1244\n1     195\nName: high_quality, dtype: int64\n# after\n>>> pd.Series(y_train_oversampled).value_counts().sort_index()\n0    1244\n1    1244\ndtype: int64\n```", "```py\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> knn_oversampled = KNeighborsClassifier(n_neighbors=5)\\ \n...     .fit(X_train_oversampled, y_train_oversampled)\n>>> knn_oversampled_preds = knn_oversampled.predict(r_X_test)\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> print(\n...     classification_report(r_y_test, knn_oversampled_preds)\n... )\n              precision    recall  f1-score   support\n           0       0.96      0.78      0.86       138\n           1       0.37      0.82      0.51        22\n    accuracy                           0.78       160\n   macro avg       0.67      0.80      0.68       160\nweighted avg       0.88      0.78      0.81       160\n```", "```py\n>>> from sklearn.linear_model import Ridge, Lasso, ElasticNet\n>>> ridge, lasso, elastic = Ridge(), Lasso(), ElasticNet()\n>>> for model in [ridge, lasso, elastic]:\n...     model.fit(pl_X_train, pl_y_train)\n...     print(\n...         f'{model.__class__.__name__}: ' # get model name\n...         f'{model.score(pl_X_test, pl_y_test):.4}'\n...     )\nRidge: 0.9206\nLasso: 0.9208\nElasticNet: 0.9047\n```"]