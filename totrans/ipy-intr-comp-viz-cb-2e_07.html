<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 7. Statistical Data Analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Statistical Data Analysis</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Exploring a dataset with pandas and matplotlib</li><li class="listitem" style="list-style-type: disc">Getting started with statistical hypothesis testing – a simple z-test</li><li class="listitem" style="list-style-type: disc">Getting started with Bayesian methods</li><li class="listitem" style="list-style-type: disc">Estimating the correlation between two variables with a contingency table and a chi-squared test</li><li class="listitem" style="list-style-type: disc">Fitting a probability distribution to data with the maximum likelihood method</li><li class="listitem" style="list-style-type: disc">Estimating a probability distribution nonparametrically with a kernel density estimation</li><li class="listitem" style="list-style-type: disc">Fitting a Bayesian model by sampling from a posterior distribution with a Markov chain Monte Carlo method</li><li class="listitem" style="list-style-type: disc">Analyzing data with the R programming language in the IPython notebook</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec65"/>Introduction</h1></div></div></div><p>In the previous chapters, we reviewed technical aspects of high-performance interactive computing in Python. We now begin the second part of this book by illustrating a variety of scientific questions that can be tackled with Python.</p><p>In this chapter, we introduce statistical methods for data analysis. In addition to covering statistical packages such as pandas, statsmodels, and PyMC, we will explain the basics of the underlying mathematical principles. Therefore, this chapter will be most profitable if you have basic experience with probability theory and calculus.</p><p>The next chapter, <a class="link" href="ch08.html" title="Chapter 8. Machine Learning">Chapter 8</a>, <span class="emphasis"><em>Machine Learning</em></span>, is closely related; the underlying mathematics is very similar, but the goals are slightly different. In this chapter, we show how to gain insight into real-world data and how to make informed decisions in the presence of uncertainty. In the next chapter, the goal is to <span class="emphasis"><em>learn from data</em></span>, that is, to generalize and to predict outcomes from partial observations.</p><p>In this introduction, we will give a broad, high-level overview of the methods we will see in this chapter.</p><div class="section" title="What is statistical data analysis?"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec233"/>What is statistical data analysis?</h2></div></div></div><p>The goal of <a id="id1002" class="indexterm"/>statistical data analysis is to understand a complex, real-world phenomenon from partial and uncertain observations. The uncertainty in the data results in uncertainty in the knowledge we get about the phenomenon. A major goal of the theory is to <span class="emphasis"><em>quantify</em></span> this uncertainty.</p><p>It is important to make the distinction between the mathematical theory underlying statistical data analysis, and the decisions made after conducting an analysis. The former is perfectly rigorous; perhaps surprisingly, mathematicians were able to build an exact mathematical framework to deal with uncertainty. Nevertheless, there is a subjective part in the way statistical analysis yields actual human decisions. Understanding the risk and the uncertainty behind statistical results is critical in the decision-making process.</p><p>In this chapter, we will see the basic notions, principles, and theories behind statistical data analysis, covering in particular how to make decisions with a quantified risk. Of course, we will always show how to implement these methods with Python.</p></div><div class="section" title="A bit of vocabulary"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec234"/>A bit of vocabulary</h2></div></div></div><p>There are many terms that need introduction before we get started with the recipes. These notions allow us to classify statistical techniques within multiple dimensions.</p><div class="section" title="Exploration, inference, decision, and prediction"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec40"/>Exploration, inference, decision, and prediction</h3></div></div></div><p>
<span class="strong"><strong>Exploratory methods</strong></span><a id="id1003" class="indexterm"/> allow us to get a preliminary look at a dataset through basic statistical aggregates and interactive visualization. We covered these basic methods in the first chapter of this book and in the book <span class="emphasis"><em>Learning IPython for Interactive Computing and Data Visualization</em></span>, <span class="emphasis"><em>Packt Publishing</em></span>. The first recipe of this chapter, <span class="emphasis"><em>Exploring a dataset with pandas and matplotlib</em></span>, shows another example.</p><p>
<span class="strong"><strong>Statistical inference</strong></span><a id="id1004" class="indexterm"/> consists of getting information about an unknown process through partial and uncertain observations. In particular, <span class="strong"><strong>estimation</strong></span><a id="id1005" class="indexterm"/> entails obtaining approximate quantities for the mathematical variables describing this process. Three recipes in this chapter deal with statistical inference:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Fitting a probability distribution to data with the maximum likelihood method</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Estimating a probability distribution nonparametrically with a kernel density estimation</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Fitting a Bayesian model by sampling from a posterior distribution with a Markov chain Monte Carlo method</em></span> recipe</li></ul></div><p>
<span class="strong"><strong>Decision theory</strong></span><a id="id1006" class="indexterm"/> allows us to make decisions about an unknown process from random observations, with a controlled risk. The following two recipes show how to make statistical decisions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Getting started with statistical hypothesis testing: a simple z-test</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Estimating the correlation between two variables with a contingency table and a chi-squared test</em></span> recipe</li></ul></div><p>
<span class="strong"><strong>Prediction</strong></span> <a id="id1007" class="indexterm"/>consists of learning from data, that is, predicting the outcomes of a random process based on a limited number of observations. This is the topic of the next chapter, <a class="link" href="ch08.html" title="Chapter 8. Machine Learning">Chapter 8</a>, <span class="emphasis"><em>Machine Learning</em></span>.</p></div><div class="section" title="Univariate and multivariate methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec41"/>Univariate and multivariate methods</h3></div></div></div><p>In most cases, you can consider two dimensions in your data:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Observations</strong></span> (or<a id="id1008" class="indexterm"/> <span class="strong"><strong>samples</strong></span>, for <a id="id1009" class="indexterm"/>machine learning <a id="id1010" class="indexterm"/>people)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Variables</strong></span><a id="id1011" class="indexterm"/> (or <a id="id1012" class="indexterm"/><span class="strong"><strong>features</strong></span>)</li></ul></div><p>Typically, observations are independent realizations of the same random process. Each observation is made of one or several variables. Most of the time, variables are either numbers, or elements belonging to a finite set (that is, taking a finite number of values). The first step in an analysis is to understand what your observations and variables are.</p><p>Your problem is <span class="strong"><strong>univariate</strong></span><a id="id1013" class="indexterm"/> if you have one variable. It is <span class="strong"><strong>bivariate</strong></span><a id="id1014" class="indexterm"/> if you have two variables and <span class="strong"><strong>multivariate</strong></span><a id="id1015" class="indexterm"/> if you have at least two variables. Univariate methods are typically simpler. That being said, univariate methods may be used on multivariate data, using one dimension at a time. Although interactions between variables cannot be explored in that case, it is often an interesting first approach.</p></div><div class="section" title="Frequentist and Bayesian methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec42"/>Frequentist and Bayesian methods</h3></div></div></div><p>There are at least two different ways of considering uncertainty, resulting in two different classes of methods for inference, decision, and other statistical questions. These are called <a id="id1016" class="indexterm"/><span class="strong"><strong>frequentist and </strong></span><a id="id1017" class="indexterm"/><span class="strong"><strong>Bayesian methods</strong></span>. Some people prefer frequentist methods, while others prefer Bayesian methods.</p><p>Frequentists interpret a probability as a <span class="strong"><strong>statistical average</strong></span><a id="id1018" class="indexterm"/> across many independent realizations (law of large numbers). Bayesians interpret it as a <span class="strong"><strong>degree of belief</strong></span><a id="id1019" class="indexterm"/> (no need for many realizations). The<a id="id1020" class="indexterm"/> Bayesian interpretation <a id="id1021" class="indexterm"/>is very useful when only a single trial is considered. In addition, Bayesian theory takes into account our <span class="strong"><strong>prior knowledge</strong></span><a id="id1022" class="indexterm"/> about a random process. This prior probability distribution is updated into a posterior distribution as we get more and more data.</p><p>Both frequentist and Bayesian methods have their advantages and disadvantages. For instance, one could say that frequentist methods might be easier to apply than Bayesian methods, but more difficult to interpret. For classic misuses<a id="id1023" class="indexterm"/> of frequentist methods, see <a class="ulink" href="http://www.refsmmat.com/statistics/">www.refsmmat.com/statistics/</a>.</p><p>In any case, if you are a beginner in statistical data analysis, you probably want to learn the basics of both approaches before choosing sides. This chapter introduces you to both types of methods.</p><p>The following recipes are exclusively Bayesian:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Getting started with Bayesian methods</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Fitting a Bayesian model by sampling from a posterior distribution with a Markov chain Monte Carlo method</em></span> recipe</li></ul></div><p>Jake Vanderplas has written several blog posts about frequentism<a id="id1024" class="indexterm"/> and <a id="id1025" class="indexterm"/>Bayesianism, with examples in Python. The first post of the series is available at <a class="ulink" href="http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/">http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/</a>.</p></div><div class="section" title="Parametric and nonparametric inference methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec43"/>Parametric and nonparametric inference methods</h3></div></div></div><p>In many cases, you base your analysis on a <a id="id1026" class="indexterm"/><span class="strong"><strong>probabilistic model</strong></span>. This model describes how your data is generated. A probabilistic model has no reality; it is only a mathematical object that guides you in your analysis. A good model can be helpful, whereas a bad model may misguide you.</p><p>With a <a id="id1027" class="indexterm"/><span class="strong"><strong>parametric method</strong></span>, you assume that your model belongs to a known family of probability distributions. The model has one or multiple numerical <span class="emphasis"><em>parameters</em></span> that you can <span class="emphasis"><em>estimate</em></span>.</p><p>With a <a id="id1028" class="indexterm"/><span class="strong"><strong>nonparametric model</strong></span>, you do not make such an assumption in your model. This gives you more flexibility. However, these methods are typically more complicated to implement and to interpret.</p><p>The following recipes are parametric and nonparametric, respectively:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Fitting a probability distribution to data with the maximum likelihood method</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Estimating a probability distribution nonparametrically with a kernel density estimation</em></span> recipe</li></ul></div><p>This chapter only gives you an idea of the wide range of possibilities that Python offers for statistical data analysis. You can find many books and online courses that cover statistical methods in much greater detail, such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Statistics on<a id="id1029" class="indexterm"/> WikiBooks at <a class="ulink" href="http://en.wikibooks.org/wiki/Statistics">http://en.wikibooks.org/wiki/Statistics</a></li><li class="listitem" style="list-style-type: disc">Free statistical textbooks<a id="id1030" class="indexterm"/> available at <a class="ulink" href="http://stats.stackexchange.com/questions/170/free-statistical-textbooks">http://stats.stackexchange.com/questions/170/free-statistical-textbooks</a></li></ul></div></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Exploring a dataset with pandas and matplotlib"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec66"/>Exploring a dataset with pandas and matplotlib</h1></div></div></div><p>In this first recipe, we will <a id="id1031" class="indexterm"/>show how to conduct a preliminary analysis of a<a id="id1032" class="indexterm"/> dataset with pandas. This is typically the first<a id="id1033" class="indexterm"/> step after getting access to the data. pandas<a id="id1034" class="indexterm"/> lets us load the data very easily, explore the variables, and make basic plots with matplotlib.</p><p>We will take a look at a dataset containing all ATP matches played by four tennis players until 2012. Here, we will focus on Roger Federer.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec235"/>Getting ready</h2></div></div></div><p>Download the <span class="emphasis"><em>Tennis</em></span> dataset from the book's GitHub repository at <a class="ulink" href="https://github.com/ipython-books/cookbook-data">https://github.com/ipython-books/cookbook-data</a>, and extract it to the current directory.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec236"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We import NumPy, pandas, and matplotlib:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">The dataset is a CSV file, that is, a text file with comma-separated values. pandas lets us load this file with a single function:<div class="informalexample"><pre class="programlisting">In [2]: player = 'Roger Federer'
        filename = "data/{name}.csv".format(
                      name=player.replace(' ', '-'))
        df = pd.read_csv(filename)</pre></div><p>We can have <a id="id1035" class="indexterm"/>a first look at this dataset by just <a id="id1036" class="indexterm"/>displaying it in the IPython<a id="id1037" class="indexterm"/> notebook:</p><div class="informalexample"><pre class="programlisting">In [3]: df
Out[3]: Int64Index: 1179 entries, 0 to 1178
        Data columns (total 70 columns):
        year                        1179  non-null values
        tournament                  1179  non-null values
        ...
        player2 total points total  1027  non-null values
        dtypes: float64(49), int64(2), object(19)</pre></div></li><li class="listitem">There are many <a id="id1038" class="indexterm"/>columns. Each row corresponds to a match played by Roger Federer. Let's add a Boolean variable indicating whether he has won the match or not. The <code class="literal">tail()</code> method displays the last rows of the column:<div class="informalexample"><pre class="programlisting">In [4]: df['win'] = df['winner'] == player
        df['win'].tail()
Out[4]: 1174    False
        1175     True
        1176     True
        1177     True
        1178    False
        Name: win, dtype: bool</pre></div></li><li class="listitem"><code class="literal">df['win']</code> is a <code class="literal">Series</code> object. It is very similar to a NumPy array, except that each value has an index (here, the match index). This object has a few standard statistical functions. For example, let's look at the proportion of matches won:<div class="informalexample"><pre class="programlisting">In [5]: print(("{player} has won {vic:.0f}% "
               "of his ATP matches.").format(
                player=player, vic=100*df['win'].mean()))
Roger Federer has won 82% of his ATP matches.</pre></div></li><li class="listitem">Now, we are going to look at the evolution of some variables across time. The <code class="literal">df['start date']</code> field contains the start date of the tournament as a string. We can convert the type to a date type using the <code class="literal">pd.to_datetime()</code> function:<div class="informalexample"><pre class="programlisting">In [6]: date = pd.to_datetime(df['start date'])</pre></div></li><li class="listitem">We are now looking at the proportion of double faults in each match (taking into account that there are logically more double faults in longer matches!). This number is an indicator of the player's state of mind, his level of self-confidence, his willingness to take risks while serving, and other parameters.<div class="informalexample"><pre class="programlisting">In [7]: df['dblfaults'] = (df['player1 double faults'] / 
                           df['player1 total points total'])</pre></div></li><li class="listitem">We can <a id="id1039" class="indexterm"/>use the <code class="literal">head()</code> and <code class="literal">tail()</code> methods to take<a id="id1040" class="indexterm"/> a look at the beginning and the<a id="id1041" class="indexterm"/> end of the column, and <code class="literal">describe()</code> to get summary statistics. In particular, let's note that some rows<a id="id1042" class="indexterm"/> have NaN values (that is, the number of double faults is not available for all matches).<div class="informalexample"><pre class="programlisting">In [8]: df['dblfaults'].tail()
Out[8]: 1174    0.018116
        1175    0.000000
        1176    0.000000
        1177    0.011561
        1178         NaN
        Name: dblfaults, dtype: float64
In [9]: df['dblfaults'].describe()
Out[9]: count    1027.000000
        mean        0.012129
        std         0.010797
        min         0.000000
        25%         0.004444
        50%         0.010000
        75%         0.018108
        max         0.060606
        dtype: float64</pre></div></li><li class="listitem">A very powerful feature in pandas is <code class="literal">groupby()</code>. This function allows us to group together rows that have the same value in a particular column. Then, we can aggregate this group by value to compute statistics in each group. For instance, here is how we can get the proportion of wins as a function of the tournament's surface:<div class="informalexample"><pre class="programlisting">In [10]: df.groupby('surface')['win'].mean()
Out[10]: surface
         Indoor: Carpet    0.736842
         Indoor: Clay      0.833333
         Indoor: Hard      0.836283
         Outdoor: Clay     0.779116
         Outdoor: Grass    0.871429
         Outdoor: Hard     0.842324
         Name: win, dtype: float64</pre></div></li><li class="listitem">Now, we are going to display the proportion of double faults as a function of the tournament date, as well as the yearly average. To do this, we also use <code class="literal">groupby()</code>:<div class="informalexample"><pre class="programlisting">In [11]: gb = df.groupby('year')</pre></div></li><li class="listitem"><code class="literal">gb</code> is a <code class="literal">GroupBy</code> instance. It is similar to a <code class="literal">DataFrame</code> object, but there are multiple rows per group (all matches played in each year). We can aggregate these rows <a id="id1043" class="indexterm"/>using the <code class="literal">mean()</code> operation. We use the <a id="id1044" class="indexterm"/>matplotlib <code class="literal">plot_date()</code> function <a id="id1045" class="indexterm"/>because the x-axis<a id="id1046" class="indexterm"/> contains dates:<div class="informalexample"><pre class="programlisting">In [12]: plt.plot_date(date, df['dblfaults'], 
                       alpha=.25, lw=0)
         plt.plot_date(gb['start date'].max(), 
                       gb['dblfaults'].mean(), '-', lw=3)
         plt.xlabel('Year')
         plt.ylabel('Proportion of double faults per 
                       match.')</pre></div><div class="mediaobject"><img src="images/4818OS_07_01.jpg" alt="How to do it..."/></div></li></ol></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec237"/>There's more...</h2></div></div></div><p>pandas<a id="id1047" class="indexterm"/> is an excellent tool for data wrangling and exploratory analysis. pandas accepts all sorts of formats (text-based, and binary files) and it lets us manipulate tables in many ways. In particular, the <code class="literal">groupby()</code> function is extremely powerful. This library is covered in much greater detail in a book by Wes McKinney, <span class="emphasis"><em>Python for Data Analysis</em></span>.</p><p>What we covered here is only the first step in a data-analysis process. We need more advanced statistical methods to obtain reliable information about the underlying phenomena, make decisions and predictions, and so on. This is the topic of the following recipes.</p><p>In addition, more complex datasets demand more sophisticated analysis methods. For example, digital recordings, images, sounds, and videos require specific signal processing treatments before we can apply statistical techniques. These questions will be covered in subsequent chapters.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Getting started with statistical hypothesis testing – a simple z-test"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec67"/>Getting started with statistical hypothesis testing – a simple z-test</h1></div></div></div><p>
<span class="strong"><strong>Statistical hypothesis testing</strong></span> <a id="id1048" class="indexterm"/>allows us to make decisions in the presence of incomplete data. By definition, these decisions are uncertain. Statisticians have developed rigorous methods to evaluate this risk. Nevertheless, some subjectivity is always involved in the decision-making process. The theory is just a tool that helps us make decisions in an uncertain world.</p><p>Here, we introduce the most basic ideas behind statistical hypothesis testing. We will follow an extremely simple example: coin tossing. More precisely, we will show how to perform a <a id="id1049" class="indexterm"/><span class="strong"><strong>z-test</strong></span>, and we will briefly explain the mathematical ideas underlying it. This kind of method (also called the <span class="emphasis"><em>frequentist method</em></span>), although widely used in science, is subject to many criticisms. We will show later a more modern approach based on Bayesian theory. It is very helpful to understand both approaches, because many studies and publications still follow frequentist methods.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec238"/>Getting ready</h2></div></div></div><p>You need to have a basic knowledge of probability theory for this recipe (random variables, distributions, expectancy, variance, central limit theorem, and so on).</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec239"/>How to do it...</h2></div></div></div><p>Many frequentist methods for hypothesis testing<a id="id1050" class="indexterm"/> roughly involve the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Writing down the hypotheses, notably the <a id="id1051" class="indexterm"/><span class="strong"><strong>null hypothesis</strong></span>, which is the <span class="emphasis"><em>opposite</em></span> of the hypothesis we want to prove (with a certain degree of confidence).</li><li class="listitem">Computing a <a id="id1052" class="indexterm"/><span class="strong"><strong>test statistic</strong></span>, a mathematical formula depending on the test type, the model, the hypotheses, and the data.</li><li class="listitem">Using the computed value to accept the hypothesis, reject it, or fail to conclude.</li></ol></div><p>Here, we flip a coin <span class="emphasis"><em>n</em></span> times and we observe <span class="emphasis"><em>h</em></span> heads. We want to know whether the coin is fair (null hypothesis). This example is extremely simple yet quite useful for pedagogical purposes. Besides, it <a id="id1053" class="indexterm"/>is the basis of many more complex methods.</p><p>We denote the Bernoulli distribution<a id="id1054" class="indexterm"/> by <span class="emphasis"><em>B(q)</em></span> with the unknown parameter <span class="emphasis"><em>q</em></span>. You can refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Bernoulli_distribution">http://en.wikipedia.org/wiki/Bernoulli_distribution</a> for more information.</p><p>A Bernoulli variable is:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">0 (tail) with probability <span class="emphasis"><em>1-q</em></span></li><li class="listitem" style="list-style-type: disc">1 (head) with probability <span class="emphasis"><em>q</em></span></li></ul></div><p>Here are the steps<a id="id1055" class="indexterm"/> required to conduct a simple statistical <span class="emphasis"><em>z</em></span>-test:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's suppose that after <span class="emphasis"><em>n=100</em></span> flips, we get <span class="emphasis"><em>h=61</em></span> heads. We choose a significance level of 0.05: is the coin fair or not? Our null hypothesis is: <span class="emphasis"><em>the coin is fair (q = 1/2)</em></span>:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import scipy.stats as st
        import scipy.special as sp
In [2]: n = 100  # number of coin flips
        h = 61  # number of heads
        q = .5  # null-hypothesis of fair coin</pre></div></li><li class="listitem">Let's compute the <a id="id1056" class="indexterm"/><span class="strong"><strong>z-score</strong></span>, which is defined by the following formula (<code class="literal">xbar</code> is the estimated average of the distribution). We will explain this formula in the next section, <span class="emphasis"><em>How it works…</em></span>.<div class="informalexample"><pre class="programlisting">In [3]: xbar = float(h)/n
        z = (xbar - q) * np.sqrt(n / (q*(1-q))); z
Out[3]: 2.1999999999999997</pre></div></li><li class="listitem">Now, from the z-score, we can compute the p-value as follows:<div class="informalexample"><pre class="programlisting">In [4]: pval = 2 * (1 - st.norm.cdf(z)); pval
Out[4]: 0.02780689502699718</pre></div></li><li class="listitem">This p-value is less than 0.05, so we reject the null hypothesis and conclude that <span class="emphasis"><em>the coin is probably not fair</em></span>.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec240"/>How it works...</h2></div></div></div><p>The coin tossing experiment<a id="id1057" class="indexterm"/> is modeled as a sequence of <span class="emphasis"><em>n</em></span> independent random variables <span class="inlinemediaobject"><img src="images/4818OS_07_46.jpg" alt="How it works..."/></span>
<span class="emphasis"><em> </em></span>following the Bernoulli distribution <span class="emphasis"><em>B(q)</em></span>. Each <span class="emphasis"><em>x<sub>i</sub></em></span> represents one coin flip. After our experiment, we get actual values (samples) for these variables. A different notation is sometimes used to distinguish between the random variables (probabilistic objects) and the actual values (samples).</p><p>The following formula gives the <span class="strong"><strong>sample mean</strong></span><a id="id1058" class="indexterm"/> (proportion of heads here):</p><div class="mediaobject"><img src="images/4818OS_07_02.jpg" alt="How it works..."/></div><p>Knowing the expectancy <span class="inlinemediaobject"><img src="images/4818OS_07_39.jpg" alt="How it works..."/></span> and variance <span class="inlinemediaobject"><img src="images/4818OS_07_40.jpg" alt="How it works..."/></span> of the distribution <span class="emphasis"><em>B(q)</em></span>, we compute:</p><div class="mediaobject"><img src="images/4818OS_07_03.jpg" alt="How it works..."/></div><p>The z-test is<a id="id1059" class="indexterm"/> the normalized version of <span class="inlinemediaobject"><img src="images/4818OS_07_41.jpg" alt="How it works..."/></span> (we remove its mean, and divide by the standard deviation, thus we get a variable with mean 0 and standard deviation 1):</p><div class="mediaobject"><img src="images/4818OS_07_04.jpg" alt="How it works..."/></div><p>Under the null hypothesis, what is the probability of obtaining a z-test higher than some quantity <span class="emphasis"><em>z<sub>0</sub></em></span>? This probability is called the (two-sided)<a id="id1060" class="indexterm"/> <span class="strong"><strong>p-value</strong></span>. According to the central limit theorem, the z-test approximately follows a standard Gaussian distribution <span class="emphasis"><em>N(0,1)</em></span> for large <span class="emphasis"><em>n</em></span>, so we get:</p><div class="mediaobject"><img src="images/4818OS_07_05.jpg" alt="How it works..."/></div><p>The following diagram illustrates the z-score and the p-value:</p><div class="mediaobject"><img src="images/4818OS_07_06.jpg" alt="How it works..."/><div class="caption"><p>Illustration of the z-score and the p-value</p></div></div><p>In this formula, <span class="inlinemediaobject"><img src="images/4818OS_07_42.jpg" alt="How it works..."/></span> <a id="id1061" class="indexterm"/>is the <span class="strong"><strong>cumulative distribution function</strong></span><a id="id1062" class="indexterm"/> of a standard normal distribution. In SciPy, we can get it with <code class="literal">scipy.stats.norm.cdf</code>. So, given the z-test computed from the data, we compute the p-value: the probability of observing a z-test more extreme than the observed test, under the null hypothesis.</p><p>If the <a id="id1063" class="indexterm"/>p-value is less than five percent (a frequently-chosen significance level, for arbitrary and historical reasons), we conclude that either:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The null hypothesis is false, thus we conclude that the coin is unfair.</li><li class="listitem" style="list-style-type: disc">The null hypothesis is true, and it's just bad luck if we obtained these values. We cannot make a conclusion.</li></ul></div><p>We cannot disambiguate between these two options in this framework, but typically the first option is chosen. We hit the limits of frequentist statistics, although there are ways to mitigate this problem (for example, by conducting several independent studies and looking at all of their conclusions).</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec241"/>There's more...</h2></div></div></div><p>Many statistical tests following this <a id="id1064" class="indexterm"/>pattern exist. Reviewing all those tests is largely beyond the scope of this book, but you can take a look at the reference at <a class="ulink" href="http://en.wikipedia.org/wiki/Statistical_hypothesis_testing">http://en.wikipedia.org/wiki/Statistical_hypothesis_testing</a>.</p><p>As a p-value is not easy to interpret, it can lead to wrong conclusions, even in peer-reviewed scientific publications. For an in-depth treatment of the subject, see <a class="ulink" href="http://www.refsmmat.com/statistics/">www.refsmmat.com/statistics/</a>.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec242"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Getting started with Bayesian methods</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Getting started with Bayesian methods"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec68"/>Getting started with Bayesian methods</h1></div></div></div><p>In the last recipe, we used a frequentist method to test a hypothesis on incomplete data. Here, we will see an alternative approach based on <a id="id1065" class="indexterm"/><span class="strong"><strong>Bayesian theory</strong></span>. The main idea is to consider that <span class="emphasis"><em>unknown parameters are random variables</em></span>, just like the variables describing the experiment. Prior knowledge about the parameters is integrated into the model. This knowledge is updated as more and more data is observed.</p><p>Frequentists and Bayesians interpret probabilities differently. Frequentists<a id="id1066" class="indexterm"/> interpret a probability as a limit of frequencies when the number of samples tends to infinity. Bayesians<a id="id1067" class="indexterm"/> interpret it as a belief; this belief is updated as more and more data is observed.</p><p>Here, we revisit the previous coin flipping example with a Bayesian approach. This example is sufficiently simple to permit an analytical treatment. In general, as we will see later in this chapter, analytical results cannot be obtained and numerical methods become essential.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec243"/>Getting ready</h2></div></div></div><p>This is a math-heavy recipe. Knowledge of basic probability theory (random variables, distributions, Bayes formula) and calculus (derivatives, integrals) is recommended. We use the same notations as in the previous recipe.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec244"/>How to do it...</h2></div></div></div><p>Let <span class="emphasis"><em>q</em></span> be the probability of <a id="id1068" class="indexterm"/>obtaining a head. Whereas <span class="emphasis"><em>q</em></span> was just a fixed number in the previous recipe, we consider here that it is a <a id="id1069" class="indexterm"/><span class="strong"><strong>random variable</strong></span>. Initially, this variable follows a distribution called the <a id="id1070" class="indexterm"/><span class="strong"><strong>prior probability distribution</strong></span>. It represents our knowledge about <span class="emphasis"><em>q</em></span> before we start flipping the coin. We will update this distribution after each trial (posterior distribution).</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we assume that <span class="emphasis"><em>q</em></span> is a <span class="emphasis"><em>uniform</em></span> random variable in the interval [0, 1]. That's our prior distribution: for all <span class="emphasis"><em>q</em></span>, <span class="emphasis"><em>P(q)=1</em></span>.</li><li class="listitem">Then, we flip our coin <span class="emphasis"><em>n</em></span> times. We note <span class="emphasis"><em>x<sub>i</sub></em></span> the outcome of the <span class="emphasis"><em>i</em></span>th flip (0 for tail and 1 for head).</li><li class="listitem">What is the probability distribution of <span class="emphasis"><em>q</em></span> knowing the observations <span class="emphasis"><em>x<sub>i</sub></em></span>? <span class="strong"><strong>Bayes' theorem</strong></span><a id="id1071" class="indexterm"/> allows us to compute the <span class="strong"><strong>posterior distribution</strong></span><a id="id1072" class="indexterm"/> analytically (see the next section for the mathematical details):<div class="mediaobject"><img src="images/4818OS_07_07.jpg" alt="How to do it..."/></div></li><li class="listitem">We define the posterior <a id="id1073" class="indexterm"/>distribution according to the previous mathematical formula. We remark that this expression is <span class="emphasis"><em>(n+1)</em></span> times the <span class="strong"><strong>probability mass function</strong></span> (<span class="strong"><strong>PMF</strong></span>)<a id="id1074" class="indexterm"/> of the binomial distribution, which is directly available in <code class="literal">scipy.stats</code>. (For more information on <a id="id1075" class="indexterm"/>Binomial distribution, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Binomial_distribution">http://en.wikipedia.org/wiki/Binomial_distribution</a>.)<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import scipy.stats as st
        import matplotlib.pyplot as plt
        %matplotlib inline
In [2]: posterior = lambda n, h, q: ((n+1) * 
                                     st.binom(n, q).pmf(h))</pre></div></li><li class="listitem">Let's plot this distribution for an observation of <span class="emphasis"><em>h=61</em></span> heads and <span class="emphasis"><em>n=100</em></span> total flips:<div class="informalexample"><pre class="programlisting">In [3]: n = 100
        h = 61
        q = np.linspace(0., 1., 1000)
        d = posterior(n, h, q)
In [4]: plt.plot(q, d, '-k')
        plt.ylim(0, d.max()+1)</pre></div><div class="mediaobject"><img src="images/4818OS_07_08.jpg" alt="How to do it..."/></div><p>This curve represents our belief about the parameter <span class="emphasis"><em>q</em></span> after we have observed 61 heads.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec245"/>How it works...</h2></div></div></div><p>In this section, we explain <a id="id1076" class="indexterm"/>Bayes' theorem, and we give the mathematical details underlying this example.</p><div class="section" title="Bayes' theorem"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec44"/>Bayes' theorem</h3></div></div></div><p>There is a very <a id="id1077" class="indexterm"/>general idea in data science that consists of explaining data with a mathematical model. This is formalized with a one-way process, <span class="emphasis"><em>model → data</em></span>.</p><p>Once this process is formalized, the task of the data scientist is to exploit the data to recover information about the model. In other words, we want to <span class="emphasis"><em>invert</em></span> the original process and get <span class="emphasis"><em>data → model</em></span>.</p><p>In a probabilistic setting, the direct process is represented as a <span class="strong"><strong>conditional probability distribution</strong></span><a id="id1078" class="indexterm"/> <span class="emphasis"><em>P(data|model)</em></span>. This is the probability of observing the data when the model is entirely specified.</p><p>Similarly, the inverse process is <span class="emphasis"><em>P(model|data)</em></span>. It gives us information about the model (what we're looking for), knowing the observations (what we have).</p><p>Bayes' theorem is at the core of a general framework for inverting a probabilistic process of <span class="emphasis"><em>model → data</em></span>. It can be stated as follows:</p><div class="mediaobject"><img src="images/4818OS_07_09.jpg" alt="Bayes' theorem"/></div><p>This equation gives us information about our model, knowing the observed data. Bayes' equation is widely used in signal processing, statistics, machine learning, inverse problems, and in many other scientific applications.</p><p>In Bayes' equation, <span class="emphasis"><em>P(model)</em></span> reflects our prior knowledge about the model. Also, <span class="emphasis"><em>P(data)</em></span> is the distribution of the data. It is generally expressed as an integral of <span class="emphasis"><em>P(data|model)P(model)</em></span>.</p><p>In conclusion, Bayes' equation gives us a general roadmap for data inference:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Specify a mathematical model for the direct process <span class="emphasis"><em>model → data</em></span> (the <span class="emphasis"><em>P(data|model)</em></span> term).</li><li class="listitem">Specify a prior probability distribution for the model (<span class="emphasis"><em>P(model) term</em></span>).</li><li class="listitem">Perform analytical or numerical calculations to solve this equation.</li></ol></div></div><div class="section" title="Computation of the posterior distribution"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec45"/>Computation of the posterior distribution</h3></div></div></div><p>In this recipe's example, we <a id="id1079" class="indexterm"/>found the posterior distribution with the following equation (deriving directly from Bayes' theorem):</p><div class="mediaobject"><img src="images/4818OS_07_10.jpg" alt="Computation of the posterior distribution"/></div><p>Knowing that the <span class="emphasis"><em>x<sub>i</sub></em></span> are independent, we get (<span class="emphasis"><em>h</em></span> being the number of heads):</p><div class="mediaobject"><img src="images/4818OS_07_11.jpg" alt="Computation of the posterior distribution"/></div><p>In addition, we can compute analytically the following integral (using an integration by parts and an induction):</p><div class="mediaobject"><img src="images/4818OS_07_12.jpg" alt="Computation of the posterior distribution"/></div><p>Finally, we get:</p><div class="mediaobject"><img src="images/4818OS_07_13.jpg" alt="Computation of the posterior distribution"/></div></div><div class="section" title="Maximum a posteriori estimation"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec46"/>Maximum a posteriori estimation</h3></div></div></div><p>We can get a point estimate from the<a id="id1080" class="indexterm"/> posterior distribution. For example, the <span class="strong"><strong>maximum a posteriori</strong></span> (<span class="strong"><strong>MAP</strong></span>)<a id="id1081" class="indexterm"/> estimation consists of considering the <span class="emphasis"><em>maximum</em></span> of the posterior distribution as an estimate for <span class="emphasis"><em>q</em></span>. We can find this maximum analytically or numerically. For more information on<a id="id1082" class="indexterm"/> MAP, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation</a>.</p><p>Here, we can get this estimate analytically by deriving the posterior distribution with respect to <span class="emphasis"><em>q</em></span>. We get (assuming <span class="emphasis"><em>1 </em></span><span class="inlinemediaobject"><img src="images/4818OS_08_40.jpg" alt="Maximum a posteriori estimation"/></span> <span class="emphasis"><em>h </em></span><span class="inlinemediaobject"><img src="images/4818OS_08_40.jpg" alt="Maximum a posteriori estimation"/></span>
<span class="emphasis"><em> n-1</em></span>):</p><div class="mediaobject"><img src="images/4818OS_07_14.jpg" alt="Maximum a posteriori estimation"/></div><p>This expression is equal to <a id="id1083" class="indexterm"/>zero when <span class="emphasis"><em>q = h/n</em></span>. This is the MAP estimate of the parameter <span class="emphasis"><em>q</em></span>. This value happens to be the proportion of heads obtained in the experiment.</p></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec246"/>There's more...</h2></div></div></div><p>In this recipe, we showed a few basic notions in Bayesian theory. We illustrated them with a simple example. The fact that we were able to derive the posterior distribution analytically is not very common in real-world applications. This example is nevertheless informative because it explains the core mathematical ideas behind the complex numerical methods we will see later.</p><div class="section" title="Credible interval"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec47"/>Credible interval</h3></div></div></div><p>The posterior distribution indicates the plausible values for <span class="emphasis"><em>q</em></span> given the observations. We could use it to derive a <a id="id1084" class="indexterm"/><span class="strong"><strong>credible interval</strong></span>, likely to contain the actual value. Credible intervals are the Bayesian analog to confidence intervals in frequentist statistics. For<a id="id1085" class="indexterm"/> more information on credible intervals, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Credible_interval">http://en.wikipedia.org/wiki/Credible_interval</a>.</p></div><div class="section" title="Conjugate distributions"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec48"/>Conjugate distributions</h3></div></div></div><p>In this recipe, the prior and posterior distributions are<a id="id1086" class="indexterm"/> <span class="strong"><strong>conjugate</strong></span>, meaning that they belong to the same family (the beta distribution). For this reason, we were able to compute the posterior distribution analytically. You will find more details about <a id="id1087" class="indexterm"/>conjugate distributions at <a class="ulink" href="http://en.wikipedia.org/wiki/Conjugate_prior">http://en.wikipedia.org/wiki/Conjugate_prior</a>.</p></div><div class="section" title="Non-informative (objective) prior distributions"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec49"/>Non-informative (objective) prior distributions</h3></div></div></div><p>We chose a uniform distribution as <a id="id1088" class="indexterm"/>prior distribution for the unknown parameter <span class="emphasis"><em>q</em></span>. It is a simple choice and it leads to tractable computations. It reflects the intuitive fact that we do not favor any particular value a priori. However, there are rigorous ways of choosing completely uninformative priors (see <a class="ulink" href="http://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors">http://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors</a>). An example is the <a id="id1089" class="indexterm"/>Jeffreys prior, based on the idea that the prior distribution should not depend on the parameterization of the parameters. For more information on Jeffreys prior, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Jeffreys_prior">http://en.wikipedia.org/wiki/Jeffreys_prior</a>. In our example, the Jeffreys prior is:</p><div class="mediaobject"><img src="images/4818OS_07_15.jpg" alt="Non-informative (objective) prior distributions"/></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec247"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Fitting a Bayesian model by sampling from a posterior distribution with a Markov chain Monte Carlo method</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Estimating the correlation between two variables with a contingency table and a chi-squared test"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec69"/>Estimating the correlation between two variables with a contingency table and a chi-squared test</h1></div></div></div><p>Whereas univariate methods deal with single-variable observations, multivariate methods consider observations with several features. Multivariate datasets allow the study of <span class="emphasis"><em>relations</em></span> between variables, more particularly their correlation or lack thereof (that is, independence).</p><p>In this recipe, we <a id="id1090" class="indexterm"/>will take a <a id="id1091" class="indexterm"/>look at the same tennis dataset as in the first recipe of this chapter. Following a frequentist approach, we will estimate the correlation between the number of aces and the proportion of points won by a tennis player.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec248"/>Getting ready</h2></div></div></div><p>Download the <span class="emphasis"><em>Tennis</em></span> dataset on the book's GitHub repository at <a class="ulink" href="https://github.com/ipython-books/cookbook-data">https://github.com/ipython-books/cookbook-data</a>, and extract it in the current directory.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec249"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import NumPy, pandas, SciPy.stats, and matplotlib:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import pandas as pd
        import scipy.stats as st
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">We load the dataset corresponding to Roger Federer:<div class="informalexample"><pre class="programlisting">In [2]: player = 'Roger Federer'
        filename = "data/{name}.csv".format(
                      name=player.replace(' ', '-'))
        df = pd.read_csv(filename)</pre></div></li><li class="listitem">Each row corresponds to a match, and the 70 columns contain many player characteristics during that match:<div class="informalexample"><pre class="programlisting">In [3]: print("Number of columns: " + str(len(df.columns)))
        df[df.columns[:4]].tail()
Number of columns: 70
              year                  tournament  start date
        1174  2012  Australian Open, Australia  16.01.2012
        1175  2012                 Doha, Qatar  02.01.2012
        1176  2012                 Doha, Qatar  02.01.2012
        1177  2012                 Doha, Qatar  02.01.2012
        1178  2012                 Doha, Qatar  02.01.2012</pre></div></li><li class="listitem">Here, we only<a id="id1092" class="indexterm"/> look at <a id="id1093" class="indexterm"/>the proportion of points won, and the (relative) number of aces:<div class="informalexample"><pre class="programlisting">In [4]: npoints = df['player1 total points total']
        points = df['player1 total points won'] / npoints
        aces = df['player1 aces'] / npoints
In [5]: plt.plot(points, aces, '.')
        plt.xlabel('% of points won')
        plt.ylabel('% of aces')
        plt.xlim(0., 1.)
        plt.ylim(0.)</pre></div><div class="mediaobject"><img src="images/4818OS_07_16.jpg" alt="How to do it..."/></div><p>If the two variables were independent, we would not see any trend in the cloud of points. On this plot, it is a bit hard to tell. Let's use pandas to compute a coefficient correlation.</p></li><li class="listitem">We create a new <code class="literal">DataFrame</code> object with only these fields (note that this step is not compulsory). We also remove the rows where one field is missing (using <code class="literal">dropna()</code>):<div class="informalexample"><pre class="programlisting">In [6]: df_bis = pd.DataFrame({'points': points,
                               'aces': aces}).dropna()
        df_bis.tail()
Out[6]:           aces    points
        1173  0.024390  0.585366
        1174  0.039855  0.471014
        1175  0.046512  0.639535
        1176  0.020202  0.606061
        1177  0.069364  0.531792</pre></div></li><li class="listitem">Let's <a id="id1094" class="indexterm"/>compute the<a id="id1095" class="indexterm"/> Pearson's correlation coefficient between the relative number of aces in the match, and the number of points won:<div class="informalexample"><pre class="programlisting">In [7]: df_bis.corr()
Out[7]:             aces    points
        aces    1.000000  0.255457
        points  0.255457  1.000000</pre></div><p>A correlation of ~0.26 seems to indicate a positive correlation between our two variables. In other words, the more aces in a match, the more points the player wins (which is not very surprising!).</p></li><li class="listitem">Now, to determine if there is a <span class="emphasis"><em>statistically significant</em></span> correlation between the variables, we use a <span class="strong"><strong>chi-squared test</strong></span><a id="id1096" class="indexterm"/> of the independence of variables in a<a id="id1097" class="indexterm"/> <span class="strong"><strong>contingency table</strong></span>.</li><li class="listitem">First, we binarize our variables. Here, the value corresponding to the number of aces is <code class="literal">True</code> if the player is serving more aces than usual in a match, and <code class="literal">False</code> otherwise:<div class="informalexample"><pre class="programlisting">In [8]: df_bis['result'] = df_bis['points'] &gt; \
                           df_bis['points'].median()
        df_bis['manyaces'] = df_bis['aces'] &gt; \
                           df_bis['aces'].median()</pre></div></li><li class="listitem">Then, we create a contingency table, with the frequencies of all four possibilities (True and True, True and False, and so on):<div class="informalexample"><pre class="programlisting">In [9]: pd.crosstab(df_bis['result'], df_bis['manyaces'])
Out[9]: manyaces  False  True 
        result                
        False       300    214
        True        214    299</pre></div></li><li class="listitem">Finally, we<a id="id1098" class="indexterm"/> compute<a id="id1099" class="indexterm"/> the chi-squared test statistic and the associated p-value. The null hypothesis is the independence between the variables. SciPy implements this test in <code class="literal">scipy.stats.chi2_contingency</code>, which returns several objects. We're interested in the second result, which is the p-value:<div class="informalexample"><pre class="programlisting">In [10]: st.chi2_contingency(_)
Out[10]: (27.809858855369555,
          1.3384233799633629e-07,
          1L,
          array([[ 257.25024343,  256.74975657],
                 [ 256.74975657,  256.25024343]]))</pre></div><p>The p-value is much lower than 0.05, so we reject the null hypothesis and conclude that there is a statistically significant correlation between the proportion of aces and the proportion of points won in a match (for Roger Federer!).</p></li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip24"/>Tip</h3><p>As always, correlation<a id="id1100" class="indexterm"/> does not imply causation. Here, it is likely that external factors influence both variables. See <a class="ulink" href="http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation">http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation</a> for more details.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec250"/>How it works...</h2></div></div></div><p>We give here a few details about the statistical concepts used in this recipe.</p><div class="section" title="Pearson's correlation coefficient"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec50"/>Pearson's correlation coefficient</h3></div></div></div><p>Pearson's correlation coefficient<a id="id1101" class="indexterm"/> measures the linear correlation between two random variables, <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span>. It is a normalized version of the covariance:</p><div class="mediaobject"><img src="images/4818OS_07_17.jpg" alt="Pearson's correlation coefficient"/></div><p>It can be estimated by substituting, in this formula, the expectancy with the sample mean, and the variance with the sample <a id="id1102" class="indexterm"/>variance. More details about its inference can be found at <a class="ulink" href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient</a>.</p></div><div class="section" title="Contingency table and chi-squared test"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec51"/>Contingency table and chi-squared test</h3></div></div></div><p>The contingency table<a id="id1103" class="indexterm"/> contains the frequencies <span class="emphasis"><em>O<sub>ij</sub></em></span> of all combinations of outcomes, when <a id="id1104" class="indexterm"/>there are multiple random variables that can take a finite number of values. Under the null hypothesis of independence, we can compute the <span class="emphasis"><em>expected</em></span> frequencies <span class="emphasis"><em>E<sub>ij</sub></em></span>, based on the marginal sums (sums in each row). The chi-squared statistic, by definition, is:</p><div class="mediaobject"><img src="images/4818OS_07_18.jpg" alt="Contingency table and chi-squared test"/></div><p>When there are sufficiently many observations, this variable approximately follows a chi-squared distribution (the distribution of the sum of normal variables squared). Once we get the p-value, as explained in the <span class="emphasis"><em>Getting started with statistical hypothesis testing – a simple z-test</em></span> recipe, we can reject or accept the null hypothesis of independence. Then, we can conclude (or not) that there exists a significant correlation between the variables.</p></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec251"/>There's more...</h2></div></div></div><p>There are many other sorts of chi-squared tests, that is, tests where the test statistic follows a chi-squared distribution. These tests are widely used for testing the goodness-of-fit of a distribution, or testing the independence of variables. More information can be found in the following pages:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Chi2 test in <a id="id1105" class="indexterm"/>SciPy documentation available at <a class="ulink" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html">http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html</a></li><li class="listitem" style="list-style-type: disc">Contingency table<a id="id1106" class="indexterm"/> introduced at <a class="ulink" href="http://en.wikipedia.org/wiki/Contingency_table">http://en.wikipedia.org/wiki/Contingency_table</a></li><li class="listitem" style="list-style-type: disc">Chi-squared test<a id="id1107" class="indexterm"/> introduced at <a class="ulink" href="http://en.wikipedia.org/wiki/Pearson's_chi-squared_test">http://en.wikipedia.org/wiki/Pearson's_chi-squared_test</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec252"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Getting started with statistical hypothesis testing – a simple z-test</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Fitting a probability distribution to data with the maximum likelihood method"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec70"/>Fitting a probability distribution to data with the maximum likelihood method</h1></div></div></div><p>A good way to explain a dataset is to apply a probabilistic model to it. Finding an adequate model can be a job in its own. Once a model is chosen, it is necessary to compare it to the data. This is what statistical estimation is about. In this recipe, we apply the <span class="strong"><strong>maximum likelihood method</strong></span><a id="id1108" class="indexterm"/> on a dataset of survival times after heart transplant (1967-1974 study).</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec253"/>Getting ready</h2></div></div></div><p>As usual in this chapter, a background in probability theory and real analysis is recommended. In addition, you need the statsmodels package to retrieve the test dataset. For more information on<a id="id1109" class="indexterm"/> statsmodels, refer to <a class="ulink" href="http://statsmodels.sourceforge.net">http://statsmodels.sourceforge.net</a>. On Anaconda, you can install statsmodel with the <code class="literal">conda install statsmodels</code> command.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec254"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">statsmodels<a id="id1110" class="indexterm"/> is a Python package for conducting statistical data analyses. It <a id="id1111" class="indexterm"/>also contains real-world datasets that we can use when<a id="id1112" class="indexterm"/> experimenting with new methods. Here, we load the <span class="emphasis"><em>heart</em></span> dataset:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import scipy.stats as st
        import statsmodels.datasets as ds
        import matplotlib.pyplot as plt
        %matplotlib inline
In [2]: data = ds.heart.load_pandas().data</pre></div></li><li class="listitem">Let's take a look at this <code class="literal">DataFrame</code>:<div class="informalexample"><pre class="programlisting">In [3]: data.tail()
Out[3]:     survival  censors   age
        64        14        1  40.3
        65       167        0  26.7
        66       110        0  23.7
        67        13        0  28.9
        68         1        0  35.2</pre></div><p>This dataset contains censored and uncensored data: a censor of 0 means that the patient was alive at the end of the study, and thus we don't know the exact survival time. We only know that the patient survived <span class="emphasis"><em>at least</em></span> the indicated number of days. For simplicity here, we only keep uncensored data (we thereby introduce a bias toward patients that did not survive very long after their transplant):</p><div class="informalexample"><pre class="programlisting">In [4]: data = data[data.censors==1]
        survival = data.survival</pre></div></li><li class="listitem">Let's take a look at <a id="id1113" class="indexterm"/>the data<a id="id1114" class="indexterm"/> graphically, by plotting the raw survival data and the histogram:<div class="informalexample"><pre class="programlisting">In [5]: plt.subplot(121)
        plt.plot(sorted(survival)[::-1], 'o')
        plt.xlabel('Patient')
        plt.ylabel('Survival time (days)')
        plt.subplot(122)
        plt.hist(survival, bins=15)
        plt.xlabel('Survival time (days)')
        plt.ylabel('Number of patients')</pre></div><div class="mediaobject"><img src="images/4818OS_07_19.jpg" alt="How to do it..."/></div></li><li class="listitem">
We observe that the histogram is decreasing very rapidly. Fortunately, the survival rates today are much higher (~70 percent after 5 years). Let's try to fit an <a id="id1115" class="indexterm"/>exponential distribution (more information on the exponential distribution is available at <a class="ulink" href="http://en.wikipedia.org/wiki/Exponential_distribution">http://en.wikipedia.org/wiki/Exponential_distribution</a>) to the data. According to this model, <span class="emphasis"><em>S</em></span> (number of days of survival) is an exponential random variable with the parameter <span class="inlinemediaobject"><img src="images/4818OS_07_43.jpg" alt="How to do it..."/></span>, and the observations <span class="emphasis"><em>s<sub>i</sub></em></span> are sampled from this distribution. Let the sample mean be:
<div class="mediaobject"><img src="images/4818OS_07_20.jpg" alt="How to do it..."/></div>The likelihood function of an exponential distribution is as follows, by definition (see proof in the next section):<div class="mediaobject"><img src="images/4818OS_07_21.jpg" alt="How to do it..."/></div><p>The <span class="strong"><strong>maximum likelihood estimate</strong></span><a id="id1116" class="indexterm"/> for the rate parameter is, by definition, the value <span class="inlinemediaobject"><img src="images/4818OS_07_43.jpg" alt="How to do it..."/></span> that <a id="id1117" class="indexterm"/>maximizes the<a id="id1118" class="indexterm"/> likelihood function. In other words, it is the parameter that maximizes the probability of observing the data, assuming that the observations are sampled from an exponential distribution.</p><p>Here, it can be shown that the likelihood function has a maximum value when <span class="inlinemediaobject"><img src="images/4818OS_07_44.jpg" alt="How to do it..."/></span>, which is the maximum likelihood estimate for the rate parameter. Let's compute this parameter numerically:</p><div class="informalexample"><pre class="programlisting">In [6]: smean = survival.mean()
        rate = 1./smean</pre></div></li><li class="listitem">To compare the fitted exponential distribution to the data, we first need to generate linearly spaced values for the x-axis (days):<div class="informalexample"><pre class="programlisting">In [7]: smax = survival.max()
        days = np.linspace(0., smax, 1000)
        dt = smax / 999.  # bin size: interval between two
                          # consecutive values in `days`</pre></div><p>We can obtain the probability density function of the exponential distribution with SciPy. The parameter is the scale, the inverse of the estimated rate.</p><div class="informalexample"><pre class="programlisting">In [8]: dist_exp = st.expon.pdf(days, scale=1./rate)</pre></div></li><li class="listitem">Now, let's plot the histogram and the obtained distribution. We need to rescale the theoretical distribution to the histogram (depending on the bin size and the total number of data points):<div class="informalexample"><pre class="programlisting">In [9]: nbins = 30
        plt.hist(survival, nbins)
        plt.plot(days, dist_exp*len(survival)*smax/nbins,
                 '-r', lw=3)</pre></div><div class="mediaobject"><img src="images/4818OS_07_22.jpg" alt="How to do it..."/></div><p>The fit is<a id="id1119" class="indexterm"/> far from perfect. We were able to find an analytical<a id="id1120" class="indexterm"/> formula for the maximum likelihood estimate here. In more complex situations, that is not always possible. Thus we may need to resort to numerical methods. SciPy actually integrates numerical maximum likelihood routines for a large number of distributions. Here, we use this other method to estimate the parameter of the exponential distribution.</p><div class="informalexample"><pre class="programlisting">In [10]: dist = st.expon
         args = dist.fit(survival); args
Out[10]: (0.99999999994836486, 222.28880590143666)</pre></div></li><li class="listitem">We can use these parameters to perform a <a id="id1121" class="indexterm"/><span class="strong"><strong>Kolmogorov-Smirnov test</strong></span>, which assesses the goodness of fit of the distribution with respect to the data. This test is based on a distance between the <span class="strong"><strong>empirical distribution function</strong></span><a id="id1122" class="indexterm"/> of the data and the <span class="strong"><strong>cumulative distribution function</strong></span> (<span class="strong"><strong>CDF</strong></span>)<a id="id1123" class="indexterm"/> of the reference distribution.<div class="informalexample"><pre class="programlisting">In [11]: st.kstest(survival, dist.cdf, args)
Out[11]: (0.36199685486406347, 8.6470960143358866e-06)</pre></div><p>The second output value is the p-value. Here, it is very low: the null hypothesis (stating that the observed data stems from an exponential distribution with a maximum likelihood rate parameter) can be rejected with high confidence. Let's try another distribution, the <a id="id1124" class="indexterm"/><span class="strong"><strong>Birnbaum-Sanders distribution</strong></span>, which is typically used to model failure times. (More information on the Birnbaum-Sanders distribution<a id="id1125" class="indexterm"/> is available at <a class="ulink" href="http://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution">http://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution</a>.)</p><div class="informalexample"><pre class="programlisting">In [12]: dist = st.fatiguelife
         args = dist.fit(survival)
         st.kstest(survival, dist.cdf, args)
Out[12]: (0.18773446101946889, 0.073211497000863268)</pre></div><p>This time, the <a id="id1126" class="indexterm"/>p-value is 0.07, so that we would not reject the null hypothesis<a id="id1127" class="indexterm"/> with a five percent confidence level. When plotting the resulting distribution, we observe a better fit than with the exponential distribution:</p><div class="informalexample"><pre class="programlisting">In [13]: dist_fl = dist.pdf(days, *args)
         nbins = 30
         plt.hist(survival, nbins)
         plt.plot(days, dist_exp*len(survival)*smax/nbins,
                  '-r', lw=3, label='exp')
         plt.plot(days, dist_fl*len(survival)*smax/nbins,
                  '-g', lw=3, label='BS')
         plt.xlabel("Survival time (days)")
         plt.ylabel("Number of patients")
         plt.legend()</pre></div><div class="mediaobject"><img src="images/4818OS_07_23.jpg" alt="How to do it..."/></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec255"/>How it works...</h2></div></div></div><p>Here, we give <a id="id1128" class="indexterm"/>the calculations<a id="id1129" class="indexterm"/> leading to the maximum likelihood estimation of the rate parameter for an exponential distribution:</p><div class="mediaobject"><img src="images/4818OS_07_24.jpg" alt="How it works..."/></div><p>Here, <span class="inlinemediaobject"><img src="images/4818OS_07_45.jpg" alt="How it works..."/></span> is the sample mean. In more complex situations, we would require numerical optimization methods in which the principle is to maximize the likelihood function using a standard numerical optimization algorithm (see <a class="link" href="ch09.html" title="Chapter 9. Numerical Optimization">Chapter 9</a>, <span class="emphasis"><em>Numerical Optimization</em></span>).</p><p>To find the maximum of this function, let's compute its derivative function with respect to <span class="inlinemediaobject"><img src="images/4818OS_07_43.jpg" alt="How it works..."/></span>:</p><div class="mediaobject"><img src="images/4818OS_07_25.jpg" alt="How it works..."/></div><p>The root of this<a id="id1130" class="indexterm"/> derivative is<a id="id1131" class="indexterm"/> therefore <span class="inlinemediaobject"><img src="images/4818OS_07_44.jpg" alt="How it works..."/></span>
</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec256"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Maximum likelihood<a id="id1132" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Maximum_likelihood">http://en.wikipedia.org/wiki/Maximum_likelihood</a></li><li class="listitem" style="list-style-type: disc">Kolmogorov-Smirnov test<a id="id1133" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test">http://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test</a></li><li class="listitem" style="list-style-type: disc">Goodness of fit<a id="id1134" class="indexterm"/> at <a class="ulink" href="http://en.wikipedia.org/wiki/Goodness_of_fit">http://en.wikipedia.org/wiki/Goodness_of_fit</a></li></ul></div><p>The maximum likelihood method is <span class="emphasis"><em>parametric</em></span>: the model belongs to a prespecified parametric family of distributions. In the next recipe, we will see a nonparametric kernel-based method.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec257"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Estimating a probability distribution nonparametrically with a kernel density estimation</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Estimating a probability distribution nonparametrically with a kernel density estimation"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec71"/>Estimating a probability distribution nonparametrically with a kernel density estimation</h1></div></div></div><p>In the previous recipe, we applied a <a id="id1135" class="indexterm"/><span class="strong"><strong>parametric estimation method</strong></span>. We had a <a id="id1136" class="indexterm"/>statistical model (the exponential distribution) describing<a id="id1137" class="indexterm"/> our data, and we estimated a single parameter (the rate of the distribution). <span class="strong"><strong>Nonparametric estimation</strong></span><a id="id1138" class="indexterm"/> deals with statistical models that do not belong to a known family of distributions. The parameter space is then <span class="emphasis"><em>infinite-dimensional</em></span> instead of finite-dimensional (that is, we estimate <span class="emphasis"><em>functions</em></span> rather than <span class="emphasis"><em>numbers</em></span>).</p><p>Here, we use a <span class="strong"><strong>kernel density estimation</strong></span> (<span class="strong"><strong>KDE</strong></span>)<a id="id1139" class="indexterm"/> to estimate the density of probability of a spatial distribution. We look at the geographical locations of tropical cyclones from 1848 to 2013, based on data provided by the NOAA, the US' National Oceanic and Atmospheric Administration.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec258"/>Getting ready</h2></div></div></div><p>Download the <span class="emphasis"><em>Storms</em></span> dataset<a id="id1140" class="indexterm"/> from the book's GitHub repository at <a class="ulink" href="https://github.com/ipython-books/cookbook-data">https://github.com/ipython-books/cookbook-data</a>, and extract it in the current directory. The data was obtained from <a class="ulink" href="http://www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data">www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data</a>.</p><p>You also need matplotlib's toolkit<a id="id1141" class="indexterm"/> <span class="strong"><strong>basemap</strong></span>, available at <a class="ulink" href="http://matplotlib.org/basemap/">http://matplotlib.org/basemap/</a>. With Anaconda, you can install it with conda install basemap. Windows users can also find an installer at <a class="ulink" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/">www.lfd.uci.edu/~gohlke/pythonlibs/</a>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec259"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import <a id="id1142" class="indexterm"/>the usual packages. The kernel density estimation with a Gaussian kernel is implemented in SciPy.stats:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import pandas as pd
        import scipy.stats as st
        import matplotlib.pyplot as plt
        from mpl_toolkits.basemap import Basemap
        %matplotlib inline</pre></div></li><li class="listitem">Let's open the data with pandas:<div class="informalexample"><pre class="programlisting">In [2]: df = pd.read_csv(
                   "data/Allstorms.ibtracs_wmo.v03r05.csv")</pre></div></li><li class="listitem">The dataset contains information about most storms since 1848. A single storm may appear multiple times across several consecutive days.<div class="informalexample"><pre class="programlisting">In [3]: df[df.columns[[0,1,3,8,9]]].head()
Out[3]:       Serial_Num  Season Basin  Latitude  Longitude
        0  1848011S09080    1848    SI      -8.6       79.8
        1  1848011S09080    1848    SI      -9.0       78.9
        2  1848011S09080    1848    SI     -10.4       73.2
        3  1848011S09080    1848    SI     -12.8       69.9
        4  1848011S09080    1848    SI     -13.9       68.9</pre></div></li><li class="listitem">We use pandas' <code class="literal">groupby()</code> function to obtain the average location of every storm:<div class="informalexample"><pre class="programlisting">In [4]: dfs = df.groupby('Serial_Num')
        pos = dfs[['Latitude', 'Longitude']].mean()
        y, x = pos.values.T
        pos.head()
Out[4]:                 Latitude  Longitude
        Serial_Num                         
        1848011S09080 -15.918182  71.854545
        1848011S15057 -24.116667  52.016667
        1848061S12075 -20.528571  65.342857
        1851080S15063 -17.325000  55.400000
        1851080S21060 -23.633333  60.200000</pre></div></li><li class="listitem">We<a id="id1143" class="indexterm"/> display the<a id="id1144" class="indexterm"/> storms on a map with basemap. This toolkit allows us to easily project the geographical coordinates on the map.<div class="informalexample"><pre class="programlisting">In [5]: m = Basemap(projection='mill', llcrnrlat=-65,
                    urcrnrlat=85, llcrnrlon=-180,
                    urcrnrlon=180)
        x0, y0 = m(-180, -65)
        x1, y1 = m(180, 85)
        m.drawcoastlines()
        m.fillcontinents(color='#dbc8b2')
        xm, ym = m(x, y)
        m.plot(xm, ym, '.r', alpha=.1)</pre></div><div class="mediaobject"><img src="images/4818OS_07_26.jpg" alt="How to do it..."/></div></li><li class="listitem">To perform the kernel density estimation, we stack the <code class="literal">x</code> and <code class="literal">y</code> coordinates of the storms into a <code class="literal">(2, N)</code> array:<div class="informalexample"><pre class="programlisting">In [6]: h = np.vstack((xm, ym))
In [7]: kde = st.gaussian_kde(h)</pre></div></li><li class="listitem">The <code class="literal">gaussian_kde()</code> routine returned a Python function. To see the results on a map, we need to evaluate this function on a 2D grid spanning the entire map. We create <a id="id1145" class="indexterm"/>this grid with <code class="literal">meshgrid()</code>, and we pass the <code class="literal">x</code> and <code class="literal">y</code> values to the <code class="literal">kde</code> function. <code class="literal">kde</code> accepts a <code class="literal">(2, N)</code> array as input, requiring us to tweak the<a id="id1146" class="indexterm"/> shape of the array:<div class="informalexample"><pre class="programlisting">In [8]: k = 50
        tx, ty = np.meshgrid(np.linspace(x0, x1, 2*k),
                             np.linspace(y0, y1, k))
        v = kde(np.vstack((tx.ravel(), 
                           ty.ravel()))).reshape((k, 2*k))</pre></div></li><li class="listitem">Finally, we display the estimated density with <code class="literal">imshow()</code>:<div class="informalexample"><pre class="programlisting">In [9]: m.drawcoastlines()
        m.fillcontinents(color='#dbc8b2')
        xm, ym = m(x, y)
        m.imshow(v, origin='lower', extent=[x0,x1,y0,y1],
                 cmap=plt.get_cmap('Reds'))</pre></div><div class="mediaobject"><img src="images/4818OS_07_27.jpg" alt="How to do it..."/></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec260"/>How it works...</h2></div></div></div><p>The <span class="strong"><strong>kernel density estimator</strong></span><a id="id1147" class="indexterm"/> of a set of <span class="emphasis"><em>n</em></span> points <span class="emphasis"><em>{x<sub>i</sub>}</em></span> is given as:</p><div class="mediaobject"><img src="images/4818OS_07_28.jpg" alt="How it works..."/></div><p>Here, <span class="emphasis"><em>h&gt;0</em></span> is a scaling parameter (the <span class="strong"><strong>bandwidth</strong></span>) and <span class="emphasis"><em>K(u)</em></span> is the <a id="id1148" class="indexterm"/><span class="strong"><strong>kernel</strong></span>, a <a id="id1149" class="indexterm"/>symmetric function that integrates to 1. This estimator is to be compared with a classical histogram, where the kernel would be a <span class="emphasis"><em>top-hat</em></span> function (a rectangle function taking its values in <span class="emphasis"><em>{0,1}</em></span>), but the blocks would be located on a regular grid instead of the data points. For more information on <a id="id1150" class="indexterm"/>kernel density estimator, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Kernel_density_estimation">http://en.wikipedia.org/wiki/Kernel_density_estimation</a>.</p><p>Multiple kernels<a id="id1151" class="indexterm"/> can be chosen. Here, we chose a <a id="id1152" class="indexterm"/><span class="strong"><strong>Gaussian kernel</strong></span>, so that the KDE is the superposition of Gaussian functions centered on all the data points. It is an estimation of the density.</p><p>The choice of the bandwidth is not trivial; there is a tradeoff between a too low value (small bias, high variance: overfitting) and a too high value (high bias, small variance: underfitting). We will return to this important concept of <span class="strong"><strong>bias-variance tradeoff</strong></span><a id="id1153" class="indexterm"/> in the next chapter. For more information on the bias-<a id="id1154" class="indexterm"/>variance tradeoff, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Bias-variance_dilemma">http://en.wikipedia.org/wiki/Bias-variance_dilemma</a>.</p><p>The following figure illustrates the KDE. The dataset contains four points in <span class="emphasis"><em>[0,1]</em></span> (black lines). The estimated density is a smooth curve, represented here with multiple bandwidth values.</p><div class="mediaobject"><img src="images/4818OS_07_29.jpg" alt="How it works..."/><div class="caption"><p>Kernel density estimation</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip25"/>Tip</h3><p>There are other KDE implementations in statsmodels<a id="id1155" class="indexterm"/> and<a id="id1156" class="indexterm"/> scikit-learn. You can find more information at <a class="ulink" href="http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/">http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/</a>.</p></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec261"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Fitting a probability distribution to data with the maximum likelihood method</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Fitting a Bayesian model by sampling from a posterior distribution with a Markov chain Monte Carlo method"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec72"/>Fitting a Bayesian model by sampling from a posterior distribution with a Markov chain Monte Carlo method</h1></div></div></div><p>In this recipe, we illustrate a<a id="id1157" class="indexterm"/> very common and useful method for characterizing a posterior distribution in a Bayesian model. Imagine that you have some data and you want to obtain information about the underlying random phenomenon. In a frequentist approach, you could try to fit a probability distribution within a given family of distributions, using a parametric method such as the maximum likelihood method. The optimization procedure would yield parameters that maximize the probability of observing the data if given the null hypothesis.</p><p>In a Bayesian approach, you consider the parameters themselves as random variables. Their prior distributions reflect your initial knowledge about these parameters. After the observations, your knowledge is updated, and this is reflected in the posterior distributions of the parameters.</p><p>A typical goal for Bayesian inference is to characterize the posterior distributions. Bayes' theorem gives an analytical way to do this, but it is often impractical in real-world problems due to the complexity of the models and the number of dimensions. A<a id="id1158" class="indexterm"/> <span class="strong"><strong>Markov chain </strong></span><a id="id1159" class="indexterm"/><span class="strong"><strong>Monte Carlo</strong></span> method, such as the <span class="strong"><strong>Metropolis-Hastings algorithm</strong></span>, gives a<a id="id1160" class="indexterm"/> numerical method to approximate a posterior distribution.</p><p>Here, we introduce the <a id="id1161" class="indexterm"/><span class="strong"><strong>PyMC</strong></span> package, which gives an effective and natural interface for fitting a probabilistic model to data in a Bayesian framework. We will look at the annual frequency of storms in the northern Atlantic Ocean since the 1850s using data from NOAA, the US' National Oceanic and Atmospheric Administration.</p><p>This recipe is largely inspired by a tutorial on PyMC's website (see the link in the <span class="emphasis"><em>There's more…</em></span> section).</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec262"/>Getting ready</h2></div></div></div><p>You can find the instructions to install PyMC<a id="id1162" class="indexterm"/> on the package's website. In this recipe, we will use PyMC2. The new version (PyMC3) is still in development at the time of writing, and it is likely to be significantly different. For more information on PyMC, refer to <a class="ulink" href="http://pymc-devs.github.io/pymc/">http://pymc-devs.github.io/pymc/</a>. With Anaconda, you can try <code class="literal">conda install -c https://conda.binstar.org/pymc pymc</code>. Windows users can also find an installer at <a class="ulink" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/">www.lfd.uci.edu/~gohlke/pythonlibs/</a>.</p><p>You also need to download the <span class="emphasis"><em>Storms</em></span> dataset from the book's GitHub repository at <a class="ulink" href="https://github.com/ipython-books/cookbook-data">https://github.com/ipython-books/cookbook-data</a> and extract it in the current directory.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec263"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import the standard packages and PyMC:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import pandas as pd
        import pymc
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">Let's import the data with pandas:<div class="informalexample"><pre class="programlisting">In [2]: df = pd.read_csv(
                "data/Allstorms.ibtracs_wmo.v03r05.csv",
                delim_whitespace=False)</pre></div></li><li class="listitem">With pandas, it <a id="id1163" class="indexterm"/>only takes a single line of code to get the annual number of storms in the North Atlantic Ocean. We first select the storms in that basin (<code class="literal">NA</code>), then we group the rows by year (<code class="literal">Season</code>), and then we take the number of unique storms (<code class="literal">Serial_Num</code>), as each storm can span several days (the <code class="literal">nunique()</code> method):<div class="informalexample"><pre class="programlisting">In [3]: cnt = df[df['Basin'] == ' NA'].groupby('Season') \
                           ['Serial_Num'].nunique()
        years = cnt.index
        y0, y1 = years[0], years[-1]
        arr = cnt.values
        plt.plot(years, arr, '-ok')
        plt.xlim(y0, y1)
        plt.xlabel("Year")
        plt.ylabel("Number of storms")</pre></div><div class="mediaobject"><img src="images/4818OS_07_30.jpg" alt="How to do it..."/></div></li><li class="listitem">Now, we define our probabilistic model. We assume that storms arise following a time-dependent Poisson process with a deterministic rate. We assume that this rate is a piecewise-constant function that takes a first value <code class="literal">early_mean</code> before a switch point <code class="literal">switchpoint</code>, and a second value <code class="literal">late_mean</code> after that point. These three unknown parameters are treated as random variables (we will describe them more in the <span class="emphasis"><em>How it works…</em></span> section).<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip26"/>Tip</h3><p>A <a id="id1164" class="indexterm"/>Poisson process (<a class="ulink" href="http://en.wikipedia.org/wiki/Poisson_process">http://en.wikipedia.org/wiki/Poisson_process</a>) is a particular <a id="id1165" class="indexterm"/><span class="strong"><strong>point process</strong></span>, that is, a stochastic process describing the random occurrence of instantaneous events. The Poisson process is fully random: the events occur independently at a given rate. See also <a class="link" href="ch13.html" title="Chapter 13. Stochastic Dynamical Systems">Chapter 13</a>, <span class="emphasis"><em>Stochastic Dynamical Systems</em></span>.</p></div></div><div class="informalexample"><pre class="programlisting">In [4]: switchpoint = pymc.DiscreteUniform('switchpoint',
                                           lower=0, 
                                           upper=len(arr))
        early_mean = pymc.Exponential('early_mean', beta=1)
        late_mean = pymc.Exponential('late_mean', beta=1)</pre></div></li><li class="listitem">We define the<a id="id1166" class="indexterm"/> piecewise-constant rate as a Python function:<div class="informalexample"><pre class="programlisting">In [5]: @pymc.deterministic(plot=False)
        def rate(s=switchpoint, e=early_mean, l=late_mean):
            out = np.empty(len(arr))
            out[:s] = e
            out[s:] = l
            return out</pre></div></li><li class="listitem">Finally, the observed variable is the annual number of storms. It follows a Poisson variable with a random mean (the rate of the underlying Poisson process). This fact is a known mathematical property of Poisson processes.<div class="informalexample"><pre class="programlisting">In [6]: storms = pymc.Poisson('storms', mu=rate, value=arr, 
                              observed=True)</pre></div></li><li class="listitem">Now, we use the MCMC method to sample from the posterior distribution, given the observed data. The <code class="literal">sample()</code> method launches the fitting iterative procedure:<div class="informalexample"><pre class="programlisting">In [7]: model = pymc.Model([switchpoint, early_mean, 
                            late_mean,
                            rate, storms])
In [8]: mcmc = pymc.MCMC(model)
        mcmc.sample(iter=10000, burn=1000, thin=10)
         [----       17%            ] 1774 of 10000 complete
         [-----------100%-----------] 10000 of 10000 complete</pre></div></li><li class="listitem">Let's plot the sampled Markov chains. Their stationary distribution corresponds to the posterior distribution we want to characterize.<div class="informalexample"><pre class="programlisting">In [9]: plt.subplot(311)
        plt.plot(mcmc.trace('switchpoint')[:])
        plt.ylabel("Switch point")
        plt.subplot(312)
        plt.plot(mcmc.trace('early_mean')[:])
        plt.ylabel("Early mean")
        plt.subplot(313)
        plt.plot(mcmc.trace('late_mean')[:])
        plt.xlabel("Iteration")
        plt.ylabel("Late mean")</pre></div><div class="mediaobject"><img src="images/4818OS_07_31.jpg" alt="How to do it..."/></div></li><li class="listitem">We also plot<a id="id1167" class="indexterm"/> the distribution of the samples, which correspond to the posterior distributions of our parameters, after the data points have been taken into account:<div class="informalexample"><pre class="programlisting">In [10]: plt.subplot(131)
         plt.hist(mcmc.trace('switchpoint')[:] + y0, 15)
         plt.xlabel("Switch point")
         plt.ylabel("Distribution")
         plt.subplot(132)
         plt.hist(mcmc.trace('early_mean')[:], 15)
         plt.xlabel("Early mean")
         plt.subplot(133)
         plt.hist(mcmc.trace('late_mean')[:], 15)
         plt.xlabel("Late mean")</pre></div><div class="mediaobject"><img src="images/4818OS_07_32.jpg" alt="How to do it..."/></div></li><li class="listitem">Taking the<a id="id1168" class="indexterm"/> sample mean of these distributions, we get posterior estimates for the three unknown parameters, including the year where the frequency of storms suddenly increased:<div class="informalexample"><pre class="programlisting">In [11]: yp = y0 + mcmc.trace('switchpoint')[:].mean()
         em = mcmc.trace('early_mean')[:].mean()
         lm = mcmc.trace('late_mean')[:].mean()
         print((yp, em, lm))
(1966.681111111111, 8.2843072252292682, 16.728831395584947)</pre></div></li><li class="listitem">Now, we can plot the estimated rate on top of the observations:<div class="informalexample"><pre class="programlisting">In [12]: plt.plot(years, arr, '-ok')
         plt.axvline(yp, color='k', ls='--')
         plt.plot([y0, yp], [em, em], '-b', lw=3)
         plt.plot([yp, y1], [lm, lm], '-r', lw=3)
         plt.xlim(y0, y1)
         plt.xlabel("Year")
         plt.ylabel("Number of storms")</pre></div><div class="mediaobject"><img src="images/4818OS_07_33.jpg" alt="How to do it..."/></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec264"/>How it works...</h2></div></div></div><p>The general <a id="id1169" class="indexterm"/>idea is to define a Bayesian probabilistic model and to fit it to the data. This model may be the starting point of an estimation or decision task. The model is essentially described by stochastic or deterministic variables linked together within a <a id="id1170" class="indexterm"/><span class="strong"><strong>direct acyclic graph</strong></span> (<span class="strong"><strong>DAG</strong></span>). <span class="emphasis"><em>A</em></span> is linked to <span class="emphasis"><em>B</em></span> if <span class="emphasis"><em>B</em></span> is entirely or partially determined by <span class="emphasis"><em>A</em></span>. The following figure shows the graph of the model used in this recipe:</p><div class="informalexample"><pre class="programlisting">In [13]: graph = pymc.graph.graph(model)
         from IPython.display import display_png
         display_png(graph.create_png(), raw=True)</pre></div><div class="mediaobject"><img src="images/4818OS_07_34.jpg" alt="How it works..."/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip27"/>Tip</h3><p>As you can see, PyMC can create graph representations of the models. You need to install GraphViz<a id="id1171" class="indexterm"/> (refer to <a class="ulink" href="http://www.graphviz.org">www.graphviz.org</a>), pydot, and pyparsing. Because of an unfortunate bug, you might need to install a specific version of pyparsing:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pip install pyparsing==1.5.7</strong></span>
<span class="strong"><strong>pip install pydot</strong></span>
</pre></div></div></div><p>Stochastic variables follow distributions that can be parameterized by fixed numbers or other variables in the model. Parameters may be random variables themselves, reflecting knowledge prior to the observations. This is the core of Bayesian modeling.</p><p>The goal of the analysis is to include the observations into the model in order to update our knowledge as more and more data is available. Although Bayes' theorem gives us an exact way to compute those posterior distributions, it is rarely practical in real-world problems. This is notably due to the complexity of the models. Alternatively, numerical methods have been developed in order to tackle this problem.</p><p>The <span class="strong"><strong>Markov chain Monte Carlo</strong></span> (<span class="strong"><strong>MCMC</strong></span>)<a id="id1172" class="indexterm"/> method used here allows us to sample from a complex distribution by simulating a Markov chain that has the desired distribution as its equilibrium distribution. The <span class="strong"><strong>Metropolis-Hastings algorithm</strong></span><a id="id1173" class="indexterm"/> is a particular application of this method to our current example.</p><p>This <a id="id1174" class="indexterm"/>algorithm is implemented in the <code class="literal">MCMC</code> class in PyMC. The <code class="literal">burn</code> parameter determines how many initial iterations are thrown away. This is necessary because it takes a number of iterations for the Markov chain to converge to its equilibrium distribution. The <code class="literal">thin</code> parameter corresponds to the number of steps to skip in the evaluation of the distribution so as to minimize the autocorrelation of the samples. You will find more information at <a class="ulink" href="http://pymc-devs.github.io/pymc/modelfitting.html">http://pymc-devs.github.io/pymc/modelfitting.html</a>.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec265"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A great PyMC tutorial<a id="id1175" class="indexterm"/> that we largely took inspiration from is available at <a class="ulink" href="http://pymc-devs.github.io/pymc/tutorial.html">http://pymc-devs.github.io/pymc/tutorial.html</a></li><li class="listitem" style="list-style-type: disc">A must-read free e-book on the subject, by Cameron Davidson-Pilon, entirely written in the <a id="id1176" class="indexterm"/>IPython notebook, available at <a class="ulink" href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/</a></li><li class="listitem" style="list-style-type: disc">The Markov chain Monte Carlo method<a id="id1177" class="indexterm"/> introduced at <a class="ulink" href="http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo</a></li><li class="listitem" style="list-style-type: disc">The Metropolis-Hastings algorithm<a id="id1178" class="indexterm"/> introduced at <a class="ulink" href="http://en.wikipedia.org/wiki/Metropolis-Hastings_algorithm">http://en.wikipedia.org/wiki/Metropolis-Hastings_algorithm</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec266"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Getting started with Bayesian methods</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Analyzing data with the R programming language in the IPython notebook"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec73"/>Analyzing data with the R programming language in the IPython notebook</h1></div></div></div><p>R (<a class="ulink" href="http://www.r-project.org">www.r-project.org</a>) is <a id="id1179" class="indexterm"/>a <a id="id1180" class="indexterm"/>free domain-specific programming language for <a id="id1181" class="indexterm"/>statistics. Its syntax is<a id="id1182" class="indexterm"/> well-adapted to statistical modeling and data <a id="id1183" class="indexterm"/>analysis. By contrast, Python's syntax is typically more convenient for general-purpose programming. Luckily, IPython allows you to have the best of both worlds. For example, you can insert R code snippets anywhere in a normal IPython notebook. You can continue using Python and pandas for data loading and wrangling, and switch to R to design and fit statistical models. Using R instead of Python for these tasks is more than a matter of programming syntax; R comes with an impressive statistical toolbox that is still unmatched by Python.</p><p>In this recipe, we will show how to use R from IPython, and we illustrate the most basic capabilities of R with a simple data analysis example.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec267"/>Getting ready</h2></div></div></div><p>You need the <a id="id1184" class="indexterm"/>statsmodels package <a id="id1185" class="indexterm"/>for this recipe. You can find installation instructions in the previous recipe, <span class="emphasis"><em>Fitting a probability distribution to data with the maximum likelihood method</em></span>.</p><p>You also need R. There <a id="id1186" class="indexterm"/>are three steps to use R from IPython. First, install R and rpy2 (R to Python interface). Of course, you only need to do this step once. Then, to use R in an IPython session, you need to load the IPython R extension.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download R for your operating system from <a class="ulink" href="http://cran.r-project.org/mirrors.html">http://cran.r-project.org/mirrors.html</a> and install it. On Ubuntu, you can do <code class="literal">sudo apt-get install r-base-dev</code>.</li><li class="listitem">Download rpy2<a id="id1187" class="indexterm"/> from <a class="ulink" href="http://rpy.sourceforge.net/rpy2.html">http://rpy.sourceforge.net/rpy2.html</a> and install it. With Anaconda on Linux, you can try <code class="literal">conda install -c https://conda.binstar.org/r rpy2</code>. Alternatively, you can do <code class="literal">pip install rpy2</code>.</li><li class="listitem">Then, to execute R code in an IPython notebook, execute <code class="literal">%load_ext rmagic</code> first.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip28"/>Tip</h3><p>rpy2 does not appear to work well on Windows. We recommend using Linux or Mac OS X.</p></div></div></li></ol></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec268"/>How to do it...</h2></div></div></div><p>Here, we will use the following workflow: first, we load data from Python. Then, we use R to design and fit a model, and to make some plots in the IPython notebook. We could also load data from R, or design and fit a statistical model with Python's statsmodels package, and so on. In particular, the analysis we do here could be done entirely in Python, without resorting to the R language. This recipe merely shows the basics of R and illustrates how R and Python can play together within an IPython session.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's load the <span class="emphasis"><em>longley</em></span> dataset with the statsmodels package. This dataset contains a few economic indicators in the US from 1947 to 1962. We also load the IPython R extension:<div class="informalexample"><pre class="programlisting">In [1]: import statsmodels.datasets as sd
In [2]: data = sd.longley.load_pandas()
In [3]: %load_ext rmagic</pre></div></li><li class="listitem">We define <code class="literal">x</code> and <code class="literal">y</code> as the exogeneous (independent) and endogenous (dependent) variables, respectively. The endogenous variable quantifies the total employment in the country.<div class="informalexample"><pre class="programlisting">In [4]: data.endog_name, data.exog_name
Out[4]: ('TOTEMP', ['GNPDEFL', 'GNP', 'UNEMP',
                    'ARMED', 'POP', 'YEAR'])
In [5]: y, x = data.endog, data.exog</pre></div></li><li class="listitem">For<a id="id1188" class="indexterm"/> convenience, we <a id="id1189" class="indexterm"/>add the endogenous variable to the <code class="literal">x</code> <a id="id1190" class="indexterm"/>DataFrame:<div class="informalexample"><pre class="programlisting">In [6]: x['TOTEMP'] = y
In [7]: x
Out[7]:     GNPDEFL     GNP  UNEMP     POP  YEAR  TOTEMP
        0      83.0  234289   2356  107608  1947   60323
        1      88.5  259426   2325  108632  1948   61122
        2      88.2  258054   3682  109773  1949   60171
        ...
        13    114.2  502601   3931  125368  1960   69564
        14    115.7  518173   4806  127852  1961   69331
        15    116.9  554894   4007  130081  1962   70551</pre></div></li><li class="listitem">We will make a simple plot in R. First, we need to pass Python variables to R. We can use the <code class="literal">%R -i var1,var2</code> magic. Then, we can call R's <code class="literal">plot()</code> command:<div class="informalexample"><pre class="programlisting">In [8]: gnp = x['GNP']
        totemp = x['TOTEMP']
In [9]: %R -i totemp,gnp plot(gnp, totemp)</pre></div><div class="mediaobject"><img src="images/4818OS_07_35.jpg" alt="How to do it..."/></div></li><li class="listitem">Now that the data <a id="id1191" class="indexterm"/>has been passed to R, we can fit a linear <a id="id1192" class="indexterm"/>model to the data. The <code class="literal">lm()</code> function lets us perform a linear<a id="id1193" class="indexterm"/> regression. Here, we want to express <code class="literal">totemp</code> (total employment) as a function of the country's GNP:<div class="informalexample"><pre class="programlisting">In [10]: %%R
         # Least-squares regression
         fit &lt;- lm(totemp ~ gnp);
         # Display the coefficients of the fit.
         print(fit$coefficients)
         plot(gnp, totemp)  # Plot the data points.
         abline(fit)  # And plot the linear regression.
 (Intercept)          gnp 
5.184359e+04 3.475229e-02</pre></div><div class="mediaobject"><img src="images/4818OS_07_36.jpg" alt="How to do it..."/></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec269"/>How it works...</h2></div></div></div><p>The <code class="literal">-i</code> and <code class="literal">-o</code> options of the <code class="literal">%R</code> magic allow us to pass variables back and forth between IPython and R. The variable names<a id="id1194" class="indexterm"/> need to be separated by commas. You can find more information about the <code class="literal">%R</code> magic in the documentation available at <a class="ulink" href="http://rpy.sourceforge.net/">http://rpy.sourceforge.net/</a>.</p><p>In R, the tilde (<code class="literal">~</code>) expresses the <a id="id1195" class="indexterm"/>dependence of a dependent variable upon one or several independent variables. The <code class="literal">lm()</code> function<a id="id1196" class="indexterm"/> allows us to fit a simple linear regression model to the data. Here, <code class="literal">totemp</code> is expressed as a function of <code class="literal">gnp</code>:</p><div class="mediaobject"><img src="images/4818OS_07_37.jpg" alt="How it works..."/></div><p>Here, <span class="emphasis"><em>b</em></span> (intercept) and <span class="emphasis"><em>a</em></span> are the coefficients of the linear regression model. These two values are returned by <code class="literal">fit$coefficients</code> in R, where <code class="literal">fit</code> is the fitted model.</p><p>Our<a id="id1197" class="indexterm"/> data points do not satisfy this relation exactly, of course. The coefficients are chosen so as to minimize the error between this linear prediction and the actual values. This is typically done by minimizing the following least squares error:</p><div class="mediaobject"><img src="images/4818OS_07_38.jpg" alt="How it works..."/></div><p>The data points are <span class="emphasis"><em>(gnp<sub>i</sub></em></span><span class="emphasis"><em>, totemp<sub>i</sub>)</em></span> here. The coefficients <span class="emphasis"><em>a</em></span> and <span class="emphasis"><em>b</em></span> that are returned by <code class="literal">lm()</code> make this sum minimal: they fit the data best.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec270"/>There's more...</h2></div></div></div><p>Regression is an important statistical concept that we will see in greater detail in the next chapter. Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Regression analysis<a id="id1198" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Regression_analysis">http://en.wikipedia.org/wiki/Regression_analysis</a></li><li class="listitem" style="list-style-type: disc">Least squares method<a id="id1199" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)">en.wikipedia.org/wiki/Linear_least_squares_(mathematics)</a></li></ul></div><p>R is an excellent platform for advanced statistics. Python has a few statistical packages such as pandas and statsmodels that implement many common features, but the number of statistical toolboxes in R remains unmatched by Python at this time. Yet, Python has a much wider range of possibilities outside of statistics and is an excellent general-purpose language that comes with an impressive number of various packages.</p><p>Thanks to the multilanguage capabilities of IPython, you don't necessarily have to choose between those languages. You can keep using Python and switch to R when you need highly specific statistical features that are still missing in Python.</p><p>Here are a few references about R:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introduction to R<a id="id1200" class="indexterm"/> available at <a class="ulink" href="http://cran.r-project.org/doc/manuals/R-intro.html">http://cran.r-project.org/doc/manuals/R-intro.html</a></li><li class="listitem" style="list-style-type: disc">R tutorial available at <a class="ulink" href="http://www.cyclismo.org/tutorial/R/">www.cyclismo.org/tutorial/R/</a></li><li class="listitem" style="list-style-type: disc">CRAN, or <a id="id1201" class="indexterm"/>Comprehensive R Archive Network, containing many packages for R, available at <a class="ulink" href="http://cran.r-project.org">http://cran.r-project.org</a></li><li class="listitem" style="list-style-type: disc">IPython<a id="id1202" class="indexterm"/> and R<a id="id1203" class="indexterm"/> tutorial available at <a class="ulink" href="http://nbviewer.ipython.org/github/ipython/ipython/blob/master/examples/Builtin%20Extensions/R%20Magics.ipynb">http://nbviewer.ipython.org/github/ipython/ipython/blob/master/examples/Builtin%20Extensions/R%20Magics.ipynb</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec271"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Exploring a dataset with pandas and matplotlib</em></span> recipe</li></ul></div></div></div></div>
</body></html>