["```py\npd.read_csv(filepath, sep=', ', dtype=None, header=None, names=None, skiprows=None, index_col=None, skip_blank_lines=TRUE, na_filter=TRUE)\n```", "```py\nimport pandas as pd\nimport os\nos.chdir(' ')\ndata=pd.read_csv('Hospital Cost.csv')\n```", "```py\ncolumn_names=pd.read_csv('Customer Churn Columns.csv')\ncolumn_names_list=column_names['Column Names'].tolist()\ndata=pd.read_csv('Customer Churn Model.txt',header=None,names=column_names_list)\n```", "```py\nfrom io import StringIO\ndata = 'col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\\nc,e,4\\ng,f,5\\ne,z,6'\npd.read_csv(StringIO(data))\n```", "```py\nfrom io import StringIO\ndata = 'col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\\nc,e,4\\ng,f,5\\ne,z,6'\npd.read_csv(StringIO(data),skiprows=lambda x: x % 3 != 0)\n```", "```py\ndata = 'a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10'\npd.read_csv(StringIO(data), index_col=0)\n```", "```py\ndata=pd.read_csv('Tab Customer Churn Model.txt',sep='/t')\n```", "```py\ndata=pd.read_csv('Tab Customer Churn Model.txt',sep='/t',usecols=[1,3,5])\ndata=pd.read_csv('Tab Customer Churn Model.txt',sep='/t',usecols=['VMail Plan','Area Code'])\n```", "```py\npd.read_csv('tmp.txt',sep='|')\n```", "```py\ndata.level.dtype returns dtype('O')\n```", "```py\npd.read_csv('tmp.txt',sep='|',thousands=',')\ndata.level.dtype now returns dtype('int64')\n```", "```py\npd.read_csv('mindex.txt')\npd.read_csv('mindex.txt',index_col=[0,1])\n```", "```py\ndata.loc[1977]\ndata.loc[(1977,'A')]\n```", "```py\nfor chunks in pd.read_csv('Chunk.txt',chunksize=500):\n    print(chunks.shape)\n```", "```py\ndata=pd.read_csv('Chunk.txt',chunksize=500)\ndata=pd.concat(data,ignore_index=True)\nprint(data.shape)\n```", "```py\nd1 = \npd.read_csv('t1.txt',index_col=0, delim_whitespace=True,quotechar=\"\\\"\")\nd1.head()\n```", "```py\nimport numpy as np\nimport pandas as pd\na=['Male','Female']\nb=['Rich','Poor','Middle Class']\ngender=[]\nseb=[]\n\nfor i in range(1,101):\n    gender.append(np.random.choice(a))\n    seb.append(np.random.choice(b))\n    height=30*np.random.randn(100)+155\n    weight=20*np.random.randn(100)+60\n    age=10*np.random.randn(100)+35\n    income=1500*np.random.randn(100)+15000\n\ndf=pd.DataFrame({'Gender':gender,'Height':height,'Weight':weight,'Age':age,'Income':income,'Socio-Eco':seb})\n```", "```py\ndf.to_csv('data.csv')\ndf.to_csv('data.txt')\n```", "```py\ndf.to_csv('data.csv',sep='|')\ndf.to_csv('data.txt',sep='/')\n```", "```py\npd.read_csv('http://bit.ly/2cLzoxH').head()\n```", "```py\nimport csv \nimport urllib2 \n\nurl='http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' \nresponse=urllib2.urlopen(url) \ncr=csv.reader(response) \n\nfor rows in cr: \n   print rows \n\n```", "```py\nimport os \nimport pandas as pd \nfrom s3fs.core import S3FileSystem \n\nos.environ['AWS_CONFIG_FILE'] = 'aws_config.ini' \n\ns3 = S3FileSystem(anon=False) \nkey = 'path\\to\\your-csv.csv' \nbucket = 'your-bucket-name' \n\ndf = pd.read_csv(s3.open('{}/{}'.format(bucket, key), \n                         mode='rb') \n                 ) \n```", "```py\n import s3fs \n\nbytes_to_write = df.to_csv(None).encode() \nfs = s3fs.S3FileSystem(key=key, secret=secret) \nwith fs.open('s3://bucket/path/to/file.csv', 'wb') as f: \n    f.write(bytes_to_write) \n\n```", "```py\npd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html') \n```", "```py\n match = 'Malta National Bank' \ndf_list = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html', match=match) \n```", "```py\ndata=pd.read_csv('http://bit.ly/2cLzoxH')\nprint(data.to_html())\n```", "```py\n print(data.to_html(columns=['country','year'])) \n```", "```py\ndata=pd.read_csv('http://bit.ly/2cLzoxH') \nprint(data.to_html('test.html')) \n```", "```py\n data.to_html('test.html',bold_rows=False) \n```", "```py\nimport numpy as np \npd.DataFrame(np.random.randn(5, 2), columns=list('AB')).to_json() \"\n```", "```py\ndfjo = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),columns=list('ABC'), index=list('xyz')) \ndfjo.to_json(orient=\"columns\")\n```", "```py\ndfjo = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),columns=list('ABC'), index=list('xyz')) \ndfjo.to_json(orient=\"index\")\n```", "```py\nd3.js. dfjo = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),columns=list('ABC'), index=list('xyz')) dfjo.to_json(orient=\"records\")\n```", "```py\ndfjo = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),columns=list('ABC'), index=list('xyz')) dfjo.to_json(orient=\"values\") \n```", "```py\ndfjo = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),columns=list('ABC'), index=list('xyz')) dfjo.to_json(orient=\"split\")\n```", "```py\nimport json with open('jsonex.txt','w') as outfile: json.dump(dfjo.to_json(orient=\"columns\"), outfile)\n```", "```py\nf=open('usagov_bitly.txt','r').readline() json.loads(f) \n```", "```py\nrecords=[] f=open('usagov_bitly.txt','r') for i in range(1000): fiterline=f.readline() d=json.loads(fiterline) records.append(d) f.close()\n```", "```py\nlatlong=[rec['ll'] for rec in records if 'll' in rec]\n```", "```py\ndf=pd.DataFrame(records) \ndf.head() \ndf['tz'].value_counts() \n```", "```py\n with open('product.json') as json_string: \n    d=json.load(json_string) \nd \n```", "```py\n d['hits']['hits'][0]['_score'] \n```", "```py\n d['hits']['hits'][0]['_source']['r']['wayfair']['image_url']\n```", "```py\nfor keys,values in d['hits']['hits'][0].items(): \nprint(keys)\n```", "```py\nfor keys,values in d['hits']['hits'][0].items():\n print(keys,values) \n```", "```py\npd.read_hdf('stat_df.h5','table')\n```", "```py\npd.read_hdf('stat_df.h5', 'table', where=['index>=2'])\n```", "```py\npd.read_feather(\"sample.feather\")\n```", "```py\npd.read_parquet(\"sample.paraquet\",engine='pyarrow')\n```", "```py\npd.read_parquet(\"sample.paraquet\",engine='pyarrow',columns=[\"First_Name\",\"Score\"])\n```", "```py\npd.read_parquet(\"sample.paraquet\",engine='fastparquet')\n```", "```py\nengine = create_engine('sqlite:///:memory:')\n```", "```py\nwith engine.connect() as conn, conn.begin():\n    print(pd.read_sql_table('data_sql', conn))\n```", "```py\npd.read_sql_table('data_sql', engine, index_col='index')\n```", "```py\npd.read_sql_table('data_sql', engine, parse_dates={'Entry_date': '%Y-%m-*%d*'})\npd.read_sql_table('data_sql', engine, parse_dates={'Entry_date': {'format': '%Y-%m-*%d* %H:%M:%S'}})\n```", "```py\npd.read_sql_query(\"SELECT Last_name, Score FROM data_sql\", engine)\n```", "```py\nfrom pandas.io import sql\nsql.execute(\"INSERT INTO tablename VALUES (90, 100, 171)\", engine)\n```", "```py\nimport sqlite3\nconn = sqlite3.connect(':memory:')\n```", "```py\ndf = pd.read_sas('sample.sas7bdat')\ndf\n```", "```py\nrdr = pd.read_sas('sample.sas7bdat', chunksize=10)\nfor chunk in rdr:\nprint(chunk.shape)\n```", "```py\ndf = pd.read_stata('sample.dta')\ndf\n```", "```py\npd.read_gbq(\"SELECT urban_area_code, geo_code, name, area_type, area_land_meters \nFROM `bigquery-public-data.utility_us.us_cities_area` LIMIT 5\", project_id, dialect = \"standard\")\n```", "```py\npd.read_clipboard()\n```", "```py\ndf = pd.DataFrame(np.random.randn(100, 3))\ndf.iloc[:95] = np.nan\n```", "```py\ndf.memory_usage()\n```", "```py\nsparse_df = df.to_sparse()\nsparse_df.memory_usage()\n```", "```py\ndf.fillna(0).to_sparse(fill_value = 0)\ndf.fillna(0).to_sparse(fill_value = 0).memory_usage()\n```", "```py\nsparse_df.to_dense()\n```", "```py\ndf = pd.DataFrame({\"Col1\": [1, 2, 3, 4, 5], \"Col2\": [\"A\", \"B\", \"B\", \"A\", \"C\"], \"Col3\": [True, False, False, True, True]})\ndf.to_json()\n```", "```py\ndf.to_json(orient=\"columns\")\n```", "```py\ndf.to_json(orient=\"index\")\n```", "```py\ndf.to_json(orient=\"records\")\n```", "```py\ndf.to_json(orient=\"values\")\n```", "```py\ndf.to_json(orient=\"split\")\n```", "```py\ndf.to_json(orient=\"table\")\n```", "```py\ndf = pd.DataFrame({\"First_Name\":[\"Mike\",\"Val\",\"George\",\"Chris\",\"Benjamin\"],\n\"Last_name\":[\"K.\",\"K.\",\"C.\",\"B.\",\"A.\"],\n\"Entry_date\":pd.to_datetime([\"June 23,1989\",\"June 16,1995\",\"June 20,1997\",\"June 25,2005\",\"March 25,2016\"],format= \"%B %d,%Y\"),\n\"Score\":np.random.random(5)})\ndf\n```", "```py\ndf.to_pickle('pickle_filename.pkl')\n```", "```py\ndf.to_pickle(\"pickle_filename.compress\", compression=\"gzip\")\n```", "```py\ndf.to_pickle(\"pickle_filename.gz\")\n```", "```py\ndf.to_parquet('sample_pyarrow.parquet', engine='pyarrow')\ndf.to_parquet('sample_fastparquet.parquet', engine='fastparquet')\n```", "```py\ndf.to_hdf('store.h5', append = True, format='table')\n```", "```py\nfrom sqlalchemy import create_engine\nengine = create_engine('sqlite:///:memory:')\ndf.to_sql('data_sql',engine)\n```", "```py\nfrom sqlalchemy.types import String\ndf.to_sql('data_dtype', engine, dtype={'Score': String})\n```", "```py\ndf.to_feather('sample.feather')\n```", "```py\ndf.to_html()\n```", "```py\ndf.to_msgpack(\"sample.msg\")\n```", "```py\narr = np.random.randint(1,10,7)\nlst = [10,20,40,60,60]\nstrg = \"Data\"\npd.to_msgpack(\"msg_all.msg\",df,arr,lst,strg)\n```", "```py\ndf.to_latex()\n```", "```py\ndf.to_stata('stata_df.dta')\n```", "```py\ndf.to_clipboard()\n```", "```py\ndf.to_clipboard(excel=True,sep=\",\")\n```", "```py\npip install geopandas \nconda install -c conda-forge geopandas \n```", "```py\n    import pandas as pd\n    import geopandas\n    import matplotlib as plt\n```", "```py\ncountries = geopandas.read_file(\"ne_110m_admin_0_countries.shp\") \n```", "```py\ncountries.head(5) \n```", "```py\ncountries.plot() \n```", "```py\ntype(countries) \n```", "```py\ncountries['POP_EST'].mean()  \n```", "```py\nafrica = countries[countries['CONTINENT'] == 'Africa'] \n```", "```py\nafrica.plot() \n```", "```py\ndf = pd.DataFrame( \n{'City': ['Buenos Aires', 'Brasilia', 'Santiago', 'Bogota', 'Caracas'], \n'Country': ['Argentina', 'Brazil', 'Chile', 'Colombia', 'Venezuela'], \n'Latitude': [-34.58, -15.78, -33.45, 4.60, 10.48], \n'Longitude': [-58.66, -47.91, -70.66, -74.08, -66.86]}) \n```", "```py\n   df['Coordinates']  = list(zip(df.Longitude, df.Latitude)) \n```", "```py\nfrom shapely.geometry import Point \n   df['Coordinates'] = df['Coordinates'].apply(Point) \n```", "```py\ngdf = geopandas.GeoDataFrame(df, geometry='Coordinates') \n```", "```py\n   type(gdf) \n```", "```py\npip install quandl\nconda install quandl\n```", "```py\n    # import quandl into your code\n    import quandl\n\n    # setting your api key\n    quandl.ApiConfig.api_key = \"[YOUR API KEY]\"\n\n    # BCCME/L6N2022 is the code for the dataset and you can see it on the right below the table\n    data = quandl.get(\"BCCME/L6N2022\", start_date=\"2015-12-31\", end_date=\"2019-06-23\")\n\n    data.head()  \n```", "```py\n    transform =  quandl.get(\"BCCME/L6N2022\", start_date=\"2015-12-31\", end_date=\"2019-06-23\",transformation='diff')\n    transform.head()\n```", "```py\n    import urllib.request\n    print('Beginning file download with urllib...')\n    url = 'https://www.quandl.com/api/v3/databases/EOD/metadata?api_key=[YOUR API KEY]'\n    urllib.request.urlretrieve(url,'file location.zip')  \n\n    # We will read the zip file contents through the zipfile package.\n    import zipfile\n    archive = zipfile.ZipFile('[Name of the zip file].zip', 'r')\n\n    # lists the contents of the zip file\n    archive.namelist()\n    ['EOD_metadata.csv']\n    df = pd.read_csv(archive.open('EOD_metadata.csv'))    \n    df.head()\n\n```", "```py\n    import pandas as pd\n    df = pd.read_csv('F:/world-happiness-report-2019.csv')\n    df.head()\n```", "```py\n    # rename Country (region) to region\n    df= df.rename(columns={'Country (region)':'region'})\n\n    # push the dataframe to postgresql using sqlalchemy\n    # Syntax:engine = db.create_engine('dialect+driver://user:pass@host:port/db')\n\n    from sqlalchemy import create_engine\n    engine = create_engine('postgresql://postgres:1128@127.0.0.1:5433/postgres')\n    df.to_sql('happy', engine,index=False)      \n```", "```py\n    import psycopg2\n    try:\n       connection = psycopg2.connect(user=\"[db-user_name]\",\n                                      password=\"[db-pwd]\",\n                                      host=\"127.0.0.1\",\n                                      port=\"5433\",\n                                      database=\"postgres\")\n\n       happy= pd.read_sql_query(\"select * from happy;\",con=connection).head()      \n```", "```py\nposgrt40 = pd.read_sql_query('select * from happy where \"Positive affect\" > 40;',con=connection).head()\n\n```", "```py\nexcept (Exception, psycopg2.Error) as error :\nprint (\"Error while fetching data from PostgreSQL\", error)  \n```", "```py\nimport pandas as pd \nimport numpy as np \n```", "```py\nmtcars = pd.DataFrame({ \n        'mpg':[21,21,22.8,21.4,18.7,18.1,18.3,24.4,22.8,19.2], \n        'cyl':[6,6,4,6,8,6,8,4,4,4], \n        'disp':[160,160,108,258,360,225,360,146.7,140.8,167.7], \n  'hp':[110,110,93,110,175,105,245,62,95,123],    \n'category':['SUV','Sedan','Sedan','Hatchback','SUV','Sedan','SUV','Hatchback','SUV','Sedan'] \n        }) \nmtcars \n```", "```py\nandrew = pd.plotting.andrews_curves(mtcars,'Category') \n```", "```py\nparallel = pd.plotting.parallel_coordinates(mtcars,'Category') \n```", "```py\nrad_viz = pd.plotting.radviz(mtcars, 'Category') \n```", "```py\nscatter = pd.plotting.scatter_matrix(mtcars,alpha = 0.5) \n```", "```py\ns = pd.Series(np.random.uniform(size=100)) \n```", "```py\nfig = pd.plotting.bootstrap_plot(s) \n```", "```py\npip install pandas-datareader  \n```", "```py\nconda config --add channels conda-forge  \n```", "```py\nconda install pandas-datareader  \n```", "```py\n    import pandas as pd\n    from pandas_datareader import data\n    symbols=['AAPL','GOOGL','FB','TWTR']\n    # initializing a dataframe\n    get_data = pd.DataFrame()\n    stock = pd.DataFrame()\n\n    for ticker in symbols:\n       get_data = get_data.append(data.DataReader(ticker, \n                           start='2015-1-1', \n                           end='2019-6-23', data_source='yahoo'))\n\n       for line in get_data:\n            get_data['symbol'] = ticker\n\n       stock = stock.append(get_data)\n       get_data = pd.DataFrame()\n\n    stock.head()\n\n```", "```py\nstock.describe() \n```", "```py\n # get the list of column names\n    cols = [ col for col in stock.columns]\n    cols                      \n```", "```py\n # returns the symbol of the highest traded value among the symbols\n stock.loc[stock['High']==stock['High'].max(), 'High']\n```", "```py\nfrom pandas_datareader import wb  \n```", "```py\ndat = wb.download(indicator='FP.CPI.TOTL.ZG', start=2005, end=2019)  \n```", "```py\ndat.head()  \n```", "```py\n    dat.loc['Canada']\n\n```", "```py\n    dat.loc['Canada'].loc['2015']\n\n```"]