<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 8. Machine Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Machine Learning</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Getting started with scikit-learn</li><li class="listitem" style="list-style-type: disc">Predicting who will survive on the Titanic with logistic regression</li><li class="listitem" style="list-style-type: disc">Learning to recognize handwritten digits with a K-nearest neighbors classifier</li><li class="listitem" style="list-style-type: disc">Learning from text – Naive Bayes for Natural Language Processing</li><li class="listitem" style="list-style-type: disc">Using support vector machines for classification tasks</li><li class="listitem" style="list-style-type: disc">Using a random forest to select important features for regression</li><li class="listitem" style="list-style-type: disc">Reducing the dimensionality of a dataset with a Principal Component Analysis</li><li class="listitem" style="list-style-type: disc">Detecting hidden structures in a dataset with clustering</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec74"/>Introduction</h1></div></div></div><p>In the previous chapter, we were interested in getting insight into data, understanding complex phenomena through partial observations, and making informed decisions in the presence of uncertainty. Here, we are still interested in analyzing and processing data using statistical tools. However, the goal is not necessarily to <span class="emphasis"><em>understand</em></span> the data, but to <span class="emphasis"><em>learn</em></span> from it.</p><p>Learning from data is close to what we do as humans. From our experience, we intuitively learn general facts and relations about the world, even if we don't fully understand their complexity. The increasing computational power of computers makes them able to learn from data too. That's the heart of <a id="id1204" class="indexterm"/><span class="strong"><strong>machine learning</strong></span>, a modern and fascinating branch of artificial intelligence, computer science, statistics, and applied mathematics. For more information on<a id="id1205" class="indexterm"/> machine learning, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Machine_learning">http://en.wikipedia.org/wiki/Machine_learning</a>.</p><p>This chapter is a hands-on introduction to some of the most basic methods in machine learning. These methods are routinely used by data scientists. We will use these methods with <span class="strong"><strong>scikit-learn</strong></span>, a popular and <a id="id1206" class="indexterm"/>user-friendly Python package for machine learning.</p><div class="section" title="A bit of vocabulary"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec272"/>A bit of vocabulary</h2></div></div></div><p>In this introduction, we will explain the fundamental definitions and concepts of machine learning.</p><div class="section" title="Learning from data"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec52"/>Learning from data</h3></div></div></div><p>In machine learning, most data can be represented as a table of numerical values. Every row is called an <a id="id1207" class="indexterm"/><span class="strong"><strong>observation</strong></span>, a <a id="id1208" class="indexterm"/><span class="strong"><strong>sample</strong></span>, or a <a id="id1209" class="indexterm"/><span class="strong"><strong>data point</strong></span>. Every column is called a <span class="strong"><strong>feature</strong></span><a id="id1210" class="indexterm"/> or a <a id="id1211" class="indexterm"/><span class="strong"><strong>variable</strong></span>.</p><p>Let's call <span class="emphasis"><em>N</em></span> the number of rows (or the number of points) and <span class="emphasis"><em>D</em></span> the number of columns (or number of features). The number <span class="emphasis"><em>D</em></span> is also called the <span class="strong"><strong>dimensionality</strong></span><a id="id1212" class="indexterm"/> of the data. The reason is that we can view this table as a set <span class="emphasis"><em>E </em></span>of <span class="strong"><strong>vectors</strong></span><a id="id1213" class="indexterm"/> in a space with <span class="emphasis"><em>D</em></span> dimensions (or <a id="id1214" class="indexterm"/><span class="strong"><strong>vector space</strong></span>). Here, a vector <span class="strong"><strong>x</strong></span> contains <span class="emphasis"><em>D</em></span> numbers <span class="emphasis"><em>(x<sub>1</sub>, ..., x<sub>D</sub>)</em></span>, also called<a id="id1215" class="indexterm"/> <span class="strong"><strong>components</strong></span>. This mathematical point of view is very useful and we will use it throughout this chapter.</p><p>We generally make the distinction between supervised learning and unsupervised learning:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Supervised learning</strong></span> is when<a id="id1216" class="indexterm"/> we have a <span class="strong"><strong>label</strong></span> <span class="emphasis"><em>y</em></span> associated with a data point <span class="emphasis"><em>x</em></span>. The goal is to <span class="emphasis"><em>learn </em></span>the mapping from <span class="emphasis"><em>x</em></span> to <span class="emphasis"><em>y</em></span> from our data. The data gives us this mapping for a finite set of points, but what we want is to <span class="emphasis"><em>generalize </em></span>this mapping to the full set <span class="emphasis"><em>E</em></span>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Unsupervised learning</strong></span> is <a id="id1217" class="indexterm"/>when we don't have any labels. What we want to do is discover some form of hidden structure in the data.</li></ul></div></div><div class="section" title="Supervised learning"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec53"/>Supervised learning</h3></div></div></div><p>Mathematically, supervised <a id="id1218" class="indexterm"/>learning consists of finding a function <span class="emphasis"><em>f</em></span> that maps the set of points <span class="emphasis"><em>E</em></span> to a set of labels <span class="emphasis"><em>F</em></span>, knowing a finite set of associations <span class="emphasis"><em>(x, y)</em></span>, which is given by our data. This is what <span class="emphasis"><em>generalization </em></span>is about: after observing the pairs <span class="emphasis"><em>(x<sub>i</sub>, y<sub>i</sub>)</em></span>, given a new <span class="emphasis"><em>x</em></span>, we are able to find the corresponding <span class="emphasis"><em>y</em></span> by applying the function <span class="emphasis"><em>f</em></span> to <span class="emphasis"><em>x</em></span>. For more information on <a id="id1219" class="indexterm"/>supervised learning, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Supervised_learning">http://en.wikipedia.org/wiki/Supervised_learning</a>.</p><p>It is a common practice to split the set of data points into two subsets: the <span class="strong"><strong>training set</strong></span><a id="id1220" class="indexterm"/> and the <a id="id1221" class="indexterm"/><span class="strong"><strong>test set</strong></span>. We learn the function <span class="emphasis"><em>f</em></span> on the training set and test it on the test set. This is essential when assessing the predictive power of a model. By training and testing a model on the same set, our model might not be able to generalize well. This is the fundamental concept of<a id="id1222" class="indexterm"/> <span class="strong"><strong>overfitting</strong></span>, which we will detail later in this chapter.</p><p>We generally make the distinction between classification and regression, two particular instances of supervised learning.</p><p>
<span class="strong"><strong>Classification</strong></span><a id="id1223" class="indexterm"/> is when <a id="id1224" class="indexterm"/>our labels <span class="emphasis"><em>y</em></span> can only take a finite set of values (categories). Examples include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Handwritten digit recognition</strong></span>: <span class="emphasis"><em>x</em></span> is an <a id="id1225" class="indexterm"/>image <a id="id1226" class="indexterm"/>with a handwritten digit; <span class="emphasis"><em>y</em></span> is a digit between 0 and 9</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spam filtering</strong></span>: <span class="emphasis"><em>x</em></span> is an <a id="id1227" class="indexterm"/>e-mail and <span class="emphasis"><em>y</em></span> is 1 or 0, depending on <a id="id1228" class="indexterm"/>whether that e-mail is spam or not</li></ul></div><p>
<span class="strong"><strong>Regression</strong></span><a id="id1229" class="indexterm"/> is when our labels <span class="emphasis"><em>y</em></span> can take any real (continuous) value. Examples include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Predicting <a id="id1230" class="indexterm"/>stock market data</li><li class="listitem" style="list-style-type: disc">Predicting sales</li><li class="listitem" style="list-style-type: disc">Detecting the age of a person from a picture</li></ul></div><p>A classification task yields a division of our space <span class="emphasis"><em>E</em></span> in different regions (also called <a id="id1231" class="indexterm"/><span class="strong"><strong>partition</strong></span>), each region being associated to one particular value of the label <span class="emphasis"><em>y</em></span>. A regression task yields a mathematical model that associates a real number to any point <span class="emphasis"><em>x</em></span> in the space <span class="emphasis"><em>E</em></span>. This difference is illustrated in the following figure:</p><div class="mediaobject"><img src="images/4818OS_08_01.jpg" alt="Supervised learning"/><div class="caption"><p>Difference between classification and regression</p></div></div><p>Classification <a id="id1232" class="indexterm"/>and regression <a id="id1233" class="indexterm"/>can be combined. For example, in the <a id="id1234" class="indexterm"/><span class="strong"><strong>probit model</strong></span>, although the dependent variable is binary (classification), the <span class="emphasis"><em>probability</em></span> that this variable belongs to one category can also be modeled (regression). We will see an example in the recipe about logistic regression. For more information on the <a id="id1235" class="indexterm"/>probit model, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Probit_model">http://en.wikipedia.org/wiki/Probit_model</a>.</p></div><div class="section" title="Unsupervised learning"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec54"/>Unsupervised learning</h3></div></div></div><p>Broadly speaking, unsupervised learning<a id="id1236" class="indexterm"/> helps us discover systemic structures in our data. This is harder to grasp than supervised learning, in that there is generally no precise question and answer. For more information on <a id="id1237" class="indexterm"/>unsupervised learning, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Unsupervised_learning">http://en.wikipedia.org/wiki/Unsupervised_learning</a>.</p><p>Here are a few important terms related to unsupervised learning:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Clustering</strong></span>: Grouping<a id="id1238" class="indexterm"/> similar points together <a id="id1239" class="indexterm"/>within <a id="id1240" class="indexterm"/><span class="strong"><strong>clusters</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Density estimation</strong></span>: Estimating a<a id="id1241" class="indexterm"/> probability density function that can explain the distribution of the data points</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Dimension reduction</strong></span>: Getting a<a id="id1242" class="indexterm"/> simple representation of high-dimensional data points by projecting them onto a lower-dimensional space (notably for<a id="id1243" class="indexterm"/> <span class="strong"><strong>data visualization</strong></span>)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Manifold learning</strong></span>: Finding a <a id="id1244" class="indexterm"/>low-dimensional manifold containing the data points (also known as <span class="strong"><strong>nonlinear dimension reduction</strong></span>)</li></ul></div></div><div class="section" title="Feature selection and feature extraction"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec55"/>Feature selection and feature extraction</h3></div></div></div><p>In a supervised learning context, when our data contains many features, it is sometimes necessary to choose a subset of them. The features we want to keep are those that are most relevant to our question. This is the problem of <a id="id1245" class="indexterm"/><span class="strong"><strong>feature selection</strong></span>.</p><p>Additionally, we might want to extract new features by applying complex transformations on our original dataset. This is <a id="id1246" class="indexterm"/><span class="strong"><strong>feature extraction</strong></span>. For example, in computer vision, training a classifier directly on the pixels is not the most efficient method in general. We might want to extract the relevant points of interest or make appropriate mathematical transformations. These steps depend on our dataset and on the questions we want to answer.</p><p>For example, it is often necessary to preprocess the data before learning models. <span class="strong"><strong>Feature scaling</strong></span> (or <span class="strong"><strong>data normalization</strong></span>) is a <a id="id1247" class="indexterm"/>common <span class="strong"><strong>preprocessing</strong></span><a id="id1248" class="indexterm"/> step where features are linearly rescaled to fit in the range <span class="emphasis"><em>[-1,1]</em></span> or <span class="emphasis"><em>[0,1]</em></span>.</p><p>Feature extraction and feature selection involve a balanced combination of domain expertise, intuition, and mathematical methods. These early steps are crucial, and they might be even more important than the learning steps themselves. The reason is that the few dimensions that are relevant to our problem are generally hidden in the high dimensionality of our dataset. We need to uncover the low-dimensional structure of interest to improve the efficiency of the learning models.</p><p>We will see a few feature selection and feature extraction methods in this chapter. Methods that are specific to signals, images, or sounds will be covered in <a class="link" href="ch10.html" title="Chapter 10. Signal Processing">Chapter 10</a>, <span class="emphasis"><em>Signal Processing</em></span>, and <a class="link" href="ch11.html" title="Chapter 11. Image and Audio Processing">Chapter 11</a>, <span class="emphasis"><em>Image and Audio Processing</em></span>.</p><p>Here are a few further references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Feature selection<a id="id1249" class="indexterm"/> in scikit-learn, documented at <a class="ulink" href="http://scikit-learn.org/stable/modules/feature_selection.html">http://scikit-learn.org/stable/modules/feature_selection.html</a></li><li class="listitem" style="list-style-type: disc">Feature selection on Wikipedia at <a class="ulink" href="http://en.wikipedia.org/wiki/Feature_selection">http://en.wikipedia.org/wiki/Feature_selection</a></li></ul></div></div><div class="section" title="Overfitting, underfitting, and the bias-variance tradeoff"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec56"/>Overfitting, underfitting, and the bias-variance tradeoff</h3></div></div></div><p>A central notion in machine learning is the trade-off between <span class="strong"><strong>overfitting</strong></span><a id="id1250" class="indexterm"/> and <a id="id1251" class="indexterm"/><span class="strong"><strong>underfitting</strong></span>. A model may be able to represent our data accurately. However, if it is <span class="emphasis"><em>too</em></span> accurate, it might not generalize well to unobserved data. For example, in facial recognition, a too-accurate model would be unable to identify someone who styled their hair differently that day. The reason is that our model might learn irrelevant features in the training data. On the contrary, an insufficiently trained model would not generalize well either. For example, it would be unable to correctly recognize twins. For more information on <a id="id1252" class="indexterm"/>overfitting, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Overfitting">http://en.wikipedia.org/wiki/Overfitting</a>.</p><p>A popular solution to reduce overfitting consists of adding <span class="emphasis"><em>structure</em></span> to the model, for example, with <a id="id1253" class="indexterm"/><span class="strong"><strong>regularization</strong></span>. This method favors simpler models during training (Occam's razor). You will find more information at <a class="ulink" href="http://en.wikipedia.org/wiki/Regularization_%28mathematics%29">http://en.wikipedia.org/wiki/Regularization_%28mathematics%29</a>.</p><p>The <span class="strong"><strong>bias-variance dilemma</strong></span><a id="id1254" class="indexterm"/> is closely related to the issue of overfitting and underfitting. The <span class="strong"><strong>bias</strong></span><a id="id1255" class="indexterm"/> of a model quantifies how precise it is across training sets. The <span class="strong"><strong>variance</strong></span><a id="id1256" class="indexterm"/> quantifies how sensitive the model is to small changes in the training set. A <span class="strong"><strong>robust</strong></span> model<a id="id1257" class="indexterm"/> is not overly sensitive to small changes. The dilemma involves minimizing both bias and variance; we want a precise and robust model. Simpler models tend to be less accurate but more robust. Complex models tend to be more accurate but less robust. For more information on the<a id="id1258" class="indexterm"/> bias-variance dilemma, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Bias-variance_dilemma">http://en.wikipedia.org/wiki/Bias-variance_dilemma</a>.</p><p>The importance of this trade-off cannot be overstated. This question pervades the entire discipline of machine learning. We will see concrete examples in this chapter.</p></div><div class="section" title="Model selection"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec57"/>Model selection</h3></div></div></div><p>As we will see in this chapter, there are many supervised and unsupervised algorithms. For example, well-known classifiers that we will cover in this chapter include logistic regression, nearest-neighbors, Naive Bayes, and support vector machines. There are many other algorithms that we can't cover here.</p><p>No model performs uniformly better than the others. One model may perform well on one dataset and badly on another. This is the question of <a id="id1259" class="indexterm"/><span class="strong"><strong>model selection</strong></span>.</p><p>We will see systematic methods to assess the quality of a model on a particular dataset (notably cross-validation). In practice, machine learning is not an "exact science" in that it frequently involves trial and error. We need to try different models and empirically choose the one that performs best.</p><p>That being said, understanding the details of the learning models allows us to gain intuition about which model is best adapted to our current problem.</p><p>Here are a few references<a id="id1260" class="indexterm"/> on this question:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Model selection on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Model_selection">http://en.wikipedia.org/wiki/Model_selection</a></li><li class="listitem" style="list-style-type: disc">Model evaluation in scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/model_evaluation.html">http://scikit-learn.org/stable/modules/model_evaluation.html</a></li><li class="listitem" style="list-style-type: disc">Blog post on how to choose a classifier, available at <a class="ulink" href="http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/">http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/</a></li></ul></div></div><div class="section" title="Machine learning references"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec58"/>Machine learning references</h3></div></div></div><p>Here are a few excellent, math-heavy textbooks on<a id="id1261" class="indexterm"/> machine learning:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Pattern Recognition and Machine Learning</em></span>, <span class="emphasis"><em>Christopher M. Bishop</em></span>, <span class="emphasis"><em>(2006)</em></span>, <span class="emphasis"><em>Springer</em></span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Machine Learning – A Probabilistic Perspective</em></span>, <span class="emphasis"><em>Kevin P. Murphy</em></span>, <span class="emphasis"><em>(2012)</em></span>, <span class="emphasis"><em>MIT Press</em></span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>The Elements of Statistical Learning</em></span>, <span class="emphasis"><em>Trevor Hastie</em></span>, <span class="emphasis"><em>Robert Tibshirani</em></span>, <span class="emphasis"><em>Jerome Friedman</em></span>, <span class="emphasis"><em>(2009)</em></span>, <span class="emphasis"><em>Springer</em></span></li></ul></div><p>Here are a few books more oriented toward programmers without a strong mathematical background:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Machine Learning for Hackers</em></span>, <span class="emphasis"><em>Drew Conway</em></span>, <span class="emphasis"><em>John Myles White</em></span>, <span class="emphasis"><em>(2012)</em></span>, <span class="emphasis"><em>O'Reilly Media</em></span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Machine Learning in Action</em></span>, <span class="emphasis"><em>Peter Harrington</em></span>, (2012), <span class="emphasis"><em>Manning Publications Co.</em></span></li></ul></div><p>You will find many other references online.</p><p>Important classes of machine learning methods that we couldn't cover in this chapter include neural networks and deep learning. Deep learning is the subject of very active research in machine learning. Many state-of-the-art results are currently achieved by using deep learning methods. For more information on <a id="id1262" class="indexterm"/>deep learning, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Deep_learning">http://en.wikipedia.org/wiki/Deep_learning</a>.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Getting started with scikit-learn"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec75"/>Getting started with scikit-learn</h1></div></div></div><p>In this recipe, we introduce the basics of the machine learning <a id="id1263" class="indexterm"/><span class="strong"><strong>scikit-learn</strong></span> package (<a class="ulink" href="http://scikit-learn.org">http://scikit-learn.org</a>). This package is the main tool we will use throughout this chapter. Its clean API makes it really easy to define, train, and test models. Plus, scikit-learn is specifically designed for speed and (relatively) big data.</p><p>We will show here a very basic <a id="id1264" class="indexterm"/>example of linear regression in the context of curve fitting. This toy example will allow us to illustrate key concepts such as linear models, overfitting, underfitting, regularization, and cross-validation.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec273"/>Getting ready</h2></div></div></div><p>You can find all instructions to install scikit-learn<a id="id1265" class="indexterm"/> in the main documentation. For more information, refer to <a class="ulink" href="http://scikit-learn.org/stable/install.html">http://scikit-learn.org/stable/install.html</a>. With anaconda, you can type <code class="literal">conda install scikit-learn</code> in a terminal.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec274"/>How to do it...</h2></div></div></div><p>We will generate a one-dimensional dataset with a simple model (including some noise), and we will try to fit a function to this data. With this function, we can predict values on new data points. This is a <a id="id1266" class="indexterm"/><span class="strong"><strong>curve fitting regression</strong></span> problem.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, let's make all the <a id="id1267" class="indexterm"/>necessary imports:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import scipy.stats as st
        import sklearn.linear_model as lm
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">We now define a deterministic nonlinear function underlying our generative model:<div class="informalexample"><pre class="programlisting">In [2]: f = lambda x: np.exp(3 * x)</pre></div></li><li class="listitem">We generate the values along the curve on <span class="emphasis"><em>[0,2]</em></span>:<div class="informalexample"><pre class="programlisting">In [3]: x_tr = np.linspace(0., 2, 200)
        y_tr = f(x_tr)</pre></div></li><li class="listitem">Now, let's generate data points within <span class="emphasis"><em>[0,1]</em></span>. We use the function <span class="emphasis"><em>f</em></span> and we add some Gaussian noise:<div class="informalexample"><pre class="programlisting">In [4]: x = np.array([0, .1, .2, .5, .8, .9, 1])
        y = f(x) + np.random.randn(len(x))</pre></div></li><li class="listitem">Let's plot our data points on <span class="emphasis"><em>[0,1]</em></span>:<div class="informalexample"><pre class="programlisting">In [5]: plt.plot(x_tr[:100], y_tr[:100], '--k')
        plt.plot(x, y, 'ok', ms=10)</pre></div><div class="mediaobject"><img src="images/4818OS_08_02.jpg" alt="How to do it..."/></div><p>In the image, the<a id="id1268" class="indexterm"/> dotted curve represents the generative model.</p></li><li class="listitem">Now, we use scikit-learn to fit a linear model to the data. There are three steps. First, we create the model (an instance of the <code class="literal">LinearRegression</code> class). Then, we fit the model to our data. Finally, we predict values from our trained model.<div class="informalexample"><pre class="programlisting">In [6]: # We create the model.
        lr = lm.LinearRegression()
        # We train the model on our training dataset.
        lr.fit(x[:, np.newaxis], y)
        # Now, we predict points with our trained model.
        y_lr = lr.predict(x_tr[:, np.newaxis])</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note28"/>Note</h3><p>We need to convert <code class="literal">x</code> and <code class="literal">x_tr</code> to column vectors, as it is a general convention in scikit-learn that observations are rows, while features are columns. Here, we have seven observations with one feature.</p></div></div></li><li class="listitem">We now plot the result of the trained linear model. We obtain a regression line in green here:<div class="informalexample"><pre class="programlisting">In [7]: plt.plot(x_tr, y_tr, '--k')
        plt.plot(x_tr, y_lr, 'g')
        plt.plot(x, y, 'ok', ms=10)
        plt.xlim(0, 1)
        plt.ylim(y.min()-1, y.max()+1)
        plt.title("Linear regression")</pre></div><div class="mediaobject"><img src="images/4818OS_08_03.jpg" alt="How to do it..."/></div></li><li class="listitem">The linear fit is not<a id="id1269" class="indexterm"/> well-adapted here, as the data points are generated according to a nonlinear model (an exponential curve). Therefore, we are now going to fit a nonlinear model. More precisely, we will fit a polynomial function to our data points. We can still use linear regression for this, by precomputing the exponents of our data points. This is done by generating a <a id="id1270" class="indexterm"/><span class="strong"><strong>Vandermonde matrix</strong></span>, using the <code class="literal">np.vander</code> function. We will explain this trick in <span class="emphasis"><em>How it works…</em></span>. In the following code, we perform and plot the fit:<div class="informalexample"><pre class="programlisting">In [8]: lrp = lm.LinearRegression()
        plt.plot(x_tr, y_tr, '--k')
        for deg in [2, 5]:
            lrp.fit(np.vander(x, deg + 1), y)
            y_lrp = lrp.predict(np.vander(x_tr, deg + 1))
            plt.plot(x_tr, y_lrp,
                     label='degree ' + str(deg))
            plt.legend(loc=2)
            plt.xlim(0, 1.4)
            plt.ylim(-10, 40)
            # Print the model's coefficients.
            print(' '.join(['%.2f' % c for c in 
                            lrp.coef_]))
        plt.plot(x, y, 'ok', ms=10)
        plt.title("Linear regression")
25.00 -8.57 0.00
-132.71 296.80 -211.76 72.80 -8.68 0.00</pre></div><div class="mediaobject"><img src="images/4818OS_08_04.jpg" alt="How to do it..."/></div><p>We have fitted <a id="id1271" class="indexterm"/>two polynomial models of degree 2 and 5. The degree 2 polynomial appears to fit the data points less precisely than the degree 5 polynomial. However, it seems more robust; the degree 5 polynomial seems really bad at predicting values outside the data points (look for example at the <span class="emphasis"><em>x </em></span><span class="inlinemediaobject"><img src="images/4818OS_08_41.jpg" alt="How to do it..."/></span>
<span class="emphasis"><em> 1</em></span> portion). This is what we call overfitting; by using a too-complex model, we obtain a better fit on the trained dataset, but a less robust model outside this set.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note29"/>Note</h3><p>Note the large coefficients of the degree 5 polynomial; this is generally a sign of overfitting.</p></div></div></li><li class="listitem">We will now use a different learning model called<a id="id1272" class="indexterm"/> <span class="strong"><strong>ridge regression</strong></span>. It works like linear regression except that it prevents the polynomial's coefficients from becoming too big. This is what happened in the previous example. By adding a <span class="strong"><strong>regularization</strong></span><a id="id1273" class="indexterm"/><span class="strong"><strong> term</strong></span> in the <a id="id1274" class="indexterm"/><span class="strong"><strong>loss function</strong></span>, ridge regression imposes some structure on the underlying model. We will see more details in the next section.<p>The ridge regression model<a id="id1275" class="indexterm"/> has a meta-parameter, which represents the weight of the regularization term. We could try different values with trial and error using the <code class="literal">Ridge</code> class. However, scikit-learn provides another model called <code class="literal">RidgeCV</code>, which includes a parameter search with<a id="id1276" class="indexterm"/> <span class="strong"><strong>cross-validation</strong></span>. In practice, this means <a id="id1277" class="indexterm"/>that we don't have to tweak this parameter by hand—scikit-learn does it for us. As the models of scikit-learn always follow the fit-predict API, all we have to do is replace <code class="literal">lm.LinearRegression()</code> with <code class="literal">lm.RidgeCV()</code> in the previous code. We will give more details in the next section.</p><div class="informalexample"><pre class="programlisting">In [9]: ridge = lm.RidgeCV()
        plt.plot(x_tr, y_tr, '--k')
        
        for deg in [2, 5]:
            ridge.fit(np.vander(x, deg + 1), y);
            y_ridge = ridge.predict(np.vander(x_tr, deg+1))
            plt.plot(x_tr, y_ridge,
                     label='degree ' + str(deg))
            plt.legend(loc=2)
            plt.xlim(0, 1.5)
            plt.ylim(-5, 80)
            # Print the model's coefficients.
            print(' '.join(['%.2f' % c 
                            for c in ridge.coef_]))
        
        plt.plot(x, y, 'ok', ms=10)
        plt.title("Ridge regression")
11.36 4.61 0.00
2.84 3.54 4.09 4.14 2.67 0.00</pre></div><div class="mediaobject"><img src="images/4818OS_08_05.jpg" alt="How to do it..."/></div><p>This time, the degree 5 polynomial seems more precise than the simpler degree 2 polynomial (which now causes <span class="strong"><strong>underfitting</strong></span>). Ridge regression mitigates the overfitting issue here. Observe how the degree 5 polynomial's coefficients are much smaller than in the previous example.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec275"/>How it works...</h2></div></div></div><p>In this section, we explain all the aspects covered in this recipe.</p><div class="section" title="The scikit-learn API"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec59"/>The scikit-learn API</h3></div></div></div><p>scikit-learn implements a clean and coherent <a id="id1278" class="indexterm"/>API for supervised and unsupervised learning. Our data points should be stored in a <span class="emphasis"><em>(N,D)</em></span> matrix <span class="emphasis"><em>X</em></span>, where <span class="emphasis"><em>N</em></span> is the number of observations and <span class="emphasis"><em>D</em></span> is the number of features. In other words, each row is an observation. The first step in a machine learning task is to define what the matrix <span class="emphasis"><em>X</em></span> is exactly.</p><p>In a supervised learning setup, we also have a <span class="emphasis"><em>target</em></span>, an <span class="emphasis"><em>N</em></span>-long vector <span class="emphasis"><em>y</em></span> with a scalar value for each observation. This value is either continuous or discrete, depending on whether we have a regression or classification problem, respectively.</p><p>In scikit-learn, models are implemented in classes that have the <code class="literal">fit()</code> and <code class="literal">predict() </code>methods. The <code class="literal">fit()</code> method<a id="id1279" class="indexterm"/> accepts the data matrix <span class="emphasis"><em>X</em></span> as input, and <span class="emphasis"><em>y</em></span> as well for supervised learning models. This method <span class="emphasis"><em>trains</em></span> the model on the given data.</p><p>The <code class="literal">predict()</code> method<a id="id1280" class="indexterm"/> also takes data points as input (as a <span class="emphasis"><em>(M,D)</em></span> matrix). It returns the labels or transformed points as predicted by the trained model.</p></div><div class="section" title="Ordinary least squares regression"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec60"/>Ordinary least squares regression</h3></div></div></div><p>
<span class="strong"><strong>Ordinary least squares regression</strong></span><a id="id1281" class="indexterm"/> is one of the simplest regression methods. It consists of approaching the output values <span class="emphasis"><em>y<sub>i</sub></em></span> with a linear combination of <span class="emphasis"><em>X<sub>ij</sub></em></span>:</p><div class="mediaobject"><img src="images/4818OS_08_06.jpg" alt="Ordinary least squares regression"/></div><p>Here, <span class="emphasis"><em>w = (w<sub>1</sub>, ..., w<sub>D</sub>)</em></span> is the (unknown) <a id="id1282" class="indexterm"/><span class="strong"><strong>parameter vector</strong></span>. Also, <span class="inlinemediaobject"><img src="images/4818OS_08_33.jpg" alt="Ordinary least squares regression"/></span> represents the model's output. We want this vector to match the data points <span class="emphasis"><em>y</em></span> as closely as possible. Of course, the exact equality <span class="inlinemediaobject"><img src="images/4818OS_08_34.jpg" alt="Ordinary least squares regression"/></span> cannot hold in general (there is always some noise and uncertainty—models are always idealizations of reality). Therefore, we want to <span class="emphasis"><em>minimize </em></span>the difference between these two vectors. The ordinary least squares regression method consists of minimizing the<a id="id1283" class="indexterm"/> following <span class="strong"><strong>loss function</strong></span>:</p><div class="mediaobject"><img src="images/4818OS_08_07.jpg" alt="Ordinary least squares regression"/></div><p>This sum of the components squared is called the<a id="id1284" class="indexterm"/> <span class="strong"><strong>L<sup>2</sup></strong></span><span class="strong"><strong> norm</strong></span>. It is convenient because it leads to <span class="emphasis"><em>differentiable</em></span> loss functions so that gradients can be computed and common optimization procedures can be performed.</p></div><div class="section" title="Polynomial interpolation with linear regression"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec61"/>Polynomial interpolation with linear regression</h3></div></div></div><p>Ordinary least squares<a id="id1285" class="indexterm"/> regression fits a linear model to the data. The model is linear both in the data points <span class="emphasis"><em>x<sub>i</sub></em></span> and in the parameters <span class="emphasis"><em>w<sub>j</sub></em></span>. In our example, we obtain a poor fit because the data points were generated according to a nonlinear generative model (an exponential function).</p><p>However, we can still use the linear regression method with a model that is linear in <span class="emphasis"><em>w<sub>j</sub></em></span> but nonlinear in <span class="emphasis"><em>x<sub>i</sub></em></span>. To do this, we need to increase the number of dimensions in our dataset by using a basis of polynomial functions. In other words, we consider the following data points:</p><div class="mediaobject"><img src="images/4818OS_08_08.jpg" alt="Polynomial interpolation with linear regression"/></div><p>Here, <span class="emphasis"><em>D</em></span> is the maximum degree. The <a id="id1286" class="indexterm"/>input matrix <span class="emphasis"><em>X</em></span> is therefore the <span class="strong"><strong>Vandermonde matrix</strong></span><a id="id1287" class="indexterm"/> associated to the original data points <span class="emphasis"><em>x<sub>i</sub></em></span>. For more information on the<a id="id1288" class="indexterm"/> Vandermonde matrix, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Vandermonde_matrix">http://en.wikipedia.org/wiki/Vandermonde_matrix</a>.</p><p>Here, it is easy to see that training a linear model on these new data points is equivalent to training a polynomial model on the original data points.</p></div><div class="section" title="Ridge regression"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec62"/>Ridge regression</h3></div></div></div><p>Polynomial interpolation with linear regression can lead to overfitting if the degree of the polynomials is too large. By capturing the random fluctuations (noise) instead of the general trend of the data, the model loses some of its predictive power. This corresponds to a divergence of the polynomial's coefficients <span class="emphasis"><em>w<sub>j</sub></em></span>.</p><p>A solution to this problem is to prevent these coefficients from growing unboundedly. With <a id="id1289" class="indexterm"/><span class="strong"><strong>ridge regression</strong></span> (also known as <span class="strong"><strong>Tikhonov regularization</strong></span>), this is done by adding a <span class="emphasis"><em>regularization</em></span> term to the loss function. For more details on Tikhonov regularization, refer <a id="id1290" class="indexterm"/>to <a class="ulink" href="http://en.wikipedia.org/wiki/Tikhonov_regularization">http://en.wikipedia.org/wiki/Tikhonov_regularization</a>.</p><div class="mediaobject"><img src="images/4818OS_08_09.jpg" alt="Ridge regression"/></div><p>By minimizing this loss function, we not only minimize the error between the model and the data (first term, related to the bias), but also the size of the model's coefficients (second term, related to the variance). The bias-variance trade-off is quantified by the hyperparameter <span class="inlinemediaobject"><img src="images/4818OS_08_43.jpg" alt="Ridge regression"/></span>, which specifies the relative weight between the two terms in the loss function.</p><p>Here, ridge regression led to a polynomial with smaller coefficients, and thus a better fit.</p></div><div class="section" title="Cross-validation and grid search"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec63"/>Cross-validation and grid search</h3></div></div></div><p>A drawback of the ridge regression model<a id="id1291" class="indexterm"/> compared to the ordinary least squares model is the presence of an extra hyperparameter <span class="inlinemediaobject"><img src="images/4818OS_08_43.jpg" alt="Cross-validation and grid search"/></span>. The quality of the prediction depends on the choice of this parameter. One possibility would be to fine-tune this parameter manually, but this procedure can be tedious and can also lead to overfitting problems.</p><p>To solve this problem, we can use a <span class="strong"><strong>grid search</strong></span><a id="id1292" class="indexterm"/>; we loop over many possible values for <span class="inlinemediaobject"><img src="images/4818OS_08_43.jpg" alt="Cross-validation and grid search"/></span>, and we evaluate the performance of the model for each possible value. Then, we choose the parameter that yields the best performance.</p><p>How can we assess the performance of a model with a given <span class="inlinemediaobject"><img src="images/4818OS_08_43.jpg" alt="Cross-validation and grid search"/></span> value? A common solution is to use <a id="id1293" class="indexterm"/><span class="strong"><strong>cross-validation</strong></span>. This procedure consists of splitting the dataset into a training set and a test set. We fit the model on the train set, and we test its predictive performance on the <span class="emphasis"><em>test set</em></span>. By testing the model on a different dataset than the one used for training, we reduce overfitting.</p><p>There are many ways to split the initial dataset into two parts like this. One possibility is to remove <span class="emphasis"><em>one </em></span>sample to form the train set and to put this one sample into the test set. This is called <a id="id1294" class="indexterm"/><span class="strong"><strong>Leave-One-Out</strong></span> cross-validation. With <span class="emphasis"><em>N</em></span> samples, we obtain <span class="emphasis"><em>N</em></span> sets of train and test sets. The cross-validated performance is the average performance on all these set decompositions.</p><p>As we will see later, scikit-learn implements several easy-to-use functions to do cross-validation and grid search. In this recipe, there exists a special estimator called <code class="literal">RidgeCV</code> that implements a cross-validation and grid search procedure that is specific to the ridge regression model. Using this class ensures that the best hyperparameter <span class="inlinemediaobject"><img src="images/4818OS_08_43.jpg" alt="Cross-validation and grid search"/></span> is found automatically for us.</p></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec276"/>There's more…</h2></div></div></div><p>Here are a few references about <a id="id1295" class="indexterm"/>least squares:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ordinary least squares on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Ordinary_least_squares">http://en.wikipedia.org/wiki/Ordinary_least_squares</a></li><li class="listitem" style="list-style-type: disc">Linear least squares on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)">http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)</a></li></ul></div><p>Here are a few references about cross-validation and grid search:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Cross-validation in <a id="id1296" class="indexterm"/>scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/cross_validation.html">http://scikit-learn.org/stable/modules/cross_validation.html</a></li><li class="listitem" style="list-style-type: disc">Grid search in<a id="id1297" class="indexterm"/> scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/grid_search.html">http://scikit-learn.org/stable/modules/grid_search.html</a></li><li class="listitem" style="list-style-type: disc">Cross-validation on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29">http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29</a></li></ul></div><p>Here are a few references about<a id="id1298" class="indexterm"/> scikit-learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">scikit-learn basic tutorial available at <a class="ulink" href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html">http://scikit-learn.org/stable/tutorial/basic/tutorial.html</a></li><li class="listitem" style="list-style-type: disc">scikit-learn tutorial given at the SciPy 2013 conference, available at <a class="ulink" href="https://github.com/jakevdp/sklearn_scipy2013">https://github.com/jakevdp/sklearn_scipy2013</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec277"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Using support vector machines for classification tasks</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Predicting who will survive on the Titanic with logistic regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec76"/>Predicting who will survive on the Titanic with logistic regression</h1></div></div></div><p>In this recipe, we will introduce<a id="id1299" class="indexterm"/> <span class="strong"><strong>logistic regression</strong></span>, a basic classifier. We will also show how to perform a <span class="strong"><strong>grid search</strong></span><a id="id1300" class="indexterm"/> with <a id="id1301" class="indexterm"/><span class="strong"><strong>cross-validation</strong></span>.</p><p>We will apply these techniques on a <span class="strong"><strong>Kaggle</strong></span><a id="id1302" class="indexterm"/> dataset where the goal is to predict survival on the Titanic based on real data.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip29"/>Tip</h3><p>Kaggle (<a class="ulink" href="http://www.kaggle.com/competitions">www.kaggle.com/competitions</a>) hosts <a id="id1303" class="indexterm"/>machine learning competitions where anyone can download a dataset, train a model, and test the predictions on the website. The author of the best model might even win a prize! It is a fun way to get started with machine learning.</p></div></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec278"/>Getting ready</h2></div></div></div><p>Download the <span class="emphasis"><em>Titanic</em></span> dataset from the book's GitHub repository at <a class="ulink" href="https://github.com/ipython-books/cookbook-data">https://github.com/ipython-books/cookbook-data</a>.</p><p>The dataset <a id="id1304" class="indexterm"/>has been<a id="id1305" class="indexterm"/> obtained from <a class="ulink" href="http://www.kaggle.com/c/titanic-gettingStarted">www.kaggle.com/c/titanic-gettingStarted</a>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec279"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We import the standard packages:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import pandas as pd
        import sklearn
        import sklearn.linear_model as lm
        import sklearn.cross_validation as cv
        import sklearn.grid_search as gs
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">We load the training and test datasets with pandas:<div class="informalexample"><pre class="programlisting">In [2]: train = pd.read_csv('data/titanic_train.csv')
        test = pd.read_csv('data/titanic_test.csv')
In [3]: train[train.columns[[2,4,5,1]]].head()
Out[3]:    
   Pclass     Sex  Age  Survived
0       3    male   22         0
1       1  female   38         1
2       3  female   26         1
3       1  female   35         1
4       3    male   35         0</pre></div></li><li class="listitem">Let's keep only a<a id="id1306" class="indexterm"/> few fields for this <a id="id1307" class="indexterm"/>example, and also convert the <code class="literal">sex</code> field to a binary variable so that it can be handled correctly by NumPy and scikit-learn. Finally, we remove the rows that contain <code class="literal">NaN</code> values:<div class="informalexample"><pre class="programlisting">In [4]: data = train[['Sex', 'Age', 'Pclass', 'Survived']].copy()
        data['Sex'] = data['Sex'] == 'female'
        data = data.dropna()</pre></div></li><li class="listitem">Now, we convert this <code class="literal">DataFrame</code> object to a NumPy array so that we can pass it to scikit-learn:<div class="informalexample"><pre class="programlisting">In [5]: data_np = data.astype(np.int32).values
        X = data_np[:,:-1]
        y = data_np[:,-1]</pre></div></li><li class="listitem">Let's have a look at the survival of male and female passengers as a function of their age:<div class="informalexample"><pre class="programlisting">In [6]: # We define a few boolean vectors.
        female = X[:,0] == 1
        survived = y == 1
        # This vector contains the age of the passengers.
        age = X[:,1]
        # We compute a few histograms.
        bins_ = np.arange(0, 81, 5)
        S = {'male': np.histogram(age[survived &amp; ~female], 
                                  bins=bins_)[0],
             'female': np.histogram(age[survived &amp; female], 
                                    bins=bins_)[0]}
        D = {'male': np.histogram(age[~survived &amp; ~female], 
                                  bins=bins_)[0],
             'female': np.histogram(age[~survived &amp; 
                                        female], 
                                    bins=bins_)[0]}
In [7]: # We now plot the data.
        bins = bins_[:-1]
        for i, sex, color in zip((0, 1),
                                 ('male', 'female'),
                                 ('#3345d0', '#cc3dc0')):
            plt.subplot(121 + i)
            plt.bar(bins, S[sex], bottom=D[sex],
                    color=color,
                    width=5, label='survived')
            plt.bar(bins, D[sex], color='k', width=5,
                    label='died')
            plt.xlim(0, 80)
            plt.grid(None)
            plt.title(sex + " survival")
            plt.xlabel("Age (years)")
            plt.legend()</pre></div><div class="mediaobject"><img src="images/4818OS_08_10.jpg" alt="How to do it..."/></div></li><li class="listitem">Let's try to <a id="id1308" class="indexterm"/>train a <code class="literal">LogisticRegression</code> classifier in <a id="id1309" class="indexterm"/>order to predict the survival of people based on their gender, age, and class. We first need to create a train and a test dataset:<div class="informalexample"><pre class="programlisting">In [8]: # We split X and y into train and test datasets.
        (X_train, X_test, y_train, 
        y_test) = cv.train_test_split(X, y, test_size=.05)
In [9]: # We instanciate the classifier.
        logreg = lm.LogisticRegression()</pre></div></li><li class="listitem">We train the model and we get the predicted values on the test set:<div class="informalexample"><pre class="programlisting">In [10]: logreg.fit(X_train, y_train)
         y_predicted = logreg.predict(X_test)</pre></div><p>The following figure shows the actual and predicted results:</p><div class="informalexample"><pre class="programlisting">In [11]: plt.imshow(np.vstack((y_test, y_predicted)),
                    interpolation='none', cmap='bone')
         plt.xticks([]); plt.yticks([])
         plt.title(("Actual and predicted survival "
                    "outcomes on the test set"))</pre></div><div class="mediaobject"><img src="images/4818OS_08_11.jpg" alt="How to do it..."/></div><p>In this screenshot, the first line shows the survival of several people from the test set (white for survival, black otherwise). The second line shows the values predicted by the model.</p></li><li class="listitem">To get an <a id="id1310" class="indexterm"/>estimation of the model's <a id="id1311" class="indexterm"/>performance, we compute the cross-validation score with the <code class="literal">cross_val_score()</code> function. This function uses a three-fold stratified cross-validation procedure by default, but this can be changed with the <code class="literal">cv</code> keyword argument:<div class="informalexample"><pre class="programlisting">In [12]: cv.cross_val_score(logreg, X, y)
Out[12]: array([ 0.78661088,  0.78991597,  0.78059072])</pre></div><p>This function returns, for each pair of train and test set, a prediction score (we give more details in <span class="emphasis"><em>How it works…</em></span>).</p></li><li class="listitem">The <code class="literal">LogisticRegression</code> class accepts a <span class="emphasis"><em>C</em></span> hyperparameter as an argument. This parameter quantifies the regularization strength. To find a good value, we can perform a grid search with the generic <code class="literal">GridSearchCV</code> class. It takes an estimator as input and a dictionary of parameter values. This new estimator uses cross-validation to select the best parameter:<div class="informalexample"><pre class="programlisting">In [13]: grid = gs.GridSearchCV(logreg, 
                            {'C': np.logspace(-5, 5, 50)})
         grid.fit(X_train, y_train)
         grid.best_params_
Out[13]: {'C':  5.35}</pre></div></li><li class="listitem">Here is the performance of the best estimator:<div class="informalexample"><pre class="programlisting">In [14]: cv.cross_val_score(grid.best_estimator_, X, y)
Out[14]: array([ 0.78661088,  0.79831933,  0.78481013])</pre></div><p>Performance is slightly better after the <span class="emphasis"><em>C</em></span> hyperparameter has been chosen with a grid search.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec280"/>How it works...</h2></div></div></div><p>Logistic regression<a id="id1312" class="indexterm"/> is <span class="emphasis"><em>not</em></span> a regression model, it is a classification <a id="id1313" class="indexterm"/>model. Yet, it is closely related to linear regression. This model predicts the probability that a binary variable is 1, by applying a <span class="strong"><strong>sigmoid function</strong></span><a id="id1314" class="indexterm"/> (more precisely, a logistic function) to a linear combination of the variables. The equation of the sigmoid is:</p><div class="mediaobject"><img src="images/4818OS_08_12.jpg" alt="How it works..."/></div><p>The following figure shows a logistic function:</p><div class="mediaobject"><img src="images/4818OS_08_13.jpg" alt="How it works..."/><div class="caption"><p>A logistic function</p></div></div><p>If a binary variable has to be obtained, we can round the value to the closest integer.</p><p>The parameter <span class="emphasis"><em>w</em></span> is obtained with an optimization procedure during the learning step.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec281"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Logistic regression on <a id="id1315" class="indexterm"/>Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Logistic_regression">http://en.wikipedia.org/wiki/Logistic_regression</a></li><li class="listitem" style="list-style-type: disc">Logistic regression in scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec282"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Getting started with scikit-learn</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Learning to recognize handwritten digits with a K-nearest neighbors classifier</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Using support vector machines for classification tasks</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Learning to recognize handwritten digits with a K-nearest neighbors classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec77"/>Learning to recognize handwritten digits with a K-nearest neighbors classifier</h1></div></div></div><p>In this recipe, we will see how to recognize handwritten digits with a<a id="id1316" class="indexterm"/> <span class="strong"><strong>K-nearest neighbors</strong></span> (<span class="strong"><strong>K-NN</strong></span>) classifier. This classifier is a simple but <a id="id1317" class="indexterm"/>powerful model, well-adapted<a id="id1318" class="indexterm"/> to complex, highly nonlinear datasets such as images. We will explain how it works later in this recipe.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec283"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We import the modules:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import sklearn
        import sklearn.datasets as ds
        import sklearn.cross_validation as cv
        import sklearn.neighbors as nb
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">Let's load the <span class="emphasis"><em>digits</em></span> dataset, part of the <code class="literal">datasets</code> module of scikit-learn. This dataset contains handwritten digits that have been manually labeled:<div class="informalexample"><pre class="programlisting">In [2]: digits = ds.load_digits()
        X = digits.data
        y = digits.target
        print((X.min(), X.max()))
        print(X.shape)
0.0 16.0
(1797L, 64L)</pre></div><p>In the matrix <code class="literal">X</code>, each row contains <span class="emphasis"><em>8 * 8=64</em></span> pixels (in grayscale, values between 0 and 16). The row-major ordering is used.</p></li><li class="listitem">Let's display <a id="id1319" class="indexterm"/>some of the images along with their labels:<div class="informalexample"><pre class="programlisting">In [3]: nrows, ncols = 2, 5
        plt.gray()
        for i in range(ncols * nrows):
            ax = plt.subplot(nrows, ncols, i + 1)
            ax.matshow(digits.images[i,...])
            plt.xticks([]); plt.yticks([])
            plt.title(digits.target[i])</pre></div><div class="mediaobject"><img src="images/4818OS_08_14.jpg" alt="How to do it..."/></div></li><li class="listitem">Now, let's fit a<a id="id1320" class="indexterm"/> K-nearest neighbors classifier on the data:<div class="informalexample"><pre class="programlisting">In [4]: (X_train, X_test, y_train, 
         y_test) = cv.train_test_split(X, y, test_size=.25)
In [5]: knc = nb.KNeighborsClassifier()
In [6]: knc.fit(X_train, y_train);</pre></div></li><li class="listitem">Let's evaluate the score of the trained classifier on the test dataset:<div class="informalexample"><pre class="programlisting">In [7]: knc.score(X_test, y_test)
Out[7]: 0.98888888888888893</pre></div></li><li class="listitem">Now, let's see if our classifier can recognize a <span class="emphasis"><em>handwritten</em></span> digit!<div class="informalexample"><pre class="programlisting">In [8]: # Let's draw a 1.
        one = np.zeros((8, 8))
        one[1:-1, 4] = 16  # The image values are 
                           # in [0,16].
        one[2, 3] = 16
In [9]: plt.imshow(one, interpolation='none')
        plt.grid(False)
        plt.xticks(); plt.yticks()
        plt.title("One")</pre></div><div class="mediaobject"><img src="images/4818OS_08_15.jpg" alt="How to do it..."/></div><p>Can our model recognize this number? Let's see:</p><div class="informalexample"><pre class="programlisting">In [10]: knc.predict(one.ravel())
Out[10]: array([1])</pre></div><p>Good job!</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec284"/>How it works...</h2></div></div></div><p>This example <a id="id1321" class="indexterm"/>illustrates<a id="id1322" class="indexterm"/> how to deal with images in scikit-learn. An image is a 2D <span class="emphasis"><em>(N, M)</em></span> matrix, which has <span class="emphasis"><em>NM</em></span> features. This matrix needs to be flattened when composing the data matrix; each row is a full image.</p><p>The idea of K-nearest neighbors is as follows: given a new point in the feature space, find the <span class="emphasis"><em>K</em></span> closest points from the training set and assign the label of the majority of those points.</p><p>The distance is generally the Euclidean distance, but other distances can be used too.</p><p>The following image shows the space partition obtained with a 15-nearest-neighbors classifier on a toy dataset (with three labels):</p><div class="mediaobject"><img src="images/4818OS_08_16.jpg" alt="How it works..."/><div class="caption"><p>K-nearest neighbors space partition</p></div></div><p>The <a id="id1323" class="indexterm"/>number <span class="emphasis"><em>K</em></span> is a hyperparameter of the model. If it is too small, the model will not generalize <a id="id1324" class="indexterm"/>well (high variance). In particular, it will be highly sensitive to outliers. By contrast, the precision of the model will worsen if <span class="emphasis"><em>K</em></span> is too large. At the extreme, if <span class="emphasis"><em>K</em></span> is equal to the total number of points, the model will always predict the exact same value disregarding the input (high bias). There are heuristics to choose this hyperparameter (see the next section).</p><p>It should be noted that no model is learned by a K-nearest neighbor algorithm; the classifier just stores all data points and compares any new target points with them. This is an example of <a id="id1325" class="indexterm"/><span class="strong"><strong>instance-based learning</strong></span>. It is in contrast to other classifiers such as the logistic regression model, which explicitly learns a simple mathematical model on the training data.</p><p>The K-nearest neighbors method works well on complex classification problems that have irregular decision boundaries. However, it might be computationally intensive with large training datasets because a large number of distances have to be computed for testing. Dedicated tree-based data structures such as <span class="strong"><strong>K-D trees</strong></span><a id="id1326" class="indexterm"/> or <span class="strong"><strong>ball trees</strong></span><a id="id1327" class="indexterm"/> can be used to accelerate the search of nearest neighbors.</p><p>The K-nearest neighbors method can be used for classification, like here, and also for regression problems. The model assigns the average of the target value of the nearest neighbors. In both cases, different weighting strategies can be used.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec285"/>There's more…</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The K-NN algorithm <a id="id1328" class="indexterm"/>in scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/neighbors.html">http://scikit-learn.org/stable/modules/neighbors.html</a></li><li class="listitem" style="list-style-type: disc">The K-NN algorithm on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a></li><li class="listitem" style="list-style-type: disc">Blog post about how to choose the K hyperparameter, available at <a class="ulink" href="http://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/">http://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/</a></li><li class="listitem" style="list-style-type: disc">Instance-based learning<a id="id1329" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Instance-based_learning">http://en.wikipedia.org/wiki/Instance-based_learning</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec286"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Predicting who will survive on the Titanic with logistic regression</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Using support vector machines for classification tasks</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Learning from text – Naive Bayes for Natural Language Processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec78"/>Learning from text – Naive Bayes for Natural Language Processing</h1></div></div></div><p>In this recipe, we show how to handle text data with <a id="id1330" class="indexterm"/>scikit-learn. Working with text requires <a id="id1331" class="indexterm"/>careful preprocessing and feature extraction. It is also quite common to deal with highly sparse matrices.</p><p>We will learn to recognize whether a comment posted during a public discussion is considered insulting to one of the participants. We will use a labeled dataset from Impermium, released during a Kaggle competition.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec287"/>Getting ready</h2></div></div></div><p>Download the <span class="emphasis"><em>Troll</em></span> dataset from the book's GitHub repository at <a class="ulink" href="https://github.com/ipython-books/cookbook-data">https://github.com/ipython-books/cookbook-data</a>.</p><p>This dataset was obtained from<a id="id1332" class="indexterm"/> Kaggle, at <a class="ulink" href="http://www.kaggle.com/c/detecting-insults-in-social-commentary">www.kaggle.com/c/detecting-insults-in-social-commentary</a>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec288"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import our libraries:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import pandas as pd
        import sklearn
        import sklearn.cross_validation as cv
        import sklearn.grid_search as gs
        import sklearn.feature_extraction.text as text
        import sklearn.naive_bayes as nb
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">Let's open the CSV file with pandas:<div class="informalexample"><pre class="programlisting">In [2]: df = pd.read_csv("data/troll.csv")</pre></div></li><li class="listitem">Each row is a comment. We will consider two columns: whether the comment is insulting (1) or not (0) and the unicode-encoded contents of the comment:<div class="informalexample"><pre class="programlisting">In [3]: df[['Insult', 'Comment']].tail()
      Insult                                            Comment
3942       1  "you are both morons and that is..."
3943       0  "Many toolbars include spell check...
3944       0  "@LambeauOrWrigley\xa0\xa0@K.Moss\xa0\n...
3945       0  "How about Felix? He is sure turning into...
3946       0  "You're all upset, defending this hipster...</pre></div></li><li class="listitem">Now, we are <a id="id1333" class="indexterm"/>going to define the feature <a id="id1334" class="indexterm"/>matrix <code class="literal">X</code> and the labels <code class="literal">y</code>:<div class="informalexample"><pre class="programlisting">In [4]: y = df['Insult']</pre></div><p>Obtaining the feature matrix from the text is not trivial. scikit-learn can only work with numerical matrices. So how do we convert text into a matrix of numbers? A classical solution is to first extract a <span class="strong"><strong>vocabulary</strong></span>, a list of words used throughout the corpus. Then, we count, for each sample, the frequency of each word. We end up with a <span class="strong"><strong>sparse matrix</strong></span><a id="id1335" class="indexterm"/>, a huge matrix containing mostly zeros. Here, we do this in two lines. We will give more details in <span class="emphasis"><em>How it works…</em></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note30"/>Note</h3><p>The general rule here is that whenever one of our features is categorical (that is, the presence of a word, a color belonging to a fixed set of <span class="emphasis"><em>n</em></span> colors, and so on), we should <span class="emphasis"><em>vectorize</em></span> it by considering one binary feature per item in the class. For example, instead of a feature <code class="literal">color</code> being <code class="literal">red</code>, <code class="literal">green</code>, or <code class="literal">blue</code>, we should consider three <span class="emphasis"><em>binary</em></span> features <code class="literal">color_red</code>, <code class="literal">color_green</code>, and <code class="literal">color_blue</code>. We give further references in the <span class="emphasis"><em>There's more…</em></span> section.</p></div></div><div class="informalexample"><pre class="programlisting">In [5]: tf = text.TfidfVectorizer()
        X = tf.fit_transform(df['Comment'])
        print(X.shape)
(3947, 16469)</pre></div></li><li class="listitem">There are 3947 comments and 16469 different words. Let's estimate the sparsity of this feature matrix:<div class="informalexample"><pre class="programlisting">In [6]: print(("Each sample has ~{0:.2f}% non-zero"
               "features.").format(
           100 * X.nnz / float(X.shape[0] * X.shape[1])))
Each sample has ~0.15% non-zero features.</pre></div></li><li class="listitem">Now, we are going to train a classifier as usual. We first split the data into a train and test set:<div class="informalexample"><pre class="programlisting">In [7]: (X_train, X_test, y_train, 
         y_test) = cv.train_test_split(X, y,
                                       test_size=.2)</pre></div></li><li class="listitem">
We use a <span class="strong"><strong>Bernoulli Naive Bayes classifier</strong></span><a id="id1336" class="indexterm"/> with a grid search on the <span class="inlinemediaobject"><img src="images/4818OS_08_43.jpg" alt="How to do it..."/></span> parameter:
<div class="informalexample"><pre class="programlisting">In [8]: bnb = gs.GridSearchCV(nb.BernoulliNB(), 
                              param_grid={
                        'alpha': np.logspace(-2., 2., 50)})
        bnb.fit(X_train, y_train)</pre></div></li><li class="listitem">Let's check the performance of this classifier on the test dataset:<div class="informalexample"><pre class="programlisting">In [9]: bnb.score(X_test, y_test)
Out[9]: 0.76455696202531642</pre></div></li><li class="listitem">Let's take a <a id="id1337" class="indexterm"/>look at the words corresponding <a id="id1338" class="indexterm"/>to the largest coefficients (the words we find frequently in insulting comments):<div class="informalexample"><pre class="programlisting">In [10]: # We first get the words corresponding 
         # to each feature.
         names = np.asarray(tf.get_feature_names())
         # Next, we display the 50 words with the largest
         # coefficients.
         print(','.join(names[np.argsort(
             bnb.best_estimator_.coef_[0,:])[::-1][:50]]))
you,are,your,to,the,and,of,that,is,it,in,like,on,have,for,not,re,just,an,with,so,all,***,***be,get,***,***up,this,what,xa0,don,***,***go,no,do,can,but,***,***or,as,if,***,***who,know,about,because,here,***,***me,was</pre></div></li><li class="listitem">Finally, let's test our estimator on a few test sentences:<div class="informalexample"><pre class="programlisting">In [11]: print(bnb.predict(tf.transform([
             "I totally agree with you.",
             "You are so stupid.",
             "I love you."
             ])))
[0 1 1]</pre></div><p>That's not bad, but we can probably do better.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec289"/>How it works...</h2></div></div></div><p>scikit-learn <a id="id1339" class="indexterm"/>implements several utility functions to obtain a sparse feature matrix from text data. A <span class="strong"><strong>vectorizer</strong></span><a id="id1340" class="indexterm"/> such as <code class="literal">CountVectorizer()</code> extracts a vocabulary from a corpus (<code class="literal">fit</code>) and constructs a sparse representation of the corpus based on this vocabulary (<code class="literal">transform</code>). Each sample is represented by the vocabulary's word frequencies. The trained instance also contains attributes and methods to map feature indices to the corresponding words (<code class="literal">get_feature_names()</code>) and conversely (<code class="literal">vocabulary_</code>).</p><p>N-grams can also be extracted. These are pairs or tuples of words occurring successively (the <code class="literal">ngram_range</code> keyword).</p><p>The frequency of the words can be weighted in different ways. Here, we have used <span class="strong"><strong>tf-idf</strong></span><a id="id1341" class="indexterm"/>, or <span class="strong"><strong>term frequency-inverse document frequency</strong></span>. This quantity reflects how important a word is to a corpus. Frequent <a id="id1342" class="indexterm"/>words in comments have a high weight except if they appear in most comments (which means that they are common terms, for example, "the" and "and" would be filtered out using this technique).</p><p>Naive Bayes algorithms are Bayesian methods based on the naive assumption of independence between the features. This strong assumption drastically simplifies the computations and leads to very fast yet decent classifiers.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec290"/>There's more…</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Text feature extraction in<a id="id1343" class="indexterm"/> scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction">http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction</a></li><li class="listitem" style="list-style-type: disc">Term frequency-inverse document-frequency<a id="id1344" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/tf-idf">http://en.wikipedia.org/wiki/tf-idf</a></li><li class="listitem" style="list-style-type: disc">Vectorizer in<a id="id1345" class="indexterm"/> scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html</a></li><li class="listitem" style="list-style-type: disc">Naive Bayes classifier <a id="id1346" class="indexterm"/>on Wikipedia, at <a class="ulink" href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier">http://en.wikipedia.org/wiki/Naive_Bayes_classifier</a></li><li class="listitem" style="list-style-type: disc">Naive Bayes in scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/naive_bayes.html">http://scikit-learn.org/stable/modules/naive_bayes.html</a></li><li class="listitem" style="list-style-type: disc">Impermium Kaggle<a id="id1347" class="indexterm"/> challenge, at <a class="ulink" href="http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/">http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/</a></li><li class="listitem" style="list-style-type: disc">Document classification example in <a id="id1348" class="indexterm"/>scikit-learn's documentation, at <a class="ulink" href="http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html">http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html</a></li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip30"/>Tip</h3><p>Besides scikit-learn, which has good support for text processing, we should also mention <a id="id1349" class="indexterm"/>NLTK (available at <a class="ulink" href="http://www.nltk.org">www.nltk.org</a>), a Natural Language Toolkit in Python.</p></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec291"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Predicting who will survive on the Titanic with logistic regression</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Learning to recognize handwritten digits with a K-nearest neighbors classifier</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Using support vector machines for classification tasks</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Using support vector machines for classification tasks"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec79"/>Using support vector machines for classification tasks</h1></div></div></div><p>In this recipe, we introduce<a id="id1350" class="indexterm"/> <span class="strong"><strong>support vector machines</strong></span>, or <span class="strong"><strong>SVMs</strong></span>. These powerful models can be used for classification and regression. Here, we illustrate how to use linear and nonlinear SVMs on a simple classification task.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec292"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's<a id="id1351" class="indexterm"/> import the<a id="id1352" class="indexterm"/> packages:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import pandas as pd
        import sklearn
        import sklearn.datasets as ds
        import sklearn.cross_validation as cv
        import sklearn.grid_search as gs
        import sklearn.svm as svm
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">We generate 2D <a id="id1353" class="indexterm"/>points and assign a binary label according to a linear operation on the coordinates:<div class="informalexample"><pre class="programlisting">In [2]: X = np.random.randn(200, 2)
        y = X[:, 0] + X[:, 1] &gt; 1</pre></div></li><li class="listitem">We now fit a linear <a id="id1354" class="indexterm"/><span class="strong"><strong>Support Vector Classifier</strong></span> (<span class="strong"><strong>SVC</strong></span>). This <a id="id1355" class="indexterm"/>classifier tries to separate the two groups of points with a linear boundary (a line here, but more generally a hyperplane):<div class="informalexample"><pre class="programlisting">In [3]: # We train the classifier.
        est = svm.LinearSVC()
        est.fit(X, y)</pre></div></li><li class="listitem">We define a function that displays the boundaries and decision function of a trained classifier:<div class="informalexample"><pre class="programlisting">In [4]: # We generate a grid in the square [-3,3 ]^2.
        xx, yy = np.meshgrid(np.linspace(-3, 3, 500),
                             np.linspace(-3, 3, 500))
        # This function takes a SVM estimator as input.
        def plot_decision_function(est):
            # We evaluate the decision function on the
            # grid.
            Z = est.decision_function(np.c_[xx.ravel(), 
                                            yy.ravel()])
            Z = Z.reshape(xx.shape)
            cmap = plt.cm.Blues
            # We display the decision function on the grid.
            plt.imshow(Z,
                       extent=(xx.min(), xx.max(), 
                               yy.min(), yy.max()),
                       aspect='auto', origin='lower', 
                       cmap=cmap)
            # We display the boundaries.
            plt.contour(xx, yy, Z, levels=[0],                
                        linewidths=2,
                        colors='k')
            # We display the points with their true labels.
            plt.scatter(X[:, 0], X[:, 1], s=30, c=.5+.5*y, 
                        lw=1, cmap=cmap, vmin=0, vmax=1)
            plt.axhline(0, color='k', ls='--')
            plt.axvline(0, color='k', ls='--')
            plt.xticks(())
            plt.yticks(())
            plt.axis([-3, 3, -3, 3])</pre></div></li><li class="listitem">Let's take a look at the classification results with the linear SVC:<div class="informalexample"><pre class="programlisting">In [5]: plot_decision_function(est)
        plt.title("Linearly separable, linear SVC")</pre></div><div class="mediaobject"><img src="images/4818OS_08_17.jpg" alt="How to do it..."/></div><p>The linear <a id="id1356" class="indexterm"/>SVC tried<a id="id1357" class="indexterm"/> to separate the points with a line and it did a pretty good job here.</p></li><li class="listitem">We now modify the labels with an <code class="literal">XOR</code> function. A point's label is 1 if the coordinates have different signs. This classification is not linearly separable. Therefore, a linear SVC fails completely:<div class="informalexample"><pre class="programlisting">In [6]: y = np.logical_xor(X[:, 0]&gt;0, X[:, 1]&gt;0)
        # We train the classifier.
        est = gs.GridSearchCV(svm.LinearSVC(), 
                          {'C': np.logspace(-3., 3., 10)})
        est.fit(X, y)
        print("Score: {0:.1f}".format(
                      v.cross_val_score(est, X, y).mean()))
        # Plot the decision function.
        plot_decision_function(est)
        plt.title("XOR, linear SVC")
Score: 0.6</pre></div><div class="mediaobject"><img src="images/4818OS_08_18.jpg" alt="How to do it..."/></div></li><li class="listitem">Fortunately, it <a id="id1358" class="indexterm"/>is possible to use nonlinear SVCs by using nonlinear <span class="strong"><strong>kernels</strong></span>. Kernels specify<a id="id1359" class="indexterm"/> a nonlinear <a id="id1360" class="indexterm"/>transformation of the points into a higher dimensional space. Transformed points in this space are assumed to be more linearly separable. By default, the <code class="literal">SVC</code> classifier in scikit-learn uses the <span class="strong"><strong>Radial Basis Function</strong></span> (<span class="strong"><strong>RBF</strong></span>)<a id="id1361" class="indexterm"/> kernel:<div class="informalexample"><pre class="programlisting">In [7]: y = np.logical_xor(X[:, 0]&gt;0, X[:, 1]&gt;0)
        est = gs.GridSearchCV(svm.SVC(), 
                     {'C': np.logspace(-3., 3., 10),
                      'gamma': np.logspace(-3., 3., 10)})
        est.fit(X, y)
        print("Score: {0:.3f}".format(
              cv.cross_val_score(est, X, y).mean()))
        plot_decision_function(est.best_estimator_)
        plt.title("XOR, non-linear SVC")
Score: 0.975</pre></div><div class="mediaobject"><img src="images/4818OS_08_19.jpg" alt="How to do it..."/></div><p>This time, the nonlinear SVC successfully managed to classify these nonlinearly separable points.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec293"/>How it works...</h2></div></div></div><p>A two-class linear<a id="id1362" class="indexterm"/> SVC tries to find a hyperplane (defined as a linear equation) that best separates the two sets of points (grouped according to their labels). There is also the constraint that this separating hyperplane needs to be as far as possible from the points. This method works best when such a hyperplane<a id="id1363" class="indexterm"/> exists. Otherwise, this method can fail completely, as we saw in the <code class="literal">XOR</code> example. <code class="literal">XOR</code> is known as being a nonlinearly separable operation.</p><p>The SVM classes in scikit-learn have a <span class="emphasis"><em>C</em></span> hyperparameter. This hyperparameter trades off misclassification of training examples against simplicity of the decision surface. A low <span class="emphasis"><em>C</em></span> value makes the decision surface smooth, while a high <span class="emphasis"><em>C</em></span> value aims at classifying all training examples correctly. This is another example where a hyperparameter quantifies the bias-variance trade-off. This hyperparameter can be chosen with cross-validation and grid search.</p><p>The linear SVC can also be extended to multiclass problems. The multiclass SVC is directly implemented in scikit-learn.</p><p>The nonlinear SVC works by considering a nonlinear transformation <span class="inlinemediaobject"><img src="images/4818OS_08_35.jpg" alt="How it works..."/></span> from the original space into a higher dimensional space. This nonlinear transformation can increase the linear separability of the classes. In practice, all dot products are replaced by the <span class="inlinemediaobject"><img src="images/4818OS_08_36.jpg" alt="How it works..."/></span>
<span class="emphasis"><em> </em></span>kernel.</p><div class="mediaobject"><img src="images/4818OS_08_20.jpg" alt="How it works..."/><div class="caption"><p>Nonlinear SVC</p></div></div><p>There are several widely-used nonlinear kernels. By default, SVC uses Gaussian radial basis functions:</p><div class="mediaobject"><img src="images/4818OS_08_21.jpg" alt="How it works..."/></div><p>Here, <span class="inlinemediaobject"><img src="images/4818OS_08_37.jpg" alt="How it works..."/></span>
<span class="emphasis"><em> </em></span>is a hyperparameter of the model that can be chosen with grid search and cross-validation.</p><p>The <span class="inlinemediaobject"><img src="images/4818OS_08_42.jpg" alt="How it works..."/></span>
<span class="emphasis"><em> </em></span>function does not need to be computed explicitly. This is the <a id="id1364" class="indexterm"/><span class="strong"><strong>kernel trick</strong></span>; it suffices to know the kernel <span class="emphasis"><em>k(x, x')</em></span>. The existence of a function <span class="inlinemediaobject"><img src="images/4818OS_08_42.jpg" alt="How it works..."/></span>
<span class="emphasis"><em> </em></span>corresponding to a given kernel <span class="emphasis"><em>k(x, x')</em></span> is<a id="id1365" class="indexterm"/> guaranteed by a mathematical<a id="id1366" class="indexterm"/> theorem in functional analysis.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec294"/>There's more…</h2></div></div></div><p>Here are a few references about <a id="id1367" class="indexterm"/>support vector machines:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Exclusive OR on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Exclusive_or">http://en.wikipedia.org/wiki/Exclusive_or</a></li><li class="listitem" style="list-style-type: disc">Support vector machines on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Support_vector_machine">http://en.wikipedia.org/wiki/Support_vector_machine</a></li><li class="listitem" style="list-style-type: disc">SVMs in scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/svm.html">http://scikit-learn.org/stable/modules/svm.html</a></li><li class="listitem" style="list-style-type: disc">Kernel trick on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Kernel_method">http://en.wikipedia.org/wiki/Kernel_method</a></li><li class="listitem" style="list-style-type: disc">Notes about the kernel trick available at <a class="ulink" href="http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html">www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html</a></li><li class="listitem" style="list-style-type: disc">An example with a nonlinear SVM available at <a class="ulink" href="http://scikit-learn.org/0.11/auto_examples/svm/plot_svm_nonlinear.html">http://scikit-learn.org/0.11/auto_examples/svm/plot_svm_nonlinear.html</a> (this example <a id="id1368" class="indexterm"/>inspired this recipe)</li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec295"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Predicting who will survive on the Titanic with logistic regression</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Learning to recognize handwritten digits with a K-nearest neighbors classifier</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Using a random forest to select important features for regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec80"/>Using a random forest to select important features for regression</h1></div></div></div><p>
<span class="strong"><strong>Decision trees</strong></span><a id="id1369" class="indexterm"/> are frequently used to represent workflows or algorithms. They also form a method for nonparametric supervised learning. A tree mapping observations to target values is learned on a training set and gives the outcomes of new observations.</p><p>
<span class="strong"><strong>Random forests</strong></span><a id="id1370" class="indexterm"/> are ensembles of decision trees. Multiple decision trees are trained and aggregated to form a model that is more performant than any of the individual trees. This general idea is the purpose of <a id="id1371" class="indexterm"/><span class="strong"><strong>ensemble learning</strong></span>.</p><p>There are many types of ensemble methods. Random forests are an instance of <a id="id1372" class="indexterm"/><span class="strong"><strong>bootstrap aggregating</strong></span>, also called <a id="id1373" class="indexterm"/><span class="strong"><strong>bagging</strong></span>, where models are trained on randomly drawn subsets of the training set.</p><p>Random forests yield information about the importance of each feature for the classification or regression task. In this recipe, we will find the most influential features of Boston house prices using a classic dataset that contains a range of diverse indicators <a id="id1374" class="indexterm"/>about the <a id="id1375" class="indexterm"/>houses' neighborhood.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec296"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We import the packages:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import sklearn as sk
        import sklearn.datasets as skd
        import sklearn.ensemble as ske
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">We load the Boston dataset:<div class="informalexample"><pre class="programlisting">In [2]: data = skd.load_boston()</pre></div><p>The details of this dataset can be found in <code class="literal">data['DESCR']</code>. Here is the description of some features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>CRIM</em></span>: Per capita crime rate by town</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>NOX</em></span>: Nitric oxide concentration (parts per 10 million)</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>RM</em></span>: Average number of rooms per dwelling</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>AGE</em></span>: Proportion of owner-occupied units built prior to 1940</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>DIS</em></span>: Weighted distances to five Boston employment centers</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>PTRATIO</em></span>: Pupil-teacher ratio by town </li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>LSTAT</em></span>: Percentage of lower status of the population</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>MEDV</em></span>: Median value of owner-occupied homes in $1000s</li></ul></div><p>The target value is <span class="emphasis"><em>MEDV</em></span>.</p></li><li class="listitem">We create a <code class="literal">RandomForestRegressor</code> model:<div class="informalexample"><pre class="programlisting">In [3]: reg = ske.RandomForestRegressor()</pre></div></li><li class="listitem">We get the samples and the target values from this dataset:<div class="informalexample"><pre class="programlisting">In [4]: X = data['data']
        y = data['target']</pre></div></li><li class="listitem">Let's fit the model:<div class="informalexample"><pre class="programlisting">In [5]: reg.fit(X, y)</pre></div></li><li class="listitem">The importance <a id="id1376" class="indexterm"/>of our features<a id="id1377" class="indexterm"/> can be found in <code class="literal">reg.feature_importances_</code>. We sort them by decreasing order of importance:<div class="informalexample"><pre class="programlisting">In [6]: fet_ind = np.argsort(reg.feature_importances_) \
                                                    [::-1]
        fet_imp = reg.feature_importances_[fet_ind]</pre></div></li><li class="listitem">Finally, we plot a histogram of the features' importance:<div class="informalexample"><pre class="programlisting">In [7]: ax = plt.subplot(111)
        plt.bar(np.arange(len(fet_imp)), 
                fet_imp, width=1, lw=2)
        plt.grid(False)
        ax.set_xticks(np.arange(len(fet_imp))+.5)
        ax.set_xticklabels(data['feature_names'][fet_ind])
        plt.xlim(0, len(fet_imp))</pre></div><div class="mediaobject"><img src="images/4818OS_08_22.jpg" alt="How to do it..."/></div><p>We find<a id="id1378" class="indexterm"/> that <span class="emphasis"><em>LSTAT</em></span> (proportion of lower status of the population) and <span class="emphasis"><em>RM</em></span> (number of rooms per dwelling) are the most important features <a id="id1379" class="indexterm"/>determining the price of a house. As an illustration, here is a scatter plot of the price as a function of <span class="emphasis"><em>LSTAT</em></span>:</p><div class="informalexample"><pre class="programlisting">In [8]: plt.scatter(X[:,-1], y)
        plt.xlabel('LSTAT indicator')
        plt.ylabel('Value of houses (k$)')</pre></div><div class="mediaobject"><img src="images/4818OS_08_23.jpg" alt="How to do it..."/></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec297"/>How it works...</h2></div></div></div><p>Several algorithms<a id="id1380" class="indexterm"/> can be used to train a<a id="id1381" class="indexterm"/> decision tree. scikit-learn uses the <span class="strong"><strong>CART</strong></span><a id="id1382" class="indexterm"/>, or <span class="strong"><strong>Classification and Regression Trees</strong></span>, algorithm. This algorithm constructs binary trees using the feature and threshold that yield the largest information gain at each node. Terminal nodes give the outcomes of input values.</p><p>Decision trees are simple to understand. They can also be visualized with <a id="id1383" class="indexterm"/><span class="strong"><strong>pydot</strong></span>, a Python package for drawing graphs and trees. This is useful when we want to understand what a tree has learned <a id="id1384" class="indexterm"/>exactly (<span class="strong"><strong>white box model</strong></span>); the conditions that apply on the observations at each node can be expressed easily with Boolean logic.</p><p>However, decision trees may suffer from overfitting, notably when they are too deep, and they might be unstable. Additionally, global convergence toward an optimal model is not guaranteed, particularly when greedy algorithms are used for training. These problems can be mitigated by using ensembles of decision trees, notably random forests.</p><p>In a random forest, multiple decision trees are trained on bootstrap samples of the training dataset (randomly sampled with replacement). Predictions are made with the averages of individual trees' predictions (bootstrap aggregating or bagging). Additionally, random subsets of the features are chosen at each<a id="id1385" class="indexterm"/> node (<span class="strong"><strong>random subspace method</strong></span>). These methods lead to an overall better model than the individual trees.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec298"/>There's more...</h2></div></div></div><p>Here are a few <a id="id1386" class="indexterm"/>references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ensemble learning in scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/ensemble.html">http://scikit-learn.org/stable/modules/ensemble.html</a></li><li class="listitem" style="list-style-type: disc">API reference of <code class="literal">RandomForestRegressor</code> available at <a class="ulink" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html</a></li><li class="listitem" style="list-style-type: disc">Random forests on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Random_forest">http://en.wikipedia.org/wiki/Random_forest</a></li><li class="listitem" style="list-style-type: disc">Decision tree learning on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Decision_tree_learning">http://en.wikipedia.org/wiki/Decision_tree_learning</a></li><li class="listitem" style="list-style-type: disc">Bootstrap aggregating on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Bootstrap_aggregating">http://en.wikipedia.org/wiki/Bootstrap_aggregating</a></li><li class="listitem" style="list-style-type: disc">Random subspace method on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Random_subspace_method">http://en.wikipedia.org/wiki/Random_subspace_method</a></li><li class="listitem" style="list-style-type: disc">Ensemble learning on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Ensemble_learning">http://en.wikipedia.org/wiki/Ensemble_learning</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec299"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Using support vector machines for classification tasks</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Reducing the dimensionality of a dataset with a principal component analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec81"/>Reducing the dimensionality of a dataset with a principal component analysis</h1></div></div></div><p>In the previous recipes, we presented <span class="emphasis"><em>supervised learning</em></span> methods; our data points came with discrete or continuous labels, and the algorithms were able to learn the mapping from the points to the labels.</p><p>Starting with this recipe, we will present <a id="id1387" class="indexterm"/><span class="strong"><strong>unsupervised learning</strong></span> methods. These methods might be helpful prior to running a supervised learning algorithm. They can give a first insight into the data.</p><p>Let's assume that our data <a id="id1388" class="indexterm"/>consists of <a id="id1389" class="indexterm"/>points <span class="emphasis"><em>x<sub>i</sub></em></span> without any labels. The goal is to discover some form of hidden structure in this set of points. Frequently, data points have intrinsic low dimensionality: a small number of features suffice to accurately describe the data. However, these features might be hidden among many other features not relevant to the problem. Dimension reduction can help us find these structures. This knowledge can considerably improve the performance of subsequent supervised learning algorithms.</p><p>Another useful application of unsupervised learning is<a id="id1390" class="indexterm"/> <span class="strong"><strong>data visualization</strong></span>; high-dimensional datasets are hard to visualize in 2D or 3D. Projecting the data points on a subspace or submanifold yields more interesting visualizations.</p><p>In this recipe, we will illustrate a basic unsupervised linear method, <span class="strong"><strong>principal component analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>). This<a id="id1391" class="indexterm"/> algorithm lets us project data points linearly on a low-dimensional subspace. Along the <a id="id1392" class="indexterm"/><span class="strong"><strong>principal components</strong></span>, which are vectors forming a basis of this low-dimensional subspace, the variance of the data points is maximum.</p><p>We will use the<a id="id1393" class="indexterm"/> classic <span class="emphasis"><em>Iris flower</em></span> dataset as an example. This dataset contains the width and length of the<a id="id1394" class="indexterm"/> petal and sepal of 150 iris flowers. These flowers belong to one of three categories: <span class="emphasis"><em>Iris setosa</em></span>, <span class="emphasis"><em>Iris virginica</em></span>, and <span class="emphasis"><em>Iris versicolor</em></span>. We have access to the category in this dataset (labeled data). However, because we are interested in illustrating an unsupervised learning method, we will only use the data matrix <span class="emphasis"><em>without</em></span> the labels.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec300"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We import NumPy, matplotlib, and scikit-learn:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import sklearn
        import sklearn.decomposition as dec
        import sklearn.datasets as ds
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">The <span class="emphasis"><em>Iris flower</em></span> dataset is available in the <code class="literal">datasets</code> module of scikit-learn:<div class="informalexample"><pre class="programlisting">In [2]: iris = ds.load_iris()
        X = iris.data
        y = iris.target
        print(X.shape)
(150L, 4L)</pre></div></li><li class="listitem">Each row contains four parameters related to the morphology of the flower. Let's display the first two dimensions. The color reflects the iris variety of the flower (the label, between 0 and 2):<div class="informalexample"><pre class="programlisting">In [3]: plt.scatter(X[:,0], X[:,1], c=y,
                    s=30, cmap=plt.cm.rainbow)</pre></div><div class="mediaobject"><img src="images/4818OS_08_24.jpg" alt="How to do it..."/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip31"/>Tip</h3><p>If you're reading the printed version of this book, you might not be able to distinguish the colors. You will find the colored images on the book's website.</p></div></div></li><li class="listitem">We now <a id="id1395" class="indexterm"/>apply PCA on the dataset to get the transformed matrix. This operation can be done in a single line with scikit-learn: we instantiate a <code class="literal">PCA </code>model and <a id="id1396" class="indexterm"/>call the <code class="literal">fit_transform()</code> method. This function computes<a id="id1397" class="indexterm"/> the principal components and projects the data on them:<div class="informalexample"><pre class="programlisting">In [4]: X_bis = dec.PCA().fit_transform(X)</pre></div></li><li class="listitem">We now display the same dataset, but in a new coordinate system (or equivalently, a linearly transformed version of the initial dataset):<div class="informalexample"><pre class="programlisting">In [5]: plt.scatter(X_bis[:,0], X_bis[:,1], c=y,
                    s=30, cmap=plt.cm.rainbow)</pre></div><div class="mediaobject"><img src="images/4818OS_08_25.jpg" alt="How to do it..."/></div><p>Points belonging to the same classes are now grouped together, even though the <code class="literal">PCA</code> estimator did <span class="emphasis"><em>not</em></span> use the labels. The PCA was able to find a projection maximizing the variance, which corresponds here to a projection where the classes are well separated.</p></li><li class="listitem">The <code class="literal">scikit.decomposition</code> module contains several variants of the classic <code class="literal">PCA</code> estimator: <code class="literal">ProbabilisticPCA</code>, <code class="literal">SparsePCA</code>, <code class="literal">RandomizedPCA</code>, <code class="literal">KernelPCA</code>, and others. As an example, let's take a look at <code class="literal">KernelPCA</code>, a nonlinear version of PCA:<div class="informalexample"><pre class="programlisting">In [6]: X_ter = dec.KernelPCA(kernel='rbf'). \
                                       fit_transform(X)
        plt.scatter(X_ter[:,0], X_ter[:,1], c=y, s=30,
                    cmap=plt.cm.rainbow)</pre></div><div class="mediaobject"><img src="images/4818OS_08_26.jpg" alt="How to do it..."/></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec301"/>How it works...</h2></div></div></div><p>Let's look at the <a id="id1398" class="indexterm"/>mathematical ideas behind PCA. This method is based on a matrix decomposition called<a id="id1399" class="indexterm"/> <a id="id1400" class="indexterm"/><span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>):</p><div class="mediaobject"><img src="images/4818OS_08_27.jpg" alt="How it works..."/></div><p>Here, <span class="emphasis"><em>X</em></span> is the <span class="emphasis"><em>(N,D)</em></span> data matrix, <span class="emphasis"><em>U</em></span> and <span class="emphasis"><em>V</em></span> are orthogonal matrices, and <span class="inlinemediaobject"><img src="images/4818OS_08_38.jpg" alt="How it works..."/></span>
<span class="emphasis"><em> </em></span>is a <span class="emphasis"><em>(N,D)</em></span> diagonal matrix.</p><p>PCA transforms <span class="emphasis"><em>X</em></span> into <span class="emphasis"><em>X'</em></span> defined by:</p><div class="mediaobject"><img src="images/4818OS_08_28.jpg" alt="How it works..."/></div><p>The diagonal elements of <span class="inlinemediaobject"><img src="images/4818OS_08_38.jpg" alt="How it works..."/></span>
<span class="emphasis"><em> </em></span>are the <span class="strong"><strong>singular values</strong></span><a id="id1401" class="indexterm"/> of <span class="emphasis"><em>X</em></span>. By convention, they are generally sorted in descending order. The columns of <span class="emphasis"><em>U</em></span> are orthonormal vectors called the <span class="strong"><strong>left singular vectors</strong></span><a id="id1402" class="indexterm"/> of <span class="emphasis"><em>X</em></span>. Therefore, the columns of <span class="emphasis"><em>X'</em></span> are the left singular vectors multiplied by the singular values.</p><p>In the end, PCA converts the initial set of observations, which are made of possibly correlated variables, into vectors of linearly uncorrelated variables called <a id="id1403" class="indexterm"/><span class="strong"><strong>principal components</strong></span>.</p><p>The first new<a id="id1404" class="indexterm"/> feature (or first component) is a transformation of all original features such that the <a id="id1405" class="indexterm"/>dispersion (variance) of the data points is the highest in that direction. In the subsequent principal components, the variance is decreasing. In other words, PCA gives us an alternative representation of our data where the new features are sorted according to how much they account for the variability of the points.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec302"/>There's more…</h2></div></div></div><p>Here are a few further references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Iris flower dataset<a id="id1406" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Iris_flower_data_set">http://en.wikipedia.org/wiki/Iris_flower_data_set</a></li><li class="listitem" style="list-style-type: disc">PCA on <a id="id1407" class="indexterm"/>Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Principal_component_analysis">http://en.wikipedia.org/wiki/Principal_component_analysis</a></li><li class="listitem" style="list-style-type: disc">SVD decomposition<a id="id1408" class="indexterm"/> on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Singular_value_decomposition">http://en.wikipedia.org/wiki/Singular_value_decomposition</a></li><li class="listitem" style="list-style-type: disc">Iris dataset example available at <a class="ulink" href="http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html">http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html</a></li><li class="listitem" style="list-style-type: disc">Decompositions in scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/decomposition.html">http://scikit-learn.org/stable/modules/decomposition.html</a></li><li class="listitem" style="list-style-type: disc">Unsupervised learning tutorial with scikit-learn<a id="id1409" class="indexterm"/> available at <a class="ulink" href="http://scikit-learn.org/dev/tutorial/statistical_inference/unsupervised_learning.html">http://scikit-learn.org/dev/tutorial/statistical_inference/unsupervised_learning.html</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec303"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Detecting hidden structures in a dataset with clustering</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Detecting hidden structures in a dataset with clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec82"/>Detecting hidden structures in a dataset with clustering</h1></div></div></div><p>A large part of unsupervised learning is devoted to the <span class="strong"><strong>clustering</strong></span><a id="id1410" class="indexterm"/> problem. The goal is to group similar points together in a totally unsupervised way. Clustering is a hard problem, as the very definition of <span class="strong"><strong>clusters</strong></span><a id="id1411" class="indexterm"/> (or <span class="strong"><strong>groups</strong></span>) is not necessarily well posed. In most datasets, stating that two points should belong to the same cluster may be context-dependent or even subjective.</p><p>There are many<a id="id1412" class="indexterm"/> clustering algorithms. We will see a few <a id="id1413" class="indexterm"/>of them in this recipe, applied to a toy<a id="id1414" class="indexterm"/> example.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec304"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import the libraries:<div class="informalexample"><pre class="programlisting">In [1]: from itertools import permutations
        import numpy as np
        import sklearn
        import sklearn.decomposition as dec
        import sklearn.cluster as clu
        import sklearn.datasets as ds
        import sklearn.grid_search as gs
        import matplotlib.pyplot as plt
        %matplotlib inline</pre></div></li><li class="listitem">Let's generate a random dataset with three clusters:<div class="informalexample"><pre class="programlisting">In [2]: X, y = ds.make_blobs(n_samples=200, n_features=2,  
                             centers=3)</pre></div></li><li class="listitem">We need a couple of functions to relabel and display the results of the clustering algorithms:<div class="informalexample"><pre class="programlisting">In [3]: def relabel(cl):
            """Relabel a clustering with three clusters
            to match the original classes."""
            if np.max(cl) != 2:
                return cl
            perms = np.array(list(permutations((0, 1, 2))))
            i = np.argmin([np.sum(np.abs(perm[cl] - y))
                           for perm in perms])
            p = perms[i]
            return p[cl]
In [4]: def display_clustering(labels, title):
            """Plot the data points with the cluster   
            colors."""
            # We relabel the classes when there are 3 
            # clusters.
            labels = relabel(labels)
            # Display the points with the true labels on 
            # the left, and with the clustering labels on
            # the right.
            for i, (c, title) in enumerate(zip(
                    [y, labels], ["True labels", title])):
                plt.subplot(121 + i)
                plt.scatter(X[:,0], X[:,1], c=c, s=30, 
                            linewidths=0,
                            cmap=plt.cm.rainbow)
                plt.xticks([]); plt.yticks([])
                plt.title(title)</pre></div></li><li class="listitem">Now, we <a id="id1415" class="indexterm"/>cluster the dataset with the<a id="id1416" class="indexterm"/> <span class="strong"><strong>K-means</strong></span> algorithm, a <a id="id1417" class="indexterm"/>classic and<a id="id1418" class="indexterm"/> simple clustering algorithm:<div class="informalexample"><pre class="programlisting">In [5]: km = clu.KMeans()
        km.fit(X)
        display_clustering(km.labels_, "KMeans")</pre></div><div class="mediaobject"><img src="images/4818OS_08_29.jpg" alt="How to do it..."/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip32"/>Tip</h3><p>If you're reading the printed version of this book, you might not be able to distinguish the colors. You will find the colored images on the book's website.</p></div></div></li><li class="listitem">This algorithm needs to know the number of clusters at initialization time. In general, however, we do not necessarily know the number of clusters in the dataset. Here, let's try with <code class="literal">n_clusters=3</code> (that's cheating, because we happen to know that there are 3 clusters!):<div class="informalexample"><pre class="programlisting">In [6]: km = clu.KMeans(n_clusters=3)
        km.fit(X)
        display_clustering(km.labels_, "KMeans(3)")</pre></div><div class="mediaobject"><img src="images/4818OS_08_30.jpg" alt="How to do it..."/></div></li><li class="listitem">Let's try a few <a id="id1419" class="indexterm"/>other clustering algorithms <a id="id1420" class="indexterm"/>implemented in scikit-learn. The simplicity of the API makes it really easy to try different methods; it is just a<a id="id1421" class="indexterm"/> matter of changing the name of the class:<div class="informalexample"><pre class="programlisting">In [7]: plt.subplot(231)
        plt.scatter(X[:,0], X[:,1], c=y, s=30,
                    linewidths=0, cmap=plt.cm.rainbow)
        plt.xticks([]); plt.yticks([])
        plt.title("True labels")
        for i, est in enumerate([clu.SpectralClustering(3),
                                 clu.AgglomerativeClustering(3),
                                 clu.MeanShift(),
                                 clu.AffinityPropagation(),
                                 clu.DBSCAN()]):
            est.fit(X)
            c = relabel(est.labels_)
            plt.subplot(232 + i)
            plt.scatter(X[:,0], X[:,1], c=c, s=30,
                        linewidths=0, cmap=plt.cm.rainbow)
            plt.xticks([]); plt.yticks([])
            plt.title(est.__class__.__name__)</pre></div><div class="mediaobject"><img src="images/4818OS_08_31.jpg" alt="How to do it..."/></div><p>The first two algorithms required the number of clusters as input. The next two did not, but they were able to find the right number: 3. The last one failed at finding the correct number of clusters (this is <span class="emphasis"><em>overclustering</em></span>—too many clusters have been found).</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec305"/>How it works...</h2></div></div></div><p>The K-means clustering algorithm consists of partitioning the data points <span class="emphasis"><em>x<sub>j</sub></em></span> into <span class="emphasis"><em>K</em></span> clusters <span class="emphasis"><em>S<sub>i</sub></em></span> so as to minimize the within-cluster sum of squares:</p><div class="mediaobject"><img src="images/4818OS_08_32.jpg" alt="How it works..."/></div><p>Here, <span class="inlinemediaobject"><img src="images/4818OS_08_39.jpg" alt="How it works..."/></span>
<span class="emphasis"><em> </em></span>is the center of the cluster <span class="emphasis"><em>i</em></span> (average of all points in <span class="emphasis"><em>S<sub>i</sub></em></span>).</p><p>Although it is very hard to <a id="id1422" class="indexterm"/>solve this problem exactly, approximation algorithms exist. A popular one is <span class="strong"><strong>Lloyd's </strong></span><a id="id1423" class="indexterm"/><span class="strong"><strong>algorithm</strong></span>. It consists of starting from an initial set of <span class="emphasis"><em>K</em></span> means <span class="inlinemediaobject"><img src="images/4818OS_08_39.jpg" alt="How it works..."/></span>
<span class="emphasis"><em> </em></span>and<a id="id1424" class="indexterm"/> alternating between two<a id="id1425" class="indexterm"/> steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In the <span class="emphasis"><em>assignment</em></span> step, the points are assigned to the cluster associated to the closest mean</li><li class="listitem" style="list-style-type: disc">In the <span class="emphasis"><em>update</em></span> step, the means are recomputed from the last assignments</li></ul></div><p>The algorithm converges to a solution that is not guaranteed to be optimal.</p><p>The <span class="strong"><strong>expectation-maximization algorithm</strong></span><a id="id1426" class="indexterm"/> can be seen as a probabilistic version of the K-means algorithm. It is implemented in the <code class="literal">mixture</code> module of scikit-learn.</p><p>The other clustering algorithms used in this recipe are explained in the scikit-learn documentation. There is no clustering algorithm that works uniformly better than all the others, and every algorithm has its strengths and weaknesses. You will find more details in the references in the next section.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec306"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <a id="id1427" class="indexterm"/>K-means clustering algorithm on Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/K-means_clustering">http://en.wikipedia.org/wiki/K-means_clustering</a></li><li class="listitem" style="list-style-type: disc">The expectation-maximization algorithm on <a id="id1428" class="indexterm"/>Wikipedia, available at <a class="ulink" href="http://en.wikipedia.org/wiki/Expectation-maximization_algorithm">http://en.wikipedia.org/wiki/Expectation-maximization_algorithm</a></li><li class="listitem" style="list-style-type: disc">Clustering in <a id="id1429" class="indexterm"/>scikit-learn's documentation, available at <a class="ulink" href="http://scikit-learn.org/stable/modules/clustering.html">http://scikit-learn.org/stable/modules/clustering.html</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec307"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Reducing the dimensionality of a dataset with principal component analysis</em></span> recipe</li></ul></div></div></div></div>
</body></html>