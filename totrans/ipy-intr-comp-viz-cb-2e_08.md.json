["```py\n    In [1]: import numpy as np\n            import scipy.stats as st\n            import sklearn.linear_model as lm\n            import matplotlib.pyplot as plt\n            %matplotlib inline\n    ```", "```py\n    In [2]: f = lambda x: np.exp(3 * x)\n    ```", "```py\n    In [3]: x_tr = np.linspace(0., 2, 200)\n            y_tr = f(x_tr)\n    ```", "```py\n    In [4]: x = np.array([0, .1, .2, .5, .8, .9, 1])\n            y = f(x) + np.random.randn(len(x))\n    ```", "```py\n    In [5]: plt.plot(x_tr[:100], y_tr[:100], '--k')\n            plt.plot(x, y, 'ok', ms=10)\n    ```", "```py\n    In [6]: # We create the model.\n            lr = lm.LinearRegression()\n            # We train the model on our training dataset.\n            lr.fit(x[:, np.newaxis], y)\n            # Now, we predict points with our trained model.\n            y_lr = lr.predict(x_tr[:, np.newaxis])\n    ```", "```py\n    In [7]: plt.plot(x_tr, y_tr, '--k')\n            plt.plot(x_tr, y_lr, 'g')\n            plt.plot(x, y, 'ok', ms=10)\n            plt.xlim(0, 1)\n            plt.ylim(y.min()-1, y.max()+1)\n            plt.title(\"Linear regression\")\n    ```", "```py\n    In [8]: lrp = lm.LinearRegression()\n            plt.plot(x_tr, y_tr, '--k')\n            for deg in [2, 5]:\n                lrp.fit(np.vander(x, deg + 1), y)\n                y_lrp = lrp.predict(np.vander(x_tr, deg + 1))\n                plt.plot(x_tr, y_lrp,\n                         label='degree ' + str(deg))\n                plt.legend(loc=2)\n                plt.xlim(0, 1.4)\n                plt.ylim(-10, 40)\n                # Print the model's coefficients.\n                print(' '.join(['%.2f' % c for c in \n                                lrp.coef_]))\n            plt.plot(x, y, 'ok', ms=10)\n            plt.title(\"Linear regression\")\n    25.00 -8.57 0.00\n    -132.71 296.80 -211.76 72.80 -8.68 0.00\n    ```", "```py\n    In [9]: ridge = lm.RidgeCV()\n            plt.plot(x_tr, y_tr, '--k')\n\n            for deg in [2, 5]:\n                ridge.fit(np.vander(x, deg + 1), y);\n                y_ridge = ridge.predict(np.vander(x_tr, deg+1))\n                plt.plot(x_tr, y_ridge,\n                         label='degree ' + str(deg))\n                plt.legend(loc=2)\n                plt.xlim(0, 1.5)\n                plt.ylim(-5, 80)\n                # Print the model's coefficients.\n                print(' '.join(['%.2f' % c \n                                for c in ridge.coef_]))\n\n            plt.plot(x, y, 'ok', ms=10)\n            plt.title(\"Ridge regression\")\n    11.36 4.61 0.00\n    2.84 3.54 4.09 4.14 2.67 0.00\n    ```", "```py\n    In [1]: import numpy as np\n            import pandas as pd\n            import sklearn\n            import sklearn.linear_model as lm\n            import sklearn.cross_validation as cv\n            import sklearn.grid_search as gs\n            import matplotlib.pyplot as plt\n            %matplotlib inline\n    ```", "```py\n    In [2]: train = pd.read_csv('data/titanic_train.csv')\n            test = pd.read_csv('data/titanic_test.csv')\n    In [3]: train[train.columns[[2,4,5,1]]].head()\n    Out[3]:    \n       Pclass     Sex  Age  Survived\n    0       3    male   22         0\n    1       1  female   38         1\n    2       3  female   26         1\n    3       1  female   35         1\n    4       3    male   35         0\n    ```", "```py\n    In [4]: data = train[['Sex', 'Age', 'Pclass', 'Survived']].copy()\n            data['Sex'] = data['Sex'] == 'female'\n            data = data.dropna()\n    ```", "```py\n    In [5]: data_np = data.astype(np.int32).values\n            X = data_np[:,:-1]\n            y = data_np[:,-1]\n    ```", "```py\n    In [6]: # We define a few boolean vectors.\n            female = X[:,0] == 1\n            survived = y == 1\n            # This vector contains the age of the passengers.\n            age = X[:,1]\n            # We compute a few histograms.\n            bins_ = np.arange(0, 81, 5)\n            S = {'male': np.histogram(age[survived & ~female], \n                                      bins=bins_)[0],\n                 'female': np.histogram(age[survived & female], \n                                        bins=bins_)[0]}\n            D = {'male': np.histogram(age[~survived & ~female], \n                                      bins=bins_)[0],\n                 'female': np.histogram(age[~survived & \n                                            female], \n                                        bins=bins_)[0]}\n    In [7]: # We now plot the data.\n            bins = bins_[:-1]\n            for i, sex, color in zip((0, 1),\n                                     ('male', 'female'),\n                                     ('#3345d0', '#cc3dc0')):\n                plt.subplot(121 + i)\n                plt.bar(bins, S[sex], bottom=D[sex],\n                        color=color,\n                        width=5, label='survived')\n                plt.bar(bins, D[sex], color='k', width=5,\n                        label='died')\n                plt.xlim(0, 80)\n                plt.grid(None)\n                plt.title(sex + \" survival\")\n                plt.xlabel(\"Age (years)\")\n                plt.legend()\n    ```", "```py\n    In [8]: # We split X and y into train and test datasets.\n            (X_train, X_test, y_train, \n            y_test) = cv.train_test_split(X, y, test_size=.05)\n    In [9]: # We instanciate the classifier.\n            logreg = lm.LogisticRegression()\n    ```", "```py\n    In [10]: logreg.fit(X_train, y_train)\n             y_predicted = logreg.predict(X_test)\n    ```", "```py\n    In [11]: plt.imshow(np.vstack((y_test, y_predicted)),\n                        interpolation='none', cmap='bone')\n             plt.xticks([]); plt.yticks([])\n             plt.title((\"Actual and predicted survival \"\n                        \"outcomes on the test set\"))\n    ```", "```py\n    In [12]: cv.cross_val_score(logreg, X, y)\n    Out[12]: array([ 0.78661088,  0.78991597,  0.78059072])\n    ```", "```py\n    In [13]: grid = gs.GridSearchCV(logreg, \n                                {'C': np.logspace(-5, 5, 50)})\n             grid.fit(X_train, y_train)\n             grid.best_params_\n    Out[13]: {'C':  5.35}\n    ```", "```py\n    In [14]: cv.cross_val_score(grid.best_estimator_, X, y)\n    Out[14]: array([ 0.78661088,  0.79831933,  0.78481013])\n    ```", "```py\n    In [1]: import numpy as np\n            import sklearn\n            import sklearn.datasets as ds\n            import sklearn.cross_validation as cv\n            import sklearn.neighbors as nb\n            import matplotlib.pyplot as plt\n            %matplotlib inline\n    ```", "```py\n    In [2]: digits = ds.load_digits()\n            X = digits.data\n            y = digits.target\n            print((X.min(), X.max()))\n            print(X.shape)\n    0.0 16.0\n    (1797L, 64L)\n    ```", "```py\n    In [3]: nrows, ncols = 2, 5\n            plt.gray()\n            for i in range(ncols * nrows):\n                ax = plt.subplot(nrows, ncols, i + 1)\n                ax.matshow(digits.images[i,...])\n                plt.xticks([]); plt.yticks([])\n                plt.title(digits.target[i])\n    ```", "```py\n    In [4]: (X_train, X_test, y_train, \n             y_test) = cv.train_test_split(X, y, test_size=.25)\n    In [5]: knc = nb.KNeighborsClassifier()\n    In [6]: knc.fit(X_train, y_train);\n    ```", "```py\n    In [7]: knc.score(X_test, y_test)\n    Out[7]: 0.98888888888888893\n    ```", "```py\n    In [8]: # Let's draw a 1.\n            one = np.zeros((8, 8))\n            one[1:-1, 4] = 16  # The image values are \n                               # in [0,16].\n            one[2, 3] = 16\n    In [9]: plt.imshow(one, interpolation='none')\n            plt.grid(False)\n            plt.xticks(); plt.yticks()\n            plt.title(\"One\")\n    ```", "```py\n    In [10]: knc.predict(one.ravel())\n    Out[10]: array([1])\n    ```", "```py\n    In [1]: import numpy as np\n            import pandas as pd\n            import sklearn\n            import sklearn.cross_validation as cv\n            import sklearn.grid_search as gs\n            import sklearn.feature_extraction.text as text\n            import sklearn.naive_bayes as nb\n            import matplotlib.pyplot as plt\n            %matplotlib inline\n    ```", "```py\n    In [2]: df = pd.read_csv(\"data/troll.csv\")\n    ```", "```py\n    In [3]: df[['Insult', 'Comment']].tail()\n          Insult                                            Comment\n    3942       1  \"you are both morons and that is...\"\n    3943       0  \"Many toolbars include spell check...\n    3944       0  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\n...\n    3945       0  \"How about Felix? He is sure turning into...\n    3946       0  \"You're all upset, defending this hipster...\n    ```", "```py\n    In [4]: y = df['Insult']\n    ```", "```py\n    In [5]: tf = text.TfidfVectorizer()\n            X = tf.fit_transform(df['Comment'])\n            print(X.shape)\n    (3947, 16469)\n    ```", "```py\n    In [6]: print((\"Each sample has ~{0:.2f}% non-zero\"\n                   \"features.\").format(\n               100 * X.nnz / float(X.shape[0] * X.shape[1])))\n    Each sample has ~0.15% non-zero features.\n    ```", "```py\n    In [7]: (X_train, X_test, y_train, \n             y_test) = cv.train_test_split(X, y,\n                                           test_size=.2)\n    ```", "```py\n    In [8]: bnb = gs.GridSearchCV(nb.BernoulliNB(), \n                                  param_grid={\n                            'alpha': np.logspace(-2., 2., 50)})\n            bnb.fit(X_train, y_train)\n    ```", "```py\n    In [9]: bnb.score(X_test, y_test)\n    Out[9]: 0.76455696202531642\n    ```", "```py\n    In [10]: # We first get the words corresponding \n             # to each feature.\n             names = np.asarray(tf.get_feature_names())\n             # Next, we display the 50 words with the largest\n             # coefficients.\n             print(','.join(names[np.argsort(\n                 bnb.best_estimator_.coef_[0,:])[::-1][:50]]))\n    you,are,your,to,the,and,of,that,is,it,in,like,on,have,for,not,re,just,an,with,so,all,***,***be,get,***,***up,this,what,xa0,don,***,***go,no,do,can,but,***,***or,as,if,***,***who,know,about,because,here,***,***me,was\n    ```", "```py\n    In [11]: print(bnb.predict(tf.transform([\n                 \"I totally agree with you.\",\n                 \"You are so stupid.\",\n                 \"I love you.\"\n                 ])))\n    [0 1 1]\n    ```", "```py\n    In [1]: import numpy as np\n            import pandas as pd\n            import sklearn\n            import sklearn.datasets as ds\n            import sklearn.cross_validation as cv\n            import sklearn.grid_search as gs\n            import sklearn.svm as svm\n            import matplotlib.pyplot as plt\n            %matplotlib inline\n    ```", "```py\n    In [2]: X = np.random.randn(200, 2)\n            y = X[:, 0] + X[:, 1] > 1\n    ```", "```py\n    In [3]: # We train the classifier.\n            est = svm.LinearSVC()\n            est.fit(X, y)\n    ```", "```py\n    In [4]: # We generate a grid in the square [-3,3 ]^2.\n            xx, yy = np.meshgrid(np.linspace(-3, 3, 500),\n                                 np.linspace(-3, 3, 500))\n            # This function takes a SVM estimator as input.\n            def plot_decision_function(est):\n                # We evaluate the decision function on the\n                # grid.\n                Z = est.decision_function(np.c_[xx.ravel(), \n                                                yy.ravel()])\n                Z = Z.reshape(xx.shape)\n                cmap = plt.cm.Blues\n                # We display the decision function on the grid.\n                plt.imshow(Z,\n                           extent=(xx.min(), xx.max(), \n                                   yy.min(), yy.max()),\n                           aspect='auto', origin='lower', \n                           cmap=cmap)\n                # We display the boundaries.\n                plt.contour(xx, yy, Z, levels=[0],                \n                            linewidths=2,\n                            colors='k')\n                # We display the points with their true labels.\n                plt.scatter(X[:, 0], X[:, 1], s=30, c=.5+.5*y, \n                            lw=1, cmap=cmap, vmin=0, vmax=1)\n                plt.axhline(0, color='k', ls='--')\n                plt.axvline(0, color='k', ls='--')\n                plt.xticks(())\n                plt.yticks(())\n                plt.axis([-3, 3, -3, 3])\n    ```", "```py\n    In [5]: plot_decision_function(est)\n            plt.title(\"Linearly separable, linear SVC\")\n    ```", "```py\n    In [6]: y = np.logical_xor(X[:, 0]>0, X[:, 1]>0)\n            # We train the classifier.\n            est = gs.GridSearchCV(svm.LinearSVC(), \n                              {'C': np.logspace(-3., 3., 10)})\n            est.fit(X, y)\n            print(\"Score: {0:.1f}\".format(\n                          v.cross_val_score(est, X, y).mean()))\n            # Plot the decision function.\n            plot_decision_function(est)\n            plt.title(\"XOR, linear SVC\")\n    Score: 0.6\n    ```", "```py\n    In [7]: y = np.logical_xor(X[:, 0]>0, X[:, 1]>0)\n            est = gs.GridSearchCV(svm.SVC(), \n                         {'C': np.logspace(-3., 3., 10),\n                          'gamma': np.logspace(-3., 3., 10)})\n            est.fit(X, y)\n            print(\"Score: {0:.3f}\".format(\n                  cv.cross_val_score(est, X, y).mean()))\n            plot_decision_function(est.best_estimator_)\n            plt.title(\"XOR, non-linear SVC\")\n    Score: 0.975\n    ```", "```py\n    In [1]: import numpy as np\n            import sklearn as sk\n            import sklearn.datasets as skd\n            import sklearn.ensemble as ske\n            import matplotlib.pyplot as plt\n            %matplotlib inline\n    ```", "```py\n    In [2]: data = skd.load_boston()\n    ```", "```py\n    In [3]: reg = ske.RandomForestRegressor()\n    ```", "```py\n    In [4]: X = data['data']\n            y = data['target']\n    ```", "```py\n    In [5]: reg.fit(X, y)\n    ```", "```py\n    In [6]: fet_ind = np.argsort(reg.feature_importances_) \\\n                                                        [::-1]\n            fet_imp = reg.feature_importances_[fet_ind]\n    ```", "```py\n    In [7]: ax = plt.subplot(111)\n            plt.bar(np.arange(len(fet_imp)), \n                    fet_imp, width=1, lw=2)\n            plt.grid(False)\n            ax.set_xticks(np.arange(len(fet_imp))+.5)\n            ax.set_xticklabels(data['feature_names'][fet_ind])\n            plt.xlim(0, len(fet_imp))\n    ```", "```py\n    In [8]: plt.scatter(X[:,-1], y)\n            plt.xlabel('LSTAT indicator')\n            plt.ylabel('Value of houses (k$)')\n    ```", "```py\n    In [1]: import numpy as np\n            import sklearn\n            import sklearn.decomposition as dec\n            import sklearn.datasets as ds\n            import matplotlib.pyplot as plt\n            %matplotlib inline\n    ```", "```py\n    In [2]: iris = ds.load_iris()\n            X = iris.data\n            y = iris.target\n            print(X.shape)\n    (150L, 4L)\n    ```", "```py\n    In [3]: plt.scatter(X[:,0], X[:,1], c=y,\n                        s=30, cmap=plt.cm.rainbow)\n    ```", "```py\n    In [4]: X_bis = dec.PCA().fit_transform(X)\n    ```", "```py\n    In [5]: plt.scatter(X_bis[:,0], X_bis[:,1], c=y,\n                        s=30, cmap=plt.cm.rainbow)\n    ```", "```py\n    In [6]: X_ter = dec.KernelPCA(kernel='rbf'). \\\n                                           fit_transform(X)\n            plt.scatter(X_ter[:,0], X_ter[:,1], c=y, s=30,\n                        cmap=plt.cm.rainbow)\n    ```", "```py\n    In [1]: from itertools import permutations\n            import numpy as np\n            import sklearn\n            import sklearn.decomposition as dec\n            import sklearn.cluster as clu\n            import sklearn.datasets as ds\n            import sklearn.grid_search as gs\n            import matplotlib.pyplot as plt\n            %matplotlib inline\n    ```", "```py\n    In [2]: X, y = ds.make_blobs(n_samples=200, n_features=2,  \n                                 centers=3)\n    ```", "```py\n    In [3]: def relabel(cl):\n                \"\"\"Relabel a clustering with three clusters\n                to match the original classes.\"\"\"\n                if np.max(cl) != 2:\n                    return cl\n                perms = np.array(list(permutations((0, 1, 2))))\n                i = np.argmin([np.sum(np.abs(perm[cl] - y))\n                               for perm in perms])\n                p = perms[i]\n                return p[cl]\n    In [4]: def display_clustering(labels, title):\n                \"\"\"Plot the data points with the cluster   \n                colors.\"\"\"\n                # We relabel the classes when there are 3 \n                # clusters.\n                labels = relabel(labels)\n                # Display the points with the true labels on \n                # the left, and with the clustering labels on\n                # the right.\n                for i, (c, title) in enumerate(zip(\n                        [y, labels], [\"True labels\", title])):\n                    plt.subplot(121 + i)\n                    plt.scatter(X[:,0], X[:,1], c=c, s=30, \n                                linewidths=0,\n                                cmap=plt.cm.rainbow)\n                    plt.xticks([]); plt.yticks([])\n                    plt.title(title)\n    ```", "```py\n    In [5]: km = clu.KMeans()\n            km.fit(X)\n            display_clustering(km.labels_, \"KMeans\")\n    ```", "```py\n    In [6]: km = clu.KMeans(n_clusters=3)\n            km.fit(X)\n            display_clustering(km.labels_, \"KMeans(3)\")\n    ```", "```py\n    In [7]: plt.subplot(231)\n            plt.scatter(X[:,0], X[:,1], c=y, s=30,\n                        linewidths=0, cmap=plt.cm.rainbow)\n            plt.xticks([]); plt.yticks([])\n            plt.title(\"True labels\")\n            for i, est in enumerate([clu.SpectralClustering(3),\n                                     clu.AgglomerativeClustering(3),\n                                     clu.MeanShift(),\n                                     clu.AffinityPropagation(),\n                                     clu.DBSCAN()]):\n                est.fit(X)\n                c = relabel(est.labels_)\n                plt.subplot(232 + i)\n                plt.scatter(X[:,0], X[:,1], c=c, s=30,\n                            linewidths=0, cmap=plt.cm.rainbow)\n                plt.xticks([]); plt.yticks([])\n                plt.title(est.__class__.__name__)\n    ```"]