- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithms and How to Apply Them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we have already looked at a variety of ways to create pandas data
    structures and select/assign data within them and have subsequently seen how to
    store those structures in common formats. These features alone can make pandas
    a powerful tool in the realm of data exchange, but we are still just scratching
    the surface of what pandas can offer.
  prefs: []
  type: TYPE_NORMAL
- en: A core component of data analysis and computing in general is the application
    of *algorithms*, which describe a sequence of steps the computer should take to
    process data. In their simplistic form, common data algorithms build upon basic
    arithmetic (for example, “sum this column”), but scale out to any sequence of
    steps that you may need for your custom calculations.
  prefs: []
  type: TYPE_NORMAL
- en: As you will see in this chapter, pandas provides many common data algorithms
    out of the box, but also gives you a robust framework through which you can compose
    and apply your own algorithms. The algorithms pandas provides out of the box would
    be faster than anything you can write by hand in Python, and as you progress in
    your data journey, you will usually find that clever use of these algorithms can
    cover a vast amount of data processing needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic `pd.Series` arithmetic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic `pd.DataFrame` arithmetic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot encoding with `pd.get_dummies`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaining with `.pipe`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the lowest-budget movies from the top 100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating a trailing stop order price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the baseball players best at…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding which position scores the most per team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic pd.Series arithmetic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest place to start when exploring pandas algorithms is with a `pd.Series`,
    given it is also the most basic structure provided by the pandas library. Basic
    arithmetic will cover the operations of addition, subtraction, multiplication,
    and division, and, as you will see in this section, pandas offers two ways to
    perform these. The first approach allows pandas to work with the `+`, `-`, `*`,
    and `/` operators built into the Python language, which is an intuitive way for
    new users coming to the library to pick up the tool. However, to cover features
    specific to data analysis not covered by the Python language, and to support the
    *Chaining with .pipe* approach that we will cover later in this chapter, pandas
    also offers `pd.Series.add`, `pd.Series.sub`, `pd.Series.mul`, and `pd.Series.div`,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The pandas library goes to great lengths to keep its API consistent across all
    data structures, so you will see that the knowledge from this section can be easily
    transferred over to the `pd.DataFrame` structure, with the only difference being
    that a `pd.Series` is one-dimensional while a `pd.DataFrame` is two-dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a simple `pd.Series` from a Python `range` expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To establish terminology, let’s briefly consider an expression like `a + b`.
    In such an expression, we are using a *binary operator* (`+`). The term *binary*
    refers to the fact that you need to add two things together for this expression
    to make sense, that is, it wouldn’t make sense to just have an expression like
    `a +`. Those two “things” are technically considered *operands*; so, with `a +
    b`, we have a left operand of `a` and a right operand of `b`.
  prefs: []
  type: TYPE_NORMAL
- en: With one of the operands being a `pd.Series`, the most basic algorithmic expression
    in pandas would encompass the other operand being a *scalar*, that is to say,
    just a single value. When that occurs, the scalar value is *broadcast* to each
    element of the `pd.Series` to apply the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we wanted to add the number 42 to each and every element of
    our `pd.Series`, we could simply express that as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The pandas library is able to take the addition expression and apply it to our
    `pd.Series` in a *vectorized* manner (i.e., the number 42 gets applied to all
    values at once without requiring users to resort to a `for` loop in Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'Subtraction may be expressed naturally using the `-` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, multiplication may be expressed with the `*` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'By now, you can probably surmise that division is expressed with the `/` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also perfectly valid for the two operands to be a `pd.Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in the introduction of this section, while the built-in Python
    operators are commonly used and viable in most cases, pandas still offers dedicated
    methods for `pd.Series.add`, `pd.Series.sub`, `pd.Series.mul`, and `pd.Series.div`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The advantage of `pd.Series.add` over the built-in operator is that it accepts
    an optional `fill_value=` argument to handle missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Later in this chapter, you will also be introduced to chaining with `.pipe`,
    which chains most naturally with the pandas methods and not with the built-in
    Python operators.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When both operands in your expression are `pd.Series` objects together, it is
    important to note that pandas will align on the row labels. This alignment behavior
    is considered a feature, but can also be surprising to newcomers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see why this matters, let’s start with two `pd.Series` objects that have
    an identical row index. When we try to add these together, we get a rather unsurprising
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'But what happens when the row index values are not identical? A simple case
    may involve adding two `pd.Series` objects together, where one `pd.Series` uses
    a row index that is a subset of the other. You can see this with `ser3` in the
    following code, which only has 2 values and uses the default `pd.RangeIndex` with
    values of `[0, 1]`. When added together with `ser1`, we still get a 3-element
    `pd.Series` in return, but values are only added where the row index labels can
    be aligned from both `pd.Series` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s take a look at what happens when two `pd.Series` objects of the same
    length get added together, but the row index values are different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'For an even more extreme case, let’s consider the situation where one `pd.Series`
    has row index values that are non-unique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have a background in SQL, the behavior of pandas here is akin to a `FULL
    OUTER JOIN` in a database. Every label from each row index gets included in the
    output, with pandas matching up the labels that can be seen in both `pd.Series`
    objects. This can be directly replicated in a database like PostgreSQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If you were to run this snippet directly in PostgreSQL, you would get back
    the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Ignoring the ordering difference, you can see that the database gives us back
    all of the unique `index` values from the combinations of `[0, 1, 2]` and `[0,
    1, 1]`, alongside any associated `val1` and `val2` values. Even though `ser1`
    only had one `index` value of `1`, that same value appeared twice in the `index`
    column in `ser5`. The `FULL OUTER JOIN` therefore shows both `val2` values from
    `ser5` (`4` and `8`), while duplicating the `val1` value originating from `ser1`
    (`1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were to subsequently add `val1` and `val2` together in the database,
    you would get back a result that matches the output of `ser1 + ser5`, sparing
    the fact that the database may choose a different order for its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Basic pd.DataFrame arithmetic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having now covered basic `pd.Series` arithmetic, you will find that the corresponding
    `pd.DataFrame` arithmetic operations are practically identical, with the lone
    exception being that our algorithms now work in two dimensions instead of just
    one. In doing so, the pandas API makes it easy to interpret data regardless of
    its shape, and without requiring users to write loops to interact with data. This
    helps significantly reduce developer effort and helps you write faster code –
    a win-win for developers.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a small 3x3 `pd.DataFrame` using random numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Much like a `pd.Series`, a `pd.DataFrame` also supports built-in binary operators
    with a scalar argument. Here is a simplistic addition operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is a simplistic multiplication operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also perform arithmetic with a `pd.Series`. By default, each row label
    in the `pd.Series` is searched for and aligned against the columns of the `pd.DataFrame`.
    To illustrate, let’s create a small `pd.Series` whose index labels match the column
    labels of `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If you were to try and add this to our `pd.DataFrame`, it would take the value
    of `col1` in the `pd.Series` and add it to every element in the `col1` column
    of the `pd.DataFrame`, repeating for each index entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In cases where the row labels of the `pd.Series` do not match the column labels
    of the `pd.DataFrame`, you may end up with missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: If you would like to control how `pd.Series` and `pd.DataFrame` align, you can
    use the `axis=` parameter of methods like `pd.DataFrame.add`, `pd.DataFrame.sub`,
    `pd.DataFrame.mul`, and `pd.DataFrame.div`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see this in action by creating a new `pd.Series` using row labels that
    align better with the row labels of our `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Specifying `df.add(ser, axis=0)` will match up the row labels from both the
    `pd.Series` and `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use two `pd.DataFrame` arguments as the operands of addition,
    subtraction, multiplication, and division. Here is how to multiply two `pd.DataFrame`
    objects together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Of course, when doing this, you still need to be aware of the index alignment
    rules – items are always aligned by label and not by position!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a new 3x3 `pd.DataFrame` with different row and column labels
    to show this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Attempting to add this to our previous `pd.DataFrame` will generate a row index
    with labels `["row1", "row2", "row3", 0, 1, 2]` and a column index with labels
    `["col1", "col2", "col3", 0, 1, 2]`. Because no labels could be aligned, everything
    comes back as a missing value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Aggregations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aggregations (also referred to as *reductions*) help you to reduce multiple
    values from a series of values down to a single value. Even if the technical term
    is new to you, you have no doubt encountered many aggregations in your data journey.
    Things like the *count* of records, the *sum* or sales, or the *average* price
    are all very common aggregations.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will explore many of the aggregations built into pandas,
    while also forming an understanding of how these aggregations are applied. Most
    analysis you will do throughout your data journey involves taking large datasets
    and aggregating the values therein into results that your audience can consume.
    Executives at most companies are not interested in receiving a data dump of transactions,
    they just want to know the sum, min, max, mean, and so on of values within those
    transactions. As such, effective use and application of aggregations is a key
    component to converting your complex data transformation pipelines into simple
    outputs that others can use and act upon.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many basic aggregations are implemented as methods directly on the `pd.Series`
    object, which makes it trivial to calculate commonly desired outputs like the
    `count`, `sum`, `max`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To kick off this recipe, let’s once again start with a `pd.Series` containing
    random numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The pandas library provides methods for many commonly used aggregations, like
    `pd.Series.count`, `pd.Series.mean`, `pd.Series.std`, `pd.Series.min`, `pd.Series.max`,
    and `pd.Series.sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of calling those methods directly, a more generic way to invoke these
    aggregations would be to use `pd.Series.agg`, providing the name of the aggregation
    you would like to perform as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'An advantage using `pd.Series.agg` is that it can perform multiple aggregations
    for you. For example, if you wanted to calculate the minimum and maximum of a
    field in one step, you could do this by providing a list to `pd.Series.agg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Aggregating a `pd.Series` is straightforward because there is only one dimension
    to be aggregated. With a `pd.DataFrame`, there are two possible dimensions to
    aggregate along, so you have a few more considerations as an end user of the library.
  prefs: []
  type: TYPE_NORMAL
- en: 'To walk through this, let’s go ahead and create a `pd.DataFrame` with random
    numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, invoking an aggregation using a built-in method like `pd.DataFrame.sum`
    will apply *along the columns*, meaning each column is individually aggregated.
    After that, pandas will display the result of each column’s aggregation as an
    entry in a `pd.Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'If you would like to aggregate data in each row, you can specify the `axis=1`
    argument, with the caveat being that pandas is way more optimized for `axis=0`
    operations, so this has a chance of being *significantly slower* than aggregating
    columns. Even still, it is a rather unique feature of pandas that can be useful
    when performance is not the main concern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Much like a `pd.Series`, a `pd.DataFrame` has a `.agg` method, which can be
    used to apply multiple aggregations at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the examples covered in the *How to do it* section, we passed functions
    as strings like `min` and `max` to `.agg`. This is great for simple functions,
    but for more complex cases, you can also pass in callable arguments. Each callable
    should accept a single argument `pd.Series` and reduce down to a scalar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrary to *aggregations*, transformations do not reduce an array of values
    to a single value but, rather, maintain the shape of the calling object. This
    particular recipe may seem rather mundane coming from the previous section on
    aggregations, but transformations and aggregations will end up being very complementary
    tools to calculate things like the “% total of group” later in the cookbook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a small `pd.Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Much like we saw with `pd.Series.agg` before, `pd.Series.transform` can accept
    a list of functions to apply. However, whereas `pd.Series.agg` expected these
    functions to return a single value, `pd.Series.transform` expects these functions
    to return a `pd.Series` with the same index and shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Much like `pd.DataFrame.agg` would *aggregate* each column by default, `pd.DataFrame.transform`
    will *transform* each column by default. Let’s create a small `pd.DataFrame` to
    see this in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Sparing implementation details, calling something like `df.transform("abs")`
    will apply the absolute value function to each column individually before piecing
    back together the result as a `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'If you were to pass multiple transformation functions to `pd.DataFrame.transform`,
    you will end up with a `pd.MultiIndex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the introduction to this recipe, transformations and aggregations
    can work naturally together alongside the `GroupBy` concept, which will be covered
    in *Chapter 8*, *Group By*. In particular, our *Group by basics* recipe will be
    helpful to compare/contrast aggregations to transformations and will highlight
    how transformations can be used to expressively and succinctly calculate “percent
    of group” calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Map
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.agg` and `.transform` methods we have seen so far apply to an entire *sequence*
    of values at once. Generally, in pandas, this is a good thing; it allows pandas
    to perform *vectorized* operations that are fast and computationally efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Still, sometimes, you as an end user may decide that you want to trade performance
    for customization or finer-grained control. This is where the `.map` methods can
    come into the picture; `.map` helps you apply functions individually to each element
    of your pandas object.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s assume we have a `pd.Series` of data that mixes together both numbers
    and lists of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '`.agg` or `.transform` are not suitable here because we do not have a uniform
    data type – we really have to inspect each element to make a decision on how to
    handle it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our analysis, let’s assume that when we encounter a number, we are happy
    to return the value as is. If we encounter a list of values, we want to average
    out all of the values within that list and return that. A function implementing
    this feature would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then apply this to each element of our `pd.Series` using `pd.Series.map`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'If we had a `pd.DataFrame` containing this type of data, `pd.DataFrame.map`
    would be able to apply this function just as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the above example, instead of using `pd.Series.map`, you could have also
    used `pd.Series.transform`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'However, you would *not* get the same results with `pd.DataFrame.transform`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Why is this? Remember that `.map` explicitly applies a function to each element,
    regardless of if you are working with a `pd.Series` or `pd.DataFrame`. `pd.Series.transform`
    is also happy to apply a function to each element that it contains, but `pd.DataFrame.transform`
    essentially loops over each column and passes that column as an argument to the
    callable arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because our function is implemented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'the `isinstance(value, list)` check fails when passed a `pd.Series` and you
    end up just returning the `pd.Series` itself. If we tweak our function slightly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'then the behavior of `pd.DataFrame.transform` becomes more clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: While there may be conceptual overlap, generally, in your code, you should think
    of `.map` as working element-wise, whereas `.agg` and `.transform` will try as
    best as they can to work with larger sequences of data at once.
  prefs: []
  type: TYPE_NORMAL
- en: Apply
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apply is a commonly used method, to the point that I would argue it is *overused*.
    The `.agg`, `.transform`, and `.map` methods seen so far have relatively clear
    semantics (`.agg` reduces, `.transform` maintains shape, `.map` applies functions
    element-wise), but when you reach for `.apply`, you can mirror any of these. That
    flexibility may seem nice at first, but because `.apply` leaves it up to pandas
    to *do the right thing*, you are typically better off picking the most explicit
    methods to avoid surprises.
  prefs: []
  type: TYPE_NORMAL
- en: Even still, you will see a lot of code out in the wild (especially from users
    who did not read this book); so, understanding what it does and what its limitations
    are can be invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Calling `pd.Series.apply` will make `.apply` act like `.map` (i.e., the function
    gets applied to each individual element of the `pd.Series`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at a rather contrived function that prints out each element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Funneling this through `.apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'gives exactly the same behavior as `pd.Series.map`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '`pd.Series.apply` works like a Python loop, calling the function for each element.
    Because our function returns nothing, our resulting `pd.Series` is a like-indexed
    array of `None` values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whereas `pd.Series.apply` works element-wise, `pd.DataFrame.apply` works across
    each column as a `pd.Series`. Let’s see this in action with a `pd.DataFrame` of
    shape `(3, 2)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the above output, the function was only called twice given
    the two columns of data, but it was applied three times with the `pd.Series` that
    had three rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from how many times `pd.DataFrame.apply` actually applies the function,
    the shape of the return value can vary between mirroring `.agg` and `.transform`
    functionality. Our preceding example is closer to a `.agg` because it returns
    a single `None` value, but if we returned the element we printed, we would get
    behavior more like a `.transform`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: If you find this confusing, you are not alone. Trusting pandas to *do the right
    thing* with `.apply` can be a risky proposition; I strongly advise users exhaust
    all options with `.agg`, `.transform`, or `.map` before reaching for `.apply`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Summary statistics provide a quick way to understand the basic properties and
    distribution of the data. In this section, we introduce two powerful pandas methods:
    `pd.Series.value_counts` and `pd.Series.describe`, which can serve as useful starting
    points for exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `pd.Series.value_counts` method attaches frequency counts to each distinct
    data point, making it easy to see how often each value occurs. This is particularly
    useful for discrete data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'For continuous data, `pd.Series.describe` is a heap of calculations packaged
    together into one method call. Through invocation of this particular method, you
    can easily see the count, mean, minimum, and maximum, alongside a high-level distribution
    of your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, we will see our distribution summarized through the 25%, 50%, 75%,
    and max (or 100%) quartiles. If your data analysis was focused on a more particular
    part of the distribution, you could control what this method presents back by
    providing a `percentiles=` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: Binning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Binning is the process of taking a continuous variable and categorizing it into
    discrete buckets. It can be useful to turn a potentially infinite amount of values
    into a finite amount of “bins” for your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s imagine we have collected survey data from users of a system. One of
    the survey questions asks users for their age, producing data that looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Rather than treating each age as an individual number, we will use `pd.cut`
    to place each record into an age group. As a first attempt, let’s pass our `pd.Series`
    and the number of bins we would like to generate as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: This produces a `pd.CategoricalDtype` with 4 distinct intervals – `(17.952,
    30.0]`, `(30.0, 42.0]`, `(42.0, 54.0]`, and `(54.0, 66.0]`. Save some unexpected
    decimal places on the first bin, which starts at `17.952`, these bins all cover
    an equidistant range of 12 years, which was derived from the fact that the maximum
    value (`66`) minus the lowest value (`18`) yields a total age gap of 48 years,
    which, when divided equally by 4, gives us the 12-year range for each bin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The age `17.952` we see in the first bin may make sense to pandas internally
    for whatever algorithm it chose to determine the buckets, but it is ultimately
    uninteresting to us since we know we are dealing with whole numbers. Fortunately,
    this can be controlled via the `precision=` keyword argument to remove any decimal
    places:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '`pd.cut` does not limit us to producing equally sized bins like this. If, instead,
    we wanted to place each person into 10-year age buckets, we could provide those
    ranges as the second argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this is a little too strict because it would not account for users
    over the age of 70\. To handle that, we could change our last bin edge from `70`
    to `999` and treat it as a catch-all:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'In turn, this produced a label of `(60, 999)`, which leaves something to be
    desired from a display perspective. If we are not happy with the default labels
    produced, we can control their output with the `labels=` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: However, our labels above are not *quite* right. Note that we provided both
    `30-40` and `40-50`, but what happens if someone is exactly 40 years old? What
    bin are they placed in?
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, we can see this in our data already from `Steve`, who perfectly
    matches this criteria. If you inspect the default bin he is placed in, it appears
    as `(30, 40]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'Binning, by default, is *right inclusive*, meaning each bin can be thought
    of as *up to and including* a particular value. If we wanted behavior that was
    *up to but not including*, we could control this with the `right` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: This changed the bin for `Steve` from `(30, 40]` to `[40, 50)`. In the default
    string representation, the square bracket signifies the edge being *inclusive*
    of a particular value, whereas the parenthesis is *exclusive*.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding with pd.get_dummies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not uncommon in data analysis and machine learning applications to take
    data that is categorical in nature and convert it into a sequence of `0/1` values,
    as the latter can be more easily interpreted by numeric algorithms. This process
    is often called *one-hot encoding*, and the outputs are typically referred to
    as *dummy indicators*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with a small `pd.Series` containing a discrete set of colors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'Passing this as an argument to `pd.get_dummies` will create a like-indexed
    `pd.DataFrame` with a Boolean column for each color. Each row has one column with
    `True` that maps it back to its original value; all other columns in the same
    row will be `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are not satisfied with the default column names, we can modify them by
    adding a prefix. A common convention in data modeling is to prefix a Boolean column
    with `is_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: Chaining with .pipe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When writing pandas code, there are two major stylistic forms that developers
    follow. The first approach makes liberal use of variables throughout a program,
    whether that means creating new variables like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'or simply reassigning to the same variable repeatedly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'The alternative approach is to express your code as a *pipeline*, where each
    step accepts and returns a `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: With the variable-based approach, you must create multiple variables in your
    program, or change the state of a `pd.DataFrame` at every reassignment. The pipeline
    approach, by contrast, does not create any new variables, nor does it change the
    state of your `pd.DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: While the pipeline approach could theoretically be better handled by a query
    optimizer, pandas does not offer such a feature as of the time of writing, and
    it is hard to guess what that may look like in the future. As such, the choice
    between the two approaches makes almost no difference for performance; it is truly
    a matter of style.
  prefs: []
  type: TYPE_NORMAL
- en: I encourage you to familiarize yourself with both approaches. You may at times
    find it easier to express your code as a pipeline; at other times, that may feel
    burdensome. There is no hard requirement to use one or the other, so you can mix
    and match the styles freely throughout your code.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with a very basic `pd.DataFrame`. The columns and their contents
    are not important for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s create some sample functions that will change the content of the
    columns. These functions should accept and return a `pd.DataFrame`, which you
    can see from the code annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in the introduction to this recipe, one of the most common ways
    to apply these functions would be to list them out as separate steps in our program,
    assigning the results of each step to a new variable along the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'If we wanted to avoid the use of intermediate variables altogether, we could
    have also tried to nest the function calls inside of one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: However, that doesn’t make the code any more readable, especially given the
    fact that `change_col1` is executed before `change_col2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'By expressing this as a pipeline, we can avoid the use of variables and more
    easily express the order of operations being applied. To achieve this, we are
    going to reach for the `pd.DataFrame.pipe` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have gotten back the same result as before, but without the
    use of variables and in a way that is arguably more readable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In case any of the functions you want to apply in a pipeline need to accept
    more arguments, `pd.DataFrame.pipe` is able to forward them along for you. For
    instance, let’s see what happens if we add a new `str_case` parameter to our `change_col2`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see with `pd.DataFrame.pipe`, you can simply pass that argument
    along as either a positional or keyword argument, just as if you were invoking
    the `change_col2` function directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: To reiterate what we mentioned in the introduction to this recipe, there is
    little to no functional difference between these styles. I encourage you to learn
    them both as you will inevitably see code written both ways. For your own development,
    you may even find that mixing and matching the approaches works best.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the lowest-budget movies from the top 100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have covered many of the core pandas algorithms from a theoretical
    level, we can start looking at more “real world” datasets and touch on common
    ways to explore them.
  prefs: []
  type: TYPE_NORMAL
- en: Top N analysis is a common technique whereby you filter your data based on how
    your data performs when measured by a single variable. Most analytics tools have
    the capability to help you filter your data to answer questions like *What are
    the top 10 customers by sales?* or, *What are the 10 products with the lowest
    inventory?*. When chained together, you can even form catchy news headlines such
    as *Out of the Top 100 Universities, These 5 Have the Lowest Tuition Fees*, or
    *From the Top 50 Cities to Live, These 10 Are the Most Affordable*.
  prefs: []
  type: TYPE_NORMAL
- en: Given how common these types of analyses are, pandas offers built-in functionality
    to help you easily perform them. In this recipe, we will take a look at `pd.DataFrame.nlargest`
    and `pd.DataFrame.nsmallest` and see how we can use them together to answer a
    question like *From the top 100 movies, which had the lowest budget?*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by reading in the movie dataset and selecting the columns `movie_title`,
    `imdb_score`, `budget`, and `gross`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pd.DataFrame.nlargest` method can be used to select the top 100 movies
    by `imdb_score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the top 100 selected, we can chain in a call to `pd.DataFrame.nsmallest`
    to return the five lowest-budget movies among those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is possible to pass a list of column names as the `columns=` parameter of
    the `pd.DataFrame.nlargest` and `pd.DataFrame.nsmallest` methods. This would only
    be useful to break ties in the event that there were duplicate values sharing
    the *nth* ranked spot in the first column in the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see where this matters, let’s try to just select the top 10 movies by `imdb_score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the lowest `imdb_score` from the top 10 is `8.9`. However,
    there are more than 10 movies that have a score of `8.9` and above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: 'The movies that were a part of the top 10 just happened to be the first two
    movies pandas came across with that score. However, you can use the `gross` column
    as the tiebreaker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: With that, you see that Pulp Fiction replaced Schindler’s List in our top 10
    analysis, given that it grossed higher.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating a trailing stop order price
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many strategies to trade stocks. One basic type of trade that many
    investors employ is the *stop order*. A stop order is an order placed by an investor
    to buy or sell a stock that executes whenever the market price reaches a certain
    point. Stop orders are useful to both prevent huge losses and protect gains.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical stop order, the price does not change throughout the lifetime of
    the order. For instance, if you purchased a stock for $100 per share, you might
    want to set a stop order at $90 per share to limit your downside to 10%.
  prefs: []
  type: TYPE_NORMAL
- en: A more advanced strategy would be to continually modify the sale price of the
    stop order to track the value of the stock if it increases in value. This is called
    a *trailing stop order*. Concretely, if the same $100 stock increases to $120,
    then a trailing stop order 10% below the current market value would move the sale
    price to $108.
  prefs: []
  type: TYPE_NORMAL
- en: The trailing stop order never moves down and is always tied to the maximum value
    since the time of purchase. If the stock fell from $120 to $110, the stop order
    would still remain at $108\. It would only increase if the price moved above $120.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe determines the trailing stop order price given an initial purchase
    price for any stock using the `pd.Series.cummax` method and how `pd.Series.cummin`
    could instead be used to handle short positions. We will also see how the `pd.Series.idxmax`
    method can be used to identify the day the stop order would have been triggered.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started, we will work with Nvidia (NVDA) stock and assume a purchase
    on the first trading day of 2020:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: 'In the pandas 2.2 series, there is a bug that prevents the preceding code block
    from running, instead throwing a `ValueError`. If affected by this bug, you can
    alternatively run `pd.read_csv` without the `dtype_backend` argument, and add
    in a call to `pd.DataFrame.convert_dtypes` instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: For more information, see pandas bug issue `#57930` ([https://github.com/pandas-dev/pandas/issues/57930](https://github.com/pandas-dev/pandas/issues/57930)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of which path you took, be aware that `pd.read_csv` returns a `pd.DataFrame`,
    but for this analysis we will only need a `pd.Series`. To perform that conversion,
    you can call `pd.DataFrame.squeeze`, which will reduce the object from two to
    one dimension, if possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we can use the `pd.Series.cummax` method to track the highest closing
    price seen to date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a trailing stop order that limits our downside to 10%, we can chain
    in a multiplication by `0.9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: The `pd.Series.cummax` method works by retaining the maximum value encountered
    up to and including the current value. Multiplying this series by 0.9, or whatever
    cushion you would like to use, creates the trailing stop order. In this particular
    example, NVDA increased in value, and thus, its trailing stop has also increased.
  prefs: []
  type: TYPE_NORMAL
- en: On the flip side, let’s say we were pessimistic about NVDA stock during this
    timeframe, and we wanted to short the stock. However, we still wanted to put a
    stop order in place to limit the downside to a 10% increase in value.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we can simply replace our usage of `pd.Series.cummax` with `pd.Series.cummin`
    and multiply by `1.1` instead of `0.9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With our trailing stop orders calculated, we can easily determine the days
    where we would have fallen off of the cumulative maximum by more than our threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: 'If we only cared to identify the very first day where we fell below the cumulative
    maximum, we could use the `pd.Series.idxmax` method. This method works by first
    calculating the maximum value within a `pd.Series`, and then returns the first-row
    index where that maximum was encountered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: The expression `ser <= stop_prices` gives back a Boolean `pd.Series` containing
    `True=/=False` values, with each `True` record indicating where the stock price
    is at or below the stop price we already calculated. `pd.Series.idxmax` will consider
    `True` to be the maximum value in that `pd.Series`; so, by returning the first
    index label where `True` was seen as a value, it tells us the first day that our
    trailing stop order should have been triggered.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe gives us just a taste of how useful pandas may be for trading securities.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the baseball players best at…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The American sport of baseball has long been a subject of intense analytical
    research, with data collection dating back to the early 1900s. For Major League
    baseball teams, advanced data analysis helps answer questions like *How much should
    I pay for X player?* and *What should I do in the game given the current state
    of things*?, For fans, that same data can be used as fodder for endless debates
    around *who is the greatest player ever*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe, we are going to use data that was collected from [retrosheet.org](https://www.retrosheet.org).
    Per the Retrosheet licensing requirements, you should be aware of the following
    legal disclaimer:'
  prefs: []
  type: TYPE_NORMAL
- en: The information used here was obtained free of charge and is copyrighted by
    Retrosheet. Interested parties may contact Retrosheet at [www.retrosheet.org](https://www.retrosheet.org).
  prefs: []
  type: TYPE_NORMAL
- en: From its raw form, the data was summarized to show the common baseball metrics
    for at bat (`ab`), hits (`h`), runs scored (`r`), and home runs (`hr`) for professional
    players in the years 2020–2023.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by reading in our summarized data and setting the `id` column (which
    represents a unique player) as the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: 'In baseball, it is rather rare for a player to dominate all statistical categories.
    Oftentimes, a player with a lot of home runs (`hr`) will be more powerful and
    can hit the ball farther, but may do so less frequently than a player more specialized
    to collect a lot of hits (`h`). With pandas, we are fortunate to not have to dive
    into each metric individually; a simple call to `pd.DataFrame.idxmax` will look
    at each column, find the maximum value, and return the row index value associated
    with that maximum value for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, player `semim001` (Marcus Semien) had the most at bats, `freef001`
    (Freddie Freeman) had the most runs and hits, and `judga001` (Aaron Judge) hit
    the most home runs in this timeframe.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to look deeper into how these great players performed across
    all categories, you could take the output of `pd.DataFrame.idxmax`, subsequently
    call `pd.Series.unique` on the values, and use that as a mask for the overall
    `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a nice visual enhancement to this data, you can use `pd.DataFrame.style.highlight_max`
    to very specifically show which category these players were the best at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B31091_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Jupyter Notebook output of a DataFrame highlighting max value per
    column'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding which position scores the most per team
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In baseball, teams are allowed 9 batters in a “lineup,” with 1 representing
    the first person to bat and 9 representing the last. Over the course of a game,
    teams cycle through batters in order, starting over with the first batter after
    the last has batted.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, teams place some of their best hitters toward the “top of the lineup”
    (i.e., lower number positions) to maximize the opportunity for them to come around
    and score. However, this does not always mean that the person who bats in position
    1 will always be the first to score.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are going to look at all Major League baseball teams from
    2000–2023 and find the position that scored the most runs for a team over each
    season.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Much like we did in the *Finding the baseball players best at…* recipe, we
    are going to use data taken from [retrosheet.org](https://www.retrosheet.org).
    For this particular dataset, we are going to set the `year` and `team` columns
    in the row index, leaving the remaining columns to show the position in the batting
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: 'With `pd.DataFrame.idxmax`, we can see for every year and team which position
    scored the most runs. However, with this dataset, the index label we would like
    `pd.DataFrame.idxmax` to identify is actually in the columns and not the rows.
    Fortunately, pandas can still calculate this easily with the `axis=1` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, we can use `pd.Series.value_counts` to understand the number of
    times a given position in the order represented the most runs scored for a team.
    We are also going to use the `normalize=True` argument, which will give us a frequency
    instead of a total:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: Unsurprisingly, the first batter scored most frequently accounted for the most
    runs, doing so for 48% of the teams.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We might want to explore more and answer the question: *For teams where the
    first batter scored the most runs, who scored the second-most*?'
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate this, we can create a mask to filter on teams where the first
    batter scored the most, drop that column from our dataset, and then repeat with
    the same `pd.DataFrame.idxmax` approach to identify the position next in line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, if a team’s first batter does not lead the team in runs scored,
    the second batter ends up being the leader almost 50% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/pandas](https://packt.link/pandas)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5040900042138312.png)'
  prefs: []
  type: TYPE_IMG
