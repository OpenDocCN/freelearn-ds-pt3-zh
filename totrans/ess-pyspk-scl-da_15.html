<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer072">
			<h1 id="_idParaDest-199"><a id="_idTextAnchor199"/>Chapter 12: Spark SQL Primer</h1>
			<p>In the previous chapter, you learned about data visualizations as a powerful and key tool of data analytics. You also learned about various Python visualization libraries that can be used to visualize data in pandas DataFrames. An equally important and ubiquitous and essential skill in any data analytics professional's repertoire is <strong class="bold">Structured Query Language</strong> or <strong class="bold">SQL</strong>. <strong class="bold">SQL</strong> has existed as long as the field of data analytics has existed, and even with the advent of big data, data science, and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), SQL is still proving to be indispensable. </p>
			<p>This chapter introduces you to the basics of SQL and looks at how SQL can be applied in a distributed computing setting via Spark SQL. You will learn about the various components that make up Spark SQL, including the storage, metastore, and the actual query execution engine. We will look at the differences between <strong class="bold">Hadoop Hive</strong> and Spark SQL, and finally, end with some techniques for improving the performance of Spark SQL queries.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Introduction to SQL</li>
				<li>Introduction to Spark SQL </li>
				<li>Spark SQL language reference</li>
				<li>Optimizing Spark SQL performance</li>
			</ul>
			<p>Some of the areas covered in this chapter include the usefulness of SQL as a language for slicing and dicing of data, the individual components of Spark SQL, and how they come together to create a powerful distributed SQL engine on Apache Spark. You will look at a Spark SQL language reference to help with your data analytics needs and some techniques to optimize the performance of your Spark SQL queries.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor200"/>Technical requirements</h1>
			<p>Here is what you'll need for this chapter:</p>
			<ul>
				<li>In this chapter, we will be using Databricks Community Edition to run our code (https://community.cloud.databricks.com). Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. </li>
				<li>The code and data used in this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter12">https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter12</a>.</li>
			</ul>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor201"/>Introduction to SQL</h1>
			<p><strong class="bold">SQL</strong> is a declarative language for storing, manipulating, and querying data stored in relational <a id="_idIndexMarker863"/>databases, also called <strong class="bold">relational database management systems</strong> (<strong class="bold">RDBMSes</strong>). A relational database contains <a id="_idIndexMarker864"/>data in tables, which in turn contain rows and columns. In the real world, entities have relationships among themselves, and a relational database tries to mimic these real-world relationships as relationships between tables. Thus, in relational databases, individual tables contain data related to individual entities, and these tables might be related. </p>
			<p>SQL is a declarative programming language that helps you specify which rows and columns you want to retrieve from a given table and specify constraints to filter out any data. An RDBMS contains a query optimizer that turns a SQL declaration into a query plan and executes it on the database engine. The query plan is finally translated into an execution plan for the database engine to read table rows and columns into memory and filter them based on the provided constraints.</p>
			<p>The SQL language includes <a id="_idIndexMarker865"/>subsets for defining schemas—called <strong class="bold">Data Definition Language</strong> (<strong class="bold">DDL</strong>)—and modifying and querying data—called <strong class="bold">Data Manipulation Language</strong> (<strong class="bold">DML</strong>), as <a id="_idIndexMarker866"/>discussed in the following sections.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor202"/>DDL</h2>
			<p><strong class="bold">DDL</strong> is used to define data structures such as databases, schemas, tables, and columns within a <a id="_idIndexMarker867"/>relational database. Basic DDL statements include <strong class="source-inline">CREATE</strong>, <strong class="source-inline">ALTER</strong>, <strong class="source-inline">DROP</strong>, <strong class="source-inline">TRUNCATE</strong>, and so on. The following SQL query represents a DDL SQL statement:</p>
			<p class="source-code">CREATE TABLE db_name.schema_name.table_name (</p>
			<p class="source-code">    column1 datatype,</p>
			<p class="source-code">    column2 datatype,</p>
			<p class="source-code">    column3 datatype,</p>
			<p class="source-code">   ....</p>
			<p class="source-code">);</p>
			<p>The previous SQL statement represents a typical command to create a new table within a specific database and a schema with a few columns and its data types defined. A database is a collection of data and log files, while a schema is a logical grouping within a database.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor203"/>DML</h2>
			<p><strong class="bold">DML</strong> is used to query, retrieve, and manipulate data stored within a database. Basic DML statements <a id="_idIndexMarker868"/>include <strong class="source-inline">SELECT</strong>, <strong class="source-inline">UPDATE</strong>, <strong class="source-inline">INSERT</strong>, <strong class="source-inline">DELETE</strong>, <strong class="source-inline">MERGE</strong>, and so on. An example DML query is shown here:</p>
			<p class="source-code">SELECT column1, SUM(column2) AS agg_value</p>
			<p class="source-code">FROM db_name.schema_name.table_name </p>
			<p class="source-code">WHERE column3 between value1 AND value2</p>
			<p class="source-code">GROUP BY column1</p>
			<p class="source-code">ORDER BY SUM(column2)</p>
			<p class="source-code">);</p>
			<p>The previous query results in <strong class="source-inline">column2</strong> being aggregated by each distinct value of <strong class="source-inline">column1</strong> after filtering rows based on the constraint specified on <strong class="source-inline">column3</strong>, and finally, the results being sorted by the aggregated value.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Although SQL generally adheres to certain standards set by the <strong class="bold">American National Standards Institute</strong> (<strong class="bold">ANSI</strong>), each RDBMS vendor has a slightly different implementation <a id="_idIndexMarker869"/>of the SQL standards, and you should refer to the specific RDBMS's documentation for the correct syntax.</p>
			<p>The previous SQL statement<a id="_idIndexMarker870"/> represents standard DDL and DML queries; however, there might be subtle implementation details between each RDBMS's implementation of the SQL standard. Similarly, Apache Spark also has its own implementation of the ANSI SQL 2000 standard.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor204"/>Joins and sub-queries</h2>
			<p>Tables within relational databases contain data that is related, and it is often required to join the data between various tables to produce meaningful analytics. Thus, SQL supports <a id="_idIndexMarker871"/>operations such as joins and sub-queries for users to be able to combine data across<a id="_idIndexMarker872"/> tables, as shown in the following SQL statement:</p>
			<p class="source-code">SELECT a.column1, b.cloumn2, b.column3</p>
			<p class="source-code">FROM table1 AS a JOIN  table2 AS b</p>
			<p class="source-code">ON a.column1 = b.column2</p>
			<p>In the previous SQL query, we join two tables using a common key column and produce columns from both the tables after the <strong class="source-inline">JOIN</strong> operation. Similarly, sub-queries are queries within queries that can occur in a <strong class="source-inline">SELECT</strong>, <strong class="source-inline">WHERE</strong>, or <strong class="source-inline">FROM</strong> clause that lets you combine data from multiple tables. A specific implementation of these SQL queries within Spark SQL will be explored in the following sections.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor205"/>Row-based versus columnar storage</h2>
			<p>Databases <a id="_idIndexMarker873"/>physically store data in one of two ways, either in a row-based manner or in a columnar fashion. Each has its own advantages <a id="_idIndexMarker874"/>and disadvantages, depending on its use case. In row-based storage, all the values are stored together, and in columnar storage, all the values of a column are stored contiguously on a physical storage medium, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="Images/B16736_12_1.jpg" alt="Figure 12.1 – Row-based versus columnar storage&#13;&#10;" width="439" height="323"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – Row-based versus columnar storage</p>
			<p>As depicted in the previous screenshot, in row-based storage, an entire row with all its column values is stored together on physical storage. This makes it easier to find an individual row and retrieve all its columns from storage in a fast and efficient manner. Columnar storage, on the other hand, stores all the values of an individual column contiguously on physical storage, which makes retrieving an individual column fast and efficient.</p>
			<p>Row-based storage is more popular with transactional systems, where quickly retrieving an individual <a id="_idIndexMarker875"/>transactional record or a row is more important. On the other hand, analytical systems typically deal with aggregates of rows and only need <a id="_idIndexMarker876"/>to retrieve a few columns per query. Thus, it is more efficient to choose columnar storage while <a id="_idIndexMarker877"/>designing analytical systems. Columnar storage also offers a <a id="_idIndexMarker878"/>better data compression ratio, thus making optimal use of available storage space <a id="_idIndexMarker879"/>when storing huge amounts of historical data. Analytical storage <a id="_idIndexMarker880"/>systems including <strong class="bold">data warehouses</strong> and <strong class="bold">data lakes</strong> prefer columnar <a id="_idIndexMarker881"/>storage over row-based storage. Popular <a id="_idIndexMarker882"/>big data file formats such as <strong class="bold">Parquet</strong> and <strong class="bold">Optimized Row Columnar</strong> (<strong class="bold">ORC</strong>) are also columnar.</p>
			<p>The ease of use and ubiquity of SQL has led the creators of many non-relational data processing frameworks such as Hadoop and Apache Spark to adopt subsets or variations of SQL in creating Hadoop Hive and Spark SQL. We will explore Spark SQL in detail in the following section.</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor206"/>Introduction to Spark SQL</h1>
			<p><strong class="bold">Spark SQL</strong> brings native support for SQL to Apache Spark and unifies the process of querying data <a id="_idIndexMarker883"/>stored both in Spark DataFrames and in external data sources. Spark SQL unifies DataFrames and relational tables and makes it easy for developers to intermix SQL commands with querying external data for complex analytics. With <a id="_idIndexMarker884"/>the release of <strong class="bold">Apache Spark 1.3</strong>, Spark DataFrames powered by Spark SQL became the de facto abstraction of Spark for expressing <a id="_idIndexMarker885"/>data processing code, while <strong class="bold">resilient distributed datasets</strong> (<strong class="bold">RDDs</strong>) still remain Spark's core abstraction method, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="Images/B16736_12_2.jpg" alt="Figure 12.2 – Spark SQL architecture&#13;&#10;" width="542" height="311"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – Spark SQL architecture</p>
			<p>As shown in the <a id="_idIndexMarker886"/>previous diagram, you can see that most of Spark's components <a id="_idIndexMarker887"/>now leverage Spark SQL and DataFrames. Spark SQL provides more information about the structure of the data and the computation being performed, and the <strong class="bold">Spark SQL engine</strong> uses this extra information to perform <a id="_idIndexMarker888"/>additional optimizations to the query. With Spark SQL, all <a id="_idIndexMarker889"/>of Spark's components—including <strong class="bold">Structured Streaming</strong>, <strong class="bold">DataFrames</strong>, <strong class="bold">Spark ML</strong>, and <strong class="bold">GraphFrames</strong>—and all its programming <strong class="bold">application </strong><strong class="bold"><a id="_idIndexMarker890"/></strong><strong class="bold">programming interfaces</strong> (<strong class="bold">APIs</strong>)—including <strong class="bold">Scala</strong>, <strong class="bold">Java</strong>, <strong class="bold">Python</strong>, <strong class="bold">R</strong>, and <strong class="bold">SQL</strong>—use the same <a id="_idIndexMarker891"/>execution engine to express computations. This unification makes it easy for you to switch back and forth between different APIs <a id="_idIndexMarker892"/>and lets you <a id="_idIndexMarker893"/>choose the right API for the task at hand. Certain <a id="_idIndexMarker894"/>data processing <a id="_idIndexMarker895"/>operations, such as joining multiple tables, are expressed much more easily in SQL, and developers can easily mix <strong class="bold">SQL</strong> with <strong class="bold">Scala</strong>, <strong class="bold">Java</strong>, or <strong class="bold">Python</strong> code.</p>
			<p>Spark SQL also brings a powerful new optimization framework called <strong class="bold">Catalyst</strong>, which can automatically <a id="_idIndexMarker896"/>transform any data processing code, whether expressed using Spark DataFrames or using Spark SQL, to execute more efficiently. We will explore the <strong class="bold">Catalyst</strong> optimizer in the following section.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor207"/>Catalyst optimizer</h2>
			<p>A <strong class="bold">SQL query optimizer</strong> in an RDBMS is a process that determines the most efficient way for a given SQL query to process <a id="_idIndexMarker897"/>data stored in a database. The SQL optimizer tries to generate the most optimal execution for a given SQL query. The <a id="_idIndexMarker898"/>optimizer typically generates multiple query execution plans and chooses the optimal one among them. It typically takes into consideration factors such as the <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>), <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>), and any available statistics on the tables being queried to choose the most optimal <a id="_idIndexMarker899"/>query execution plan. The optimizer based on the chosen query execution plan chooses to re-order, merge, and process a query in any order that yields the optimal results.</p>
			<p>The Spark SQL engine also comes equipped with a query optimizer named <strong class="bold">Catalyst</strong>. <strong class="bold">Catalyst</strong> is based on <strong class="bold">functional programming</strong> concepts, like the rest of Spark's code base, and uses Scala's programming <a id="_idIndexMarker900"/>language features to build a robust and extensible query optimizer. Spark's Catalyst optimizer generates an optimal execution plan for a given Spark SQL query by following a series of steps, as depicted in the following diagram:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="Images/B16736_12_03.jpg" alt="Figure 12.3 – Spark's Catalyst optimizer&#13;&#10;" width="1263" height="242"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – Spark's Catalyst optimizer</p>
			<p>As shown in the previous diagram, the Catalyst optimizer first generates a logical plan after resolving <a id="_idIndexMarker901"/>references, then optimizes the logical plan based on standard rule-based optimization techniques. It then generates a set of physical execution <a id="_idIndexMarker902"/>plans using the optimized logical plan and chooses the best physical plan, and finally, generates <strong class="bold">Java virtual machine</strong> (<strong class="bold">JVM</strong>) bytecode using the best possible physical plan. This process allows Spark SQL to translate user queries into the best possible data processing code without the developer having any nuanced understanding of the microscopic inner workings of Spark's distributed data processing paradigm. Moreover, Spark SQL DataFrame APIs in the Java, Scala, Python, and R programming languages all go through the same Catalyst optimizer. Thus, Spark SQL or any data processing written using DataFrame APIs irrespective of the programming language yields comparable performance. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">There are a few exceptions where PySpark DataFrame code might not be comparable to Scala or Java code in <a id="_idIndexMarker903"/>performance. One such example is when using non-vectorized <strong class="bold">user-defined functions</strong> (<strong class="bold">UDFs</strong>) in PySpark DataFrame operations. Catalyst doesn't have any visibility into UDFs in Python and will not be able to optimize the code. Thus, these should be replaced with either Spark SQL's built-in function or the UDFs defined in Scala or Java.</p>
			<p>A seasoned data engineer with a thorough understanding of the RDD API can possibly write a little more optimized code than the Catalyst optimizer; however, by letting Catalyst handle the <a id="_idIndexMarker904"/>code-generation complexity, developers can focus their valuable time on actual data processing tasks, thus making them even more efficient. After gaining an understanding of the inner workings of the Spark SQL engine, it would be useful to understand the kinds of data sources that Spark SQL can work with.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor208"/>Spark SQL data sources </h2>
			<p>Since Spark DataFrame <a id="_idIndexMarker905"/>API and SQL API are both based on the same Spark SQL engine powered by the Catalyst optimizer, they also support the same set of data sources. A few prominent Spark SQL data sources are presented here.</p>
			<h3>File data source</h3>
			<p>File-based data <a id="_idIndexMarker906"/>sources such<a id="_idIndexMarker907"/> as Parquet, ORC, Delta, and so on are supported by Spark SQL out of the box, as shown in the following SQL query example:</p>
			<p class="source-code">SELECT * FROM delta.'/FileStore/shared_uploads/delta/retail_features.delta' LIMIT 10;</p>
			<p>In the previous <a id="_idIndexMarker908"/>SQL statement, data is directly queried from a <strong class="bold">Delta Lake</strong> location by simply using the <strong class="source-inline">delta.</strong> prefix. The same SQL construct can also be <a id="_idIndexMarker909"/>used with a<a id="_idIndexMarker910"/> Parquet file location on the data lake. </p>
			<p>Other file types such as <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) and <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>) would require first registering a table or a view with the metastore, as <a id="_idIndexMarker911"/>these files are not self-describing and <a id="_idIndexMarker912"/>lack any inherent schema information. An example of using a CSV file with Spark SQL is presented in the following SQL query:</p>
			<p class="source-code">CREATE OR REPLACE TEMPORARY VIEW csv_able</p>
			<p class="source-code">USING csv</p>
			<p class="source-code">OPTIONS (</p>
			<p class="source-code">  header "true",</p>
			<p class="source-code">  inferSchema "true",</p>
			<p class="source-code">  path "/FileStore/ConsolidatedCities.csv"</p>
			<p class="source-code">);</p>
			<p class="source-code">SELECT * FROM csv_able LIMIT 5;</p>
			<p>In the previous SQL statement, we first create a temporary view by using a CSV file on the data lake using a CSV data source. We also use <strong class="source-inline">OPTIONS</strong> to specify the CSV file has a header row and to infer a schema from the file itself. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">A metastore is an RDBMS database <a id="_idIndexMarker913"/>where Spark SQL persists metadata information such as databases, tables, columns, and partitions.</p>
			<p>You could also create a permanent table instead of a temporary view if the table needs to persist across cluster restarts and will be reused later.</p>
			<h3>JDBC data source</h3>
			<p>Existing <a id="_idIndexMarker914"/>RDBMS databases <a id="_idIndexMarker915"/>can also be registered with the metastore via <strong class="bold">Java Database Connectivity</strong> (<strong class="bold">JDBC</strong>) and used as a data source in Spark SQL. An <a id="_idIndexMarker916"/>example is presented in the following code block:</p>
			<p class="source-code">CREATE TEMPORARY VIEW jdbcTable</p>
			<p class="source-code">USING org.apache.spark.sql.jdbc</p>
			<p class="source-code">OPTIONS (</p>
			<p class="source-code">  url "jdbc:mysql://localhost:3306/pysparkdb",</p>
			<p class="source-code">  dbtable "authors",</p>
			<p class="source-code">  user 'username',</p>
			<p class="source-code">  password 'password'</p>
			<p class="source-code">);</p>
			<p class="source-code">SELECT * FROM resultTable; </p>
			<p>In the previous code block, we create a temporary view using the <strong class="source-inline">jdbc</strong> data source and specify database connectivity <a id="_idIndexMarker917"/>options such as the database <strong class="bold">Uniform Resource Locator</strong> (<strong class="bold">URL</strong>), table name, username, password, and so on.</p>
			<h3>Hive data source</h3>
			<p><strong class="bold">Apache Hive</strong> is a data <a id="_idIndexMarker918"/>warehouse in the Hadoop ecosystem that can be used to read, write, and manage <a id="_idIndexMarker919"/>datasets on a Hadoop filesystem or a data lake using SQL. Spark SQL can be<a id="_idIndexMarker920"/> used with Apache Hive, including a Hive metastore, Hive <strong class="bold">Serializer/Deserializer</strong> (<strong class="bold">SerDes</strong>), and Hive UDFs. Spark SQL supports most Hive features such <a id="_idIndexMarker921"/>as the Hive query language, Hive expressions, user-defined aggregate functions, window functions, joins, unions, sub-queries, and so on. However, features such as Hive <strong class="bold">atomicity, consistency, isolation, durability</strong> (<strong class="bold">ACID</strong>) table updates, Hive I/O formats, and certain Hive-specific optimizations are not supported. A full list of the supported and unsupported <a id="_idIndexMarker922"/>features can be found in Databricks' public documentation here: <a href="https://docs.databricks.com/spark/latest/spark-sql/compatibility/hive.html">https://docs.databricks.com/spark/latest/spark-sql/compatibility/hive.html</a>.</p>
			<p>Now that you have gained an understanding of Spark SQL components such as the Catalyst optimizer and its data sources, we can delve into Spark SQL-specific syntax and functions.</p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor209"/>Spark SQL language reference</h1>
			<p>Being a part of the overarching <a id="_idIndexMarker923"/>Hadoop ecosystem, Spark has traditionally been Hive-compliant. While the Hive query language diverges greatly from ANSI SQL standards, Spark 3.0 Spark SQL can be made ANSI SQL-compliant using a <strong class="source-inline">spark.sql.ansi.enabled</strong> configuration. With this configuration enabled, Spark SQL uses an ANSI SQL-compliant dialect instead of a Hive dialect.</p>
			<p>Even with ANSI SQL compliance enabled, Spark SQL may not entirely conform to ANSI SQL dialect, and in this section, we will explore some of the prominent DDL and DML syntax of Spark SQL.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor210"/>Spark SQL DDL</h2>
			<p>The syntax for <a id="_idIndexMarker924"/>creating a database and a table using Spark SQL is presented as follows:</p>
			<p class="source-code">CREATE DATABASE IF NOT EXISTS feature_store;</p>
			<p class="source-code">CREATE TABLE IF NOT EXISTS feature_store.retail_features</p>
			<p class="source-code">USING DELTA</p>
			<p class="source-code">LOCATION '/FileStore/shared_uploads/delta/retail_features.delta';</p>
			<p>In the previous code block, we do the following:</p>
			<ul>
				<li>First, we create a database if it doesn't already exist, using the <strong class="source-inline">CREATE DATABASE</strong> command. With this command, options such as the physical warehouse location on persistent storage and other database properties can also be specified as options.</li>
				<li>Then, we create a table using <strong class="source-inline">delta</strong> as the data source and specify the location of the data. Here, data at the specified location already exists, so there is no need to specify <a id="_idIndexMarker925"/>any schema information such as column names and their data types. However, to create an empty table structure, columns and their data types need to be specified.</li>
			</ul>
			<p>To change certain properties of an existing table such as renaming the table, altering or dropping columns, or ammending table partition information, the <strong class="source-inline">ALTER</strong> command can be used, as shown in the following code sample:</p>
			<p class="source-code">ALTER TABLE feature_store.retail_features RENAME TO feature_store.etailer_features;</p>
			<p class="source-code">ALTER TABLE feature_store.etailer_features ADD COLUMN (new_col String);</p>
			<p>In the previous code sample, we rename the table in the first SQL statement. The second SQL statement alters the table and adds a new column of the <strong class="source-inline">String</strong> type. Only changing column comments and adding new columns are supported in Spark SQL. The following code sample presents Spark SQL syntax for dropping or deleting artifacts altogether:</p>
			<p class="source-code">TRUNCATE TABLE feature_store.etailer_features;</p>
			<p class="source-code">DROP TABLE feature_store.etailer_features;</p>
			<p class="source-code">DROP DATABASE feature_store;</p>
			<p>In the previous code sample, the <strong class="source-inline">TRUNCATE</strong> command deletes all the rows of the table and leaves the table structure and schema intact. The <strong class="source-inline">DROP TABLE</strong> command deletes the table along with its schema, and the <strong class="source-inline">DROP DATABASE</strong> command deletes the entire database itself.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor211"/>Spark DML</h2>
			<p>Data manipulation <a id="_idIndexMarker926"/>involves adding, changing, and deleting data from tables. Some examples of this are presented in the following code statements:</p>
			<p class="source-code">INSERT INTO feature_store.retail_features</p>
			<p class="source-code">SELECT * FROM delta.'/FileStore/shared_uploads/delta/retail_features.delta';</p>
			<p>The previous SQL statement inserts data into an existing table using results from another SQL query. Similarly, the <strong class="source-inline">INSERT OVERWRITE</strong> command can be used to overwrite <a id="_idIndexMarker927"/>existing data and then load new data into a table. The following SQL statement can be used to selectively delete data from a table:</p>
			<p class="source-code">DELETE FROM feature_store.retail_features WHERE country_code = 'FR';</p>
			<p>The previous SQL statement deletes selective data from the table based on a filter condition. Though <strong class="source-inline">SELECT</strong> statements are not necessary, they are quintessential in data analysis. The following SQL statement depicts the use of <strong class="source-inline">SELECT</strong> statements for data analysis using Spark SQL:</p>
			<p class="source-code">SELECT</p>
			<p class="source-code">  year AS emp_year</p>
			<p class="source-code">  max(m.last_name),</p>
			<p class="source-code">  max(m.first_name),</p>
			<p class="source-code">  avg(s.salary) AS avg_salary</p>
			<p class="source-code">FROM</p>
			<p class="source-code">  author_salary s</p>
			<p class="source-code">  JOIN mysql_authors m ON m.uid = s.id</p>
			<p class="source-code">GROUP BY year</p>
			<p class="source-code">ORDER BY s.salary DESC</p>
			<p>The previous SQL statement performs<a id="_idIndexMarker928"/> an <strong class="bold">inner join</strong> of two tables based on a common key and calculates the average salary of each employee by year. The results of this query give insights into employee salary changes over the years and can be easily scheduled to be refreshed periodically.</p>
			<p>This way, using the powerful distributed SQL engine of Apache Spark and its expressive Spark SQL language, you can perform complex data analysis in a fast and efficient manner without having to learn any new programming languages. A complete Spark SQL reference guide <a id="_idIndexMarker929"/>for an exhaustive list of supported data types, function <a id="_idIndexMarker930"/>libraries, and SQL syntax can be found in Apache Spark's public documentation here: <a href="https://spark.apache.org/docs/latest/sql-ref-syntax.html">https://spark.apache.org/docs/latest/sql-ref-syntax.html</a>.</p>
			<p>Though Spark SQL's <strong class="bold">Catalyst</strong> optimizer does most of the heavy lifting, it's useful to know a few techniques to further tune Spark SQL's performance, and a few prominent ones are presented in the following section<span class="hidden">.</span></p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor212"/>Optimizing Spark SQL performance</h1>
			<p>In the previous section, you learned how the Catalyst optimizer optimizes user code by running the code through a set of optimization steps until an optimal execution plan is derived. To take <a id="_idIndexMarker931"/>advantage of the Catalyst optimizer, it is recommended to use Spark code that leverages the Spark SQL engine—that is, Spark SQL and DataFrame APIs—and avoid using RDD-based Spark code as much as possible. The Catalyst optimizer has no visibility into UDFs, thus users could end up writing sub-optimal code that might degrade performance. Thus, it is recommended to use built-in functions instead of UDFs or to define functions in Scala and Java and then use them in SQL and Python APIs.</p>
			<p>Though Spark SQL supports file-based formats such as CSV and JSON, it is recommended to use serialized data formats such as Parquet, AVRO, and ORC. Semi-structured formats such as CSV or JSON incur performance costs, firstly during the schema inference phase, as they cannot present their schema readily to the Spark SQL engine. Secondly, they do not support any data filtering features such as <strong class="bold">Predicate Pushdown</strong>, thus entire files must be loaded into memory before any data can be filtered out at the source. Being inherently uncompressed file formats, CSV and JSON also consume more memory compared to binary compressed formats such as Parquet. Even traditional relational databases are preferred over using semi-structured data formats as they support Predicate Pushdown, and some data processing responsibility can be delegated down to the databases.</p>
			<p>For iterative workloads such as ML, where the same dataset is accessed multiple times, it is useful to cache the dataset in memory so that subsequent scans of the table or DataFrame happen in memory, improving query performance greatly.</p>
			<p>Spark comes with various <strong class="bold">join</strong> strategies to <a id="_idIndexMarker932"/>improve performance while joining tables, such as <strong class="source-inline">BROADCAST</strong>, <strong class="source-inline">MERGE</strong>, <strong class="source-inline">SHUFFLE_HASH</strong>, and so on. However, the Spark SQL engine might sometimes not be able to predict the strategy for a given query. This can be mitigated <a id="_idIndexMarker933"/>by passing in <strong class="bold">hints</strong> to the Spark SQL query, as shown in the following code block:</p>
			<p class="source-code">SELECT /*+ BROADCAST(m) */</p>
			<p class="source-code">  year AS emp_year</p>
			<p class="source-code">  max(m.last_name),</p>
			<p class="source-code">  max(m.first_name),</p>
			<p class="source-code">  avg(s.salary) AS avg_salary</p>
			<p class="source-code">FROM</p>
			<p class="source-code">  author_salary s</p>
			<p class="source-code">  JOIN mysql_authors m ON m.uid = s.id</p>
			<p class="source-code">GROUP BY year</p>
			<p class="source-code">ORDER BY s.salary DESC;</p>
			<p>In the previous code block, we are<a id="_idIndexMarker934"/> passing in a <strong class="bold">join hint</strong> via a comment in the <strong class="source-inline">SELECT</strong> clause. This specifies that the smaller table is broadcasted to all the worker nodes, which should improve join performance and thus the overall query performance. Similarly, <strong class="source-inline">COALESCE</strong> and <strong class="source-inline">REPARTITION</strong> hints can also be passed to Spark SQL queries; these hints reduce the number of output files, thus improving performance.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">SQL hints, query hints, or optimizer hints are additions to standard SQL statements used to nudge <a id="_idIndexMarker935"/>the SQL execution engine to choose a particular physical execution plan <a id="_idIndexMarker936"/>that the developer thinks is optimal. SQL hints have <a id="_idIndexMarker937"/>traditionally been supported by all RDBMS engines and are now supported by Spark SQL as well as for certain kinds of queries, as discussed previously. </p>
			<p>While the Catalyst optimizer does an excellent job of producing the best possible physical query execution plan, it can still be thrown off by stale statistics on the table. Starting with Spark 3.0, <strong class="bold">Adaptive Query Execution </strong>can be deployed to make use of runtime statistics to choose the most efficient query execution plan. Adaptive Query Execution <a id="_idIndexMarker938"/>can be enabled using a <strong class="source-inline">spark.sql.adaptive.enabled</strong> configuration. These are just a few of the Spark SQL performance-tuning techniques available, and detailed descriptions of each can be found in <a id="_idIndexMarker939"/>the Apache Spark public documentation here: <a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html">https://spark.apache.org/docs/latest/sql-performance-tuning.html</a>.</p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor213"/>Summary</h1>
			<p>In this chapter, you learned about SQL as a declarative language that has been universally accepted as the language for structured data analysis because of its ease of use and expressiveness. You learned about the basic constructions of SQL, including the DDL and DML dialects of SQL. You were introduced to the Spark SQL engine as the unified distributed query engine that powers both Spark SQL and DataFrame APIs. SQL optimizers, in general, were introduced, and Spark's very own query optimizer Catalyst was also presented, along with its inner workings as to how it takes a Spark SQL query and converts it into Java JVM bytecode. A reference to the Spark SQL language was also presented, along with the most important DDL and DML statements, with examples. Finally, a few performance optimizations techniques were also discussed to help you get the best out of Spark SQL for all your data analysis needs. In the next chapter, we will extend our Spark SQL knowledge and see how external data analysis tools such as <strong class="bold">business intelligence</strong> (<strong class="bold">BI</strong>) tools and SQL analysis tools can also leverage Apache Spark's distributed SQL engine to process and visualize massive amounts of data in a fast and efficient manner.</p>
		</div>
	</div></body></html>