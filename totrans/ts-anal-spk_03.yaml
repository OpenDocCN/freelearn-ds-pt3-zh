- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides an overview of Apache Spark, explaining its distributed
    computing capabilities and suitability for processing large-scale time series
    data. It explains how Spark addresses the challenges of parallel processing, scalability,
    and fault tolerance. This foundational knowledge is essential as it sets the stage
    for leveraging Spark’s strengths in handling vast temporal datasets, facilitating
    efficient time series analysis. Practical knowledge of Spark’s role enhances practitioners’
    ability to harness its power for complex computations, making it a valuable resource
    for scalable, high-performance time series applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark and its architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Apache Spark works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation of Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The hands-on focus of this chapter will be to deploy a multi-node Apache Spark
    cluster to get familiar with important components of a deployment. The code for
    this chapter can be found in the `ch3` folder of this book’s GitHub repository
    at this URL: https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3.'
  prefs: []
  type: TYPE_NORMAL
- en: The hands-on section of this chapter will go into further detail. This requires
    some skills in building an open source environment. If you do not intend to build
    your own Apache Spark environment and your focus is instead on time series and
    using but not deploying Spark, you can skip the hands-on section of this chapter.
    You can use a managed platform such as Databricks, which comes pre-built with
    Spark, as we will do in future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: What is Apache Spark?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a distributed computing system that is open source, with a programming
    interface and clusters for parallel data processing at scale and with fault tolerance.
    Started as a project at Berkeley’s AMPLab in 2009, Spark became open source in
    2010 as part of the Apache Software Foundation. The original creators of Spark
    have since founded the Databricks company, which provides a managed version of
    Spark on their multi-cloud platform.
  prefs: []
  type: TYPE_NORMAL
- en: Spark can handle both batch and stream processing, making it a widely usable
    tool for big data processing. Bringing significant performance improvement over
    existing big data systems, Spark uses in-memory computing and optimized query
    execution for very fast analytic queries on data of any size. It is built on the
    concept of **Resilient Distributed Datasets** (**RDDs**) and DataFrames. These
    are collections of data elements distributed across a cluster of computers that
    can be operated on in parallel with fault tolerance. We will expand further on
    these concepts in the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Why use Apache Spark?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are numerous benefits to using Spark, which explains its popularity as
    a large-scale data processing solution, as shown in *Figure 3**.1* based on Google
    Trends. We can see here the increasing interest in Apache Spark software in line
    with the big data topic, while the trend for Hadoop software had been increasing,
    then decreased when it was overtaken by Apache Spark software in March 2017.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Increasing interest in Apache Spark compared to Hadoop and big
    data'
  prefs: []
  type: TYPE_NORMAL
- en: 'This surge in interest can be explained by some key benefits, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speed**: Spark runs up to 100 times faster in memory and up to 10 times faster
    even when running on disk, when compared to non-Spark Hadoop clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: With the use of distributed computing, Spark provides
    a fault-tolerant mechanism with recovery on failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modularity**: Spark includes support for SQL and structured data processing,
    machine learning, graph processing, and stream data processing. With libraries
    for diverse tasks, it can handle a wide range of data processing tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usability**: With APIs in Python, Java, Scala, and R, as well as Spark Connect,
    Spark is accessible to a wide range of developers and data scientists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility**: Spark can run on different platforms – including Databricks,
    Hadoop, Apache Mesos, and Kubernetes, standalone, or in the cloud. It can also
    access diverse data sources, which will be discussed in the *Interfaces and* *integrations*
    section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The growing popularity of Spark, and the numerous benefits explaining it, came
    over several years of evolution, which we will look at next.
  prefs: []
  type: TYPE_NORMAL
- en: Evolutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apache Spark has gone through several evolutions over the years, with the following
    major release versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.x**: These were early versions of Spark, starting with RDDs and some distributed
    data processing capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2.x**: Spark 2.0, in 2016, had significant improvements with the introduction
    of Spark SQL, structured streaming, and the Dataset API, which is more efficient
    than RDDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3.x**: From 2020, Spark 3.0 had further improvements, with **Adaptive Query
    Execution** (**AQE**), which dynamically adjusts query plans based on runtime
    statistics, enhanced performance optimizations, and dynamic partition pruning.
    It also included support for newer Python versions as well as additions to the
    **machine learning** **library** (**MLlib**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As of the time of writing, the latest version is 3.5.3\. To understand the
    direction the project is going in, let’s now zoom in on the highlights of some
    of the most recent versions, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PySpark** gains user-friendly support for Python-type hints, the pandas API
    on Spark, and enhanced performance thanks to optimizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive Query Execution improvements drive more efficient query execution and
    resource utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured Streaming** enhancements give better stability and performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes supports better integration and resource management capabilities
    for running Spark on Kubernetes. This results in greater efficiency and ease of
    use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API and SQL enhancements bring more efficient data processing and analysis,
    with new functions and improvements to existing ones. The key themes here are
    better usability and performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see from the preceding, the recent focus is on support for modern
    infrastructure, performance, and usability. As a tool for large-scale data processing
    and analysis, this is turning Spark into an even more widely adopted tool.
  prefs: []
  type: TYPE_NORMAL
- en: Distributions of Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With its popularity and wide adoption have come several distributions of Spark.
    These have been developed by different organizations, with Apache Spark, at its
    core, providing different integration capabilities, usability features, and enhancements
    to functionalities. Bundled with other big data tools, these distributions often
    offer improved management interfaces, enhanced security, and different storage
    integrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following distributions are the most common ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Spark** is the original open source version maintained by the Apache
    Software Foundation. It is the basis for the other distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Databricks Runtime** is developed by Databricks, the company founded by the
    creators of Spark. It is optimized for cloud environments, with a unified analytics
    platform facilitating collaboration between data engineers, data scientists, and
    business analysts. Databricks provides optimized Spark performance with a C++
    rewritten version called **Photon**, interactive notebooks, integrated workflows
    for data engineering with **Delta Live Tables** (**DLT**), and machine learning
    with MLflow, along with enterprise-grade compliance and security as part of its
    Unity Catalog-based governance capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloudera Data Platform** (**CDP**) includes Spark as part of its data platform,
    which includes Hadoop and other big data tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hortonworks Data Platform** (**HDP**), before merging with Cloudera, offered
    its own distribution that included Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft Azure** includes Spark as part of **Azure Databricks**, which is
    a first-party service on Azure, HDInsight, Synapse, and, moving forward, Fabric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Web Services** (**AWS**) offers Databricks in its Marketplace, as
    well as **Elastic MapReduce** (**EMR**) running as a cloud service to run big
    data frameworks such as Apache Spark on AWS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Cloud Platform** (**GCP**) hosts Databricks, as well as **Dataproc**,
    which is Google’s managed service for Apache Spark and Hadoop clusters in the
    cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From on-premises to cloud-native solutions to those that integrate with other
    data platforms, each distribution of Apache Spark answers different needs. When
    organizations choose a distribution, factors typically considered are performance
    requirements, ease of management, the existing technology stack, and specific
    capabilities provided by each distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have gone through what Apache Spark is, its benefits, and its evolutions,
    let’s dive deeper into its architecture and components.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary objective of an architecture with Apache Spark is to process large
    datasets across distributed clusters. Architectures can vary based on the specific
    requirements of the application, whether it is batch processing, stream processing,
    machine learning, querying for reports, or even a combination of these. A typical
    Spark architecture includes several key components that contribute to the data
    processing requirements. An example of such architecture is represented in *Figure
    3**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Example of Apache Spark-based architecture (standalone mode)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now drill down into what each of these parts does.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster managers are responsible for allocating resources to the clusters,
    which are the operating system environments on which the Spark workloads execute.
    These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standalone**: A basic cluster manager is included with Spark, making it easy
    to set up a cluster to get started. This cluster manager node is also known as
    the master node:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes**: Spark can be deployed to Kubernetes, which is an open source
    container-based system that automates the deployment, management, and scaling
    of containerized applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Mesos**: As a cluster manager, Mesos supports Spark, in addition to
    running Hadoop MapReduce.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop YARN**: Spark can share clusters and datasets with other Hadoop components
    when running with YARN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proprietary and commercial**: The solutions that incorporate Spark have their
    own cluster managers – usually a variation and improvement of the preceding open
    source versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will look at what is within these Spark clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Core, libraries, and API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have one or more clusters provided by the cluster manager, Spark Core
    then manages memory and fault recovery, as well as everything related to Spark
    jobs, such as scheduling, distributing, and monitoring. Spark Core abstracts storage
    read and write, using RDDs and, more recently, DataFrames as the data structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'On top of (and working very closely with) Core, several libraries and APIs
    provide additional functionalities specific to the data processing requirements.
    These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark SQL** allows querying structured data via SQL'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark Structured Streaming** processes data streaming from various sources,
    such as Kafka and Kinesis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLlib** provides multiple types of machine learning algorithms for classification,
    regression, and clustering, among others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphX** allows the use of graph algorithms for the creation, transformation,
    and querying of graphs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark is about data processing, and as such, an important part of the solution
    is the data structure, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs, DataFrames, and Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have mentioned RDDs and DataFrames a few times since the start of the chapter
    without going into detail, which we will do now, as well as introducing Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, these are the in-memory data structures representing the data and
    providing us with a programmatic way, more formerly termed an abstraction, to
    manipulate the data. Each of these data structures has its use cases, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An **RDD** is Spark’s fundamental data structure. Immutable and distributed,
    it can store data in memory across a cluster. Fault-tolerant, an RDD can automatically
    recover from failures. Note that in case of insufficient memory on the cluster,
    Spark does store part of the RDD on disk, but as this is managed behind the scenes,
    we will keep referring to RDDs as being in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are less and less likely to use RDDs, as more operations become possible
    with easier-to-use DataFrames, which we will see next. RDDs are more suitable
    for low-level transformations with direct manipulation of data, useful when you
    need low-level control over computations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A **DataFrame** is built upon an RDD as a distributed collection of data with
    named columns. This is like a table in a relational database. In addition to the
    more user-friendly higher-level API, which makes code more concise and easier
    to understand, DataFrames benefit from performance gain over RDDs thanks to Spark’s
    Catalyst optimizer, which we will discuss later in the chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already started using DataFrames as part of the hands-on exercises done
    so far. You may have noticed pandas DataFrames in addition to Spark DataFrames
    while doing the exercises. While similar in concept, they are part of different
    libraries and have their underlying implementation differences. Fundamentally,
    pandas DataFrames are on single machines while Spark DataFrames are distributed.
    pandas DataFrames can be converted to pandas-on-Spark DataFrames, with the benefit
    of pandas DataFrame API support in addition to parallelism.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A **Dataset** provides the type safety of RDDs with the optimizations of DataFrame.
    Type safety means that you can catch data type errors at compilation time, resulting
    in more runtime reliability. This is, however, dependent on the programming language
    supporting data type definition at the time of coding and verification and enforcement
    during compilation. As such, Datasets are only supported in Scala and Java, with
    Python and R, being dynamically typed, using DataFrames.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, you will get low-level control with RDDs, optimized higher-level
    abstraction with DataFrames, and type safety with Datasets. Which data structure
    to use depends on the specific requirements of your application.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have considered the internal components. We will next go into the
    external facing parts and how Spark integrates in the backend with storage and
    in the frontend with applications and users.
  prefs: []
  type: TYPE_NORMAL
- en: Interfaces and integrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When considering interfacing and integrating with the environment, there are
    a few ways in which this is fulfilled with Apache Spark. These are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`csv`, `json`, `xml`, `orc`, `avro`, `parquet`, and `protobuf`. Of these, Parquet
    is the most common as it gives good performance with snappy compression. In addition,
    Spark can be extended with packages to support several storage protocols and external
    data sources. Delta is one of these, which we will discuss further in [*Chapter
    4*](B18568_04.xhtml#_idTextAnchor087) and [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103).
    Other formats include Iceberg and Hudi. Note that we are talking here about the
    disk representation of the data, which is loaded into the memory-based data structures
    in RDDs and DataFrames discussed previously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We already have some experience with Spark and storage as part of the hands-on
    exercises done so far, where we have been reading CSV files from the local storage
    on the Databricks Community Edition’s Spark clusters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Applications**: This is the code with the logic for data processing, calling
    the Spark APIs and libraries for tasks such as data transformations, streaming,
    SQL queries, or machine learning. Developers can write in Python, R, Scala, or
    Java. The code is then executed on the Spark clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our experience with the application side has started as well, with the hands-on
    code used so far.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Platform user interface**: In addition to the web interface for Databricks
    Community Edition, which we have seen in the hands-on exercises, open source Apache
    Spark has a web **user interface** (**UI**) for monitoring the cluster and Spark
    applications. This provides insights into stages of job execution, resource usage,
    and the execution environment. Other data platforms that incorporate Apache Spark
    have their own UIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application end user interface**: Another type of UI is for end users consuming
    the outcome of the processing by Apache Spark. This can be reporting tools or,
    for example, an application using Apache Spark in the backend for data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section on Apache Spark architecture, we saw how the architecture enables
    data to be ingested from various sources into the Spark system, to be processed
    using Spark’s libraries, and then stored or served to users or downstream applications.
    The chosen architecture is dependent on requirements, such as latency, throughput,
    data size, and the complexity and type of data processing tasks. In the next section,
    we will focus on how Spark performs distributed processing at scale.
  prefs: []
  type: TYPE_NORMAL
- en: How Apache Spark works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we have viewed the components and their roles, but not
    so much about their interactions. We will now cover this part, to understand how
    Spark manages distributed data processing across a cluster, starting with transformations
    and actions.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations and actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apache Spark does, at a high level, two types of data operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`filter` and `groupBy`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count` and `save` types of operations, such as writing to Parquet files or
    using the `saveAsTable` operation. Actions trigger the execution of all transformations
    defined as prior steps in the DAG. This results in Spark computing the result
    of the series of transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distinction between transformations and actions is an important consideration
    when writing efficient Spark code. This enables Spark to use its execution engine
    for high-performance processing of jobs, which will be explained next.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs, stages, and tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark applications are executed as jobs, which are split into stages, and further
    into tasks, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Job**: Spark submits a job when an action is called on an RDD, DataFrame,
    or Dataset. The job is converted into a physical execution plan with several stages,
    which we will explain next. The purpose of a Spark job is to execute a sequence
    of computational steps as a logical unit of work to achieve a specific goal, such
    as aggregating data or sorting, with the aim of producing an output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stage**: A job can have multiple stages, as defined in its physical execution
    plan. A stage is a group of contiguous tasks that can be completed without moving
    data across the cluster. The data movement between stages is referred to as shuffle.
    The separation of a job into stages is beneficial as shuffling is costly in terms
    of performance impact. A stage is further broken down into tasks, which we will
    look at next.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task**: As the most granular unit of processing, a task is a single operation
    on a Spark in-memory partition of data. Each task processes a different set of
    data and can run in parallel with other tasks. These run on worker nodes, which
    we will look at next.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, jobs, stages, and tasks are related hierarchically. Spark applications
    can have multiple jobs, which are divided into stages based on data shuffling
    boundaries. Stages are further broken down into tasks, which run on different
    partitions in parallel on the cluster. This execution hierarchy allows Spark to
    efficiently distribute the workload across several nodes in a cluster, thus efficiently
    processing data at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen the units of processing, the next consideration is how
    these units are run on compute resources with driver and worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Driver and worker nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Driver and worker nodes are the compute resources created by the cluster manager
    to form part of a Spark cluster. They work together for Spark to process large
    datasets in parallel, using the resources of multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss these resources in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Driver nodes**: The driver node is where the main process of a Spark application
    runs. It principally does the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resources**: The driver requests resources from the cluster manager for processes
    to run on the worker nodes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SparkSession**: This is an object created by the driver and used to programmatically
    access Spark for data processing operations on the cluster.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tasks**: The driver translates code into tasks, schedules the tasks on worker
    nodes, and thereafter manages the tasks’ execution.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Worker nodes**: The worker node is where the data processing happens, via
    what is called the executor process. The executors interact with the storage and
    keep the data in their own memory space, as well as having their own set of CPU
    cores. The tasks are scheduled by the driver nodes to execute on the executors
    with direct communication between drivers and executors. They communicate on task
    status and results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Driver and worker node interaction**: *Figure 3**.3* summarizes the sequence
    of interactions between driver and worker nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Driver and worker nodes in action'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialization**: When the Spark application is started, the driver converts
    jobs into stages, further broken into tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scheduling**: The driver node schedules tasks on executors on the worker
    nodes, keeping track of status and rescheduling in case of failure.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Execution**: The tasks assigned by the driver are run by the executor on
    the worker node. In addition, the driver coordinates between executors when data
    needs to be shuffled across executors. This is required for certain operations
    such as joins.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Result**: Finally, the results of processing tasks by the executors are sent
    back to the driver node, which aggregates the results and sends them back to the
    user.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This cooperative process between the driver and worker nodes is at the core
    of Spark, enabling data processing at scale, in parallel across a cluster, while
    handling fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen the workings of Spark clusters, let’s zoom in on what
    makes it even more performant and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Catalyst optimizer and the Tungsten execution engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we’ve discussed that among the successive improvements brought to Apache
    Spark over the different versions, two notable ones are the Catalyst optimizer
    and the Tungsten execution engine. They play crucial roles in ensuring the Spark
    processes are optimized for fast execution time and efficient use of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Catalyst optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Introduced in Spark SQL, the Catalyst optimizer is a query optimization framework
    that significantly improves the performance of queries by using tree transformation
    on the **abstract syntax tree** (**AST**) of queries. It does this through several
    stages, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Analysis**: The query is transformed into a tree of operators called a logical
    plan.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Logical optimization**: The optimizer uses rule-based transformations to
    optimize the logical plan.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Physical planning**: The logical plan is converted to physical plans, which
    are based on the choice of algorithm to use for the query operation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cost model**: The physical plans are then compared based on a cost model
    to find the most efficient one in terms of time and resources.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Code generation**: As a final stage, the physical plan is converted to executable
    code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these stages, the Catalyst optimizer ensures that the most performant and
    efficient code is run.
  prefs: []
  type: TYPE_NORMAL
- en: Tungsten execution engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another area of focus is the efficient use of CPU and memory by Spark processes.
    The Tungsten execution engine achieves this in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code generation**: Tungsten works in conjunction with the Catalyst optimizer
    to generate optimized, compact code, which reduces runtime overhead while maximizing
    speed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cache-awareness**: Reducing cache misses improves the computation speed.
    Tungsten achieves this by making algorithms and data structures cache-aware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory management**: Tungsten manages memory efficiently, improving the impact
    of the cache while reducing the overhead of garbage collection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working together, the Catalyst optimizer and the Tungsten execution engine significantly
    contribute to Spark’s performance by optimizing query plans, generating efficient
    code, and reducing computation overhead. This improves Spark’s efficiency for
    big data processing, at scale and fast.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how Apache Spark works, we will move on to how to set
    up our own Spark environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in the previous chapters, we have successfully executed Spark code on
    Databricks Community Edition. This has, however, been on a single-node cluster.
    If we want to make full use of Spark’s parallel processing power, we will need
    multiple nodes. We have the option of using a Databricks-managed **Platform as
    a Service** (**PaaS**) cloud solution, another equivalent cloud PaaS, or we can
    build our own Apache Spark platform. This is what we will do now to deploy the
    environment as per *Figure 3**.2* shown in the section on *Apache* *Spark architecture*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you do not intend to build your own Apache Spark environment, you can skip
    the practical part of this section and use a managed Spark platform such as Databricks,
    as we will do in future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Using a container for deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can install Apache Spark directly on our local machine, but this will give
    us only one node. By deploying it in containers, such as Docker, we can have multiple
    containers running on the same machine. This effectively provides us with a way
    to have a multi-node cluster. Other advantages of this method include maintaining
    separation with the local execution environment, as well as providing a portable
    and repeatable way to deploy to other machines, including to cloud-based container
    services such as Amazon **Elastic Kubernetes Service** (**EKS**), **Azure Kubernetes
    Service** (**AKS**), or **Google Kubernetes** **Engine** (**GKE**).
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, we will be using Docker containers, starting by first installing
    Docker, then building and starting the containers with Apache Spark, and finally
    validating our deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative to Docker
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use Podman as an open source alternative to Docker. See more information
    here: [https://podman.io/](https://podman.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following instructions guide you on how to install Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following link to download and install Docker to your local environment,
    based on your OS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For macOS users, follow the instructions here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once Docker is installed, launch it as shown in *Figure 3**.4*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18568_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Docker Desktop'
  prefs: []
  type: TYPE_NORMAL
- en: 'On macOS, you may see a Docker Desktop warning: “**Another application changed
    your Desktop configurations**”. Depending on your setup, the following command
    may resolve the warning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once Docker Desktop is up and running, we can build the containers with Apache
    Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Network ports
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following network ports need to be available on your local machine or development
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark: `7077`, `8080`, `8081`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jupyter Notebook: `4040`, `4041`, `4042`, `8888`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can check for the current use of these ports by existing applications with
    the following command, run from the command line or terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you see the required ports in the list of ports already in use, you must
    either stop the application using that port or change the docker-compose file
    to use another port.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s assume that the output of the above `netstat` command reveals
    that port `8080` is already in use on your local machine or development environment,
    and you are not able to stop the existing application using this port.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, you will need to change port `8080` (meant for Apache Spark)
    in the `docker-compose.yaml` file to another, unused port. Just search and replace
    `8080` on the left of `:` to, say, `8070` if this port is free, as per the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Keep note of the new port and use this instead of the existing one whenever
    you need to type the corresponding URL. In this example, port `8080` is changed
    to `8070`, and the matching URL change for the Airflow web server is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From: [http://localhost:8080/](http://localhost:8080/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To: [http://localhost:8070/](http://localhost:8070/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You will need to change the network port in all URLs in the following sections
    that you had to modify as per this section.
  prefs: []
  type: TYPE_NORMAL
- en: Building and deploying Apache Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following instructions guide you on how to build and deploy the Docker
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first download the deployment script from the Git repository for this chapter,
    which is at the following URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will be using the git clone-friendly URL, which is the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To do this, start a terminal or command line and run the following commands:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the preceding is for a macOS or Linux/Unix-based system, and you will
    need to run the equivalent for Windows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'On macOS, you may see the following error when you run this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this case, you will need to reinstall the command-line tools with the following
    command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now start the container build and startup. A makefile is provided to
    simplify the process of starting and stopping the containers. The following command
    builds the Docker images for the containers and then starts them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Windows environment
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a Windows environment, you can install a Windows version of
    Make, as per the following documentation: [https://gnuwin32.sourceforge.net/packages/make.htm](https://gnuwin32.sourceforge.net/packages/make.htm)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will give the following or equivalent output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: make down
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Part 2: From Data to Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on the foundations, in this part, you will get a holistic view of all
    the stages involved in a time series analysis project, with a focus on the data
    and models. Starting with the ingestion and preparation of time series data, we
    will then do exploratory analysis to understand the nature of the time series.
    The data readiness and analysis will then lead us to the choice of model for analysis,
    development, and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B18568_04.xhtml#_idTextAnchor087), *End-to-End View of a Time
    Series Analysis Project*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B18568_05.xhtml#_idTextAnchor103), *Data Preparation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18568_06.xhtml#_idTextAnchor116), *Exploratory Data Analysis*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18568_07.xhtml#_idTextAnchor133), *Building and Testing Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
