["```py\n(spark\n   .read\n     .csv(\"/FileStore/shared_uploads/online_retail/\")\n   .write\n     .mode(\"overwrite\")\n     .format(\"parquet\")\n     .save(\"/tmp/retail.parquet\")\n)\n```", "```py\n(spark\n   .read\n     .format(\"parquet\")\n     .load(\"dbfs:/tmp/retail.parquet/part-00006-tid-6775149024880509486-a83d662e-809e-4fb7-beef-208d983f0323-212-1-c000.snappy.parquet\")\n   .count()\n) \n```", "```py\nfrom pyspark.sql.functions import lit\ndf1 = spark.range(3).withColumn(\"customer_id\", lit(\"1\"))\n(df1\n   .write\n     .format(\"parquet\")\n     .mode(\"overwrite\")\n   .save(\"/tmp/customer\")\n)\ndf2 = spark.range(2).withColumn(\"customer_id\", lit(2))\n(df2\n   .write\n     .format(\"parquet\")\n     .mode(\"append\")\n   .save(\"/tmp/customer\"))\n```", "```py\n(spark\n   .read\n     .option(\"header\", True)\n     .option(\"inferSchema\", True)\n     .csv(\"/FileStore/shared_uploads/online_retail/\")\n   .write\n     .format(\"delta\")\n     .save(\"/FileStore/shared_uploads/delta/online_retail\")\n)\n```", "```py\n%fs ls /FileStore/shared_uploads/delta/online_retail\n```", "```py\n%fs ls dbfs:/FileStore/shared_uploads/delta/online_retail/_delta_log/\n```", "```py\nspark.read.json(\"/FileStore/shared_uploads/delta/online_retail/_delta_log/\").show()\n```", "```py\nfrom pyspark.sql.functions import lit\ndf1 = spark.range(3).withColumn(\"customer_id\", lit(\"1\"))\n(df1\n   .write\n     .format(\"delta\")\n     .mode(\"overwrite\")\n   .save(\"/tmp/delta/customer\"))\ndf2 = spark.range(2).withColumn(\"customer_id\", lit(2))\n(df2\n   .write\n     .format(\"delta\")\n     .mode(\"append\")\n   .save(\"/tmp/delta/customer\"))\n```", "```py\nfrom pyspark.sql.functions import lit\ndf1 = spark.range(3)\n(df1\n   .write\n     .format(\"delta\")\n     .mode(\"overwrite\")\n   .save(\"/tmp/delta/customer\"))\ndf2 = spark.range(2).withColumn(\"customer_id\", lit(2))\n(df2\n   .write\n     .format(\"delta\")\n     .option(\"mergeSchema\", True)\n     .mode(\"append\")\n   .save(\"/tmp/delta/customer\"))\n```", "```py\nspark.read.format(\"delta\").load(\"/tmp/delta/customer\").show()\n```", "```py\nfrom pyspark.sql.functions import lit\ndf1 = spark.range(5).withColumn(\"customer_id\", lit(2))\ndf1.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/df1\")\n```", "```py\n%sql\nUPDATE delta.`/tmp/df1` SET customer_id = 5 WHERE id > 2;\nSELECT * FROM delta.`/tmp/df1`;\n```", "```py\n%sql\nDELETE FROM delta.`/tmp/df1` WHERE id = 4;\nSELECT * FROM delta.`/tmp/df1`;\n```", "```py\n%sql DESCRIBE HISTORY delta.`/tmp/df1`\n```", "```py\n%sql SELECT * from delta.`/tmp/delta/df1` VERSION AS OF 0\n```", "```py\n%sql \nINSERT OVERWRITE delta.`/tmp/df1`\nSELECT * from delta.`/tmp/df1` VERSION AS OF 0\n```", "```py\nfrom pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, StringType, DoubleType\nschema = (StructType()\n            .add(\"InvoiceNo\", StringType(), True)\n            .add(\"StockCode\", StringType(), True)\n            .add(\"Description\", StringType(), True)\n            .add(\"Quantity\", StringType(), True)\n            .add(\"InvoiceDate\", StringType(), True)\n            .add(\"UnitPrice\", StringType(), True)\n            .add(\"CustomerID\", StringType(), True)\n            .add(\"Country\", StringType(), True))\ndf1 = spark.read.schema(schema).option(\"header\", True).csv(\"dbfs:/FileStore/shared_uploads/online_retail/online_retail.csv\")\ndf2 = spark.read.schema(schema).option(\"header\", True).csv(\"dbfs:/FileStore/shared_uploads/online_retail/online_retail_II.csv\")\ndf1.printSchema()\ndf2.printSchema()\n```", "```py\nretail_df = df1.union(df2)\nretail_df.show()\n```", "```py\ndf3 = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(\"/FileStore/shared_uploads/countries_codes.csv\")\ncountry_df = (df3\n   .withColumnRenamed(\"OFFICIAL LANG CODE\", \"CountryCode\")\n   .withColumnRenamed(\"ISO2 CODE\", \"ISO2Code\")\n   .withColumnRenamed(\"ISO3 CODE\", \"ISO3Code\")\n   .withColumnRenamed(\"LABEL EN\", \"CountryName\")\n   .withColumnRenamed(\"Geo Shape\", \"GeoShape\")\n   .drop(\"ONU CODE\")\n   .drop(\"IS ILOMEMBER\")\n   .drop(\"IS RECEIVING QUEST\")\n   .drop(\"LABEL FR\")\n   .drop(\"LABEL SP\")\n   .drop(\"geo_point_2d\")\n)\nintegrated_df = retail_df.join(country_df, retail_df.Country == country_df.CountryName, \"left_outer\")\n```", "```py\nfrom pyspark.sql.functions import monotonically_increasing_id\nincome_df = spark.read.schema(schema).csv(\"/FileStore/shared_uploads/adult.data\").withColumn(\"idx\", monotonically_increasing_id())\nretail_dfx = retail_df.withColumn(\"CustomerIDx\", monotonically_increasing_id())\nincome_dfx = income_df.withColumn(\"CustomerIDx\", monotonically_increasing_id()) \nincome_df = spark.read.schema(schema).csv(\"/FileStore/shared_uploads/adult.data\").withColumn(\"idx\", monotonically_increasing_id())\nretail_dfx = integrated_df.withColumn(\"RetailIDx\", monotonically_increasing_id())\nincome_dfx = income_df.withColumn(\"IncomeIDx\", monotonically_increasing_id()) \nretail_enriched_df = retail_dfx.join(income_dfx, retail_dfx.RetailIDx == income_dfx.IncomeIDx, \"left_outer\")\n```", "```py\n(retail_enriched_df\n   .coalesce(1)\n   .write\n     .format(\"delta\", True)\n     .mode(\"overwrite\")\n    .save(\"/FileStore/shared_uploads/retail.delta\"))\n```", "```py\n%sql\nCREATE TABLE mysql_authors IF NOT EXISTS\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n  url \"jdbc:mysql://localhost:3306/pysparkdb\",\n  dbtable \"authors\",\n  user \"@@@@@@\",\n  password \"######\"\n);\n```", "```py\nfrom pyspark.sql.functions import rand, col\nauthors_df = spark.range(16).withColumn(\"salary\", rand(10)*col(\"id\")*10000)\nauthors_df.write.format(\"csv\").saveAsTable(\"author_salary\")\n```", "```py\n%sql\nSELECT\n  m.last_name,\n  m.first_name,\n  s.salary\nFROM\n  author_salary s\n  JOIN mysql_authors m ON m.uid = s.id\nORDER BY s.salary DESC\n```", "```py\nretail_enriched_df.printSchema()\n```", "```py\nretail_clean_df = (retail_enriched_df\n                    .drop(\"Country\")\n                    .drop(\"ISO2Code\")\n                    .drop(\"ISO3Code\")\n                    .drop(\"RetailIDx\")\n                    .drop(\"idx\")\n                    .drop(\"IncomeIDx\")\n                   )\n```", "```py\n(retail_enriched_df\n   .select(\"InvoiceNo\", \"InvoiceDate\")\n   .groupBy(\"InvoiceNo\", \"InvoiceDate\")\n   .count()\n   .show())\n```", "```py\n(retail_enriched_df.where(\"InvoiceNo in ('536373', '536382', '536387') AND StockCode in ('85123A', '22798', '21731')\")\n   .display()\n)\n```", "```py\nretail_nodupe = retail_clean_df.drop_duplicates([\"InvoiceNo\", \"InvoiceDate\", \"StockCode\"])\n```", "```py\n(retail_nodupe\n   .select(\"InvoiceNo\", \"InvoiceDate\", \"StockCode\")\n   .groupBy(\"InvoiceNo\", \"InvoiceDate\", \"StockCode\")\n   .count()\n   .where(\"count > 1\")\n   .show())\n```", "```py\nretail_final_df = (retail_nodupe.selectExpr(\n    \"InvoiceNo AS invoice_num\", \"StockCode AS stock_code\", \n    \"description AS description\", \"Quantity AS quantity\", \n    \"CAST(InvoiceDate AS TIMESTAMP) AS invoice_date\", \n    \"CAST(UnitPrice AS DOUBLE) AS unit_price\", \n    \"CustomerID AS customer_id\",\n    \"CountryCode AS country_code\",\n    \"CountryName AS country_name\", \"GeoShape AS geo_shape\",\n    \"age\", \"workclass AS work_class\", \n    \"fnlwgt AS final_weight\", \"education\", \n    \"CAST('education-num' AS NUMERIC) AS education_num\", \n    \"'marital-status' AS marital_status\", \"occupation\", \n    \"relationship\", \"race\", \"gender\", \n    \"CAST('capital-gain' AS DOUBLE) AS capital_gain\", \n    \"CAST('capital-loss' AS DOUBLE) AS capital_loss\", \n    \"CAST('hours-per-week' AS DOUBLE) AS hours_per_week\", \n    \"'native-country' AS native_country\")\n)\n```", "```py\nretail_final_df.write.format(\"delta\").save(\"dbfs:/FileStore/shared_uploads/delta/retail_silver.delta\")\n```"]