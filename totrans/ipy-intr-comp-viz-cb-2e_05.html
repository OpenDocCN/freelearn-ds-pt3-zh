<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 5. High-performance Computing"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. High-performance Computing</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Accelerating pure Python code with Numba and Just-In-Time compilation</li><li class="listitem" style="list-style-type: disc">Accelerating array computations with Numexpr</li><li class="listitem" style="list-style-type: disc">Wrapping a C library in Python with ctypes</li><li class="listitem" style="list-style-type: disc">Accelerating Python code with Cython</li><li class="listitem" style="list-style-type: disc">Optimizing Cython code by writing less Python and more C</li><li class="listitem" style="list-style-type: disc">Releasing the GIL to take advantage of multi-core processors with Cython and OpenMP</li><li class="listitem" style="list-style-type: disc">Writing massively parallel code for NVIDIA graphics cards (GPUs) with CUDA</li><li class="listitem" style="list-style-type: disc">Writing massively parallel code for heterogeneous platforms with OpenCL</li><li class="listitem" style="list-style-type: disc">Distributing Python code across multiple cores with IPython</li><li class="listitem" style="list-style-type: disc">Interacting with asynchronous parallel tasks in IPython</li><li class="listitem" style="list-style-type: disc">Parallelizing code with MPI in IPython</li><li class="listitem" style="list-style-type: disc">Trying the Julia language in the notebook</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec45"/>Introduction</h1></div></div></div><p>The previous chapter presented techniques for code optimization. Sometimes, these methods are not sufficient, and we need to resort to advanced high-performance computing techniques.</p><p>In this chapter, we will see three broad, but not mutually exclusive categories of methods:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Just-In-Time compilation</strong></span> (<span class="strong"><strong>JIT</strong></span>)<a id="id599" class="indexterm"/> of Python code</li><li class="listitem" style="list-style-type: disc">Resorting to a lower-level language, such as C, from Python</li><li class="listitem" style="list-style-type: disc">Dispatching tasks across multiple computing units using parallel computing</li></ul></div><p>With Just-In-Time compilation, Python code is dynamically compiled into a lower-level language. Compilation occurs at runtime rather than ahead of execution. The translated code runs faster since it is compiled rather that interpreted. JIT compilation is a popular technique as it can lead to fast <span class="emphasis"><em>and</em></span> high-level languages, whereas these two characteristics used to be mutually exclusive in general.</p><p>JIT compilation techniques are implemented in packages such as<a id="id600" class="indexterm"/> <span class="strong"><strong>Numba</strong></span>, <a id="id601" class="indexterm"/><span class="strong"><strong>Numexpr</strong></span>, <span class="strong"><strong>Blaze</strong></span>, and<a id="id602" class="indexterm"/> others. In this chapter, we will cover the first two packages. Blaze is a promising project but it is still in its infancy at the time of writing this book.</p><p>We will also introduce a new high-level language, <span class="strong"><strong>Julia</strong></span>, which <a id="id603" class="indexterm"/>uses JIT compilation to achieve high performance. This language can be used effectively in the IPython notebook, thanks to the<a id="id604" class="indexterm"/> <span class="strong"><strong>IJulia</strong></span> package.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note22"/>Note</h3><p>
<span class="strong"><strong>PyPy</strong></span> (<a class="ulink" href="http://pypy.org">http://pypy.org</a>), successor <a id="id605" class="indexterm"/>of Psyco, is <a id="id606" class="indexterm"/>another related project. This alternative implementation of Python (the reference implementation being CPython) integrates a JIT compiler. Thus, it is typically much faster than CPython. However, at the time of writing this book, PyPy does not fully support NumPy yet. Additionally, PyPy and SciPy tend to form distinct communities.</p></div></div><p>Resorting to a lower-level language such as C is another interesting method<a id="id607" class="indexterm"/>. Popular libraries include<a id="id608" class="indexterm"/> <span class="strong"><strong>ctypes</strong></span>, <a id="id609" class="indexterm"/><span class="strong"><strong>SWIG</strong></span>, or<a id="id610" class="indexterm"/> <span class="strong"><strong>Cython</strong></span>. Using ctypes requires writing C code and having access to a C compiler, or using a compiled C library. By contrast, Cython lets us write code in a superset of Python, which is translated to C with various performance results. Unfortunately, it is not always easy to write efficient Cython code. In this chapter, we will cover ctypes and Cython, and we will see how to achieve interesting speedups on complex examples.</p><p>Finally, we will cover two classes of parallel computing techniques: using multiple CPU cores with IPython and using massively parallel architectures such as <a id="id611" class="indexterm"/><span class="strong"><strong>Graphics Processing Units</strong></span> (<span class="strong"><strong>GPUs</strong></span>).</p><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A blog post on PyPy and NumPy by Travis Oliphant<a id="id612" class="indexterm"/> available<a id="id613" class="indexterm"/> at <a class="ulink" href="http://technicaldiscovery.blogspot.com/2011/10/thoughts-on-porting-numpy-to-pypy.html">http://technicaldiscovery.blogspot.com/2011/10/thoughts-on-porting-numpy-to-pypy.html</a></li><li class="listitem" style="list-style-type: disc">Interfacing Python with C, a tutorial in<a id="id614" class="indexterm"/> the scikit lectures notes available at <a class="ulink" href="http://scipy-lectures.github.io/advanced/interfacing_with_c/interfacing_with_c.html">http://scipy-lectures.github.io/advanced/interfacing_with_c/interfacing_with_c.html</a></li></ul></div><div class="section" title="CPython and concurrent programming"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec152"/>CPython and concurrent programming</h2></div></div></div><p>Python is sometimes<a id="id615" class="indexterm"/> criticized for its poor native support of multi-core processors. Let's explain why.</p><p>The mainstream implementation of the Python language is <a id="id616" class="indexterm"/><span class="strong"><strong>CPython</strong></span>, written in C. CPython integrates a mechanism called the <a id="id617" class="indexterm"/><span class="strong"><strong>Global Interpreter Lock</strong></span> (<span class="strong"><strong>GIL</strong></span>). As <a id="id618" class="indexterm"/>mentioned at <a class="ulink" href="http://wiki.python.org/moin/GlobalInterpreterLock">http://wiki.python.org/moin/GlobalInterpreterLock</a>:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>The GIL facilitates memory management by preventing multiple native threads from executing Python bytecodes at once.</em></span></p></blockquote></div><p>In other words, by<a id="id619" class="indexterm"/> disabling concurrent threads within one Python process, the GIL considerably simplifies the memory management system. Memory management is therefore not thread-safe in CPython.</p><p>An important implication is that with CPython, a pure Python program cannot be easily executed in parallel over multiple cores. This is an important issue as modern processors contain more and more cores.</p><p>What possible solutions do we have in order to take advantage of multi-core processors?</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Removing the GIL in CPython. This solution has been tried but has never made it into CPython. It would bring too much complexity in the implementation of CPython, and it would degrade the performance of single-threaded programs.</li><li class="listitem" style="list-style-type: disc">Using multiple processes instead of multiple threads. This is a popular solution; it can be done with the native <span class="strong"><strong>multiprocessing</strong></span> module or with IPython. We will cover the latter in this chapter.</li><li class="listitem" style="list-style-type: disc">Rewriting specific portions of your code in Cython and replacing all Python variables with C variables. This allows you to remove the GIL temporarily in a loop, thereby enabling use of multi-core processors. We will cover this solution in the <span class="emphasis"><em>Releasing the GIL to take advantage of multi-core processors with Cython and OpenMP</em></span> recipe.</li><li class="listitem" style="list-style-type: disc">Implementing a specific portion of your code in a language that offers better support for multi-core processors and calling it from your Python program.</li><li class="listitem" style="list-style-type: disc">Making your code use the NumPy functions that benefit from multi-core processors, such as <code class="literal">numpy.dot()</code>. NumPy needs to be compiled with BLAS/LAPACK/ATLAS/MKL.</li></ul></div><p>A must-read reference on the GIL<a id="id620" class="indexterm"/> can be found at <a class="ulink" href="http://www.dabeaz.com/GIL/">http://www.dabeaz.com/GIL/</a>.</p></div><div class="section" title="Compiler-related installation instructions"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec153"/>Compiler-related installation instructions</h2></div></div></div><p>In this section, we will give a few <a id="id621" class="indexterm"/>instructions for using compilers with Python. Use-cases include using ctypes, using Cython, and building C extensions for Python.</p><div class="section" title="Linux"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec33"/>Linux</h3></div></div></div><p>On Linux, you <a id="id622" class="indexterm"/>can install <a id="id623" class="indexterm"/>the <span class="strong"><strong>GCC</strong></span> (<span class="strong"><strong>GNU Compiler Collection</strong></span>)<a id="id624" class="indexterm"/> compiler. On Ubuntu or Debian, you can install GCC with the <code class="literal">sudo apt-get install build-essential</code> command.</p></div><div class="section" title="Mac OS X"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec34"/>Mac OS X</h3></div></div></div><p>On <a id="id625" class="indexterm"/>Mac OS X, you can<a id="id626" class="indexterm"/> install Apple XCode. Starting with XCode 4.3, you must manually install command-line tools from XCode's menu through <span class="strong"><strong>Preferences</strong></span> | <span class="strong"><strong>Downloads</strong></span> | <span class="strong"><strong>Command Line Tools</strong></span>.</p></div><div class="section" title="Windows"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec35"/>Windows</h3></div></div></div><p>On Windows, using compilers with<a id="id627" class="indexterm"/> Python is notoriously tedious. It is generally difficult to find all the necessary instructions online. We <a id="id628" class="indexterm"/>detail these instructions here (you'll also find them on the book's GitHub repository):</p><p>The instructions differ according to whether you use a 32-bit or 64-bit version of Python, and whether you use Python 2.x or Python 3.x. To quickly find out this information in a Python terminal, type the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import sys</strong></span>
<span class="strong"><strong>print(sys.version)</strong></span>
<span class="strong"><strong>print(64 if sys.maxsize &gt; 2**32 else 32)</strong></span>
</pre></div><div class="section" title="Python 32-bit"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec01"/>Python 32-bit</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, you <a id="id629" class="indexterm"/>need to<a id="id630" class="indexterm"/> install a C compiler. With Python 32-bit, you can download and install MinGW<a id="id631" class="indexterm"/> from <a class="ulink" href="http://www.mingw.org">http://www.mingw.org</a>, which is an open source distribution of GCC.</li><li class="listitem">Depending on your version of the <code class="literal">distutils</code> library, you may need to manually fix a bug in its source code. Open <code class="literal">C:\Python27\Lib\distutils\cygwinccompiler.py</code> in a text editor (or a similar path depending on your specific configuration), and replace all occurrences of <code class="literal">-mno-cygwin</code> with an empty string.</li><li class="listitem">Open or <a id="id632" class="indexterm"/>create a text <a id="id633" class="indexterm"/>file named <code class="literal">distutils.cfg</code> in <code class="literal">C:\Python27\Lib\distutils\</code> and add the following lines:<div class="informalexample"><pre class="programlisting">[build]
compiler = mingw32</pre></div></li></ol></div></div><div class="section" title="Python 64-bit"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec02"/>Python 64-bit</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">With Python 2.x, you<a id="id634" class="indexterm"/> need Visual Studio 2008 Express. With Python 3.x, you need <a id="id635" class="indexterm"/>Visual Studio 2010 Express.</li><li class="listitem">You also need the Microsoft Windows SDK (2008 or 2010 according to your Python version):<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Python 2.x</strong></span>: Microsoft <a id="id636" class="indexterm"/>Windows SDK for Windows 7 and .NET Framework 3.5 available at <a class="ulink" href="http://www.microsoft.com/en-us/download/details.aspx?id=3138">http://www.microsoft.com/en-us/download/details.aspx?id=3138</a></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Python 3.x</strong></span>: Microsoft <a id="id637" class="indexterm"/>Windows SDK for Windows 7 and .NET Framework 4 available at <a class="ulink" href="http://www.microsoft.com/en-us/download/details.aspx?id=8279">http://www.microsoft.com/en-us/download/details.aspx?id=8279</a></li></ul></div></li><li class="listitem">Make sure that the path to the folder containing <code class="literal">cl.exe</code> is in the system's <code class="literal">PATH</code> environment variable. This path should look like <code class="literal">C:\Program Files (x86)\Microsoft Visual Studio 9.0\VC\bin\amd64</code> (using Visual Studio 2008's C compiler available with the Microsoft Windows SDK for Windows 7 and .NET Framework 3.5).</li><li class="listitem">You need to execute a few commands in Windows' command-line terminal every time you want to use the compiler with Python (for example, before typing <code class="literal">ipython notebook</code>):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>call "C:\Program Files\Microsoft SDKs\Windows\v7.0\Bin\SetEnv.Cmd" /x64 /release</strong></span>
<span class="strong"><strong>set DISTUTILS_USE_SDK=1</strong></span>
</pre></div></li></ol></div></div><div class="section" title="DLL hell"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec03"/>DLL hell</h4></div></div></div><p>When using compiled <a id="id638" class="indexterm"/>packages, particularly <a id="id639" class="indexterm"/>those obtained on Chris Gohlke's webpage at <a class="ulink" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a>, you may get obscure DLL-related errors. To fix those problems, you can open the spurious DLLs in Dependency Walker<a id="id640" class="indexterm"/> available at <a class="ulink" href="http://www.dependencywalker.com">http://www.dependencywalker.com</a>. This program can tell you that a DLL is missing. You can search for it in your computer and add its location to the <code class="literal">PATH</code> environment variable.</p></div><div class="section" title="References"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec04"/>References</h4></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Installing Cython on Windows<a id="id641" class="indexterm"/>, at <a class="ulink" href="http://wiki.cython.org/InstallingOnWindows">http://wiki.cython.org/InstallingOnWindows</a></li><li class="listitem" style="list-style-type: disc">Cython on Windows 64-bit<a id="id642" class="indexterm"/>, at <a class="ulink" href="https://github.com/cython/cython/wiki/64BitCythonExtensionsOnWindows">https://github.com/cython/cython/wiki/64BitCythonExtensionsOnWindows</a></li><li class="listitem" style="list-style-type: disc">Building Python wheels<a id="id643" class="indexterm"/> for Windows, at <a class="ulink" href="http://cowboyprogrammer.org/building-python-wheels-for-windows/">http://cowboyprogrammer.org/building-python-wheels-for-windows/</a></li></ul></div></div></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Accelerating pure Python code with Numba and just-in-time compilation"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec46"/>Accelerating pure Python code with Numba and just-in-time compilation</h1></div></div></div><p>Numba (<a class="ulink" href="http://numba.pydata.org">http://numba.pydata.org</a>) <a id="id644" class="indexterm"/>is a <a id="id645" class="indexterm"/>package created by Continuum Analytics<a id="id646" class="indexterm"/> (<a class="ulink" href="http://www.continuum.io">http://www.continuum.io</a>). At the time of writing, Numba is still a young and relatively experimental package, but its technology is promising. Numba takes pure Python code and translates it automatically (just-in-time) into optimized machine code. In practice, this means that we can write a non-vectorized function in pure Python, using <code class="literal">for</code> loops, and have this function vectorized automatically by using a single decorator. Performance <a id="id647" class="indexterm"/>speedups when compared to pure Python code<a id="id648" class="indexterm"/> can reach several orders of <a id="id649" class="indexterm"/>magnitude and may<a id="id650" class="indexterm"/> even outmatch manually-vectorized NumPy code.</p><p>In this section, we will show how to accelerate pure Python code generating the Mandelbrot fractal.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec154"/>Getting ready</h2></div></div></div><p>The easiest way to install Numba is to use the Anaconda distribution (also maintained by Continuum Analytics), and type in a terminal <code class="literal">conda install numba</code>. On Windows, an alternative is to download a binary installer from <a id="id651" class="indexterm"/>Chris Gohlke's webpage at <a class="ulink" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#numba">http://www.lfd.uci.edu/~gohlke/pythonlibs/#numba</a>. In this case, there are dependencies (Numpy-MKL, LLVMPy, llvmmath, and Meta), all available on the same page.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec155"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's<a id="id652" class="indexterm"/> import <a id="id653" class="indexterm"/>NumPy and <a id="id654" class="indexterm"/>define a<a id="id655" class="indexterm"/> few variables:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
In [2]: size = 200
        iterations = 100</pre></div></li><li class="listitem">The following function generates the fractal in pure Python. It accepts an empty array <code class="literal">m</code> as argument.<div class="informalexample"><pre class="programlisting">In [3]: def mandelbrot_python(m, size, iterations):
            for i in range(size):
                for j in range(size):
                    c = -2 + 3./size*j + 1j*(1.5-3./size*i)
                    z = 0
                    for n in range(iterations):
                        if np.abs(z) &lt;= 10:
                            z = z*z + c
                            m[i, j] = n
                        else:
                            break</pre></div></li><li class="listitem">Let's run the simulation and display the fractal:<div class="informalexample"><pre class="programlisting">In [4]: m = np.zeros((size, size))
        mandelbrot_python(m, size, iterations)
In [5]: import matplotlib.pyplot as plt
        %matplotlib inline
        plt.imshow(np.log(m), cmap=plt.cm.hot)
        plt.xticks([]); plt.yticks([])</pre></div><div class="mediaobject"><img src="images/4818OS_05_01.jpg" alt="How to do it…"/><div class="caption"><p>The Mandelbrot fractal</p></div></div></li><li class="listitem">Now, we<a id="id656" class="indexterm"/> evaluate the time taken by this<a id="id657" class="indexterm"/> function:<div class="informalexample"><pre class="programlisting">In [6]: %%timeit m = np.zeros((size, size))
        mandelbrot_python(m, size, iterations)
1 loops, best of 1: 6.18 s per loop</pre></div></li><li class="listitem">Let's try to accelerate<a id="id658" class="indexterm"/> this function using<a id="id659" class="indexterm"/> Numba. First, we import the package:<div class="informalexample"><pre class="programlisting">In [7]: import numba
        from numba import jit, complex128</pre></div></li><li class="listitem">Next, we add the <code class="literal">@jit</code> decorator right above the function definition. Numba tries to automatically infer the type of the local variables, but we can also specify the types explicitly:<div class="informalexample"><pre class="programlisting">In [8]: @jit(locals=dict(c=complex128, z=complex128))
        def mandelbrot_numba(m, size, iterations):
            for i in range(size):
                for j in range(size):
                    c = -2 + 3./size*j + 1j*(1.5-3./size*i)
                    z = 0
                    for n in range(iterations):
                        if np.abs(z) &lt;= 10:
                            z = z*z + c
                            m[i, j] = n
                        else:
                            break</pre></div></li><li class="listitem">This function works just like the pure Python version. How much faster is it?<div class="informalexample"><pre class="programlisting">In [10]: %%timeit m = np.zeros((size, size))
         mandelbrot_numba(m, size, iterations)
1 loops, best of 10: 44.8 ms per loop</pre></div><p>The Numba version is more than 100 times faster than the pure Python version here!</p></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec156"/>How it works…</h2></div></div></div><p>Python <a id="id660" class="indexterm"/>bytecode is normally interpreted at runtime by the<a id="id661" class="indexterm"/> Python<a id="id662" class="indexterm"/> interpreter (for example, CPython). By contrast, a Numba<a id="id663" class="indexterm"/> function is parsed and translated directly to machine code ahead of execution, using a powerful compiler architecture named <span class="strong"><strong>LLVM</strong></span> (<span class="strong"><strong>Low Level Virtual Machine</strong></span>)<a id="id664" class="indexterm"/>. Citing the official documentation:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>Numba is aware of NumPy arrays as typed memory regions and so can speedup code using NumPy arrays. Other, less well-typed code will be translated to Python C-API calls effectively removing the "interpreter" but not removing the dynamic indirection.</em></span></p></blockquote></div><p>Numba is not able to compile any Python functions. There are also some subtle restrictions on the type of local variables. Numba tries to infer the type of the function's variables automatically, but it is not always successful. In this case, we can specify the types explicitly.</p><p>Numba generally gives the most impressive speedups on functions that involve tight loops on NumPy arrays (such as in this recipe).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note23"/>Note</h3><p>
<span class="strong"><strong>Blaze</strong></span>, another project <a id="id665" class="indexterm"/>from Continuum Analytics, is the next generation of NumPy. It will offer data structures with much more flexibility than NumPy arrays, and it will support out-of-core computations. Together with Numba, Blaze will form a highly efficient compiler-like infrastructure for big data algorithms and complex numerical simulations. We can expect Blaze to play an important role in the future, as it should combine the nice and easy syntax of Python with the performance of native code and parallel processing techniques (notably multi-core processors and Graphical Processing Units). Other worthwhile related projects, but slightly older than Blaze and Numba, include <span class="strong"><strong>Theano</strong></span><a id="id666" class="indexterm"/> and <span class="strong"><strong>Numexpr</strong></span><a id="id667" class="indexterm"/> (which we will see in the next recipe).</p></div></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec157"/>There's more…</h2></div></div></div><p>Let's compare the performance of Numba with manually-vectorized code<a id="id668" class="indexterm"/> using NumPy, which is the standard way of accelerating pure <a id="id669" class="indexterm"/>Python code such as the code given in this recipe. In practice, it means replacing the code inside the two loops over <code class="literal">i</code> and <code class="literal">j</code> with array computations. This is relatively easy here as the operations closely follow the <span class="strong"><strong>Single Instruction, Multiple Data</strong></span> (<span class="strong"><strong>SIMD</strong></span>)<a id="id670" class="indexterm"/> paradigm:</p><div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import matplotlib.pyplot as plt
        %matplotlib inline
In [2]: def initialize(size):
            x, y = np.meshgrid(np.linspace(-2, 1, size),
                               np.linspace(-1.5, 1.5, size))
            c = x + 1j*y
            z = c.copy()
            m = np.zeros((size, size))
            return c, z, m
In [3]: size = 200
        iterations = 100
        
        def mandelbrot_numpy(c, z, m, iterations):
            for n in range(iterations):
                indices = np.abs(z) &lt;= 10
                z[indices] = z[indices]**2 + c[indices]
                m[indices] = n
In [4]: %%timeit -n1 -r10 c, z, m = initialize(size)
        mandelbrot_numpy(c, z, m, iterations)
1 loops, best of 10: 245 ms per loop</pre></div><p>Here, Numba beats NumPy. However, we cannot draw any firm conclusion from this single experiment. Whether Numba or NumPy is faster depends on the particular implementation of the algorithm, simulation parameters, machine characteristics, and so on.</p><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Documentation of Numba<a id="id671" class="indexterm"/> available at <a class="ulink" href="http://numba.pydata.org/doc.html">http://numba.pydata.org/doc.html</a></li><li class="listitem" style="list-style-type: disc">Types supported by Numba available at <a class="ulink" href="http://numba.pydata.org/numba-doc/dev/types.html">http://numba.pydata.org/numba-doc/dev/types.html</a></li><li class="listitem" style="list-style-type: disc">Numba examples available at <a class="ulink" href="http://numba.pydata.org/numba-doc/dev/examples.html">http://numba.pydata.org/numba-doc/dev/examples.html</a></li><li class="listitem" style="list-style-type: disc">Blaze available<a id="id672" class="indexterm"/> at <a class="ulink" href="http://blaze.pydata.org">http://blaze.pydata.org</a></li><li class="listitem" style="list-style-type: disc">Theano <a id="id673" class="indexterm"/>available at <a class="ulink" href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec158"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Accelerating array computations with Numexpr</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Accelerating array computations with Numexpr"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec47"/>Accelerating array computations with Numexpr</h1></div></div></div><p>
<span class="strong"><strong>Numexpr</strong></span> <a id="id674" class="indexterm"/>is a package that improves upon a weakness of NumPy; the evaluation of complex array expressions is sometimes slow. The reason is that multiple temporary arrays are created for the intermediate steps, which is suboptimal with large arrays. Numexpr evaluates algebraic expressions involving arrays, parses them, compiles them, and finally executes them faster than NumPy.</p><p>This principle is<a id="id675" class="indexterm"/> somewhat similar to Numba, in <a id="id676" class="indexterm"/>that normal Python code is compiled dynamically by a JIT compiler. However, Numexpr only tackles algebraic array expressions rather than arbitrary Python code. We will see how that works in this recipe.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec159"/>Getting ready</h2></div></div></div><p>You will find the instructions to install Numexpr<a id="id677" class="indexterm"/> in the documentation available at <a class="ulink" href="http://github.com/pydata/numexpr">http://github.com/pydata/numexpr</a>.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec160"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import NumPy and Numexpr:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import numexpr as ne</pre></div></li><li class="listitem">Then, we generate three large vectors:<div class="informalexample"><pre class="programlisting">In [2]: x, y, z = np.random.rand(3, 1000000)</pre></div></li><li class="listitem">Now, we evaluate the time taken by NumPy to calculate a complex algebraic expression involving our vectors:<div class="informalexample"><pre class="programlisting">In [3]: %timeit x + (y**2 + (z*x + 1)*3)
10 loops, best of 3: 48.1 ms per loop</pre></div></li><li class="listitem">Let's perform the same calculation with Numexpr. We need to give the expression as a string:<div class="informalexample"><pre class="programlisting">In [4]: %timeit ne.evaluate('x + (y**2 + (z*x + 1)*3)')
100 loops, best of 3: 11.5 ms per loop</pre></div></li><li class="listitem">Numexpr can use multiple cores. Here, we have 2 physical cores and 4 virtual threads with Intel's Hyper-Threading Technology. We can specify how many cores we want Numexpr to use using the <code class="literal">set_num_threads()</code> function:<div class="informalexample"><pre class="programlisting">In [5]: ne.ncores
Out[5]: 4
In [6]: for i in range(1, 5):
            ne.set_num_threads(i)
            %timeit ne.evaluate('x + (y**2 + (z*x + 1)*3)')
10 loops, best of 3: 19.4 ms per loop
10 loops, best of 3: 14 ms per loop
10 loops, best of 3: 12.8 ms per loop
10 loops, best of 3: 11.5 ms per loop</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec161"/>How it works...</h2></div></div></div><p>Numexpr analyzes<a id="id678" class="indexterm"/> the array expression, parses it, and<a id="id679" class="indexterm"/> compiles it into a lower-level language. Numexpr is aware of CPU-vectorized instructions as well as CPU cache characteristics. As such, Numexpr can optimize vectorized computations dynamically.</p><p>There is some overlap between Numexpr, Numba, and Blaze. We can probably expect some crosstalk between these projects in the future.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec162"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Accelerating pure Python code with Numba and just-in-time compilation</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Wrapping a C library in Python with ctypes"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec48"/>Wrapping a C library in Python with ctypes</h1></div></div></div><p>Wrapping a C library in Python allows us to leverage existing C code or to implement a critical part of the code in a fast language such as C.</p><p>It is relatively easy to use <a id="id680" class="indexterm"/>externally-compiled libraries with <a id="id681" class="indexterm"/>Python. The first possibility is to call a command-line executable with an <code class="literal">os.system</code> command, but this method does not extend to compiled libraries (on Windows, <a id="id682" class="indexterm"/><span class="strong"><strong>Dynamically Linked Libraries</strong></span>, or <span class="strong"><strong>DLLs</strong></span>). A more powerful method consists of using a native Python module called <a id="id683" class="indexterm"/><span class="strong"><strong>ctypes</strong></span>. This module allows us to call functions defined in a compiled library (written in C) from Python. The <code class="literal">ctypes</code> module<a id="id684" class="indexterm"/> takes care of the data type conversions between C and Python. In addition, the <code class="literal">numpy.ctypeslib</code> module<a id="id685" class="indexterm"/> provides facilities to use NumPy arrays wherever data buffers are used in the external library.</p><p>In this example, we will rewrite the code of the Mandelbrot fractal in C, compile it in a shared library, and call it from Python.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec163"/>Getting ready</h2></div></div></div><p>The code of this<a id="id686" class="indexterm"/> recipe is written for Windows. It can be adapted<a id="id687" class="indexterm"/> to other systems with minor changes.</p><p>A C compiler is required. You will find all compiler-related instructions in this chapter's introduction. In particular, for the C compiler to work on Windows, you need to execute a sequence of instructions in the Windows terminal before launching the IPython notebook. You will find a batch script with the appropriate instructions on the book's repository (in the folder containing the code for this chapter).</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec164"/>How to do it…</h2></div></div></div><p>The first step is to write and compile the Mandelbrot example in C. The second step is to access the library from Python using ctypes. If you are only interested in discovering how to access an existing compiled library, you can go straight to step 3, assuming that <code class="literal">mandelbrot.dll</code> is a compiled library defining a function <a id="id688" class="indexterm"/>named <code class="literal">mandelbrot()</code>.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's write the code of the Mandelbrot fractal in C:<div class="informalexample"><pre class="programlisting">In [1]: %%writefile mandelbrot.c
        
        // Needed when creating a DLL.
        #define EXPORT __declspec(dllexport)
        
        #include "stdio.h"
        #include "stdlib.h"
        
        // This function will be available in the DLL.
        EXPORT void __stdcall mandelbrot(int size,
                                         int iterations,
                                         int *col)
        {
            // Variable declarations.
            int i, j, n, index;
            double cx, cy;
            double z0, z1, z0_tmp, z0_2, z1_2;
            
            // Loop within the grid.
            for (i = 0; i &lt; size; i++)
            {
                cy = -1.5 + (double)i / size * 3;
                for (j = 0; j &lt; size; j++)
                {
                    // We initialize the loop of the 
                    // system.
                    cx = -2.0 + (double)j / size * 3;
                    index = i * size + j;
                    // Let's run the system.
                    z0 = 0.0;
                    z1 = 0.0;
                    for (n = 0; n &lt; iterations; n++)
                    {
                        z0_2 = z0 * z0;
                        z1_2 = z1 * z1;
                        if (z0_2 + z1_2 &lt;= 100)
                        {
                            // Update the system.
                            z0_tmp = z0_2 - z1_2 + cx;
                            z1 = 2 * z0 * z1 + cy;
                            z0 = z0_tmp;
                            col[index] = n;
                        }
                        else
                        {
                            break;
                        }
                    }
                }
            }
        }</pre></div></li><li class="listitem">Now, let's <a id="id689" class="indexterm"/>build this C source file into a DLL<a id="id690" class="indexterm"/> with Microsoft Visual Studio's <code class="literal">cl.exe</code>. The <code class="literal">/LD</code> option specifies that a DLL is to be created:<div class="informalexample"><pre class="programlisting">In [2]: !cl /LD mandelbrot.c
/out:mandelbrot.dll
Creating library mandelbrot.lib and object mandelbrot.exp</pre></div></li><li class="listitem">Let's access the library with ctypes:<div class="informalexample"><pre class="programlisting">In [3]: import ctypes
In [4]: # Load the DLL file in Python.
        lb = ctypes.CDLL('mandelbrot.dll')
        lib = ctypes.WinDLL(None, handle=lb._handle)
        # Access the mandelbrot function.
        mandelbrot = lib.mandelbrot</pre></div></li><li class="listitem">NumPy and ctypes allow us to wrap the C function defined in the DLL:<div class="informalexample"><pre class="programlisting">In [5]: from numpy.ctypeslib import ndpointer
In [6]: # Define the types of the output and arguments.
        mandelbrot.restype = None
        mandelbrot.argtypes = [ctypes.c_int, ctypes.c_int,
                               ndpointer(ctypes.c_int)]</pre></div></li><li class="listitem">To use this function, we first need to initialize an empty array and pass it as an argument to the <code class="literal">mandelbrot()</code> wrapper function:<div class="informalexample"><pre class="programlisting">In [7]: import numpy as np
        # We initialize an empty array.
        size = 200
        iterations = 100
        col = np.empty((size, size), dtype=np.int32)
        # We execute the C function.
        mandelbrot(size, iterations, col)
In [8]: %timeit mandelbrot(size, iterations, col)
100 loops, best of 3: 12.5 ms per loop</pre></div></li><li class="listitem">We free <a id="id691" class="indexterm"/>the library<a id="id692" class="indexterm"/> at the end of the script:<div class="informalexample"><pre class="programlisting">In [9]: from ctypes.wintypes import HMODULE
        ctypes.windll.kernel32.FreeLibrary.argtypes = [HMODULE]
        ctypes.windll.kernel32.FreeLibrary(lb._handle)</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec165"/>How it works…</h2></div></div></div><p>In the C code, the <code class="literal">__declspec(dllexport)</code> command declares the function visible in the DLL. The <code class="literal">__stdcall</code> keyword declares the standard calling convention on Windows.</p><p>As arguments, the <code class="literal">mandelbrot()</code> function accepts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="strong"><strong>size</strong></span><a id="id693" class="indexterm"/> of the <code class="literal">col</code> buffer (the <code class="literal">col</code> value is the last iteration where the corresponding point is within a disc around the origin)</li><li class="listitem" style="list-style-type: disc">The number of<a id="id694" class="indexterm"/> <span class="strong"><strong>iterations</strong></span></li><li class="listitem" style="list-style-type: disc">A <span class="strong"><strong>pointer</strong></span><a id="id695" class="indexterm"/> to the buffer of integers</li></ul></div><p>
<code class="literal">mandelbrot()</code> does not return any value; rather, it updates the buffer that was passed by reference to the function (it is a pointer).</p><p>To wrap this function in Python, we need to declare the types of the input arguments. The ctypes module defines constants for the different data types. In addition, the <code class="literal">numpy.ctypeslib.ndpointer()</code> function lets us use a NumPy array wherever a pointer is expected in the C function. The data type given as argument to <code class="literal">ndpointer()</code>needs to correspond to the NumPy data type of the array passed to the function.</p><p>Once the function has been correctly wrapped, it can be called as if it was a standard Python function. Here, the initially-empty NumPy array is filled<a id="id696" class="indexterm"/> with the Mandelbrot fractal after <a id="id697" class="indexterm"/>the call to <code class="literal">mandelbrot()</code>.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec166"/>There's more…</h2></div></div></div><p>SciPy contains a module called <span class="strong"><strong>weave</strong></span><a id="id698" class="indexterm"/> that provides similar functionality. We can write C code in a Python string and let weave compile and execute it at runtime using a C compiler. This module does not seem well-maintained and appears to be incompatible with Python 3. Cython or ctypes are probably better options.</p><p>A more recent alternative to ctypes is<a id="id699" class="indexterm"/> cffi (<a class="ulink" href="http://cffi.readthedocs.org">http://cffi.readthedocs.org</a>), which may be a bit faster and more convenient to use. You can also refer to <a class="ulink" href="http://eli.thegreenplace.net/2013/03/09/python-ffi-with-ctypes-and-cffi/">http://eli.thegreenplace.net/2013/03/09/python-ffi-with-ctypes-and-cffi/</a>.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Accelerating Python code with Cython"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec49"/>Accelerating Python code with Cython</h1></div></div></div><p>
<span class="strong"><strong>Cython</strong></span><a id="id700" class="indexterm"/> is both a language (a superset of Python) and a Python library. With Cython, we start from a regular Python program and we add annotations about the type of the variables. Then, Cython <a id="id701" class="indexterm"/>translates that code to C and compiles the<a id="id702" class="indexterm"/> result to a Python extension module. Finally, we can use this compiled module in any Python program.</p><p>While dynamic typing comes with a performance cost in Python, statically-typed variables in Cython generally lead to faster code execution.</p><p>Performance gains are most significant in CPU-bound programs, notably in tight Python loops. By contrast, I/O-bound programs are not expected to benefit much from a Cython implementation.</p><p>In this recipe, we will see how to accelerate the Mandelbrot code example with Cython.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec167"/>Getting ready</h2></div></div></div><p>A C compiler is required. You will find all compiler-related instructions in the introduction of this chapter.</p><p>You also need to install Cython<a id="id703" class="indexterm"/> from <a class="ulink" href="http://www.cython.org">http://www.cython.org</a>. With Anaconda, you can type <code class="literal">conda install cython</code> in a terminal.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec168"/>How to do it…</h2></div></div></div><p>We assume that the variables <code class="literal">size</code> and <code class="literal">iterations</code> have been defined as in the previous recipes.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">To use <a id="id704" class="indexterm"/>Cython in the IPython notebook, we first need to<a id="id705" class="indexterm"/> import the <code class="literal">cythonmagic</code> extension provided by IPython:<div class="informalexample"><pre class="programlisting">In [6]: %load_ext cythonmagic</pre></div></li><li class="listitem">As a first try, let's just add the <code class="literal">%%cython</code> magic before the definition of the <code class="literal">mandelbrot()</code> function. Internally, this cell magic compiles the cell into a standalone<a id="id706" class="indexterm"/> Cython module, hence the need for all required imports to occur within the same cell. This cell does not have access to any variable or function defined in the interactive namespace:<div class="informalexample"><pre class="programlisting">In [6]: %%cython
        import numpy as np
        def mandelbrot_cython(m, size, iterations):
            # The exact same content as in 
            # mandelbrot_python (first recipe of 
            # this chapter).</pre></div></li><li class="listitem">How fast is this version?<div class="informalexample"><pre class="programlisting">In [7]: %%timeit -n1 -r1 m = np.zeros((size, size),
                                      dtype=np.int32)
        mandelbrot_cython(m, size, iterations)
1 loops, best of 1: 5.7 s per loop</pre></div><p>We get virtually no speedup here. We need to specify the type of our Python variables.</p></li><li class="listitem">Let's add type information using typed memory views for NumPy arrays (we explain these in the <span class="emphasis"><em>How it works…</em></span> section). We also use a slightly different way to test whether particles have escaped from the domain (<code class="literal">if</code> test):<div class="informalexample"><pre class="programlisting">In [8]: %%cython
        import numpy as np
        def mandelbrot_cython(int[:,::1] m,
                              int size,
                              int iterations):
            cdef int i, j, n
            cdef complex z, c
            for i in range(size):
                for j in range(size):
                    c = -2 + 3./size*j + 1j*(1.5-3./size*i)
                    z = 0
                    for n in range(iterations):
                        if z.real**2 + z.imag**2 &lt;= 100:
                            z = z*z + c
                            m[i, j] = n
                        else:
                            break</pre></div></li><li class="listitem">How fast<a id="id707" class="indexterm"/> is this<a id="id708" class="indexterm"/> new version?<div class="informalexample"><pre class="programlisting">In [9]: %%timeit -n1 -r1 m = np.zeros((size, size),
                                      dtype=np.int32)
        mandelbrot_cython(m, size, iterations)
1 loops, best of 1: 230 ms per loop</pre></div><p>All we have done is specified the type of the local variables and function arguments and bypassed NumPy's <code class="literal">np.abs()</code> function when computing the absolute value of <code class="literal">z</code>. These changes have helped Cython to generate more optimized C code from Python code.</p></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec169"/>How it works…</h2></div></div></div><p>The <code class="literal">cdef</code> keyword<a id="id709" class="indexterm"/> declares a variable as a statically-typed C variable. C variables lead to faster code execution because the overhead from Python's dynamic typing is mitigated. Function arguments can also be declared as statically-typed C variables.</p><p>In general, variables used inside tight loops should be declared with <code class="literal">cdef</code>. To ensure that our code is well-optimized, we can use <a id="id710" class="indexterm"/><span class="strong"><strong>annotations</strong></span>. We just add the <code class="literal">-a</code> flag after the <code class="literal">%%cython</code> magic and the non-optimized lines will be shown in a gradient of yellow (white lines are faster, yellow lines are slower). This is shown in the following screenshot. The color depends on the relative number of Python API calls at each line.</p><div class="mediaobject"><img src="images/4818OS_05_02.jpg" alt="How it works…"/><div class="caption"><p>Annotations in Cython</p></div></div><p>There are two ways of declaring NumPy arrays as C variables with Cython: using <span class="strong"><strong>array buffers</strong></span><a id="id711" class="indexterm"/> or using <a id="id712" class="indexterm"/><span class="strong"><strong>typed memory views</strong></span>. In this recipe, we used typed memory views. We will cover array buffers in the next recipe.</p><p>Typed <a id="id713" class="indexterm"/>memory views allow efficient access to data buffers with a<a id="id714" class="indexterm"/> NumPy-like indexing syntax. For example, we can use <code class="literal">int[:,::1]</code> to declare a C-ordered 2D NumPy array with integer values, with <code class="literal">::1</code> meaning a contiguous layout in this dimension. Typed memory views can be indexed just like NumPy arrays.</p><p>However, memory views do not implement element-wise operations like NumPy. Thus, memory views act as convenient data containers within tight <code class="literal">for</code> loops. For element-wise NumPy-like operations, array buffers should be used instead.</p><p>We could achieve a significant performance speedup by replacing the call to <code class="literal">np.abs</code> with a faster expression. The reason is that <code class="literal">np.abs</code> is a NumPy function with a slight call overhead. It is designed to work with relatively large arrays, not scalar values. This overhead results in a significant performance hit in a tight loop such as here. This bottleneck can be spotted with Cython annotations.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec170"/>There's more…</h2></div></div></div><p>Using Cython from IPython is very convenient with<a id="id715" class="indexterm"/> the <code class="literal">%%cython</code> cell magic. However, it is sometimes necessary to create a reusable C extension module with Cython. This is actually what IPython's <code class="literal">%%cython</code> cell magic does under the hood.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first step is to <a id="id716" class="indexterm"/>write a standalone<a id="id717" class="indexterm"/> Cython script in a <code class="literal">.pyx</code> file. This should correspond exactly to the entire contents of a <code class="literal">%%cython</code> cell magic.</li><li class="listitem">The second step is to create a <code class="literal">setup.py</code> file that we will use to compile the Cython module. Here is a basic <code class="literal">setup.py</code> file, assuming a <code class="literal">mandelbrot.pyx</code> file:<div class="informalexample"><pre class="programlisting">from distutils.core import setup
from distutils.extension import Extension
from Cython.Distutils import build_ext

setup(
    cmdclass = {'build_ext': build_ext},
    ext_modules = [Extension("mandelbrot",
                             ["mandelbrot.pyx"])]
)</pre></div></li><li class="listitem">The third step is to execute this setup script with Python:<div class="informalexample"><pre class="programlisting">In [3]: !python setup.py build_ext --inplace
running build_ext
cythoning mandelbrot.pyx to mandelbrot.c
building 'mandelbrot' extension</pre></div></li><li class="listitem">Two files have been created during the build process: the C source file and a compiled Python extension. The file extension is <code class="literal">.pyd</code> on Windows (DLL files) and <code class="literal">.so</code> on UNIX:<div class="informalexample"><pre class="programlisting">In [4]: !dir mandelbrot.*
mandelbrot.c
mandelbrot.pyd
mandelbrot.pyx</pre></div></li><li class="listitem">Finally, we can load the compiled module as usual (using <code class="literal">from mandelbrot import mandelbrot</code>).</li></ol></div><p>With this technique, Cython code can also be integrated within a Python package. Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Distributing Cython modules<a id="id718" class="indexterm"/>, explained at <a class="ulink" href="http://docs.cython.org/src/userguide/source_files_and_compilation.html">http://docs.cython.org/src/userguide/source_files_and_compilation.html</a></li><li class="listitem" style="list-style-type: disc">Compilation with <a id="id719" class="indexterm"/>Cython, explained at <a class="ulink" href="http://docs.cython.org/src/reference/compilation.html">http://docs.cython.org/src/reference/compilation.html</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec171"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Optimizing Cython code by writing less Python and more C</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Releasing the GIL to take advantage of multicore processors with Cython and OpenMP</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Optimizing Cython code by writing less Python and more C"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec50"/>Optimizing Cython code by writing less Python and more C</h1></div></div></div><p>In this recipe, we will<a id="id720" class="indexterm"/> consider a more complicated Cython example. Starting from a slow implementation in pure Python, we will use different Cython features to speed it up progressively.</p><p>We will implement a very simple ray tracing engine. <span class="strong"><strong>Ray tracing</strong></span> consists of rendering a scene by simulating the physical properties of light propagation. This rendering method leads to photorealistic scenes, but it is computationally intensive.</p><p>Here, we will render a single sphere with diffuse and specular lighting. First we'll give the example's code in pure Python. Then, we will accelerate it incrementally with Cython.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note24"/>Note</h3><p>The code is long and contains many functions. We will first give the full code of the pure Python version. Then, we will just describe the changes required to accelerate the code with Cython. The entire scripts are available on the book's website.</p></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec172"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, let's implement the pure Python version:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import matplotlib.pyplot as plt
In [2]: %matplotlib inline
In [3]: w, h = 200, 200  # Size of the window in pixels.</pre></div><p>We create a normalization function for vectors:</p><div class="informalexample"><pre class="programlisting">def normalize(x):
    # This function normalizes a vector.
    x /= np.linalg.norm(x)
    return x</pre></div><p>Now, we create a function that computes the intersection of a ray with a sphere:</p><div class="informalexample"><pre class="programlisting">def intersect_sphere(O, D, S, R):
    # Return the distance from O to the intersection
    # of the ray (O, D) and the sphere (S, R), or
    # +inf if there is no intersection.
    # O and S are 3D points, D (direction) is a
    # normalized vector, R is a scalar.
    a = np.dot(D, D)
    OS = O - S
    b = 2 * np.dot(D, OS)
    c = np.dot(OS, OS) - R*R
    disc = b*b - 4*a*c
    if disc &gt; 0:
        distSqrt = np.sqrt(disc)
        q = (-b - distSqrt) / 2.0 if b &lt; 0 \
            else (-b + distSqrt) / 2.0
        t0 = q / a
        t1 = c / q
        t0, t1 = min(t0, t1), max(t0, t1)
        if t1 &gt;= 0:
            return t1 if t0 &lt; 0 else t0
    return np.inf</pre></div><p>The following <a id="id721" class="indexterm"/>function traces a ray:</p><div class="informalexample"><pre class="programlisting"> def trace_ray(rayO, rayD):
    # Find first point of intersection with the scene.
    t = intersect_sphere(rayO, rayD, position, radius)
    # No intersection?
    if t == np.inf:
        return
    # Find the point of intersection on the object.
    M = rayO + rayD * t
    N = normalize(M - position)
    toL = normalize(L - M)
    toO = normalize(O - M)
    # Ambient color.
    col = ambient
    # Diffuse color.
    col += diffuse * max(np.dot(N, toL), 0) * color
    # Specular color.
    col += specular_c * color_light * \
        max(np.dot(N, normalize(toL + toO)), 0) \
           ** specular_k
    return col</pre></div><p>Finally, the main loop is implemented in the following function:</p><div class="informalexample"><pre class="programlisting">def run():
    img = np.zeros((h, w, 3))
    # Loop through all pixels.
    for i, x in enumerate(np.linspace(-1.,1.,w)):
        for j, y in enumerate(np.linspace(-1.,1.,h)):
            # Position of the pixel.
            Q[0], Q[1] = x, y
            # Direction of the ray going through the
            # optical center.
            D = normalize(Q - O)
            depth = 0
            rayO, rayD = O, D
            # Launch the ray and get the
            # color of the pixel.
            col = trace_ray(rayO, rayD)
            if col is None:
                continue
            img[h - j - 1, i, :] = np.clip(col, 0, 1)
    return img</pre></div><p>Now, we initialize <a id="id722" class="indexterm"/>the scene and define a few parameters:</p><div class="informalexample"><pre class="programlisting">In [4]: # Sphere properties.
        position = np.array([0., 0., 1.])
        radius = 1.
        color = np.array([0., 0., 1.])
        diffuse = 1.
        specular_c = 1.
        specular_k = 50
        
        # Light position and color.
        L = np.array([5., 5., -10.])
        color_light = np.ones(3)
        ambient = .05
        
        # Camera.
        O = np.array([0., 0., -1.])  # Position.
        Q = np.array([0., 0., 0.])  # Pointing to.</pre></div><p>Let's render the scene:</p><div class="informalexample"><pre class="programlisting">In [5]: img = run()
In [6]: plt.imshow(img)
        plt.xticks([]); plt.yticks([])</pre></div><div class="mediaobject"><img src="images/4818OS_05_03.jpg" alt="How to do it…"/><div class="caption"><p>Ray tracing with Python and Cython. Left: the outcome of this recipe. Right: outcome of an extended version.</p></div></div></li><li class="listitem">How slow is this<a id="id723" class="indexterm"/> implementation?<div class="informalexample"><pre class="programlisting">In [7]: %timeit run()
1 loops, best of 1: 3.58 s per loop</pre></div></li><li class="listitem">If we just use the <code class="literal">%%cython</code> magic with the adequate <code class="literal">import numpy as np</code> and <code class="literal">cimport numpy as np</code> commands at the top of the cell, we only get a modest improvement, only a tenth of a second quicker.</li><li class="listitem">We could do better by giving information about the type of the variables. Since we use vectorized computations on NumPy arrays, we cannot easily use memory views. Rather, we will use array buffers. First, at the very beginning of the Cython module (or <code class="literal">%%cython</code> cell), we declare NumPy data types as follows:<div class="informalexample"><pre class="programlisting">import numpy as np
cimport numpy as np
DBL = np.double
ctypedef np.double_t DBL_C</pre></div><p>Then, we declare a NumPy array with <code class="literal">cdef np.ndarray[DBL_C, ndim=1]</code> (in this example, a 1D array of double precision floating point numbers). There is a difficulty here because NumPy arrays can only be declared inside functions, not at the top level. Thus, we need to slightly tweak the overall architecture of the code by passing some arrays as function arguments instead of using global variables. However, even by declaring the type of all variables, we gain virtually no speedup at all.</p></li><li class="listitem">In the current implementation, we incur a performance hit because of the large number of NumPy function calls on tiny arrays (three elements). NumPy is designed to deal with large arrays, and it does not make much sense to use it for arrays that small.<p>In this specific situation, we can try to bypass NumPy by rewriting some functions using the C standard library. We use the <code class="literal">cdef</code> keyword to declare a C-style function. These functions can yield significant performance speedups. Here, we get a 2-3x speedup by replacing the <code class="literal">normalize()</code> Python function with the following C function:</p><div class="informalexample"><pre class="programlisting">from libc.math cimport sqrt
cdef normalize(np.ndarray[DBL_C, ndim=1] x):
    cdef double n
    n = sqrt(x[0]*x[0] + x[1]*x[1] + x[2]*x[2])
    x[0] /= n
    x[1] /= n
    x[2] /= n
    return x</pre></div></li><li class="listitem">To get the most<a id="id724" class="indexterm"/> interesting speedups, we need to completely bypass NumPy. Where do we use NumPy precisely?<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Many variables are NumPy arrays (mostly one-dimensional vectors with three elements).</li><li class="listitem" style="list-style-type: disc">Element-wise operations yield implicit NumPy API calls.</li><li class="listitem" style="list-style-type: disc">We also use a few NumPy built-in functions such as <code class="literal">numpy.dot()</code>.</li></ul></div><p>In order to bypass NumPy in our example, we need to reimplement all these features for our specific needs. The first possibility is to use a native Python type for vectors (for example, tuples), and write C-style functions that implement operations on tuples (always assuming they have exactly three elements). For example, the addition between two tuples can be implemented as follows:</p><div class="informalexample"><pre class="programlisting">cdef tuple add(tuple x, tuple y):
    return (x[0]+y[0], x[1]+y[1], x[2]+y[2])</pre></div><p>We get an interesting speedup (30x compared to pure Python), but we can do even better by using a pure C data type.</p></li><li class="listitem">We are going to define a pure C structure instead of using a Python type for our vectors. In other words, we are not only bypassing NumPy, but we are also bypassing Python by moving to pure C code. To declare a C structure representing a 3D vector in Cython, we can use the following code:<div class="informalexample"><pre class="programlisting">cdef struct Vec3:
    double x, y, z</pre></div><p>To create a new <code class="literal">Vec3</code> variable, we can use the following function:</p><div class="informalexample"><pre class="programlisting">cdef Vec3 vec3(double x, double y, double z):
    cdef Vec3 v
    v.x = x
    v.y = y
    v.z = z
    return v</pre></div><p>As an example, here is the function used to add two <code class="literal">Vec3</code> objects:</p><div class="informalexample"><pre class="programlisting">cdef Vec3 add(Vec3 u, Vec3 v):
    return vec3(u.x + v.x, u.y + v.y, u.z + v.z)</pre></div><p>The code can be updated to make use of these fast C-style functions. Finally, the image can be declared as a 3D memory view. With all these changes, the Cython implementation runs in ~12 ms, 300 times faster than the pure Python version!</p></li></ol></div><p>In summary, we have<a id="id725" class="indexterm"/> achieved a very interesting speedup by basically rewriting the entire implementation in C with an enhanced Python syntax.</p></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec173"/>How it works…</h2></div></div></div><p>Let's explain briefly how the ray tracing code works. We model a three-dimensional scene with objects such as planes and spheres (here, there is only one sphere). There is also a camera and a plane representing the rendered image:</p><div class="mediaobject"><img src="images/4818OS_05_04.jpg" alt="How it works…"/><div class="caption"><p>Principles of ray tracing ("Ray trace diagram" by Henrik, Wikimedia Commons)</p></div></div><p>There is a main loop <a id="id726" class="indexterm"/>over all pixels of the image. For each pixel, we launch a ray from the camera center to the scene through the current pixel and compute the first intersection point between that ray and an object from the scene. Then, we compute the pixel's color as a function of the object material's color, the position of the lights, the normal of the object at the intersection point, and so on. There are several physics-inspired lighting equations that describe how the color depends on these parameters. Here, we use the <span class="strong"><strong>Blinn-Phong shading model</strong></span><a id="id727" class="indexterm"/> with ambient, diffuse, and specular lighting components:</p><div class="mediaobject"><img src="images/4818OS_05_05.jpg" alt="How it works…"/><div class="caption"><p>Blinn-Phong shading model ("Phong components", Wikimedia Commons)</p></div></div><p>Of course, a full ray tracing engine is far more complex than what we have implemented in this example. We can model other optic and lighting characteristics such as reflections, refractions, shadows, depth of field, and others. It is also possible to implement ray tracing algorithms on the graphics card for real-time photorealistic rendering. Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Blinn-Phong shading model on Wikipedia,<a id="id728" class="indexterm"/> available at <a class="ulink" href="http://en.wikipedia.org/wiki/Blinn-Phong_shading_model">http://en.wikipedia.org/wiki/Blinn-Phong_shading_model</a></li><li class="listitem" style="list-style-type: disc">Ray tracing on Wikipedia, <a id="id729" class="indexterm"/>available at <a class="ulink" href="http://en.wikipedia.org/wiki/Ray_tracing_(graphics)">http://en.wikipedia.org/wiki/Ray_tracing_(graphics)</a></li></ul></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec174"/>There's more…</h2></div></div></div><p>Although powerful, Cython requires a good understanding of Python, NumPy, and C. The most interesting performance speedups are achieved when dynamically-typed Python variables are converted to statically-typed C variables, notably within tight loops.</p><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Cython extension types<a id="id730" class="indexterm"/> available at <a class="ulink" href="http://docs.cython.org/src/userguide/extension_types.html">http://docs.cython.org/src/userguide/extension_types.html</a></li><li class="listitem" style="list-style-type: disc">Extended version of our ray tracing engine<a id="id731" class="indexterm"/> available at <a class="ulink" href="http://gist.github.com/rossant/6046463">http://gist.github.com/rossant/6046463</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec175"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Accelerating Python code with Cython</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Releasing the GIL to take advantage of multicore processors with Cython and OpenMP</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Releasing the GIL to take advantage of multicore processors with Cython and OpenMP"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec51"/>Releasing the GIL to take advantage of multicore processors with Cython and OpenMP</h1></div></div></div><p>As we have seen in this chapter's introduction, CPython's GIL prevents pure Python code from taking advantage of multi-core processors. With Cython, we have a way to release the GIL temporarily in a portion of the code in order to enable multi-core computing. This is done with<a id="id732" class="indexterm"/> <span class="strong"><strong>OpenMP</strong></span>, a multiprocessing API that is supported by most C compilers.</p><p>In this recipe, we will see how to <a id="id733" class="indexterm"/>parallelize the previous recipe's code on multiple cores.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec176"/>Getting ready</h2></div></div></div><p>To enable OpenMP in Cython, you just need to specify some options to the compiler. There is nothing special to install on your computer besides a good C compiler. See the instructions in this chapter's introduction for more details.</p><p>In this recipe, we use Microsoft's Visual C++ compiler on Windows, but the code can be easily adapted to other systems.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec177"/>How to do it…</h2></div></div></div><p>Our simple ray tracing engine implementation is <span class="strong"><strong>embarrassingly parallel</strong></span>; there is a main loop over all pixels, within which the exact same function is called repetitively. There is no crosstalk between loop iterations. Therefore, it would be theoretically possible to execute all iterations in parallel.</p><p>Here, we will execute one loop (over all columns in the image) in parallel with OpenMP.</p><p>You will find the entire code on the book's website. We will only show the most important steps here:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We add the <a id="id734" class="indexterm"/>following options to the <code class="literal">%%cython</code> magic command: <code class="literal">--compile-args=/openmp --link-args=/openmp</code>. The exact syntax may depend on your compiler and/or your system. For example, <code class="literal">/openmp</code> should be replaced by <code class="literal">-fopenmp</code> with GCC.</li><li class="listitem">We import the <code class="literal">prange()</code> function:<div class="informalexample"><pre class="programlisting">from cython.parallel import prange</pre></div></li><li class="listitem">We add <code class="literal">nogil</code> after each function definition in order to remove the GIL. We cannot use any Python variable or function inside a function annotated with <code class="literal">nogil</code>. For example:<div class="informalexample"><pre class="programlisting">cdef Vec3 add(Vec3 x, Vec3 y) nogil:
    return vec3(x.x + y.x, x.y + y.y, x.z + y.z)</pre></div></li><li class="listitem">To run a loop in parallel over the cores with OpenMP, we use <code class="literal">prange()</code>:<div class="informalexample"><pre class="programlisting">with nogil:
    for i in prange(w):
        # ...</pre></div><p>The GIL needs to be released before using any parallel computing feature such as  <code class="literal">prange()</code>.</p></li><li class="listitem">With these changes, we reach a 4x speedup on a quad-core processor.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec178"/>How it works…</h2></div></div></div><p>The GIL has been described in the introduction of this chapter. The <code class="literal">nogil</code> keyword<a id="id735" class="indexterm"/> tells Cython that a particular function or code section should be executed without the GIL. When the GIL is released, it is not possible to make any Python API calls, meaning that only C variables and C functions (declared with <code class="literal">cdef</code>) can be used.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec179"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Accelerating Python code with Cython</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Optimizing Cython code by writing less Python and more C</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Distributing Python code across multiple cores with IPython</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Writing massively parallel code for NVIDIA graphics cards (GPUs) with CUDA"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec52"/>Writing massively parallel code for NVIDIA graphics cards (GPUs) with CUDA</h1></div></div></div><p>
<span class="strong"><strong>Graphics Processing Units</strong></span> (<span class="strong"><strong>GPUs</strong></span>)<a id="id736" class="indexterm"/> are powerful processors specialized in real-time rendering. We find GPUs in virtually any computer, laptop, video game console, tablet, or smartphone. Their massively parallel architecture comprises tens to thousands of cores. The video game industry has been fostering the development of increasingly powerful GPUs over the last two decades.</p><p>GPUs are routinely <a id="id737" class="indexterm"/>used in <a id="id738" class="indexterm"/>modern supercomputers (for example in Cray's Titan at Oak Ridge National Laboratory, ~20 petaFLOPS, ~20,000 CPUs, and as many NVIDIA GPUs). A high-end $1000 GPU today is roughly as powerful as a $100 million supercomputer from 2000 (several teraFLOPS).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note25"/>Note</h3><p>FLOPS<a id="id739" class="indexterm"/> means FLoating-point Operations Per Second. A 1 teraFLOPS GPU can perform up to one trillion floating-point operations per second.</p></div></div><p>Since the mid-2000s, GPUs are no longer limited to graphics processing. We can now implement scientific algorithms on a GPU. The only condition is that the algorithm follows the <a id="id740" class="indexterm"/><span class="strong"><strong>SIMD (Single Instruction, Multiple Data) paradigm</strong></span>, where a sequence of instructions is executed in parallel with multiple data. This is called <span class="strong"><strong>General Purpose Programming on Graphics Processing Units</strong></span> (<span class="strong"><strong>GPGPU</strong></span>). GPGPU<a id="id741" class="indexterm"/> is used in many areas: meteorology, data mining, computer vision, image processing, finance, physics, bioinformatics, and many more. Writing code for GPUs can be challenging as it requires understanding the internal architecture of the hardware.</p><p>
<span class="strong"><strong>CUDA</strong></span><a id="id742" class="indexterm"/> is a proprietary GPGPU framework created in 2007 by NVIDIA Corporation, one of the main GPU manufacturers. Programs written in CUDA only work on NVIDIA graphics cards. There is another competing GPGPU framework called<a id="id743" class="indexterm"/> <span class="strong"><strong>OpenCL</strong></span>, an open standard supported by other major companies. OpenCL programs can work on GPUs and CPUs from most manufacturers (notably NVIDIA, AMD, and Intel).</p><p>In this recipe, we will show a very basic example of GPGPU. We'll implement the embarrassingly parallel computation of the Mandelbrot fractal in CUDA. In the next recipe, we will implement the exact same example in OpenCL.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip21"/>Tip</h3><p>Should you choose OpenCL or CUDA for a new project? The answer depends most notably on the hardware of your user base. If you need the highest performance possible for a specific project in your lab where all computers have an NVIDIA card, and if releasing your program to the world is not a high priority, you could choose CUDA. If you envision distributing your program to many people running different platforms, you should probably choose OpenCL. Featurewise, these two platforms are very roughly equivalent.</p></div></div><p>We can use CUDA in Python thanks to PyCUDA<a id="id744" class="indexterm"/>, a Python package written by Andreas Klöckner (<a class="ulink" href="http://documen.tician.de/pycuda/">http://documen.tician.de/pycuda/</a>).</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec180"/>Getting ready</h2></div></div></div><p>Installing and <a id="id745" class="indexterm"/>configuring<a id="id746" class="indexterm"/> PyCUDA is not straightforward in general. First, you need an NVIDIA GPU. Then, you need to install the CUDA SDK. Finally, you have to install and configure PyCUDA. Note that PyCUDA depends on a few external packages, notably pytools.</p><p>On Windows, you should use Chris Gohlke's package. Make sure your version of CUDA matches the version used in the PyCUDA package. If you have DLL-related problems, use Dependency Walker on the <code class="literal">*.pyd</code> files in PyCUDA's installation folder (with Anaconda, it should look like <code class="literal">C:\anaconda\lib\site-packages\pycuda</code>). If you use Windows 64-bit, make sure that <code class="literal">C:\Windows\SysWOW64</code> is in your system PATH. Finally, make sure you have the version of Visual Studio that corresponds to your version of Python (see the instructions related to C compilers at the beginning of this chapter).</p><p>You will find more information at the following links:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">CUDA SDK<a id="id747" class="indexterm"/> available at <a class="ulink" href="http://developer.nvidia.com/cuda-downloads">http://developer.nvidia.com/cuda-downloads</a></li><li class="listitem" style="list-style-type: disc">PyCUDA wiki<a id="id748" class="indexterm"/> available at <a class="ulink" href="http://wiki.tiker.net">http://wiki.tiker.net</a></li></ul></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec181"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import PyCUDA:<div class="informalexample"><pre class="programlisting">In [1]: import pycuda.driver as cuda
        import pycuda.autoinit
        from pycuda.compiler import SourceModule
        import numpy as np</pre></div></li><li class="listitem">We initialize the NumPy array that will contain the fractal:<div class="informalexample"><pre class="programlisting">In [2]: size = 200
        iterations = 100
        col = np.empty((size, size), dtype=np.int32)</pre></div></li><li class="listitem">We allocate GPU memory for this array:<div class="informalexample"><pre class="programlisting">In [3]: col_gpu = cuda.mem_alloc(col.nbytes)</pre></div></li><li class="listitem">We write the CUDA kernel in a string. Arguments to the <code class="literal">mandelbrot()</code> function are:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The<a id="id749" class="indexterm"/> figure <span class="strong"><strong>size</strong></span></li><li class="listitem" style="list-style-type: disc">The <a id="id750" class="indexterm"/>number of <span class="strong"><strong>iterations</strong></span></li><li class="listitem" style="list-style-type: disc">The <a id="id751" class="indexterm"/><span class="strong"><strong>pointer</strong></span> to the memory buffer</li></ul></div><p>This <a id="id752" class="indexterm"/>function executes<a id="id753" class="indexterm"/> on every single pixel. It updates the <code class="literal">col</code> buffer with the pixel's color:</p><div class="informalexample"><pre class="programlisting">In [4]: code = """
        __global__ void mandelbrot(int size,
                                   int iterations,
                                   int *col) {
            // Get the row and column of the thread.
            int i = blockIdx.y * blockDim.y + threadIdx.y;
            int j = blockIdx.x * blockDim.x + threadIdx.x;
            int index = i * size + j;
            
            // Declare and initialize the variables.
            double cx, cy;
            double z0, z1, z0_tmp, z0_2, z1_2;
            cx = -2.0 + (double)j / size * 3;
            cy = -1.5 + (double)i / size * 3;
        
            // Main loop.
            z0 = z1 = 0.0;
            for (int n = 0; n &lt; iterations; n++)
            {
                z0_2 = z0 * z0;
                z1_2 = z1 * z1;
                if (z0_2 + z1_2 &lt;= 100)
                {
                    // Need to update both z0 and z1,
                    // hence the need for z0_tmp.
                    z0_tmp = z0_2 - z1_2 + cx;
                    z1 = 2 * z0 * z1 + cy;
                    z0 = z0_tmp;
                    col[index] = n;
                }
                else break;
            }
        }
        """</pre></div></li><li class="listitem">Now, we compile the CUDA program:<div class="informalexample"><pre class="programlisting">In [5]: prg = SourceModule(code)
        mandelbrot = prg.get_function("mandelbrot")</pre></div></li><li class="listitem">We define the block size and the grid size, specifying how the threads will be parallelized with respect to the data:<div class="informalexample"><pre class="programlisting">In [6]: block_size = 10
        block = (block_size, block_size, 1)
        grid = (size // block_size, size // block_size, 1)</pre></div></li><li class="listitem">We call the compiled function:<div class="informalexample"><pre class="programlisting">In [7]: mandelbrot(np.int32(size),
                   np.int32(iterations),
                   col_gpu,
                   block=block, grid=grid)</pre></div></li><li class="listitem">Once the function has completed, we copy the contents of the CUDA buffer back to the NumPy array <code class="literal">col</code>:<div class="informalexample"><pre class="programlisting">In [8]: cuda.memcpy_dtoh(col, col_gpu)</pre></div></li><li class="listitem">The <code class="literal">col</code> array now contains the Mandelbrot fractal. We find that this CUDA program is executed in 0.7 ms on a mobile GeForce GPU.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec182"/>How it works…</h2></div></div></div><p>GPU <a id="id754" class="indexterm"/>programming is a<a id="id755" class="indexterm"/> rich and highly technical topic, encompassing low-level architectural details of GPUs. Of course, we only scratched the surface here with the simplest paradigm possible (the "embarrassingly parallel" problem). We give further references in a later section.</p><p>A CUDA GPU has a number of <a id="id756" class="indexterm"/><span class="strong"><strong>multiprocessors</strong></span>, and each multiprocessor has multiple<a id="id757" class="indexterm"/> <span class="strong"><strong>stream processors</strong></span> (also called <a id="id758" class="indexterm"/><span class="strong"><strong>CUDA cores</strong></span>). Each multiprocessor executes in parallel with the others. Within a multiprocessor, the stream processors execute the same instruction at the same time, but on multiple data bits (SIMD paradigm).</p><p>Central notions to the CUDA programming model are those of kernels, threads, blocks, and grids:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A <span class="strong"><strong>kernel</strong></span><a id="id759" class="indexterm"/> is a program <a id="id760" class="indexterm"/>written in a C-like language that runs on the GPU.</li><li class="listitem" style="list-style-type: disc">A <span class="strong"><strong>thread</strong></span><a id="id761" class="indexterm"/> represents one <a id="id762" class="indexterm"/>execution of a kernel on one <span class="emphasis"><em>stream processor</em></span>.</li><li class="listitem" style="list-style-type: disc">A <span class="strong"><strong>block</strong></span><a id="id763" class="indexterm"/> contains multiple <a id="id764" class="indexterm"/>threads executing on one <span class="emphasis"><em>multiprocessor</em></span>.</li><li class="listitem" style="list-style-type: disc">A <span class="strong"><strong>grid</strong></span><a id="id765" class="indexterm"/> contains a<a id="id766" class="indexterm"/> number of blocks.</li></ul></div><p>The <a id="id767" class="indexterm"/>number of threads per <a id="id768" class="indexterm"/>block is limited by the size of the multiprocessors and depends on the graphics card model (1024 in recent models). However, a grid can contain an arbitrary number of blocks.</p><p>Within a block, threads are executed within <span class="strong"><strong>warps</strong></span><a id="id769" class="indexterm"/> of typically 32 threads. Better performance is achieved when conditional branching in a kernel is organized into groups of 32 threads.</p><p>Threads within a block can synchronize at synchronization barriers using the <code class="literal">__syncthreads()</code> function. This feature enables inter-thread communication within one block. However, blocks execute independently so that two threads from different blocks cannot synchronize.</p><p>Within a block, threads are organized into a 1D, 2D, or 3D structure, and similarly for blocks within a grid, as shown in the following figure. This structure is convenient as it matches most common multidimensional datasets encountered in real-world problems.</p><div class="mediaobject"><img src="images/4818OS_05_06.jpg" alt="How it works…"/><div class="caption"><p>The CUDA programming model (showing threads, blocks, and grids — image by NVIDIA Corporation)</p></div></div><p>The kernel can retrieve the thread index within the block (<code class="literal">threadIdx</code>), as well as the block index within the grid (<code class="literal">blockIdx</code>) to determine which bit of data it should work on. In this recipe, the 2D image of the fractal is partitioned into 10 x 10 blocks, each block containing 100 pixels, with one thread per pixel. The kernel <code class="literal">mandelbrot</code> computes the color of a single pixel.</p><p>There <a id="id770" class="indexterm"/>are several levels of<a id="id771" class="indexterm"/> memory on the GPU, ranging from small, fast, and local memory shared by a few threads within a block; to large, slow, and global memory shared by all blocks. We need to tweak the memory access patterns in the code to match the hardware constraints and achieve higher performance. In particular, data access is more efficient when the threads within a warp access <span class="emphasis"><em>consecutive</em></span> addresses in the global memory; the hardware <span class="strong"><strong>coalesces</strong></span><a id="id772" class="indexterm"/> all memory accesses into a single access to consecutive <span class="strong"><strong>DRAM</strong></span> (<span class="strong"><strong>Dynamic Random Access Memory</strong></span>)<a id="id773" class="indexterm"/> locations.</p><p>PyCUDA lets us upload/download data from NumPy arrays to buffers residing on the GPU. This operation is generally costly. Complex real-world problems frequently involve iterative steps happening on both the CPU and on the GPU, such that communication between the two is a common performance bottleneck. Higher performance is achieved when there are few of these exchanges.</p><p>There is some boilerplate code in (Py)CUDA on the C/Python side that consists of initializing the GPU, allocating data, uploading/downloading data to/from the GPU, compiling the kernel, executing the kernel, and so on. You can find all the details in the CUDA/PyCUDA documentation, but as a first approach, you can also just copy and paste code from this recipe or any tutorial.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec183"/>There's more…</h2></div></div></div><p>Here are a few<a id="id774" class="indexterm"/> references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Official CUDA portal at <a class="ulink" href="http://developer.nvidia.com/category/zone/cuda-zone">http://developer.nvidia.com/category/zone/cuda-zone</a></li><li class="listitem" style="list-style-type: disc">Education and training for CUDA, at <a class="ulink" href="http://developer.nvidia.com/cuda-education-training">http://developer.nvidia.com/cuda-education-training</a></li><li class="listitem" style="list-style-type: disc">Suggested books about CUDA, at <a class="ulink" href="http://developer.nvidia.com/suggested-reading">http://developer.nvidia.com/suggested-reading</a></li><li class="listitem" style="list-style-type: disc">Choosing between CUDA or OpenCL, at <a class="ulink" href="http://wiki.tiker.net/CudaVsOpenCL">http://wiki.tiker.net/CudaVsOpenCL</a></li><li class="listitem" style="list-style-type: disc">A blog post on CUDA and OpenCL<a id="id775" class="indexterm"/> available at <a class="ulink" href="http://streamcomputing.eu/blog/2011-06-22/opencl-vs-cuda-misconceptions/">http://streamcomputing.eu/blog/2011-06-22/opencl-vs-cuda-misconceptions/</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec184"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Writing massively parallel code for heterogeneous platforms with OpenCL</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Writing massively parallel code for heterogeneous platforms with OpenCL"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec53"/>Writing massively parallel code for heterogeneous platforms with OpenCL</h1></div></div></div><p>In the previous recipe, we <a id="id776" class="indexterm"/>introduced <a id="id777" class="indexterm"/>CUDA, a <span class="emphasis"><em>proprietary</em></span> GPGPU framework created by NVIDIA Corporation. In this recipe, we present OpenCL, an alternative <span class="emphasis"><em>open</em></span> framework initiated by Apple in 2008. It is now adopted by mainstream companies including Intel, NVIDIA, AMD, Qualcomm, ARM, and others. These companies are regrouped within the non-profit technology consortium <span class="strong"><strong>Khronos Group</strong></span><a id="id778" class="indexterm"/> (which also maintains the OpenGL real-time rendering specification). Programs written in OpenCL can run on GPUs and <a id="id779" class="indexterm"/>CPUs (<span class="strong"><strong>heterogeneous computing</strong></span>).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip22"/>Tip</h3><p>CUDA and OpenCL are relatively similar in terms of concepts, syntax, and features. CUDA sometimes leads to slightly higher performance, since its API matches the hardware more closely than OpenCL's generic API.</p></div></div><p>We can use OpenCL in Python thanks to <a id="id780" class="indexterm"/><span class="strong"><strong>PyOpenCL</strong></span>, a Python package written by <a id="id781" class="indexterm"/>Andreas Klöckner (<a class="ulink" href="http://documen.tician.de/pyopencl/">http://documen.tician.de/pyopencl/</a>).</p><p>In this recipe, we will implement the Mandelbrot fractal in OpenCL. The OpenCL kernel is very similar to the CUDA kernel from the previous recipe. The Python API used to access OpenCL is somewhat different from PyCUDA, but the concepts are equivalent.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec185"/>Getting ready</h2></div></div></div><p>Installing PyOpenCL is<a id="id782" class="indexterm"/> generally<a id="id783" class="indexterm"/> not straightforward. The first step is to install the OpenCL SDK for your hardware (CPU and/or GPU). Then, you have to install and configure PyOpenCL. On Windows, you should use Chris Gohlke's package. Some installation instructions in the previous recipe apply here as well.</p><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The PyOpenCL Wiki <a id="id784" class="indexterm"/>available at <a class="ulink" href="http://wiki.tiker.net">http://wiki.tiker.net</a></li><li class="listitem" style="list-style-type: disc">The documentation of PyOpenCL available at <a class="ulink" href="http://documen.tician.de/pyopencl/">http://documen.tician.de/pyopencl/</a></li></ul></div><p>Here are the links to the various <a id="id785" class="indexterm"/>OpenCL SDKs:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Intel's SDK is available at <a class="ulink" href="http://software.intel.com/en-us/vcsource/tools/opencl-sdk">http://software.intel.com/en-us/vcsource/tools/opencl-sdk</a></li><li class="listitem" style="list-style-type: disc">AMD's SDK is available at <a class="ulink" href="http://developer.amd.com/tools-and-sdks/heterogeneous-computing/">http://developer.amd.com/tools-and-sdks/heterogeneous-computing/</a></li><li class="listitem" style="list-style-type: disc">NVIDIA's SDK is available at <a class="ulink" href="http://developer.nvidia.com/opencl">http://developer.nvidia.com/opencl</a></li></ul></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec186"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import PyOpenCL:<div class="informalexample"><pre class="programlisting">In [1]: import pyopencl as cl
        import numpy as np</pre></div></li><li class="listitem">The following object defines some flags related to memory management on the device:<div class="informalexample"><pre class="programlisting">In [2]: mf = cl.mem_flags</pre></div></li><li class="listitem">We create an OpenCL context and a command queue:<div class="informalexample"><pre class="programlisting">In [3]: ctx = cl.create_some_context()
        queue = cl.CommandQueue(ctx)</pre></div></li><li class="listitem">Now, we initialize the NumPy array that will contain the fractal:<div class="informalexample"><pre class="programlisting">In [4]: size = 200
        iterations = 100
        col = np.empty((size, size), dtype=np.int32)</pre></div></li><li class="listitem">We allocate GPU memory for this array:<div class="informalexample"><pre class="programlisting">In [5]: col_buf = cl.Buffer(ctx,
                            mf.WRITE_ONLY,
                            col.nbytes)</pre></div></li><li class="listitem">We write the <a id="id786" class="indexterm"/>OpenCL kernel<a id="id787" class="indexterm"/> in a string:<div class="informalexample"><pre class="programlisting">In [6]: code = """
        __kernel void mandelbrot(int size,
                                 int iterations,
                                 global int *col)
        {
            // Get the row and column index of the thread.
            int i = get_global_id(1);
            int j = get_global_id(0);
            int index = i * size + j;
            
            // Declare and initialize the variables.
            double cx, cy;
            double z0, z1, z0_tmp, z0_2, z1_2;
            cx = -2.0 + (double)j / size * 3;
            cy = -1.5 + (double)i / size * 3;
        
            // Main loop.
            z0 = z1 = 0.0;
            for (int n = 0; n &lt; iterations; n++)
            {
                z0_2 = z0 * z0;
                z1_2 = z1 * z1;
                if (z0_2 + z1_2 &lt;= 100)
                {
                    // Need to update both z0 and z1.
                    z0_tmp = z0_2 - z1_2 + cx;
                    z1 = 2 * z0 * z1 + cy;
                    z0 = z0_tmp;
                    col[index] = n;
                }
                else break;
            }
        }
        """</pre></div></li><li class="listitem">Now, we compile the OpenCL program:<div class="informalexample"><pre class="programlisting">In [7]: prg = cl.Program(ctx, code).build()
Build on &lt;pyopencl.Device 'Intel(R) Core(TM) i3-2365M CPU @ 1.40GHz' on 'Intel(R) OpenCL' at 0x765b188&gt; succeeded.</pre></div></li><li class="listitem">We call the compiled function, passing the command queue, the grid size, and the buffers as arguments:<div class="informalexample"><pre class="programlisting">In [8]: prg.mandelbrot(queue, col.shape, None, np.int32(size), np.int32(iterations), col_buf).wait()</pre></div></li><li class="listitem">Once the function has completed, we copy the contents of the OpenCL buffer back into the NumPy array <code class="literal">col</code>:<div class="informalexample"><pre class="programlisting">In [9]: cl.enqueue_copy(queue, col, col_buf)</pre></div></li><li class="listitem">Finally, we can check that the function was successful by <code class="literal">imshow()</code>-ing the NumPy array <code class="literal">col</code>. We can also do a quick benchmark with <code class="literal">%timeit</code>, and we find that this function takes ~3.7 ms to complete on an Intel i3 dual-core CPU.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec187"/>How it works…</h2></div></div></div><p>The principles detailed<a id="id788" class="indexterm"/> in the <a id="id789" class="indexterm"/>previous recipe apply here as well. There is a change of terminology between CUDA and OpenCL:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">CUDA threads are equivalent to <a id="id790" class="indexterm"/>OpenCL <span class="strong"><strong>work items</strong></span>.</li><li class="listitem" style="list-style-type: disc">CUDA blocks are equivalent to <a id="id791" class="indexterm"/>OpenCL <span class="strong"><strong>work groups</strong></span>.</li><li class="listitem" style="list-style-type: disc">A CUDA grid is equivalent to an <a id="id792" class="indexterm"/>OpenCL <span class="strong"><strong>NDRange</strong></span>.</li><li class="listitem" style="list-style-type: disc">A CUDA streaming processor is equivalent to an <a id="id793" class="indexterm"/>OpenCL <span class="strong"><strong>compute unit</strong></span>.</li></ul></div><p>In the kernel, we can get a work item's index with <code class="literal">get_local_id(dim)</code>, <code class="literal">get_group_id(dim)</code>, and <code class="literal">get_global_id(dim)</code>. The <code class="literal">global</code> qualifier in the function's arguments specifies that a variable corresponds to an object in global memory.</p><p>An OpenCL context is the environment within which work items execute. It includes devices with their memories and command queues. The command queue is a queue used by the host application to submit work to a device.</p><p>This program works the same on a CPU or a GPU, depending on the installed OpenCL SDK and on the available OpenCL context. If multiple contexts exist, PyOpenGL may ask the user to choose the device. The context may also<a id="id794" class="indexterm"/> be specified programmatically (see <a class="ulink" href="http://documen.tician.de/pyopencl/runtime.html#pyopencl.Context">http://documen.tician.de/pyopencl/runtime.html#pyopencl.Context</a>). On a CPU, the code is parallelized and vectorized over multiple cores and with vector instructions such as SSE <a id="id795" class="indexterm"/>or <a id="id796" class="indexterm"/>AVX.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec188"/>There's more…</h2></div></div></div><p>OpenCL is a relatively young standard but we should expect it to have more and more importance in the future. It is supported by the biggest companies in the GPU industry. It supports interoperability with OpenGL, the industry standard for real-time, hardware-accelerated computer graphics (maintained by the very same Khronos Group). It is on its way to being supported on mobile platforms (smartphones and tablets), and in the browser as well with <span class="strong"><strong>WebCL</strong></span><a id="id797" class="indexterm"/> (which is still a draft at the time of writing).</p><p>Here are a few OpenCL resources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">OpenCL tutorial <a id="id798" class="indexterm"/>available at <a class="ulink" href="http://opencl.codeplex.com">http://opencl.codeplex.com</a></li><li class="listitem" style="list-style-type: disc">Courses available at <a class="ulink" href="http://developer.amd.com/partners/university-programs/opencl-university-course-listings/">http://developer.amd.com/partners/university-programs/opencl-university-course-listings/</a></li><li class="listitem" style="list-style-type: disc">Books on OpenCL, at <a class="ulink" href="http://streamcomputing.eu/knowledge/for-developers/books/">http://streamcomputing.eu/knowledge/for-developers/books/</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec189"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Writing massively parallel code for NVIDIA graphics cards (GPUs) with CUDA</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Distributing Python code across multiple cores with IPython"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec54"/>Distributing Python code across multiple cores with IPython</h1></div></div></div><p>Despite CPython's GIL, it is possible to execute several tasks in parallel on multi-core computers using multiple processes instead of multiple threads. Python offers a native <a id="id799" class="indexterm"/><span class="strong"><strong>multiprocessing</strong></span> module. IPython offers an even simpler interface that brings powerful parallel computing features in an interactive environment. We will describe this tool here.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec190"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we launch<a id="id800" class="indexterm"/> four IPython engines <a id="id801" class="indexterm"/>in separate processes. We have basically two options to do this:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Executing <code class="literal">ipcluster start -n 4</code> in a system shell</li><li class="listitem" style="list-style-type: disc">Using the web interface provided in the IPython notebook's main page by clicking on the <span class="strong"><strong>Clusters</strong></span> tab and launching four engines</li></ul></div></li><li class="listitem">Then, we create a client that will act as a proxy to the IPython engines. The client automatically detects the running engines:<div class="informalexample"><pre class="programlisting">In [2]: from IPython.parallel import Client
        rc = Client()</pre></div></li><li class="listitem">Let's check the number of running engines:<div class="informalexample"><pre class="programlisting">In [3]: rc.ids
Out[3]: [0, 1, 2, 3]</pre></div></li><li class="listitem">To run commands in parallel over the engines, we can use the <code class="literal">%px</code> line magic or the <code class="literal">%%px</code> cell magic:<div class="informalexample"><pre class="programlisting">In [4]: %%px
        import os
        print("Process {0:d}.".format(os.getpid()))
[stdout:0] Process 2988.
[stdout:1] Process 5192.
[stdout:2] Process 4484.
[stdout:3] Process 1360.</pre></div></li><li class="listitem">We can specify which engines to run the commands on using the <code class="literal">--targets</code> or <code class="literal">-t</code> option:<div class="informalexample"><pre class="programlisting">In [5]: %%px -t 1,2
        # The os module has already been imported in
        # the previous cell.
        print("Process {0:d}.".format(os.getpid()))
[stdout:1] Process 5192.
[stdout:2] Process 4484.</pre></div></li><li class="listitem">By default, the <code class="literal">%px</code> magic executes commands in <a id="id802" class="indexterm"/><span class="strong"><strong>blocking mode</strong></span>; the cell only returns when the commands have completed on all engines. It is possible to run non-blocking commands with the <code class="literal">--noblock</code> or <code class="literal">-a</code> option. In this case, the cell returns immediately, and the task's status <a id="id803" class="indexterm"/>and results <a id="id804" class="indexterm"/>can be polled asynchronously from IPython's interactive session:<div class="informalexample"><pre class="programlisting">In [6]: %%px -a
        import time
        time.sleep(5)
Out[6]: &lt;AsyncResult: execute&gt;</pre></div></li><li class="listitem">The previous command returned an <code class="literal">ASyncResult</code> instance that we can use to poll the task's status:<div class="informalexample"><pre class="programlisting">In [7]: print(_.elapsed, _.ready())
(0.061, False)</pre></div></li><li class="listitem">The <code class="literal">%pxresult</code> blocks until the task finishes:<div class="informalexample"><pre class="programlisting">In [8]: %pxresult
In [9]: print(_.elapsed, _.ready())
(5.019, True)</pre></div></li><li class="listitem">IPython provides convenient functions for common use cases, such as a parallel <code class="literal">map</code> function:<div class="informalexample"><pre class="programlisting">In [10]: v = rc[:]
         res = v.map(lambda x: x*x, range(10))
In [11]: print(res.get())
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec191"/>How it works…</h2></div></div></div><p>There are several steps to distribute code across multiple cores:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Launching several IPython <span class="strong"><strong>engines</strong></span><a id="id805" class="indexterm"/> (there is typically one process per core).</li><li class="listitem">Creating a <code class="literal">Client</code> that acts as a proxy to these engines.</li><li class="listitem">Using the client to launch tasks on the engines and retrieve the results.</li></ol></div><p>Engines are Python processes that execute code on different computing units. They are very similar to IPython kernels.</p><p>There are two main interfaces for accessing the engines:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">With the <a id="id806" class="indexterm"/><span class="strong"><strong>direct interface</strong></span>, we access engines directly and explicitly with their identifiers.</li><li class="listitem" style="list-style-type: disc">With the<a id="id807" class="indexterm"/> <span class="strong"><strong>load-balanced interface</strong></span>, we access engines through an interface that automatically and dynamically assigns work to appropriate engines.</li></ul></div><p>We can also create custom interfaces for alternative styles of parallelism.</p><p>In this recipe, we used the direct interface; we addressed individual engines explicitly by specifying their identifiers in the <code class="literal">%px</code> magics.</p><p>As we have seen in this <a id="id808" class="indexterm"/>recipe, tasks can be<a id="id809" class="indexterm"/> launched synchronously or asynchronously. The <code class="literal">%px*</code> magic commands<a id="id810" class="indexterm"/> are particularly convenient in the notebook, as they let us work seamlessly on multiple engines in parallel.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec192"/>There's more…</h2></div></div></div><p>The parallel computing capabilities of IPython offer an easy way to launch independent jobs in parallel over multiple cores. A more advanced use case is when jobs have <a id="id811" class="indexterm"/><span class="strong"><strong>dependencies</strong></span>.</p><div class="section" title="Dependent parallel tasks"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec36"/>Dependent parallel tasks</h3></div></div></div><p>There are two types of<a id="id812" class="indexterm"/> dependencies:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Functional dependency</strong></span>: It <a id="id813" class="indexterm"/>determines whether a given task can <a id="id814" class="indexterm"/>execute on a given engine, according to the engine's operating system, the presence or absence of specific Python modules, or other conditions. IPython provides a <code class="literal">@require</code> decorator for functions that need specific Python modules to run on the engines. Another decorator is <code class="literal">@depend</code>; it lets us define arbitrary conditions implemented in a Python function returning <code class="literal">True</code> or <code class="literal">False</code>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Graph dependency</strong></span>: It <a id="id815" class="indexterm"/>determines whether a given task can execute at a given<a id="id816" class="indexterm"/> time on a given engine. We may require a task to run only after one or several other tasks have finished. Additionally, we can impose this condition within any individual engine; an engine may need to execute a specific set of tasks before executing our task. For example, here is how to require tasks B and C (with asynchronous results <code class="literal">arB</code> and <code class="literal">arC</code>) to finish before task A starts:<div class="informalexample"><pre class="programlisting">with view.temp_flags(after=[arB, arC]):
    arA = view.apply_async(f)</pre></div></li></ul></div><p>IPython provides options to specify whether all or any of the dependencies should be met for the task to run. Additionally, we can specify whether success- and/or failure-dependent tasks should be considered as met or not.</p><p>When a task's dependency is <a id="id817" class="indexterm"/>unmet, the scheduler reassigns it to one engine, then to another engine, and so on until an appropriate engine is found. If <a id="id818" class="indexterm"/>the dependency cannot be met on any engine, an <code class="literal">ImpossibleDependency</code> error is raised for the task.</p><p>Passing data between dependent tasks is not particularly easy with IPython.parallel. A first possibility is to use blocking calls in the interactive session; wait for tasks to finish, retrieve the results, and send them back to the next tasks. Another possibility is to share data between engines via the filesystem, but this solution does not work well on multiple computers. An alternate solution is described at: <a class="ulink" href="http://nbviewer.ipython.org/gist/minrk/11415238">http://nbviewer.ipython.org/gist/minrk/11415238</a>.</p></div><div class="section" title="Alternative parallel computing solutions"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec37"/>Alternative parallel computing solutions</h3></div></div></div><p>Besides IPython, there are <a id="id819" class="indexterm"/>numerous alternative parallel computing frameworks in Python, including<a id="id820" class="indexterm"/> <span class="strong"><strong>ParallelPython</strong></span>, <span class="strong"><strong>joblib</strong></span>, and <a id="id821" class="indexterm"/>many others.</p><p>There are also third-party (often commercial) services that provide Python-based clouds, such as <span class="strong"><strong>PythonAnywhere</strong></span><a id="id822" class="indexterm"/> or <a id="id823" class="indexterm"/><span class="strong"><strong>Wakari</strong></span>. They are generally used in two ways:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Distributing a large number of computational tasks across multiple nodes in parallel</strong></span>: Instead of being limited to a few cores with one or several local computers, we can use hundreds or thousands of servers in parallel without worrying about the maintenance of the whole infrastructure (it is handled by the company).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hosting Python applications online, typically with a web interface</strong></span>: For example, with Wakari, IPython notebooks can run on the cloud. An interesting use case is teaching, where students can instantaneously use IPython from a web browser connected to the Internet without installing anything locally.</li></ul></div></div><div class="section" title="References"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec38"/>References</h3></div></div></div><p>Here are a few references about<a id="id824" class="indexterm"/> IPython.parallel:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Documentation of IPython.parallel available at <a class="ulink" href="http://ipython.org/ipython-doc/dev/parallel/">http://ipython.org/ipython-doc/dev/parallel/</a></li><li class="listitem" style="list-style-type: disc">IPython parallel tutorial by the IPython developers available at <a class="ulink" href="http://nbviewer.ipython.org/github/minrk/IPython-parallel-tutorial/blob/master/Index.ipynb">http://nbviewer.ipython.org/github/minrk/IPython-parallel-tutorial/blob/master/Index.ipynb</a></li><li class="listitem" style="list-style-type: disc">Dependencies in IPython.parallel, explained at <a class="ulink" href="http://ipython.org/ipython-doc/dev/parallel/parallel_task.html#dependencies">http://ipython.org/ipython-doc/dev/parallel/parallel_task.html#dependencies</a></li><li class="listitem" style="list-style-type: disc">DAG dependencies, described at <a class="ulink" href="http://ipython.org/ipython-doc/dev/parallel/dag_dependencies.html">http://ipython.org/ipython-doc/dev/parallel/dag_dependencies.html</a></li><li class="listitem" style="list-style-type: disc">Examples of advanced<a id="id825" class="indexterm"/> techniques with IPython.parallel available at <a class="ulink" href="http://github.com/ipython/ipython/tree/master/examples/Parallel%20Computing">http://github.com/ipython/ipython/tree/master/examples/Parallel%20Computing</a></li></ul></div><p>Here are some references about alternative<a id="id826" class="indexterm"/> parallel computing solutions in Python:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Parallel Python available at <a class="ulink" href="http://www.parallelpython.com">http://www.parallelpython.com</a></li><li class="listitem" style="list-style-type: disc">Joblib available at <a class="ulink" href="http://pythonhosted.org/joblib/parallel.html">http://pythonhosted.org/joblib/parallel.html</a></li><li class="listitem" style="list-style-type: disc">List of parallel computing packages available at <a class="ulink" href="http://wiki.python.org/moin/ParallelProcessing">http://wiki.python.org/moin/ParallelProcessing</a></li><li class="listitem" style="list-style-type: disc">Python Anywhere available at <a class="ulink" href="http://www.pythonanywhere.com">http://www.pythonanywhere.com</a></li><li class="listitem" style="list-style-type: disc">Wakari available at <a class="ulink" href="http://wakari.io">http://wakari.io</a></li><li class="listitem" style="list-style-type: disc">IPCluster on Wakari described at <a class="ulink" href="http://continuum.io/blog/ipcluster-wakari-intro">http://continuum.io/blog/ipcluster-wakari-intro</a></li><li class="listitem" style="list-style-type: disc">Teaching with Wakari described at <a class="ulink" href="http://continuum.io/blog/teaching-with-wakari">http://continuum.io/blog/teaching-with-wakari</a></li></ul></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec193"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Interacting with asynchronous parallel tasks in IPython</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Parallelizing code with MPI in IPython</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Interacting with asynchronous parallel tasks in IPython"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec55"/>Interacting with asynchronous parallel tasks in IPython</h1></div></div></div><p>In this recipe, we will show how<a id="id827" class="indexterm"/> to interact with asynchronous<a id="id828" class="indexterm"/> tasks running in parallel with IPython.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec194"/>Getting ready</h2></div></div></div><p>You need to start the IPython engines (see the previous recipe). The simplest option is to launch them from the <span class="strong"><strong>Clusters</strong></span> tab in the notebook dashboard. In this recipe, we use four engines.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec195"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import a few modules:<div class="informalexample"><pre class="programlisting">In [1]: import time
        import sys
        from IPython import parallel
        from IPython.display import clear_output, display
        from IPython.html import widgets</pre></div></li><li class="listitem">We create a <code class="literal">Client</code>:<div class="informalexample"><pre class="programlisting">In [2]: rc = parallel.Client()</pre></div></li><li class="listitem">Now, we create a load-balanced view on the IPython engines:<div class="informalexample"><pre class="programlisting">In [3]: view = rc.load_balanced_view()</pre></div></li><li class="listitem">We define a simple function for our parallel tasks:<div class="informalexample"><pre class="programlisting">In [4]: def f(x):
            import time
            time.sleep(.1)
            return x*x</pre></div></li><li class="listitem">We will run this function on 100 integer numbers in parallel:<div class="informalexample"><pre class="programlisting">In [5]: numbers = list(range(100))</pre></div></li><li class="listitem">We execute <code class="literal">f()</code> on<a id="id829" class="indexterm"/> our list <code class="literal">numbers</code> in <a id="id830" class="indexterm"/>parallel across all of our engines, using <code class="literal">map_async()</code>. This function immediately returns an <code class="literal">AsyncResult</code> object that allows us to interactively retrieve information about the tasks:<div class="informalexample"><pre class="programlisting">In [6]: ar = view.map_async(f, numbers)</pre></div></li><li class="listitem">This object has a <code class="literal">metadata</code> attribute: a list of dictionaries for all engines. We can get the date of submission and completion, the status, the standard output and error, and other information:<div class="informalexample"><pre class="programlisting">In [7]: ar.metadata[0]
Out[7]: {
 'execute_result': None,
 'engine_id': None,
...
 'submitted': datetime.datetime(2014, 1, 1, 10, 30, 38, 9487),
 'follow': None}</pre></div></li><li class="listitem">Iterating over the <code class="literal">AsyncResult</code> instance works normally; the iteration progresses in real-time while the tasks are being completed:<div class="informalexample"><pre class="programlisting">In [8]: for _ in ar:
            print(_, end=', ')
0, 1, 4,..., 9409, 9604, 9801,</pre></div></li><li class="listitem">Now, we create a simple progress bar for our asynchronous tasks. The idea is to create a loop polling for the tasks' status at every second. An <code class="literal">IntProgressWidget</code> widget is updated in real-time and shows the progress of the tasks:<div class="informalexample"><pre class="programlisting">In [9]: def progress_bar(ar):
            # We create a progress bar.
            w = widgets.IntProgressWidget()
            # The maximum value is the number of tasks.
            w.max = len(ar.msg_ids)
            # We display the widget in the output area.
            display(w)
            # Repeat every second:
            while not ar.ready():
                # Update the widget's value with the
                # number of tasks that have finished
                # so far.
                w.value = ar.progress
                time.sleep(1)
            w.value = w.max
In [10]: ar = view.map_async(f, numbers)
In [11]: progress_bar(ar)</pre></div><p>The progress bar is shown in the following screenshot:</p><div class="mediaobject"><img src="images/4818OS_05_07.jpg" alt="How to do it…"/></div></li><li class="listitem">Finally, it is easy to <a id="id831" class="indexterm"/>debug a parallel <a id="id832" class="indexterm"/>task on an engine. We can launch a Qt client on the remote kernel by calling <code class="literal">%qtconsole</code> within a <code class="literal">%%px</code> cell magic:<div class="informalexample"><pre class="programlisting">In [12]: %%px -t 1
         %qtconsole</pre></div><p>The Qt console allows us to inspect the remote namespace for debugging or analysis purposes, as shown in the following screenshot:</p><div class="mediaobject"><img src="images/4818OS_05_08.jpg" alt="How to do it…"/><div class="caption"><p>Qt console for debugging an IPython engine</p></div></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec196"/>How it works…</h2></div></div></div><p>
<code class="literal">AsyncResult</code> instances are<a id="id833" class="indexterm"/> returned by asynchronous parallel functions. They<a id="id834" class="indexterm"/> implement several useful attributes and methods, notably:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">elapsed</code>: Elapsed<a id="id835" class="indexterm"/> time since submission</li><li class="listitem" style="list-style-type: disc"><code class="literal">progress</code>: Number of<a id="id836" class="indexterm"/> tasks that have competed so far</li><li class="listitem" style="list-style-type: disc"><code class="literal">serial_time</code>: Sum of the computation <a id="id837" class="indexterm"/>time of all of the tasks done in parallel</li><li class="listitem" style="list-style-type: disc"><code class="literal">metadata</code>: Dictionary with <a id="id838" class="indexterm"/>further information about the task</li><li class="listitem" style="list-style-type: disc"><code class="literal">ready()</code>: Returns whether the<a id="id839" class="indexterm"/> call has finished</li><li class="listitem" style="list-style-type: disc"><code class="literal">successful()</code>: Returns whether <a id="id840" class="indexterm"/>the call has completed without raising an exception (an exception is raised if the task has not completed yet)</li><li class="listitem" style="list-style-type: disc"><code class="literal">wait()</code>: Blocks until the tasks have<a id="id841" class="indexterm"/> completed (there is an optional timeout argument)</li><li class="listitem" style="list-style-type: disc"><code class="literal">get()</code>: Blocks<a id="id842" class="indexterm"/> until the tasks have completed and returns the result (there is an optional timeout argument)</li></ul></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec197"/>There's more…</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Documentation of the AsyncResult class<a id="id843" class="indexterm"/> available at <a class="ulink" href="http://ipython.org/ipython-doc/dev/parallel/asyncresult.html">http://ipython.org/ipython-doc/dev/parallel/asyncresult.html</a></li><li class="listitem" style="list-style-type: disc">Documentation of the task interface<a id="id844" class="indexterm"/> available at <a class="ulink" href="http://ipython.org/ipython-doc/dev/parallel/parallel_task.html">http://ipython.org/ipython-doc/dev/parallel/parallel_task.html</a></li><li class="listitem" style="list-style-type: disc">Printing engines output in real-time<a id="id845" class="indexterm"/>, demonstrated at <a class="ulink" href="http://github.com/ipython/ipython/blob/master/examples/Parallel%20Computing/iopubwatcher.py">http://github.com/ipython/ipython/blob/master/examples/Parallel%20Computing/iopubwatcher.py</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec198"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Distributing Python code across multiple cores with IPython</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Parallelizing code with MPI in IPython</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Parallelizing code with MPI in IPython"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec56"/>Parallelizing code with MPI in IPython</h1></div></div></div><p>
<span class="strong"><strong>Message Passing Interface</strong></span> (<span class="strong"><strong>MPI</strong></span>) is <a id="id846" class="indexterm"/>a standardized communication protocol for parallel systems. It is used in many parallel computing applications to exchange data between nodes. MPI has a high barrier to entry, but it is very efficient and powerful.</p><p>IPython's parallel computing<a id="id847" class="indexterm"/> system has been designed from the ground up to work <a id="id848" class="indexterm"/>with MPI. If you are new to MPI, it is a good idea to<a id="id849" class="indexterm"/> start using it from IPython. If you are an experienced MPI user, you will find that IPython integrates seamlessly with your parallel application.</p><p>In this recipe, we will see how to use MPI with IPython through a very simple example.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec199"/>Getting ready</h2></div></div></div><p>To use MPI with IPython, you need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A standard MPI implementation such as OpenMPI available at <a class="ulink" href="http://www.open-mpi.org">http://www.open-mpi.org</a> or MPICH<a id="id850" class="indexterm"/> available at <a class="ulink" href="http://www.mpich.org">http://www.mpich.org</a></li><li class="listitem" style="list-style-type: disc">The mpi4py package<a id="id851" class="indexterm"/> available at <a class="ulink" href="http://mpi4py.scipy.org">http://mpi4py.scipy.org</a></li></ul></div><p>For example, here are the commands to install MPI for IPython on Ubuntu and Anaconda:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>conda install mpich2</strong></span>
<span class="strong"><strong>conda install mpi4py</strong></span>
</pre></div><p>You can also do <code class="literal">pip install mpi4py</code> for mpi4py. MPI can also be used on Windows. The website of <span class="emphasis"><em>Python Tools for Visual Studio</em></span><a id="id852" class="indexterm"/> available at <a class="ulink" href="http://pytools.codeplex.com">http://pytools.codeplex.com</a> contains the instructions to do this.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec200"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We first <a id="id853" class="indexterm"/>need to<a id="id854" class="indexterm"/> create <a id="id855" class="indexterm"/>a MPI profile with:<div class="informalexample"><pre class="programlisting">In [1]: !ipython profile create --parallel --profile=mpi</pre></div></li><li class="listitem">Then, we open <code class="literal">~/.ipython/profile_mpi/ipcluster_config.py</code> and add the line <code class="literal">c.IPClusterEngines.engine_launcher_class = 'MPI'</code>.</li><li class="listitem">Once the MPI profile has been created and configured, we can launch the engines by typing in a terminal: <code class="literal">ipcluster start -n 2 --engines MPI --profile=mpi</code>.</li><li class="listitem">Now, to actually use the engines, we create a client in the notebook:<div class="informalexample"><pre class="programlisting">In [2]: import numpy as np
        from IPython.parallel import Client
In [3]: c = Client(profile='mpi')</pre></div></li><li class="listitem">Let's create a view on all engines:<div class="informalexample"><pre class="programlisting">In [4]: view = c[:]</pre></div></li><li class="listitem">In this example, we compute the sum of all integers between 0 and 15 in parallel over two cores. We first distribute the array with the 16 values across the engines (each engine gets a subarray):<div class="informalexample"><pre class="programlisting">In [5]: view.scatter('a', np.arange(16., dtype='float'))
Out[5]: &lt;AsyncResult: scatter&gt;</pre></div></li><li class="listitem">We compute the total sum in parallel using MPI's <code class="literal">allreduce()</code> function. Every node makes the same computation and returns the same result:<div class="informalexample"><pre class="programlisting">In [6]: %%px
        from mpi4py import MPI
        import numpy as np
        print MPI.COMM_WORLD.allreduce(np.sum(a), op=MPI.SUM)
[stdout:0] 120.0
[stdout:1] 120.0</pre></div></li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip23"/>Tip</h3><p>If you get a different result, it means that the engines were not actually started with MPI (see <a class="ulink" href="http://stackoverflow.com/a/20159018/1595060">http://stackoverflow.com/a/20159018/1595060</a>).</p></div></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec201"/>How it works…</h2></div></div></div><p>In this example, each node:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Receives a subset of the integers</li><li class="listitem" style="list-style-type: disc">Computes the local sum of those integers</li><li class="listitem" style="list-style-type: disc">Sends this local sum to all other engines</li><li class="listitem" style="list-style-type: disc">Receives the local sum of the other engines</li><li class="listitem" style="list-style-type: disc">Computes the total sum of those local sums</li></ul></div><p>This is<a id="id856" class="indexterm"/> how <code class="literal">allreduce()</code> works <a id="id857" class="indexterm"/>in MPI; the principle is to <span class="strong"><strong>scatter</strong></span> data <a id="id858" class="indexterm"/>across engines first, then to <span class="strong"><strong>reduce</strong></span> the local<a id="id859" class="indexterm"/> computations through a global operator (here, <code class="literal">MPI.SUM</code>).</p><p>IPython's direct interface also supports the scatter/gather paradigm natively, without resorting to MPI. However, these operations can only be launched from the interactive session, not from the engines themselves.</p><p>There are many other parallel computing paradigms in MPI. You can find more information here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">MPI tutorials by Wes Kendall<a id="id860" class="indexterm"/> available at <a class="ulink" href="http://mpitutorial.com">http://mpitutorial.com</a></li><li class="listitem" style="list-style-type: disc">MPI tutorials by Blaise Barney, Lawrence Livermore National Laboratory, available at <a class="ulink" href="http://computing.llnl.gov/tutorials/mpi/">http://computing.llnl.gov/tutorials/mpi/</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec202"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Distributing Python code across multiple cores with IPython</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Interacting with asynchronous parallel tasks in IPython</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Trying the Julia language in the notebook"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec57"/>Trying the Julia language in the notebook</h1></div></div></div><p>Julia (<a class="ulink" href="http://julialang.org">http://julialang.org</a>) is <a id="id861" class="indexterm"/>a <a id="id862" class="indexterm"/>young, high-level, dynamic language for high-performance numerical computing. The first version was released in 2012 after three years of development at MIT. Julia borrows ideas from Python, R, MATLAB, Ruby, Lisp, C, and other languages. Its major strength is to combine the expressivity and ease of use of high-level, dynamic languages with the speed of C (almost). This is achieved via an LLVM-based Just-In-Time (JIT) compiler that targets machine code for x86-64 architectures.</p><p>In this recipe, we will try Julia in the IPython notebook using the <span class="strong"><strong>IJulia</strong></span> package<a id="id863" class="indexterm"/> available at <a class="ulink" href="http://github.com/JuliaLang/IJulia.jl">http://github.com/JuliaLang/IJulia.jl</a>. We will also show how to use Python packages (such as NumPy and matplotlib) from Julia. Specifically, we will compute and display a Julia set.</p><p>This recipe is inspired by a Julia tutorial given by David P. Sanders at the<a id="id864" class="indexterm"/> SciPy 2014 conference (<a class="ulink" href="http://nbviewer.ipython.org/github/dpsanders/scipy_2014_julia/tree/master/">http://nbviewer.ipython.org/github/dpsanders/scipy_2014_julia/tree/master/</a>).</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec203"/>Getting ready</h2></div></div></div><p>You first need to install Julia. You will find packages for Windows, Mac OS X, and Linux on Julia<a id="id865" class="indexterm"/>'s website at <a class="ulink" href="http://julialang.org/downloads/">http://julialang.org/downloads/</a>. On Ubuntu, you can type <code class="literal">sudo apt-get install julia</code> in a terminal. For IJulia, you also need a C++ compiler. On Ubuntu, you can type <code class="literal">sudo apt-get install build-essential</code>.</p><p>Then, open a Julia terminal with the <code class="literal">julia</code> command, and install IJulia by typing <code class="literal">Pkg.add("IJulia")</code> in the Julia terminal. This package should also create a <code class="literal">julia</code> profile in your IPython installation.</p><p>Finally, to launch a Julia notebook, run <code class="literal">ipython notebook --profile=julia</code> in a terminal. You'll recognize the dashboard of the IPython notebook. The only difference is that the Julia language is used in the notebook instead of Python.</p><p>This recipe has been tested on Ubuntu 14.04 with Julia 0.2.1.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec204"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We can't <a id="id866" class="indexterm"/>avoid<a id="id867" class="indexterm"/> the customary <span class="emphasis"><em>Hello World</em></span> example. The <code class="literal">println()</code> function displays a string and adds a line break at the end:<div class="informalexample"><pre class="programlisting">In [1]: println("Hello world!")
Hello world!</pre></div></li><li class="listitem">We create a polymorphic function, <code class="literal">f</code>, that computes the expression <code class="literal">z*z+c</code>. We will evaluate this function on arrays, so we use element-wise operators with a dot (<code class="literal">.</code>) prefix:<div class="informalexample"><pre class="programlisting">In [2]: f(z, c) = z.*z .+ c
Out[2]: f (generic function with 1 method)</pre></div></li><li class="listitem">Let's evaluate <code class="literal">f</code> on scalar complex numbers (the imaginary number <span class="emphasis"><em>i</em></span> is <code class="literal">1im</code>).<div class="informalexample"><pre class="programlisting">In [3]: f(2.0 + 1.0im, 1.0)
Out[3]: 4.0 + 4.0im</pre></div></li><li class="listitem">Now, we create a (2, 2) matrix. Components are separated by a space and rows are separated by a semicolon (<code class="literal">;</code>). The type of this <code class="literal">Array</code> is automatically inferred from its components. The <code class="literal">Array</code> type is a built-in data type in Julia, similar, but not identical, to NumPy's <code class="literal">ndarray</code> type:<div class="informalexample"><pre class="programlisting">In [4]: z = [-1.0 - 1.0im  1.0 - 1.0im;
             -1.0 + 1.0im  1.0 + 1.0im]
Out[4]: 2x2 Array{Complex{Float64},2}:
 -1.0-1.0im  1.0-1.0im
 -1.0+1.0im  1.0+1.0im</pre></div></li><li class="listitem">We can index arrays with brackets <code class="literal">[]</code>. A notable difference with Python is that indexing starts from 1 instead of 0. MATLAB has the same convention. Furthermore, the keyword <code class="literal">end</code> refers to the last item in that dimension:<div class="informalexample"><pre class="programlisting">In [5]: z[1,end]
Out[5]: 1.0 - 1.0im</pre></div></li><li class="listitem">We can evaluate <code class="literal">f</code> on the matrix <code class="literal">z</code> and a scalar <code class="literal">c</code> (polymorphism):<div class="informalexample"><pre class="programlisting">In [6]: f(z, 0)
Out[6]: 2x2 Array{Complex{Float64},2}:
 0.0+2.0im  0.0-2.0im
 0.0-2.0im  0.0+2.0im</pre></div></li><li class="listitem">Now, we create a<a id="id868" class="indexterm"/> function, <code class="literal">julia</code>, that computes a Julia set. Optional <a id="id869" class="indexterm"/>named arguments are separated from positional arguments by a semicolon (<code class="literal">;</code>). Julia's syntax for flow control is close to that of Python's, except that colons are dropped, indentation doesn't count, and block <code class="literal">end</code> keywords are mandatory:<div class="informalexample"><pre class="programlisting">In [7]: function julia(z, c; maxiter=200)
            for n = 1:maxiter
                if abs2(z) &gt; 4.0
                    return n-1
                end
                z = f(z, c)
            end
            return maxiter
        end
Out[7]: julia (generic function with 1 method)</pre></div></li><li class="listitem">We can use Python packages from Julia. First, we have to install the <code class="literal">PyCall</code> package by using Julia's built-in package manager (<code class="literal">Pkg</code>). Once the package is installed, we can use it in the interactive session with <code class="literal">using PyCall</code>:<div class="informalexample"><pre class="programlisting">In [8]: Pkg.add("PyCall")
        using PyCall</pre></div></li><li class="listitem">We can import Python packages with the <code class="literal">@pyimport</code> <span class="strong"><strong>macro</strong></span><a id="id870" class="indexterm"/> (a metaprogramming feature in Julia). This macro is the equivalent of Python's <code class="literal">import</code> command:<div class="informalexample"><pre class="programlisting">In [9]: @pyimport numpy as np</pre></div></li><li class="listitem">The <code class="literal">np</code> namespace is now available in the Julia interactive session. NumPy arrays are automatically converted to Julia <code class="literal">Array</code> objects:<div class="informalexample"><pre class="programlisting">In [10]: z = np.linspace(-1., 1., 100)
Out[10]: 100-element Array{Float64,1}:
 -1.0     
 -0.979798
...
  0.979798
  1.0</pre></div></li><li class="listitem">We can use list comprehensions to evaluate the function <code class="literal">julia</code> on many arguments:<div class="informalexample"><pre class="programlisting">In [11]: m = [julia(z[i], 0.5) for i=1:100]
Out[11]: 100-element Array{Int64,1}:
 2
...
 2</pre></div></li><li class="listitem">Let's try the <a id="id871" class="indexterm"/>Gadfly plotting package. This library offers a<a id="id872" class="indexterm"/> high-level plotting interface inspired by Dr. Leland Wilkinson's textbook <span class="emphasis"><em>The Grammar of Graphics</em></span>. In the notebook, plots are interactive thanks to the <span class="strong"><strong>D3.js</strong></span> library:<div class="informalexample"><pre class="programlisting">In [12]: Pkg.add("Gadfly")
         using Gadfly 
In [13]: plot(x=1:100, y=m, Geom.point, Geom.line)
Out[13]: Plot(...)</pre></div><p>Here is a screenshot:</p><div class="mediaobject"><img src="images/4818OS_05_09.jpg" alt="How to do it…"/><div class="caption"><p>A Gadfly plot in the IPython notebook with Julia</p></div></div></li><li class="listitem">Now, we compute a Julia set by using two nested loops. In general, and unlike Python, there is no significant performance penalty in using <code class="literal">for</code> loops instead of vectorized operations. High-performance code can be written either with vectorized operations or <code class="literal">for</code> loops:<div class="informalexample"><pre class="programlisting">In [14]: @time m = [julia(complex(r, i), complex(-0.06, 0.67)) 
                    for i = 1:-.001:-1,
                        r = -1.5:.001:1.5];
elapsed time: 0.881234749 seconds (48040248 bytes allocated)</pre></div></li><li class="listitem">Finally, we <a id="id873" class="indexterm"/>use the <code class="literal">PyPlot</code> package to draw matplotlib<a id="id874" class="indexterm"/> figures in Julia:<div class="informalexample"><pre class="programlisting">In [15]: Pkg.add("PyPlot")
         using PyPlot
In [16]: imshow(m, cmap="RdGy", 
                extent=[-1.5, 1.5, -1, 1]);</pre></div><div class="mediaobject"><img src="images/4818OS_05_10.jpg" alt="How to do it…"/></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec205"/>How it works…</h2></div></div></div><p>Languages used to be either low-level, difficult to use, but fast (such as C); or high-level, easy to use, but slow (such as Python). In Python, solutions to this problem include NumPy and Cython, among others.</p><p>Julia developers chose to create a new high-level but fast language, bringing the best of both worlds together. This is essentially achieved through modern Just-In-Time compilation techniques implemented with LLVM.</p><p>Julia dynamically parses code and generates low-level code in the LLVM Intermediate Representation. This representation features a language-independent instruction set that is then compiled to machine code. Code written with explicit loops is directly compiled to machine code. This explains why the performance-motivated vectorization of code is generally not required with Julia.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec206"/>There's more…</h2></div></div></div><p>Strengths<a id="id875" class="indexterm"/> of Julia include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A powerful and flexible dynamic type system based on multiple dispatch for parametric polymorphism</li><li class="listitem" style="list-style-type: disc">Facilities for metaprogramming</li><li class="listitem" style="list-style-type: disc">A simple interface for calling C, FORTRAN, or Python code from Julia</li><li class="listitem" style="list-style-type: disc">Built-in support for fine-grained parallel and distributed computing</li><li class="listitem" style="list-style-type: disc">A built-in multidimensional array data type and numerical computing library</li><li class="listitem" style="list-style-type: disc">A built-in package manager based on Git</li><li class="listitem" style="list-style-type: disc">External packages for data analysis such as DataFrames (equivalent of pandas) and Gadfly (statistical plotting library)</li><li class="listitem" style="list-style-type: disc">Integration in the IPython notebook</li></ul></div><p>What are the strengths of Python over Julia? At the time of this writing, Julia is much younger and less mature than Python and SciPy. Therefore, there are fewer packages and less documentation in Julia than in Python. The syntax of the Julia language is still changing. Additionally, Python is much more commonly found in production environments than Julia. Thus, bringing numerical computing code to a production environment is easier when the code is in Python.</p><p>That being said, the Julia ecosystem and its community are growing fast. We can reasonably expect Julia to become increasingly popular in the future. Also, since both languages can be used in the IPython notebook, we don't necessarily have to <span class="emphasis"><em>choose</em></span> between Python and Julia. We can call Python code and use Python modules from Julia and vice versa.</p><p>We have only scratched the surface of the Julia language in this recipe. Topics of interest we couldn't cover in details here include Julia's type system, the metaprogramming features, the support for parallel computing, and the package manager, among others.</p><p>Here are some references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The Julia language on <a id="id876" class="indexterm"/>Wikipedia available at <a class="ulink" href="http://en.wikipedia.org/wiki/Julia_%28programming_language%29">http://en.wikipedia.org/wiki/Julia_%28programming_language%29</a></li><li class="listitem" style="list-style-type: disc">Official documentation of Julia available at <a class="ulink" href="http://docs.julialang.org/en/latest/">http://docs.julialang.org/en/latest/</a></li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Why We Created Julia</em></span> blog post available at <a class="ulink" href="http://julialang.org/blog/2012/02/why-we-created-julia/">http://julialang.org/blog/2012/02/why-we-created-julia/</a></li><li class="listitem" style="list-style-type: disc">PyCall.jl for calling Python from Julia available at <a class="ulink" href="http://github.com/stevengj/PyCall.jl">http://github.com/stevengj/PyCall.jl</a></li><li class="listitem" style="list-style-type: disc">PyPlot.jl for using matplotlib in Julia available at <a class="ulink" href="http://github.com/stevengj/PyPlot.jl">http://github.com/stevengj/PyPlot.jl</a></li><li class="listitem" style="list-style-type: disc">Gadfly.jl, a Julia plotting library, available at <a class="ulink" href="http://dcjones.github.io/Gadfly.jl/">http://dcjones.github.io/Gadfly.jl/</a></li><li class="listitem" style="list-style-type: disc">DataFrames.jl, an equivalent of pandas for Julia, available at <a class="ulink" href="http://juliastats.github.io/DataFrames.jl/">http://juliastats.github.io/DataFrames.jl/</a></li><li class="listitem" style="list-style-type: disc">Julia Studio, an IDE for Julia, available at <a class="ulink" href="http://forio.com/labs/julia-studio/">http://forio.com/labs/julia-studio/</a></li></ul></div></div></div></div>
</body></html>