["```py\ndf = spark.read.csv(\"file_path\", header=True, sep=\";\", inferSchema=True)\n```", "```py\ndf = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n    .option(\"dbtable\", \"schema.tablename\") \\\n    .option(\"user\", \"username\") \\\n    .option(\"password\", \"password\") \\\n    .load()\n```", "```py\ndf = spark.read.format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:postgresql://localhost:8812/questdb\") \\\n    .option(\"driver\", \"org.postgresql.Driver\") \\\n    .option(\"user\", \"admin\") \\\n    .option(\"password\", \"quest\") \\\n    .option(\"dbtable\", \"timeseries_table\") \\\n    .load()\n```", "```py\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n    .option(\"subscribe\", \"topic1\") \\\n    .load()\n```", "```py\ndf.delta_table_path storage location. The overwrite mode means that existing data at this location will be overwritten. With Delta format, the data is written as a table that is given the name specified in table_name.\nThis example is based on the code in `ts-spark_ch5_1.dbc`, which we imported in the earlier section on batch ingestion.\nThere are many other options for the `spark.write` command, depending on the destination being written to. The following Apache Spark documentation on saving details these options:\n[https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#saving-to-persistent-tables](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#saving-to-persistent-tables)\nWhen the data is persisted in Delta format, in addition to the data, metadata is also stored together to disk. This can be retrieved with the following code:\n\n```", "```py\n\n Note\nIn the code example, we did not have to install Delta as it is already installed when using the Databricks Community Edition. You will need to install the Delta packages if you are using another Apache Spark environment where Delta is not pre-installed. You can find the instructions here: [https://docs.delta.io/latest/quick-start.html](https://docs.delta.io/latest/quick-start.html).\n*Figure 5**.2* shows some of the metadata such as location and creation date.\n![](img/B18568_05_02.jpg)\n\nFigure 5.2: Metadata for the Delta table\nOnce data has been persisted, it can be read from storage as needed at a later stage for querying and analysis. The `spark.read` command can be used here as well, as per the following example:\n\n```", "```py\n\n The Delta table storage location, `delta_table_path`, is passed to the `load` command, which retrieves the stored table from the disk storage.\nAs mentioned earlier, Spark can also write to a database, among other destinations. The following example shows how to write to a PostgreSQL database.\n\n```", "```py\n\n This is further detailed in the following documentation: [https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)\nNote\nYou will need to include the JDBC driver for the particular database on the Spark classpath. The previously referenced documentation explains this.\nAs seen in this section, persistence allows longer-term storage and retrieval. Delta also stores different versions of the data whenever it changes, which we will investigate next.\nVersioning\nData versioning is one of the key features provided by Delta Lake, allowing you to keep track of changes made to your data over time. This storage of different versions is done in an optimal way to minimize storage footprint.\nWith a record of different versions, Delta enables a functionality called **time travel**. With this, you can query data at specific versions or timestamps, revert to previous versions, and perform time travel queries. This is also useful from a reproducibility point of view, whereby we can go back to the specific version of data used previously, even if it has since changed, to audit, review, and redo an analysis.\nThe code provided in this chapter has an example of using versioning and time travel. The following extract shows how to read a specific version of the Delta table. `version_as_of` is an integer representing the version number:\n\n```", "```py\ndf_ = spark.read.history command, as follows:\n\n```", "```py\n\n An example of output from the history is shown in *Figure 5**.3*.\n![](img/B18568_05_03.jpg)\n\nFigure 5.3: Delta table versions\nFinally, it is possible to restore the Delta table back to a previous version with the `restoreToVersion` command, overwriting the latest version, as per the following:\n\n```", "```py\n\n You can also find more information on time travel here:\n[https://delta.io/blog/2023-02-01-delta-lake-time-travel/](https://delta.io/blog/2023-02-01-delta-lake-time-travel/)\nThis concludes the section on ingestion and persistence. We will now move on to verify and clean the data.\nData quality checks, cleaning, and transformation\nOnce the data has been ingested from source systems to a storage location from which we can access it, we will need to ensure that it is of usable quality and, if not, do the necessary cleaning and transformation.\nData quality checks\nThe outcome of any analysis done with the data can be only as good as the data, making data quality checks an important next step.\nConsistency, accuracy, and completeness\nData quality checks for consistency, accuracy, and completeness are essential to ensure the reliability of your data. With its powerful tools for data processing and analysis, Apache Spark is suitable for implementing these checks. The following are examples of how you can perform data quality checks for consistency, accuracy, and completeness using Apache Spark in Python.\nConsistency check\nIn the following consistency test example, we are counting the number of records for each date:\n\n```", "```py\n\n As per *Figure 5**.4*, this simple check shows that some dates do not consistently have the same number of records, which can indicate missing values for some dates.\n![](img/B18568_05_04.jpg)\n\nFigure 5.4: Consistency check results\nAccuracy check\nIn the accuracy test example, we want to verify the accuracy of `Global_active_power`, as follows:\n\n```", "```py\n\n As per *Figure 5**.5*, this check shows that in two cases, `Global_active_power` is outside of the accuracy criteria that we have defined for this check. This indicates that either these values are wrong or that they are correct but are now going beyond the previously known ranges that we have used to define the criteria. We must update the criteria in this latter case.\n![](img/B18568_05_05.jpg)\n\nFigure 5.5: Accuracy check results\nCompleteness check\nIn the completeness test example, we want to verify whether `Global_active_power` has null values:\n\n```", "```py\n\n Note\nThe consistency check example presented earlier can also be used for completeness.\nThese examples show basic data quality checks for consistency, accuracy, and completeness using Apache Spark. These checks can be extended and integrated into your data pipelines for more comprehensive data quality assurance.\nData quality framework\nTo better manage the suite of tests, it is recommended that a framework such as *Great Expectations* be used for data quality checks. You can find more information here: [https://github.com/great-expectations/great_expectations](https://github.com/great-expectations/great_expectations)\nWe will cover another framework approach with the integration of data quality in the Delta Live Tables pipeline, and monitoring and alerting in [*Chapter 10*](B18568_10.xhtml#_idTextAnchor190).\nOnce the data quality has been tested, the next step is to clean the data.\nData cleaning\nThe previous step of data quality checks indicates the issues with the data that need to be corrected, which we will now address.\nMissing values\nApache Spark offers various methods to handle missing values in time series data. The following examples show how you can clean time series data for missing values using Apache Spark in Python.\nForward filling\nThe forward filling method to handle missing values replaces the missing values with the previous known value, with the values sorted in chronological order based on their timestamp. In the following code example, missing values for `Global_active_power` are replaced in this way. The `Window.rowsBetween` function in the following case goes from the first record to the current one. The `last` function then finds the last non-null value within that window. As the window slides over all the records, all the missing values are replaced with the last known value:\n\n```", "```py\n\n The result of forward filling can be seen in *Figure 5**.6*, where the filled values are shown in the `filled_Global_active_power` column.\n![](img/B18568_05_06.jpg)\n\nFigure 5.6: Forward filling\nForward filling works well when the last known value is a good indication of the next value, such as for a slow-changing value. It is not a good method when the value can change suddenly or when there is seasonality.\nBackward filling\nThe backward filling method to handle missing values replaces the missing values with the next known value, with the values sorted in chronological order based on their timestamp. In the following code example, missing values for `Global_active_power` are replaced in this way. The `Window.rowsBetween` function in the following case goes from the current one to the last record. The `first` function then finds the next non-null value within that window. As the window slides over all the records, all the missing values are replaced with the next known value:\n\n```", "```py\n\n The result of backward filling can be seen in *Figure 5**.7*, where the filled values are shown in the `filled_Global_active_power` column.\n![](img/B18568_05_07.jpg)\n\nFigure 5.7: Backward filling\nBackward filling works well when the next known value can reasonably indicate the previous value, such as with slow-changing data or when collecting data retrospectively with gaps in the past. However, it is not suitable for analyzing causality or leading indicators.\nInterpolation\nThe interpolation method to handle missing values replaces the missing values with a combination, such as the average, of the previous and next non-missing values, with the values sorted in chronological order based on their timestamp.\nNote\nThere are several different interpolation calculation methods, including linear, polynomial, and spline interpolation. The average method used here is a simple form of linear interpolation.\nIn the following code example, missing values for `Global_active_power` are replaced in this way. The `Window.rowsBetween` function, used twice, in the following case, goes from the first record to the current one for `windowF`, and from the current one to the last record for `windowB`. The `last` function then finds the previous non-null value within `windowF`, while the `first` function finds the next non-null value within `windowB`. These two non-null values are averaged. As the window slides over all the records, all the missing values are replaced by the averaged value:\n\n```", "```py\n\n The result of interpolation can be seen in *Figure 5**.8*, where the filled values are shown in the `filled_Global_active_power` column.\n![](img/B18568_05_08.jpg)\n\nFigure 5.8: Interpolation\nInterpolation works well for slow-changing values, when there is a predictable cyclical pattern, or when there is a small gap in data. It is not a good method when the value can change suddenly, is discrete, or when there is a large gap in data.\nOf the three methods shown for handling missing values, the appropriate method to use is based on the characteristics of your time series data and the requirements of your analysis.\nData leakage\nNote that the backward filling and interpolation methods can leak future data across the boundaries of training, validation, and test data splits. Use these methods within the splits, and not across, or use forward filling if this is going to be an issue.\nDuplicates\nThe presence of duplicate values in time series data can skew analysis and lead to incorrect results. Apache Spark has functions to efficiently remove duplicate values. In the following example, we clean time series data for duplicate values using Apache Spark in Python.\nThe `dropDuplicates` function removes duplicates by comparing all columns by default and only considers a row to be a duplicate if all the columns match those of one or more other rows. This will not work if we have multiple rows with, say, the same `timestamp` column value but different values in one or more other columns. In this case, we can pass a subset of one or more columns as a parameter to be used to identify the duplicates, as opposed to using all the columns.\nIn the most common cases, we want to have one and only one row of values for each timestamp and consider the other rows with the same timestamp to be duplicates. Passing the timestamp as the subset parameter to `dropDuplicates` will remove all the other rows having the same timestamp value, as we will see in the following code example:\n\n```", "```py\n\n Depending on your dataset and use case, you can choose the appropriate method based on the columns that uniquely identify duplicates in your time series data.\nOutliers\nThe detection and handling of outliers in time series data is crucial to ensure the accuracy of analysis and modeling. Apache Spark provides various functions to detect and handle outliers efficiently. The following example shows how to clean time series data for outliers using Apache Spark in Python.\nThe z-score method used is based on how far the data point is from the `mean` relative to the standard deviation, `stddev`. The parametrizable threshold value, `z_score_threshold`, then specifies beyond which z-score value the data point is considered an outlier. A high threshold will allow more data points in, while a low threshold will flag more outliers:\n\n```", "```py\n\n *Figure 5**.9* shows the outcome of the outlier detection based on the z-score chosen.\n![](img/B18568_05_09.jpg)\n\nFigure 5.9: Outlier detection\nBeyond this example, the choice of z-score threshold and outlier detection techniques is based on the data characteristics and requirements of the use case.\nNote\nOutliers can be indicative of one or more anomalies in the source system that generated the measurement, or data processing or transmission issues post source system. The identification of outliers flags the requirement to further investigate the source system and the data transmission chain to find the root cause.\nAfter cleaning the data based on the issues identified with the data quality checks, other transformations, which we will look at next, are required to get the data into the right shape for the analytics algorithm to work.\nTransformations\nIn this section, we will look at examples of normalizing and standardizing, and touch briefly on stationary transformation.\nNormalizing\nNormalizing time series data ensures that features are on a similar scale, which can improve the performance of machine learning algorithms while facilitating analysis. Apache Spark provides various functions for normalization. The following example shows how to normalize time series data using Apache Spark in Python.\nThe min-max normalization technique is used to scale the data points relative to the min-max range. The `min` and `max` values are calculated first. This brings the value to the range of `0` for the minimum value and `1` for the maximum value:\n\n```", "```py\n\n *Figure 5**.10* shows the outcome of normalizing the example time series data.\n![](img/B18568_05_10.jpg)\n\nFigure 5.10: Normalizing\nDepending on the specific requirements and data characteristics, the normalization method can be adjusted with the use of other techniques such as z-score normalization and decimal scaling, in addition to the min-max technique used in the example.\nStandardizing\nStandardizing time series data ensures that features are on a similar scale, which can improve the performance of machine learning algorithms while facilitating analysis. This method transforms the data such that it has a mean of `0` and a standard deviation of `1`. Apache Spark provides various functions for standardization. The following example shows how to standardize time series data using Apache Spark in Python.\nThis example uses `log` values to account for the skewness of the data. First, `mean` and  `stddev` are calculated. These values are then used in the formula to standardize:\n\n```", "```py\n\n *Figure 5**.11* shows the outcome of standardizing the example time series data.\n![](img/B18568_05_11.jpg)\n\nFigure 5.11: Standardizing\nThe standardization method can be adjusted depending on the specific requirements and data characteristics.\nStationary\nIn [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016), we discussed the requirement of stationary time series for some analysis methods. Making time series data stationery involves removing trends and seasonality, which we will cover in the following chapter.\nThis concludes the section on testing for data quality and then cleaning and transforming time series data. We will cover the scalability considerations in data preparation when we discuss feature engineering in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151).\nSummary\nIn conclusion, this chapter focused on the critical steps of organizing, cleaning, and transforming time series data for effective analysis. We have covered data preparation techniques using Apache Spark for ingestion, persistence, data quality checks, cleaning, and transformations. We looked at code examples for, among others, handling missing values and duplicates, addressing outliers, and normalizing data. This has set the stage for an accurate and efficient analytical process using Apache Spark. Proper data preparation significantly enhances the reliability of subsequent analytical processes, which is what we will progress toward in the next chapter.\nJoin our community on Discord\nJoin our community’s Discord space for discussions with the authors and other readers:\n[https://packt.link/ds](https://packt.link/ds)\n![](img/ds_(1).jpg)\n\n```", "```py\n\n```"]