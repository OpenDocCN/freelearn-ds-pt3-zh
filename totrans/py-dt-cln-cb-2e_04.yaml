- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identifying Outliers in Subsets of Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outliers and unexpected values may not be errors. They often are not. Individuals
    and events are complicated and surprise the analyst. Some people really are 7’4”
    tall and some really have $50 million salaries. Sometimes, data is messy because
    people and situations are messy; however, extreme values can have an out-sized
    impact on our analysis, particularly when we are using parametric techniques that
    assume a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: These issues may become even more apparent when working with subsets of data.
    That is not just because extreme or unexpected values have more weight with smaller
    samples. It is also because they may make less sense when bivariate and multivariate
    relationships are considered. When the 7’4” person, or the person making $50 million,
    is 10 years old, the red flag gets even redder. This may suggest some measurement
    or data collection error.
  prefs: []
  type: TYPE_NORMAL
- en: But the key issue is the undue influence that outliers can have on the inferences
    we draw from our data. Indeed, it may be helpful to think of an outlier as an
    observation with variable values, or relationships between variable values, that
    are so unusual that they cannot help to explain relationships in the rest of the
    data. This matters for statistical inference because we cannot assume a neutral
    impact of outliers on our summary statistics or parameter estimates. Sometimes
    our models work so hard to construct parameter estimates that can account for
    patterns in outlier observations that we compromise the model’s explanatory or
    predictive power for all other observations. Raise your hand if you have ever
    spent days trying to interpret a model only to discover that your coefficients
    and predictions completely changed once you removed a few outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The identification and handling of outliers is among the most important data
    preparation tasks we have in a data analysis project. We explore a range of strategies
    for detecting and treating outliers in this chapter. Specifically, the recipes
    in this chapter examine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying outliers with one variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying outliers and unexpected values in bivariate relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using subsetting to examine logical inconsistencies in variable relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using linear regression to identify data points with significant influence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using *k*-nearest neighbors (KNN) to find outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Isolation Forest to find anomalies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PandasAI to identify outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need pandas, NumPy, and Matplotlib to complete the recipes in this
    chapter. I used pandas 2.1.4, but the code will run on pandas 1.5.3 or later.
  prefs: []
  type: TYPE_NORMAL
- en: The code in this chapter can be downloaded from the book’s GitHub repository,
    [https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition).
  prefs: []
  type: TYPE_NORMAL
- en: Identifying outliers with one variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of an outlier is somewhat subjective but is closely tied to the
    properties of a particular distribution; to its central tendency, spread, and
    shape. We make assumptions about whether a value is expected or unexpected based
    on how likely we are to get that value given the variable’s distribution. We are
    more inclined to view a value as an outlier if it is multiple standard deviations
    away from the mean and it is from a distribution that is approximately normal;
    one that is symmetrical (has low skew) and has relatively skinny tails (low kurtosis).
  prefs: []
  type: TYPE_NORMAL
- en: This becomes clear if we imagine trying to identify outliers from a uniform
    distribution. There is no central tendency and there are no tails. Each value
    is equally likely. If, for example, COVID-19 cases per country were uniformly
    distributed, with a minimum of 1 and a maximum of 10,000,000, neither 1 nor 10,000,000
    would be considered an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: We need to understand how a variable is distributed, then, before we can identify
    outliers. Several Python libraries provide tools to help us understand how variables
    of interest are distributed. We use a couple of them in this recipe to identify
    when a value is sufficiently out of range to be of concern.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need the `matplotlib`, `statsmodels`, and `scipy` libraries, in addition
    to `pandas` and `numpy`, to run the code in this recipe. You can install `matplotlib`,
    `statsmodels`, and `scipy` by entering `pip install matplotlib`, `pip install
    statsmodels`, and `pip install scipy` in a terminal client or PowerShell (in Windows).
    You may also need to install `openpyxl` to save Excel files.
  prefs: []
  type: TYPE_NORMAL
- en: We will work with COVID-19 cases data in this recipe. This dataset has one observation
    for each country with total COVID-19 cases and deaths.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: Our World in Data provides COVID-19 public use data at [https://ourworldindata.org/covid-cases](https://ourworldindata.org/covid-cases).
    The dataset includes total cases and deaths, tests administered, hospital beds,
    and demographic data such as median age, gross domestic product, and a human development
    index, which is a composite measure of standard of living, educational levels,
    and life expectancy. The dataset used in this recipe was downloaded on March 3,
    2024.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We take a good look at the distribution of some of the key continuous variables
    in the COVID-19 data. We examine the central tendency and shape of the distribution,
    generating measures and visualizations of normality:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the `pandas`, `numpy`, `matplotlib`, `statsmodels`, and `scipy` libraries,
    and the COVID-19 case data file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Also, set up the COVID-19 case and demographic columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Get descriptive statistics for the COVID-19 case data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a DataFrame with just the key case data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Show more detailed percentile data. We indicate that we only want to do this
    for numeric values so that the location column is skipped:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: Starting with pandas version 2.0.0, the default value for the `numeric_only`
    parameter is `False` for the `quantile` function. We needed to set the `numeric_only`
    value to `True` to get `quantile` to skip the `location` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should also show skewness and kurtosis. Skewness and kurtosis describe
    how symmetrical the distribution is and how fat the tails of the distribution
    are, respectively. Both measures, for `total_cases` and `total_deaths`, are significantly
    higher than we would expect if our variables were distributed normally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The prototypical normal distribution has a skewness of `0` and a kurtosis of
    `3`.
  prefs: []
  type: TYPE_NORMAL
- en: Test the COVID-19 data for normality.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the Shapiro-Wilk test from the `scipy` library. Print out the *p*-value
    from the test (the `null` hypothesis of a normal distribution can be rejected
    at the 95% level at any *p*-value below `0.05`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Show normal quantile-quantile plots (`qqplots`) of total cases and total cases
    per million.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The straight lines show what the distributions would look like if they were
    normal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following scatterplots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Distribution of COVID-19 cases compared with a normal distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'When adjusted by population with the total cases per million column, the distribution
    is closer to normal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Distribution of COVID-19 cases per million compared with a normal
    distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Show the outlier range for total cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One way to define an outlier for a continuous variable is by the distance above
    the third quartile or below the first quartile. If that distance is more than
    1.5 times the *interquartile range* (the distance between the first and third
    quartiles), that value is considered an outlier. The calculation in this step
    indicates that values above 3,197,208 can be considered outliers. In this case,
    we can ignore an outlier threshold that is less than 0, as that is not possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Generate a DataFrame of outliers and write it to Excel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate over the four COVID-19 case columns. Calculate the outlier thresholds
    for each column as we did in the previous step. From the DataFrame, select those
    rows above the high threshold or below the low threshold. Add columns that indicate
    the variable examined (`varname`) for outliers and the threshold levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following Excel file (some columns are hidden to save space):'
  prefs: []
  type: TYPE_NORMAL
- en: '![outliers](img/B18596_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 Excel file with outlier cases
  prefs: []
  type: TYPE_NORMAL
- en: There were 39 countries identified as outliers in the `total_deaths` values
    according to the interquartile method, and 33 `total_cases` outliers. Notice that
    there were no outliers for `total _cases_pm`.
  prefs: []
  type: TYPE_NORMAL
- en: Look a little more closely at outliers for total deaths per million.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `varname` column we created in the previous step to select the outliers
    for `total_deaths_pm`. Show the columns `(median_age` and `hum_dev_ind`) that
    might help to explain the extreme values for those columns. We also show the 25^(th),
    50^(th), and 75^(th) percentile for those columns for the whole dataset for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: All four countries are well beyond the 75^(th) percentile for deaths per million.
    Three of the four countries are near or above the 75^(th) percentile for both
    median age and the human development index. Surprisingly, there is a positive
    correlation between the human development index and deaths per million. We display
    a correlation matrix in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Show a histogram of total cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Histogram of total COVID-19 cases'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform a log transformation of the COVID-19 data. Show a histogram of the
    log transformation of total cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Histogram of total COVID-19 cases with log transformation'
  prefs: []
  type: TYPE_NORMAL
- en: The tools we used in the preceding steps tell us a fair bit about how COVID-19
    cases and deaths are distributed, and about where outliers are located.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The percentile data shown in *step 3* reflect the skewness of the cases and
    deaths data. If, for example, we look at the range of values between the 20^(th)
    and 30^(th) percentiles, and compare it with the range from the 70^(th) to the
    80^(th) percentiles, we see that the range is much greater in the higher percentiles
    for each variable. This is confirmed by the very high values for skewness and
    kurtosis, compared with normal distribution values of `0` and `3`, respectively.
    We run formal tests of normality in *step 4*, which indicate that the distributions
    of the COVID-19 variables are not normal at high levels of significance.
  prefs: []
  type: TYPE_NORMAL
- en: This is consistent with the `qqplots` that we ran in *step 5*. The distributions
    of both total cases and total cases per million differ significantly from normal,
    as represented by the straight line. Many cases hover around zero, and there is
    a dramatic increase in slope at the right tail.
  prefs: []
  type: TYPE_NORMAL
- en: We identify outliers in *steps 6 and 7*. Using 1.5 times the interquartile range
    to determine outliers is a reasonable rule of thumb. I like to output those values
    to an Excel file, along with the associated data, to see what patterns I can detect
    in the data. This often leads to more questions, of course. We will try to answer
    some of them in the next recipe, but one question we can consider now is what
    accounts for the countries with high deaths per million, as displayed in *step
    8*. Median age and the human development index seem like they might be a part
    of the story. It is worth exploring these bivariate relationships further, which
    we do in subsequent recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Our identification of outliers in *step 7* assumes a normal distribution, an
    assumption that we have shown to be unwarranted. Looking at the distribution of
    total cases in *step 9*, it seems much more like a log-normal distribution, with
    values clustered around `0` and a right skew. We transform the data in *step 10*
    and plot the results of the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We could have also used standard deviation, rather than interquartile ranges,
    to identify outliers in *steps 6 and 7*.
  prefs: []
  type: TYPE_NORMAL
- en: I should add here that outliers are not necessarily data collection or measurement
    errors, and we may or may not need to make adjustments to the data. However, extreme
    values can have a meaningful and persistent impact on our analysis, particularly
    with small datasets like this one.
  prefs: []
  type: TYPE_NORMAL
- en: The overall impression we should have of the COVID-19 case data is that it is
    relatively clean; that is, there are not many invalid values, narrowly defined.
    Looking at each variable independently of how it moves with other variables does
    not identify much that screams out as a clear data error. However, the distribution
    of the variables is statistically quite problematic. Building statistical models
    dependent on these variables will be complicated, as we might have to rule out
    parametric tests.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth remembering that our sense of what constitutes an outlier is
    shaped by our assumption of a normal distribution. If, instead, we allow our expectations
    to be guided by the actual distribution of the data, we have a different understanding
    of extreme values. If our data reflects a social, or biological, or physical process
    that is inherently not normally distributed (uniform, logarithmic, exponential,
    Weibull, Poisson, and so on), our sense of what constitutes an outlier should
    adjust accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boxplots might have also been illuminating here. We do a box plot on this data
    in *Chapter 5*, *Using Visualizations for the Identification of Unexpected Values*.
    We examine variable transformations in more detail in *Chapter 8*, *Encoding,
    Transforming, and Scaling Features*.
  prefs: []
  type: TYPE_NORMAL
- en: We explore bivariate relationships in this same dataset in the next recipe for
    any insights they might provide about outliers and unexpected values. In subsequent
    chapters, we consider strategies for imputing values for missing data and for
    making adjustments to extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying outliers and unexpected values in bivariate relationships
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A value might be unexpected, even if it is not an extreme value, when it does
    not deviate significantly from the distribution mean. Some values for a variable
    are unexpected when a second variable has certain values. This is easy to illustrate
    when one variable is categorical and the other is continuous.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram illustrates the number of bird sightings per day over
    a period of several years, but shows different distributions for each of the two
    sites. One site has a mean sightings per day of 33, and the other 52\. (This is
    fictional data.) The overall mean (not shown) is 42\. What should we make of a
    value of 58 for daily sightings? Is that an outlier? That clearly depends on which
    of the two sites was being observed.
  prefs: []
  type: TYPE_NORMAL
- en: 'If there were 58 sightings on a day at site A, 58 would be an unusually high
    number. Not so for site B, where 58 sightings would not be very different from
    the mean for that site:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Daily bird sightings by site'
  prefs: []
  type: TYPE_NORMAL
- en: 'This hints at a useful rule of thumb: whenever a variable of interest is significantly
    correlated with another variable, we should take that relationship into account
    when trying to identify outliers (or any statistical analysis with that variable
    actually). It is helpful to state this a little more precisely, and extend it
    to cases where both variables are continuous. If we assume a linear relationship
    between variable *x* and variable *y*, we can describe that relationship with
    the familiar *y* = *mx* + *b* equation, where *m* is the slope and *b* is the
    *y*-intercept. We can then expect *y* to increase by *m* for every 1 unit increase
    in *x*. Unexpected values are those that deviate substantially from this relationship,
    where the value of *y* is much higher or lower than what would be predicted given
    the value of *x*. This can be extended to multiple *x*, or predictor, variables.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to identify outliers and unexpected values
    by examining the relationship of a variable to one other variable. In subsequent
    recipes in this chapter, we use multivariate techniques to make additional improvements
    in our outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use the `matplotlib` and `seaborn` libraries in this recipe. You can install
    them with `pip` by entering `pip install matplotlib` and `pip install seaborn`
    with a terminal client or PowerShell (in Windows).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We examine the relationship between total cases and total deaths in the COVID-19
    database. We take a closer look at those countries where deaths are higher or
    lower than expected given the number of cases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `pandas`, `matplotlib`, `seaborn`, and the COVID-19 cumulative data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Generate a correlation matrix for the cumulative and demographic columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Unsurprisingly, there is a high correlation (`0.76`) between total cases and
    total deaths and less of a correlation (`0.44`) between total cases per million
    and total deaths per million. There is a strong (`0.66`) relationship between
    GDP per capita and cases per million (Note that not all of the correlations are
    shown):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Check to see whether some countries have unexpectedly high or low total deaths,
    given the total cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use `qcut` to create a column that breaks the data into quantiles. Show a crosstab
    of total cases quantiles by total deaths quantiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Take a look at countries that do not fit along the diagonal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There is one country with high total cases but low total deaths. Since the
    `covidtotals` and `covidtotalsonly` DataFrames have the same index, we can use
    the Boolean series created from the latter to return selected rows from the former:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Make a scatterplot of total cases by total deaths.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use Seaborn’s `regplot` method to generate a linear regression line in addition
    to the scatterplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following scatterplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Scatterplot of total cases and deaths with a linear regression
    line'
  prefs: []
  type: TYPE_NORMAL
- en: Examine unexpected values above the regression line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It is good to take a closer look at countries with cases and deaths coordinates
    that are noticeably above or below the regression line through the data. There
    are two countries with fewer than 40 million cases and more than 400 thousand
    deaths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Examine unexpected values below the regression line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are two countries with more than 30 million cases but fewer than 100
    thousand deaths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a scatterplot of total cases per million by total deaths per million:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following scatterplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Scatterplot of cases and deaths per million with a linear regression
    line'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding steps examined the relationship between variables in order to
    identify outliers.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A number of questions are raised by looking at the bivariate relationships that
    did not surface in our univariate exploration in the previous recipe. There is
    confirmation of anticipated relationships, such as with the total cases and total
    deaths, but this makes deviations from this all the more curious. There are possible
    substantive explanations for unusually high death rates, given a certain number
    of cases, but measurement error or poor reporting of cases cannot be ruled out
    either.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2* shows a high correlation (0.76) between total cases and total deaths,
    but there is variation even there. We divide the cases and deaths into quantiles
    in *step 3* and then do a crosstab of the quantile values. Most countries are
    along the diagonal or close to it. However, one country has a very high number
    of cases but low deaths, Qatar. It is reasonable to wonder if there are potential
    reporting issues.'
  prefs: []
  type: TYPE_NORMAL
- en: We make a scatterplot in *step 5* of the total cases and deaths. The strong
    upward sloping relationship between the two is confirmed, but there are a couple
    of countries whose deaths are above the regression line. We can see that two countries
    (Brazil and Russia) have higher deaths than would be predicted by the number of
    cases. Two countries, Japan and South Korea, have a much lower number of deaths.
  prefs: []
  type: TYPE_NORMAL
- en: Not surprisingly, there is even more scatter around the regression line in the
    scatterplot of cases per million and deaths per million. There is a positive relationship,
    but the slope of the line is not very steep.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are beginning to get a good sense of what our data looks like, but the data
    in this form does not enable us to examine how the univariate distributions and
    bivariate relationships might change over time. For example, one reason why countries
    might have more deaths per million than the number of cases per million would
    indicate could be that more time has passed since the first confirmed cases. We
    are not able to explore that in the cumulative data. We need the daily data for
    that, which we will look at in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe, and the previous one, show how much data cleaning can bleed into
    exploratory data analysis, even when you are first starting to get a sense of
    your data. I would definitely draw a distinction between data exploration and
    what we are doing here. We are trying to get a sense of how the data hangs together
    and why certain variables take on certain values in certain situations and not
    others. We want to get to the point where there are no huge surprises when we
    begin to do the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: I find it helpful to do small things to formalize this process. I use different
    naming conventions for files that are not quite ready for analysis. If nothing
    else, this helps remind me that any numbers produced at this point are far from
    ready for distribution.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We still have not done much to examine possible data issues that only become
    apparent when examining subsets of data; for example, positive wage income values
    for people who say they are not working (both variables are on the **National
    Longitudinal Survey** or **NLS**). We do that in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We do much more with Matplotlib and Seaborn in *Chapter 5*, *Using Visualizations
    for the Identification of Unexpected Values*.
  prefs: []
  type: TYPE_NORMAL
- en: Using subsetting to examine logical inconsistencies in variable relationships
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At a certain point, data issues come down to deductive logic problems, such
    as variable *x* has to be greater than some quantity *a* when variable *y* is
    less than some quantity *b*. Once we are through some initial data cleaning, it
    is important to check for logical inconsistencies. `pandas` makes this kind of
    error checking relatively straightforward with subsetting tools such as `loc`
    and Boolean indexing. This can be combined with summary methods on Series and
    DataFrames to allow us to easily compare values for a particular row with values
    for the whole dataset or some subset of rows. We can also easily aggregate over
    columns. Just about any question we might have about the logical relationships
    between variables can be answered with these tools. We work through some examples
    in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will work with the NLS data, mainly with data on employment and education.
    We use `apply` and `lambda` functions several times in this recipe, but go into
    more detail on their use in *Chapter 9*, *Fixing Messy Data When Aggregating*.
    However, it is not necessary to review *Chapter 9* to follow along, even if you
    have no experience with those tools.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: The National Longitudinal Survey of Youth is conducted by the United States
    Bureau of Labor Statistics. This survey started with a cohort of individuals in
    1997 who were born between 1980 and 1985, with annual follow-ups each year up
    until 2023\. For this recipe, I pulled 106 variables on grades, employment, income,
    and attitudes toward government from the hundreds of data items on the survey.
    NLS data can be downloaded from [nlsinfo.org](https://nlsinfo.org).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We run a number of logical checks on the NLS data, such as individuals with
    post-graduate enrollment but no undergraduate enrollment, or those with wage income
    but no weeks worked. We also check for large changes in key values for a given
    individual from one period to the next:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and then load the NLS data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Look at some of the employment and education data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The dataset has weeks worked each year from 2000 through 2023, and college
    enrollment status each month from February 1997 through October 2022\. We use
    the ability of the `loc` accessor to choose all columns from the column indicated
    on the left of the colon through to the column indicated on the right; for example,
    `nls97.loc[:, "colenroct15":"colenrfeb22"]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Show individuals with wage income but no weeks worked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Check whether an individual was ever enrolled in a four-year college.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chain several methods. First, create a DataFrame with columns that start with
    `colenr` (`nls97.filter(like="colenr")`). These are the college enrollment columns
    for October and February of each year. Then, use `apply` to run a `lambda` function
    that examines the first character of each `colenr` column (`apply(lambda x: x.str[0:1]==''3'')`).
    This returns a value of `True` or `False` for all of the college enrollment columns;
    `True` if the first value of the string is `3`, meaning enrollment at a four year
    college. Finally, use the `any` function to test whether any of the values returned
    from the previous step has a value of `True` (`any(axis=1)`). This will identify
    whether the individual was enrolled in a four-year college between February 1997
    and October 2022\. The first statement here shows the results of the first two
    steps for explanatory purposes only. Only the second statement needs to be run
    to get the desired results, which are to see whether the individual was enrolled
    at a four-year college at some point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Show individuals with post-graduate enrollment but no bachelor’s enrollment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can use what we tested in *step 4* to do some checking. We want individuals
    who have a `4` (graduate enrollment) as the first character for `colenr` of any
    month, but who never had a value of `3` (bachelor enrollment). Note the ~ before
    the second half of the test, for negation. There are 24 individuals who fall into
    this category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Show individuals with bachelor’s degrees or more, but no four-year college enrollment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use `isin` to compare the first character in `highestdegree` with all of the
    values in a list (`nls97.highestdegree.str[0:1].isin([''4'',''5'',''6'',''7''])`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Show individuals with a high wage income.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define high wages as three standard deviations above the mean. It looks as
    though wage income values have been truncated at $380,288:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Show individuals with large changes in weeks worked for the most recent year.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the average value for weeks worked between 2016 and 2020 for each
    person (`nls97.loc[:, "weeksworked16":"weeksworked20"].mean(axis=1)`). We indicate
    `axis=1` to calculate the mean across columns for each individual, rather than
    over individuals. We then find rows where the mean is *not* between half and twice
    the number of weeks worked in 2021\. (Notice our use of the *~* operator earlier.)
    We also indicate that we are not interested in rows that satisfy those criteria
    by being `null` for weeks worked in 2021\. There are 1,099 individuals with sharp
    changes in weeks worked in 2021, compared with the 2016 to 2020 average:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Show inconsistencies in the highest grade completed and the highest degree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `crosstab` function to show `highestgradecompleted` by `highestdegree`
    for people with `highestgradecompleted` less than 12\. A good number of these
    individuals indicate that they have completed high school, which is unusual in
    the United States if the highest grade completed is less than 12:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: These steps reveal several logical inconsistencies in the NLS data.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The syntax required to do the kind of subsetting that we have done in this recipe
    may seem a little complicated if you are seeing it for the first time. You do
    get used to it, however, and it allows you to quickly run any query against the
    data that you might imagine.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the inconsistencies or unexpected values suggest either respondent or
    entry error and warrants further investigation. It is hard to explain positive
    values for wage income when the `weeks worked` value is `0`. Other unexpected
    values might not be data problems at all, but suggest that we should be careful
    about how we use that data. For example, we might not want to use the weeks worked
    in 2021 by itself. Instead, we might consider using three-year averages in many
    analyses.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Selecting and organizing columns* and *Selecting rows* recipes in *Chapter
    3*, *Taking the Measure of Your Data*, demonstrate some of the techniques for
    subsetting data used here. We examine the `apply` functions in more detail in
    *Chapter 9*, *Fixing Messy Data When Aggregating*.
  prefs: []
  type: TYPE_NORMAL
- en: Using linear regression to identify data points with significant influence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The remaining recipes in this chapter use statistical modeling to identify outliers.
    The advantage of these techniques is that they are less dependent on the distribution
    of the variable of concern, and take more into account than can be revealed in
    either univariate or bivariate analyses. This allows us to identify outliers that
    are not otherwise apparent. On the other hand, by taking more factors into account,
    multivariate techniques may provide evidence that a previously suspect value is
    actually within an expected range, and provides meaningful information.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we use linear regression to identify observations (rows) that
    have an out-sized influence on models of a target or dependent variable. This
    can indicate that one or more values for a few observations are so extreme that
    they compromise the model fit for all of the other observations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code in this recipe requires the `matplotlib` and `statsmodels` libraries.
    You can install them by entering `pip install matplotlib` and `pip install statsmodels`
    in a terminal window or PowerShell (in Windows).
  prefs: []
  type: TYPE_NORMAL
- en: We will be working with data on total COVID-19 cases and deaths per country.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the statsmodels `OLS` method to fit a linear regression model of
    the total cases per million of the population. We will then identify those countries
    that have the greatest influence on that model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas`, `matplotlib`, and `statsmodels`, and load the COVID-19 case
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create an analysis file and generate descriptive statistics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get just the columns required for analysis. Drop any row with missing data
    for the analysis columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Fit a linear regression model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are good conceptual reasons to believe that population density, median
    age, and GDP per capita may be predictors of total cases per million. We use those
    three variables in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Identify those countries with an out-sized influence on the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cook’s Distance values of greater than 0.5 should be scrutinized closely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Create an influence plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Countries with higher Cook’s Distance values have larger circles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Influence plot, including countries with the highest Cook’s Distance'
  prefs: []
  type: TYPE_NORMAL
- en: Run the model without the two outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Removing these outliers impacts each coefficient of the model, but particularly
    population density (which is still not significant at the 95% confidence level):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: This gives us a sense of the countries that are most unlike the others in terms
    of the relationship between demographic variables and total cases per million
    in population.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cook’s Distance is a measure of how much each observation influences the model.
    The large impact of the two outliers is confirmed in *step 6* when we rerun the
    model without them. The question for the analyst is whether outliers such as these
    add important information or distort the model and limit its applicability. The
    coefficient of 11570 for median age in the first regression results indicates
    that every one-year increase in median age is associated with an 11570 increase
    in cases per million people. That number is substantially smaller in the model
    without outliers, 9969.
  prefs: []
  type: TYPE_NORMAL
- en: The `P>|t|` value in the regression output tells us whether the coefficient
    is significantly different from `0`. In the first regression, the coefficients
    for `median_age` and `gdp_per_capita` are significant at the 99% level; that is,
    the `P>|t|` value is less than 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We ran a linear regression model in this recipe, not so much because we were
    interested in the parameter estimates of the model, but because we wanted to determine
    whether there were observations with potential out-sized influence on any multivariate
    analysis we might conduct. That definitely seems to be true in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Often, it makes sense to remove the outliers, as we have done here, but that
    is not always true. When we have independent variables that do a good job of capturing
    what makes outliers different, then the parameter estimates for the other independent
    variables are less vulnerable to distortion. We also might consider transformations,
    such as the log transformation we did in a previous recipe, and the scaling we
    will do in the next two recipes. An appropriate transformation, given your data,
    can reduce the influence of outliers by limiting the size of residuals at the
    extremes.
  prefs: []
  type: TYPE_NORMAL
- en: Using k-nearest neighbors to find outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised machine learning tools can help us identify observations that are
    unlike others when we have unlabeled data; that is, when there is no target or
    dependent variable. (In the previous recipe, we used total cases per million as
    the dependent variable.) Even when selecting targets and factors is relatively
    straightforward, it might be helpful to identify outliers without making any assumptions
    about relationships between variables. We can use **k****-nearest neighbors**
    (**KNN**) to find observations that are most unlike others, those where there
    is the greatest difference between their values and their nearest neighbors’ values.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need **Python Outlier Detection** (**PyOD**) and scikit-learn to run
    the code in this recipe. You can install both by entering `pip install pyod` and
    `pip install sklearn` in the terminal or PowerShell (in Windows).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use KNN to identify countries whose attributes indicate that they are
    most anomalous:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `pandas`, `pyod`, and `sklearn`, along with the COVID-19 case data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a standardized DataFrame for the analysis columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the KNN model and generate anomaly scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We create an arbitrary number of outliers by setting the contamination parameter
    to 0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Show the predictions from the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a DataFrame from the `y_pred` and `y_scores` NumPy arrays. Set the index
    to the `covidanalysis` DataFrame index so that we can easily combine it with that
    DataFrame later. Notice that the decision scores for outliers are all higher than
    those for the inliers (outlier = 0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Show COVID-19 data for the outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, merge the `covidanalysis` and `pred` DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: These steps show how we can use KNN to identify outliers based on multivariate
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyOD is a package of Python outlier detection tools. We use it here as a wrapper
    around scikit-learn’s KNN package. This simplifies some tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Our focus in this recipe is not on building a model, but on getting a sense
    of which observations (countries) are significant outliers once we take all the
    data we have into account. This analysis supports our developing sense that Singapore
    and Qatar are very different observations than the others in our dataset. They
    have very high decision scores. (The table in *step 5* is sorted in descending
    order of score.)
  prefs: []
  type: TYPE_NORMAL
- en: Countries such as Bahrain and Luxembourg might also be considered outliers,
    though that is less clear cut. The previous recipe did not indicate that they
    had an overwhelming influence on a regression model. However, that model did not
    take both cases per million and deaths per million into account at the same time.
    That could also explain why Singapore is even more of an outlier than Qatar here.
    It has both high cases per million and below-average deaths per million.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn makes scaling very easy. We used the standard scaler in *step 2*,
    which returned the *z*-score for each value in the DataFrame. The *z*-score subtracts
    the variable mean from each variable value and divides it by the standard deviation
    for the variable. Many machine learning tools require standardized data to run
    well.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KNN is a very popular machine learning algorithm. It is easy to run and interpret.
    Its main limitation is that it will run slowly on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We have skipped steps we might usually take when building machine learning models.
    We did not create separate training and testing datasets, for example. PyOD allows
    this to be done easily, but this is not necessary for our purposes here.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We go into detail on transforming data in *Chapter 8*, *Encoding, Transforming,
    and Scaling Features*. A good resource on using KNN is *Data Cleaning and Exploration
    with Machine Learning*, also by me.
  prefs: []
  type: TYPE_NORMAL
- en: The PyOD toolkit has a large number of supervised and unsupervised learning
    techniques for detecting anomalies in data. You can get the documentation for
    this at [https://pyod.readthedocs.io/en/latest/](https://pyod.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: Using Isolation Forest to find anomalies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Isolation Forest is a relatively new machine learning technique for identifying
    anomalies. It has quickly become popular, partly because its algorithm is optimized
    to find anomalies, rather than normal values. It finds outliers by successive
    partitioning of the data until a data point has been isolated. Points that require
    fewer partitions to be isolated receive higher anomaly scores. This process turns
    out to be fairly easy on system resources. In this recipe, we demonstrate how
    to use it to detect outlier COVID-19 cases and deaths.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need scikit-learn and Matplotlib to run the code in this recipe. You
    can install them by entering `pip install sklearn` and `pip install matplotlib`
    in the terminal or PowerShell (in Windows).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use Isolation Forest to find the countries whose attributes indicate
    that they are most anomalous:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `pandas`, `matplotlib`, and the `StandardScaler` and `IsolationForest`
    modules from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a standardized analysis DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, remove all rows with missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Run an Isolation Forest model to detect outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass the standardized data to the `fit` method. 18 countries are identified
    as outliers. (These countries have anomaly values of `-1`.) This is determined
    by the contamination number of `0.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Create outlier and inlier DataFrames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'List the top 10 outliers according to anomaly score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the outliers and inliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Inlier and outlier countries by GDP, median age, and cases per
    million'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding steps demonstrate the use of Isolation Forest as an alternative
    to KNN for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We used Isolation Forest in this recipe much like we used KNN in the previous
    recipe. In *step 3*, we passed a standardized dataset to the Isolation Forest
    `fit` method, and then used its `predict` and `decision_function` methods to get
    the anomaly flag and score, respectively. We used the anomaly flag in *step 4*
    to separate the data into inliers and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: We plot the inliers and outliers in *step 5*. Since there are only three dimensions
    in the plot, it does not quite capture all of the features in our Isolation Forest
    model, but the outliers (the red dots) clearly have a higher GDP per capita and
    median age; these are typically to the right of, and behind, the inliers.
  prefs: []
  type: TYPE_NORMAL
- en: The results from Isolation Forest are quite similar to the KNN results. Singapore,
    Bahrain, and Qatar have three of the four highest (most negative) anomaly scores.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Isolation Forest is a good alternative to KNN, particularly when working with
    large datasets. The efficiency of its algorithm allows it to handle large samples
    and a high number of variables.
  prefs: []
  type: TYPE_NORMAL
- en: The anomaly detection techniques we have used in the last three recipes were
    designed to improve multivariate analyses and the training of machine learning
    models. However, we might want to exclude the outliers they helped us identify
    much earlier in the analysis process. For example, if it makes sense to exclude
    Qatar from our modeling, it might also make sense to exclude Qatar from some descriptive
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to being useful for anomaly detection, the Isolation Forest algorithm
    is quite satisfying intuitively. (I think the same could be said about KNN.) You
    can read more about Isolation Forest here: [https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Using PandasAI to identify outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use PandasAI to support some of the work we have done in this chapter
    to identify outliers. We can check for extreme values based on a univariate analysis.
    We can look at bivariate and multivariate relationships as well. PandasAI will
    also help us generate visualizations easily.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to install PandasAI to run the code in this recipe. You can do that
    with `pip install pandasai`. We will work with the COVID-19 data again, which
    is available in the GitHub repository, as well as the code.
  prefs: []
  type: TYPE_NORMAL
- en: You will also need an API key from OpenAI. You can get one at [platform.openai.com](https://platform.openai.com).
    You will need to setup an account and then click on your profile in the upper-right
    corner and then View API keys.
  prefs: []
  type: TYPE_NORMAL
- en: The PandasAI library is improving rapidly, and some things have changed, even
    since I began writing this book. I have used PandasAI version 2.0.30 in this recipe.
    It also matters which version of pandas you use with it. I have use pandas version
    2.2.1 in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create a PandasAI instance in the following steps and use it to look for
    extreme and unexpected values in the COVID-19 data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import `pandas` and the `PandasAI` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the COVID-19 data and create a `PandasAI SmartDataframe`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can pass natural language queries to the `chat` method of the `SmartDataframe`.
    This includes plotting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Histogram of total cases per million'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also create a boxplot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Boxplot of total cases per million'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also show a scatterplot of the relationship between cases and deaths.
    We indicate that we want to use `regplot` to make sure that we get a regression
    line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_04_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Scatterplot of the relationship between cases and deaths'
  prefs: []
  type: TYPE_NORMAL
- en: 'Show high and low values for total cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This sorted the data in ascending order for the high group, and then sorted
    the data in ascending order for the low group.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can find the countries with the highest number of cases for each region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can get `chat` to show us countries where the number of cases was high but
    the number of deaths was relatively low:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These were just a few examples of how PandasAI can be used to help us find outliers
    or unexpected values with very little code.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We used PandasAI with the large language model provided by OpenAI in this recipe.
    You just need an API token. You can get one from [platform.openai.com](https://platform.openai.com).
    After you have a token, all you need to do to get started with sending natural
    language queries to a database is import the OpenAI and `SmartDataframe` modules
    and instantiate a `SmartDataframe` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'We created a `SmartDataframe` object in *step 2* with `covidtotalssdf = SmartDataframe(covidtotals,
    config={"llm": llm})`. Once we have a `SmartDataframe`, we can pass a variety
    of natural language instructions to its `chat` method. In this recipe, this ranged
    from requesting visualizations and finding the highest and lowest values to examining
    values for subsets of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a good idea to regularly check the `pandasai.log` file, which will be
    in the same folder as your Python script. Here is the code PandasAI generated
    in response to `covidtotalssdf.chat("Show total cases per million and total deaths
    per million for locationss with high total_cases_pm and low total_deaths_pm")`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '2024-04-18 09:30:01 [INFO] Executing Step 4: CachePopulation'
  prefs: []
  type: TYPE_NORMAL
- en: '2024-04-18 09:30:01 [INFO] Executing Step 5: CodeCleaning'
  prefs: []
  type: TYPE_NORMAL
- en: 2024-04-18 09:30:01 [INFO]
  prefs: []
  type: TYPE_NORMAL
- en: 'Code running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We generated a boxplot in *step 4*, which is an incredibly useful tool for visualizing
    the distribution of a continuous variable. The box shows the interquartile range,
    which is the distance between the first and third quartiles. The line in the box
    shows the median. We go into much more detail on boxplots in *Chapter 5*, *Using
    Visualizations for the Identification of Unexpected Values*.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Using generative AI to create descriptive statistics* recipe in *Chapter
    3*, *Taking the Measure of Your Data*, provides some additional information on
    how PandasAI uses OpenAI, and on generating overall and by-group statistics and
    visualizations. We use PandasAI throughout this book whenever it is a good tool
    for improving our data preparation work or making it easier.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced pandas tools for identifying outliers in our data. We
    explored a variety of univariate, bivariate, and multivariate approaches to detect
    observations sufficiently out of range, or otherwise unusual enough, to distort
    our analysis. These approaches included using the interquartile range to identify
    extreme values, investigating relationships with a correlated variable, and using
    parametric and non-parametric multivariate techniques such as linear regression
    and KNN respectively. We also saw how visualizations can help us get a better
    feel for how a variable is distributed, and how it moves with a correlated variable.
    We will go into much greater detail on how to create and interpret visualizations
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://discord.gg/p8uSgEAETX](https://discord.gg/p8uSgEAETX )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code10336218961138498953.png)'
  prefs: []
  type: TYPE_IMG
