<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Unsupervised Learning</h1>
                </header>
            
            <article>
                
<p>The goal of unsupervised learning is to discover the hidden patterns or structures of the data in which no target variable exists to perform either classification or regression methods. Unsupervised learning methods are often more challenging, as the outcomes are subjective and there is no simple goal for the analysis, such as predicting the class or continuous variable. These methods are performed as part of exploratory data analysis. On top of that, it can be hard to assess the results obtained from unsupervised learning methods, since there is no universally accepted mechanism for performing the validation of results.</p>
<p>Nonetheless, unsupervised learning methods have growing importance in various fields as a trending topic nowadays, and many researchers are actively working on them at the moment to explore this new horizon. A few good applications are:</p>
<ul>
<li><strong>Genomics</strong>: Unsupervised learning applied to understanding genomic-wide biological insights from DNA to better understand diseases and peoples. These types of tasks are more exploratory in nature.</li>
<li><strong>Search engine</strong>: Search engines might choose which search results to display to a particular individual based on the click histories of other similar users.</li>
<li><strong>Knowledge extraction</strong>: To extract the taxonomies of concepts from raw text to generate the knowledge graph to create the semantic structures in the field of NLP.</li>
<li><strong>Segmentation of customers</strong>: In the banking industry, unsupervised learning like clustering is applied to group similar customers, and based on those segments, marketing departments design their contact strategies. For example, older, low-risk customers will be targeted with fixed deposit products and high-risk, younger customers will be targeted with credit cards or mutual funds, and so on.</li>
<li><strong>Social network analysis</strong>: To identify the cohesive groups of people in networks who are more connected with each other and have similar characteristics in common.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In this chapter, we will be covering the following techniques to perform unsupervised learning with data which is openly available:</p>
<ul>
<li>K-means clustering</li>
<li>Principal component analysis</li>
<li>Singular value decomposition</li>
<li>Deep auto encoders</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means clustering</h1>
                </header>
            
            <article>
                
<p>Clustering is the task of grouping observations in such a way that members of the same cluster are more similar to each other and members of different clusters are very different from each other.</p>
<p>Clustering is commonly used to explore a dataset to either identify the underlying patterns in it or to create a group of characteristics. In the case of social networks, they can be clustered to identify communities and to suggest missing connections between people. Here are a few examples:</p>
<ul>
<li>In anti-money laundering measures, suspicious activities and individuals can be identified using anomaly detection</li>
<li>In biology, clustering is used to find groups of genes with similar expression patterns</li>
<li>In marketing analytics, clustering is used to find segments of similar customers so that different marketing strategies can be applied to different customer segments accordingly</li>
</ul>
<p>The k-means clustering algorithm is an iterative process of moving the centers of clusters or centroids to the mean position of their constituent points, and reassigning instances to their closest clusters iteratively until there is no significant change in the number of cluster centers possible or number of iterations reached.</p>
<p>The cost function of k-means is determined by the Euclidean distance (square-norm) between the observations belonging to that cluster with its respective centroid value. An intuitive way to understand the equation is, if there is only one cluster (<em>k=1</em>), then the distances between all the observations are compared with its single mean. Whereas, if, number of clusters increases to <em>2</em> (<em>k= 2</em>), then two-means are calculated and a few of the observations are assigned to cluster <em>1</em> and other observations are assigned to cluster two<em>-</em>based on proximity. Subsequently, distances are calculated in cost functions by applying the same distance measure, but separately to their cluster centers:</p>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/5b6d065b-e4eb-4099-98ea-1bb4ce828e7a.jpg" style="width:13.83em;height:4.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means working methodology from first principles</h1>
                </header>
            
            <article>
                
<p>The k-means working methodology is illustrated in the following example in which 12 instances are considered with their <em>X</em> and <em>Y</em> values. The task is to determine the optimal clusters out of the data.</p>
<table style="border-collapse: collapse;width: 50%" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td>
<p><strong>Instance</strong></p>
</td>
<td>
<p><strong>X</strong></p>
</td>
<td>
<p><strong>Y</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>8</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>4</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>4</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>5</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>7</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>3</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>4</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>4</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>7</p>
</td>
</tr>
<tr>
<td>
<p>11</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>6</p>
</td>
</tr>
<tr>
<td>
<p>12</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p>After plotting the data points on a 2D chart, we can see that roughly two clusters are possible, where below-left is the first cluster and the top-right is another cluster, but in many practical cases, there would be so many variables (or dimensions) that, we cannot simply visualize them. Hence, we need a mathematical and algorithmic way to solve these types of problems.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/f0c97d44-75e6-4853-92fe-a3372e260516.png" style="width:41.58em;height:37.08em;"/></div>
<p>Iteration 1: Let us assume two centers from two instances out of all the <em>12</em> instances. Here, we have chosen instance <em>1</em> (<em>X = 7, Y = 8</em>) and instance <em>8</em> (<em>X = 1, Y = 4</em>), as they seem to be at both extremes. For each instance, we will calculate its Euclidean distances with respect to both centroids and assign it to the nearest cluster center.</p>
<table style="border-collapse: collapse;width: 50%" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td>
<p><strong>Instance</strong></p>
</td>
<td>
<p><strong>X</strong></p>
</td>
<td>
<p><strong>Y</strong></p>
</td>
<td>
<p><strong>Centroid 1 distance</strong></p>
</td>
<td>
<p><strong>Centroid 2 distance</strong></p>
</td>
<td>
<p><strong>Assigned cluster</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>7.21</p>
</td>
<td>
<p>0.00</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>1.00</p>
</td>
<td>
<p>6.40</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>5.00</p>
</td>
<td>
<p>4.12</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>2.83</p>
</td>
<td>
<p>7.21</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>5.10</p>
</td>
<td>
<p>3.16</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>5.00</p>
</td>
<td>
<p>2.24</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>2.24</p>
</td>
<td>
<p>6.40</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>0.00</p>
</td>
<td>
<p>7.21</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>4.00</p>
</td>
<td>
<p>4.47</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>6.71</p>
</td>
<td>
<p>1.00</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>11</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>6.32</p>
</td>
<td>
<p>2.00</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>12</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>3.16</p>
</td>
<td>
<p>8.60</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>Centroid 1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>4</p>
</td>
<td><img src="assets/438e74a5-745c-43f0-ad9c-2362b2b59b9c.png"/></td>
<td><img src="assets/cbff530d-df02-4220-9c3e-08fed5173b00.png"/></td>
<td><img src="assets/8af3b2a7-62cc-4f43-92ae-40a7b23fd7bf.png"/></td>
</tr>
<tr>
<td>
<p>Centroid 2</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>8</p>
</td>
<td><img src="assets/358bb8ba-84bc-41af-adbc-ba5af7a1c240.png"/></td>
<td><img src="assets/4e3fe26d-4992-4faa-85bb-0f9e97e3c876.png"/></td>
<td><img src="assets/a739b078-ad01-43e3-a39d-ef9175add4bd.png"/></td>
</tr>
</tbody>
</table>
<p> </p>
<p>The Euclidean distance between two points <em>A (X1, Y1)</em> and <em>B (X2, Y2)</em> is shown as follows:</p>
<div style="padding-left: 30px" class="CDPAlignLeft CDPAlign"><img src="assets/55d4981b-74a0-49ff-847c-6979ec3f53eb.jpg" style="width:33.25em;height:1.42em;"/></div>
<p>Centroid distance calculations are performed by taking Euclidean distances. A sample calculation has been shown as follows. For instance, six with respect to both centroids (centroid 1 and centroid 2).</p>
<div style="padding-left: 30px" class="CDPAlignLeft CDPAlign"><img src="assets/110a5c4f-55e5-4f06-ba05-d66bdffb1d7f.jpg" style="width:39.33em;height:2.00em;"/></div>
<div style="padding-left: 30px" class="CDPAlignLeft CDPAlign"><img src="assets/9f064a47-95f4-4124-aa3f-8a92fadc15b2.jpg" style="width:39.75em;height:1.75em;"/></div>
<p>The following chart describes the assignment of instances to both centroids, which was shown in the preceding table format:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/77001e36-ac36-4787-a869-233ccf9d4785.png"/></div>
<p>If we carefully observe the preceding chart, we realize that all the instances seem to be assigned appropriately apart from instance <em>9 (X =5, Y = 4)</em>. However, in later stages, it should be assigned appropriately. Let us see in the below steps how the assignments evolve.</p>
<p>Iteration 2: In this iteration, new centroids are calculated from the assigned instances for that cluster or centroid. New centroids are calculated based on the simple average of the assigned points.</p>
<table style="border-collapse: collapse;width: 50%" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td>
<p><strong>Instance</strong></p>
</td>
<td>
<p><strong>X</strong></p>
</td>
<td>
<p><strong>Y</strong></p>
</td>
<td>
<p><strong>Assigned cluster</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>11</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>C2</p>
</td>
</tr>
<tr>
<td>
<p>12</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>C1</p>
</td>
</tr>
<tr>
<td>
<p>Centroid 1</p>
</td>
<td>
<p>2.67</p>
</td>
<td>
<p>3</p>
</td>
<td><img src="assets/742e1f1f-b3d1-4f1c-9784-abc6d1a3094c.png"/></td>
</tr>
<tr>
<td>
<p>Centroid 2</p>
</td>
<td>
<p>6.33</p>
</td>
<td>
<p>6.17</p>
</td>
<td><img src="assets/43b9c467-4b37-4ffa-8eaa-839f8fd21095.png"/></td>
</tr>
</tbody>
</table>
<p> </p>
<p>Sample calculations for centroids 1 and 2 are shown as follows. A similar methodology will be applied on all subsequent iterations as well:</p>
<div style="padding-left: 30px" class="CDPAlignLeft CDPAlign"><img src="assets/9c84a254-ad28-4346-96d2-fd7375d5af80.jpg" style="width:36.33em;height:1.25em;"/></div>
<div style="padding-left: 30px" class="CDPAlignLeft CDPAlign"><img src="assets/bc01e19a-688e-441c-af12-a666e4697e47.jpg" style="width:36.75em;height:1.42em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/e17ff033-4f21-4475-844f-93fa65c45e6b.jpg" style="width:24.33em;height:2.67em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/f978f046-94fd-4db3-9e3b-f5e12cbe22b9.jpg" style="width:25.00em;height:2.75em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/285ee94c-54a6-4438-827d-d21c75b808e6.jpg" style="width:24.00em;height:3.17em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/cbd81fbc-7003-4f1e-a7f3-5577aada051c.jpg" style="width:24.25em;height:2.83em;"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/29582c7e-f166-4f38-8d69-6369cd60f01e.png"/></div>
<p>After updating the centroids, we need to reassign the instances to the nearest centroids, which we will be performing in iteration 3.</p>
<p>Iteration 3: In this iteration, new assignments are calculated based on the Euclidean distance between instances and new centroids. In the event of any changes, new centroids will be calculated iteratively until no changes in assignments are possible or the number of iterations is reached. The following table describes the distance measures between new centroids and all the instances:</p>
<table style="border-collapse: collapse;width: 50%" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td>
<p><strong>Instance</strong></p>
</td>
<td>
<p><strong>X</strong></p>
</td>
<td>
<p><strong>Y</strong></p>
</td>
<td>
<p><strong>Centroid 1 distance</strong></p>
</td>
<td>
<p><strong>Centroid 2 distance</strong></p>
</td>
<td>
<p><strong>Previously assigned cluster</strong></p>
</td>
<td>
<p><strong>Newly assigned cluster</strong></p>
</td>
<td>
<p><strong>Changed?</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>6.61</p>
</td>
<td>
<p>1.95</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>1.20</p>
</td>
<td>
<p>4.84</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>3.48</p>
</td>
<td>
<p>2.19</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1.05</p>
</td>
<td>
<p>5.34</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>3.88</p>
</td>
<td>
<p>1.22</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>4.63</p>
</td>
<td>
<p>1.57</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>0.33</p>
</td>
<td>
<p>4.60</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>1.95</p>
</td>
<td>
<p>5.75</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>2.54</p>
</td>
<td>
<p>2.55</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>5.89</p>
</td>
<td>
<p>1.07</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>11</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>6</p>
</td>
<td>
<p>5.27</p>
</td>
<td>
<p>0.69</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>C2</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>12</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>2.11</p>
</td>
<td>
<p>6.74</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>C1</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>Centroid 1</p>
</td>
<td>
<p>2.67</p>
</td>
<td>
<p>3</p>
</td>
<td><img src="assets/646a430e-92f0-4fa3-aa08-984cd2bdbd34.png"/></td>
<td><img src="assets/09a53854-a61d-49ce-a6cb-1a001a8e975e.png"/></td>
<td><img src="assets/dd3ea496-cba7-4453-8c04-a1eea53f23db.png"/></td>
<td><img src="assets/c3da0bc7-e7ee-40ee-a5cb-03893bd6aedd.png"/></td>
<td><img src="assets/4ed2a736-8357-494a-ab4d-17be40cf3fc5.png"/></td>
</tr>
<tr>
<td>
<p>Centroid 2</p>
</td>
<td>
<p>6.33</p>
</td>
<td>
<p>6.17</p>
</td>
<td><img src="assets/a3eb34ab-9293-4099-9786-90591849fe29.png"/></td>
<td><img src="assets/252f60f5-038e-411c-92cb-66d4a9d35e5c.png"/></td>
<td><img src="assets/dbf84cbb-e970-4bc6-80d0-d9c30687e625.png"/></td>
<td><img src="assets/e9b5dd81-94dc-4f29-8a10-dd27556a7680.png"/></td>
<td><img src="assets/06298a03-a5fa-4365-8734-12379b51a337.png"/></td>
</tr>
</tbody>
</table>
<p> </p>
<p>It seems that there are no changes registered. Hence, we can say that the solution is converged. One important thing to note here is that all the instances are very clearly classified well, apart from instance <em>9 (X = 5, Y = 4).</em> Based on instinct, it seems like it should be assigned to centroid 2, but after careful calculation, that instance is more proximate to cluster 1 than cluster 2. However, the difference in distance is low (2.54 with centroid 1 and 2.55 with centroid 2).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimal number of clusters and cluster evaluation</h1>
                </header>
            
            <article>
                
<p>Though selecting number of clusters is more of an art than science, optimal number of clusters are chosen where there will not be a much marginal increase in explanation ability by increasing number of clusters are possible. In practical applications, usually business should be able to provide what would be approximate number of clusters they are looking for.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The elbow method</h1>
                </header>
            
            <article>
                
<p>The elbow method is used to determine the optimal number of clusters in k-means clustering. The elbow method plots the value of the cost function produced by different values of <em>k</em>. As you know, if <em>k</em> increases, average distortion will decrease, each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as <em>k</em> increases. The value of <em>k</em> at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/bc03a902-fad6-4f77-8799-00a69a78965f.png" style="width:42.67em;height:28.50em;"/></div>
<p>Evaluation of clusters with silhouette coefficient: the silhouette coefficient is a measure of the compactness and separation of the clusters. Higher values represent a better quality of cluster. The silhouette coefficient is higher for compact clusters that are well separated and lower for overlapping clusters. Silhouette coefficient values do change from -1 to +1, and the higher the value is, the better.</p>
<p>The silhouette coefficient is calculated per instance. For a set of instances, it is calculated as the mean of the individual sample's scores.</p>
<div style="padding-left: 210px" class="CDPAlignLeft CDPAlign"><img src="assets/c2801e6f-85c6-4465-ab6c-1a32129a3059.jpg" style="width:6.67em;height:3.00em;"/></div>
<p><em>a</em> is the mean distance between the instances in the cluster, <em>b</em> is the mean distance between the instance and the instances in the next closest cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means clustering with the iris data example</h1>
                </header>
            
            <article>
                
<p>The famous iris data has been used from the UCI machine learning repository for illustration purposes using k-means clustering. The link for downloading the data is here: <a href="http://archive.ics.uci.edu/ml/datasets/Iris"><span class="URLPACKT">http://archive.ics.uci.edu/ml/datasets/Iris</span></a>. The iris data has three types of flowers: setosa, versicolor, and virginica and their respective measurements of sepal length, sepal width, petal length, and petal width. Our task is to group the flowers based on their measurements. The code is as follows:</p>
<pre><strong>&gt;&gt;&gt; import os 
""" First change the following directory link to where all input files do exist """ 
&gt;&gt;&gt; os.chdir("D:\\Book writing\\Codes\\Chapter 8") 
 
K-means algorithm from scikit-learn has been utilized in the following example 
 
# K-means clustering 
&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; import pandas as pd 
&gt;&gt;&gt; import matplotlib.pyplot as plt 
&gt;&gt;&gt; from scipy.spatial.distance import cdist, pdist 
 
&gt;&gt;&gt; from sklearn.cluster import KMeans 
&gt;&gt;&gt; from sklearn.metrics import silhouette_score</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<pre><strong>&gt;&gt;&gt; iris = pd.read_csv("iris.csv") 
&gt;&gt;&gt; print (iris.head())</strong> </pre>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/ef89819f-082a-43d6-8e09-9faba82ae4b7.png"/></div>
<p>Following code is used to separate <kbd>class</kbd> variable as dependent variable for creating colors in plot and unsupervised learning algorithm applied on given <kbd>x</kbd> variables without any target variable does present:</p>
<pre><strong>&gt;&gt;&gt; x_iris = iris.drop(['class'],axis=1) 
&gt;&gt;&gt; y_iris = iris["class"]</strong> </pre>
<p>As sample metrics, three clusters have been used, but in real life, we do not know how many clusters data will fall under in advance, hence we need to test the results by trial and error. The maximum number of iterations chosen here is 300 in the following, however, this value can also be changed and the results checked accordingly:</p>
<pre><strong>&gt;&gt;&gt; k_means_fit = KMeans(n_clusters=3,max_iter=300) 
&gt;&gt;&gt; k_means_fit.fit(x_iris) 
 
&gt;&gt;&gt; print ("\nK-Means Clustering - Confusion Matrix\n\n",pd.crosstab(y_iris, k_means_fit.labels_,rownames = ["Actuall"],colnames = ["Predicted"]) )      
&gt;&gt;&gt; print ("\nSilhouette-score: %0.3f" % silhouette_score(x_iris, k_means_fit.labels_, metric='euclidean')) </strong></pre>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/eb623d02-bec4-4a18-b0f7-50ea36f59690.png"/></div>
<p>From the previous confusion matrix, we can see that all the setosa flowers are clustered correctly, whereas 2 out of 50 versicolor, and 14 out of 50 virginica flowers are incorrectly classified.</p>
<div class="packt_infobox">Again, to reiterate, in real-life examples we do not have the category names in advance, so we cannot measure accuracy, and so on.</div>
<p>Following code is used to perform sensitivity analysis to check how many number of clusters does actually provide better explanation of segments:</p>
<pre><strong>&gt;&gt;&gt; for k in range(2,10): 
...     k_means_fitk = KMeans(n_clusters=k,max_iter=300) 
...     k_means_fitk.fit(x_iris) 
...     print ("For K value",k,",Silhouette-score: %0.3f" % silhouette_score(x_iris, k_means_fitk.labels_, metric='euclidean'))</strong> </pre>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="image-border" src="assets/87ed59be-8c75-4a70-9081-061fbf072085.png"/></div>
<p>The silhouette coefficient values in the preceding results shows that <kbd>K value 2</kbd> and <kbd>K value 3</kbd> have better scores than all the other values. As a thumb rule, we need to take the next <kbd>K value</kbd> of the highest silhouette coefficient. Here, we can say that <kbd>K value 3</kbd> is better. In addition, we also need to see the average within cluster variation value and elbow plot before concluding the optimal <kbd>K value</kbd>.</p>
<pre><strong># Avg. within-cluster sum of squares 
&gt;&gt;&gt; K = range(1,10) 
 
&gt;&gt;&gt; KM = [KMeans(n_clusters=k).fit(x_iris) for k in K] 
&gt;&gt;&gt; centroids = [k.cluster_centers_ for k in KM] 
 
&gt;&gt;&gt; D_k = [cdist(x_iris, centrds, 'euclidean') for centrds in centroids] 
 
&gt;&gt;&gt; cIdx = [np.argmin(D,axis=1) for D in D_k] 
&gt;&gt;&gt; dist = [np.min(D,axis=1) for D in D_k] 
&gt;&gt;&gt; avgWithinSS = [sum(d)/x_iris.shape[0] for d in dist] 
 
# Total with-in sum of square 
&gt;&gt;&gt; wcss = [sum(d**2) for d in dist] 
&gt;&gt;&gt; tss = sum(pdist(x_iris)**2)/x_iris.shape[0] 
&gt;&gt;&gt; bss = tss-wcss 
 
# elbow curve - Avg. within-cluster sum of squares 
&gt;&gt;&gt; fig = plt.figure() 
&gt;&gt;&gt; ax = fig.add_subplot(111) 
&gt;&gt;&gt; ax.plot(K, avgWithinSS, 'b*-') 
&gt;&gt;&gt; plt.grid(True)</strong> 
<strong>&gt;&gt;&gt; plt.xlabel('Number of clusters') 
&gt;&gt;&gt; plt.ylabel('Average within-cluster sum of squares')</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/8daf39b3-428f-408c-85f1-2b5f00e27337.png" style="width:39.75em;height:33.08em;"/></div>
<p>From the elbow plot, it seems that at the value of three, the slope changes drastically. Here, we can select the optimal k-value as three.</p>
<pre><strong># elbow curve - percentage of variance explained 
&gt;&gt;&gt; fig = plt.figure() 
&gt;&gt;&gt; ax = fig.add_subplot(111) 
&gt;&gt;&gt; ax.plot(K, bss/tss*100, 'b*-') 
&gt;&gt;&gt; plt.grid(True) 
&gt;&gt;&gt; plt.xlabel('Number of clusters') 
&gt;&gt;&gt; plt.ylabel('Percentage of variance explained')</strong><br/><strong>&gt;&gt;&gt; plt.show()</strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/d33fe9d2-c24d-462e-954b-eaec1784dc56.png" style="width:38.75em;height:33.00em;"/></div>
<p>Last but not least, the total percentage of variance explained value should be greater than 80 percent to decide the optimal number of clusters. Even here, a k-value of three seems to give a decent value of total variance explained. Hence, we can conclude from all the preceding metrics (silhouette, average within cluster variance, and total variance explained), that three clusters are ideal.</p>
<p>The R code for k-means clustering using iris data is as follows:</p>
<pre><strong>setwd("D:\\Book writing\\Codes\\Chapter 8")   
   
iris_data = read.csv("iris.csv")   
x_iris =   iris_data[,!names(iris_data) %in% c("class")]   
y_iris = iris_data$class   
   
km_fit = kmeans(x_iris,centers   = 3,iter.max = 300 )   
   
print(paste("K-Means   Clustering- Confusion matrix"))   
table(y_iris,km_fit$cluster)   
   
mat_avgss = matrix(nrow = 10,   ncol = 2)   
   
# Average within the cluster   sum of square   
print(paste("Avg. Within   sum of squares"))   
for (i in (1:10)){   
  km_fit =   kmeans(x_iris,centers = i,iter.max = 300 )   
  mean_km =   mean(km_fit$withinss)   
  print(paste("K-Value",i,",Avg.within   sum of squares",round(mean_km, 2)))   
  mat_avgss[i,1] = i   
  mat_avgss[i,2] = mean_km   
}  </strong> 
<strong>   
plot(mat_avgss[,1],mat_avgss[,2],type   = 'o',xlab = "K_Value",ylab = "Avg. within sum of square")   
title("Avg. within sum of   squares vs. K-value")   
   
mat_varexp = matrix(nrow = 10,   ncol = 2)   
# Percentage of Variance   explained   
print(paste("Percent.   variance explained"))   
for (i in (1:10)){   
  km_fit =   kmeans(x_iris,centers = i,iter.max = 300 )   
  var_exp =   km_fit$betweenss/km_fit$totss   
  print(paste("K-Value",i,",Percent   var explained",round(var_exp,4)))   
  mat_varexp[i,1]=i   
  mat_varexp[i,2]=var_exp   
}   
   
plot(mat_varexp[,1],mat_varexp[,2],type   = 'o',xlab = "K_Value",ylab = "Percent Var explained")   
title("Avg. within sum of   squares vs. K-value")   <br/></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Principal Component Analysis - PCA</h1>
                </header>
            
            <article>
                
<p><strong>Principal Component Analysis</strong> (<strong>PCA</strong>) is the dimensionality reduction technique which has so many utilities. PCA reduces the dimensions of a dataset by projecting the data onto a lower-dimensional subspace. For example, a 2D dataset could be reduced by projecting the points onto a line. Each instance in the dataset would then be represented by a single value, rather than a pair of values. In a similar way, a 3D dataset could be reduced to two dimensions by projecting variables onto a plane. PCA has the following utilities:</p>
<ul>
<li>Mitigate the course of dimensionality</li>
<li>Compress the data while minimizing the information lost at the same time</li>
<li>Principal components will be further utilized in the next stage of supervised learning, in random forest, boosting, and so on</li>
<li>Understanding the structure of data with hundreds of dimensions can be difficult, hence, by reducing the dimensions to 2D or 3D, observations can be visualized easily</li>
</ul>
<p>PCA can easily be explained with the following diagram of a mechanical bracket which has been drawn in the machine drawing module of a mechanical engineering course. The left-hand side of the diagram depicts the top view, front view, and side view of the component. However, on the right-hand side, an isometric view has been drawn, in which one single image has been used to visualize how the component looks. So, one can imagine that the left-hand images are the actual variables and the right-hand side is the first principal component, in which most variance has been captured.</p>
<p>Finally, three images have been replaced by a single image by rotating the axis of direction. In fact, we replicate the same technique in PCA analysis.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/2b01cbe2-ce2d-4f9c-8f32-2726291ccd9c.png" style="width:25.92em;height:20.58em;"/></div>
<p>Principal component working methodology is explained in the following sample example, in which actual data has been shown in a 2D space, in which <em>X</em> and <em>Y</em> axis are used to plot the data. Principal components are the ones in which maximum variation of the data is captured.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/bb756076-0484-4d81-91e0-7a3408877bbc.png" style="width:28.92em;height:22.33em;"/></div>
<p>The following diagram illustrates how it looks after fitting the principal components. The first principal component covers the maximum variance in the data and the second principal component is orthogonal to the first principal component, as we know all principal components are orthogonal to each other. We can represent whole data with the first principal component itself. In fact, that is how it is advantageous to represent the data with fewer dimensions, to save space and also to grab maximum variance in the data, which can be utilized for supervised learning in the next stage. This is the core advantage of computing principal components.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/c14a3e04-d0bd-4e75-a88c-3bc333313de5.png" style="width:36.83em;height:30.42em;"/></div>
<p>Eigenvectors and eigenvalues have significant importance in the field of linear algebra, physics, mechanics, and so on. Refreshing, basics on eigenvectors and eigenvalues is necessary when studying PCAs. Eigenvectors are the axes (directions) along which a linear transformation acts simply by <em>stretching/compressing</em> and/or <em>flipping</em>; whereas, eigenvalues give you the factors by which the compression occurs. In another way, an eigenvector of a linear transformation is a nonzero vector whose direction does not change when that linear transformation is applied to it.</p>
<p>More formally, <em>A</em> is a linear transformation from a vector space and <img src="assets/3db9b070-a499-4f85-adbb-f94db5952d8e.jpg" style="width:0.92em;height:0.92em;"/> is a nonzero vector, then eigen vector of <em>A</em> if <img src="assets/3cb9f44d-af9b-458c-9ab7-30d0c07d0097.jpg" style="width:2.00em;height:1.25em;"/> is a scalar multiple of <img src="assets/2bbd2b5e-e3f5-4a73-9fcf-e22a9fab8389.jpg" style="width:0.92em;height:0.92em;"/>. The condition can be written as the following equation:</p>
<div style="padding-left: 210px" class="CDPAlignLeft CDPAlign"><img src="assets/1f928a6c-47af-4385-a6d7-fab5714d5e3e.jpg" style="width:7.17em;height:1.67em;"/></div>
<p>In the preceding equation, <img src="assets/3be9b079-d8cb-429a-a0bb-846f2f1e8376.jpg" style="width:1.33em;height:1.33em;"/> is an eigenvector, <em>A</em> is a square matrix, and λ is a scalar called an eigenvalue. The direction of an eigenvector remains the same after it has been transformed by <em>A</em>; only its magnitude has changed, as indicated by the eigenvalue, That is, multiplying a matrix by one of its eigenvectors is equal to scaling the eigenvector, which is a compact representation of the original matrix. The following graph describes eigenvectors and eigenvalues in a graphical representation in a 2D space:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/17245038-7b03-404a-a156-64d171bafc2b.png" style="width:26.58em;height:22.25em;"/></div>
<p>The following example describes how to calculate eigenvectors and eigenvalues from the square matrix and its understanding. Note that eigenvectors and eigenvalues can be calculated only for square matrices (those with the same dimensions of rows and columns).</p>
<div style="padding-left: 210px" class="CDPAlignLeft CDPAlign"><img src="assets/cea11ff3-de17-44c2-a6f3-3ea239962f68.jpg" style="width:7.75em;height:2.58em;"/></div>
<p>Recall the equation that the product of <em>A</em> and any eigenvector of <em>A</em> must be equal to the eigenvector multiplied by the magnitude of eigenvalue:</p>
<div style="padding-left: 180px" class="CDPAlignLeft CDPAlign"><img src="assets/cba22798-df2e-4158-888c-8df53ef0ade2.jpg" style="width:8.50em;height:1.75em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/f58e2ccc-bbc1-416b-8a2b-0e4454dcc257.jpg" style="width:20.17em;height:2.42em;"/></div>
<p>A characteristic equation states that the determinant of the matrix, that is the difference between the data matrix and the product of the identity matrix and an eigenvalue is <em>0</em>.</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/40df230b-283f-4970-9b17-c0d969b6a754.jpg" style="width:23.50em;height:2.58em;"/></div>
<p>Both eigenvalues for the preceding matrix are equal to <em>-2</em>. We can use eigenvalues to substitute for eigenvectors in an equation:</p>
<div style="padding-left: 210px" class="CDPAlignLeft CDPAlign"><img src="assets/c8910138-9f31-4248-9895-e8cb45c7cfdd.jpg" style="width:6.42em;height:1.50em;"/></div>
<div style="padding-left: 210px" class="CDPAlignLeft CDPAlign"><img src="assets/7c827736-7dc5-4c02-8f0c-0c90a94300eb.jpg" style="width:8.92em;height:1.50em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/7dc3ba7d-5947-436f-a1ae-35ddcb67a9ab.jpg" style="width:23.00em;height:5.42em;"/></div>
<p>Substituting the value of eigenvalue in the preceding equation, we will obtain the following formula:</p>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/49410af9-b4e6-45af-8312-458cb9ad23e8.jpg" style="width:26.00em;height:2.83em;"/></div>
<p>The preceding equation can be rewritten as a system of equations, as follows:</p>
<div style="padding-left: 180px" class="CDPAlignLeft CDPAlign"><img src="assets/e1073470-0ede-40de-beef-e5f1922bb29a.jpg" style="width:12.17em;height:4.00em;"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">This equation indicates it can have multiple solutions of eigenvectors we can substitute with any values which hold the preceding equation for verification of equation. Here, we have used the vector <em>[1 1]</em> for verification, which seems to be proved.</p>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/99fb5711-0835-4d42-a455-1bfb86610443.jpg" style="width:16.42em;height:2.67em;"/></div>
<p>PCA needs unit eigenvectors to be used in calculations, hence we need to divide the same with the norm or we need to normalize the eigenvector. The 2-norm equation is shown as follows:</p>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/59838abb-7a26-498f-914a-2c37a0ed985b.jpg" style="width:14.58em;height:3.17em;"/></div>
<p>The norm of the output vector is calculated as follows:</p>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/f2c8bb1d-4a74-4d73-aef8-5f08fb11791f.jpg" style="width:13.58em;height:2.58em;"/></div>
<p>The unit eigenvector is shown as follows:</p>
<div style="padding-left: 180px" class="CDPAlignLeft CDPAlign"><img src="assets/0339799d-78a7-4016-accb-4d2b1b091791.jpg" style="width:10.83em;height:2.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PCA working methodology from first principles</h1>
                </header>
            
            <article>
                
<p>PCA working methodology is described in the following sample data, which has two dimensions for each instance or data point. The objective here is to reduce the 2D data into one dimension (also known as the <strong>principal component</strong>):</p>
<table style="border-collapse: collapse;width: 50%" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td>
<p><strong>Instance</strong></p>
</td>
<td>
<p><strong>X</strong></p>
</td>
<td>
<p><strong>Y</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>0.72</p>
</td>
<td>
<p>0.13</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>0.18</p>
</td>
<td>
<p>0.23</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>2.50</p>
</td>
<td>
<p>2.30</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>0.45</p>
</td>
<td>
<p>0.16</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>0.04</p>
</td>
<td>
<p>0.44</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>0.13</p>
</td>
<td>
<p>0.24</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>0.30</p>
</td>
<td>
<p>0.03</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>2.65</p>
</td>
<td>
<p>2.10</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>0.91</p>
</td>
<td>
<p>0.91</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>0.46</p>
</td>
<td>
<p>0.32</p>
</td>
</tr>
<tr>
<td>
<p>Column mean</p>
</td>
<td>
<p>0.83</p>
</td>
<td>
<p>0.69</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The first step, prior to proceeding with any analysis, is to subtract the mean from all the observations, which removes the scale factor of variables and makes them more uniform across dimensions.</p>
<table style="border-collapse: collapse;width: 50%" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td>
<p><strong>X</strong></p>
</td>
<td>
<p><strong>Y</strong></p>
</td>
</tr>
<tr>
<td>
<p><em>0.72 - 0.83 = -0.12</em></p>
</td>
<td>
<p><em>0.13 - 0.69 = - 0.55</em></p>
</td>
</tr>
<tr>
<td>
<p><em>0.18 - 0.83 = -0.65</em></p>
</td>
<td>
<p><em>0.23 - 0.69 = - 0.46</em></p>
</td>
</tr>
<tr>
<td>
<p><em>2.50 - 0.83 = 1.67</em></p>
</td>
<td>
<p><em>2.30 - 0.69 = 1.61</em></p>
</td>
</tr>
<tr>
<td>
<p><em>0.45 - 0.83 = -0.38</em></p>
</td>
<td>
<p><em>0.16 - 0.69 = - 0.52</em></p>
</td>
</tr>
<tr>
<td>
<p><em>0.04 - 0.83 = -0.80</em></p>
</td>
<td>
<p><em>0.44 - 0.69 = - 0.25</em></p>
</td>
</tr>
<tr>
<td>
<p><em>0.13 - 0.83 = -0.71</em></p>
</td>
<td>
<p><em>0.24 - 0.69 = - 0.45</em></p>
</td>
</tr>
<tr>
<td>
<p><em>0.30 - 0.83 = -0.53</em></p>
</td>
<td>
<p><em>0.03 - 0.69 = - 0.66</em></p>
</td>
</tr>
<tr>
<td>
<p><em>2.65 - 0.83 = 1.82</em></p>
</td>
<td>
<p><em>2.10 - 0.69 = 1.41</em></p>
</td>
</tr>
<tr>
<td>
<p><em>0.91 - 0.83 = 0.07</em></p>
</td>
<td>
<p><em>0.91 - 0.69 = 0.23</em></p>
</td>
</tr>
<tr>
<td>
<p><em>0.46 - 0.83 = -0.37</em></p>
</td>
<td>
<p><em>0.32 - 0.69 = -0.36</em></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Principal components are calculated using two different techniques:</p>
<ul>
<li>Covariance matrix of the data</li>
<li>Singular value decomposition</li>
</ul>
<p>We will be covering the singular value decomposition technique in the next section. In this section, we will solve eigenvectors and eigenvalues using covariance matrix methodology.</p>
<p>Covariance is a measure of how much two variables change together and it is a measure of the strength of the correlation between two sets of variables. If the covariance of two variables is zero, we can conclude that there will not be any correlation between two sets of the variables. The formula for covariance is as follows:</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/ff9c2871-d222-4ea6-ad0a-7709e9255386.jpg" style="width:19.25em;height:3.00em;"/></div>
<p>A sample covariance calculation is shown for <em>X</em> and <em>Y</em> variables in the following formulas. However, it is a 2 x 2 matrix of an entire covariance matrix (also, it is a square matrix).</p>
<div class="CDPAlignLeft CDPAlign"><img src="assets/df0457af-fc6c-407e-9500-1a329a999902.jpg" style="width:52.33em;height:4.25em;"/></div>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/56608b63-bc70-4109-bec9-369e7131b43c.jpg" style="width:12.08em;height:2.00em;"/></div>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/a2526e8a-4126-408b-8c35-516225fa0e7d.jpg" style="width:21.92em;height:2.17em;"/></div>
<p>Since the covariance matrix is square, we can calculate eigenvectors and eigenvalues from it. You can refer to the methodology explained in an earlier section.</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/bbf6df6e-5d7a-4a74-8d50-4cd569311ae0.jpg" style="width:23.33em;height:2.25em;"/></div>
<p>By solving the preceding equation, we can obtain eigenvectors and eigenvalues, as follows:</p>
<div style="padding-left: 120px" class="CDPAlignLeft CDPAlign"><img src="assets/e6786705-3ce4-498c-8a3f-1f8c54286647.jpg" style="width:16.67em;height:1.75em;"/></div>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/aeb1ea0e-ce34-4a15-b69e-8b5d4fa6cbf5.jpg" style="width:19.75em;height:2.42em;"/></div>
<p>The preceding mentioned results can be obtained with the following Python syntax:</p>
<pre>&gt;&gt;&gt; import numpy as np<br/>&gt;&gt;&gt; w, v = np.linalg.eig(np.array([[ 0.91335 ,0.75969 ],[ 0.75969,0.69702]]))<br/>\&gt;&gt;&gt; print ("\nEigen Values\n", w)<br/>&gt;&gt;&gt; print ("\nEigen Vectors\n", v)</pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/4ecbc0fb-7f02-4f7b-8c3f-450f594cf379.png"/></div>
<p>Once we obtain the eigenvectors and eigenvalues, we can project data into principal components. The first eigenvector has the greatest eigenvalue and is the first principal component, as we would like to reduce the original 2D data into 1D data.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3a1b86ba-2783-41a7-9b6b-c9ba58786a34.jpg" style="width:16.50em;height:16.17em;"/></div>
<p>From the preceding result, we can see the 1D projection of the first principal component from the original 2D data. Also, the eigenvalue of 1.5725 explains the fact that the principal component explains variance of 57 percent more than the original variables. In the case of multi-dimensional data, the thumb rule is to select the eigenvalues or principal components with a value greater than what should be considered for projection.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PCA applied on handwritten digits using scikit-learn</h1>
                </header>
            
            <article>
                
<p>The PCA example has been illustrated with the handwritten digits example from scikit-learn datasets, in which handwritten digits are created from 0-9 and its respective 64 features (8 x 8 matrix) of pixel intensities. Here, the idea is to represent the original features of 64 dimensions into as few as possible:</p>
<pre><strong># PCA - Principal Component Analysis 
&gt;&gt;&gt; import matplotlib.pyplot as plt 
&gt;&gt;&gt; from sklearn.decomposition import PCA 
&gt;&gt;&gt; from sklearn.datasets import load_digits 
 
&gt;&gt;&gt; digits = load_digits() 
&gt;&gt;&gt; X = digits.data 
&gt;&gt;&gt; y = digits.target 
 
 
&gt;&gt;&gt; print (digits.data[0].reshape(8,8))</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/2153e5c6-a7cd-4825-b7f0-18243c674eee.png" style="width:24.17em;height:9.25em;"/></div>
<p>Plot the graph using the <kbd>plt.show</kbd> function:</p>
<pre><strong>&gt;&gt;&gt; plt.matshow(digits.images[0])  
&gt;&gt;&gt; plt.show() </strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/d5d0f2fd-2576-4511-9ec2-985333812d25.png" style="width:24.92em;height:26.08em;"/></div>
<p>Before performing PCA, it is advisable to perform scaling of input data to eliminate any issues due to different dimensions of the data. For example, while applying PCA on customer data, their salary has larger dimensions than the customer's age. Hence, if we do not put all the variables in a similar dimension, one variable will explain the entire variation rather than its actual impact. In the following code, we have applied scaling on all the columns separately:</p>
<pre><strong>&gt;&gt;&gt; from sklearn.preprocessing import scale 
&gt;&gt;&gt; X_scale = scale(X,axis=0)</strong></pre>
<p>In the following, we have used two principal components, so that we can represent the performance on a 2D graph. In later sections, we have applied 3D as well.</p>
<pre><strong>&gt;&gt;&gt; pca = PCA(n_components=2) 
&gt;&gt;&gt; reduced_X = pca.fit_transform(X_scale) 
 
&gt;&gt;&gt; zero_x, zero_y = [],[] ; one_x, one_y = [],[] 
&gt;&gt;&gt; two_x,two_y = [],[]; three_x, three_y = [],[] 
&gt;&gt;&gt; four_x,four_y = [],[]; five_x,five_y = [],[] 
&gt;&gt;&gt; six_x,six_y = [],[]; seven_x,seven_y = [],[] 
&gt;&gt;&gt; eight_x,eight_y = [],[]; nine_x,nine_y = [],[] </strong></pre>
<p>In the following section of code, we are appending the relevant principal components to each digit separately so that we can create a scatter plot of all 10 digits:</p>
<pre><strong>&gt;&gt;&gt; for i in range(len(reduced_X)): 
...     if y[i] == 0: 
...         zero_x.append(reduced_X[i][0]) 
...         zero_y.append(reduced_X[i][1]) 
         
...     elif y[i] == 1: 
...         one_x.append(reduced_X[i][0]) 
...         one_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 2: 
...         two_x.append(reduced_X[i][0]) 
...         two_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 3: 
...         three_x.append(reduced_X[i][0]) 
...         three_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 4: 
...         four_x.append(reduced_X[i][0]) 
...         four_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 5: 
...         five_x.append(reduced_X[i][0])</strong> 
<strong>...         five_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 6: 
...         six_x.append(reduced_X[i][0]) 
...         six_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 7: 
...         seven_x.append(reduced_X[i][0]) 
...         seven_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 8: 
...         eight_x.append(reduced_X[i][0]) 
...         eight_y.append(reduced_X[i][1]) 
     
...     elif y[i] == 9: 
...         nine_x.append(reduced_X[i][0]) 
...         nine_y.append(reduced_X[i][1]) 
 </strong>
<strong>&gt;&gt;&gt; zero = plt.scatter(zero_x, zero_y, c='r', marker='x',label='zero') 
&gt;&gt;&gt; one = plt.scatter(one_x, one_y, c='g', marker='+') 
&gt;&gt;&gt; two = plt.scatter(two_x, two_y, c='b', marker='s') 
 
&gt;&gt;&gt; three = plt.scatter(three_x, three_y, c='m', marker='*') 
&gt;&gt;&gt; four = plt.scatter(four_x, four_y, c='c', marker='h') 
&gt;&gt;&gt; five = plt.scatter(five_x, five_y, c='r', marker='D') 
 
&gt;&gt;&gt; six = plt.scatter(six_x, six_y, c='y', marker='8') 
&gt;&gt;&gt; seven = plt.scatter(seven_x, seven_y, c='k', marker='*') 
&gt;&gt;&gt; eight = plt.scatter(eight_x, eight_y, c='r', marker='x') 
 
&gt;&gt;&gt; nine = plt.scatter(nine_x, nine_y, c='b', marker='D') 
 
 
&gt;&gt;&gt; plt.legend((zero,one,two,three,four,five,six,seven,eight,nine), 
...            ('zero','one','two','three','four','five','six', 'seven','eight','nine'), 
...            scatterpoints=1, 
...            loc='lower left', 
...            ncol=3, 
...            fontsize=10) 
 
&gt;&gt;&gt; plt.xlabel('PC 1') 
&gt;&gt;&gt; plt.ylabel('PC 2') 
 
&gt;&gt;&gt; plt.show() </strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/e1250b44-8893-4b58-bfef-9856a5144b97.png" style="width:45.33em;height:38.75em;"/></div>
<p>Though the preceding plot seems a bit cluttered, it does provide some idea of how the digits are close to and distant from each other. We get the idea that digits <em>6</em> and <em>8</em> are very similar and digits <em>4</em> and <em>7</em> are very distant from the center group, and so on. However, we should also try with a higher number of PCAs, as, sometimes, we might not be able to represent every variation in two dimensions itself.</p>
<p>In the following code, we have applied three PCAs so that we can get a better view of the data in a 3D space. The procedure is very much similar as with two PCAs, except for creating one extra dimension for each digit (<em>X</em>, <em>Y</em>, and <em>Z</em>).</p>
<pre><strong># 3-Dimensional data 
&gt;&gt;&gt; pca_3d = PCA(n_components=3) 
&gt;&gt;&gt; reduced_X3D = pca_3d.fit_transform(X_scale) 
 
&gt;&gt;&gt; zero_x, zero_y,zero_z = [],[],[] ; one_x, one_y,one_z = [],[],[] 
&gt;&gt;&gt; two_x,two_y,two_z = [],[],[]; three_x, three_y,three_z = [],[],[] 
&gt;&gt;&gt; four_x,four_y,four_z = [],[],[]; five_x,five_y,five_z = [],[],[] 
&gt;&gt;&gt; six_x,six_y,six_z = [],[],[]; seven_x,seven_y,seven_z = [],[],[] 
&gt;&gt;&gt; eight_x,eight_y,eight_z = [],[],[]; nine_x,nine_y,nine_z = [],[],[] 
 
 
&gt;&gt;&gt; for i in range(len(reduced_X3D)): 
     
...     if y[i]==10: 
...         continue  
     
...     elif y[i] == 0: 
...         zero_x.append(reduced_X3D[i][0]) 
...         zero_y.append(reduced_X3D[i][1]) 
...         zero_z.append(reduced_X3D[i][2]) 
         
...     elif y[i] == 1: 
...         one_x.append(reduced_X3D[i][0])</strong> 
<strong>...         one_y.append(reduced_X3D[i][1]) 
...         one_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 2: 
...         two_x.append(reduced_X3D[i][0]) 
...         two_y.append(reduced_X3D[i][1]) 
...         two_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 3: 
...         three_x.append(reduced_X3D[i][0]) 
...         three_y.append(reduced_X3D[i][1]) 
...         three_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 4: 
...         four_x.append(reduced_X3D[i][0]) 
...         four_y.append(reduced_X3D[i][1]) 
...         four_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 5: 
...         five_x.append(reduced_X3D[i][0]) 
...         five_y.append(reduced_X3D[i][1]) 
...         five_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 6: 
...         six_x.append(reduced_X3D[i][0]) 
...         six_y.append(reduced_X3D[i][1]) 
...         six_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 7: 
...         seven_x.append(reduced_X3D[i][0]) 
...         seven_y.append(reduced_X3D[i][1]) 
...         seven_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 8: 
...         eight_x.append(reduced_X3D[i][0]) 
...         eight_y.append(reduced_X3D[i][1]) 
...         eight_z.append(reduced_X3D[i][2]) 
     
...     elif y[i] == 9: 
...         nine_x.append(reduced_X3D[i][0]) 
...         nine_y.append(reduced_X3D[i][1]) 
...         nine_z.append(reduced_X3D[i][2]) 
</strong> 
 
<strong> 
# 3- Dimensional plot 
&gt;&gt;&gt; from mpl_toolkits.mplot3d import Axes3D 
&gt;&gt;&gt; fig = plt.figure() 
&gt;&gt;&gt; ax = fig.add_subplot(111, projection='3d') 
 
&gt;&gt;&gt; ax.scatter(zero_x, zero_y,zero_z, c='r', marker='x',label='zero') 
&gt;&gt;&gt; ax.scatter(one_x, one_y,one_z, c='g', marker='+',label='one') 
&gt;&gt;&gt; ax.scatter(two_x, two_y,two_z, c='b', marker='s',label='two') 
 
&gt;&gt;&gt; ax.scatter(three_x, three_y,three_z, c='m', marker='*',label='three') 
&gt;&gt;&gt; ax.scatter(four_x, four_y,four_z, c='c', marker='h',label='four') 
&gt;&gt;&gt; ax.scatter(five_x, five_y,five_z, c='r', marker='D',label='five') 
 
&gt;&gt;&gt; ax.scatter(six_x, six_y,six_z, c='y', marker='8',label='six') 
&gt;&gt;&gt; ax.scatter(seven_x, seven_y,seven_z, c='k', marker='*',label='seven') 
&gt;&gt;&gt; ax.scatter(eight_x, eight_y,eight_z, c='r', marker='x',label='eight') 
 
&gt;&gt;&gt; ax.scatter(nine_x, nine_y,nine_z, c='b', marker='D',label='nine') 
</strong> 
<strong>&gt;&gt;&gt; ax.set_xlabel('PC 1') 
&gt;&gt;&gt; ax.set_ylabel('PC 2') 
&gt;&gt;&gt; ax.set_zlabel('PC 3')</strong></pre>
<p class="mce-root"/>
<pre><strong>&gt;&gt;&gt; plt.legend(loc='upper left', numpoints=1, ncol=3, fontsize=10, bbox_to_anchor=(0, 0)) 
 
&gt;&gt;&gt; plt.show()</strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/558b7610-9ae7-417c-9a54-af1327c3dcd2.png"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>matplotlib plots have one great advantage over other software plots such as R plot, and so on. They are interactive, which means that we can rotate them and see how they look from various directions. We encourage the reader to observe the plot by rotating and exploring. In a 3D plot, we can see a similar story with more explanation. Digit <em>2</em> is at the extreme left and digit <em>0</em> is at the lower part of the plot. Whereas digit <em>4</em> is at the top-right end, digit <em>6</em> seems to be more towards the <em>PC 1</em> axis. In this way, we can visualize and see how digits are distributed. In the case of 4 PCAs, we need to go for subplots and visualize them separately.</p>
<p>Choosing the number of PCAs to be extracted is an open-ended question in unsupervised learning, but there are some turnarounds to get an approximated view. There are two ways we can determine the number of clusters:</p>
<ul>
<li>Check where the total variance explained is diminishing marginally</li>
<li>Total variance explained greater than 80 percent</li>
</ul>
<p>The following code does provide the total variance explained with the change in number of principal components. With the more number of PCs, more variance will be explained. But however, the challenge is to restrict it as fewer PCs possible, this will be achieved by restricting where the marginal increase in variance explained start diminishes.</p>
<pre><strong># Choosing number of Principal Components 
&gt;&gt;&gt; max_pc = 30 
 
&gt;&gt;&gt; pcs = [] 
&gt;&gt;&gt; totexp_var = [] 
 
&gt;&gt;&gt; for i in range(max_pc): 
...     pca = PCA(n_components=i+1) 
...     reduced_X = pca.fit_transform(X_scale) 
...     tot_var = pca.explained_variance_ratio_.sum() 
...     pcs.append(i+1) 
...     totexp_var.append(tot_var) 
 
&gt;&gt;&gt; plt.plot(pcs,totexp_var,'r') 
&gt;&gt;&gt; plt.plot(pcs,totexp_var,'bs') 
&gt;&gt;&gt; plt.xlabel('No. of PCs',fontsize = 13) 
&gt;&gt;&gt; plt.ylabel('Total variance explained',fontsize = 13)</strong> </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<pre><strong>&gt;&gt;&gt; plt.xticks(pcs,fontsize=13) 
&gt;&gt;&gt; plt.yticks(fontsize=13) 
&gt;&gt;&gt; plt.show() </strong>
 </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/3cb54060-0a40-42a4-8245-5a788f0c293b.png"/></div>
<p>From the previous plot, we can see that total variance explained diminishes marginally at 10 PCAs; whereas, total variance explained greater than 80 percent is given at 21 PCAs. It is up to the business and user which value to choose.</p>
<p>The R code for PCA applied on handwritten digits data is as follows:</p>
<pre><strong># PCA   
digits_data = read.csv("digitsdata.csv")   
   
remove_cols = c("target")   
x_data =   digits_data[,!(names(digits_data) %in% remove_cols)]   
y_data = digits_data[,c("target")]   
   
# Normalizing the data   
normalize &lt;- function(x)   {return((x - min(x)) / (max(x) - min(x)))}   
data_norm &lt;-   as.data.frame(lapply(x_data, normalize))   
data_norm &lt;-   replace(data_norm, is.na(data_norm), 0.0)   
   
   
# Extracting Principal   Components   
pr_out =prcomp(data_norm)   
pr_components_all = pr_out$x   
   
# 2- Dimensional PCA   
K_prcomps = 2   
pr_components =   pr_components_all[,1:K_prcomps]   
   
pr_components_df =   data.frame(pr_components)   
pr_components_df =   cbind(pr_components_df,digits_data$target)   
names(pr_components_df)[K_prcomps+1]   = "target"   
   
out &lt;- split(   pr_components_df , f = pr_components_df$target )   
zero_df = out$`0`;one_df =   out$`1`;two_df = out$`2`; three_df = out$`3`; four_df = out$`4`   
five_df = out$`5`;six_df =   out$`6`;seven_df = out$`7`;eight_df = out$`8`;nine_df = out$`9`   
   
library(ggplot2)   
# Plotting 2-dimensional PCA   
ggplot(pr_components_df, aes(x   = PC1, y = PC2, color = factor(target,labels = c("zero","one","two",   "three","four", "five","six","seven","eight","nine"))))   +    
geom_point()+ggtitle("2-D   PCA on Digits Data") +   
labs(color = "Digtis")   
   
# 3- Dimensional PCA   
# Plotting 3-dimensional PCA   
K_prcomps = 3   
  </strong> 
<strong>pr_components =   pr_components_all[,1:K_prcomps]   
pr_components_df =   data.frame(pr_components)   
pr_components_df =   cbind(pr_components_df,digits_data$target)   
names(pr_components_df)[K_prcomps+1]   = "target"   
   
pr_components_df$target =   as.factor(pr_components_df$target)   
   
out &lt;- split(   pr_components_df , f = pr_components_df$target )   
zero_df = out$`0`;one_df =   out$`1`;two_df = out$`2`; three_df = out$`3`; four_df = out$`4`   
five_df = out$`5`;six_df =   out$`6`;seven_df = out$`7`;eight_df = out$`8`;nine_df = out$`9`   
   
library(scatterplot3d)   
colors &lt;- c("darkred",   "darkseagreen4", "deeppink4", "greenyellow", "orange",   "navyblue", "red", "tan3", "steelblue1",   "slateblue")   
colors &lt;- colors[as.numeric(pr_components_df$target)]   
s3d =   scatterplot3d(pr_components_df[,1:3], pch = 16, color=colors,   
xlab = "PC1",ylab = "PC2",zlab   = "PC3",col.grid="lightblue",main = "3-D PCA on   Digits Data")   
legend(s3d$xyz.convert(3.1,   0.1, -3.5), pch = 16, yjust=0,   
       legend =   levels(pr_components_df$target),col =colors,cex = 1.1,xjust = 0)   
          
# Choosing number of Principal   Components   
pr_var =pr_out$sdev ^2   
pr_totvar = pr_var/sum(pr_var)   
plot(cumsum(pr_totvar), xlab="Principal   Component", ylab ="Cumilative Prop. of Var.",   ylim=c(0,1),type="b",main = "PCAs vs. Cum prop of Var   Explained")   </strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Singular value decomposition - SVD</h1>
                </header>
            
            <article>
                
<p>Many implementations of PCA use singular value decomposition to calculate eigenvectors and eigenvalues. SVD is given by the following equation:</p>
<div style="padding-left: 210px" class="CDPAlignLeft CDPAlign"><img src="assets/2bad9800-489d-425d-abdf-8e81028dabbd.jpg" style="width:6.67em;height:1.67em;"/></div>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/34c6d790-5379-452b-a386-e8fd4a2b1359.jpg" style="width:30.25em;height:1.75em;"/></div>
<p>Columns of <em>U</em> are called left singular vectors of the data matrix, the columns of <em>V</em> are its right singular vectors, and the diagonal entries of <img src="assets/d869b053-93bb-4e03-9a0d-52edbde6c63e.png" style="width:0.67em;height:1.25em;"/> are its singular values. Left singular vectors are the eigenvectors of the covariance matrix and the diagonal element of <img src="assets/c808febf-3aac-4e39-bf3d-de049bfe7bb3.png" style="width:0.67em;height:1.17em;"/> are the square roots of the eigenvalues of the covariance matrix.</p>
<p>Before proceeding with SVD, it would be advisable to understand a few advantages and important points about SVD:</p>
<ul>
<li>SVD can be applied even on rectangular matrices; whereas, eigenvalues are defined only for square matrices. The equivalent of eigenvalues obtained through the SVD method are called singular values, and vectors obtained equivalent to eigenvectors are known as singular vectors. However, as they are rectangular in nature, we need to have left singular vectors and right singular vectors respectively for their dimensions.</li>
<li>If a matrix <em>A</em> has a matrix of eigenvectors <em>P</em> that is not invertible, then <em>A</em> does not have an eigen decomposition. However, if <em>A</em> is <em>m</em> x <em>n</em> real matrix with <em>m</em> &gt; <em>n</em>, then A can be written using a singular value decomposition.</li>
<li>Both <em>U</em> and <em>V</em> are orthogonal matrices, which means <em>U<sup>T</sup> U = I</em> (<em>I</em> with <em>m</em> x <em>m</em> dimension) or <em>V<sup>T</sup> V = I</em> (here <em>I</em> with <em>n</em> x <em>n</em> dimension), where two identity matrices may have different dimensions.</li>
<li><img src="assets/beca8756-97e1-46e6-b726-4bb3bcc97c1a.png" style="width:0.67em;height:1.17em;"/>is a non-negative diagonal matrix with <em>m</em> x <em>n</em> dimensions.</li>
</ul>
<p>Then computation of singular values and singular vectors is done with the following set of equations:</p>
<div style="padding-left: 240px" class="CDPAlignLeft CDPAlign"><img src="assets/d7d80ce1-8853-409b-b9fa-b5743561ba83.jpg" style="width:8.83em;height:1.75em;"/></div>
<p>In the first stage, singular values/eigenvalues are calculated with the equation. Once we obtain the singular/eigenvalues, we will substitute to determine the <em>V</em> or right singular/eigen vectors:</p>
<div style="padding-left: 240px" class="CDPAlignLeft CDPAlign"><img src="assets/bbdd9880-81f0-4372-a22a-f2bd4930c904.jpg" style="width:10.33em;height:1.25em;"/></div>
<p>Once we obtain the right singular vectors and diagonal values, we will substitute to obtain the left singular vectors <em>U</em> using the equation mentioned as follows:</p>
<div style="padding-left: 270px" class="CDPAlignLeft CDPAlign"><img src="assets/585fb802-b3e1-4703-8e0b-ce0835889d61.jpg" style="width:5.67em;height:1.33em;"/></div>
<p>In this way, we will calculate the singular value decompositions of the original system of equations matrix.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SVD applied on handwritten digits using scikit-learn</h1>
                </header>
            
            <article>
                
<p>SVD can be applied on the same handwritten digits data to perform an apple-to-apple comparison of techniques.</p>
<pre><strong># SVD 
&gt;&gt;&gt; import matplotlib.pyplot as plt 
&gt;&gt;&gt; from sklearn.datasets import load_digits 
 
&gt;&gt;&gt; digits = load_digits() 
&gt;&gt;&gt; X = digits.data 
&gt;&gt;&gt; y = digits.target</strong> </pre>
<p>In the following code, 15 singular vectors with 300 iterations are used, but we encourage the reader to change the values and check the performance of SVD. We have used two types of SVD functions, as a function <kbd>randomized_svd</kbd> provide the decomposition of the original matrix and a <kbd>TruncatedSVD</kbd> can provide total variance explained ratio. In practice, uses may not need to view all the decompositions and they can just use the <kbd>TruncatedSVD</kbd> function for their practical purposes.</p>
<pre><strong>&gt;&gt;&gt; from sklearn.utils.extmath import randomized_svd 
&gt;&gt;&gt; U,Sigma,VT = randomized_svd(X,n_components=15,n_iter=300,random_state=42) 
 
&gt;&gt;&gt; import pandas as pd 
&gt;&gt;&gt; VT_df = pd.DataFrame(VT) 
 
&gt;&gt;&gt; print ("\nShape of Original Matrix:",X.shape) 
&gt;&gt;&gt; print ("\nShape of Left Singular vector:",U.shape) 
&gt;&gt;&gt; print ("Shape of Singular value:",Sigma.shape) 
&gt;&gt;&gt; print ("Shape of Right Singular vector",VT.shape)</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/b74ae632-a243-4362-b0fd-a1f652afd617.png"/></div>
<p>By looking at the previous screenshot, we can see that the original matrix of dimension (1797 x 64) has been decomposed into a left singular vector (1797 x 15), singular value (diagonal matrix of 15), and right singular vector (15 x 64). We can obtain the original matrix by multiplying all three matrices in order.</p>
<pre><strong>&gt;&gt;&gt; n_comps = 15 
&gt;&gt;&gt; from sklearn.decomposition import TruncatedSVD 
&gt;&gt;&gt; svd = TruncatedSVD(n_components=n_comps, n_iter=300, random_state=42) 
&gt;&gt;&gt; reduced_X = svd.fit_transform(X) 
 
&gt;&gt;&gt; print("\nTotal Variance explained for %d singular features are %0.3f"%(n_comps, svd.explained_variance_ratio_.sum()))  </strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/63a4fdf0-fb70-4097-b069-33fa26fdefa5.png"/></div>
<p>The total variance explained for 15 singular value features is 83.4 percent. But the reader needs to change the different values to decide the optimum value.</p>
<p>The following code illustrates the change in total variance explained with respective to change in number of singular values:</p>
<pre><strong># Choosing number of Singular Values 
&gt;&gt;&gt; max_singfeat = 30 
&gt;&gt;&gt; singfeats = [] 
&gt;&gt;&gt; totexp_var = [] 
 
&gt;&gt;&gt; for i in range(max_singfeat): 
...     svd = TruncatedSVD(n_components=i+1, n_iter=300, random_state=42) 
...     reduced_X = svd.fit_transform(X) 
...     tot_var = svd.explained_variance_ratio_.sum() 
...     singfeats.append(i+1) 
...     totexp_var.append(tot_var) 
 
&gt;&gt;&gt; plt.plot(singfeats,totexp_var,'r') 
&gt;&gt;&gt; plt.plot(singfeats,totexp_var,'bs') 
&gt;&gt;&gt; plt.xlabel('No. of Features',fontsize = 13) 
&gt;&gt;&gt; plt.ylabel('Total variance explained',fontsize = 13) 
 
&gt;&gt;&gt; plt.xticks(pcs,fontsize=13) 
&gt;&gt;&gt; plt.yticks(fontsize=13) 
&gt;&gt;&gt; plt.show()</strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/c77ef1df-2649-4377-8f21-a37a43d75474.png" style="width:33.33em;height:27.00em;"/></div>
<p>From the previous plot, we can choose either 8 or 15 singular vectors based on the requirement.</p>
<p>The R code for SVD applied on handwritten digits data is as follows:</p>
<pre><strong>#SVD    
library(svd)   
   
digits_data = read.csv("digitsdata.csv")   
   
remove_cols = c("target")   
x_data =   digits_data[,!(names(digits_data) %in% remove_cols)]   
y_data = digits_data[,c("target")]   
   
sv2 &lt;- svd(x_data,nu=15)   
   
# Computing the square of the   singular values, which can be thought of as the vector of matrix energy   
# in order to pick top singular   values which preserve at least 80% of variance explained   
energy &lt;- sv2$d ^ 2   
tot_varexp = data.frame(cumsum(energy)   / sum(energy))   
   
names(tot_varexp) = "cum_var_explained"   
tot_varexp$K_value =   1:nrow(tot_varexp)   
   
plot(tot_varexp[,2],tot_varexp[,1],type   = 'o',xlab = "K_Value",ylab = "Prop. of Var Explained")   
title("SVD - Prop. of Var   explained with K-value")   </strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep auto encoders</h1>
                </header>
            
            <article>
                
<p>The auto encoder neural network is an unsupervised learning algorithm that applies backpropagation setting the target values to be equal to the inputs <em>y<sup>(i)</sup> = x<sup>(i)</sup></em>. Auto encoder tries to learn a function <em>h<sub>w,b</sub>(x) ≈ x</em>, means it tries to learn an approximation to the identity function, so that output  <img src="assets/f7e07b21-0c88-4e4f-8778-a576c73e30c6.jpg" style="width:1.08em;height:1.33em;"/>  that is similar to <em>x</em>.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/3a9a491a-e5af-49d7-b618-eb7f46b02642.png" style="width:33.75em;height:35.58em;"/></div>
<p>Though trying to replicate the identity function seems trivial function to learn, by placing the constraints on the network, such as by limiting number of hidden units, we can discover interesting structures about the data. Let's say input picture of size 10 x 10 pixels has intensity values which have, altogether, 100 input values, the number of neurons in the second layer (hidden layer) is 50 units, and the output layer, finally, has 100 units of neurons as we need to pass the image to map it to itself and while achieving this representation in the process we would force the network to learn a compressed representation of the input, which is hidden unit activations <em>a<sup>(2)</sup> ε  R<sup>100</sup></em>, with which we must try to reconstruct the 100 pixel input <em>x</em>. If the input data is completely random without any correlations, and so on. it would be very difficult to compress, whereas if the underlying data have some correlations or detectable structures, then this algorithm will be able to discover the correlations and represent them compactly. In fact, auto encoder often ends up learning a low-dimensional representation very similar to PCAs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model building technique using encoder-decoder architecture</h1>
                </header>
            
            <article>
                
<p>Training the auto encoder model is a bit tricky, hence a detailed illustration has been provided for better understanding. During the training phase, the whole encoder-decoder section is trained against the same input as an output of decoder. In order to achieve the desired output, features will be compressed during the middle layer, as we are passing through the convergent and divergent layers. Once enough training has been done by reducing the error values over the number of iterations, we will use the trained encoder section to create the latent features for next stage of modeling, or for visualization, and so on.</p>
<p>In the following diagram, a sample has been shown. The input and output layers have five neurons, whereas the number of neurons has been gradually decreased in the middle sections. The compressed layer has only two neurons, which is the number of latent dimensions we would like to extract from the data.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/bfec2005-fe5a-4f24-a0f8-9899219bdb98.png" style="width:41.58em;height:20.67em;"/></div>
<p>The following diagram depicts using the trained encoder section to create latent features from the new input data, which will be utilized for visualization or for utilizing in the next stage of the model:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/410aa78f-a964-4567-82ee-c944940b6f2b.png" style="width:34.25em;height:19.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep auto encoders applied on handwritten digits using Keras</h1>
                </header>
            
            <article>
                
<p>Deep auto encoders are explained with same handwritten digits data to show the comparison of how this non-linear method differs to linear methods like PCA and SVD. Non-linear methods generally perform much better, but these methods are kind of black-box models and we cannot determine the explanation behind that. Keras software has been utilized to build the deep auto encoders here, as they work like Lego blocks, which makes it easy for users to play around with different architectures and parameters of the model for better understanding:</p>
<pre><strong># Deep Auto Encoders 
&gt;&gt;&gt; import matplotlib.pyplot as plt 
&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler 
&gt;&gt;&gt; from sklearn.datasets import load_digits 
 
&gt;&gt;&gt; digits = load_digits() 
&gt;&gt;&gt; X = digits.data 
&gt;&gt;&gt; y = digits.target 
 
&gt;&gt;&gt; print (X.shape) 
&gt;&gt;&gt; print (y.shape) 
&gt;&gt;&gt; x_vars_stdscle = StandardScaler().fit_transform(X) 
&gt;&gt;&gt; print (x_vars_stdscle.shape) </strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/78b1f846-5712-45a8-b1d1-1c4e16682e72.png" style="width:7.08em;height:3.58em;"/></div>
<p>Dense neuron modules from Keras used for constructing encoder-decoder architecture:</p>
<pre><strong>&gt;&gt;&gt; from keras.layers import Input,Dense 
&gt;&gt;&gt; from keras.models import Model</strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/73cfd2fa-5076-4ff6-85cd-5c5ab85ef714.png"/></div>
<p>GPU of NVIDIA GTX 1060 has been used here, also <kbd>cuDNN</kbd> and <kbd>CNMeM</kbd> libraries are installed for further enhancement of speed up to 4x-5x on the top of regular GPU performance. These libraries utilize 20 percent of GPU memory, which left the 80 percent of memory for working on the data. The user needs to be careful, if they have low memory GPUs like 3 GB to 4 GB, they may not be able to utilize these libraries.</p>
<div class="packt_infobox">The reader needs to consider one important point that, syntax of Keras code, will remain same in both CPU and GPU mode.</div>
<p>The following few lines of codes are the heart of the model. Input data have 64 columns. We need to take those columns into the input of the layers, hence we have given the shape of 64. Also, names have been assigned to each layer of the neural network, which we will explain the reason for in an upcoming section of the code. In the first hidden layer, 32 dense neurons are utilized, which means all the 64 inputs from the input layer are connected to 32 neurons in first hidden layer. The entire flow of dimensions are like <em>64, 32, 16, 2, 16, 32, 64</em>. We have compressed input to two neurons, in order to plot the components on a 2D plot, whereas, if we need to plot a 3D data (which we will be covering in the next section), we need to change the hidden three-layer number to three instead of two. After training is complete, we need to use encoder section and predict the output.</p>
<pre><strong># 2-Dimensional Architecture 
 
&gt;&gt;&gt; input_layer = Input(shape=(64,),name="input") 
 
&gt;&gt;&gt; encoded = Dense(32, activation='relu',name="h1encode")(input_layer) 
&gt;&gt;&gt; encoded = Dense(16, activation='relu',name="h2encode")(encoded) 
&gt;&gt;&gt; encoded = Dense(2, activation='relu',name="h3latent_layer")(encoded) 
 
&gt;&gt;&gt; decoded = Dense(16, activation='relu',name="h4decode")(encoded) 
&gt;&gt;&gt; decoded = Dense(32, activation='relu',name="h5decode")(decoded) 
&gt;&gt;&gt; decoded = Dense(64, activation='sigmoid',name="h6decode")(decoded) <br/></strong></pre>
<p>To train the model, we need to pass the starting and ending point of the architecture. In the following code, we have provided input as <kbd>input_layer</kbd> and output as <kbd>decoded</kbd>, which is the last layer (the name is <kbd>h6decode</kbd>):</p>
<pre><strong>&gt;&gt;&gt; autoencoder = Model(input_layer, decoded)</strong> </pre>
<p>Adam optimization has been used to optimize the mean square error, as we wanted to reproduce the original input at the end of the output layer of the network:</p>
<pre><strong>&gt;&gt;&gt; autoencoder.compile(optimizer="adam", loss="mse")</strong> </pre>
<p>The network is trained with 100 epochs and a batch size of 256 observations per each batch. Validation split of 20 percent is used to check the accuracy on randomly selected validation data in order to ensure robustness, as if we just train only on the train data may create the overfitting problem, which is very common with highly non-linear models:</p>
<pre><strong># Fitting Encoder-Decoder model 
&gt;&gt;&gt; autoencoder.fit(x_vars_stdscle, x_vars_stdscle, epochs=100,batch_size=256, shuffle=True,validation_split= 0.2 )</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/72ac3852-820e-495d-a7a8-08d87908150e.png"/></div>
<p>From the previous results, we can see that the model has been trained on 1,437 train examples and validation on 360 examples. By looking into the loss value, both train and validation losses have decreased from 1.2314 to 0.9361 and 1.0451 to 0.7326 respectively. Hence, we are moving in the right direction. However, readers are encouraged to try various architectures and number of iterations, batch sizes, and so on to see how much the accuracies can be further improved.</p>
<p>Once the encoder-decoder section has been trained, we need to take only the encoder section to compress the input features in order to obtain the compressed latent features, which is the core idea of dimensionality reduction altogether! In the following code, we have constructed another model with a trained input layer and a middle hidden layer (<kbd>h3latent_layer</kbd>). This is the reason behind assigning names for each layer of the network.</p>
<pre><strong># Extracting Encoder section of the Model for prediction of latent variables 
&gt;&gt;&gt; encoder = Model(autoencoder.input,autoencoder.get_layer("h3latent_layer").output) 
 
Extracted encoder section of the whole model used for prediction of input variables to generate sparse 2-dimensional representation, which is being performed with the following code 
  
# Predicting latent variables with extracted Encoder model 
&gt;&gt;&gt; reduced_X = encoder.predict(x_vars_stdscle) </strong> </pre>
<p>Just to check the dimensions of the reduced input variables and we can see that for all observations, we can see two dimensions or two column vector:</p>
<pre><strong> &gt;&gt;&gt; print (reduced_X.shape)</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/d340c571-4e8e-4dc5-a16e-c8fda6499fe3.png"/></div>
<p>The following section of the code is very much similar to 2D PCA:</p>
<pre><strong>&gt;&gt;&gt; zero_x, zero_y = [],[] ; one_x, one_y = [],[] 
&gt;&gt;&gt; two_x,two_y = [],[]; three_x, three_y = [],[] 
&gt;&gt;&gt; four_x,four_y = [],[]; five_x,five_y = [],[] 
&gt;&gt;&gt; six_x,six_y = [],[]; seven_x,seven_y = [],[] 
&gt;&gt;&gt; eight_x,eight_y = [],[]; nine_x,nine_y = [],[] 
 
# For 2-Dimensional data 
&gt;&gt;&gt; for i in range(len(reduced_X)): 
...     if y[i] == 0: 
...         zero_x.append(reduced_X[i][0]) 
...         zero_y.append(reduced_X[i][1]) 
         
...     elif y[i] == 1: 
...         one_x.append(reduced_X[i][0]) 
...         one_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 2: 
...         two_x.append(reduced_X[i][0]) 
...         two_y.append(reduced_X[i][1])</strong> 
<strong> 
...     elif y[i] == 3: 
...         three_x.append(reduced_X[i][0]) 
...         three_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 4: 
...         four_x.append(reduced_X[i][0]) 
...         four_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 5: 
...         five_x.append(reduced_X[i][0]) 
...         five_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 6: 
...         six_x.append(reduced_X[i][0]) 
...         six_y.append(reduced_X[i][1])</strong> 
 
<strong>...     elif y[i] == 7: 
...         seven_x.append(reduced_X[i][0]) 
...         seven_y.append(reduced_X[i][1]) 
 
...     elif y[i] == 8: 
...         eight_x.append(reduced_X[i][0]) 
 ...        eight_y.append(reduced_X[i][1]) 
     
 ...    elif y[i] == 9: 
 ...        nine_x.append(reduced_X[i][0]) 
 ...        nine_y.append(reduced_X[i][1]) 
 
 
&gt;&gt;&gt; zero = plt.scatter(zero_x, zero_y, c='r', marker='x',label='zero') 
&gt;&gt;&gt; one = plt.scatter(one_x, one_y, c='g', marker='+') 
&gt;&gt;&gt; two = plt.scatter(two_x, two_y, c='b', marker='s') 
 
&gt;&gt;&gt; three = plt.scatter(three_x, three_y, c='m', marker='*') 
&gt;&gt;&gt; four = plt.scatter(four_x, four_y, c='c', marker='h') 
&gt;&gt;&gt; five = plt.scatter(five_x, five_y, c='r', marker='D') 
 
&gt;&gt;&gt; six = plt.scatter(six_x, six_y, c='y', marker='8') 
&gt;&gt;&gt; seven = plt.scatter(seven_x, seven_y, c='k', marker='*') 
&gt;&gt;&gt; eight = plt.scatter(eight_x, eight_y, c='r', marker='x') 
 
&gt;&gt;&gt; nine = plt.scatter(nine_x, nine_y, c='b', marker='D') 
 
 
&gt;&gt;&gt; plt.legend((zero,one,two,three,four,five,six,seven,eight,nine), 
...  ('zero','one','two','three','four','five','six','seven','eight','nine'),</strong> 
<strong>...            scatterpoints=1,loc='lower right',ncol=3,fontsize=10) 
 
&gt;&gt;&gt; plt.xlabel('Latent Feature 1',fontsize = 13) 
&gt;&gt;&gt; plt.ylabel('Latent Feature 2',fontsize = 13) 
 
&gt;&gt;&gt; plt.show()</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/969f6b4b-0500-4d46-ae3f-4d363cd134b3.png"/></div>
<p>From the previous plot we can see that data points are well separated, but the issue is the direction of view, as these features do not vary as per the dimensions perpendicular to each other, similar to principal components, which are orthogonal to each other. In the case of deep auto encoders, we need to change the view of direction from the <em>(0, 0)</em> to visualize this non-linear classification, which we will see in detail in the following 3D case.</p>
<p>The following is the code for 3D latent features. All the code remains the same apart from the <kbd>h3latent_layer</kbd>, in which we have to replace the value from <kbd>2</kbd> to <kbd>3</kbd>, as this is the end of encoder section and we will utilize it in creating the latent features and, eventually, it will be used for plotting purposes.</p>
<pre><strong># 3-Dimensional architecture 
&gt;&gt;&gt; input_layer = Input(shape=(64,),name="input") 
 
&gt;&gt;&gt; encoded = Dense(32, activation='relu',name="h1encode")(input_layer) 
&gt;&gt;&gt; encoded = Dense(16, activation='relu',name="h2encode")(encoded) 
&gt;&gt;&gt; encoded = Dense(3, activation='relu',name="h3latent_layer")(encoded) 
 
&gt;&gt;&gt; decoded = Dense(16, activation='relu',name="h4decode")(encoded) 
&gt;&gt;&gt; decoded = Dense(32, activation='relu',name="h5decode")(decoded) 
&gt;&gt;&gt; decoded = Dense(64, activation='sigmoid',name="h6decode")(decoded) 
 
&gt;&gt;&gt; autoencoder = Model(input_layer, decoded) 
autoencoder.compile(optimizer="adam", loss="mse") 
 
# Fitting Encoder-Decoder model 
&gt;&gt;&gt; autoencoder.fit(x_vars_stdscle, x_vars_stdscle, epochs=100,batch_size=256, shuffle=True,validation_split= 0.2) </strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/336bf646-4ee2-4721-8e45-ef4a33e53a74.png"/></div>
<p>From the previous results we can see that, with the inclusion of three dimensions instead of two, loss values obtained are less than in the 2D use case. Train and validation losses for two latent factors after 100 epochs are 0.9061 and 0.7326, and for three latent factors after 100 epochs, are 0.8032 and 0.6464. This signifies that, with the inclusion of one more dimension, we can reduce the errors significantly. This way, the reader can change various parameters to determine the ideal architecture for dimensionality reduction:</p>
<pre><strong># Extracting Encoder section of the Model for prediction of latent variables 
&gt;&gt;&gt; encoder = Model(autoencoder.input,autoencoder.get_layer("h3latent_layer").output) 
 
# Predicting latent variables with extracted Encoder model 
&gt;&gt;&gt; reduced_X3D = encoder.predict(x_vars_stdscle) 
 
&gt;&gt;&gt; zero_x, zero_y,zero_z = [],[],[] ; one_x, one_y,one_z = [],[],[] 
&gt;&gt;&gt; two_x,two_y,two_z = [],[],[]; three_x, three_y,three_z = [],[],[] 
&gt;&gt;&gt; four_x,four_y,four_z = [],[],[]; five_x,five_y,five_z = [],[],[] 
&gt;&gt;&gt; six_x,six_y,six_z = [],[],[]; seven_x,seven_y,seven_z = [],[],[] 
&gt;&gt;&gt; eight_x,eight_y,eight_z = [],[],[]; nine_x,nine_y,nine_z = [],[],[] 
 
&gt;&gt;&gt; for i in range(len(reduced_X3D)): 
     
...     if y[i]==10: 
...         continue 
    </strong> 
<strong>...     elif y[i] == 0: 
...         zero_x.append(reduced_X3D[i][0]) 
...         zero_y.append(reduced_X3D[i][1]) 
...         zero_z.append(reduced_X3D[i][2]) 
         
...     elif y[i] == 1: 
...         one_x.append(reduced_X3D[i][0]) 
...         one_y.append(reduced_X3D[i][1]) 
...         one_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 2: 
...         two_x.append(reduced_X3D[i][0]) 
...         two_y.append(reduced_X3D[i][1]) 
...         two_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 3: 
...         three_x.append(reduced_X3D[i][0]) 
...         three_y.append(reduced_X3D[i][1]) 
...         three_z.append(reduced_X3D[i][2]) 
</strong> 
<strong>...     elif y[i] == 4: 
...         four_x.append(reduced_X3D[i][0]) 
...         four_y.append(reduced_X3D[i][1]) 
...         four_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 5: 
...         five_x.append(reduced_X3D[i][0]) 
...         five_y.append(reduced_X3D[i][1]) 
...         five_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 6: 
...         six_x.append(reduced_X3D[i][0]) 
...         six_y.append(reduced_X3D[i][1]) 
...         six_z.append(reduced_X3D[i][2]) 
 
...     elif y[i] == 7: 
...         seven_x.append(reduced_X3D[i][0]) 
...         seven_y.append(reduced_X3D[i][1]) 
...         seven_z.append(reduced_X3D[i][2]) 
</strong> 
<strong>...     elif y[i] == 8: 
...         eight_x.append(reduced_X3D[i][0]) 
...         eight_y.append(reduced_X3D[i][1]) 
...         eight_z.append(reduced_X3D[i][2]) 
     
...     elif y[i] == 9: 
...         nine_x.append(reduced_X3D[i][0]) 
...         nine_y.append(reduced_X3D[i][1]) 
...         nine_z.append(reduced_X3D[i][2]) 
 
</strong> 
<strong> 
# 3- Dimensional plot 
&gt;&gt;&gt; from mpl_toolkits.mplot3d import Axes3D 
&gt;&gt;&gt; fig = plt.figure() 
&gt;&gt;&gt; ax = fig.add_subplot(111, projection='3d') 
 
&gt;&gt;&gt; ax.scatter(zero_x, zero_y,zero_z, c='r', marker='x',label='zero') 
&gt;&gt;&gt; ax.scatter(one_x, one_y,one_z, c='g', marker='+',label='one') 
&gt;&gt;&gt; ax.scatter(two_x, two_y,two_z, c='b', marker='s',label='two') 
 
&gt;&gt;&gt; ax.scatter(three_x, three_y,three_z, c='m', marker='*',label='three') 
&gt;&gt;&gt; ax.scatter(four_x, four_y,four_z, c='c', marker='h',label='four') 
&gt;&gt;&gt; ax.scatter(five_x, five_y,five_z, c='r', marker='D',label='five') 
 
&gt;&gt;&gt; ax.scatter(six_x, six_y,six_z, c='y', marker='8',label='six') 
&gt;&gt;&gt; ax.scatter(seven_x, seven_y,seven_z, c='k', marker='*',label='seven') 
&gt;&gt;&gt; ax.scatter(eight_x, eight_y,eight_z, c='r', marker='x',label='eight')</strong> 
<strong> 
&gt;&gt;&gt; ax.scatter(nine_x, nine_y,nine_z, c='b', marker='D',label='nine') 
 
&gt;&gt;&gt; ax.set_xlabel('Latent Feature 1',fontsize = 13) 
&gt;&gt;&gt; ax.set_ylabel('Latent Feature 2',fontsize = 13) 
&gt;&gt;&gt; ax.set_zlabel('Latent Feature 3',fontsize = 13) 
 
&gt;&gt;&gt; ax.set_xlim3d(0,60) 
 
&gt;&gt;&gt; plt.legend(loc='upper left', numpoints=1, ncol=3, fontsize=10, bbox_to_anchor=(0, 0)) 
 
&gt;&gt;&gt; plt.show() </strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/d9c4ebd5-4049-4312-b0ad-120b367d964e.png" style="width:43.75em;height:43.17em;"/></div>
<p>3D plots from deep auto encoders do provide well separated classification compared with three PCAs. Here we have got better separation of the digits. One important point the reader should consider here is that the above plot is the rotated view from <em>(0, 0, 0)</em>, as data separation does not happen across orthogonal planes (like PCAs), hence we need to see the view from origin in order to see this non-linear classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you have learned about various unsupervised learning methods to identify the structures and patterns within the data using k-mean clustering, PCA, SVD and deep auto encoders. Also, the k-means clustering algorithm explained with iris data. Methods were shown on how to choose the optimal k-value based on various performance metrics. Handwritten data from scikit-learn was been utilized to compare the differences between linear methods like PCA and SVD with non-linear techniques and deep auto encoders. The differences between PCA and SVD were given in detail so that the reader can understand SVD, which can be applied even on rectangular matrices where the number of users and number of products is not necessarily equal. In the end, through visualization, it has been proven that deep auto encoders are better at separating digits than linear unsupervised learning methods like PCA and SVD.</p>
<p>In the next chapter, we will be discussing various reinforcement learning methods and their utilities in artificial intelligence and so on.</p>


            </article>

            
        </section>
    </body></html>