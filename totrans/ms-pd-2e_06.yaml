- en: I/Os of Different Data Formats with pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A data scientist has to work on data that comes from a variety of sources and
    hence in a variety of formats. The most common are the ubiquitous spreadsheets,
    Excel sheets, and `CSV` and text files. But there are many others, such as `URL`, `API`, `JSON`, `XML`, `HDF`, `Feather` and
    so on, depending on where it is being accessed. In this chapter, we will cover
    the following topics among others:'
  prefs: []
  type: TYPE_NORMAL
- en: Data sources and pandas methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSV and TXT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL and S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading HDF formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Data sources and pandas methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data sources for a data science project can be clubbed into the following
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Databases**: Most of the CRM, ERP, and other business operations tools store
    data in a database. Depending on the volume, velocity, and variety, it can be
    a traditional or NoSQL database. To connect to most of the popular databases,
    we need `JDBC/ODBC` drivers from Python. Fortunately, there are such drivers available
    for all the popular databases. Working with data in such databases involves making
    a connection through Python to these databases, querying the data through Python,
    and then manipulating it using pandas. We will look at an example of how to do
    this later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Web services**: Many of the business operations tools, especially **Software
    as a Services** (**SaaS**) tools, make their data accessible through **Application
    Programming Interfaces** (**APIs**) instead of a database. This reduces the infrastructure
    cost of hosting a database permanently. Instead, data is made available as a service,
    as and when required. An `API` call can be made through Python, which returns
    packets of data in formats such as `JSON` or `XML`. This data is parsed and then
    manipulated using pandas for further usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data files**: A lot of data for prototyping data science models comes as
    data files. One example of data being stored as a physical file is the data from
    IoT sensors – more often than not, the data from these sensors is stored in a
    flat file, a `.txt` file, or a `.csv` file. Another source for a data file is
    the sample data that''s been extracted from a database and stored in such files.
    The output of many data science and machine learning algorithms are also stored
    in such files, such as CSV, Excel, and `.txt` files. Another example is that the
    trained weight matrices of a deep learning neural network model can be stored
    as an HDF file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Web and document scraping**: Two other sources of data are the tables and
    text present on web pages. This data is gleaned from these pages using Python
    packages such as BeautifulSoup and Scrapy and are put into a data file or database
    to be used further. The tables and data that are present in another non-data format
    file, such as PDF or Docs, are also a major source of data. This is then extracted
    using Python packages such as Tesseract and Tabula-py.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will look at how to read and write data to and from these
    formats/sources using pandas and ancillary libraries. We will also discuss a little
    bit about these formats, their utilities, and various operations that can be performed
    on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a summary of the read and write methods in Python for some
    of the data formats we are going to discuss in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8abf637c-9d15-45c5-b8e7-fdef295dd9d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Reader and writer methods in pandas for different types of data file formats
    and their sources
  prefs: []
  type: TYPE_NORMAL
- en: The section headers mean that we are dealing with I/O operations of that file
    type in that section.
  prefs: []
  type: TYPE_NORMAL
- en: CSV and TXT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CSV stands for comma-separated values, which means that the comma is the default
    delimiter for these files. However, they accept other delimiters as well.
  prefs: []
  type: TYPE_NORMAL
- en: CSVs are made of columns and rows and the cell value is arranged in a tabular
    format. They can come with or without column names and row indices. The primary
    reasons for a CSV file's existence include manually gathered data, data that's
    been extracted and downloaded from a database, a direct download from a tool or
    website, web scraping, and the result of running a data science algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Reading CSV and TXT files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`read_csv` is the go-to method for reading CSV files in `pandas`. It can also
    be used to read `txt` files. The syntax of using `read_csv` is shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters of the `read_csv` method are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`filepath`: A string or filename with or without a filepath.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype`: Can be passed as a dictionary containing name and type as a key-value
    pair. Specifies the data type of the column name. Generally, pandas guesses the
    type of columns based on the first few rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header`: True/False. This specifies whether the first row in the data is a
    header or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names`: List. Specifies column names for all the columns of a dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skiprows`: List. Skip certain rows of data by specifying row indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_col`: Series/List. Specifies the column that can work as a row number/identifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_blank_lines`: True/False. Specifies whether to skip blank lines or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_filter`: True/False. Specifies whether to filter NA values or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`usecols`: List. Returns the subset of data with columns in the passed list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `read_csv` method returns a DataFrame. The following are some examples of
    reading files using the `read_csv` method.
  prefs: []
  type: TYPE_NORMAL
- en: Reading a CSV file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can read a CSV file by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Specifying column names for a dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code will specify the column names for a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that the column names are read from a file and then converted into a list
    to be passed to the names parameter in `read_csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Reading from a string of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s how we can use `read_csv` to create a DataFrame from a list of strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Skipping certain rows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also skip certain rows. Let''s say that we only want the rows whose
    indices are multiples of 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e5b5fa6-3e3b-4995-a8db-66caaf0727eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Demonstration of using the skiprows parameter in read_csv. The right-hand panel
    shows the data that's been filtered through skiprows (keeping only rows with row
    numbers that are multiples of 3)
  prefs: []
  type: TYPE_NORMAL
- en: The left-hand side diagram shows the resultant DataFrame without skipping any
    row, while the right-hand side shows the same DataFrame after filtering the rows
    whose indices are not multiples of 3\. Note that this method considers the real
    index (3rd and 6th from the top, starting from 1) and not the Python index (starting
    from 0) for filtering the rows based on their index.
  prefs: []
  type: TYPE_NORMAL
- en: Row index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If a file has one more column of data than the number of column names, the
    first column will be used as the DataFrame''s row names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e787128-6e43-4df6-a46d-728f1e129fb9.png)'
  prefs: []
  type: TYPE_IMG
- en: The column with values but no corresponding column name is used as a row index.
  prefs: []
  type: TYPE_NORMAL
- en: Reading a text file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`read_csv` can help read text files as well. Often, data is stored in `.txt`
    files with different kinds of delimiters. The `sep` parameter can be used to specify
    the delimiter of a particular file, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding file has `Tab` as a delimiter, which is specified using the `sep`
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Subsetting while reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Only a selected list of columns can be subsetted and loaded using the `usecols`
    parameter while reading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Numeric lists, as well as explicit lists with column names, can be used. Numeric
    indexing follows Python indexing, that is, starting from 0.
  prefs: []
  type: TYPE_NORMAL
- en: Reading thousand format numbers as numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If a dataset contains a numeric column that has thousand numbers formatted
    by a comma or any other delimiter, the default data type for such a column is
    a string or object. The problem is that it is actually a numeric field and it
    needs to be read as a numeric field to be used further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1ae239f-3e96-45a6-85a3-47f6f43ba54d.png)'
  prefs: []
  type: TYPE_IMG
- en: Data with a level column with thousand format numbers
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To overcome this problem, the `thousands` parameter can be used while reading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Indexing and multi-indexing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`index_col` can be used to specify one column to provide row indices. A list
    of columns can be passed as indices, which leads to multi-indexing. Let''s look
    at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c22b0911-c5a3-40af-9b87-92c6b3161b6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Single index (left) and multi-index (right) on the same data
  prefs: []
  type: TYPE_NORMAL
- en: 'This kind of multi-indexing makes it easy to subset based on either an index
    or both:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4752a1fa-6e39-4f44-80b3-529f1880ba9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Subsetting multi-indexed data using one index (left) and both indices (right)
  prefs: []
  type: TYPE_NORMAL
- en: Reading large files in chunks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reading a large file in memory at once may consume the entire RAM of the computer
    and may cause it to throw an error. In such cases, it becomes pertinent to divide
    the data into chunks. These chunks can then be read sequentially and processed.
    This is achieved by using the `chunksize` parameter in `read_csv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting chunks can be iterated over using a for loop. In the following
    code, we are printing the shape of the chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'These chunks can then be concatenated to each other using the `concat` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Handling delimiter characters in column data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, a column separate character is present as part of the data in one
    of the columns. This leads to incorrectly parsing data as this would split the
    column that was supposed to be read as one into two. To avoid such a situation,
    a quote character should be used around the data in the specified columns. This
    quote character forces `read_csv` to ignore the delimiter for the data that's
    present inside the quote character and not break it into two pieces.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quote characters can be specified using the `quotechar` argument of `read_csv`.
    For example, consider the following dataset. Here, white space is used as a delimiter
    and double quotes have been used as a grouping element:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c343c737-1b11-46a8-9dd4-f8dbb4d2f424.png)'
  prefs: []
  type: TYPE_IMG
- en: Usage of quotechar keyword 1—input dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'To parse this, we would use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We would get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e92e0f9-5111-4605-bc3a-9110692a106d.png)'
  prefs: []
  type: TYPE_IMG
- en: Usage of quotechar keyword 2—output dataset
  prefs: []
  type: TYPE_NORMAL
- en: Writing to a CSV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A DataFrame is an in-memory object. Often, DataFrames need to be saved as physical
    files for later use. In such cases, the DataFrames can be written as a `CSV` or `TXT` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a synthesized DataFrame using random numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be written to a `.csv` or `.txt` file using the `to_csv` method, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: These files would be written to the current working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'A delimiter of choice can be provided while writing to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many other useful options available, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`index`: True/False. Indicates whether we should have row indices or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_label`: String/Column name. Column to be used as a row index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header`: True/False. Specifies whether to write the column names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep`: String. A string representation for missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Excel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Excel files are similar to `CSV` files but are different in the sense that they
    can have multiple sheets, formatted data and tables, charts, and formulas. In
    many cases, reading data from Excel files is required.
  prefs: []
  type: TYPE_NORMAL
- en: '`xlrd` is the package of choice while working with Excel sheets. Some of the
    major functionalities of the `xlrd` package are summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Code snippet** | **Goal achieved** |'
  prefs: []
  type: TYPE_TB
- en: '| `import xlrd` | Importing the xlrd library |'
  prefs: []
  type: TYPE_TB
- en: '| `book=xlrd.open_workbook(''SRS Career.xlsx'')` | Reading the Excel workbook
    |'
  prefs: []
  type: TYPE_TB
- en: '| `n=book.nsheets` | Finding the number of sheets in a workbook |'
  prefs: []
  type: TYPE_TB
- en: '| `book.sheet_names()` | Finding the names of sheets in a workbook |'
  prefs: []
  type: TYPE_TB
- en: '| `last_sheet=book.sheet_by_index(n-1)` | Reading the sheets by sheet index
    |'
  prefs: []
  type: TYPE_TB
- en: '| `last_sheet.row_values(0)` | Getting the first row of a sheet |'
  prefs: []
  type: TYPE_TB
- en: '| `last_sheet.cell(0,0)` | Getting the first cell of the sheet |'
  prefs: []
  type: TYPE_TB
- en: '| `last_sheet.row_slice(rowx=0,start_colx=1,end_colx=5)` | Getting the 1^(st)
    to the 5^(th) columns of the first row |'
  prefs: []
  type: TYPE_TB
- en: URL and S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, the data is directly available as a URL. In such cases, `read_csv`
    can be directly used to read from these URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, to work with URLs in order to get data, we can use a couple
    of Python packages that we haven''t used so far, such as `.csv` and `.urllib`.
    It would suffice to know that `.csv` provides a range of methods for handling `.csv`
    files and that `urllib` is used to navigate to and access information from the
    URL. Here is how we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`AWS S3` is a popular file-sharing and storage repository on the web. Many
    enterprises store their business operations data as files on S3, which needs to
    be read and processed directly or be moved to a database. Python allows us to
    directly read files from S3, as shown in the following code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python 3.4 and above use the `s3fs` package in addition to pandas to read files
    directly from S3\. An AWS config file needs to be placed in the current working
    directory. The bucket name, as well as the path and filename, need to be passed
    for reading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'A DataFrame can be written to a CSV file and saved directly in S3 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: HTML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HTML is the popular file format for creating and wrapping web elements and
    pages. Sometimes, tabular data is stored in a file. In such cases, the `read_html`
    method is directly used to read such data. This function parses table elements
    from HTML files and reads the tables as DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find all of the table elements containing a particular match word by
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'A DataFrame can be converted into an HTML table element so that it can be placed
    into an HTML file like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99e747e1-7272-4977-8aad-2b2550d77cb7.png)'
  prefs: []
  type: TYPE_IMG
- en: HTML table element created from a DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'A selected list of columns can be filtered and converted into HTML like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Writing to an HTML file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The HTML file can be saved as a physical file like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f0d904f-7480-42a1-a071-7acbf2a42ad1.png)'
  prefs: []
  type: TYPE_IMG
- en: Subsetting multi-indexed data using one index (left) and both indices (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'The row and column names are bold by default. This can be changed with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`JSON` is a popular dictionary-like, key-value pair-based data structure that''s
    suitable for exposing data as APIs from SaaS tools. `address`, `postalCode`, `state`,
    `streetAddress`, `age`, `firstName`, `lastName`, and `phoneNumber` are keys whose
    values are shown to the right of them. `JSON` files can be nested (the values
    of a key are JSON) as well. Here, `address` has nested values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b14ed272-db57-4fba-9a57-9c9cea728544.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of JSON data (dictionary; key-value pairs)
  prefs: []
  type: TYPE_NORMAL
- en: 'DataFrames can be converted into JSON using `to_json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/502b72f1-18c4-4307-ab86-8fd00f5b27d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting a DataFrame into JSON format
  prefs: []
  type: TYPE_NORMAL
- en: While converting the DataFrame into a JSON file, the orientation can be set.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to keep the column name as the primary index and the row indices
    as the secondary index, then we can choose the orientation to be `columns`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/016ceada-6767-443e-8442-efbebb525bb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting a DataFrame into JSON with the column orientation
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to keep the row indices as the primary index and the column names
    as the secondary index, then we can choose the orientation to be `index`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f1817dc-3dee-4536-8d37-613c445ee872.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting a DataFrame into JSON with the index orientation
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option is to convert a DataFrame into an array of JSONs. This is useful
    while passing data to a visualization library, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f25c39c-f779-4804-8e0e-e03ee72f54ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting a DataFrame into JSON with the records orientation
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also contain the bare-bones values as a list of values, without any
    row or column index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3ea3f67-3fe1-4968-bfd3-302fcc8198d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting a DataFrame into JSON with the values orientation
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can also orient the converted JSON in order to separate the row
    indices, column names, and data values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69ac8ae1-0481-40fa-a39f-dcbc19b84268.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting a DataFrame into JSON with the split orientation
  prefs: []
  type: TYPE_NORMAL
- en: Writing a JSON to a file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'JSON can be written to physical files like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Reading a JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`json_loads` is used to read a physical file containing JSONs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04b51282-c183-492b-a784-6eeeec586db7.png)'
  prefs: []
  type: TYPE_IMG
- en: First record in a list of JSONs
  prefs: []
  type: TYPE_NORMAL
- en: 'The files can be read one JSON at a time using the `open` and `readline` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, `records` contains a list of JSONs from which all the values of a particular
    key can be pulled out. For example, here, we are  pulling out all the `latlong`
    (`''ll''` column) wherever it has a non-zero value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Writing JSON to a DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A list of JSON objects can be converted into a DataFrame (much like a dictionary
    can). The records element we created previously is a list of JSONs (we can check
    this by using `records[0:3]` or `type(records)`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In the last line, we are trying to find the count of different time zones contained
    in the `'tz'` column.
  prefs: []
  type: TYPE_NORMAL
- en: Subsetting a JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s have a look at a new JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d6b3b39-b011-49be-a8ed-edfd9191c8e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Loading a JSON file with several degrees of nesting
  prefs: []
  type: TYPE_NORMAL
- en: This is a JSON with several degrees of nesting. The `hits` key contains a JSON
    as a value whose key value is `hits`. The value of this JSON is a list containing
    another JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we want to find out the score value from this JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the image URL can be found as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Looping over JSON keys
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'JSON data can be looped on its keys and values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f5e5349-afc0-410e-845e-10fe1314ea0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Printing keys of the loaded JSON by looping over the keys and values
  prefs: []
  type: TYPE_NORMAL
- en: We can print both the keys and values together as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb4bd0e2-2b88-4120-a6e4-1bc8d5ef1f16.png)'
  prefs: []
  type: TYPE_IMG
- en: Printing keys and values of the loaded JSON by looping over the keys and values
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will look at how to use reading and writing operations with exotic file
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: Reading HDF formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Hierarchical Data Format** (**HDF**) is efficient in handling large and
    complex data models. The versatility and flexibility of HDF in data storage make
    it a sought after format for storing scientific data. In fact, HDF was selected
    as the standard data and information system by NASA, for use in the Earth Observing
    System. HDF5 is the current technological suite used by the HDF file format and
    replaced the older HDF4.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some unique features of HDF5:'
  prefs: []
  type: TYPE_NORMAL
- en: HDF5 has no set limits regarding file size and the objects in the file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HDF5 can group and link objects in the file, thereby facilitating as a supportive
    mechanism for complex relationships and dependencies in data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HDF5 also supports metadata.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While accommodating a variety of predefined and user-defined data types, HDF5
    also has the ability to store and share data type descriptions in HDF files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For efficiency in the data transfer process, HDF5 incorporates Standard (Posix),
    Parallel, and Network I/O file drivers. Additional file drivers can also be developed
    and integrated with HDF5 for any custom data transfer and storage requirements.
    HDF5 makes data storage more optimized through techniques such as compression,
    extensibility, and chunking. Being able to perform data transformations, make
    changes to data types, and select subsets of data during data transfer makes the
    reading and writing processes efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s read an HDF file using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d925a312-cffd-4dd2-8e60-44d2d1fad99c.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_hdf
  prefs: []
  type: TYPE_NORMAL
- en: 'A subset of the data could be extracted during the reading process using the
    index argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/743debf2-53b8-4f10-b761-dada60221e07.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_hdf with indexing
  prefs: []
  type: TYPE_NORMAL
- en: Reading feather files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The feather format is a binary file format for storing data that makes use of
    Apache Arrow, an in-memory columnar data structure. It was developed by Wes Mckinney
    and Hadley Wickham, chief scientists at RStudio as an initiative for a data sharing
    infrastructure across Python and R. The columnar serialization of data in feather
    files makes way for efficient read and write operations, making it far faster
    than CSV and JSON files where storage is record-wise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feather files have the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: Fast I/O operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feather files can be read and written in languages other than R or Python, such
    as Julia and Scala.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They have compatibility with all pandas datatypes, such as Datetime and Categorical.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feather currently supports the following datatypes:'
  prefs: []
  type: TYPE_NORMAL
- en: All numeric datatypes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timestamps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UTF-8 encoded strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since feather is merely a simplistic version of Arrow, it has several caveats
    associated with it. The following are some limitations of using a feather file:'
  prefs: []
  type: TYPE_NORMAL
- en: Not recommended for long-term data storage as their stability between versions
    cannot be guaranteed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any index or multi-index, other than the default indexing scheme, is not supported
    in Feather format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python data types such as Period are not supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicates in column names are not supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reading a feather file in pandas is done like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b96d3b4f-a50f-4a52-91c6-a4165aaf499b.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_feather
  prefs: []
  type: TYPE_NORMAL
- en: Reading parquet files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Parquet is another file format that makes use of columnar compression
    for efficient read and write operations. It was designed to be compatible with
    big data ecosystems such as Hadoop and can handle nested data structures and sparsely
    populated columns. Though the parquet and feather formats share a similar base,
    parquet has a better compression routine than feather. The compressed file is
    smaller in parquet than it is in feather. Columns with similar data types use
    the same encoding for compression. The use of different encoding schemes for the
    compression of parquet makes it efficient. Just like feather, parquet is a binary
    file format that can work well with all pandas data types and is supported across
    several languages. Parquet can be used for the long-term storage of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some limitations of the parquet file format:'
  prefs: []
  type: TYPE_NORMAL
- en: While parquet can accept multi-level indices, it requires that the index level
    name is in string format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python data types such as Period are not supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicates in column names are not supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When Categorical objects are serialized in a parquet file, they are deserialized
    as an object datatype.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serialization or deserialization of parquet files t in pandas can take place
    in either of the `pyarrow` and `fastparquet` engines. These two engines have different
    dependencies. Pyarrow does not support Timedelta.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s read a parquet file using the `pyarrow` engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a4f82f0-be20-408d-805a-3cb1e7160638.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_parquet
  prefs: []
  type: TYPE_NORMAL
- en: 'Parquet allows us to select columns when reading a file, which saves time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The same works for the `fastparquet` engine as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Reading a SQL file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interacting with a SQL database through pandas requires the sqlalchemy dependency
    to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define the engine from which connection parameters can be obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s read the `data_sql` table from the SQL database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e991f948-810a-4d79-9894-cd22d844b593.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_sql_table
  prefs: []
  type: TYPE_NORMAL
- en: 'The `read_sql_table()` function reads an entire table for the given table name.
    A specific column can be set as the index when reading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/357a68c7-d791-454a-8dfe-fdff32a5188c.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_sql_table with indexing
  prefs: []
  type: TYPE_NORMAL
- en: 'The columns argument lets us choose specific columns when reading data by passing
    the column names as a list. Any date columns can be parsed into a specific format
    during the read process, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The `schema` argument in this function helps specify the schema from which the
    table is to be extracted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of reading the entire table, it is also possible to use a SQL query
    to get data in the necessary format. We can do this with the `read_sql_query()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5276913a-81d8-4b1e-a138-bf3e32b90d67.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_sql_query
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the `INSERT` and `CREATE` queries, which do not return any output, the
    `sql.execute()` function can be used. This requires an `sql` file of `pandas.io`
    to be imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'With a `sqlite` database, the connection to the engine has to be defined as
    follows so that it can be used in the `read_sql_table()` or `read_sql_query()`
    functions. The `sqlite` module must be imported prior to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Reading a SAS/Stata file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas can read two file formats from SAS – SAS xports (`.XPT`) and SAS data
    files (`.sas7bdat`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `read_sas()` function helps read SAS files. Here, a SAS data file has been
    read and displayed as a pandas dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83cb9f2b-0d6c-428a-bde7-f4e3ebaa063e.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_sas
  prefs: []
  type: TYPE_NORMAL
- en: 'The `chunksize` and `iterator` arguments help in reading the SAS file in groups
    of the same size. If the SAS data file that was used earlier is read with a chunksize
    of 10, then the 51 records will be divided into six groups, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b71ebc09-1929-443e-bdf7-1a75cf72d09c.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_sas with chunksize
  prefs: []
  type: TYPE_NORMAL
- en: However, these SAS files cannot be written using pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas also provides support for reading and writing files that have been generated
    from Stata. Stata only supports limited datatypes: `int8`, `int16`, `int32`, `float32`,
    `float64`, and strings with a length less than 244\. When writing a Stata data
    file through pandas, type conversion is applied wherever applicable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s read a Stata datafile using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a760ba3a-248a-4b8d-9d52-6bb2a95e589c.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_stata
  prefs: []
  type: TYPE_NORMAL
- en: 'The `read_stata()` function also has `chunksize` and `iterator` arguments to
    read data in smaller groups. The following arguments are the available `stata`
    reader functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`convert_categoricals`: Converts a suitable column into a categorical data
    type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_col`: Identifies the column to be defined as an index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_missing`: Specifies whether to represent missing values as NaN or
    with a Stata missing value object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns`: Columns to select from the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading from Google BigQuery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BigQuery is an extremely powerful data warehousing solution provided by Google.
    Pandas can directly connect to BigQuery and bring your data to a Python environment
    for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of reading a dataset from BigQuery:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7131995c-654c-4c9f-a9a1-74c5225ac87a.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_gbq
  prefs: []
  type: TYPE_NORMAL
- en: 'The `read_gbq()` function accepts the query and the Google Cloud project-id
    (which serves as a key) so that it can access the database and bring out the data.
    The dialect argument takes care of the SQL syntax to be used: BigQuery''s legacy
    SQL dialect or the standard SQL dialect. In addition, there are arguments that
    allow the index column to be set (`index_col`), columns to be reordered (`col_order`),
    and reauthentication to be enabled (`reauth`).'
  prefs: []
  type: TYPE_NORMAL
- en: Reading from a clipboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a rather interesting feature in pandas. Any tabular data that has been
    copied onto the clipboard can be read as a DataFrame in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s copy the following tabular data with the usual *ctrl + C* keyboard command:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Gender** | **Entry_Date** | **Flag** |'
  prefs: []
  type: TYPE_TB
- en: '| **A** | M | 2012-01-19 | True |'
  prefs: []
  type: TYPE_TB
- en: '| **B** | F | 2012-12-30 | False |'
  prefs: []
  type: TYPE_TB
- en: '| **C** | M | 2012-05-05 | False |'
  prefs: []
  type: TYPE_TB
- en: 'Calling the `read_clipboard()` function makes this data available as a pandas
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9479c71d-b380-484b-9e00-c217d19a956f.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read_clipboard
  prefs: []
  type: TYPE_NORMAL
- en: 'This function also recognizes the **Flag** column as a bool data type by default
    and assigns the unnamed column to be the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ab745a9-7b80-4af3-bd88-83f89e40fe90.png)'
  prefs: []
  type: TYPE_IMG
- en: Data types after reading the clipboard
  prefs: []
  type: TYPE_NORMAL
- en: Managing sparse data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sparse data refers to data structures such as arrays, series, DataFrames, and
    panels in which there is a very high proportion of missing data or NaNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a sparse DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'This DataFrame has NaNs in 95% of the records. The memory usage of this data
    can be estimated with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/395b1fb1-ccac-4920-9d60-8879499c651a.png)'
  prefs: []
  type: TYPE_IMG
- en: Memory usage of a DataFrame with 95% NaNs
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, each element consumes 8 bytes of data, irrespective of whether
    it is actual data or a NaN. Pandas offers a memory-efficient solution for handling
    sparse data, as depicted in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/006a584e-c19f-4aa2-b459-408b6d75f790.png)'
  prefs: []
  type: TYPE_IMG
- en: Memory usage of sparse data
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the memory usage has come down, with memory not being allotted to NaNs.
    This can also be implemented by defining a `fill_value` instead of NaN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e92501ca-3ad0-4c92-9cdb-c15e3cfb8a0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Memory usage of sparse data after filling in the values
  prefs: []
  type: TYPE_NORMAL
- en: 'The sparse data can also be converted back into the original dense form, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: This way of handling sparse data can be applied in a similar way to series,
    panels, and arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Writing JSON objects to a file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `to_json()` function allows any DataFrame object to be converted into a
    JSON string or written to a JSON file if the file path is specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3093bb79-a18a-40aa-9758-d5a8d340405b.png)'
  prefs: []
  type: TYPE_IMG
- en: JSON output
  prefs: []
  type: TYPE_NORMAL
- en: 'The orientation of the data in the JSON can be altered. The `to_json()` function
    has an orient argument which can be set for the following modes: columns, index,
    record, value, and split. Columns is the default setting for orientation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a98a5f64-dc5b-44bd-8e85-be398a6fc207.png)'
  prefs: []
  type: TYPE_IMG
- en: JSON output – column orientation
  prefs: []
  type: TYPE_NORMAL
- en: 'Orienting along the index acts like a transpose of the former case with a reversal
    of row and column indices in the JSON dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b257e514-0a3d-4c8e-b9cb-4e236929305f.png)'
  prefs: []
  type: TYPE_IMG
- en: JSON output – index orientation
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting orient as records creates a JSON structure where each record or row
    from the original DataFrame retains its structural form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2238dc0e-150a-451d-866b-651b27862cb4.png)'
  prefs: []
  type: TYPE_IMG
- en: JSON output – records orientation
  prefs: []
  type: TYPE_NORMAL
- en: 'When the orient option is set to values, both row indices and column indices
    vanish from the picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0d7aa85-911e-4556-94e7-6db105f4765f.png)'
  prefs: []
  type: TYPE_IMG
- en: JSON output – values orientation
  prefs: []
  type: TYPE_NORMAL
- en: 'The split orientation defines a JSON made up of entities such as column, index,
    and data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c909c169-02bf-4ebc-a2e7-40fb9626bad0.png)'
  prefs: []
  type: TYPE_IMG
- en: JSON output—split orientation
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting orient to table brings out aspects such as schema and field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4214d637-f8be-4ec1-9a61-40762c7b7e15.png)'
  prefs: []
  type: TYPE_IMG
- en: JSON output—table orientation
  prefs: []
  type: TYPE_NORMAL
- en: The `date_format` argument of `to_json()` allows timestamps in the DataFrame
    to be converted into either `epoch` format or `iso` format.
  prefs: []
  type: TYPE_NORMAL
- en: An unsupported datatype such as `complex` can be handled by specifying the type
    conversion to be followed through the `default_handler` argument.
  prefs: []
  type: TYPE_NORMAL
- en: Serialization/deserialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serialization is the process of translating [data structures](https://en.wikipedia.org/wiki/Data_structure)
    or [object](https://en.wikipedia.org/wiki/Object_(computer_science)) state into
    a format that can be stored (for example, in a [file](https://en.wikipedia.org/wiki/Computer_file)
    or memory [buffer](https://en.wikipedia.org/wiki/Data_buffer)) or transmitted
    (for example, across a [network](https://en.wikipedia.org/wiki/Computer_network)
    connection link) and reconstructed later (possibly in a different computer environment).[[1]](https://en.wikipedia.org/wiki/Serialization#cite_note-1)
    When the resulting series of bits is reread according to the serialization format,
    it can be used to create a semantically identical clone of the original object.
  prefs: []
  type: TYPE_NORMAL
- en: Data structures such as JSON, arrays, DataFrames, and Series sometimes need
    to be stored as physical files or transmitted over a network. These serializations
    can be understood as a dump of data where data can be stored in any format (text,
    CSV, and so on) or structure but all the important data points can be recreated
    by loading/deserializing them.
  prefs: []
  type: TYPE_NORMAL
- en: Some examples of this are storing the parameters of the trained model object
    of a statistical model. This serialized file containing trained parameters can
    be loaded and the testing data can be passed through it for prediction. This is
    a popular method that's used to put statistical models to use.
  prefs: []
  type: TYPE_NORMAL
- en: Other uses of serialized data formats include transferring data through wires,
    storing objects in databases or HDDs, to make remote procedure calls, and to detect
    changes in time-varying data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a sample DataFrame to understand the serialization of various
    file formats supported by Pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20da6e5b-781d-4b37-9945-e38614c41d24.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame for serialization
  prefs: []
  type: TYPE_NORMAL
- en: Writing to exotic file types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are various formats that a data structure or object can be stored in.
    Let's go over a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: to_pickle()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a Python object is pickled, it gets saved to disk. Pickling serializes
    the object first, before writing it. It involves converting objects such as lists,
    Dicts, DataFrames, and trained machine learning models into a character stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s convert the DataFrame we defined earlier into pickle format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to compress pickle files before they are written. Compression
    schemes such as `gzip`, `bz2`, and `xz` are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the compression type is inferred from the extension that''s provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: The `read_pickle()` function will deserialize the `pickle` file. Zip compression
    is only supported for reading a single file and not for writing.
  prefs: []
  type: TYPE_NORMAL
- en: to_parquet()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed in the *Reading parquet files* section, two engines can be
    used for deserialization as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: to_hdf()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An HDF file is like a dictionary and it can store multiple objects. The `to_hdf()`
    function converts a Pandas object into an HDF file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: When all the columns of a row are NaNs, they are not automatically dropped.
    This can be done by setting the `dropna` argument to `True` when writing to HDF.
  prefs: []
  type: TYPE_NORMAL
- en: to_sql()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With support from the `sqlalchemy` package, data can be transferred to databases
    through pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Data can also be pushed iteratively in batches by using the `chunksize` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data type of any column can also be changed when pushing to the database,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The `Timedelta` datatype, which is not supported across databases, is converted
    into its equivalent integral value in nanoseconds before being stored in the database.
  prefs: []
  type: TYPE_NORMAL
- en: to_feather()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Serializing a pandas object into feather format just requires the `to_feather()`
    function to be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: to_html()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `to_html()` function converts a DataFrame into a raw HTML format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91c4cda7-34b2-4cd7-bb92-f9548df02cf8.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame in HTML format
  prefs: []
  type: TYPE_NORMAL
- en: A horde of options in the `to_html()` function allow the raw HTML to be enriched.
    Being able to select columns and control escape sequences is possible through
    the use of the `columns` and `escape` arguments.
  prefs: []
  type: TYPE_NORMAL
- en: to_msgpack()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Msgpack offers fast and efficient binary serialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'A single object can be directly converted into `msgpack` format like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'If we have multiple objects, they can be serialized into a single `msgpack`
    file like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: to_latex()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `to_latex()` function takes a DataFrame and converts it into an aesthetic
    tabular structure that''s compatible with `latex` documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3f596fb-88e6-4ca5-99d7-6ef2497ef3a2.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame in latex format
  prefs: []
  type: TYPE_NORMAL
- en: to_stata()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pandas can help with creating `stata` data files with the `.dta` extension,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: to_clipboard()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `to_clipboard()` function transfers a DataFrame from a Python environment
    to the clipboard. From the clipboard, the object can be pasted elsewhere through
    the use of the *ctrl* + *V* keyboard command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'This DataFrame can also be sent to the clipboard in a format that''s more compatible
    with CSV like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: GeoPandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GeoPandas is a Python package written on top of pandas that's used to work with
    geospatial data. It is designed to work with existing tools, such as desktop GIS,
    geospatial databases, web maps, and Python data tools.
  prefs: []
  type: TYPE_NORMAL
- en: GeoPandas allows you to easily perform operations in Python that would otherwise
    require a spatial database such as PostGIS.
  prefs: []
  type: TYPE_NORMAL
- en: What is geospatial data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spatial data, geospatial data, GIS data, and geodata are the names for numeric
    data that identifies the geographical location of a physical object such as a
    building, street, town, city, country, and so on according to a geographic coordinate
    system**.** Apart from the geographical location, geospatial data often also stores
    socioeconomic data, transaction data, and so on for each location.
  prefs: []
  type: TYPE_NORMAL
- en: Installation and dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GeoPandas can be installed through pip or Anaconda, or directly through GitHub.
    The most common ways are through `pip` and Anaconda through a Terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'GeoPandas depends on the following Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shapely`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fiona`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pyproj`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`six`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rtree`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with GeoPandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use the GeoPandas library to read many GIS file formats (relying on the
    `fiona` library, which is an interface to GDAL/OGR) using the `geopandas.read_file`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Data can be read through shapefiles as well. In this section, we will look at
    an example of working with GeoPandas. We will explain how to read a shapefile
    that contains geospatial data, performing aggregations on it, sorting it, and
    finally plotting the required Geo DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following code to call in the required prerequisites libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to read a shapefile that has geospatial information
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to access the first five rows of the dataset, just like
    we do with pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippets result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ce0578f-c959-4a75-bd6b-e3c778dcbd1f.png)'
  prefs: []
  type: TYPE_IMG
- en: A geospatial shape file read as a DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s plot a quick basic visualization of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5db74b7f-59a8-46b4-bed9-66be5731c66d.png)'
  prefs: []
  type: TYPE_IMG
- en: The countries in the shapefile plotted on a map
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the data type of our geospatial data, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7a1f2d4-33c6-46b5-b852-2b0e93eb46b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Asserting that the data type of the countries shapefile is a GeoDataFrame after
    conversion
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the DataFrame is a GeoDataFrame. Now, let's discuss what
    a GeoDataFrame is.
  prefs: []
  type: TYPE_NORMAL
- en: GeoDataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A GeoDataFrame contains a geospatial dataset. It is just like a pandas DataFrame
    but with some additional functionality for working with geospatial data. This
    additional functionality is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A `.geometry` attribute that always returns the column that includes geometry
    information (returning a GeoSeries). The column name itself does not necessarily
    need to be `.geometry`, but it will always be accessible as the `.geometry` attribute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has some extra methods for working with spatial data (area, distance, buffer,
    intersection, and so on), all of which we will look at in later chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GeoDataFrame is still a DataFrame, so we have all the functionalities that we
    have available for DataFrames. We can perform aggregation, sorting, filtering,
    and so on in GeoDataFrames as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following code to perform a simple aggregation with GeoPandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '`POP_EST` is a column in the `countries` GeoDataFrame and is of the numeric
    type. This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9bc4306-c07b-4ef9-af33-5110ae805ff2.png)'
  prefs: []
  type: TYPE_IMG
- en: Aggregating a numeric column in a GeoDataFrame shows that it works exactly the
    same way as a normal DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can use boolean filtering to select a subset of the DataFrame
    based on a condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will try to plot the filtered GeoDataFrame by using the `plot()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1812f806-00e9-48b4-83f9-553a54086457.png)'
  prefs: []
  type: TYPE_IMG
- en: Subsetting one continent and plotting it for better visibility
  prefs: []
  type: TYPE_NORMAL
- en: GeoPandas also helps in converting an ordinary DataFrame into a GeoDataFrame,
    provided that you have `Latitude` and `Longitude` coordinates. Let's take a look
    at this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that we have a simple DataFrame, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33331910-d469-4863-9e25-20be419b7a83.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a normal DataFrame of country capitals with latitude and longitude
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s append a new column called `''Coordinates''` which concatenates the
    latitude and longitude columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d86862f8-a170-4390-97fd-193d8b46b2bb.png)'
  prefs: []
  type: TYPE_IMG
- en: A normal DataFrame with latitude and longitude zipped in one column
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `Point` function from the shapely package, we can correctly identify
    these as positional coordinates or point tuple parameters, which are the vertebra
    of a GeoDataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86e05119-7fff-4636-80a6-b3214ef99f72.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting the zipped latitude and longitude into a point so that it is usable
    by geopandas for converting it into a GeoDataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that everything is in place, let''s convert this to a GeoDataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print the type as `gdf` to see its GeodataFrame type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecdd1986-6e59-4459-9087-39a7fe552661.png)'
  prefs: []
  type: TYPE_IMG
- en: Asserting that the type of the newly created DataFrame is a GeoDataFrame
  prefs: []
  type: TYPE_NORMAL
- en: This has given us a basic idea about GeoPandas and how it works. Its wings are
    so widespread that you can glide across the various features of it and get benefits
    from it.
  prefs: []
  type: TYPE_NORMAL
- en: Open source APIs – Quandl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python can be used to fetch data from open source and commercial APIs. We can
    use it to fetch data in several formats. Some of them output data in JSON format,
    some in XML, and some in tabular formats such as CSV and DataFrames. Once converted
    into DataFrames, this data is generally processed in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at an example of fetching data from the Quandl
    API, which is an open source API that contains data on a variety of topics such
    as financial, economic, and alternative data. You can have a look at this famous
    data repository here: [https://www.quandl.com/](https://www.quandl.com/).
  prefs: []
  type: TYPE_NORMAL
- en: An `api` key is an application programming interface that acts as a mediator
    between a developer or any other user who wishes to access the data within the
    website using a computer code. An `api` key is a piece of code that identifies
    the user and their associated account.
  prefs: []
  type: TYPE_NORMAL
- en: To get started with this example, you will need to sign up for a Quandl account,
    which is free. Post signup, the API key can be found under the Account setting
    options, which will be available under the dropdown of the profile image.
  prefs: []
  type: TYPE_NORMAL
- en: Python has made it easier to work with the Quandl API by providing us with a
    package that can be used to interact with the latest version of the Quandl data
    repository. This package is compatible with Python version 2.7 and above.
  prefs: []
  type: TYPE_NORMAL
- en: 'First things first, you need to install the Quandl package through `pip` or
    `conda` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'You can fetch any dataset you wish to use. Here, I am using the Brazilian Real
    Futures, July 2022 dataset for illustration purposes. You will need to find the
    data code for the dataset you want to download. This can be obtained from the
    Quandl website and is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b21ddba-b76c-4576-b7e6-109183f6a825.png)'
  prefs: []
  type: TYPE_IMG
- en: Finding the data code for a dataset on the Quandl website
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at how we can use the Quandl API to fetch the data we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b2db996-2bea-4c2c-9b41-18c247cd38f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Brazilian Real Futures data fetched via the Quandl API
  prefs: []
  type: TYPE_NORMAL
- en: 'The API can also be used to fetch a subset of data and not all of it at once.
    For example, here, we have filtered the data for a given date range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4377abc8-7f48-40fc-bd88-b142a4cb1b36.png)'
  prefs: []
  type: TYPE_IMG
- en: Brazilian Real Futures data fetched via the Quandl API with a date range filter
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been read, pandas can be used to perform all the transformations
    on the data, which will be helpful for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets can also be downloaded by providing the URLs of the dataset. This can
    be checked by downloading a file that will list the available datasets. Let's
    try downloading a file from the URL through Python's `urllib` package rather than
    following the `Quandl` package method.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the URL that can be used in this method, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the dataset header/link (marked in the red box):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/27f37797-063f-4bb8-bdd5-858dc4fb0b66.png)'
  prefs: []
  type: TYPE_IMG
- en: Data topic link to get more details about the topic
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the link will take you to the next page, where you would see these
    options. Select API under the Usage option, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3805a13b-b080-48cf-b9c3-48710a3fbe5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Data topic documentation
  prefs: []
  type: TYPE_NORMAL
- en: 'After this selection, you should scroll down a bit to find the following URLs,
    which can be used in the code to fetch the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bd0bf04a-881e-4eb0-af08-610d5189a8f4.png)'
  prefs: []
  type: TYPE_IMG
- en: API links for Quandl data that were obtained from the Data topic documentation
  prefs: []
  type: TYPE_NORMAL
- en: 'Since one topic may contain multiple datasets, the topic is downloaded as a
    `.zip` file containing all the datasets. It provides a metadata table, as well
    as the details (including the dataset key) of each dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/507b5a15-cb3c-4309-99b6-531fd6a82d48.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of the metadata table of the downloaded data topic
  prefs: []
  type: TYPE_NORMAL
- en: read_sql_query
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python supports a lot of database operations using libraries such as `psycopg2`
    and `sqlalchemy`. Both of them are quite comprehensive and useful when working
    with databases from a Python interface. However, they have their own paraphernalia,
    which sometimes gets too much information for simple querying tasks. Fortunately,
    there is a hidden gem in pandas called `read_sql_query` method. It does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Runs simple queries involving select, where, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs all the queries that return a table or its subset in a tabular form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can't use the INSERT, UPDATE, and DELETE statements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is a DataFrame and hence all the pandas methods can be used for further
    data processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at how we can make use of this method. To illustrate this, we will
    insert a dataset as a table into a database. To do this, you will need to install
    a PostgreSQL or SQL database to your local directory. If you already have a database
    set up, you can ignore the table creation process and jump to the queries process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s download the World Happiness 2019 dataset from Kaggle, push it to a
    `db`, and perform various DB and pandas operations on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'The following data shows us the World Happiness report as a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3bac349-5720-4957-9446-3d44cee6a066.png)'
  prefs: []
  type: TYPE_IMG
- en: World Happiness report as a DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are going to directly create a table from the DataFame that we generated
    previously, it is necessary to change the column name to postgresql since it does
    not support column names with spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to use the `read_sql_query` method, we need to make a connection with
    the database using either `psycopg2` or `sqlalchemy`. Once the connection has
    been established, `read_sql_query` can be used in its full form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97849629-087f-4e41-a3f4-2c00306967da.png)'
  prefs: []
  type: TYPE_IMG
- en: World Happiness report data as a DataFrame queried from the table in the PostgreSQL
    DB
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the following code. This helps in running a SQL query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce2363d9-2dc4-43f9-9fc5-31592f4d4844.png)'
  prefs: []
  type: TYPE_IMG
- en: World Happiness report data with filters as a DataFrame queried from the table
    in the PostgreSQL DB
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: The `pd.read_sql_query()` method returns the table as a DataFrame rather than
    requiring the programmer to intervene and convert the data into the necessary
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas plotting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A picture is worth a thousand words. This is why graphs are commonly used to
    visually illustrate relationships in data. The purpose of a graph is to present
    data that is too numerous or complicated to be described adequately in terms of
    text and in less space. With Python's **plotting function**, it takes far less
    than a few words of code to create a production-quality graphic.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by installing the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using the `mtcars` data here to explain the plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d0b263d-0e95-49e8-9a1b-f32841cfce3b.png)'
  prefs: []
  type: TYPE_IMG
- en: mtcars DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss the various plots in `pandas.plotting` in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Andrews curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Andrews curve is a method that's used to visualize multidimensional data. It
    does this by mapping each observation onto a function. Here, each color that's
    used represents a class and we can easily note that the lines that represent samples
    from the same class have similar curves. This curve is very useful in analyzing
    time series and signal data.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, each data point is sent through a Fourier transform according to
    the Fourier function. Each line in the following chart represents a separate data
    point. It can be plotted using the snippet below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c372bbe4-daf0-4f94-946e-9706f991bbf0.png)'
  prefs: []
  type: TYPE_IMG
- en: Andrews curve plot
  prefs: []
  type: TYPE_NORMAL
- en: Parallel plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel plots are best used when we need to compare many variables for each
    point and to understand the relationship between them, for example, if you need
    to compare an array of variables with the same attributes but differing values
    (for example, comparing motorcycle specs across different models).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each connected line represents one data point. The vertical lines represent
    the columns or variables whose values have been plotted for each data point. The
    inflection point (marked in red) represents the values of those variables for
    those points. A parallel chart can be plotted very easily using pandas, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d63564e2-d01a-4300-8c17-66bc6ddb8dd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallel plot
  prefs: []
  type: TYPE_NORMAL
- en: Radviz plots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Radviz plots allow for the exploration of multi-task classification problems.
    It displays data of three or more variables in a two-dimensional projection. This
    plot is like a circle with data points inside it. The variables are present around
    the perimeter of the circle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The position of each point is determined by the values of all the variable
    values that make it. An imaginary circle is created and the variables are placed
    on this circle. The points are placed within the perimeter of the circle. The
    exact position of the point is determined by the position where the force that''s
    exerted on it by each variable sums to zero. The force that''s applied by each
    variable can be thought of as a spring force and is governed by Hook''s law (F
    = kx):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29eadd53-789c-4924-b7c2-96032063ac2c.png)'
  prefs: []
  type: TYPE_IMG
- en: Radviz plot explanation
  prefs: []
  type: TYPE_NORMAL
- en: The plot above can be obtained by running the snippet below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d7e97fb-4d6c-44ee-a265-6e41c7fd895c.png)'
  prefs: []
  type: TYPE_IMG
- en: Radviz plot
  prefs: []
  type: TYPE_NORMAL
- en: Scatter matrix plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A scatter matrix plot consists of several plots of variables, all of which
    are present in a matrix format. Basically, a 2 x 2 matrix of variables is created
    where each cell represents a combination of two variables. Then, a scatter plot
    is generated for each combination. It can be used to determine the correlation
    between variables. It is used in a lot of dimension reduction cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e32e1878-bc5c-4c40-a6f0-19eeb6f9c660.png)'
  prefs: []
  type: TYPE_IMG
- en: Scatter matrix plot
  prefs: []
  type: TYPE_NORMAL
- en: Lag plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lag plot is a special type of scatter plot with variables (X, X-lagged, and
    so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'X -lagged is the variable that''s derived from X with a time lag. The graph
    is plotted among two variables and the plot is used to determine the randomness,
    model suitability, outliers, and serial correlation in the data – especially time
    series data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68f06f9e-8f57-4809-b3a3-1bd8cf53baf8.png)'
  prefs: []
  type: TYPE_IMG
- en: Lag plot
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A bootstrap plot is used to determine the uncertainty of statistics such as
    mean, median, midrange, and so on. It relies on the random sampling method with
    replacement. Calculating a statistic by randomly sampling from the same data multiple
    times and then averaging the individual result from each sample is called bootstrapping.
    A bootstrapping plot basically plots all the resultant values that were obtained
    from each random sample. It calculates the mean, median, and mode for all the
    samples and plots them as bar and line charts.
  prefs: []
  type: TYPE_NORMAL
- en: 'A random sample is selected from the data and the process is repeated a specified
    number of times to obtain the required metrics. The resulting plot that''s obtained
    is a bootstrap plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdd433ce-4e14-47e1-ad5a-40d2c899fec3.png)'
  prefs: []
  type: TYPE_IMG
- en: Lag plot
  prefs: []
  type: TYPE_NORMAL
- en: pandas-datareader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use pandas to not only read data from local CSV or text files but also
    from various popular remote data sources such as Yahoo Finance, World Bank, and
    so on. Without any support from pandas, this would have been tedious and we would
    have to resort to web scraping. This simple and powerful functionality is provided
    through the `pandas-datareader`.
  prefs: []
  type: TYPE_NORMAL
- en: It provides us with a direct way of connecting through various data sources
    from the comfort of the pandas ecosystem without having to delve into the complexity
    of HTML/JavaScript code where data is enmeshed. These data sources can be accessed
    by providing the source name and data code. Only a subset of the data can be obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s delve deeper and see how we can use it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `pandas-datareader` through `pip` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: You can also install it through `conda` using the following  set of commands
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to add `conda-forge` to our channel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'After enabling `pandas-datareader`, it can be installed with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's get hands-on with some of the remote data sources and perform various
    functions provided by the pandas library to get an idea of how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Yahoo Finance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are interested in knowing about the trends of the business world and
    have a thirst to get updates about [stocks](https://money.howstuffworks.com/personal-finance/financial-planning/stocks.htm) and [bonds](https://money.howstuffworks.com/personal-finance/budgeting/bonds.htm),
    or if you are the one who has invested in them, then you may crave an update that
    occurs every minute. Google Finance is a financial website that was developed
    by Google that's made this straightforward by providing the required information
    and also letting us customize our needs according to our interests.
  prefs: []
  type: TYPE_NORMAL
- en: Google's API became less reliable during 2017 and has become highly deprecated
    because of the unavailability of a stable replacement due to large breaks in the
    API.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to it is **Yahoo** Finance, which is similar to Google Finance,
    and is popular among users for its robust data and consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's use `pandas-datareader` to get information related to stocks, mutual
    funds, and anything related to finance using the Google Finance API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c88926d-7f47-4230-b091-596d14be8e2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Date filtered stock data for Apple, Google, Facebook, and Twitter
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0845aa70-e0bd-489d-9f36-6bfc68a14f7c.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary statistics of the stock data
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1b34b5d-8aca-4956-9ebb-42e4a4861bb4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d74262cf-8abb-48c4-8a9b-f2c91efecc78.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding code will return a DataFrame that gives you full details about
    the stock prices for each and every day between the two dates for `Apple[AAPL]`,
    `Google[GOOGL]`, `FB[FB]`, and `Twitter[TWTR]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to get to know your data before performing any kind of analysis.
    Please take the following things into account:'
  prefs: []
  type: TYPE_NORMAL
- en: High is the highest price that the stock was traded for on that particular date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low is the lowest price that the stock was traded for on that particular date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open is the price that the stock was when the date started.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Close is the price that the stock was when the market closed for that date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volume is the number of physical shares that were traded for that particular
    stock.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adj Close is the price that the stock was after the market closed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: World Bank
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The World Bank is an organization that provides financial advice and helps various
    nations in terms of their economical state. It also provides a variety of data,
    including time series, geospatial, financial data, and so on, all of which will
    be helpful for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start fetching data from the World Bank website, we must sign up to
    it. This allows us to get the indicator code for the dataset that we want to download.
    Signing up for the World Bank is free and doesn't take much time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of this example, I have used the World Development Indicators
    dataset. You can choose any dataset of your choice and start working on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the indicator code, select the Databank tab and choose Metadata Glossary
    from the list that''s displayed on the left-hand pane. You can find the indicator
    below each dataset, on the left-hand side of the panel (marked in red in the attached
    screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbf5065e-9268-4764-aff2-898397b13765.png)'
  prefs: []
  type: TYPE_IMG
- en: Indicator code for the dataset, which is required for fetching data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'The DataFrame is returned in a multi-index row format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8bb985a-5374-4ce0-a358-e528b1090bd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-indexed DataFrame output of the World Bank indicator data
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s display the data for one particular country, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35220b81-cb9a-4a59-a420-71a2dcadc30c.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-indexed DataFrame output of the World Bank indicator data for one country.
    It becomes single indexed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also return the price inflation data related to only one particular
    year for a particular country, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40efc2f1-f406-4a72-aa41-2ec6a597c839.png)'
  prefs: []
  type: TYPE_IMG
- en: Data subsetted by both indices
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After reading this chapter, the following points have been observed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` provides powerful methods so that we can read from and write to a
    variety of data structures and a variety of sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `read_csv` method in pandas can be used for reading CSV files, TXT files,
    and tables. This method has a multitude of arguments in order to specify delimiters,
    which rows to skip while reading, reading a file in smaller chunks, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas can be used to read data directly from URLs or S3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrames can be converted into JSON and vice versa. JSON can be stored in
    text files that can be read.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSONs have dictionary-like structures that can be nested an infinite number
    of times. This nested data can be subsetted just like a dictionary with keys.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pandas provide methods so that we can read data from the HD5, HTML, SAS, SQL,
    parquet, feather, and Google BigQuery data formats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serialization helps in dumping data structures or objects to physical files,
    storing them in a database, or transmitting them through a message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to access and select data from panda
    data structures. We will also look in detail at basic indexing and label-, integer-,
    and mixed indexing.
  prefs: []
  type: TYPE_NORMAL
