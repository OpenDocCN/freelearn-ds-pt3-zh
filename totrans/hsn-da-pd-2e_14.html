<html><head></head><body>
		<div id="_idContainer449">
			<h1 id="_idParaDest-218"><em class="italic"><a id="_idTextAnchor217"/>Chapter 10</em>: Making Better Predictions – Optimizing Models</h1>
			<p>In the previous chapter, we learned how to build and evaluate our machine learning models. However, we didn't touch upon what we can do if we want to improve their performance. Of course, we could try out a different model and see if it performs better—unless there are requirements that we use a specific method for legal reasons or in order to be able to explain how it works. We want to make sure we use the best version of the model that we can, and for that, we need to discuss how to tune our models.</p>
			<p>This chapter will introduce techniques for the optimization of machine learning model performance using <strong class="source-inline">scikit-learn</strong>, as a continuation of the content in <a href="B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 9</em></a>, <em class="italic">Getting Started with Machine Learning in Python</em>. Nonetheless, it should be noted that there is no panacea. It is entirely possible to try everything we can think of and still have a model with little predictive value; such is the nature of modeling.</p>
			<p>Don't be discouraged though—if the model doesn't work, consider whether the data collected suffices to answer the question, and whether the algorithm chosen is appropriate for the task at hand. Often, subject matter expertise will prove crucial when building machine learning models, because it helps us determine which data points will be relevant, as well as to take advantage of known interactions between the variables collected.</p>
			<p>In particular, the following topics will be covered:</p>
			<ul>
				<li>Hyperparameter tuning with grid search</li>
				<li>Feature engineering</li>
				<li>Building ensemble models combining many estimators</li>
				<li>Inspecting classification prediction confidence</li>
				<li>Addressing class imbalance</li>
				<li>Penalizing high regression coefficients with regularization</li>
			</ul>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor218"/>Chapter materials</h1>
			<p>In this chapter, we will be working with three datasets. The first two come from data on wine quality donated to the UCI Machine Learning Data Repository (<a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a>) by P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis, and contain information on the chemical properties of various wine samples along with a rating of the quality from a blind tasting session by a panel of wine experts. These files can be found in the <strong class="source-inline">data/</strong> folder inside this chapter's folder in the GitHub repository (<a href="https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_10">https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_10</a>) as <strong class="source-inline">winequality-red.csv</strong> and <strong class="source-inline">winequality-white.csv</strong> for red and white wine, respectively.</p>
			<p>Our third dataset was collected using the Open Exoplanet Catalogue database, at <a href="https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/">https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/</a>, which provides data in XML format. The parsed planet data can be found in the <strong class="source-inline">data/planets.csv</strong> file. For the exercises, we will also be working with the star temperature data from <a href="B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 9</em></a>, <em class="italic">Getting Started with Machine Learning in Python</em>, which can be found in the <strong class="source-inline">data/stars.csv</strong> file.</p>
			<p>For reference, the following data sources were used:</p>
			<ul>
				<li><em class="italic">Open Exoplanet Catalogue database</em>, available at <a href="https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure">https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure</a>.</li>
				<li><em class="italic">P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.</em> Available online at <a href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">http://archive.ics.uci.edu/ml/datasets/Wine+Quality</a>.</li>
				<li><em class="italic">Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository (</em><a href="http://archive.ics.uci.edu/ml/index.php">http://archive.ics.uci.edu/ml/index.php</a><em class="italic">). Irvine, CA: University of California, School of Information and Computer Science.</em></li>
			</ul>
			<p>We will be using the <strong class="source-inline">red_wine.ipynb</strong> notebook to predict red wine quality, <strong class="source-inline">wine.ipynb</strong> to distinguish between red and white wine based on their chemical properties, and the <strong class="source-inline">planets_ml.ipynb</strong> notebook to build a regression model to predict the year length of planets in Earth days.</p>
			<p>Before we get started, let's handle our imports and read in our data:</p>
			<p class="source-code">&gt;&gt;&gt; %matplotlib inline</p>
			<p class="source-code">&gt;&gt;&gt; import matplotlib.pyplot as plt</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; import seaborn as sns</p>
			<p class="source-code">&gt;&gt;&gt; planets = pd.read_csv('data/planets.csv') </p>
			<p class="source-code">&gt;&gt;&gt; red_wine = pd.read_csv('data/winequality-red.csv')</p>
			<p class="source-code">&gt;&gt;&gt; white_wine = \</p>
			<p class="source-code">...     pd.read_csv('data/winequality-white.csv', sep=';') </p>
			<p class="source-code">&gt;&gt;&gt; wine = pd.concat([</p>
			<p class="source-code">...     white_wine.assign(kind='white'),</p>
			<p class="source-code">...     red_wine.assign(kind='red')</p>
			<p class="source-code">... ])</p>
			<p class="source-code">&gt;&gt;&gt; red_wine['high_quality'] = pd.cut(</p>
			<p class="source-code">...     red_wine.quality, bins=[0, 6, 10], labels=[0, 1]</p>
			<p class="source-code">... )</p>
			<p>Let's also create our training and testing sets for the red wine quality, wine type by chemical properties, and planets models:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.model_selection import train_test_split</p>
			<p class="source-code">&gt;&gt;&gt; red_y = red_wine.pop('high_quality')</p>
			<p class="source-code">&gt;&gt;&gt; red_X = red_wine.drop(columns='quality')</p>
			<p class="source-code">&gt;&gt;&gt; r_X_train, r_X_test, \</p>
			<p class="source-code">... r_y_train, r_y_test = train_test_split(</p>
			<p class="source-code">...     red_X, red_y, test_size=0.1, random_state=0,</p>
			<p class="source-code">...     stratify=red_y</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; wine_y = np.where(wine.kind == 'red', 1, 0)</p>
			<p class="source-code">&gt;&gt;&gt; wine_X = wine.drop(columns=['quality', 'kind'])</p>
			<p class="source-code">&gt;&gt;&gt; w_X_train, w_X_test, \</p>
			<p class="source-code">... w_y_train, w_y_test = train_test_split(</p>
			<p class="source-code">...     wine_X, wine_y, test_size=0.25, </p>
			<p class="source-code">...     random_state=0, stratify=wine_y</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; data = planets[</p>
			<p class="source-code">...     ['semimajoraxis', 'period', 'mass', 'eccentricity']</p>
			<p class="source-code">... ].dropna()</p>
			<p class="source-code">&gt;&gt;&gt; planets_X = data[</p>
			<p class="source-code">...     ['semimajoraxis', 'mass', 'eccentricity']</p>
			<p class="source-code">... ]</p>
			<p class="source-code">&gt;&gt;&gt; planets_y = data.period</p>
			<p class="source-code">&gt;&gt;&gt; pl_X_train, pl_X_test, \</p>
			<p class="source-code">... pl_y_train, pl_y_test = train_test_split(</p>
			<p class="source-code">...     planets_X, planets_y, test_size=0.25, random_state=0</p>
			<p class="source-code">... )</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Remember that we will be working in dedicated notebooks for each of the datasets, so while the setup code is all in the same code block to make it easier to follow in the book, make sure to work in the notebook corresponding to the data in question.</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor219"/>Hyperparameter tuning with grid search</h1>
			<p>No doubt you have noticed that we can provide various parameters to the model classes when we <a id="_idIndexMarker1369"/>instantiate them. These model parameters are not derived from the data itself and are referred to as <strong class="bold">hyperparameters</strong>. Some examples of these are regularization terms, which we will discuss later <a id="_idIndexMarker1370"/>in this chapter, and weights. Through the process of <strong class="bold">model tuning</strong>, we seek to optimize our model's performance by tuning these hyperparameters.</p>
			<p>How can we <a id="_idIndexMarker1371"/>know we are picking the best values to optimize our model's performance? One way is to <a id="_idIndexMarker1372"/>use a technique called <strong class="bold">grid search</strong> to tune these hyperparameters. Grid search allows us to define a search space and test all combinations of hyperparameters in that space, keeping the ones that result in the best model. The scoring criterion we define will determine the best model.</p>
			<p>Remember the elbow point method we discussed in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>, for finding a good value for <em class="italic">k</em> in k-means clustering? We can employ a similar visual method to find the best value for our hyperparameters. This will involve <a id="_idIndexMarker1373"/>splitting our training data into <strong class="bold">training</strong> and <strong class="bold">validation sets</strong>. We need to <a id="_idIndexMarker1374"/>save the test set for the final evaluation of the model, so we use the validation set to test each of our models when searching for the best values of the hyperparameters. To reiterate, the validation set and the test set are not the same—they must be disjoint datasets. This split can be done with <strong class="source-inline">train_test_split()</strong>. Here, we will use the red wine quality dataset:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.model_selection import train_test_split</p>
			<p class="source-code">&gt;&gt;&gt; r_X_train_new, r_X_validate,\</p>
			<p class="source-code">... r_y_train_new, r_y_validate = train_test_split(</p>
			<p class="source-code">...     <strong class="bold">r_X_train, r_y_train, test_size=0.3,</strong> </p>
			<p class="source-code">...     random_state=0, <strong class="bold">stratify=r_y_train</strong></p>
			<p class="source-code">... )</p>
			<p>Then, we can build the model multiple times for all the values of the hyperparameters we want to test, and score them based on the metric that matters most to us. Let's try to find a <a id="_idIndexMarker1375"/>good value for <strong class="source-inline">C</strong>, the inverse of the regularization strength, which determines the weight of the penalty term for logistic regression and is discussed more in-depth in the <em class="italic">Regularization</em> section toward the end of this chapter; we tune this hyperparameter to reduce overfitting:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.metrics import f1_score</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler</p>
			<p class="source-code"># we will try 10 values from 10^-1 to 10^1 for C</p>
			<p class="source-code">&gt;&gt;&gt; inv_regularization_strengths = \</p>
			<p class="source-code">...     np.logspace(-1, 1, num=10)</p>
			<p class="source-code">&gt;&gt;&gt; scores = []</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">for inv_reg_strength in inv_regularization_strengths</strong>:</p>
			<p class="source-code">...     pipeline = Pipeline([</p>
			<p class="source-code">...         ('scale', MinMaxScaler()),</p>
			<p class="source-code">...         ('lr', LogisticRegression(</p>
			<p class="source-code">...             class_weight='balanced', random_state=0,</p>
			<p class="source-code">...             <strong class="bold">C=inv_reg_strength</strong></p>
			<p class="source-code">...         ))</p>
			<p class="source-code">...     ]).fit(r_X_train_new, r_y_train_new)</p>
			<p class="source-code">...     <strong class="bold">scores.append(f1_score(</strong></p>
			<p class="source-code">...         <strong class="bold">pipeline.predict(r_X_validate), r_y_validate</strong></p>
			<p class="source-code">...     <strong class="bold">))</strong></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Here, we are using <strong class="source-inline">np.logspace()</strong> to get our range of values to try for <strong class="source-inline">C</strong>. To use this function, we supply starting and stopping exponents to use with a base number (10, by default). So <strong class="source-inline">np.logspace(-1, 1, num=10)</strong> gives us 10 evenly spaced numbers between 10<span class="superscript">-1</span> and 10<span class="superscript">1</span>.</p>
			<p>This is <a id="_idIndexMarker1376"/>then plotted as follows:</p>
			<p class="source-code">&gt;&gt;&gt; plt.plot(inv_regularization_strengths, scores, 'o-')</p>
			<p class="source-code">&gt;&gt;&gt; plt.xscale('log')</p>
			<p class="source-code">&gt;&gt;&gt; plt.xlabel('inverse of regularization strength (C)')</p>
			<p class="source-code">&gt;&gt;&gt; plt.ylabel(r'$F_1$ score')</p>
			<p class="source-code">&gt;&gt;&gt; plt.title(</p>
			<p class="source-code">...     r'$F_1$ score vs. '</p>
			<p class="source-code">...     'Inverse of Regularization Strength'</p>
			<p class="source-code">... )</p>
			<p>Using the resulting plot, we can pick the value that maximizes our performance:</p>
			<div>
				<div id="_idContainer429" class="IMG---Figure">
					<img src="image/Figure_10.1_B16834.jpg" alt="Figure 10.1 – Searching for the best hyperparameters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Searching for the best hyperparameters</p>
			<p>Scikit-learn provides the <strong class="source-inline">GridSearchCV</strong> class in the <strong class="source-inline">model_selection</strong> module for carrying out <a id="_idIndexMarker1377"/>this exhaustive search much more easily. Classes that end with <em class="italic">CV</em> utilize <strong class="bold">cross-validation</strong>, meaning they divide up the <a id="_idIndexMarker1378"/>training data into subsets, some of which will be the validation set for scoring the model (without needing the testing data until after the model is fit).</p>
			<p>One common <a id="_idIndexMarker1379"/>method of cross-validation is <strong class="bold">k-fold cross-validation</strong>, which splits the training data into <em class="italic">k</em> subsets and will train the model <em class="italic">k</em> times, each time leaving one subset out to use as the validation set. The score for the model will be the average across the <em class="italic">k</em> validation sets. Our initial attempt was 1-fold cross-validation. When <em class="italic">k</em>=3, this process looks like the following diagram:</p>
			<div>
				<div id="_idContainer430" class="IMG---Figure">
					<img src="image/Figure_10.2_B16834.jpg" alt="Figure 10.2 – Understanding k-fold cross-validation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Understanding k-fold cross-validation</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">When working with classification problems, <strong class="source-inline">scikit-learn</strong> will implement stratified k-fold cross-validation. This ensures that the percentage of samples belonging to each class will be preserved across folds. Without stratification, it's possible some validation sets will see a disproportionately low (or high) amount of a given class, which can distort the results.</p>
			<p><strong class="source-inline">GridSearchCV</strong> uses cross-validation to find the best hyperparameters in the search space, without the need to use the testing data. Remember, test data should not influence the training process in any way—neither when training the model nor when tuning hyperparameters—otherwise, the model will have issues generalizing. This happens because <a id="_idIndexMarker1380"/>we would be picking the hyperparameters that give the best performance on the test set, thus leaving no way to test on unseen data, and overestimating our performance.</p>
			<p>In order to use <strong class="source-inline">GridSearchCV</strong>, we need to provide a model (or pipeline) and a search space, which will be a dictionary mapping the hyperparameter to tune (by name) to a list of values to try. Optionally, we can provide a scoring metric to use, as well as the number of folds to use with cross-validation. We can tune any step in the pipeline by prefixing the hyperparameter name with the name of that step, followed by two underscores. For instance, if we have a logistic regression step called <strong class="source-inline">lr</strong> and want to tune <strong class="source-inline">C</strong>, we use <strong class="source-inline">lr__C</strong> as the key in the search space dictionary. Note that if our model has any preprocessing steps, it's imperative that we use a pipeline.</p>
			<p>Let's use <strong class="source-inline">GridSearchCV</strong> for the red wine quality logistic regression, searching for whether or not to fit our model with an intercept and the best value for the inverse of the regularization strength (<strong class="source-inline">C</strong>). We will use the F<span class="subscript">1</span> score macro average as the scoring metric. Note that, due to the consistency of the API, <strong class="source-inline">GridSearchCV</strong> can be used to score, fit, and predict with the same methods as the underlying models. By default, the grid search will run in series, but <strong class="source-inline">GridSearchCV</strong> is capable of performing multiple searches in parallel, greatly speeding up this process:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.model_selection import GridSearchCV</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler</p>
			<p class="source-code">&gt;&gt;&gt; pipeline = Pipeline([</p>
			<p class="source-code">...     ('scale', MinMaxScaler()),</p>
			<p class="source-code">...     ('lr', LogisticRegression(class_weight='balanced',</p>
			<p class="source-code">...                               random_state=0))</p>
			<p class="source-code">... ])</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">search_space = {</strong></p>
			<p class="source-code">...     <strong class="bold">'lr__C': np.logspace(-1, 1, num=10),</strong></p>
			<p class="source-code">...     <strong class="bold">'lr__fit_intercept': [True, False]</strong></p>
			<p class="source-code">... <strong class="bold">}</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">lr_grid = GridSearchCV(</strong></p>
			<p class="source-code">...     <strong class="bold">pipeline, search_space, scoring='f1_macro', cv=5</strong></p>
			<p class="source-code">... <strong class="bold">)</strong>.fit(r_X_train, r_y_train)</p>
			<p>Once the grid <a id="_idIndexMarker1381"/>search completes, we can isolate the best hyperparameters from the search space with the <strong class="source-inline">best_params_</strong> attribute. Notice that this result is different from our 1-fold cross-validation attempt because each of the folds has been averaged together to find the best hyperparameters overall, not just for a single fold:</p>
			<p class="source-code"># best values of `C` and `fit_intercept` in search space</p>
			<p class="source-code">&gt;&gt;&gt; lr_grid.<strong class="bold">best_params_</strong></p>
			<p class="source-code">{'lr__C': 3.593813663804626, 'lr__fit_intercept': True}</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can also retrieve the best version of the pipeline from the grid search with the <strong class="source-inline">best_estimator_</strong> attribute. If we want to see the score the best estimator (model) had, we can grab it from the <strong class="source-inline">best_score_</strong> attribute; note that this will be the score we specified with the <strong class="source-inline">scoring</strong> argument.</p>
			<p>Our F<span class="subscript">1</span> score <a id="_idIndexMarker1382"/>macro average is now higher than what we achieved in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(</p>
			<p class="source-code">...     r_y_test, lr_grid.predict(r_X_test)</p>
			<p class="source-code">... ))</p>
			<p class="source-code">              precision    recall  <strong class="bold">f1-score</strong>   support</p>
			<p class="source-code">           0       0.94      0.80      0.87       138</p>
			<p class="source-code">           1       0.36      0.68      0.47        22</p>
			<p class="source-code">    accuracy                           0.79       160</p>
			<p class="source-code">   <strong class="bold">macro avg</strong>       0.65      0.74      <strong class="bold">0.67</strong>       160</p>
			<p class="source-code">weighted avg       0.86      0.79      0.81       160</p>
			<p>Note that the <strong class="source-inline">cv</strong> argument doesn't have to be an integer—we can provide one of the splitter <a id="_idIndexMarker1383"/>classes mentioned at <a href="https://scikit-learn.org/stable/modules/classes.html#splitter-classes">https://scikit-learn.org/stable/modules/classes.html#splitter-classes</a> if we want to use a method other than the default of k-fold for regression or stratified k-fold for classification. For example, when working with time series, we can use <strong class="source-inline">TimeSeriesSplit</strong> as the cross-validation object to work with successive samples and avoid shuffling. Scikit-learn shows how the cross-validation classes compare at <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html">https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html</a>.</p>
			<p>Let's test <a id="_idIndexMarker1384"/>out <strong class="source-inline">RepeatedStratifiedKFold</strong> on the red wine quality model instead of the default <strong class="source-inline">StratifiedKFold</strong>, which will repeat the stratified k-fold cross-validation 10 times by default. All we have to do is change what we passed in as <strong class="source-inline">cv</strong> in the first <strong class="source-inline">GridSearchCV</strong> example to be a <strong class="source-inline">RepeatedStratifiedKFold</strong> object. Note that—despite <a id="_idIndexMarker1385"/>using the same pipeline, search space, and scoring metric—we have different values for <strong class="source-inline">best_params_</strong> because our cross-validation process has changed:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.model_selection import RepeatedStratifiedKFold</strong></p>
			<p class="source-code">&gt;&gt;&gt; lr_grid = GridSearchCV(</p>
			<p class="source-code">...     pipeline, search_space, scoring='f1_macro', </p>
			<p class="source-code">...     <strong class="bold">cv=RepeatedStratifiedKFold(random_state=0)</strong></p>
			<p class="source-code">... ).fit(r_X_train, r_y_train)</p>
			<p class="source-code">&gt;&gt;&gt; print('Best parameters (CV score=%.2f):\n    %s' % (</p>
			<p class="source-code">...     lr_grid.best_score_, lr_grid.best_params_</p>
			<p class="source-code">... )) # f1 macro score</p>
			<p class="source-code">Best parameters (CV score=0.69): </p>
			<p class="source-code">    {'lr__C': 5.994842503189409, 'lr__fit_intercept': True}</p>
			<p>In addition to cross-validation, <strong class="source-inline">GridSearchCV</strong> allows us to specify the metric we want to optimize with the <strong class="source-inline">scoring</strong> parameter. This can be a string for the name of the score (as in the previous code blocks), provided that it is in the list at <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values">https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values</a>; otherwise, we can either pass the function itself or make our own using the <strong class="source-inline">make_scorer()</strong> function from <strong class="source-inline">sklearn.metrics</strong>. We can even provide a dictionary of scorers (in the form of <strong class="source-inline">{name: function}</strong>) for grid search, provided that we specify which one we want to use for optimization by passing its name to the <strong class="source-inline">refit</strong> parameter. Therefore, we can use grid search to find the hyperparameters that help us maximize our performance on the metrics we discussed in the previous chapter.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The time it takes to train our model should also be something we evaluate and look to optimize. If it takes us double the training time to get one more correct classification, it's probably not worth it. If we have a <strong class="source-inline">GridSearchCV</strong> object called <strong class="source-inline">grid</strong>, we can see the average fit time by running <strong class="source-inline">grid.cv_results_['mean_fit_time']</strong>.</p>
			<p>We can use <strong class="source-inline">GridSearchCV</strong> to search for the best parameters for any step in our pipeline. For example, let's use grid search with a pipeline of preprocessing and linear regression <a id="_idIndexMarker1386"/>on the planets data (similar to when we <a id="_idIndexMarker1387"/>modeled planet year length in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>) while minimizing <strong class="bold">mean absolute error</strong> (<strong class="bold">MAE</strong>) instead of the default R<span class="superscript">2</span>:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LinearRegression</p>
			<p class="source-code">&gt;&gt;&gt;<strong class="bold"> from sklearn.metrics import \</strong></p>
			<p class="source-code">...<strong class="bold">     make_scorer, mean_squared_error</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.model_selection import GridSearchCV</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; model_pipeline = Pipeline([</p>
			<p class="source-code">...     ('scale', StandardScaler()),</p>
			<p class="source-code">...     ('lr', LinearRegression())</p>
			<p class="source-code">... ])</p>
			<p class="source-code">&gt;&gt;&gt; search_space = {</p>
			<p class="source-code">...     <strong class="bold">'scale__with_mean': [True, False],</strong></p>
			<p class="source-code">...     <strong class="bold">'scale__with_std': [True, False],</strong></p>
			<p class="source-code">...     'lr__fit_intercept': [True, False], </p>
			<p class="source-code">...     'lr__normalize': [True, False]</p>
			<p class="source-code">... }</p>
			<p class="source-code">&gt;&gt;&gt; grid = GridSearchCV(</p>
			<p class="source-code">...     model_pipeline, search_space, cv=5,</p>
			<p class="source-code">...     <strong class="bold">scoring={</strong></p>
			<p class="source-code">...         <strong class="bold">'r_squared': 'r2',</strong></p>
			<p class="source-code">...         <strong class="bold">'mse': 'neg_mean_squared_error',</strong></p>
			<p class="source-code">...         <strong class="bold">'mae': 'neg_mean_absolute_error',</strong></p>
			<p class="source-code">...         <strong class="bold">'rmse': make_scorer(</strong></p>
			<p class="source-code">...<strong class="bold">             lambda x, y: \</strong></p>
			<p class="source-code">...<strong class="bold">                 -np.sqrt(mean_squared_error(x, y))</strong></p>
			<p class="source-code">...<strong class="bold">         )</strong></p>
			<p class="source-code">...     <strong class="bold">}, refit='mae'</strong></p>
			<p class="source-code">... ).fit(pl_X_train, pl_y_train)</p>
			<p>Note that <a id="_idIndexMarker1388"/>we are using the negative of all the metrics except R<span class="superscript">2</span>. This is because <strong class="source-inline">GridSearchCV</strong> will attempt to maximize the score, and we want to minimize our errors. Let's check the best parameters for the scaling and linear regression in this grid:</p>
			<p class="source-code">&gt;&gt;&gt; print('Best parameters (CV score=%.2f):\n%s' % (</p>
			<p class="source-code">...     grid.best_score_, grid.best_params_</p>
			<p class="source-code">... )) # MAE score * -1</p>
			<p class="source-code">Best parameters (CV score=-1215.99):</p>
			<p class="source-code">{'lr__fit_intercept': False, 'lr__normalize': True, </p>
			<p class="source-code"> 'scale__with_mean': False, 'scale__with_std': True}</p>
			<p>The tuned model's MAE is more than 120 Earth days smaller than the MAE we got in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import mean_absolute_error</p>
			<p class="source-code">&gt;&gt;&gt; mean_absolute_error(pl_y_test, grid.predict(pl_X_test))</p>
			<p class="source-code">1248.3690943844194</p>
			<p>It's important to note that while a model may be fast to train, we shouldn't create a large, granular search space; in practice, it's better to start with a few different spread-out values, and then examine the results to see which areas warrant a more in-depth search. For instance, say we are looking to tune the <strong class="source-inline">C</strong> hyperparameter. On our first pass, we may look at the result of <strong class="source-inline">np.logspace(-1, 1)</strong>. If we see that the best value for <strong class="source-inline">C</strong> is at either <a id="_idIndexMarker1389"/>end of the spectrum, we can then look at values above/below the value. If the best value is in the range, we may look at a few values around it. This process can be performed iteratively until we don't see additional improvement. Alternatively, we could use <strong class="source-inline">RandomizedSearchCV</strong>, which will try 10 random combinations in the search space (by default) and find the best estimator (model). We can change this number with the <strong class="source-inline">n_iter</strong> argument.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Since the process of tuning hyperparameters requires us to train our model multiple times, we must consider the time complexity of our models. Models that take a long time to train will be very costly to use with cross-validation. This will likely cause us to shrink our search space.</p>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor220"/>Feature engineering</h1>
			<p>When trying to <a id="_idIndexMarker1390"/>improve performance, we may also consider <a id="_idIndexMarker1391"/>ways to provide the best <strong class="bold">features</strong> (model inputs) to our model through the process of <strong class="bold">feature engineering</strong>. The <em class="italic">Preprocessing data</em> section in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>, introduced us to <strong class="bold">feature transformation</strong> when we scaled, encoded, and imputed our data. Unfortunately, feature transformation may mute some elements of our data that we want to use in our model, such as the unscaled value of the mean of a specific feature. For this situation, we can create a new feature with this value; this and other new features <a id="_idIndexMarker1392"/>are added during <strong class="bold">feature construction</strong> (sometimes <a id="_idIndexMarker1393"/>called <strong class="bold">feature creation</strong>).</p>
			<p><strong class="bold">Feature selection</strong> is the <a id="_idIndexMarker1394"/>process of determining which features to train the model on. This can be done manually or through another process, such as machine learning. When looking to choose features for our model, we want features that have an impact on our dependent variable without unnecessarily increasing the complexity of our problem. Models built with many features increase in complexity, but also, unfortunately, have a higher tendency to fit noise, because our data is sparse in <a id="_idIndexMarker1395"/>such a high-dimensional space. This is referred to as the <strong class="bold">curse of dimensionality</strong>. When a model has learned the noise in the training data, it will have a hard time generalizing to unseen data; this is called <strong class="bold">overfitting</strong>. By restricting <a id="_idIndexMarker1396"/>the number of features the model uses, feature selection can help address overfitting.</p>
			<p><strong class="bold">Feature extraction</strong> is another <a id="_idIndexMarker1397"/>way we can address the curse of dimensionality. During feature extraction, we reduce the dimensionality of our data by constructing combinations of features through a transformation. These new features can be used in place of the originals, thereby reducing the dimensionality of the problem. This <a id="_idIndexMarker1398"/>process, called <strong class="bold">dimensionality reduction</strong>, also includes techniques where we find a certain number of components (less than the original) that <a id="_idIndexMarker1399"/>explain most of the variance in the data. Feature extraction is often used in image recognition problems, since the dimensionality of the task is the total number of pixels in the image. For instance, square ads on websites are 350x350 pixels (this is one of the most common sizes), so an image recognition task using images that size has 122,500 dimensions.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Thorough EDA and domain knowledge are a must for feature engineering.</p>
			<p>Feature engineering is the subject of entire books; however, as it is a more advanced topic, we will go over just a few techniques in this section. There is a good book on the subject in the <em class="italic">Further reading</em> section, which also touches upon using machine learning for feature learning.</p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor221"/>Interaction terms and polynomial features</h2>
			<p>We discussed the use of dummy variables back in the <em class="italic">Preprocessing data</em> section of <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>; however, we merely considered the effect of that variable on its own. In our model that tries to predict red wine <a id="_idIndexMarker1400"/>quality using chemical properties, we are considering each property separately. However, it is important <a id="_idIndexMarker1401"/>to consider whether the interaction between these properties has an effect. Perhaps when the levels of citric acid and fixed acidity are both high or both low, the wine quality is different than if one is high and one is low. In order to capture the effect of this, we need to add an <strong class="bold">interaction term</strong>, which will be the product of the features.</p>
			<p>We may also <a id="_idIndexMarker1402"/>be interested in increasing the effect of a feature in the model through feature construction; we can achieve this by adding <strong class="bold">polynomial features</strong> made from this feature. This involves adding higher degrees of the original feature, so we could have <em class="italic">citric acid</em>, <em class="italic">citric acid</em><span class="superscript">2</span>, <em class="italic">citric acid</em><span class="superscript">3</span>, and so on in the model.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can generalize linear models by using interaction terms and polynomial features because they allow us to model the linear relationship of non-linear terms. Since linear models tend to underperform in the presence of multiple or non-linear decision boundaries (the surface or hypersurface that separates the classes), this can improve performance.</p>
			<p>Scikit-learn provides the <strong class="source-inline">PolynomialFeatures</strong> class in the <strong class="source-inline">preprocessing</strong> module for easily creating interaction terms and polynomial features. This comes in handy when building models with categorical and continuous features. By specifying just the degree, we can get every combination of the features less than or equal to the degree. High degrees will increase model complexity greatly and may lead to overfitting.</p>
			<p>If we use <strong class="source-inline">degree=2</strong>, we can turn <em class="italic">citric acid </em>and<em class="italic"> fixed acidity</em> into the following, where <em class="italic">1</em> is the bias term that can be used in a model as an intercept term: </p>
			<div>
				<div id="_idContainer431" class="IMG---Figure">
					<img src="image/Formula_10_001.jpg" alt=""/>
				</div>
			</div>
			<p>By calling the <strong class="source-inline">fit_transform()</strong> method on the <strong class="source-inline">PolynomialFeatures</strong> object, we <a id="_idIndexMarker1403"/>can generate these features:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.preprocessing import PolynomialFeatures</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">PolynomialFeatures(degree=2)</strong>.fit_transform(</p>
			<p class="source-code">...     r_X_train[['citric acid', 'fixed acidity']]</p>
			<p class="source-code">... )</p>
			<p class="source-code">array([<strong class="bold">[1.000e+00, 5.500e-01, 9.900e+00, 3.025e-01, </strong></p>
			<p class="source-code"><strong class="bold">        5.445e+00, 9.801e+01],</strong></p>
			<p class="source-code">       [1.000e+00, 4.600e-01, 7.400e+00, 2.116e-01, </p>
			<p class="source-code">        3.404e+00, 5.476e+01],</p>
			<p class="source-code">       [1.000e+00, 4.100e-01, 8.900e+00, 1.681e-01, </p>
			<p class="source-code">        3.649e+00, 7.921e+01],</p>
			<p class="source-code">       ...,</p>
			<p class="source-code">       [1.000e+00, 1.200e-01, 7.000e+00, 1.440e-02, </p>
			<p class="source-code">        8.400e-01, 4.900e+01],</p>
			<p class="source-code">       [1.000e+00, 3.100e-01, 7.600e+00, 9.610e-02, </p>
			<p class="source-code">        2.356e+00, 5.776e+01],</p>
			<p class="source-code">       [1.000e+00, 2.600e-01, 7.700e+00, 6.760e-02, </p>
			<p class="source-code">        2.002e+00, 5.929e+01]])</p>
			<p>Let's dissect <a id="_idIndexMarker1404"/>the first row of our array in the previous code block (highlighted in bold) to understand how we got each of these values:</p>
			<div>
				<div id="_idContainer432" class="IMG---Figure">
					<img src="image/Figure_10.3_B16834.jpg" alt="Figure 10.3 – Examining the interaction terms and polynomial features created&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 10.3 – Examining the interaction terms and polynomial features created</p>
			<p>If we are only interested in the interaction variables (<em class="italic">citric acid × fixed acidity</em>, here), we can specify <strong class="source-inline">interaction_only=True</strong>. In this case, we also don't want the bias term, so we specify <strong class="source-inline">include_bias=False</strong> as well. This will give us the original variables along with their interaction term(s):</p>
			<p class="source-code">&gt;&gt;&gt; PolynomialFeatures(</p>
			<p class="source-code">...     degree=2, <strong class="bold">include_bias=False, interaction_only=True</strong></p>
			<p class="source-code">... ).fit_transform(</p>
			<p class="source-code">...     r_X_train[['citric acid', 'fixed acidity']]</p>
			<p class="source-code">... )</p>
			<p class="source-code">array([[0.55 , 9.9  , 5.445],</p>
			<p class="source-code">       [0.46 , 7.4  , 3.404],</p>
			<p class="source-code">       [0.41 , 8.9  , 3.649],</p>
			<p class="source-code">       ...,</p>
			<p class="source-code">       [0.12 , 7.   , 0.84 ],</p>
			<p class="source-code">       [0.31 , 7.6  , 2.356],</p>
			<p class="source-code">       [0.26 , 7.7  , 2.002]])</p>
			<p>We can <a id="_idIndexMarker1405"/>add these polynomial features to our pipeline:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.preprocessing import (</strong></p>
			<p class="source-code">...     <strong class="bold">MinMaxScaler, PolynomialFeatures</strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p class="source-code">&gt;&gt;&gt; pipeline = Pipeline([</p>
			<p class="source-code">...     <strong class="bold">('poly', PolynomialFeatures(degree=2))</strong>,</p>
			<p class="source-code">...     ('scale', MinMaxScaler()),</p>
			<p class="source-code">...     ('lr', LogisticRegression(</p>
			<p class="source-code">...         class_weight='balanced', random_state=0</p>
			<p class="source-code">...     ))</p>
			<p class="source-code">... ]).fit(r_X_train, r_y_train)</p>
			<p>Note that this model is slightly better than before we added these additional terms, which was the model used in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; preds = pipeline.predict(r_X_test)</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(r_y_test, preds))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           0       0.95      0.79      0.86       138</p>
			<p class="source-code">           1       0.36      0.73      0.48        22</p>
			<p class="source-code">    accuracy                           0.78       160</p>
			<p class="source-code">   <strong class="bold">macro avg       0.65      0.76      0.67       160</strong></p>
			<p class="source-code">weighted avg       0.87      0.78      0.81       160</p>
			<p>Adding polynomial features and interaction terms increases the dimensionality of our data, which <a id="_idIndexMarker1406"/>may not be desirable. Sometimes, rather than looking to create more features, we look for ways to consolidate them and reduce the dimensionality of our data.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor222"/>Dimensionality reduction</h2>
			<p><strong class="bold">Dimensionality reduction</strong> shrinks the number of features we train our model on. This is done to <a id="_idIndexMarker1407"/>reduce the computational complexity of training <a id="_idIndexMarker1408"/>the model without sacrificing much performance. We could just choose to train on a subset of the features (feature selection); however, if we think there is value in those features, albeit small, we may look for ways to extract the information we need from them.</p>
			<p>One common strategy for feature selection is to discard features with low variance. These features aren't very informative since they are mostly the same value throughout the data. Scikit-learn provides the <strong class="source-inline">VarianceThreshold</strong> class for carrying out feature selection according to a minimum variance threshold. By default, it will discard any features that have zero variance; however, we can provide our own threshold. Let's perform feature selection on our model that predicts whether a wine is red or white based on its chemical composition. Since we have no features with zero variance, we will choose to keep features whose variance is greater than 0.01:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.feature_selection import VarianceThreshold</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; white_or_red_min_var = Pipeline([</p>
			<p class="source-code">...<strong class="bold">     ('feature_selection',</strong></p>
			<p class="source-code">...<strong class="bold">      VarianceThreshold(threshold=0.01)), </strong></p>
			<p class="source-code">...     ('scale', StandardScaler()), </p>
			<p class="source-code">...     ('lr', LogisticRegression(random_state=0))</p>
			<p class="source-code">... ]).fit(w_X_train, w_y_train)</p>
			<p>This removed <a id="_idIndexMarker1409"/>two features with low variance. We can get their names with the Boolean mask returned by the <strong class="source-inline">VarianceThreshold</strong> object's <strong class="source-inline">get_support()</strong> method, which indicates the features that were kept:</p>
			<p class="source-code">&gt;&gt;&gt; w_X_train.columns[</p>
			<p class="source-code">...     <strong class="bold">~white_or_red_min_var.named_steps[</strong></p>
			<p class="source-code">...         <strong class="bold">'feature_selection'</strong></p>
			<p class="source-code">...     <strong class="bold">].get_support()</strong></p>
			<p class="source-code">... ]</p>
			<p class="source-code">Index(<strong class="bold">['chlorides', 'density']</strong>, dtype='object')</p>
			<p>Using only 9 of the 11 features, our performance hasn't been affected much:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(</p>
			<p class="source-code">...     w_y_test, white_or_red_min_var.predict(w_X_test)</p>
			<p class="source-code">... ))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           0       0.98      0.99      0.99      1225</p>
			<p class="source-code">           1       0.98      0.95      0.96       400</p>
			<p class="source-code">    accuracy                           0.98      1625</p>
			<p class="source-code"><strong class="bold">   macro avg       0.98      0.97      0.97      1625</strong></p>
			<p class="source-code">weighted avg       0.98      0.98      0.98      1625</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Check out <a id="_idIndexMarker1410"/>the other feature selection options in the <strong class="source-inline">feature_selection</strong> module at <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection</a>.</p>
			<p>If we believe <a id="_idIndexMarker1411"/>there is value in all the features, we may decide to use feature extraction rather than discarding them entirely. <strong class="bold">Principal component analysis</strong> (<strong class="bold">PCA</strong>) performs feature extraction by projecting <a id="_idIndexMarker1412"/>high-dimensional data into lower dimensions, thereby reducing the dimensionality. In return, we get the <em class="italic">n</em> components that maximize explained variance. This will be sensitive to the scale of the data, so we need to do some preprocessing beforehand.</p>
			<p>Let's take a look at the <strong class="source-inline">pca_scatter()</strong> function in the <strong class="source-inline">ml_utils.pca</strong> module, which will help us visualize our data when reduced to two dimensions:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code"><strong class="bold">from sklearn.decomposition import PCA</strong></p>
			<p class="source-code">from sklearn.pipeline import Pipeline</p>
			<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p>
			<p class="source-code">def pca_scatter(X, labels, cbar_label, cmap='brg'):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Create a 2D scatter plot from 2 PCA components of X</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - X: The X data for PCA</p>
			<p class="source-code">        - labels: The y values</p>
			<p class="source-code">        - cbar_label: The label for the colorbar</p>
			<p class="source-code">        - cmap: Name of the colormap to use.</p>
			<p class="source-code">    Returns:</p>
			<p class="source-code">        Matplotlib `Axes` object</p>
			<p class="source-code">    """</p>
			<p class="source-code">    <strong class="bold">pca = Pipeline([</strong></p>
			<p class="source-code"><strong class="bold">        ('scale', MinMaxScaler()),</strong></p>
			<p class="source-code"><strong class="bold">        ('pca', PCA(2, random_state=0))</strong></p>
			<p class="source-code"><strong class="bold">    ]).fit(X)</strong></p>
			<p class="source-code">    data, classes = <strong class="bold">pca.transform(X)</strong>, np.unique(labels)</p>
			<p class="source-code">    ax = plt.scatter(</p>
			<p class="source-code">        data[:, 0], data[:, 1],</p>
			<p class="source-code">        c=labels, edgecolor='none', alpha=0.5,</p>
			<p class="source-code">        cmap=plt.cm.get_cmap(cmap, classes.shape[0])</p>
			<p class="source-code">    )</p>
			<p class="source-code">    plt.xlabel('component 1')</p>
			<p class="source-code">    plt.ylabel('component 2')</p>
			<p class="source-code">    cbar = plt.colorbar()</p>
			<p class="source-code">    cbar.set_label(cbar_label)</p>
			<p class="source-code">    cbar.set_ticks(classes)</p>
			<p class="source-code">    plt.legend([</p>
			<p class="source-code">        'explained variance\n'</p>
			<p class="source-code">        'comp. 1: {:.3}\ncomp. 2: {:.3}'.format(</p>
			<p class="source-code">           <strong class="bold">*pca.named_steps['pca'].explained_variance_ratio_</strong></p>
			<p class="source-code">        ) </p>
			<p class="source-code">    ])</p>
			<p class="source-code">    return ax</p>
			<p>Let's <a id="_idIndexMarker1413"/>visualize the wine data with two PCA components to see if there is a way to separate red from white:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.pca import pca_scatter</p>
			<p class="source-code">&gt;&gt;&gt; pca_scatter(wine_X, wine_y, 'wine is red?')</p>
			<p class="source-code">&gt;&gt;&gt; plt.title('Wine Kind PCA (2 components)')</p>
			<p>Most of the red wines are in the bright green mass of points at the top, and the white wines are in the blue point mass at the bottom. Visually, we can see how to separate them, but there is still some overlap:</p>
			<div>
				<div id="_idContainer433" class="IMG---Figure">
					<img src="image/Figure_10.4_B16834.jpg" alt="Figure 10.4 – Using two PCA components to separate wines by type&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Using two PCA components to separate wines by type</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">PCA components will be linearly uncorrelated, since they were obtained through an orthogonal transformation (perpendicularity extended to higher dimensions). Linear regression assumes the regressors (input data) are not correlated, so this can help address multicollinearity.</p>
			<p>Note the <a id="_idIndexMarker1414"/>explained variances of each component from the previous plot's legend—the components explain over 50% of the variance in the wine data. Let's see if using three dimensions improves the separation. The <strong class="source-inline">pca_scatter_3d()</strong> function in the <strong class="source-inline">ml_utils.pca</strong> module uses <strong class="source-inline">mpl_toolkits</strong>, which comes with <strong class="source-inline">matplotlib</strong> for 3D visualizations:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code"><strong class="bold">from mpl_toolkits.mplot3d import Axes3D</strong></p>
			<p class="source-code">from sklearn.decomposition import PCA</p>
			<p class="source-code">from sklearn.pipeline import Pipeline</p>
			<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p>
			<p class="source-code">def pca_scatter_3d(X, labels, cbar_label, cmap='brg', </p>
			<p class="source-code">                   elev=10, azim=15):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Create a 3D scatter plot from 3 PCA components of X</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - X: The X data for PCA</p>
			<p class="source-code">        - labels: The y values</p>
			<p class="source-code">        - cbar_label: The label for the colorbar</p>
			<p class="source-code">        - cmap: Name of the colormap to use.</p>
			<p class="source-code">        - elev: The degrees of elevation to view the plot from. </p>
			<p class="source-code">        - azim: The azimuth angle on the xy plane (rotation </p>
			<p class="source-code">                around the z-axis).</p>
			<p class="source-code">    Returns:</p>
			<p class="source-code">        Matplotlib `Axes` object</p>
			<p class="source-code">    """</p>
			<p class="source-code">    pca = Pipeline([</p>
			<p class="source-code">        ('scale', MinMaxScaler()),</p>
			<p class="source-code">        ('pca', PCA(3, random_state=0))</p>
			<p class="source-code">    ]).fit(X)</p>
			<p class="source-code">    data, classes = pca.transform(X), np.unique(labels)</p>
			<p class="source-code">    fig = plt.figure()</p>
			<p class="source-code">    ax = fig.add_subplot(111, <strong class="bold">projection='3d'</strong>)</p>
			<p class="source-code">    p = <strong class="bold">ax.scatter3D(</strong></p>
			<p class="source-code"><strong class="bold">        data[:, 0], data[:, 1], data[:, 2],</strong></p>
			<p class="source-code"><strong class="bold">        alpha=0.5, c=labels,</strong></p>
			<p class="source-code"><strong class="bold">        cmap=plt.cm.get_cmap(cmap, classes.shape[0])</strong></p>
			<p class="source-code"><strong class="bold">    )</strong></p>
			<p class="source-code">    ax.view_init(elev=elev, azim=azim)</p>
			<p class="source-code">    ax.set_xlabel('component 1')</p>
			<p class="source-code">    ax.set_ylabel('component 2')</p>
			<p class="source-code">    ax.set_zlabel('component 3')</p>
			<p class="source-code">    cbar = fig.colorbar(p, pad=0.1)</p>
			<p class="source-code">    cbar.set_ticks(classes)</p>
			<p class="source-code">    cbar.set_label(cbar_label)</p>
			<p class="source-code">    plt.legend([</p>
			<p class="source-code">        'explained variance\ncomp. 1: {:.3}\n'</p>
			<p class="source-code">        'comp. 2: {:.3}\ncomp. 3: {:.3}'.format(</p>
			<p class="source-code">            *pca.named_steps['pca'].explained_variance_ratio_</p>
			<p class="source-code">        ) </p>
			<p class="source-code">    ])</p>
			<p class="source-code">    return ax</p>
			<p>Let's use <a id="_idIndexMarker1415"/>our 3D visualization function on the wine data again to see if white and red are easier to separate with three PCA components:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.pca import pca_scatter_3d</p>
			<p class="source-code">&gt;&gt;&gt; pca_scatter_3d(</p>
			<p class="source-code">...     wine_X, wine_y, 'wine is red?', elev=20, azim=-10</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; plt.suptitle('Wine Type PCA (3 components)')</p>
			<p>It seems like we could slice off the green (right) point mass from this angle, although we still have a few points in the wrong section:</p>
			<div>
				<div id="_idContainer434" class="IMG---Figure">
					<img src="image/Figure_10.5_B16834.jpg" alt="Figure 10.5 – Using three PCA components to separate wines by type&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Using three PCA components to separate wines by type</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">PCA performs linear dimensionality reduction. Check out t-SNE and Isomap to perform manifold learning for non-linear dimensionality reduction.</p>
			<p>We can <a id="_idIndexMarker1416"/>use the <strong class="source-inline">pca_explained_variance_plot()</strong> function from the <strong class="source-inline">ml_utils.pca</strong> module to visualize the cumulative explained variance as a function of the number of PCA components:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">def pca_explained_variance_plot(pca_model, ax=None):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Plot the cumulative explained variance of PCA components.</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - pca_model: The PCA model that has been fit already</p>
			<p class="source-code">        - ax: Matplotlib `Axes` object to plot on.</p>
			<p class="source-code">    Returns:</p>
			<p class="source-code">        A matplotlib `Axes` object</p>
			<p class="source-code">    """</p>
			<p class="source-code">    if not ax:</p>
			<p class="source-code">        fig, ax = plt.subplots()</p>
			<p class="source-code">    ax.plot(</p>
			<p class="source-code">        np.append(</p>
			<p class="source-code">            0, pca_model.<strong class="bold">explained_variance_ratio_.cumsum()</strong></p>
			<p class="source-code">        ), 'o-'</p>
			<p class="source-code">    )</p>
			<p class="source-code">    ax.set_title(</p>
			<p class="source-code">        'Total Explained Variance Ratio for PCA Components'</p>
			<p class="source-code">    )</p>
			<p class="source-code">    ax.set_xlabel('PCA components used')</p>
			<p class="source-code">    ax.set_ylabel('cumulative explained variance ratio')</p>
			<p class="source-code">    return ax</p>
			<p>We can <a id="_idIndexMarker1417"/>pass the PCA part of our pipeline to this function in order to see the cumulative explained variance:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.decomposition import PCA</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from ml_utils.pca import pca_explained_variance_plot</strong></p>
			<p class="source-code">&gt;&gt;&gt; pipeline = Pipeline([</p>
			<p class="source-code">...     ('normalize', MinMaxScaler()),</p>
			<p class="source-code">...     ('pca', PCA(8, random_state=0))</p>
			<p class="source-code">... ]).fit(w_X_train, w_y_train) </p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">pca_explained_variance_plot(pipeline.named_steps['pca'])</strong></p>
			<p>The first four PCA components explain about 80% of the variance:</p>
			<div>
				<div id="_idContainer435" class="IMG---Figure">
					<img src="image/Figure_10.6_B16834.jpg" alt="Figure 10.6 – Explained variance for PCA components used&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Explained variance for PCA components used</p>
			<p>We can also use the elbow point method to find a good value for the number of PCA components <a id="_idIndexMarker1418"/>to use, just as we did with k-means in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>. For this, we need to make a <strong class="bold">scree plot</strong>, which shows the explained variance for each component. The <strong class="source-inline">ml_utils.pca</strong> module has the <strong class="source-inline">pca_scree_plot()</strong> function for creating this visualization:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">def pca_scree_plot(pca_model, ax=None):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Plot explained variance of each consecutive PCA component.</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - pca_model: The PCA model that has been fit already</p>
			<p class="source-code">        - ax: Matplotlib `Axes` object to plot on.</p>
			<p class="source-code">    Returns: A matplotlib `Axes` object</p>
			<p class="source-code">    """</p>
			<p class="source-code">    if not ax:</p>
			<p class="source-code">        fig, ax = plt.subplots()</p>
			<p class="source-code">    values = <strong class="bold">pca_model.explained_variance_</strong></p>
			<p class="source-code">    ax.plot(np.arange(1, values.size + 1), values, 'o-')</p>
			<p class="source-code">    ax.set_title('Scree Plot for PCA Components')</p>
			<p class="source-code">    ax.set_xlabel('component')</p>
			<p class="source-code">    ax.set_ylabel('explained variance')</p>
			<p class="source-code">    return ax</p>
			<p>We can <a id="_idIndexMarker1419"/>pass the PCA part of our pipeline to this function in order to see the variance explained by each PCA component:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.decomposition import PCA</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from ml_utils.pca import pca_scree_plot</strong></p>
			<p class="source-code">&gt;&gt;&gt; pipeline = Pipeline([</p>
			<p class="source-code">...     ('normalize', MinMaxScaler()),</p>
			<p class="source-code">...     ('pca', PCA(8, random_state=0))</p>
			<p class="source-code">... ]).fit(w_X_train, w_y_train)</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">pca_scree_plot(pipeline.named_steps['pca'])</strong></p>
			<p>The scree <a id="_idIndexMarker1420"/>plot tells us we should try four PCA components because there are diminishing returns after that component:</p>
			<div>
				<div id="_idContainer436" class="IMG---Figure">
					<img src="image/Figure_10.7_B16834.jpg" alt="Figure 10.7 – Diminishing returns for each additional PCA component after the fourth&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 10.7 – Diminishing returns for each additional PCA component after the fourth</p>
			<p>We can <a id="_idIndexMarker1421"/>build a model on top of these four PCA features in a process called <strong class="bold">meta-learning</strong>, where the last model in the pipeline is trained on the output from a different model, not the original data itself:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.decomposition import PCA</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</p>
			<p class="source-code">&gt;&gt;&gt; pipeline = <strong class="bold">Pipeline([</strong></p>
			<p class="source-code">...     <strong class="bold">('normalize', MinMaxScaler()),</strong></p>
			<p class="source-code">...     <strong class="bold">('pca', PCA(4, random_state=0)),</strong></p>
			<p class="source-code">...     <strong class="bold">('lr', LogisticRegression(</strong></p>
			<p class="source-code">...         <strong class="bold">class_weight='balanced', random_state=0</strong></p>
			<p class="source-code">...     <strong class="bold">))</strong></p>
			<p class="source-code">... <strong class="bold">])</strong>.fit(w_X_train, w_y_train)</p>
			<p>Our new <a id="_idIndexMarker1422"/>model performs nearly as well as the original logistic regression that used 11 features, with just 4 features made with PCA:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; preds = pipeline.predict(w_X_test)</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(w_y_test, preds))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code"><strong class="bold">           0       0.99      0.99      0.99      1225</strong></p>
			<p class="source-code"><strong class="bold">           1       0.96      0.96      0.96       400</strong></p>
			<p class="source-code">    accuracy                           0.98      1625</p>
			<p class="source-code">   macro avg       0.98      0.98      0.98      1625</p>
			<p class="source-code">weighted avg       0.98      0.98      0.98      1625</p>
			<p>After performing dimensionality reduction, we no longer have all of the features we started with—reducing the number of features was the point after all. However, it is possible <a id="_idIndexMarker1423"/>that we will want to perform different feature engineering techniques on subsets of our features; in order to do so, we need to understand feature unions.</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor223"/>Feature unions</h2>
			<p>We may want to build a model on features from a variety of sources, such as PCA, <a id="_idIndexMarker1424"/>in addition to selecting a subset of the features. For these purposes, <strong class="source-inline">scikit-learn</strong> provides the <strong class="source-inline">FeatureUnion</strong> class in the <strong class="source-inline">pipeline</strong> module. This also allows us to perform multiple feature engineering techniques at once, such as feature extraction followed by feature transformation, when we combine this with a pipeline.</p>
			<p>Creating a <strong class="source-inline">FeatureUnion</strong> object is just like creating a pipeline, but rather than passing the steps in order, we pass the transformations we want to make. These will be stacked side by side in the result. Let's use a feature union of interaction terms and select the features with a variance above 0.01 to predict red wine quality:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.feature_selection import VarianceThreshold</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.pipeline import FeatureUnion, Pipeline</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import (</p>
			<p class="source-code">...     MinMaxScaler, PolynomialFeatures</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">combined_features = FeatureUnion([</strong></p>
			<p class="source-code">...     <strong class="bold">('variance', VarianceThreshold(threshold=0.01)),</strong></p>
			<p class="source-code">...     <strong class="bold">('poly', PolynomialFeatures(</strong></p>
			<p class="source-code">...         <strong class="bold">degree=2, include_bias=False, interaction_only=True</strong></p>
			<p class="source-code">...     <strong class="bold">))</strong></p>
			<p class="source-code">... <strong class="bold">])</strong></p>
			<p class="source-code">&gt;&gt;&gt; pipeline = Pipeline([</p>
			<p class="source-code">...     ('normalize', MinMaxScaler()),</p>
			<p class="source-code">...     ('feature_union', combined_features),</p>
			<p class="source-code">...     ('lr', LogisticRegression(</p>
			<p class="source-code">...         class_weight='balanced', random_state=0</p>
			<p class="source-code">...     ))</p>
			<p class="source-code">... ]).fit(r_X_train, r_y_train)</p>
			<p>To illustrate the transformation that took place, let's examine the first row from the training set for the red wine quality data after the <strong class="source-inline">FeatureUnion</strong> object transforms it. Since we <a id="_idIndexMarker1425"/>saw that our variance threshold results in nine features, we know they are the first nine entries in the resulting NumPy array, and the rest are the interaction terms:</p>
			<p class="source-code">&gt;&gt;&gt; pipeline.named_steps['feature_union']\</p>
			<p class="source-code">...     .transform(r_X_train)[0]</p>
			<p class="source-code">array([9.900000e+00, 3.500000e-01, 5.500000e-01, 5.000000e+00,</p>
			<p class="source-code">       1.400000e+01, 9.971000e-01, 3.260000e+00, 1.060000e+01,</p>
			<p class="source-code">       9.900000e+00, 3.500000e-01, 5.500000e-01, 2.100000e+00,</p>
			<p class="source-code">       6.200000e-02, 5.000000e+00, 1.400000e+01, 9.971000e-01,</p>
			<p class="source-code">       ..., 3.455600e+01, 8.374000e+00])</p>
			<p>We can also look at the classification report to see that we got a marginal improvement in F<span class="subscript">1</span> score:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; preds = pipeline.predict(r_X_test)</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(r_y_test, preds))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           0       0.94      0.80      0.87       138</p>
			<p class="source-code">           1       0.36      0.68      0.47        22</p>
			<p class="source-code">    accuracy                           0.79       160</p>
			<p class="source-code"><strong class="bold">   macro avg       0.65      0.74      0.67       160</strong></p>
			<p class="source-code">weighted avg       0.86      0.79      0.81       160</p>
			<p>In this example, we selected our features such that they had variance greater than 0.01, making the <a id="_idIndexMarker1426"/>assumption that if the feature doesn't take on many different values then it may not be that helpful. Rather than making this assumption, we can use a machine learning model to help determine which features are important.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor224"/>Feature importances</h2>
			<p><strong class="bold">Decision trees</strong> recursively <a id="_idIndexMarker1427"/>split the data, making decisions on which features <a id="_idIndexMarker1428"/>to use for each split. They are <strong class="bold">greedy learners</strong>, meaning <a id="_idIndexMarker1429"/>they look for the largest split they can make each time; this isn't necessarily the optimal split when looking at the output of <a id="_idIndexMarker1430"/>the tree. We can use a decision tree to gauge <strong class="bold">feature importances</strong>, which determine how the tree splits the data at the decision nodes. These feature importances can help inform feature selection. Note that feature importances will sum to one, and higher values are better. Let's use a decision tree to see how red and white wine can be separated on a chemical level:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.tree import DecisionTreeClassifier</strong></p>
			<p class="source-code">&gt;&gt;&gt; dt = <strong class="bold">DecisionTreeClassifier</strong>(random_state=0).fit(</p>
			<p class="source-code">...     w_X_train, w_y_train</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; pd.DataFrame([(col, coef) for col, coef in zip(</p>
			<p class="source-code">...     w_X_train.columns, <strong class="bold">dt.feature_importances_</strong></p>
			<p class="source-code">... )], columns=['feature', 'importance']</p>
			<p class="source-code">... ).set_index('feature').sort_values(</p>
			<p class="source-code">...     'importance', ascending=False</p>
			<p class="source-code">... ).T</p>
			<p>This shows <a id="_idIndexMarker1431"/>us that the most important chemical <a id="_idIndexMarker1432"/>properties in distinguishing between red and white wine are total sulfur dioxide and chlorides:</p>
			<div>
				<div id="_idContainer437" class="IMG---Figure">
					<img src="image/Figure_10.8_B16834.jpg" alt="Figure 10.8 – Importance of each chemical property in predicting wine type&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – Importance of each chemical property in predicting wine type</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Using the top features, as indicated by the feature importances, we can try to build a simpler model (by using fewer features). If possible, we want to simplify our models without sacrificing much performance. See the <strong class="source-inline">wine.ipynb</strong> notebook for an example.</p>
			<p>If we train another decision tree with a max depth of two, we can visualize the top of the tree (it is too large to visualize if we don't limit the depth):</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.tree import export_graphviz</p>
			<p class="source-code">&gt;&gt;&gt; import graphviz</p>
			<p class="source-code">&gt;&gt;&gt; graphviz.Source(export_graphviz(</p>
			<p class="source-code">...     DecisionTreeClassifier(</p>
			<p class="source-code">...         <strong class="bold">max_depth=2</strong>, random_state=0</p>
			<p class="source-code">...     ).fit(w_X_train, w_y_train),</p>
			<p class="source-code">...     feature_names=w_X_train.columns</p>
			<p class="source-code">... ))</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Graphviz software will need to be installed (if it isn't already) in order to visualize the tree. It can <a id="_idIndexMarker1433"/>be downloaded at <a href="https://graphviz.gitlab.io/download/">https://graphviz.gitlab.io/download/</a>, with the installation guide at <a href="https://graphviz.readthedocs.io/en/stable/manual.html#installation">https://graphviz.readthedocs.io/en/stable/manual.html#installation</a>. Note that the kernel <a id="_idIndexMarker1434"/>will need to be restarted after installing. Otherwise, pass <strong class="source-inline">out_file='tree.dot'</strong> to the <strong class="source-inline">export_graphviz()</strong> <strong class="source-inline">function</strong> and then generate a PNG file by running <strong class="source-inline">dot -T png tree.dot -o tree.png</strong> from the command line. As an alternative, <strong class="source-inline">scikit-learn</strong> provides the <strong class="source-inline">plot_tree()</strong> function, which uses <strong class="source-inline">matplotlib</strong>; consult the notebook for an example.</p>
			<p>This results in the following tree, which first splits on total sulfur dioxide (which has the highest feature importance), followed by chlorides on the second level. The information at each node tells us the criterion for the split (the top line), the value of the cost function (<strong class="bold">gini</strong>), the number of samples at that node (<strong class="bold">samples</strong>), and the number of samples in each class at that node (<strong class="bold">values</strong>):</p>
			<div>
				<div id="_idContainer438" class="IMG---Figure">
					<img src="image/Figure_10.9_B16834.jpg" alt="Figure 10.9 – Decision tree for predicting wine type based on chemical properties&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – Decision tree for predicting wine type based on chemical properties</p>
			<p>We can <a id="_idIndexMarker1435"/>also apply decision trees to regression problems. Let's find the feature importances for the planets data using the <strong class="source-inline">DecisionTreeRegressor</strong> class:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.tree import DecisionTreeRegressor</strong></p>
			<p class="source-code">&gt;&gt;&gt; dt = <strong class="bold">DecisionTreeRegressor</strong>(random_state=0).fit(</p>
			<p class="source-code">...     pl_X_train, pl_y_train</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; [(col, coef) for col, coef in zip(</p>
			<p class="source-code">...     pl_X_train.columns, dt.feature_importances_</p>
			<p class="source-code">... )]</p>
			<p class="source-code">[<strong class="bold">('semimajoraxis', 0.9969449557611615)</strong>,</p>
			<p class="source-code"> ('mass', 0.0015380986260574154),</p>
			<p class="source-code"> ('eccentricity', 0.0015169456127809738)]</p>
			<p>Basically, the semi-major axis is the main determinant in the period length, which we already knew, but if we visualize a tree, we can see why. The first four splits are all based on the semi-major axis:</p>
			<div>
				<div id="_idContainer439" class="IMG---Figure">
					<img src="image/Figure_10.10_B16834.jpg" alt="Figure 10.10 – Decision tree for predicting planet period&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 10.10 – Decision tree for predicting planet period</p>
			<p>Decision trees can be <strong class="bold">pruned</strong> after being grown to maximum depth, or provided with a max depth <a id="_idIndexMarker1436"/>before training, to limit growth and thus avoid overfitting. The <strong class="source-inline">scikit-learn</strong> documentation provides tips to address overfitting and other potential issues when using decision trees at <a href="https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use">https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use</a>. Keep this in mind as we discuss ensemble methods.</p>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor225"/>Ensemble methods</h1>
			<p><strong class="bold">Ensemble methods</strong> combine <a id="_idIndexMarker1437"/>many models (often weak ones) to create a stronger one that will either minimize the average error between observed and predicted values (the <strong class="bold">bias</strong>) or improve how well it generalizes to unseen data (minimize the <strong class="bold">variance</strong>). We have to strike a balance between complex models that may increase variance, as they tend to overfit, and simple models that may have high bias, as <a id="_idIndexMarker1438"/>these tend to underfit. This is called the <strong class="bold">bias-variance trade-off</strong>, which is illustrated in the following subplots:</p>
			<div>
				<div id="_idContainer440" class="IMG---Figure">
					<img src="image/Figure_10.11_B16834.jpg" alt="Figure 10.11 – The bias-variance trade-off&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – The bias-variance trade-off</p>
			<p>Ensemble methods <a id="_idIndexMarker1439"/>can be broken down into three categories: <strong class="bold">boosting</strong>, <strong class="bold">bagging</strong>, and <strong class="bold">stacking</strong>. <strong class="bold">Boosting</strong> trains many weak learners, which learn <a id="_idIndexMarker1440"/>from each other's mistakes to reduce bias, making a stronger learner. <strong class="bold">Bagging</strong>, on the <a id="_idIndexMarker1441"/>other hand, uses <strong class="bold">bootstrap aggregation</strong> to train <a id="_idIndexMarker1442"/>many models on bootstrap samples of the data and aggregate the results together (using voting for classification, and the average for regression) to reduce variance. We can also combine many different model types together with voting. <strong class="bold">Stacking</strong> is an <a id="_idIndexMarker1443"/>ensemble technique where we combine many different model types using the outputs of some as the inputs to others; this is done to improve predictions. We saw an example of stacking when we combined PCA and logistic regression in the <em class="italic">Dimensionality reduction</em> section earlier in this chapter.</p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor226"/>Random forest</h2>
			<p>Decision trees <a id="_idIndexMarker1444"/>have a tendency to overfit, especially if we <a id="_idIndexMarker1445"/>don't set limits on how far they can grow (with the <strong class="source-inline">max_depth</strong> and <strong class="source-inline">min_samples_leaf</strong> parameters). We can address this overfitting issue with a <strong class="bold">random forest</strong>, which is a bagging algorithm where we train many decision trees in parallel using bootstrap samples of our data and aggregate the output. In addition, we have the option of scoring each tree on the data in the training set that it didn't receive in <a id="_idIndexMarker1446"/>its bootstrap sample, called <strong class="bold">out-of-bag samples</strong>, with the <strong class="source-inline">oob_score</strong> parameter. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The <strong class="source-inline">min_samples_leaf</strong> parameter requires a minimum number of samples to be on the final nodes in the tree (or leaves); this prevents the trees from being fit until they only have a single observation at each leaf.</p>
			<p>Each of the trees also gets a subset of the features (random feature selection), which defaults to the square root of the number of features (the <strong class="source-inline">max_features</strong> parameter). This can help address the curse of dimensionality. As a consequence, however, the random <a id="_idIndexMarker1447"/>forest can't be as easily interpreted as <a id="_idIndexMarker1448"/>the decision trees that make it up. We can, however, extract feature importances from the random forest, just as we did with the decision tree.</p>
			<p>Let's use the <strong class="source-inline">RandomForestClassifier</strong> class from the <strong class="source-inline">ensemble</strong> module to build a random forest (with <strong class="source-inline">n_estimators</strong> trees in it) for the classification of high-quality red wines:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.ensemble import RandomForestClassifier</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV</p>
			<p class="source-code">&gt;&gt;&gt; rf = RandomForestClassifier(</p>
			<p class="source-code">...     <strong class="bold">n_estimators=100</strong>, random_state=0</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; search_space = {</p>
			<p class="source-code">...     <strong class="bold">'max_depth': [4, 8],</strong> # keep trees small</p>
			<p class="source-code">...     <strong class="bold">'min_samples_leaf': [4, 6]</strong></p>
			<p class="source-code">... }</p>
			<p class="source-code">&gt;&gt;&gt; rf_grid = GridSearchCV(</p>
			<p class="source-code">...     rf, search_space, cv=5, scoring='precision'</p>
			<p class="source-code">... ).fit(r_X_train, r_y_train)</p>
			<p class="source-code">&gt;&gt;&gt; rf_grid.score(r_X_test, r_y_test)</p>
			<p class="source-code"><strong class="bold">0.6</strong></p>
			<p>Note that <a id="_idIndexMarker1449"/>our precision with the random forest is <a id="_idIndexMarker1450"/>already much better than the 0.35 we got in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>. The random forest is robust to outliers and able to model non-linear decision boundaries to separate the classes, which may explain part of this dramatic improvement.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor227"/>Gradient boosting</h2>
			<p>Boosting looks to improve upon the mistakes of previous models. One way of doing this is to move in the <a id="_idIndexMarker1451"/>direction of the steepest reduction in the loss function for the model. Since the <strong class="bold">gradient</strong> (the multi-variable generalization of the derivative) is the direction <a id="_idIndexMarker1452"/>of steepest ascent, this can be done by calculating the negative gradient, which yields the direction of steepest descent, meaning the <a id="_idIndexMarker1453"/>best improvement in the loss function <a id="_idIndexMarker1454"/>from the current result. This technique is called <strong class="bold">gradient descent</strong>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Although gradient descent sounds great, there are some potential issues with it. It's possible to end up in a local minimum (a minimum in a certain region of the cost function); the algorithm will stop, thinking that we have the optimal solution, when in fact we don't, because we would like the global minimum (the minimum over the whole region).</p>
			<p>Scikit-learn's <strong class="source-inline">ensemble</strong> module provides the <strong class="source-inline">GradientBoostingClassifier</strong> and <strong class="source-inline">GradientBoostingRegressor</strong> classes for gradient boosting using decision trees. These trees will boost their performance through gradient descent. Note that gradient boosted trees are more sensitive to noisy training data than the random forest. In addition, we must consider the additional time required to build all the trees in series, unlike the parallel training we can benefit from with the random forest.</p>
			<p>Let's use <a id="_idIndexMarker1455"/>grid search and gradient boosting to train another model for classifying the red wine quality data. In addition to searching for the <a id="_idIndexMarker1456"/>best values for the <strong class="source-inline">max_depth</strong> and <strong class="source-inline">min_samples_leaf</strong> parameters, we will search for a good value for the <strong class="source-inline">learning_rate</strong> parameter, which determines the contribution each tree will make in the final estimator:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.ensemble import GradientBoostingClassifier</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV</p>
			<p class="source-code">&gt;&gt;&gt; gb = <strong class="bold">GradientBoostingClassifier(</strong></p>
			<p class="source-code">...     <strong class="bold">n_estimators=100, random_state=0</strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p class="source-code">&gt;&gt;&gt; search_space = {</p>
			<p class="source-code">...     'max_depth': [4, 8], # keep trees small</p>
			<p class="source-code">...     'min_samples_leaf': [4, 6],</p>
			<p class="source-code">...     <strong class="bold">'learning_rate': [0.1, 0.5, 1]</strong></p>
			<p class="source-code">... }</p>
			<p class="source-code">&gt;&gt;&gt; gb_grid = GridSearchCV(</p>
			<p class="source-code">...     gb, search_space, cv=5, scoring='f1_macro'</p>
			<p class="source-code">... ).fit(r_X_train, r_y_train)</p>
			<p>The F<span class="subscript">1</span> macro score we achieve with gradient boosting is better than the 0.66 we got with logistic regression in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>:</p>
			<p class="source-code">&gt;&gt;&gt; gb_grid.score(r_X_test, r_y_test)</p>
			<p class="source-code"><strong class="bold">0.7226024272287617</strong></p>
			<p>Both bagging and boosting have given us better performance than the logistic regression <a id="_idIndexMarker1457"/>model; however, we may find that the models don't always agree and that we could <a id="_idIndexMarker1458"/>improve performance even more by having the models vote before making the final prediction.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor228"/>Voting</h2>
			<p>When trying out different models for classification, it may be interesting to measure their <a id="_idIndexMarker1459"/>agreement using Cohen's kappa score. We can use <a id="_idIndexMarker1460"/>the <strong class="source-inline">cohen_kappa_score()</strong> function in the <strong class="source-inline">sklearn.metrics</strong> module to do so. The score ranges from complete disagreement (-1) to complete agreement (1). Our boosting and bagging predictions have a high level of agreement:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import cohen_kappa_score</p>
			<p class="source-code">&gt;&gt;&gt; cohen_kappa_score(</p>
			<p class="source-code">...     rf_grid.predict(r_X_test), gb_grid.predict(r_X_test)</p>
			<p class="source-code">... )</p>
			<p class="source-code">0.7185929648241206</p>
			<p>Sometimes, we can't find a single model that works well for all of our data, so we may want to find a way to combine the opinions of various models to make the final decision. Scikit-learn provides the <strong class="source-inline">VotingClassifier</strong> class for aggregating model opinions on classification tasks. We have the option of specifying the voting type, where <strong class="source-inline">hard</strong> results in majority rules and <strong class="source-inline">soft</strong> will predict the class with the highest sum of probabilities across the models. </p>
			<p>As an example, let's create a classifier for each voting type using the three estimators (models) from this chapter—logistic regression, random forest, and gradient boosting. Since we will run <strong class="source-inline">fit()</strong>, we pass in the best estimator from each of our grid searches (<strong class="source-inline">best_estimator_</strong>). This avoids running each grid search again unnecessarily, which will also train our model faster:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.ensemble import VotingClassifier</strong></p>
			<p class="source-code">&gt;&gt;&gt; majority_rules = <strong class="bold">VotingClassifier(</strong></p>
			<p class="source-code">...     <strong class="bold">[('lr', lr_grid.best_estimator_),</strong><strong class="bold"> </strong></p>
			<p class="source-code">...      <strong class="bold">('rf', rf_grid.best_estimator_), </strong></p>
			<p class="source-code">...      <strong class="bold">('gb', gb_grid.best_estimator_)],</strong></p>
			<p class="source-code">...     <strong class="bold">voting='hard'</strong></p>
			<p class="source-code">... <strong class="bold">)</strong>.fit(r_X_train, r_y_train)</p>
			<p class="source-code">&gt;&gt;&gt; max_probabilities = VotingClassifier(</p>
			<p class="source-code">...     [('lr', lr_grid.best_estimator_), </p>
			<p class="source-code">...      ('rf', rf_grid.best_estimator_), </p>
			<p class="source-code">...      ('gb', gb_grid.best_estimator_)],</p>
			<p class="source-code">...     <strong class="bold">voting='soft'</strong></p>
			<p class="source-code">... ).fit(r_X_train, r_y_train)</p>
			<p>Our <strong class="source-inline">majority_rules</strong> classifier requires two of the three models to agree (at a minimum), while the <strong class="source-inline">max_probabilities</strong> classifier has each model vote with its predicted probabilities. We can <a id="_idIndexMarker1461"/>measure how well they perform on the test data with <a id="_idIndexMarker1462"/>the <strong class="source-inline">classification_report()</strong> function, which tells us that <strong class="source-inline">majority_rules</strong> is a little better than <strong class="source-inline">max_probabilities</strong> in terms of precision. Both are better than the other models we have tried:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(</p>
			<p class="source-code">...     r_y_test, <strong class="bold">majority_rules</strong>.predict(r_X_test)</p>
			<p class="source-code">... ))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code"><strong class="bold">           0       0.92      0.95      0.93       138</strong></p>
			<p class="source-code"><strong class="bold">           1       0.59      0.45      0.51        22</strong></p>
			<p class="source-code">    accuracy                           0.88       160</p>
			<p class="source-code"><strong class="bold">   macro avg       0.75      0.70      0.72       160</strong></p>
			<p class="source-code">weighted avg       0.87      0.88      0.87       160</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(</p>
			<p class="source-code">...     r_y_test, <strong class="bold">max_probabilities</strong>.predict(r_X_test)</p>
			<p class="source-code">... ))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code"><strong class="bold">           0       0.92      0.93      0.92       138</strong></p>
			<p class="source-code"><strong class="bold">           1       0.52      0.50      0.51        22</strong></p>
			<p class="source-code">    accuracy                           0.87       160</p>
			<p class="source-code"><strong class="bold">   macro avg       0.72      0.71      0.72       160</strong></p>
			<p class="source-code">weighted avg       0.87      0.87      0.87       160</p>
			<p>Another important option with the <strong class="source-inline">VotingClassifier</strong> class is the <strong class="source-inline">weights</strong> parameter, which <a id="_idIndexMarker1463"/>lets us place more or less emphasis on certain estimators <a id="_idIndexMarker1464"/>when voting. For example, if we pass <strong class="source-inline">weights=[1, 2, 2]</strong> to <strong class="source-inline">majority_rules</strong>, we are giving extra weight to the predictions made by the random forest and gradient boosting estimators. In order to determine which models (if any) should be given extra weight, we can look at individual performance and prediction confidence.</p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor229"/>Inspecting classification prediction confidence</h1>
			<p>As we saw with ensemble methods, when we know the strengths and weaknesses of our model, we can employ strategies to attempt to improve performance. We may have two models <a id="_idIndexMarker1465"/>to classify something, but they most likely won't agree on everything. However, say that we know that one does better on edge cases, while the other is better on the more common ones. In that case, we would likely want to investigate a voting classifier to improve our performance. How can we know how the models perform in different situations, though?</p>
			<p>By looking at the probabilities the model predicts of an observation belonging to a given class, we can gain insight into how confident our model is when it is correct and when it errs. We can use our <strong class="source-inline">pandas</strong> data wrangling skills to make quick work of this. Let's see how confident our original <strong class="source-inline">white_or_red</strong> model from <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>, was in its predictions:</p>
			<p class="source-code">&gt;&gt;&gt; prediction_probabilities = pd.DataFrame(</p>
			<p class="source-code">...     <strong class="bold">white_or_red.predict_proba(w_X_test)</strong>, </p>
			<p class="source-code">...     columns=['prob_white', 'prob_red']</p>
			<p class="source-code">... ).assign(</p>
			<p class="source-code">...     is_red=w_y_test == 1,</p>
			<p class="source-code">...     pred_white=lambda x: x.prob_white &gt;= 0.5,</p>
			<p class="source-code">...     pred_red=lambda x: np.invert(x.pred_white),</p>
			<p class="source-code">...     <strong class="bold">correct=lambda x: (np.invert(x.is_red) &amp; x.pred_white)</strong></p>
			<p class="source-code">...                        <strong class="bold">| (x.is_red &amp; x.pred_red)</strong></p>
			<p class="source-code">... )</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can tweak the probability threshold for our model's predictions by using the <strong class="source-inline">predict_proba()</strong> method, instead of <strong class="source-inline">predict()</strong>. This will give us the probabilities that the observation belongs to each class. We can then compare that to our custom threshold. For example, we could use 75%: <strong class="source-inline">white_or_red.predict_proba(w_X_test)[:,1] &gt;= .75</strong>. </p>
			<p class="callout">One way to identify this threshold is to determine the false positive rate we are comfortable with, and then use the data from the <strong class="source-inline">roc_curve()</strong> function in the <strong class="source-inline">sklearn.metrics</strong> module to find the threshold that results in that false positive rate. Another way is to find a satisfactory spot along the precision-recall curve, and then get the threshold from the <strong class="source-inline">precision_recall_curve()</strong> function. We will work through an example in <a href="B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Machine Learning Anomaly Detection</em>.</p>
			<p>Let's use <strong class="source-inline">seaborn</strong> to make a plot showing the distribution of the prediction probabilities <a id="_idIndexMarker1466"/>when the model was correct versus when it was incorrect. The <strong class="source-inline">displot()</strong> function makes it easy to <a id="_idIndexMarker1467"/>plot the <strong class="bold">kernel density estimate</strong> (<strong class="bold">KDE</strong>) superimposed <a id="_idIndexMarker1468"/>on a histogram. Here, we will also add a <strong class="bold">rug plot</strong>, which shows where each of our predictions ended up:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">g = sns.displot(</strong></p>
			<p class="source-code">...     <strong class="bold">data=prediction_probabilities, x='prob_red', </strong></p>
			<p class="source-code">...     <strong class="bold">rug=True, kde=True, bins=20, col='correct',</strong></p>
			<p class="source-code">...     <strong class="bold">facet_kws={'sharey': True} </strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p class="source-code">&gt;&gt;&gt; g.set_axis_labels('probability wine is red', None) </p>
			<p class="source-code">&gt;&gt;&gt; plt.suptitle('Prediction Confidence', y=1.05)</p>
			<p>The KDE for correct predictions is bimodal, with modes near 0 and near 1, meaning the model is very confident when it is correct, which, since it is correct most of the time, means it is very confident in general. The peak of the correct predictions KDE at 0 is much higher than the one at 1 because we have many more white wines than red wines in the data. Note that the KDE shows probabilities of less than zero and greater than one as possible. For this reason, we add the histogram to confirm that the shape we are seeing is meaningful. The histogram for correct predictions doesn't have much in the middle of the distribution, so we include the rug plot to better see which probabilities were predicted. The incorrect predictions don't have many data points, but it appears to be all over the place, because when the model got it wrong, it got fooled pretty badly:</p>
			<div>
				<div id="_idContainer441" class="IMG---Figure">
					<img src="image/Figure_10.12_B16834.jpg" alt="Figure 10.12 – Prediction confidence when the model was correct versus incorrect&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Prediction confidence when the model was correct versus incorrect</p>
			<p>This outcome tells us we may want to look into the chemical properties of the wines that were <a id="_idIndexMarker1469"/>incorrectly classified. It's possible they were outliers and that is why they fooled the model. We can modify the box plots by wine type from the <em class="italic">Exploratory data analysis</em> section in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>, to see if anything stands out (<em class="italic">Figure 9.6</em>).</p>
			<p>First, we isolate the chemical properties for the incorrectly classified wines:</p>
			<p class="source-code">&gt;&gt;&gt; incorrect = w_X_test.assign(is_red=w_y_test).iloc[</p>
			<p class="source-code">...     prediction_probabilities.query('not correct').index</p>
			<p class="source-code">... ]</p>
			<p>Then, we add some calls to <strong class="source-inline">scatter()</strong> on the <strong class="source-inline">Axes</strong> object to mark these wines on the box plots from before:</p>
			<p class="source-code">&gt;&gt;&gt; import math</p>
			<p class="source-code">&gt;&gt;&gt; chemical_properties = [col for col in wine.columns</p>
			<p class="source-code">...                        if col not in ['quality', 'kind']]</p>
			<p class="source-code">&gt;&gt;&gt; melted = \</p>
			<p class="source-code">...     wine.drop(columns='quality').melt(id_vars=['kind'])</p>
			<p class="source-code">&gt;&gt;&gt; fig, axes = plt.subplots(</p>
			<p class="source-code">...     math.ceil(len(chemical_properties) / 4), 4,</p>
			<p class="source-code">...     figsize=(15, 10)</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; axes = axes.flatten()</p>
			<p class="source-code">&gt;&gt;&gt; for prop, ax in zip(chemical_properties, axes):</p>
			<p class="source-code">...     sns.boxplot(</p>
			<p class="source-code">...         data=melted[melted.variable.isin([prop])], </p>
			<p class="source-code">...         x='variable', y='value', hue='kind', ax=ax,</p>
			<p class="source-code">...         palette={'white': 'lightyellow', 'red': 'orchid'},</p>
			<p class="source-code">...         saturation=0.5, fliersize=2</p>
			<p class="source-code">...     ).set_xlabel('')</p>
			<p class="source-code">...     <strong class="bold">for _, wrong in incorrect.iterrows():</strong><strong class="bold"> </strong></p>
			<p class="source-code">...         # _ is convention for collecting info we won't use</p>
			<p class="source-code">...         <strong class="bold">x_coord = -0.2 if not wrong['is_red'] else 0.2</strong></p>
			<p class="source-code">...         <strong class="bold">ax.scatter(</strong></p>
			<p class="source-code">...             <strong class="bold">x_coord, wrong[prop], marker='x',</strong></p>
			<p class="source-code">...             <strong class="bold">color='red', s=50</strong></p>
			<p class="source-code">...         <strong class="bold">)</strong></p>
			<p class="source-code">&gt;&gt;&gt; for ax in axes[len(chemical_properties):]:</p>
			<p class="source-code">...     ax.remove()</p>
			<p class="source-code">&gt;&gt;&gt; plt.suptitle(</p>
			<p class="source-code">...     'Comparing Chemical Properties of Red and White Wines'</p>
			<p class="source-code">...     '\n(classification errors are red x\'s)'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; plt.tight_layout() # clean up layout</p>
			<p>This results <a id="_idIndexMarker1470"/>in each of the incorrectly classified wines being marked with a red <strong class="bold">X</strong>. In each subplot, the points on the left box plot are white wines and those on the right box plot are red wines. It appears that some of them may have been outliers for a few characteristics—such as red wines with high residual sugar or sulfur dioxide, and white wines with high volatile acidity:</p>
			<div>
				<div id="_idContainer442" class="IMG---Figure">
					<img src="image/Figure_10.13_B16834.jpg" alt="Figure 10.13 – Checking whether incorrect predictions were outliers&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.13 – Checking whether incorrect predictions were outliers</p>
			<p>Despite <a id="_idIndexMarker1471"/>having many more white wines than red wines in the data, our model is able to distinguish between them pretty well. This isn't always the case. Sometimes, in order to improve our performance, we need to address the class imbalance.</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor230"/>Addressing class imbalance</h1>
			<p>When faced with a class imbalance in our data, we may want to try to balance the training data <a id="_idIndexMarker1472"/>before we build a model around it. In order to do this, we can use one of the following imbalanced sampling techniques:</p>
			<ul>
				<li>Over-sample the minority class.</li>
				<li>Under-sample the majority class.</li>
			</ul>
			<p>In the case of <strong class="bold">over-sampling</strong>, we pick a larger proportion from the minority class in order to get <a id="_idIndexMarker1473"/>closer to the amount of the majority class; this may involve a technique such as bootstrapping or generating new data similar to the values in the existing data (using machine learning algorithms such as nearest neighbors). <strong class="bold">Under-sampling</strong>, on the other hand, will take less data overall by reducing the amount taken <a id="_idIndexMarker1474"/>from the majority class. The decision to use over-sampling or under-sampling will depend on the amount of data we started with, and in some cases, computational costs. In practice, we wouldn't try either of these without first trying to build the model with the class imbalance. It's important not to try to optimize things <a id="_idIndexMarker1475"/>prematurely; not to mention that by building the model first, we have a baseline to compare our imbalanced sampling attempts against.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Huge performance issues can arise if the minority class that we have in the data isn't truly representative of the full spectrum present in the population. For this reason, our method of collecting the data in the first place should be both known to us and carefully evaluated before proceeding to modeling. If we aren't careful, we could easily build a model that can't generalize to new data, regardless of how we handle the class imbalance.</p>
			<p>Before we explore any imbalanced sampling techniques, let's create a baseline model using <strong class="bold">k-nearest neighbors</strong> (<strong class="bold">k-NN</strong>) classification, which will classify observations according to <a id="_idIndexMarker1476"/>the class of the k-nearest observations in the n-dimensional space of the data (our red wine quality data is 11-dimensional). For comparison purposes, we will use the same number of neighbors for all the models in this section; however, it is certainly possible that the sampling techniques will result in a different value performing better. We will use five neighbors:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.neighbors import KNeighborsClassifier</strong></p>
			<p class="source-code">&gt;&gt;&gt; knn = <strong class="bold">KNeighborsClassifier(n_neighbors=5)</strong>.fit(</p>
			<p class="source-code">...     r_X_train, r_y_train</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; knn_preds = knn.predict(r_X_test)</p>
			<p>Our k-NN model is fast to train because it is a <strong class="bold">lazy learner</strong>—calculations are made at classification time. It is important to keep in mind the time our models take to train and make predictions, as this can dictate which models we can use in practice. A model that performs marginally better but takes twice as long to train or predict may not be worth it. As the dimensionality of our data increases, the k-NN model will become less and less feasible. We can use the <strong class="source-inline">%%timeit</strong> magic to get an estimate of how long it takes on average to train. Note that this will train the model multiple times, so it might not be the best strategy <a id="_idIndexMarker1477"/>to time a computationally intense model:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">%%timeit</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.neighbors import KNeighborsClassifier</p>
			<p class="source-code">&gt;&gt;&gt; knn = KNeighborsClassifier(n_neighbors=5).fit(</p>
			<p class="source-code">...     r_X_train, r_y_train</p>
			<p class="source-code">... )</p>
			<p class="source-code"><strong class="bold">3.24 ms</strong> ± 599 µs per loop </p>
			<p class="source-code">(mean ± std. dev. of 7 runs, 100 loops each)</p>
			<p>Let's compare <a id="_idIndexMarker1478"/>this result with training a <strong class="bold">support vector machine</strong> (<strong class="bold">SVM</strong>), which projects the data into a higher dimension to find the <strong class="bold">hyperplane</strong> that separates the classes. A hyperplane is the n-dimensional equivalent of a plane, just like <a id="_idIndexMarker1479"/>a plane is the two-dimensional equivalent of a line. SVMs are typically robust to outliers and can model non-linear decision boundaries; however, SVMs get slow very quickly, so it will be a good comparison:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">%%timeit</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.svm import SVC</p>
			<p class="source-code">&gt;&gt;&gt; svc = SVC(gamma='auto').fit(r_X_train, r_y_train)</p>
			<p class="source-code"><strong class="bold">153 ms</strong> ± 6.7 ms per loop </p>
			<p class="source-code">(mean ± std. dev. of 7 runs, 1 loop each)</p>
			<p>Now that we have our baseline model and an idea of how it works, let's see how the baseline k-NN model performs:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(r_y_test, knn_preds))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           0       0.91      0.93      0.92       138</p>
			<p class="source-code">           <strong class="bold">1       0.50      0.41      0.45</strong>        22</p>
			<p class="source-code">    accuracy                           0.86       160</p>
			<p class="source-code">   <strong class="bold">macro avg       0.70      0.67      0.69</strong>       160</p>
			<p class="source-code">weighted avg       0.85      0.86      0.86       160</p>
			<p>With this <a id="_idIndexMarker1480"/>performance benchmark, we are ready to try out imbalanced sampling. We will be using the <strong class="source-inline">imblearn</strong> package, which is provided by the <strong class="source-inline">scikit-learn</strong> community. It provides implementations for over- and under-sampling using various strategies, and it is just as easy to use as <strong class="source-inline">scikit-learn</strong>, since they both follow the same API conventions. For reference, the documentation can be found at <a href="https://imbalanced-learn.readthedocs.io/en/stable/api.html">https://imbalanced-learn.readthedocs.io/en/stable/api.html</a>.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor231"/>Under-sampling</h2>
			<p>As we hinted at earlier, under-sampling will reduce the amount of data available to train our model on. This means we should only attempt this if we have enough data that we can accept <a id="_idIndexMarker1481"/>eliminating some of it. Let's <a id="_idIndexMarker1482"/>see what happens with the red wine quality data, since we don't have much data to begin with.</p>
			<p>We will use the <strong class="source-inline">RandomUnderSampler</strong> class from <strong class="source-inline">imblearn</strong> to randomly under-sample the low-quality red wines in the training set:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from imblearn.under_sampling import RandomUnderSampler</strong></p>
			<p class="source-code">&gt;&gt;&gt; X_train_undersampled, y_train_undersampled = \</p>
			<p class="source-code">...     <strong class="bold">RandomUnderSampler(random_state=0)</strong>\</p>
			<p class="source-code">...         <strong class="bold">.fit_resample(r_X_train, r_y_train)</strong></p>
			<p>We went <a id="_idIndexMarker1483"/>from almost 14% of the <a id="_idIndexMarker1484"/>training data being high-quality red wine to 50% of it; however, notice that this came at the price of 1,049 training samples (more than half of our training data):</p>
			<p class="source-code"># before</p>
			<p class="source-code">&gt;&gt;&gt; r_y_train.value_counts() </p>
			<p class="source-code"><strong class="bold">0    1244</strong></p>
			<p class="source-code">1     195</p>
			<p class="source-code">Name: high_quality, dtype: int64</p>
			<p class="source-code"># after</p>
			<p class="source-code">&gt;&gt;&gt; pd.Series(y_train_undersampled).value_counts().sort_index()</p>
			<p class="source-code"><strong class="bold">0    195</strong></p>
			<p class="source-code">1    195</p>
			<p class="source-code">dtype: int64</p>
			<p>Fitting our model with the under-sampled data is no different from before:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.neighbors import KNeighborsClassifier</p>
			<p class="source-code">&gt;&gt;&gt; knn_undersampled = KNeighborsClassifier(n_neighbors=5)\</p>
			<p class="source-code">...     <strong class="bold">.fit(X_train_undersampled, y_train_undersampled)</strong></p>
			<p class="source-code">&gt;&gt;&gt; knn_undersampled_preds = knn_undersampled.predict(r_X_test)</p>
			<p>Using the classification report, we see that under-sampling is definitely not an improvement—we hardly had any data for this model:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; print(</p>
			<p class="source-code">...     classification_report(r_y_test, knn_undersampled_preds)</p>
			<p class="source-code">... )</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           0       0.93      0.65      0.77       138</p>
			<p class="source-code">           <strong class="bold">1       0.24      0.68      0.35</strong>        22</p>
			<p class="source-code">    accuracy                           0.66       160</p>
			<p class="source-code">   <strong class="bold">macro avg       0.58      0.67      0.56</strong>       160</p>
			<p class="source-code">weighted avg       0.83      0.66      0.71       160</p>
			<p>In situations <a id="_idIndexMarker1485"/>where we have limited <a id="_idIndexMarker1486"/>data to start with, under-sampling is simply not feasible. Here, we lost over half of the already small amount of data we had. Models need a good amount of data to learn from, so let's try over-sampling the minority class now.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor232"/>Over-sampling</h2>
			<p>It's clear that with smaller datasets, it won't be beneficial to under-sample. Instead, we can try <a id="_idIndexMarker1487"/>over-sampling the minority <a id="_idIndexMarker1488"/>class (the high-quality red wines, in this case). Rather than <a id="_idIndexMarker1489"/>doing random over-sampling with the <strong class="source-inline">RandomOverSampler</strong> class, we are going to use the <strong class="bold">Synthetic Minority Over-sampling Technique</strong> (<strong class="bold">SMOTE</strong>) to create <em class="italic">new</em> (synthetic) red wines similar to the high-quality ones using the k-NN algorithm. By doing this, we are making a big assumption that the data we have collected about the chemical properties of the red wine does influence the quality rating of the wine.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The SMOTE implementation in <strong class="source-inline">imblearn</strong> comes from this paper:</p>
			<p class="callout"><em class="italic">N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, SMOTE: synthetic minority over-sampling technique, Journal of Artificial Intelligence Research, 321-357, 2002</em>, available at <a href="https://arxiv.org/pdf/1106.1813.pdf">https://arxiv.org/pdf/1106.1813.pdf</a>.</p>
			<p>Let's <a id="_idIndexMarker1490"/>use SMOTE with the five nearest <a id="_idIndexMarker1491"/>neighbors to over-sample the high-quality red wines in our training data:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from imblearn.over_sampling import SMOTE</strong></p>
			<p class="source-code">&gt;&gt;&gt; X_train_oversampled, y_train_oversampled = <strong class="bold">SMOTE(</strong></p>
			<p class="source-code">...     <strong class="bold">k_neighbors=5, random_state=0</strong></p>
			<p class="source-code">... <strong class="bold">).fit_resample(r_X_train, r_y_train)</strong></p>
			<p>Since we over-sampled, we will have more data than we did before, gaining an extra 1,049 high-quality red wine samples:</p>
			<p class="source-code"># before</p>
			<p class="source-code">&gt;&gt;&gt; r_y_train.value_counts()</p>
			<p class="source-code">0    1244</p>
			<p class="source-code"><strong class="bold">1     195</strong></p>
			<p class="source-code">Name: high_quality, dtype: int64</p>
			<p class="source-code"># after</p>
			<p class="source-code">&gt;&gt;&gt; pd.Series(y_train_oversampled).value_counts().sort_index()</p>
			<p class="source-code">0    1244</p>
			<p class="source-code"><strong class="bold">1    1244</strong></p>
			<p class="source-code">dtype: int64</p>
			<p>Once again, we will fit a k-NN model, using the over-sampled data this time:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.neighbors import KNeighborsClassifier</p>
			<p class="source-code">&gt;&gt;&gt; knn_oversampled = KNeighborsClassifier(n_neighbors=5)\ </p>
			<p class="source-code">...     .<strong class="bold">fit(X_train_oversampled, y_train_oversampled)</strong></p>
			<p class="source-code">&gt;&gt;&gt; knn_oversampled_preds = knn_oversampled.predict(r_X_test)</p>
			<p>Over-sampling <a id="_idIndexMarker1492"/>performed much better <a id="_idIndexMarker1493"/>than under-sampling, but unless we were looking to maximize recall, we are better off sticking with our original strategy for k-NN:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; print(</p>
			<p class="source-code">...     classification_report(r_y_test, knn_oversampled_preds)</p>
			<p class="source-code">... )</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           0       0.96      0.78      0.86       138</p>
			<p class="source-code">           <strong class="bold">1       0.37      0.82      0.51</strong>        22</p>
			<p class="source-code">    accuracy                           0.78       160</p>
			<p class="source-code">   <strong class="bold">macro avg       0.67      0.80      0.68</strong>       160</p>
			<p class="source-code">weighted avg       0.88      0.78      0.81       160</p>
			<p>Note that since SMOTE is creating synthetic data, we must carefully consider the side effects this <a id="_idIndexMarker1494"/>may have on our model. If we <a id="_idIndexMarker1495"/>can't make the assumption that all the values of a given class are representative of the full spectrum of the population and that this won't change over time, we cannot expect SMOTE to work well.</p>
			<h1 id="_idParaDest-234"><a id="_idTextAnchor233"/>Regularization</h1>
			<p>When working with regressions, we may look to add a penalty term to our regression equation to reduce overfitting by punishing certain decisions for coefficients made by the model; this is called <strong class="bold">regularization</strong>. We are looking for the coefficients that will minimize <a id="_idIndexMarker1496"/>this penalty term. The idea is to shrink the coefficients toward zero for features that don't contribute much to reducing the error of the model. Some common techniques are ridge regression, LASSO (short for <em class="italic">Least Absolute Shrinkage and Selection Operator</em>) regression, and elastic net regression, which combines the LASSO and ridge penalty terms. Note that since these techniques rely on the magnitude <a id="_idIndexMarker1497"/>of the coefficients, the data should be scaled beforehand.</p>
			<p><strong class="bold">Ridge regression</strong>, also called <strong class="bold">L2 regularization</strong>, punishes high coefficients (<img src="image/Formula_10_002.png" alt=""/>) by adding the <a id="_idIndexMarker1498"/>sum of the squares of the coefficients to the cost function (which regression looks to minimize when fitting), as per the following penalty term:</p>
			<div>
				<div id="_idContainer444" class="IMG---Figure">
					<img src="image/Formula_10_003.jpg" alt=""/>
				</div>
			</div>
			<p>This penalty term is also weighted by λ (lambda), which indicates how large the penalty will be. When this is zero, we have ordinary least squares regression, as before.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Remember the <strong class="source-inline">C</strong> parameter from the <strong class="source-inline">LogisticRegression</strong> class? By default, the <strong class="source-inline">LogisticRegression</strong> class will use the L2 penalty term, where <strong class="source-inline">C</strong> is 1/λ. However, it also supports L1, but only with certain solvers.</p>
			<p><strong class="bold">LASSO regression</strong>, also called <strong class="bold">L1 regularization</strong>, drives coefficients to zero by adding the <a id="_idIndexMarker1499"/>sum of the absolute values of the coefficients to the cost function. This is <a id="_idIndexMarker1500"/>more robust than L2 regularization because it is less sensitive to extreme values:</p>
			<div>
				<div id="_idContainer445" class="IMG---Figure">
					<img src="image/Formula_10_004.jpg" alt=""/>
				</div>
			</div>
			<p>Since LASSO drives <a id="_idIndexMarker1501"/>coefficients of certain features in the regression to zero (where they won't contribute to the model), it is said to perform feature selection.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Both the L1 and L2 penalties are also referred to as <strong class="bold">L1 and L2 norms</strong> (a mathematical transformation on a vector to be in the range [0, ∞)) and written as <img src="image/Formula_10_005.png" alt=""/> and <img src="image/Formula_10_006.png" alt=""/>, respectively.</p>
			<p><strong class="bold">Elastic net regression</strong> combines both LASSO and ridge penalty terms into the following <a id="_idIndexMarker1502"/>penalty term, where we can tune both the strength of the penalty (λ) and the percentage of the penalty that is L1 (and consequently, the percentage that is L2) with α (alpha):</p>
			<div>
				<div id="_idContainer448" class="IMG---Figure">
					<img src="image/Formula_10_007.jpg" alt=""/>
				</div>
			</div>
			<p>Scikit-learn implements ridge, LASSO, and elastic net regressions with the <strong class="source-inline">Ridge</strong>, <strong class="source-inline">Lasso</strong>, and <strong class="source-inline">ElasticNet</strong> classes, respectively, which can be used in the same way as the <strong class="source-inline">LinearRegression</strong> class. There is also a <strong class="source-inline">CV</strong> version of each of these (<strong class="source-inline">RidgeCV</strong>, <strong class="source-inline">LassoCV</strong>, and <strong class="source-inline">ElasticNetCV</strong>), which features built-in cross-validation. Using all the defaults for <a id="_idIndexMarker1503"/>these models, we find that LASSO performs the best at predicting the length of the year in Earth days with the planet data:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import Ridge, Lasso, ElasticNet</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">ridge, lasso, elastic = Ridge(), Lasso(), ElasticNet()</strong></p>
			<p class="source-code">&gt;&gt;&gt; for model in [ridge, lasso, elastic]:</p>
			<p class="source-code">...     model.fit(pl_X_train, pl_y_train)</p>
			<p class="source-code">...     print(</p>
			<p class="source-code">...         f'{model.__class__.__name__}: ' # get model name</p>
			<p class="source-code">...         f'{model.score(pl_X_test, pl_y_test):.4}'</p>
			<p class="source-code">...     )</p>
			<p class="source-code"><strong class="bold">Ridge: 0.9206</strong></p>
			<p class="source-code"><strong class="bold">Lasso: 0.9208</strong></p>
			<p class="source-code"><strong class="bold">ElasticNet: 0.9047</strong></p>
			<p>Note that these <strong class="source-inline">scikit-learn</strong> classes have an <strong class="source-inline">alpha</strong> parameter, which lines up with λ in the previous equations (not α). For <strong class="source-inline">ElasticNet</strong>, α in the equations lines up with the <strong class="source-inline">l1_ratio</strong> parameter, which defaults to 50% LASSO. In practice, both of these hyperparameters are determined with cross-validation.</p>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor234"/>Summary</h1>
			<p>In this chapter, we reviewed various techniques we can employ to improve model performance. We learned how to use grid search to find the best hyperparameters in a search space, and how to tune our model using the scoring metric of our choosing with <strong class="source-inline">GridSearchCV</strong>. This means we don't have to accept the default in the <strong class="source-inline">score()</strong> method of our model and can customize it to our needs.</p>
			<p>In our discussion of feature engineering, we learned how to reduce the dimensionality of our data using techniques such as PCA and feature selection. We saw how to use the <strong class="source-inline">PolynomialFeatures</strong> class to add interaction terms to models with categorical and numerical features. Then, we learned how to use the <strong class="source-inline">FeatureUnion</strong> class to augment our training data with transformed features. In addition, we saw how decision trees can help us understand which features in the data contribute most to the classification or regression task at hand, using feature importances. This helped us see the importance of sulfur dioxide and chlorides in distinguishing between red and white wine on a chemical level, as well as the importance of a planet's semi-major axis in determining its period.</p>
			<p>Afterward, we took a look at the random forest, gradient boosting, and voting classifiers to discuss ensemble methods and how they seek to address the bias-variance trade-off through bagging, boosting, and voting strategies. We also saw how to measure agreement between classifiers with Cohen's kappa score. This led us to examine our <strong class="source-inline">white_or_red</strong> wine classifier's confidence in its correct and incorrect predictions. Once we know the ins and outs of our model's performance, we can try to improve upon it through the appropriate ensemble method to capitalize on its strengths and mitigate its weaknesses.</p>
			<p>After that, we learned how to use the <strong class="source-inline">imblearn</strong> package to implement over- and under-sampling strategies when faced with a class imbalance. We tried to use this to improve our ability to predict red wine quality scores. In this example, we got some exposure to the k-NN algorithm and the issues with modeling small datasets. Finally, we learned how we can use regularization to penalize high coefficients and reduce overfitting with regression, using ridge (L2 norm), LASSO (L1 norm), and elastic net penalties; remember, LASSO is often used as a method of feature selection since it drives coefficients to zero.</p>
			<p>In the next chapter, we will revisit the simulated login attempt data and use machine learning to detect anomalies. We will also see how we can apply both unsupervised and supervised learning in practice.</p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor235"/>Exercises</h1>
			<p>Complete the following exercises to practice the skills covered in this chapter. Be sure to consult the <em class="italic">Machine learning workflow</em> section in the <em class="italic">Appendix</em> as a refresher on the process of building models:</p>
			<ol>
				<li>Predict star temperature with elastic net linear regression as follows:<p>a) Using the <strong class="source-inline">data/stars.csv</strong> file, build a pipeline to normalize the data with a <strong class="source-inline">MinMaxScaler</strong> object and then run elastic net linear regression using all the numeric columns to predict the temperature of the star.</p><p>b) Run grid search on the pipeline to find the best values for <strong class="source-inline">alpha</strong>, <strong class="source-inline">l1_ratio</strong>, and <strong class="source-inline">fit_intercept</strong> for the elastic net in the search space of your choice.</p><p>c) Train the model on 75% of the initial data.</p><p>d) Calculate the R<span class="superscript">2</span> of your model.</p><p>e) Find the coefficients for each regressor and the intercept.</p><p>f) Visualize the residuals using the <strong class="source-inline">plot_residuals()</strong> function from the <strong class="source-inline">ml_utils.regression</strong> module.</p></li>
				<li>Perform multiclass classification of white wine quality using a support vector machine and feature union as follows:<p>a) Using the <strong class="source-inline">data/winequality-white.csv</strong> file, build a pipeline to standardize data, then create a feature union between interaction terms and a feature selection method of your choice from the <strong class="source-inline">sklearn.feature_selection</strong> module, followed by an SVM (use the <strong class="source-inline">SVC</strong> class).</p><p>b) Run grid search on your pipeline with 85% of the data to find the best values for the <strong class="source-inline">include_bias</strong> parameter (<strong class="source-inline">PolynomialFeatures</strong>) and the <strong class="source-inline">C</strong> parameter (<strong class="source-inline">SVC</strong>) in the search space of your choosing with <strong class="source-inline">scoring='f1_macro'</strong>.</p><p>c) Look at the classification report for your model.</p><p>d) Create a confusion matrix using the <strong class="source-inline">confusion_matrix_visual()</strong> function from the <strong class="source-inline">ml_utils.classification</strong> module.</p><p>e) Plot a precision-recall curve for multiclass data using the <strong class="source-inline">plot_multiclass_pr_curve()</strong> function from the <strong class="source-inline">ml_utils.classification</strong> module.</p></li>
				<li>Perform multiclass classification of white wine quality using k-NN and over-sampling as follows:<p>a) Using the <strong class="source-inline">data/winequality-white.csv</strong> file, create a test and training set with 85% of the data in the training set. Stratify on <strong class="source-inline">quality</strong>.</p><p>b) With <strong class="source-inline">imblearn</strong>, use the <strong class="source-inline">RandomOverSampler</strong> class to over-sample the minority quality scores.</p><p>c) Build a pipeline to standardize data and run k-NN.</p><p>d) Run grid search on your pipeline with the over-sampled data on the search space of your choosing to find the best value for k-NN's <strong class="source-inline">n_neighbors</strong> parameter with <strong class="source-inline">scoring='f1_macro'</strong>.</p><p>e) Look at the classification report for your model.</p><p>f) Create a confusion matrix using the <strong class="source-inline">confusion_matrix_visual()</strong> function from the <strong class="source-inline">ml_utils.classification</strong> module.</p><p>g) Plot a precision-recall curve for multiclass data using the <strong class="source-inline">plot_multiclass_pr_curve()</strong> function from the <strong class="source-inline">ml_utils.classification</strong> module.</p></li>
				<li>Can wine type (red or white) help determine the quality score?<p>a) Using the <strong class="source-inline">data/winequality-white.csv</strong> and <strong class="source-inline">data/winequality-red.csv</strong> files, create a dataframe with the concatenated data and a column indicating which wine type the data belongs to (red or white).</p><p>b) Create a test and training set with 75% of the data in the training set. Stratify on <strong class="source-inline">quality</strong>.</p><p>c) Build a pipeline using a <strong class="source-inline">ColumnTransformer</strong> object to standardize the numeric data while one-hot encoding the wine type column (something like <strong class="source-inline">is_red</strong> and <strong class="source-inline">is_white</strong>, each with binary values), and then train a random forest.</p><p>d) Run grid search on your pipeline with the search space of your choosing to find the best value for the random forest's <strong class="source-inline">max_depth</strong> parameter with <strong class="source-inline">scoring='f1_macro'</strong>.</p><p>e) Take a look at the feature importances from the random forest.</p><p>f) Look at the classification report for your model.</p><p>g) Plot a ROC curve for multiclass data using the <strong class="source-inline">plot_multiclass_roc()</strong> function from the <strong class="source-inline">ml_utils.classification</strong> module.</p><p>h) Create a confusion matrix using the <strong class="source-inline">confusion_matrix_visual()</strong> function from the <strong class="source-inline">ml_utils.classification</strong> module.</p></li>
				<li>Make a multiclass classifier to predict wine quality with majority rules voting by performing the following steps:<p>a) Using the <strong class="source-inline">data/winequality-white.csv</strong> and <strong class="source-inline">data/winequality-red.csv</strong> files, create a dataframe with concatenated data and a column indicating which wine type the data belongs to (red or white).</p><p>b) Create a test and training set with 75% of the data in the training set. Stratify on <strong class="source-inline">quality</strong>.</p><p>c) Build a pipeline for each of the following models: random forest, gradient boosting, k-NN, logistic regression, and Naive Bayes (<strong class="source-inline">GaussianNB</strong>). The pipeline should use a <strong class="source-inline">ColumnTransformer</strong> object to standardize the numeric data while one-hot encoding the wine type column (something like <strong class="source-inline">is_red</strong> and <strong class="source-inline">is_white</strong>, each with binary values), and then build the model. Note that we will discuss Naive Bayes in <a href="B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Machine Learning Anomaly Detection</em>.</p><p>d) Run grid search on each pipeline except Naive Bayes (just run <strong class="source-inline">fit()</strong> on it) with <strong class="source-inline">scoring='f1_macro'</strong> on the search space of your choosing to find the best values for the following:</p><p>i) <strong class="bold">Random forest</strong>: <strong class="source-inline">max_depth</strong></p><p>ii) <strong class="bold">Gradient boosting</strong>: <strong class="source-inline">max_depth</strong></p><p>iii) <strong class="bold">k-NN</strong>: <strong class="source-inline">n_neighbors</strong></p><p>iv) <strong class="bold">Logistic regression</strong>: <strong class="source-inline">C</strong></p><p>e) Find the level of agreement between each pair of two models using the <strong class="source-inline">cohen_kappa_score()</strong> function from the <strong class="source-inline">metrics</strong> module in <strong class="source-inline">scikit-learn</strong>. Note that you can get all the combinations of the two easily using the <strong class="source-inline">combinations()</strong> function from the <strong class="source-inline">itertools</strong> module in the Python standard library.</p><p>f) Build a voting classifier with the five models built using majority rules (<strong class="source-inline">voting='hard'</strong>) and weighting the Naive Bayes model half as much as the others.</p><p>g) Look at the classification report for your model.</p><p>h) Create a confusion matrix using the <strong class="source-inline">confusion_matrix_visual()</strong> function from the <strong class="source-inline">ml_utils.classification</strong> module.</p></li>
			</ol>
			<h1 id="_idParaDest-237"><a id="_idTextAnchor236"/>Further reading</h1>
			<p>Check out the following resources for more information on the topics covered in this chapter:</p>
			<ul>
				<li><em class="italic">A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning</em>: <a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/">https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/</a></li>
				<li><em class="italic">A Kaggler's Guide to Model Stacking in Practice</em>: <a href="https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/">https://datasciblog.github.io/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/</a><span class="hidden"> </span></li>
				<li><em class="italic">Choosing the right estimator</em>: <a href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html">https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html</a></li>
				<li><em class="italic">Cross-validation: evaluating estimator performance</em>: <a href="https://scikit-learn.org/stable/modules/cross_validation.html">https://scikit-learn.org/stable/modules/cross_validation.html</a></li>
				<li><em class="italic">Decision Trees in Machine Learning</em>: <a href="https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052">https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052</a></li>
				<li><em class="italic">Ensemble Learning to Improve Machine Learning Results</em>: <a href="https://blog.statsbot.co/ensemble-learning-d1dcd548e936">https://blog.statsbot.co/ensemble-learning-d1dcd548e936</a></li>
				<li><em class="italic">Ensemble Methods</em>: <a href="https://scikit-learn.org/stable/modules/ensemble.html">https://scikit-learn.org/stable/modules/ensemble.html</a></li>
				<li><em class="italic">Feature Engineering Made Easy by Divya Susarla and Sinan Ozdemir</em>: <a href="https://www.packtpub.com/big-data-and-business-intelligence/feature-engineering-made-easy">https://www.packtpub.com/big-data-and-business-intelligence/feature-engineering-made-easy</a></li>
				<li><em class="italic">Feature Selection</em>: <a href="https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection">https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection</a></li>
				<li><em class="italic">Gradient Boosting vs Random Forest</em>: <a href="mailto:https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80">https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80</a></li>
				<li><em class="italic">Hyperparameter Optimization in Machine Learning</em>: <a href="https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models">https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models</a></li>
				<li><em class="italic">L1 Norms versus L2 Norms</em>: <a href="https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms">https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms</a></li>
				<li><em class="italic">Modern Machine Learning Algorithms: Strengths and Weaknesses</em>: <a href="https://elitedatascience.com/machine-learning-algorithms">https://elitedatascience.com/machine-learning-algorithms</a></li>
				<li><em class="italic">Principal component analysis</em>: <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">https://en.wikipedia.org/wiki/Principal_component_analysis</a></li>
				<li><em class="italic">Regularization in Machine Learning</em>: <a href="https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a">https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a</a></li>
				<li><em class="italic">The Elements of Statistical Learning by Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie</em>: <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">https://web.stanford.edu/~hastie/ElemStatLearn/</a></li>
			</ul>
		</div>
	</body></html>