- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anticipating Data Cleaning Issues When Importing Tabular Data with pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scientific distributions of **Python** (Anaconda, WinPython, Canopy, and so
    on) provide analysts with an impressive range of data manipulation, exploration,
    and visualization tools. One important tool is pandas. Developed by Wes McKinney
    in 2008, but really gaining in popularity after 2012, pandas is now an essential
    library for data analysis in Python. The recipes in this book demonstrate how
    many common data preparation tasks can be done more easily with pandas than with
    other tools. While we work with pandas extensively in this book, we also use other
    popular packages such as Numpy, matplotlib, and scipy.
  prefs: []
  type: TYPE_NORMAL
- en: A key pandas object is the **DataFrame**, which represents data as a tabular
    structure, with rows and columns. In this way, it is similar to the other data
    stores we discuss in this chapter. However, a pandas DataFrame also has indexing
    functionality that makes selecting, combining, and transforming data relatively
    straightforward, as the recipes in this book will demonstrate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can make use of this great functionality, we have to import our data
    into pandas. Data comes to us in a wide variety of formats: as CSV or Excel files,
    as tables from SQL databases, from statistical analysis packages such as SPSS,
    Stata, SAS, or R, from non-tabular sources such as JSON, and from web pages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We examine tools to import tabular data in this recipe. Specifically, we cover
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Importing CSV files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing Excel files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing data from SQL databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing SPSS, Stata, and SAS data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing R data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persisting tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code and notebooks for this chapter are available on GitHub at [https://github.com/michaelbwalker/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/michaelbwalker/Python-Data-Cleaning-Cookbook-Second-Edition).
    You can use any **IDE** (**Integrated Development Environment**) of your choice
    – IDLE, Visual Studio, Sublime, Spyder, and so on – or Jupyter Notebook to work
    with any of the code in this chapter, or any chapter in this book. A good guide
    to get started with Jupyter Notebook can be found here: [https://www.dataquest.io/blog/jupyter-notebook-tutorial/](https://www.dataquest.io/blog/jupyter-notebook-tutorial/).
    I used the Spyder IDE to write the code in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: I used pandas 2.2.1 and NumPy version 1.24.3 for all of the code in this chapter
    and subsequent chapters. I have also tested all code with pandas 1.5.3.
  prefs: []
  type: TYPE_NORMAL
- en: Importing CSV files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `read_csv` method of the `pandas` library can be used to read a file with
    **comma separated values** (**CSV**) and load it into memory as a pandas DataFrame.
    In this recipe, we import a CSV file and address some common issues: creating
    column names that make sense to us, parsing dates, and dropping rows with critical
    missing data.'
  prefs: []
  type: TYPE_NORMAL
- en: Raw data is often stored as CSV files. These files have a carriage return at
    the end of each line of data to demarcate a row, and a comma between each data
    value to delineate columns. Something other than a comma can be used as the delimiter,
    such as a tab. Quotation marks may be placed around values, which can be helpful
    when the delimiter occurs naturally within certain values, which sometimes happens
    with commas.
  prefs: []
  type: TYPE_NORMAL
- en: All data in a CSV file are characters, regardless of the logical data type.
    This is why it is easy to view a CSV file, presuming it is not too large, in a
    text editor. The pandas `read_csv` method will make an educated guess about the
    data type of each column, but you will need to help it along to ensure that these
    guesses are on the mark.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a folder for this chapter, and then create a new Python script or **Jupyter
    Notebook** file in that folder. Create a data subfolder, and then place the `landtempssample.csv`
    file in that subfolder. Alternatively, you could retrieve all of the files from
    the GitHub repository, including the data files. Here is a screenshot of the beginning
    of the CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot from 2023-05-28 21-00-25](img/B18596_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Land Temperatures Data'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset, taken from the Global Historical Climatology Network integrated
    database, is made available for public use by the United States National Oceanic
    and Atmospheric Administration at [https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly).
    I used the data from version 4\. The data in this recipe uses a 100,000-row sample
    of the full dataset, which is also available in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will import a CSV file into pandas, taking advantage of some very useful
    `read_csv` options:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` library, and set up the environment to make viewing the
    output easier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Read the data file, set new names for the headings, and parse the date column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass an argument of `1` to the `skiprows` parameter to skip the first row,
    pass a list of columns to `parse_dates` to create a pandas datetime column from
    those columns, and set `low_memory` to `False`. This will cause pandas to load
    all of the data into memory at once, rather than in chunks. We do this so that
    pandas can identify the data type of each column automatically. In the *There’s
    more…* section, we see how to set the data type for each column manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: We have to use `skiprows` because we are passing a list of column names to `read_csv`.
    If we use the column names in the CSV file, we do not need to specify values for
    either `names` or `skiprows`.
  prefs: []
  type: TYPE_NORMAL
- en: Get a quick glimpse of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'View the first few rows. Show the data type for all columns, as well as the
    number of rows and columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Give the date column a more appropriate name and view the summary statistics
    for average monthly temperature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Look for missing values for each column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use `isnull`, which returns `True` for each value that is missing for each
    column, and `False` when not missing. Chain this with `sum` to count the missing
    values for each column. (When working with Boolean values, `sum` treats `True`
    as `1` and `False` as `0`. I will discuss method chaining in the *There’s more...*
    section of this recipe):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Remove rows with missing data for `avgtemp`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `subset` parameter to tell `dropna` to drop rows when `avgtemp` is
    missing. Set `inplace` to `True`. Leaving `inplace` at its default value of `False`
    would display the DataFrame, but the changes we have made would not be retained.
    Use the `shape` attribute of the DataFrame to get the number of rows and columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! Importing CSV files into pandas is as simple as that.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost all of the recipes in this book use the `pandas` library. We refer to
    it as `pd` to make it easier to reference later. This is customary. We also use
    `float_format` to display float values in a readable way and `set_option` to make
    the Terminal output wide enough to accommodate the number of variables.
  prefs: []
  type: TYPE_NORMAL
- en: Much of the work is done by the first line in *Step 2*. We use `read_csv` to
    load a pandas DataFrame in memory and call it `landtemps`. In addition to passing
    a filename, we set the `names` parameter to a list of our preferred column headings.
    We also tell `read_csv` to skip the first row, by setting `skiprows` to 1, since
    the original column headings are in the first row of the CSV file. If we do not
    tell it to skip the first row, `read_csv` will treat the header row in the file
    as actual data.
  prefs: []
  type: TYPE_NORMAL
- en: '`read_csv` also solves a date conversion issue for us. We use the `parse_dates`
    parameter to ask it to convert the `month` and `year` columns to a date value.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3* runs through a few standard data checks. We use `head(7)` to print
    out all columns for the first seven rows. We use the `dtypes` attribute of the
    DataFrame to show the data type of all columns. Each column has the expected data
    type. In pandas, character data has the object data type, a data type that allows
    for mixed values. `shape` returns a tuple, whose first element is the number of
    rows in the DataFrame (100,000 in this case) and whose second element is the number
    of columns (9).'
  prefs: []
  type: TYPE_NORMAL
- en: When we used `read_csv` to parse the `month` and `year` columns, it gave the
    resulting column the name `month_year`. We used the `rename` method in *Step 4*
    to give that column a more appropriate name. We need to specify `inplace=True`
    to replace the old column name with the new column name in memory. The `describe`
    method provides summary statistics on the `avgtemp` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the count for `avgtemp` indicates that there are 85,554 rows that
    have valid values for `avgtemp`. This is out of 100,000 rows for the whole DataFrame,
    as provided by the `shape` attribute. The listing of missing values for each column
    in *Step 5* (`landtemps.isnull().sum()`) confirms this: *100,000 – 85,554 = 14,446*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 6* drops all rows where `avgtemp` is `NaN`. (The `NaN` value, not a number,
    is the pandas representation of missing values.) `subset` is used to indicate
    which column to check for missing values. The `shape` attribute for `landtemps`
    now indicates that there are 85,554 rows, which is what we would expect, given
    the previous count from `describe`.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the file you are reading uses a delimiter other than a comma, such as a tab,
    this can be specified in the `sep` parameter of `read_csv`. When creating the
    pandas DataFrame, an index was also created. The numbers to the far left of the
    output when `head` was run are index values. Any number of rows can be specified
    for `head`. The default value is `5`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of setting `low_memory` to `False`, to get pandas to make good guesses
    regarding data types, we could have set data types manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `landtemps.isnull().sum()` statement is an example of chaining methods.
    First, `isnull` returns a DataFrame of `True` and `False` values, resulting from
    testing whether each column value is `null`. The `sum` function takes that DataFrame
    and sums the `True` values for each column, interpreting the `True` values as
    `1` and the `False` values as `0`. We would have obtained the same result if we
    had used the following two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: There is no hard and fast rule for when to chain methods and when not to do
    so. I find chaining helpful when the overall operation feels like a single step,
    even if it’s two or more steps mechanically. Chaining also has the side benefit
    of not creating extra objects that I might not need.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset used in this recipe is just a sample from the full land temperatures
    database, with almost 17 million records. You can run the larger file if your
    machine can handle it, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`read_csv` can read a compressed ZIP file. We get it to do this by passing
    the name of the ZIP file and the type of compression.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Subsequent recipes in this chapter, and in other chapters, set indexes to improve
    navigation over rows and merging.
  prefs: []
  type: TYPE_NORMAL
- en: A significant amount of reshaping of the Global Historical Climatology Network
    raw data was done before using it in this recipe. We demonstrate this in *Chapter
    11*, *Tidying and Reshaping Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Importing Excel files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `read_excel` method of the `pandas` library can be used to import data
    from an Excel file and load it into memory as a pandas DataFrame. In this recipe,
    we import an Excel file and handle some common issues when working with Excel
    files: extraneous header and footer information, selecting specific columns, removing
    rows with no data, and connecting to particular sheets.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the tabular structure of Excel, which invites the organization of data
    into rows and columns, spreadsheets are not datasets and do not require people
    to store data in that way. Even when some data conforms with those expectations,
    there is often additional information in rows or columns before or after the data
    to be imported. Data types are not always as clear as they are to the person who
    created the spreadsheet. This will be all too familiar to anyone who has ever
    battled with importing leading zeros. Moreover, Excel does not insist that all
    data in a column be of the same type, or that column headings be appropriate for
    use with a programming language such as Python.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, `read_excel` has a number of options for handling messiness in
    Excel data. These options make it relatively easy to skip rows, select particular
    columns, and pull data from a particular sheet or sheets.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can download the `GDPpercapita22b.xlsx` file, as well as the code for this
    recipe, from the GitHub repository for this book. The code assumes that the Excel
    file is in a data subfolder. Here is a view of the beginning of the file (some
    columns were hidden for display purposes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: View of the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'And here is a view of the end of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: View of the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset, from the Organisation for Economic Co-operation and Development,
    is available for public use at [https://stats.oecd.org/](https://stats.oecd.org/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import an Excel file into pandas and do some initial data cleaning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Read the Excel per capita GDP data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the sheet with the data we need, but skip the columns and rows that
    we do not want. Use the `sheet_name` parameter to specify the sheet. Set `skiprows`
    to `4` and `skipfooter` to `1` to skip the first four rows (the first row is hidden)
    and the last row. We provide values for `usecols` to get data from column `A`
    and columns `C` through `W` (column `B` is blank). Use `head` to view the first
    few rows and `shape` to get the number of rows and columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: You may encounter a problem with `read_excel` if the Excel file does not use
    utf-8 encoding. One way to resolve this is to save the Excel file as a CSV file,
    reopen it, and then save it with utf-8 encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `info` method of the DataFrame to view data types and the `non-null`
    count. Notice that all columns have the `object` data type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Rename the `Year` column to `metro`, and remove the leading spaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Give an appropriate name to the metropolitan area column. There are extra spaces
    before the metro values in some cases. We can test for leading spaces with `startswith(''
    '')` and then use `any` to establish whether there are one or more occasions when
    the first character is blank. We can use `endswith('' '')` to examine trailing
    spaces. We use strip to remove both leading and trailing spaces. When we test
    for trailing spaces again, we see that there are none:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Convert the data columns to numeric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate over all of the GDP year columns (2000–2020) and convert the data type
    from `object` to `float`. Coerce the conversion even when there is character data
    – the `..` in this example. We want character values in those columns to become
    `missing`, which is what happens. Rename the year columns to better reflect the
    data in those columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `describe` method to generate summary statistics for all numeric data
    in the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remove rows where all of the per capita GDP values are missing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `subset` parameter of `dropna` to inspect all columns, starting with
    the second column (it is zero-based) and going through to the last column. Use
    `how` to specify that we want to drop rows only if all of the columns specified
    in `subset` are missing. Use `shape` to show the number of rows and columns in
    the resulting DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Set the index for the DataFrame using the metropolitan area column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Confirm that there are 692 valid values for `metro` and that there are 692
    unique values, before setting the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We have now imported the Excel data into a pandas DataFrame and cleaned up some
    of the messiness in the spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We mostly manage to get the data we want in *Step 2* by skipping rows and columns
    we do not want, but there are still a number of issues – `read_excel` interprets
    all of the GDP data as character data, many rows are loaded with no useful data,
    and the column names do not represent the data well. In addition, the metropolitan
    area column might be useful as an index, but there are leading and trailing blanks,
    and there may be missing or duplicated values.
  prefs: []
  type: TYPE_NORMAL
- en: '`read_excel` interprets `Year` as the column name for the metropolitan area
    data because it looks for a header above the data for that Excel column and finds
    `Year` there. We rename that column `metro` in *Step 4*. We also use `strip` to
    fix the problem with leading and trailing blanks. We could have just used `lstrip`
    to remove leading blanks, or `rstrip` if there had been trailing blanks. It is
    a good idea to assume that there might be leading or trailing blanks in any character
    data, cleaning that data shortly after the initial import.'
  prefs: []
  type: TYPE_NORMAL
- en: The spreadsheet authors used `..` to represent missing data. Since this is actually
    valid character data, those columns get the object data type (that is how pandas
    treats columns with character or mixed data). We coerce a conversion to numeric
    type in *Step 5*. This also results in the original values of `..` being replaced
    with `NaN` (not a number), how pandas represents missing values for numbers. This
    is what we want.
  prefs: []
  type: TYPE_NORMAL
- en: We can fix all of the per capita GDP columns with just a few lines because pandas
    makes it easy to iterate over the columns of a DataFrame. By specifying `[1:]`,
    we iterate from the second column to the last column. We can then change those
    columns to numeric and rename them to something more appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: There are several reasons why it is a good idea to clean up the column headings
    for the annual GDP columns – it helps us to remember what the data actually is;
    if we merge it with other data by metropolitan area, we will not have to worry
    about conflicting variable names; and we can use attribute access to work with
    pandas Series based on those columns, which I will discuss in more detail in the
    *There’s more…* section of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '`describe` in *Step 6* shows us that fewer than 500 rows have valid data for
    per capita GDP for some years. When we drop all rows that have missing values
    for all per capita GDP columns in *step 7*, we end up with 692 rows in the DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have a pandas DataFrame, we have the ability to treat columns as more
    than just columns. We can use attribute access (such as `percapitaGPA.metro`)
    or bracket notation (`percapitaGPA['metro']`) to get the functionality of a pandas
    Series. Either method makes it possible to use Series string inspecting methods,
    such as `str.startswith`, and counting methods, such as `nunique`. Note that the
    original column names of `20##` did not allow attribute access because they started
    with a number, so `percapitaGDP.pcGDP2001.count()` works, but `percapitaGDP.2001.count()`
    returns a syntax error because `2001` is not a valid Python identifier (since
    it starts with a number).
  prefs: []
  type: TYPE_NORMAL
- en: pandas is rich with features for string manipulation and for Series operations.
    We will try many of them out in subsequent recipes. This recipe showed those that
    I find most useful when importing Excel data.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are good reasons to consider reshaping this data. Instead of 21 columns
    of GDP per capita data for each metropolitan area, we should have 21 rows of data
    for each metropolitan area, with columns for year and GDP per capita. Recipes
    for reshaping data can be found in *Chapter 11*, *Tidying and Reshaping Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data from SQL databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use `pymssql` and `mysql apis` to read data from **Microsoft
    SQL Server** and **MySQL** (now owned by **Oracle**) databases, respectively.
    Data from sources such as these tends to be well structured, since it is designed
    to facilitate simultaneous transactions by members of organizations and those
    who interact with them. Each transaction is also likely related to some other
    organizational transaction.
  prefs: []
  type: TYPE_NORMAL
- en: This means that although data tables from enterprise systems such as these are
    more reliably structured than data from CSV files and Excel files, their logic
    is less likely to be self-contained. You need to know how the data from one table
    relates to data from another table to understand its full meaning. These relationships
    need to be preserved, including the integrity of primary and foreign keys, when
    pulling data. Moreover, well-structured data tables are not necessarily uncomplicated
    data tables. There are often sophisticated coding schemes that determine data
    values, and these coding schemes can change over time. For example, codes for
    merchandise at a retail store chain might be different in 1998 than they are in
    2024\. Similarly, frequently there are codes for missing values, such as 99,999,
    that pandas will understand as valid values.
  prefs: []
  type: TYPE_NORMAL
- en: Since much of this logic is business logic, and implemented in stored procedures
    or other applications, it is lost when pulled out of this larger system. Some
    of what is lost will eventually have to be reconstructed when preparing data for
    analysis. This almost always involves combining data from multiple tables, so
    it is important to preserve the ability to do that. However, it also may involve
    adding some of the coding logic back after loading the SQL table into a pandas
    DataFrame. We explore how to do that in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe assumes you have `pymssql` and `mysql apis` installed. If you do
    not, it is relatively straightforward to install them with `pip`. From the Terminal,
    or `powershell` (in Windows), enter `pip install pymssql` or `pip install mysql-connector-python`.
    We will work with data on educational attainment in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used in this recipe is available for public use at [https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip](https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import SQL Server and MySQL data tables into a pandas DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `pandas`, `numpy`, `pymssql`, and `mysql`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This step assumes that you have installed `pymssql` and `mysql apis`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Use `pymssql api` and `read_sql` to retrieve and load data from a SQL Server
    instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the columns we want from the SQL Server data, and use SQL aliases to
    improve column names (for example, `fedu AS fathereducation`). Create a connection
    to the SQL Server data by passing database credentials to the `pymssql` `connect`
    function. Create a pandas DataFrame by passing the `SELECT` statement and connection
    object to `read_sql`. Use `close` to return the connection to the pool on the
    server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: Although tools such as `pymssql` make connecting to a SQL Server instance relatively
    straightforward, the syntax still might take a little time to get used to if it
    is unfamiliar. The previous step shows the parameter values you will typically
    need to pass to a connection object – the name of the server, the name of a user
    with credentials on the server, the password for that user, and the name of a
    SQL database on the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the data types and the first few rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Connecting to a MySQL server is not very different from connecting to a SQL
    Server instance. We can use the `connect` method of the `mysql` connector to do
    that and then use `read_sql` to load the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a connection to the `mysql` data, pass that connection to `read_sql`
    to retrieve the data, and load it into a pandas DataFrame (the same data file
    on student math scores was uploaded to SQL Server and MySQL, so we can use the
    same SQL `SELECT` statement we used in the previous step):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Rearrange the columns, set an index, and check for missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Move the grade data to the left of the DataFrame, just after `studentid`. Also,
    move the `freetime` column to the right after `traveltime` and `studytime`. Confirm
    that each row has an ID and that the IDs are unique, and set `studentid` as the
    index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the DataFrame’s `count` function to check for missing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Replace coded data values with more informative values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a dictionary with the replacement values for the columns, and then use
    `replace` to set those values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Change the type for columns with the changed data to `category`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check any changes in memory usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Calculate percentages for values in the `famrel` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run `value_counts`, and set `normalize` to `True` to generate percentages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `apply` to calculate percentages for multiple columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding steps retrieved a data table from a SQL database, loaded that
    data into pandas, and did some initial data checking and cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since data from enterprise systems is typically better structured than CSV or
    Excel files, we do not need to do things such as skip rows or deal with different
    logical data types in a column. However, some massaging is still usually required
    before we can begin exploratory analysis. There are often more columns than we
    need, and some column names are not intuitive or not ordered in the best way for
    analysis. The meaningfulness of many data values is not stored in the data table
    to avoid entry errors and save on storage space. For example, `3` is stored for
    mother’s education rather than secondary education. It is a good idea to reconstruct
    that coding as early in the cleaning process as possible.
  prefs: []
  type: TYPE_NORMAL
- en: To pull data from a SQL database server, we need a connection object to authenticate
    us on the server, as well as a SQL select string. These can be passed to `read_sql`
    to retrieve the data and load it into a pandas DataFrame. I usually use the SQL
    `SELECT` statement to do a bit of cleanup of column names at this point. I sometimes
    also reorder columns, but I did that later in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We set the index in *Step 5*, first confirming that every row has a value for
    `studentid` and that it is unique. This is often more important when working with
    enterprise data because we will almost always need to merge the retrieved data
    with other data files on the system. Although an index is not required for this
    merging, the discipline of setting one prepares us for the tricky business of
    merging data further down the road. It will also likely improve the speed of the
    merge.
  prefs: []
  type: TYPE_NORMAL
- en: We use the DataFrame’s `count` function to check for missing values and that
    there are no missing values – for non-missing values, the count is 395 (the number
    of rows) for every column. This is almost too good to be true. There may be values
    that are logically missing – that is, valid numbers that nonetheless connote missing
    values, such as `-1`, `0`, `9`, or `99`. We address this possibility in the next
    step.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 7* demonstrates a useful technique for replacing data values for multiple
    columns. We create a dictionary to map original values to new values for each
    column and then run it using `replace`. To reduce the amount of storage space
    taken up by the new verbose values, we convert the data type of those columns
    to `category`. We do this by generating a list of the keys of our `setvalues`
    dictionary – `setvalueskeys = [k for k in setvalues]` generates [`famrel`, `freetime`,
    `goout`, `mothereducation`, and `fathereducation`]. We then iterate over those
    five columns and use the `astype` method to change the data type to `category`.
    Notice that the memory usage for those columns is reduced substantially.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we check the assignment of new values by using `value_counts` to view
    relative frequencies. We use `apply` because we want to run `value_counts` on
    multiple columns. To prevent `value_counts` sorting by frequency, we set sort
    to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame `replace` method is also a handy tool for dealing with logical
    missing values that will not be recognized as missing when retrieved by `read_sql`.
    The `0` values for `mothereducation` and `fathereducation` seem to fall into that
    category. We fix this problem in the `setvalues` dictionary by indicating that
    the `0` values for `mothereducation` and `fathereducation` should be replaced
    with `NaN`. It is important to address these kinds of missing values shortly after
    the initial import because they are not always obvious and can significantly impact
    all subsequent work.
  prefs: []
  type: TYPE_NORMAL
- en: Users of packages such as *SPPS*, *SAS*, and *R* will notice the difference
    between this approach and value labels in SPSS and R, as well as the `proc` format
    in SAS. In pandas, we need to change the actual data to get more informative values.
    However, we reduce how much data is actually stored by giving the column a `category`
    data type. This is similar to `factors` in R.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I moved the grade data to near the beginning of the DataFrame. I find it helpful
    to have potential target or dependent variables in the leftmost columns, keeping
    them at the forefront of your mind. It is also helpful to keep similar columns
    together. In this example, personal demographic variables (sex and age) are next
    to one another, as are family variables (`mothereducation` and `fathereducation`),
    and how students spend their time (`traveltime`, `studytime`, and `freetime`).
  prefs: []
  type: TYPE_NORMAL
- en: You could have used `map` instead of `replace` in *Step 7*. Prior to version
    19.2 of pandas, `map` was significantly more efficient. Since then, the difference
    in efficiency has been much smaller. If you are working with a very large dataset,
    the difference may still be enough to consider using `map`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recipes in *Chapter 10*, *Addressing Data Issues When Combining DataFrames*,
    go into detail on merging data. We will take a closer look at bivariate and multivariate
    relationships between variables in *Chapter 4*, *Identifying Outliers in Subsets
    of Data*. We will demonstrate how to use some of these same approaches in packages
    such as SPSS, SAS, and R in subsequent recipes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Importing SPSS, Stata, and SAS data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use `pyreadstat` to read data from three popular statistical packages
    into pandas. The key advantage of `pyreadstat` is that it allows data analysts
    to import data from these packages without losing metadata, such as variable and
    value labels.
  prefs: []
  type: TYPE_NORMAL
- en: The SPSS, Stata, and SAS data files we receive often come to us with the data
    issues of CSV and Excel files and SQL databases having been resolved. We do not
    typically have the invalid column names, changes in data types, and unclear missing
    values that we can get with CSV or Excel files, nor do we usually get the detachment
    of data from business logic, such as the meaning of data codes, that we often
    get with SQL data. When someone or some organization shares a data file from one
    of these packages with us, they have often added variable labels and value labels
    for categorical data. For example, a hypothetical data column called `presentsat`
    has the `overall satisfaction with presentation` variable label and `1`–`5` value
    labels, with `1` being not at all satisfied and `5` being highly satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge is retaining that metadata when importing data from those systems
    into pandas. There is no precise equivalent to variable and value labels in pandas,
    and built-in tools for importing SAS, Stata, and SAS data lose the metadata. In
    this recipe, we will use `pyreadstat` to load variable and value label information
    and use a couple of techniques to represent that information in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe assumes you have installed the `pyreadstat` package. If it is not
    installed, you can install it with `pip`. From the Terminal, or Powershell (in
    Windows), enter `pip install pyreadstat`. You will need the SPSS, Stata, and SAS
    data files for this recipe to run the code.
  prefs: []
  type: TYPE_NORMAL
- en: We will work with data from the United States **National Longitudinal Surveys**
    (**NLS**) of Youth.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: The NLS of Youth is conducted by the United States Bureau of Labor Statistics.
    This survey started with a cohort of individuals in 1997\. Each survey respondent
    was high school age when they first completed the survey, having been born between
    1980 and 1985\. There were annual follow-up surveys each year through 2023\. For
    this recipe, I pulled 42 variables on grades, employment, income, and attitudes
    toward government, from the hundreds of data items on the survey. Separate files
    for SPSS, Stata, and SAS can be downloaded from the repository.
  prefs: []
  type: TYPE_NORMAL
- en: The original NLS data can be downloaded from [https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search),
    along with code for creating SPSS, Stata, or SAS files from the ASCII data files
    included in the download.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will import data from SPSS, Stata, and SAS, retaining metadata such as value
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `pandas`, `numpy`, and `pyreadstat`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This step assumes that you have installed `pyreadstat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Retrieve the SPSS data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass a path and filename to the `read_sav` method of `pyreadstat`. Display
    the first few rows and a frequency distribution. Note that the column names and
    value labels are non-descriptive, and that `read_sav` returns both a pandas DataFrame
    and a `meta` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Grab the metadata to improve column labels and value labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `metaspss` object created when we called `read_sav` has the column labels
    and the value labels from the SPSS file. Use the `variable_value_labels` dictionary
    to map values to value labels for one column (`R0536300`). (This does not change
    the data. It only improves our display when we run `value_counts`.) Use the `set_value_labels`
    method to actually apply the value labels to the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Use column labels in the metadata to rename the columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To use the column labels from `metaspss` in our DataFrame, we can simply assign
    the column labels in `metaspss` to our DataFrame’s column names. Clean up the
    column names a bit by changing them to lowercase, changing spaces to underscores,
    and removing all remaining non-alphanumeric characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Simplify the process by applying the value labels from the beginning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The data values can actually be applied in the initial call to `read_sav` by
    setting `apply_value_formats` to `True`. This eliminates the need to call the
    `set_value_labels` function later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Show the columns and a few rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run frequencies on one of the columns, and set the index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That demonstrated how to convert data from SPSS. Let’s try that with Stata data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the Stata data, apply value labels, and improve the column headings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the same methods for the Stata data that we used for the SPSS data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'View a few rows of the data and run frequencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fix the logical missing values that show up with the Stata data and set an
    index. We can use the `replace` method to set any value that is between `–9` and
    `–1` in any column to missing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The process is fairly similar when working with SAS data files, as the next
    few steps illustrate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieve the SAS data, using the SAS catalog file for value labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The data values for SAS are stored in a catalog file. Setting the catalog file
    path and filename retrieves the value labels and applies them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: This demonstrates how to import SPSS, SAS, and Stata data without losing important
    metadata.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `read_sav`, `read_dta`, and `read_sas7bdat` methods of `Pyreadstat`, for
    SPSS, Stata, and SAS data files, respectively, work in a similar manner. Value
    labels can be applied when reading in the data by setting `apply_value_formats`
    to `True` for SPSS and Stata files (*Steps 5 and 8*), or by providing a catalog
    file path and filename for SAS (*Step 12*).
  prefs: []
  type: TYPE_NORMAL
- en: We can set `formats_as_category` to `True` to change the data type to `category`
    for those columns where the data values will change. The meta object has the column
    names and the column labels from the statistical package, so metadata column labels
    can be assigned to pandas DataFrame column names at any point (`nls97spss.columns
    = metaspss.column_labels`). We can even revert to the original column headings
    after assigning meta column labels to them by setting pandas column names to the
    metadata column names (`nls97spss.columns = metaspss.column_names`).
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, we looked at some of the SPSS data before applying value labels.
    We looked at the dictionary for one variable (`metaspss.variable_value_labels['R0536300']`),
    but we could have viewed it for all variables (`metaspss.variable_value_labels`).
    When we are satisfied that the labels make sense, we can set them by calling the
    `set_value_labels` function. This is a good approach when you do not know the
    data well and want to inspect the labels before applying them.
  prefs: []
  type: TYPE_NORMAL
- en: The column labels from the meta object are often a better choice than the original
    column headings. Column headings can be quite cryptic, particularly when the SPSS,
    Stata, or SAS file is based on a large survey, as in this example. However, the
    labels are not usually ideal for column headings either. They sometimes have spaces,
    capitalization that is not helpful, and non-alphanumeric characters. We chain
    some string operations to switch to lowercase, replace spaces with underscores,
    and remove non-alphanumeric characters.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values is not always straightforward with these data files,
    since there are often many reasons why data is missing. If the file is from a
    survey, the missing value may be because of a survey skip pattern, or a respondent
    failed to respond, or the response was invalid, and so on. The NLS has nine possible
    values for missing, from `–1` to `–9`. The SPSS import automatically set those
    values to `NaN`, while the Stata import retained the original values. (We could
    have gotten the SPSS import to retain those values by setting `user_missing` to
    `True`.) For the Stata data, we need to tell it to replace all values from `–1`
    to `–9` with `NaN`. We do this by using the DataFrame’s `replace` function and
    passing it a list of integers from `–9` to `–1` (`list(range(-9,0))`).
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed similarities between this recipe and the previous one in
    terms of how value labels are set. The `set_value_labels` function is like the
    DataFrame `replace` operation we used to set value labels in that recipe. We passed
    a dictionary to `replace` that mapped columns to value labels. The `set_value_labels`
    function in this recipe essentially does the same thing, using the `variable_value_labels`
    property of the meta object as the dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Data from statistical packages is often not as well structured as SQL databases
    tend to be in one significant way. Since they are designed to facilitate analysis,
    they often violate database normalization rules. There is often an implied relational
    structure that might have to be *unflattened* at some point. For example, the
    data may combine individual and event-level data – a person and hospital visits,
    a brown bear and the date it emerged from hibernation. Often, this data will need
    to be reshaped for some aspects of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `pyreadstat` package is nicely documented at [https://github.com/Roche/pyreadstat](https://github.com/Roche/pyreadstat).
    The package has many useful options for selecting columns and handling missing
    data that space did not permit me to demonstrate in this recipe. In *Chapter 11*,
    *Tidying and Reshaping Data*, we will examine how to normalize data that may have
    been flattened for analytical purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Importing R data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use `pyreadr` to read an R data file into pandas. Since `pyreadr` cannot
    capture the metadata, we will write code to reconstruct value labels (analogous
    to R factors) and column headings. This is similar to what we did in the *Importing
    data from SQL databases* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The R statistical package is, in many ways, similar to the combination of Python
    and pandas, at least in its scope. Both have strong tools across a range of data
    preparation and data analysis tasks. Some data scientists work with both R and
    Python, perhaps doing data manipulation in Python and statistical analysis in
    R, or vice versa, depending on their preferred packages. However, there is currently
    a scarcity of tools for reading data saved in R, as `rds` or `rdata` files, into
    Python. The analyst often saves the data as a CSV file first and then loads it
    into Python. We will use `pyreadr`, from the same author as `pyreadstat`, because
    it does not require an installation of R.
  prefs: []
  type: TYPE_NORMAL
- en: When we receive an R file, or work with one we have created ourselves, we can
    count on it being fairly well structured, at least compared to CSV or Excel files.
    Each column will have only one data type, column headings will have appropriate
    names for Python variables, and all rows will have the same structure. However,
    we may need to restore some of the coding logic, as we did when working with SQL
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe assumes you have installed the `pyreadr` package. If it is not installed,
    you can install it with `pip`. From the Terminal, or Powershell (in Windows),
    enter `pip install pyreadr`.
  prefs: []
  type: TYPE_NORMAL
- en: We will again work with the NLS in this recipe. You will need to download the
    `rds` file used in this recipe from the GitHub repository in order to run the
    code.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will import data from R without losing important metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `pandas`, `numpy`, `pprint`, and the `pyreadr` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Get the R data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass the path and filename to the `read_r` method to retrieve the R data, and
    load it into memory as a pandas DataFrame. `read_r` can return one or more objects.
    When reading an `rds` file (as opposed to an `rdata` file), it will return one
    object, having the key `None`. We indicate `None` to get the pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: Set up dictionaries for value labels and column headings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load a dictionary that maps columns to the value labels and create a list of
    preferred column names as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Set value labels and missing values, and change selected columns to the `category`
    data type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `setvalues` dictionary to replace existing values with value labels.
    Replace all values from `–9` to `–1` with `NaN`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'Set meaningful column headings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This shows how R data files can be imported into pandas and value labels assigned.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reading R data into pandas with `pyreadr` is fairly straightforward. Passing
    a filename to the `read_r` function is all that is required. Since `read_r` can
    return multiple objects with one call, we need to specify which object. When reading
    an `rds` file (as opposed to an `rdata` file), only one object is returned. It
    has the key `None`.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, we loaded a dictionary that maps our variables to value labels,
    and a list for our preferred column headings. In *Step 4* we applied the value
    labels. We also changed the data type to `category` for the columns where we applied
    the values. We did this by generating a list of the keys in our `setvalues` dictionary
    with `[k for k in setvalues]` and then iterating over those columns.
  prefs: []
  type: TYPE_NORMAL
- en: We change the column headings in *Step 5* to ones that are more intuitive. Note
    that the order matters here. We need to set the value labels before changing the
    column names, since the `setvalues` dictionary is based on the original column
    headings.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of using `pyreadr` to read R files directly into pandas is
    that we do not have to convert the R data into a CSV file first. Once we have
    written our Python code to read the file, we can just rerun it whenever the R
    data changes. This is particularly helpful when we do not have R on the machine
    where we work.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Pyreadr` is able to return multiple DataFrames. This is useful when we save
    several data objects in R as an `rdata` file. We can return all of them with one
    call.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Pprint` is a handy tool for improving the display of Python dictionaries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We could have used `rpy2` instead of `pyreadr` to import R data. `rpy2` requires
    that R also be installed, but it is more powerful than `pyreadr`. It will read
    R factors and automatically set them to pandas DataFrame values. See the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: This generates unusual *–2147483648* values. This is what happened when `readRDS`
    interpreted missing data in numeric columns. A global replacement of that number
    with `NaN`, after confirming that that is not a valid value, would be a good next
    step.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clear instructions and examples for `pyreadr` are available at [https://github.com/ofajardo/pyreadr](https://github.com/ofajardo/pyreadr).
  prefs: []
  type: TYPE_NORMAL
- en: Feather files, a relatively new format, can be read by both R and Python. I
    discuss those files in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We persist data, copy it from memory to local or remote storage, for several
    reasons: to be able to access the data without having to repeat the steps we used
    to generate it, to share the data with others, or to make it available for use
    with different software. In this recipe, we save data that we have loaded into
    a pandas DataFrame as different file types (CSV, Excel, Pickle, and Feather).'
  prefs: []
  type: TYPE_NORMAL
- en: Another important, but sometimes overlooked, reason to persist data is to preserve
    some segment of our data that needs to be examined more closely; perhaps it needs
    to be scrutinized by others before our analysis can be completed. For analysts
    who work with operational data in medium- to large-sized organizations, this process
    is part of the daily data-cleaning workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to these reasons for persisting data, our decisions about when
    and how to serialize data are shaped by several other factors: where we are in
    terms of our data analysis projects, the hardware and software resources of the
    machine(s) saving and reloading the data, and the size of our dataset. Analysts
    end up having to be much more intentional when saving data than they are when
    pressing *Ctrl* + *S* in their word-processing application.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we persist data, it is stored separately from the logic that we used to
    create it. I find this to be one of the most important threats to the integrity
    of our analysis. Often, we end up loading data that we saved some time in the
    past (a week ago? A month ago? A year ago?) and forget how a variable was defined
    and how it relates to other variables. If we are in the middle of a data-cleaning
    task, it is best not to persist our data, so long as our workstation and network
    can easily handle the burden of regenerating the data. It is a good idea to persist
    data only once we have reached milestones in our work.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the question of *when* to persist data, there is the question of *how*.
    If we are persisting it for our own reuse with the same software, it is best to
    save it in a binary format native to that software. That is pretty straightforward
    for tools such as SPSS, SAS, Stata, and R, but not so much for pandas. But that
    is good news in a way. We have lots of choices, from CSV and Excel to Pickle and
    Feather. We save as all these file types in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: Pickle and Feather are binary file formats that can be used to store pandas
    DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need to install Feather if you do not have it on your system. You can
    do that by entering `pip install pyarrow` in a Terminal window or `powershell`
    (in Windows). If you do not already have a subfolder named `Views` in your `chapter
    1` folder, you will need to create it in order to run the code for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset, taken from the Global Historical Climatology Network integrated
    database, is made available for public use by the United States National Oceanic
    and Atmospheric Administration at [https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly).
    I used the data from version 4\. The data in this recipe uses a 100,000-row sample
    of the full dataset, which is also available in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will load a CSV file into pandas and then save it as a Pickle and a Feather
    file. We will also save subsets of the data to the CSV and Excel formats:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `pandas` and `pyarrow`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pyarrow` needs to be imported in order to save pandas to Feather:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the land temperatures CSV file into pandas, drop rows with missing data,
    and set an index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Write extreme values for `temperature` to CSV and Excel files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `quantile` method to select outlier rows, which are those at the 1
    in 1,000 level at each end of the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: Save to Pickle and Feather files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The index needs to be reset in order to save a Feather file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: Load the Pickle and Feather files we just saved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that our index was preserved when saving and loading the Pickle file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: The previous steps demonstrated how to serialize pandas DataFrames using two
    different formats, Pickle and Feather.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Persisting pandas data is quite straightforward. DataFrames have the `to_csv`,
    `to_excel`, `to_pickle`, and `to_feather` methods. Pickling preserves our index.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advantage of storing data in CSV files is that saving it uses up very little
    additional memory. The disadvantage is that writing CSV files is quite slow, and
    we lose important metadata, such as data types. (`read_csv` can often figure out
    the data type when we reload the file, but not always.) Pickle files keep that
    data but can burden a system that is low on resources when serializing. Feather
    is easier on resources and can be easily loaded in R as well as Python, but we
    have to sacrifice our index in order to serialize. Also, the authors of Feather
    make no promises regarding long-term support.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that I do not make a global recommendation about what to
    use for data serialization – other than to limit your persistence of full datasets
    to project milestones. This is definitely one of those “right tools for the right
    job” kind of situations. I use CSV or Excel files when I want to share a segment
    of a file with colleagues for discussion. I use Feather for ongoing Python projects,
    particularly when I am using a machine with sub-par RAM and an outdated chip and
    also using R. When I am wrapping up a project, I pickle the DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our Python data projects typically start with raw data stored in a range of
    formats and exported from a variety of software tools. Among the most popular
    tabular formats and tools are CSV and Excel files, SQL tables, and SPSS, Stata,
    SAS, and R datasets. We converted data from all of these sources into a pandas
    DataFrame in this chapter, and addressed the most common challenges. We also explored
    approaches to persisting tabular data. We will work with data in other formats
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://discord.gg/p8uSgEAETX](https://discord.gg/p8uSgEAETX )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code10336218961138498953.png)'
  prefs: []
  type: TYPE_IMG
