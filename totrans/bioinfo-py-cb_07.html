<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer052">
<h1 class="chapter-number" id="_idParaDest-155"><a id="_idTextAnchor154"/>6</h1>
<h1 id="_idParaDest-156"><a id="_idTextAnchor155"/>Population Genetics</h1>
<p>Population genetics is the study of the changes in the frequency of alleles in a population on the basis of selection, drift, mutation, and migration. The previous chapters focused mainly on data processing and cleanup; this is the first chapter in which we will actually infer interesting biological results.</p>
<p>There is a lot of interesting population genetics analysis based on sequence data, but as we already have quite a few recipes for dealing with sequence data, we will divert our attention elsewhere. Also, we <a id="_idIndexMarker411"/>will not cover genomic structural variations such as <strong class="bold">Copy Number Variations</strong> (<strong class="bold">CNVs</strong>) or inversions here. We will concentrate on analyzing SNP data, which is one of the most common data types. We will perform many standard population genetic analyses with Python, such <a id="_idIndexMarker412"/>as using the <strong class="bold">Fixation Index</strong> (<strong class="bold">FST</strong>) with computing <a id="_idIndexMarker413"/>F-statistics, <strong class="bold">Principal Components Analysis</strong> (<strong class="bold">PCA</strong>), and studying population structure.</p>
<p>We will use Python mostly as a scripting language that glues together applications that perform necessary computations, which is the old-fashioned way of doing things. Having said that, as the Python software ecology is still evolving, you can at least perform the PCA in Python using scikit-learn as we will see in <a href="B17942_11.xhtml#_idTextAnchor272"><em class="italic">Chapter 11</em></a>.</p>
<p>There is no such thing as a default file format for population genetics data. The bleak reality of this field is that there is a plenitude of formats, most of them developed with a specific application in mind; therefore, none are generically applicable. Some of the efforts to create a more general format (or even just a file converter to support many formats) had limited success. Furthermore, as our knowledge of genomics increases, we will require new formats anyway (for example, to support some kind of previously unknown <a id="_idIndexMarker414"/>genomic structural variation). Here, we will work with PLINK (<a href="https://www.cog-genomics.org/plink/2.0/">https://www.cog-genomics.org/plink/2.0/</a>), which was originally developed <a id="_idIndexMarker415"/>to perform <strong class="bold">Genome-Wide Association Studies</strong> (<strong class="bold">GWAS</strong>) with human data but has many more applications. If <a id="_idIndexMarker416"/>you have <strong class="bold">Next-Generation Sequencing</strong> (<strong class="bold">NGS</strong>) data, you may question, why not use the <strong class="bold">Variant Call Format</strong> (<strong class="bold">VCF</strong>)? Well, a VCF <a id="_idIndexMarker417"/>file is normally annotated to help with sequencing analysis, which you do not need at this stage (you should now have a filtered dataset). If you convert your <strong class="bold">Single-Nucleotide Polymorphism</strong> (<strong class="bold">SNP</strong>) calls <a id="_idIndexMarker418"/>from VCF to PLINK, you will get roughly a 95 percent reduction in terms of size (this is in comparison to a compressed VCF). More importantly, the computational cost of processing a VCF file is much bigger (think of processing all this highly structured text) than the cost of the other two formats. If you use Docker, use the image tiagoantao/bioinformatics_popgen.</p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Managing datasets with PLINK</li>
<li>Using sgkit for population genetics analysis with xarray</li>
<li>Exploring a dataset with sgkit</li>
<li>Analyzing population structure</li>
<li>Performing a PCA</li>
<li>Investigating population structure with admixture</li>
</ul>
<p>First, let’s start with a discussion on file format issues and then continue to discuss interesting data analysis.</p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor156"/>Managing datasets with PLINK</h1>
<p>Here, we will <a id="_idIndexMarker419"/>manage our dataset using PLINK. We will create subsets of our main dataset (from the HapMap project) that are suitable for analysis <a id="_idIndexMarker420"/>in the following recipes.</p>
<p class="callout-heading">Warning</p>
<p class="callout">Note that neither PLINK nor any similar programs were developed for their file formats. There was probably no objective to create a default file standard for population genetics data. In this field, you will need to be ready to convert from format to format (for this, Python is quite appropriate) because every application that you will use will probably have its own quirky requirements. The most important point to learn from this recipe is that it’s not formats that are being used, although these are relevant, but a ‘file conversion mentality’. Beyond this, some of the steps in this recipe also convey genuine analytical techniques <a id="_idIndexMarker421"/>that you may want to consider using, for example, subsampling or <strong class="bold">Linkage Disequilibrium-</strong> (<strong class="bold">LD-</strong>) pruning.</p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor157"/>Getting ready</h2>
<p>Throughout this chapter, we will use data from the International HapMap Project. You may recall that we used data from the 1,000 Genomes Project in <a href="B17942_03.xhtml#_idTextAnchor068"><em class="italic">Chapter 3</em></a>, <em class="italic">Next-Generation Sequencing</em>, and that the HapMap project is in many ways the precursor to the 1,000 Genomes Project; instead of whole genome sequencing, genotyping was used. Most of the <a id="_idIndexMarker422"/>samples of the HapMap project were used <a id="_idIndexMarker423"/>in the 1,000 Genomes Project, so if you have read the recipes in <a href="B17942_03.xhtml#_idTextAnchor068"><em class="italic">Chapter 3</em></a>, <em class="italic">Next-Generation Sequencing</em>, you will already have an idea of the dataset (including the available population). I will not introduce the dataset much more, but you can refer to <a href="B17942_03.xhtml#_idTextAnchor068"><em class="italic">Chapter 3</em></a>, <em class="italic">Next-Generation Sequencing</em>, and, of <a id="_idIndexMarker424"/>course, the HapMap site (<a href="https://www.genome.gov/10001688/international-hapmap-project">https://www.genome.gov/10001688/international-hapmap-project</a>) for more information. Remember that we have genotyping data for many individuals split across populations around the globe. We will refer to these populations by their acronyms. Here is the list taken from <a href="http://www.sanger.ac.uk/resources/downloads/human/hapmap3.xhtml">http://www.sanger.ac.uk/resources/downloads/human/hapmap3.xhtml</a>:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Acronym</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Population</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>ASW</p>
</td>
<td class="No-Table-Style">
<p>African ancestry in Southwest USA</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>CEU</p>
</td>
<td class="No-Table-Style">
<p>Utah residents with Northern and Western European ancestry from the CEPH collection</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>CHB</p>
</td>
<td class="No-Table-Style">
<p>Han Chinese in Beijing, China</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>CHD</p>
</td>
<td class="No-Table-Style">
<p>Chinese in Metropolitan Denver, Colorado</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>GIH</p>
</td>
<td class="No-Table-Style">
<p>Gujarati Indians in Houston, Texas</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>JPT</p>
</td>
<td class="No-Table-Style">
<p>Japanese in Tokyo, Japan</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>LWK</p>
</td>
<td class="No-Table-Style">
<p>Luhya in Webuye, Kenya</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>MXL</p>
</td>
<td class="No-Table-Style">
<p>Mexican ancestry in Los Angeles, California</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>MKK</p>
</td>
<td class="No-Table-Style">
<p>Maasai in Kinyawa, Kenya</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>TSI</p>
</td>
<td class="No-Table-Style">
<p>Toscani in Italy</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>YRI</p>
</td>
<td class="No-Table-Style">
<p>Yoruba in Ibadan, Nigeria</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.1 - The populations in the Genome Project</p>
<p class="callout-heading">Note</p>
<p class="callout">We will be using data from the HapMap project that has, in practice, been replaced by the 1,000 Genomes Project. For the purpose of teaching population genetics programming techniques in Python, the HapMap Project dataset is more manageable than the 1,000 Genomes Project, as the data is considerably smaller. The HapMap samples are a subset of the 1,000 Genomes samples. If you do research in human population genetics, you are strongly advised to use the 1,000 Genomes Project as a base dataset.</p>
<p>This will <a id="_idIndexMarker425"/>require a fairly big download (approximately 1 GB), which will have to be uncompressed. Make sure that you have <a id="_idIndexMarker426"/>approximately 20 GB of disk space for this chapter. The files can be found at <a href="https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/">https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3/plink_format/</a>.</p>
<p>Decompress the PLINK file using the following commands:</p>
<p class="source-code">bunzip2 hapmap3_r3_b36_fwd.consensus.qc.poly.map.gz</p>
<p class="source-code">bunzip2 hapmap3_r3_b36_fwd.consensus.qc.poly.ped.gz</p>
<p>Now, we have PLINK files; the MAP file has information on the marker position across the genome, whereas the PED file has actual markers for each individual, along with some pedigree information. We also downloaded a metadata file that contains information about each individual. Take a look at all these files and familiarize yourself with them. As usual, this is also available in the <strong class="source-inline">Chapter06/Data_Formats.py</strong> Notebook file, where everything has been taken care of.</p>
<p>Finally, most of this recipe will make heavy usage of PLINK (<a href="https://www.cog-genomics.org/plink/2.0/">https://www.cog-genomics.org/plink/2.0/</a>). Python will mostly be used as the glue language to call PLINK.</p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor158"/>How to do it...</h2>
<p>Take a look at the following steps:</p>
<ol>
<li value="1">Let’s get <a id="_idIndexMarker427"/>the metadata for our samples. We <a id="_idIndexMarker428"/>will load the population of each sample and note all the individuals that are offspring of others in the dataset:<p class="source-code">from collections import defaultdict</p><p class="source-code">f = open('relationships_w_pops_041510.txt')</p><p class="source-code">pop_ind = defaultdict(list)</p><p class="source-code">f.readline() # header</p><p class="source-code">offspring = []</p><p class="source-code">for l in f:</p><p class="source-code">    toks = l.rstrip().split('\t')</p><p class="source-code">    fam_id = toks[0]</p><p class="source-code">    ind_id = toks[1]</p><p class="source-code">    mom = toks[2]</p><p class="source-code">    dad = toks[3]</p><p class="source-code">    if mom != '0' or dad != '0':</p><p class="source-code">        offspring.append((fam_id, ind_id))</p><p class="source-code">    pop = toks[-1]</p><p class="source-code">pop_ind[pop].append((fam_id, ind_id))</p><p class="source-code">f.close()</p></li>
</ol>
<p>This will load a dictionary where the population is the key (<strong class="source-inline">CEU</strong>, <strong class="source-inline">YRI</strong>, and so on) and its value is the list of individuals in that population. This dictionary will also store information on whether the individual is the offspring of another. Each individual is identified by the family and individual ID (information that can be found in the PLINK file). The file provided by the HapMap project is a simple tab-delimited file, which is not difficult to process. While we are reading the files using standard Python text processing, this is a typical example where pandas would help.</p>
<p>There is an <a id="_idIndexMarker429"/>important point to make here: the reason <a id="_idIndexMarker430"/>this information is provided in a separate, ad hoc file is that the PLINK format makes no provision for the population structure (this format makes provision only for the case and control information for which PLINK was designed). This is not a flaw of the format, as it was never designed to support standard population genetic studies (it’s a GWAS tool). However, this is a general feature of data formats in population genetics: whichever you end up working with, there will be something important missing.</p>
<p>We will use this metadata in other recipes in this chapter. We will also perform some consistency analysis between the metadata and the PLINK file, but we will defer this to the next recipe.</p>
<ol>
<li value="2">Now, let’s subsample the dataset at 10 percent and 1 percent of the number of markers, as follows:<p class="source-code">import os</p><p class="source-code">os.system('plink2 --pedmap hapmap3_r3_b36_fwd.consensus.qc.poly --out hapmap10 --thin 0.1 --geno 0.1 --export ped')</p><p class="source-code">os.system('plink2 --pedmap hapmap3_r3_b36_fwd.consensus.qc.poly --out hapmap1 --thin 0.01 --geno 0.1 --export ped')</p></li>
</ol>
<p>With Jupyter Notebook, you can just do this instead:</p>
<p class="source-code">!plink2 --pedmap hapmap3_r3_b36_fwd.consensus.qc.poly --out hapmap10 --thin 0.1 --geno 0.1 --export ped</p>
<p class="source-code">!plink2 --pedmap hapmap3_r3_b36_fwd.consensus.qc.poly --out hapmap1 --thin 0.01 --geno 0.1 --export ped</p>
<p>Note the subtlety that you will not really get 1 or 10 percent of the data; each marker will have a 1 or 10 percent chance of being selected, so you will get approximately 1 or 10 percent of the markers.</p>
<p>Obviously, as the <a id="_idIndexMarker431"/>process is random, different <a id="_idIndexMarker432"/>runs will produce different marker subsets. This will have important implications further down the road. If you want to replicate the exact same result, you can nonetheless use the <strong class="source-inline">--seed</strong> option.</p>
<p>We will also remove all SNPs that have a genotyping rate lower than 90 percent (with the <strong class="source-inline">--geno 0.1</strong> parameter).</p>
<p class="callout-heading">Note</p>
<p class="callout">There is nothing special about Python in this code, but there are two reasons you may want to subsample your data. First, if you are performing an exploratory analysis of your own dataset, you may want to start with a smaller version because it will be easy to process. Also, you will have a broader view of your data. Second, some analytical methods may not require all your data (indeed, some methods might not be even able to use all of your data). Be very careful with the last point though; that is, for every method that you use to analyze your data, be sure that you understand the data requirements for the scientific questions you want to answer. Feeding too much data may be okay normally (even if you pay a time and memory penalty) but feeding too little will lead to unreliable results.</p>
<ol>
<li value="3">Now, let’s generate subsets with just the autosomes (that is, let’s remove the sex chromosomes and mitochondria), as follows:<p class="source-code">def get_non_auto_SNPs(map_file, exclude_file):</p><p class="source-code">    f = open(map_file)</p><p class="source-code">    w = open(exclude_file, 'w')</p><p class="source-code">    for l in f:</p><p class="source-code">        toks = l.rstrip().split('\t')</p><p class="source-code">        try:</p><p class="source-code">            chrom = int(toks[0])</p><p class="source-code">        except ValueError:</p><p class="source-code">            rs = toks[1]</p><p class="source-code">            w.write('%s\n' % rs)</p><p class="source-code">    w.close()</p><p class="source-code">get_non_auto_SNPs('hapmap1.map', 'exclude1.txt')</p><p class="source-code">get_non_auto_SNPs('hapmap10.map', 'exclude10.txt')</p><p class="source-code">os.system('plink2 –-pedmap hapmap1 --out hapmap1_auto --exclude exclude1.txt --export ped')</p><p class="source-code">os.system('plink2 --pedmap hapmap10 --out hapmap10_auto --exclude exclude10.txt --export ped')</p></li>
<li>Let’s create a function that generates a list with all the SNPs not belonging to autosomes. With human data, that means all non-numeric chromosomes. If you use another <a id="_idIndexMarker433"/>species, be careful with your chromosome <a id="_idIndexMarker434"/>coding because PLINK is geared toward human data. If your species are diploid, have less than 23 autosomes, and a sex determination system, that is, X/Y, this will be straightforward; if not, refer to <a href="https://www.cog-genomics.org/plink2/input#allow_extra_chr">https://www.cog-genomics.org/plink2/input#allow_extra_chr</a> for some alternatives (such as the <strong class="source-inline">--allow-extra-chr</strong> flag).</li>
<li>We then create autosome-only PLINK files for subsample datasets of 10 and 1 percent (prefixed as <strong class="source-inline">hapmap10_auto</strong> and <strong class="source-inline">hapmap1_auto</strong>).</li>
<li>Let’s create some datasets without offspring. These will be needed for most population genetic analysis, which requires unrelated individuals to a certain degree:<p class="source-code">os.system('plink2 --pedmap hapmap10_auto --filter-founders --out hapmap10_auto_noofs --export ped')</p></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This step is representative of the fact that most population genetic analyses require samples to be unrelated to a certain degree. Obviously, as we know that some offspring are in HapMap, we remove them.</p>
<p class="callout">However, note that with your dataset, you are expected to be much more refined than this. For instance, run <strong class="source-inline">plink --genome</strong> or use another program to detect related individuals. The fundamental point here is that you have to dedicate some effort to detect related individuals in your samples; this is not a trivial task.</p>
<ol>
<li value="7">We will <a id="_idIndexMarker435"/>also generate an LD-pruned dataset, as required <a id="_idIndexMarker436"/>by many PCA and admixture algorithms, as follows:<p class="source-code">os.system('plink2 --pedmap hapmap10_auto_noofs --indep-pairwise 50 10 0.1 --out keep --export ped')</p><p class="source-code">os.system('plink2 --pedmap hapmap10_auto_noofs --extract keep.prune.in --recode --out hapmap10_auto_noofs_ld --export ped')</p></li>
</ol>
<p>The first step generates a list of markers to be kept if the dataset is LD-pruned. This uses a sliding window of <strong class="source-inline">50</strong> SNPs, advancing by <strong class="source-inline">10</strong> SNPs at a time with a cut value of <strong class="source-inline">0.1</strong>. The second step extracts SNPs from the list that was generated earlier.</p>
<ol>
<li value="8">Let’s recode a couple of cases in different formats:<p class="source-code">os.system('plink2 --file hapmap10_auto_noofs_ld --recode12 tab --out hapmap10_auto_noofs_ld_12 --export ped 12')</p><p class="source-code">os.system('plink2 --make-bed --file hapmap10_auto_noofs_ld --out hapmap10_auto_noofs_ld')</p></li>
</ol>
<p>The first <a id="_idIndexMarker437"/>operation will convert a PLINK format that <a id="_idIndexMarker438"/>uses nucleotide letters from the ACTG to another, which recodes alleles with <strong class="source-inline">1</strong> and <strong class="source-inline">2</strong>. We will use this in the <em class="italic">Performing a PCA</em> recipe later.</p>
<p>The second operation recodes a file in a binary format. If you work inside PLINK (using the many useful operations that PLINK has), the binary format is probably the most appropriate format (offering, for example, a smaller file size). We will use this in the admixture recipe.</p>
<ol>
<li value="9">We will also extract a single chromosome (<strong class="source-inline">2</strong>) for analysis. We will start with the autosome dataset, which has been subsampled at 10 percent:<p class="source-code">os.system('plink2 --pedmap hapmap10_auto_noofs --chr 2 --out hapmap10_auto_noofs_2 --export ped')</p></li>
</ol>
<h2 id="_idParaDest-160"><a id="_idTextAnchor159"/>There’s more...</h2>
<p>There are many reasons why you might want to create different datasets for analysis. You may want to perform some fast initial exploration of data – for example, if the analysis algorithm that you plan to use has some data format requirements or a constraint on the input, such as the number of markers or relationships between individuals. Chances are that you will have lots of subsets to analyze (unless your dataset is very small to start with, for instance, a microsatellite dataset).</p>
<p>This may seem to be a minor point, but it’s not: be very careful with file naming (note that I have followed some simple conventions while generating filenames). Make sure that the name of the file gives some information about the subset options. When you perform the downstream analysis, you will want to be sure that you choose the correct dataset; you will want your dataset management to be agile and reliable, above all. The worst thing that can happen is that you create an analysis with an erroneous dataset that does not obey the constraints required by the software.</p>
<p>The LD-pruning that we used is somewhat standard for human analysis, but be sure to check the parameters, especially if you are using non-human data.</p>
<p>The HapMap file <a id="_idIndexMarker439"/>that we downloaded is based on an <a id="_idIndexMarker440"/>old version of the reference genome (build 36). As stated in the previous chapter, <a href="B17942_05.xhtml#_idTextAnchor122"><em class="italic">Chapter 5</em></a>, <em class="italic">Working with Genomes</em>, be sure to use annotations from build 36 if you plan to use this file for more analysis of your own.</p>
<p>This recipe sets the stage for the following recipes and its results will be used extensively.</p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor160"/>See also</h2>
<ul>
<li>The Wikipedia page <a href="http://en.wikipedia.org/wiki/Linkage_disequilibrium">http://en.wikipedia.org/wiki/Linkage_disequilibrium</a> on LD is a good place to start.</li>
<li>The website of PLINK <a href="https://www.cog-genomics.org/plink/2.0/">https://www.cog-genomics.org/plink/2.0/</a> is very well documented, something lacking in much of genetics software.</li>
</ul>
<h1 id="_idParaDest-162"><a id="_idTextAnchor161"/>Using sgkit for population genetics analysis with xarray</h1>
<p>Sgkit is the most <a id="_idIndexMarker441"/>advanced Python library for doing population genetics <a id="_idIndexMarker442"/>analysis. It’s a modern <a id="_idIndexMarker443"/>implementation, leveraging almost all of the fundamental data science libraries in Python. When I say almost all, I am not exaggerating; it uses NumPy, pandas, xarray, Zarr, and Dask. NumPy and pandas were introduced in <a href="B17942_02.xhtml#_idTextAnchor040"><em class="italic">Chapter 2</em></a>. Here, we will introduce xarray as the main data container for sgkit. Because I feel that I cannot ask you to get to know data engineering libraries to an extreme level, I will gloss over the Dask part (mostly by treating Dask structures as equivalent NumPy structures). You can find more advanced details about out-of-memory Dask data structures in <a href="B17942_11.xhtml#_idTextAnchor272"><em class="italic">Chapter 11</em></a>.</p>
<h2 id="_idParaDest-163"><a id="_idTextAnchor162"/>Getting ready</h2>
<p>You will <a id="_idIndexMarker444"/>need to run the previous <a id="_idIndexMarker445"/>recipe because its output is required for this one: we will be using one of the PLINK datasets. You will need to install sgkit.</p>
<p>As usual, this is available in the <strong class="source-inline">Chapter06/Sgkit.py</strong> Notebook file, but it will still require you to run the previous Notebook file in order to generate the required files.</p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor163"/>How to do it...</h2>
<p>Take a look at the following steps:</p>
<ol>
<li value="1">Let’s load the <strong class="source-inline">hapmap10_auto_noofs_ld</strong> dataset generated in the previous recipe:<p class="source-code">import numpy as np</p><p class="source-code">from sgkit.io import plink</p><p class="source-code">data = plink.read_plink(path='hapmap10_auto_noofs_ld', fam_sep='\t')</p></li>
</ol>
<p>Remember that we are loading a set of PLINK files. It turns out that sgkit creates a very rich and structured representation for that data. That representation is based on an xarray dataset.</p>
<ol>
<li value="2">Let’s check the structure of our data – if you are in a notebook, just enter the following:<p class="source-code">data</p></li>
</ol>
<p><strong class="source-inline">sgkit</strong> – if in a notebook – will generate the following representation:</p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<img alt="Figure 6.1 - An overview of the xarray data loaded by sgkit for our PLINK file " height="799" src="image/B17942_06_1.jpg" width="766"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 - An overview of the xarray data loaded by sgkit for our PLINK file</p>
<p><strong class="source-inline">data</strong> is an xarray DataSet. An xarray DataSet is essentially a dictionary in which each <a id="_idIndexMarker446"/>value is a Dask array. For our purposes, you can assume it is a NumPy array. In this case, we can <a id="_idIndexMarker447"/>see that we have <strong class="bold">56241</strong> variants for <strong class="bold">1198</strong> samples. We have <strong class="bold">2</strong> alleles per variant and a ploidy of <strong class="bold">2</strong>.</p>
<p>In the notebook, we can expand each entry. In our case, we expanded <strong class="source-inline">call_genotype</strong>. This is a three-dimensional array, with <strong class="source-inline">variants</strong>, <strong class="source-inline">samples</strong>, and <strong class="source-inline">ploidy</strong> dimensions. The type of the array is <strong class="source-inline">int8</strong>. After this, we can find some metadata relevant to the entry, <strong class="source-inline">mixed_ploidy</strong>, and comment. Finally, you have a summary of the Dask implementation. The <strong class="bold">Array</strong> column presents details about the size and shape of the array. For the <strong class="bold">Chunk</strong> column, see <a href="B17942_11.xhtml#_idTextAnchor272"><em class="italic">Chapter 11</em></a> – but you can safely ignore it for now.</p>
<ol>
<li value="3">Another <a id="_idIndexMarker448"/>way to get <a id="_idIndexMarker449"/>summary information, which is especially useful if you are not using notebooks, is by inspecting the <strong class="source-inline">dims</strong> field:<p class="source-code">print(data.dims)</p></li>
</ol>
<p>The output should be self-explanatory:</p>
<p class="source-code"><strong class="bold">Frozen({'variants': 56241, 'alleles': 2, 'samples': 1198, 'ploidy': 2})</strong></p>
<ol>
<li value="4">Let’s extract some information about the samples:<p class="source-code">print(len(data.sample_id.values))</p><p class="source-code">print(data.sample_id.values)</p><p class="source-code">print(data.sample_family_id.values)</p><p class="source-code">print(data.sample_sex.values)</p></li>
</ol>
<p>The output is as follows:</p>
<p class="source-code"><strong class="bold">1198</strong></p>
<p class="source-code"><strong class="bold">['NA19916' 'NA19835' 'NA20282' ... 'NA18915' 'NA19250' 'NA19124']</strong></p>
<p class="source-code"><strong class="bold">['2431' '2424' '2469' ... 'Y029' 'Y113' 'Y076']</strong></p>
<p class="source-code"><strong class="bold">[1 2 2 ... 1 2 1]</strong></p>
<p>We have <strong class="source-inline">1198</strong> samples. The first one has a sample ID of <strong class="source-inline">NA19916</strong>, a family ID of <strong class="source-inline">2431</strong>, and a sex of <strong class="source-inline">1</strong> (Male). Remember that, given PLINK as the data source, a sample ID is not enough to be a primary key (you can have different samples with the same sample ID). The primary key is a composite of the sample ID and sample family ID.</p>
<p class="callout-heading">TIP</p>
<p class="callout">You might have noticed that we add <strong class="source-inline">.values</strong> to all the data fields: this is actually rendering a lazy Dask array into a materialized NumPy one. For now, I suggest that you ignore it, but if you revisit this chapter after reading <a href="B17942_11.xhtml#_idTextAnchor272"><em class="italic">Chapter 11</em></a>, <strong class="source-inline">.values</strong> is akin to the <strong class="source-inline">compute</strong> method in Dask.</p>
<p class="callout">The <strong class="source-inline">.values</strong> call is no nuisance – the reason our code works is that our dataset is small enough to fit into memory, which is great for our teaching example. But if you have a very large dataset, the preceding code is too naive. Again, <a href="B17942_11.xhtml#_idTextAnchor272"><em class="italic">Chapter 11</em></a> will help you with this. For now, the simplicity is pedagogical.</p>
<ol>
<li value="5">Before <a id="_idIndexMarker450"/>we look at the <a id="_idIndexMarker451"/>variant data, we have to be aware of how sgkit stores <strong class="source-inline">contigs</strong>:<p class="source-code">print(data.contigs)</p></li>
</ol>
<p>The output is as follows:</p>
<p class="source-code"><strong class="bold">['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']</strong></p>
<p>The <strong class="source-inline">contigs</strong> here are the human autosomes (you will not be so lucky if your data is based on most other species – you will probably have some ugly identifier here).</p>
<ol>
<li value="6">Now, let’s look at the variants:<p class="source-code">print(len(data.variant_contig.values))</p><p class="source-code">print(data.variant_contig.values)</p><p class="source-code">print(data.variant_position.values)</p><p class="source-code">print(data.variant_allele.values)</p><p class="source-code">print(data.variant_id.values)</p></li>
</ol>
<p>Here is an abridged version of the output:</p>
<p class="source-code"><strong class="bold">56241</strong></p>
<p class="source-code"><strong class="bold">[ 0  0  0 ... 21 21 21]</strong></p>
<p class="source-code"><strong class="bold">[  557616   782343   908247 ... 49528105 49531259 49559741]</strong></p>
<p class="source-code"><strong class="bold">[[b'G' b'A']</strong></p>
<p class="source-code"><strong class="bold"> ...</strong></p>
<p class="source-code"><strong class="bold"> [b'C' b'A']]</strong></p>
<p class="source-code"><strong class="bold">['rs11510103' 'rs2905036' 'rs13303118' ... 'rs11705587' 'rs7284680'</strong></p>
<p class="source-code"><strong class="bold"> 'rs2238837']</strong></p>
<p>We have <strong class="source-inline">56241</strong> variants. The <strong class="source-inline">contig</strong> index is <strong class="source-inline">0</strong>, which if you look at the step from the <a id="_idIndexMarker452"/>previous recipe, is chromosome <strong class="source-inline">1</strong>. The variant is in position <strong class="source-inline">557616</strong> (against build 36 <a id="_idIndexMarker453"/>of the human genome) and has possible alleles <strong class="source-inline">G</strong> and <strong class="source-inline">A</strong>. It has an SNP ID of <strong class="source-inline">rs11510103</strong>.</p>
<ol>
<li value="7">Finally, let’s look at the <strong class="source-inline">genotype</strong> data:<p class="source-code">call_genotype = data.call_genotype.values</p><p class="source-code">print(call_genotype.shape)</p><p class="source-code">first_individual = call_genotype[:,0,:]</p><p class="source-code">first_variant = call_genotype[0,:,:]</p><p class="source-code">first_variant_of_first_individual = call_genotype[0,0,:]</p><p class="source-code">print(first_variant_of_first_individual)</p><p class="source-code">print(data.sample_family_id.values[0], data.sample_id.values[0])</p><p class="source-code">print(data.variant_allele.values[0])</p></li>
</ol>
<p><strong class="source-inline">call_genotype</strong> has a shape of 56,241 x 1,1198,2, which is its dimensioned variants, samples, and ploidy.</p>
<p>To get all <a id="_idIndexMarker454"/>variants for the first <a id="_idIndexMarker455"/>individual, you fixate the second dimension. To get all the samples for the first variant, you fixate the first dimension.</p>
<p>If you print the first individual’s details (sample and family ID), you get <strong class="source-inline">2431</strong> and <strong class="source-inline">NA19916</strong> – as expected, exactly as in the first case in the previous sample exploration.</p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor164"/>There’s more...</h2>
<p>This recipe is mostly an introduction to xarray, disguised as a sgkit tutorial. There is much more to be said about xarray – be sure to check <a href="https://docs.xarray.dev/">https://docs.xarray.dev/</a>. It is worth reiterating that xarray depends on a plethora of Python data science libraries and that we are glossing over Dask for now.</p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor165"/>Exploring a dataset with sgkit</h1>
<p>In this recipe, we will perform an initial exploratory analysis of one of our generated datasets. Now that <a id="_idIndexMarker456"/>we have some basic knowledge of xarray, we can <a id="_idIndexMarker457"/>actually try to do some data analysis. In this recipe, we will ignore population structure, an issue we will return to in the following one.</p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor166"/>Getting ready</h2>
<p>You will need to have run the first recipe and should have the <strong class="source-inline">hapmap10_auto_noofs_ld</strong> files available. There is a Notebook file with this recipe called <strong class="source-inline">Chapter06/Exploratory_Analysis.py</strong>. You will need the software that you installed for the previous recipe.</p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor167"/>How to do it...</h2>
<p>Take a look at the following steps:</p>
<ol>
<li value="1">We start <a id="_idIndexMarker458"/>by loading the PLINK data with sgkit, exactly <a id="_idIndexMarker459"/>as in the previous recipe:<p class="source-code">import numpy as np</p><p class="source-code">import xarray as xr</p><p class="source-code">import sgkit as sg</p><p class="source-code">from sgkit.io import plink</p><p class="source-code"> </p><p class="source-code">data = plink.read_plink(path='hapmap10_auto_noofs_ld', fam_sep='\t')</p></li>
<li>Let’s ask sgkit for <strong class="source-inline">variant_stats</strong>:<p class="source-code">variant_stats = sg.variant_stats(data)</p><p class="source-code">variant_stats</p></li>
</ol>
<p>The output is the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer044">
<img alt="Figure 6.2 - The variant statistics provided by sgkit’s variant_stats " height="658" src="image/B17942_06_2.jpg" width="557"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 - The variant statistics provided by sgkit’s variant_stats</p>
<ol>
<li value="3">Let’s now <a id="_idIndexMarker460"/>look at the statistic, <strong class="source-inline">variant_call_rate</strong>:<p class="source-code">variant_stats.variant_call_rate.to_series().describe()</p></li>
</ol>
<p>There is <a id="_idIndexMarker461"/>more to unpack here than it may seem. The fundamental part is the <strong class="source-inline">to_series()</strong> call. Sgkit is returning a Pandas series to you – remember that sgkit is highly integrated with Python data science libraries. After you get the Series object, you can call the Pandas <strong class="source-inline">describe</strong> function and get the following:</p>
<p class="source-code"><strong class="bold">count    56241.000000</strong></p>
<p class="source-code"><strong class="bold">mean         0.997198</strong></p>
<p class="source-code"><strong class="bold">std          0.003922</strong></p>
<p class="source-code"><strong class="bold">min          0.964107</strong></p>
<p class="source-code"><strong class="bold">25%          0.996661</strong></p>
<p class="source-code"><strong class="bold">50%          0.998331</strong></p>
<p class="source-code"><strong class="bold">75%          1.000000</strong></p>
<p class="source-code"><strong class="bold">max          1.000000</strong></p>
<p class="source-code"><strong class="bold">Name: variant_call_rate, dtype: float64</strong></p>
<p>Our variant <a id="_idIndexMarker462"/>call rate is quite good, which is not shocking <a id="_idIndexMarker463"/>because we are looking at array data – you would have worse numbers if you had a dataset based on NGS.</p>
<ol>
<li value="4">Let’s now look at sample statistics:<p class="source-code">sample_stats = sg.sample_stats(data)</p><p class="source-code">sample_stats</p></li>
</ol>
<p>Again, sgkit provides a lot of sample statistics out of the box:</p>
<div>
<div class="IMG---Figure" id="_idContainer045">
<img alt="Figure 6.3 - The sample statistics obtained by calling sample_stats  " height="597" src="image/B17942_06_3.jpg" width="566"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 - The sample statistics obtained by calling sample_stats </p>
<ol>
<li value="5">We will <a id="_idIndexMarker464"/>now have <a id="_idIndexMarker465"/>a look at sample call rates:<p class="source-code">sample_stats.sample_call_rate.to_series().hist()</p></li>
</ol>
<p>This time, we plot a histogram of sample call rates. Again, sgkit gets this for free by leveraging Pandas:</p>
<div>
<div class="IMG---Figure" id="_idContainer046">
<img alt="Figure 6.4 - The histogram of sample call rates " height="228" src="image/B17942_06_4.jpg" width="340"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 - The histogram of sample call rates</p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor168"/>There’s more...</h2>
<p>The truth is <a id="_idIndexMarker466"/>that for population genetic analysis, nothing beats R; you are definitely encouraged to take a look at the existing R libraries for <a id="_idIndexMarker467"/>population genetics. Do not forget that there is a Python-R bridge, which was discussed in <a href="B17942_01.xhtml#_idTextAnchor020"><em class="italic">Chapter 1</em></a>, <em class="italic">Python and the Surrounding Software Ecology</em>.</p>
<p>Most of the analysis presented here will be computationally costly if done on bigger datasets. Indeed, sgkit is prepared to deal with that because it leverages Dask. It would be too complex to introduce Dask at this stage, but for large datasets, <a href="B17942_11.xhtml#_idTextAnchor272"><em class="italic">Chapter 11</em></a> will discuss ways to address those.</p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/>See also</h2>
<ul>
<li>A list of R packages for statistical genetics is available at <a href="http://cran.r-project.org/web/views/Genetics.xhtml">http://cran.r-project.org/web/views/Genetics.xhtml</a>.</li>
<li>If you need to know more about population genetics, I recommend the book <em class="italic">Principles of Population Genetics</em>, by <em class="italic">Daniel L. Hartl and Andrew G. Clark</em>, <em class="italic">Sinauer Associates.</em></li>
</ul>
<h1 id="_idParaDest-171"><a id="_idTextAnchor170"/>Analyzing population structure</h1>
<p>Previously, we introduced <a id="_idIndexMarker468"/>data analysis with sgkit ignoring the population structure. Most datasets, including the one we are using, actually do have a population structure. Sgkit provides functionality to analyze genomic datasets with population structure and that is what we are going to investigate here.</p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor171"/>Getting ready</h2>
<p>You will need to have run the first recipe, and should have the <strong class="source-inline">hapmap10_auto_noofs_ld</strong> data we produced and also the original population meta data <strong class="source-inline">relationships_w_pops_041510.txt</strong> file downloaded. There is a Notebook file with the <strong class="source-inline">06_PopGen/Pop_Stats.py</strong> recipe in it.</p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor172"/>How to do it...</h2>
<p>Take a look at the following steps:</p>
<ol>
<li value="1">First, let’s load the PLINK data with sgkit:<p class="source-code">from collections import defaultdict</p><p class="source-code">from pprint import pprint</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import seaborn as sns</p><p class="source-code">import pandas as pd</p><p class="source-code">import xarray as xr</p><p class="source-code">import sgkit as sg</p><p class="source-code">from sgkit.io import plink</p><p class="source-code"> </p><p class="source-code">data = plink.read_plink(path='hapmap10_auto_noofs_ld', fam_sep='\t')</p></li>
<li>Now, let’s load the data assigning individuals to populations:<p class="source-code">f = open('relationships_w_pops_041510.txt')</p><p class="source-code">pop_ind = defaultdict(list)</p><p class="source-code">f.readline()  # header</p><p class="source-code">for line in f:</p><p class="source-code">    toks = line.rstrip().split('\t')</p><p class="source-code">    fam_id = toks[0]</p><p class="source-code">    ind_id = toks[1]</p><p class="source-code">    pop = toks[-1]</p><p class="source-code">    pop_ind[pop].append((fam_id, ind_id))</p><p class="source-code">pops = list(pop_ind.keys())</p></li>
</ol>
<p>We end up <a id="_idIndexMarker469"/>with a dictionary, <strong class="source-inline">pop_ind</strong>, where the key is the population code, and the value is a list of samples. Remember that a sample primary key is the family ID and the sample ID.</p>
<p>We also have a list of populations in the <strong class="source-inline">pops</strong> variable.</p>
<ol>
<li value="3">We now need to inform sgkit about to which population or cohort each sample belongs:<p class="source-code">def assign_cohort(pops, pop_ind, sample_family_id, sample_id):</p><p class="source-code">    cohort = []</p><p class="source-code">    for fid, sid in zip(sample_family_id, sample_id):</p><p class="source-code">        processed = False</p><p class="source-code">        for i, pop in enumerate(pops):</p><p class="source-code">            if (fid, sid) in pop_ind[pop]:</p><p class="source-code">                processed = True</p><p class="source-code">                cohort.append(i)</p><p class="source-code">                break</p><p class="source-code">        if not processed:</p><p class="source-code">            raise Exception(f'Not processed {fid}, {sid}')</p><p class="source-code">    return cohort</p><p class="source-code">cohort = assign_cohort(pops, pop_ind, data.sample_family_id.values, data.sample_id.values)</p><p class="source-code">data['sample_cohort'] = xr.DataArray(</p><p class="source-code">    cohort, dims='samples')</p></li>
</ol>
<p>Remember <a id="_idIndexMarker470"/>that each sample in sgkit has a position in an array. So, we have to create an array where each element refers to a specific population or cohort within a sample. The <strong class="source-inline">assign_cohort</strong> function does exactly that: it takes the metadata that we loaded from the <strong class="source-inline">relationships</strong> file and the list of samples from the sgkit file, and gets the population index for each sample. </p>
<ol>
<li value="4">Now that we have loaded population information structure into the sgkit dataset, we can start computing statistics at the population or cohort level. Let’s start by getting the number of monomorphic loci per population:<p class="source-code">cohort_allele_frequency = sg.cohort_allele_frequencies(data)['cohort_allele_frequency'].values</p><p class="source-code">monom = {}</p><p class="source-code">for i, pop in enumerate(pops):</p><p class="source-code">    monom[pop] = len(list(filter(lambda x: x, np.isin(cohort_allele_frequency[:, i, 0], [0, 1]))))</p><p class="source-code">pprint(monom)</p></li>
</ol>
<p>We start by asking sgkit to calculate the allele frequencies per cohort or population. After that, we filter all loci per population where the allele frequency of the first allele is either <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong> (that is, there is the fixation of one of the alleles). Finally, we print it. Incidentally, we use the <strong class="source-inline">pprint.pprint</strong> function to make <a id="_idIndexMarker471"/>it look a bit better (the function is quite useful for more complex structures if you want to render the output in a readable way):</p>
<p class="source-code"><strong class="bold">{'ASW': 3332,</strong></p>
<p class="source-code"><strong class="bold"> 'CEU': 8910,</strong></p>
<p class="source-code"><strong class="bold"> 'CHB': 11130,</strong></p>
<p class="source-code"><strong class="bold"> 'CHD': 12321,</strong></p>
<p class="source-code"><strong class="bold"> 'GIH': 8960,</strong></p>
<p class="source-code"><strong class="bold"> 'JPT': 13043,</strong></p>
<p class="source-code"><strong class="bold"> 'LWK': 3979,</strong></p>
<p class="source-code"><strong class="bold"> 'MEX': 6502,</strong></p>
<p class="source-code"><strong class="bold"> 'MKK': 3490,</strong></p>
<p class="source-code"><strong class="bold"> 'TSI': 8601,</strong></p>
<p class="source-code"><strong class="bold"> 'YRI': 5172}</strong></p>
<ol>
<li value="5">Let’s get the minimum allele frequency for all loci per population. This is still based in <strong class="source-inline">cohort_allele_frequency</strong> – so no need to call sgkit again:<p class="source-code">mafs = {}</p><p class="source-code">for i, pop in enumerate(pops):</p><p class="source-code">    min_freqs = map(</p><p class="source-code">        lambda x: x if x &lt; 0.5 else 1 - x,</p><p class="source-code">        filter(</p><p class="source-code">            lambda x: x not in [0, 1],</p><p class="source-code">            cohort_allele_frequency[:, i, 0]))</p><p class="source-code">    mafs[pop] = pd.Series(min_freqs)</p></li>
</ol>
<p>We create Pandas <strong class="source-inline">Series</strong> objects for each population, as this permits lots of helpful functions, such as plotting.</p>
<ol>
<li value="6">We will now <a id="_idIndexMarker472"/>print the MAF histograms for the <strong class="source-inline">YRI</strong> and <strong class="source-inline">JPT</strong> populations. We will leverage Pandas and Matplotlib for this:<p class="source-code">maf_plot, maf_ax = plt.subplots(nrows=2, sharey=True)</p><p class="source-code">mafs['YRI'].hist(ax=maf_ax[0], bins=50)</p><p class="source-code">maf_ax[0].set_title('*YRI*')</p><p class="source-code">mafs['JPT'].hist(ax=maf_ax[1], bins=50)</p><p class="source-code">maf_ax[1].set_title('*JPT*')</p><p class="source-code">maf_ax[1].set_xlabel('MAF')</p></li>
</ol>
<p>We get Pandas to generate the histograms and put the results in a Matplotlib plot. The result is the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<img alt="Figure 6.5 - A MAF histogram for the YRI and JPT populations " height="267" src="image/B17942_06_5.jpg" width="351"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 - A MAF histogram for the YRI and JPT populations</p>
<ol>
<li value="7">We are now going to concentrate on computing the FST. The FST is a widely used statistic that tries to represent the genetic variation created by population structure. Let’s compute it with sgkit:<p class="source-code">fst = sg.Fst(data)</p><p class="source-code">fst = fst.assign_coords({"cohorts_0": pops, "cohorts_1": pops})</p></li>
</ol>
<p>The first line computes <strong class="source-inline">fst</strong>, which, in this case, will be pairwise <strong class="source-inline">fst</strong> across cohorts or <a id="_idIndexMarker473"/>populations. The second line assigns names to each cohorts by using the xarray coordinates feature. This makes it easier and more declarative.</p>
<ol>
<li value="8">Let’s compare <strong class="source-inline">fst</strong> between the <strong class="source-inline">CEU</strong> and <strong class="source-inline">CHB</strong> populations with <strong class="source-inline">CHB</strong> and <strong class="source-inline">CHD</strong>:<p class="source-code">remove_nan = lambda data: filter(lambda x: not np.isnan(x), data)</p><p class="source-code">ceu_chb = pd.Series(remove_nan(fst.stat_Fst.sel(cohorts_0='CEU', cohorts_1='CHB').values))</p><p class="source-code">chb_chd = pd.Series(remove_nan(fst.stat_Fst.sel(cohorts_0='CHB', cohorts_1='CHD').values))</p><p class="source-code">ceu_chb.describe()</p><p class="source-code">chb_chd.describe()</p></li>
</ol>
<p>We take the pairwise results returned by the <strong class="source-inline">sel</strong> function from <strong class="source-inline">stat_FST</strong> to both compare and create a Pandas Series with it. Note that we can refer to populations by name, as we have prepared the coordinates in the previous step.</p>
<ol>
<li value="9">Let’s plot the distance matrix across populations based on the multi-locus pairwise FST. Before we do it, we will prepare the computation:<p class="source-code">mean_fst = {}</p><p class="source-code">for i, pop_i in enumerate(pops):</p><p class="source-code">    for j, pop_j in enumerate(pops):</p><p class="source-code">        if j &lt;= i:</p><p class="source-code">            continue</p><p class="source-code">        pair_fst = pd.Series(remove_nan(fst.stat_Fst.sel(cohorts_0=pop_i, cohorts_1=pop_j).values))</p><p class="source-code">        mean = pair_fst.mean()</p><p class="source-code">        mean_fst[(pop_i, pop_j)] = mean</p><p class="source-code">min_pair = min(mean_fst.values())</p><p class="source-code">max_pair = max(mean_fst.values())</p></li>
</ol>
<p>We compute all the FST values for the population pairs. The execution of this code will be <a id="_idIndexMarker474"/>demanding in terms of time and memory, as we are actually requiring Dask to perform a lot of computations to render our NumPy arrays.</p>
<ol>
<li value="10">We can now do a pairwise plot of all mean FSTs across populations:<p class="source-code">sns.set_style("white")</p><p class="source-code">num_pops = len(pops)</p><p class="source-code">arr = np.ones((num_pops - 1, num_pops - 1, 3), dtype=float)</p><p class="source-code">fig = plt.figure(figsize=(16, 9))</p><p class="source-code">ax = fig.add_subplot(111)</p><p class="source-code">for row in range(num_pops - 1):</p><p class="source-code">    pop_i = pops[row]</p><p class="source-code">    for col in range(row + 1, num_pops):</p><p class="source-code">        pop_j = pops[col]</p><p class="source-code">        val = mean_fst[(pop_i, pop_j)]</p><p class="source-code">        norm_val = (val - min_pair) / (max_pair - min_pair)</p><p class="source-code">        ax.text(col - 1, row, '%.3f' % val, ha='center')</p><p class="source-code">        if norm_val == 0.0:</p><p class="source-code">            arr[row, col - 1, 0] = 1</p><p class="source-code">            arr[row, col - 1, 1] = 1</p><p class="source-code">            arr[row, col - 1, 2] = 0</p><p class="source-code">        elif norm_val == 1.0:</p><p class="source-code">            arr[row, col - 1, 0] = 1</p><p class="source-code">            arr[row, col - 1, 1] = 0</p><p class="source-code">            arr[row, col - 1, 2] = 1</p><p class="source-code">        else:</p><p class="source-code">            arr[row, col - 1, 0] = 1 - norm_val</p><p class="source-code">            arr[row, col - 1, 1] = 1</p><p class="source-code">            arr[row, col - 1, 2] = 1</p><p class="source-code">ax.imshow(arr, interpolation='none')</p><p class="source-code">ax.set_title('Multilocus Pairwise FST')</p><p class="source-code">ax.set_xticks(range(num_pops - 1))</p><p class="source-code">ax.set_xticklabels(pops[1:])</p><p class="source-code">ax.set_yticks(range(num_pops - 1))</p><p class="source-code">ax.set_yticklabels(pops[:-1])</p></li>
</ol>
<p>In the following diagram, we will draw an upper triangular matrix, where the background color <a id="_idIndexMarker475"/>of a cell represents the measure of differentiation; white means less different (a lower FST) and blue means more different (a higher FST). The lowest value between <strong class="bold">CHB</strong> and <strong class="bold">CHD</strong> is represented in yellow, and the biggest value between <strong class="bold">JPT</strong> and <strong class="bold">YRI</strong> is represented in magenta. The value on each cell is the average pairwise FST between these two populations: </p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<img alt="Figure 6.6 - The average pairwise FST across the 11 populations in the HapMap project for all autosomes " height="481" src="image/B17942_06_6.jpg" width="489"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 - The average pairwise FST across the 11 populations in the HapMap project for all autosomes</p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor173"/>See also</h2>
<ul>
<li>F-statistics is <a id="_idIndexMarker476"/>an immensely complex topic, so I will direct you firstly to the Wikipedia page at <a href="http://en.wikipedia.org/wiki/F-statistics">http://en.wikipedia.org/wiki/F-statistics</a>.</li>
<li>A very good explanation can be found in Holsinger and Weir’s paper (<em class="italic">Genetics in geographically structured populations: defining, estimating, and interpreting FST</em>) in <em class="italic">Nature Reviews Genetics</em>, at <a href="http://www.nature.com/nrg/journal/v10/n9/abs/nrg2611.xhtml">http://www.nature.com/nrg/journal/v10/n9/abs/nrg2611.xhtml</a>.</li>
</ul>
<h1 id="_idParaDest-175"><a id="_idTextAnchor174"/>Performing a PCA</h1>
<p>PCA is a statistical procedure that’s used to perform a reduction of the dimension of a number of variables to a smaller subset that is linearly uncorrelated. Its practical application in <a id="_idIndexMarker477"/>population genetics is assisting with the visualization of the relationships between the individuals that are being studied.</p>
<p>While most of the recipes in this chapter make use of Python as a <em class="italic">glue language</em> (Python calls external applications that actually do most of the work), with PCA, we have an option: we can either use an external application (for example, EIGENSOFT SmartPCA) or use scikit-learn and perform everything on Python. In this recipe, we will use SmartPCA – for a native machine learning experience with scikit-learn, see <a href="B17942_10.xhtml#_idTextAnchor255"><em class="italic">Chapter 10</em></a>.</p>
<p class="callout-heading">TIP</p>
<p class="callout">You actually have a third option: using sgkit. However, I want to show you alternatives on how to perform computations. There are two good reasons for this. Firstly, you might prefer not to use sgkit – while I recommend it, I don’t want to force it – and secondly, you might be required to run an alternative method that is not implemented in sgkit. PCA is actually a good example of this: a reviewer on a paper might require you to run a published and widely used method such as EIGENSOFT SmartPCA.</p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor175"/>Getting ready</h2>
<p>You will need to run the first recipe in order to make use of the <strong class="source-inline">hapmap10_auto_noofs_ld_12</strong> PLINK file (with alleles recoded as <strong class="source-inline">1</strong> and <strong class="source-inline">2</strong>). PCA requires LD-pruned markers; we will not risk using the offspring here because it will probably bias the result. We will use the recoded PLINK file with alleles as <strong class="source-inline">1</strong> and <strong class="source-inline">2</strong> because this makes processing with SmartPCA and scikit-learn easier.</p>
<p>I have a simple library to help with some genomics processing. You can find this code at <a href="https://github.com/tiagoantao/pygenomics">https://github.com/tiagoantao/pygenomics</a>. You can install it with the following command:</p>
<p class="source-code">pip install pygenomics</p>
<p>For this recipe, you <a id="_idIndexMarker478"/>will need to download EIGENSOFT (<a href="http://www.hsph.harvard.edu/alkes-price/software/">http://www.hsph.harvard.edu/alkes-price/software/</a>), which includes the SmartPCA application that we will use.</p>
<p>There is a <a id="_idIndexMarker479"/>Notebook file in the <strong class="source-inline">Chapter06/PCA.py</strong> recipe, but you will still need to run the first recipe.</p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor176"/>How to do it...</h2>
<p>Take a look at the following steps:</p>
<ol>
<li value="1">Let’s load the metadata, as follows:<p class="source-code">f = open('relationships_w_pops_041510.txt')</p><p class="source-code">ind_pop = {}</p><p class="source-code">f.readline() # header</p><p class="source-code">for l in f:</p><p class="source-code">    toks = l.rstrip().split('\t')</p><p class="source-code">    fam_id = toks[0]</p><p class="source-code">    ind_id = toks[1]</p><p class="source-code">    pop = toks[-1]</p><p class="source-code">    ind_pop['/'.join([fam_id, ind_id])] = pop</p><p class="source-code">f.close()</p><p class="source-code">ind_pop['2469/NA20281'] = ind_pop['2805/NA20281']</p></li>
</ol>
<p>In this case, we will add an entry that is consistent with what is available in the PLINK file.</p>
<ol>
<li value="2">Let’s convert the PLINK file into the EIGENSOFT format:<p class="source-code">from genomics.popgen.plink.convert import to_eigen</p><p class="source-code">to_eigen('hapmap10_auto_noofs_ld_12', 'hapmap10_auto_noofs_ld_12')</p></li>
</ol>
<p>This uses a function that I have written to convert from PLINK to the EIGENSOFT format. This is mostly text manipulation—not exactly the most exciting code.</p>
<ol>
<li value="3">Now, we will run <strong class="source-inline">SmartPCA</strong> and parse its results, as follows:<p class="source-code">from genomics.popgen.pca import smart</p><p class="source-code">ctrl = smart.SmartPCAController('hapmap10_auto_noofs_ld_12')</p><p class="source-code">ctrl.run()</p><p class="source-code">wei, wei_perc, ind_comp = smart.parse_evec('hapmap10_auto_noofs_ld_12.evec', 'hapmap10_auto_noofs_ld_12.eval')</p></li>
</ol>
<p>Again, this <a id="_idIndexMarker480"/>will use a couple of functions from <strong class="source-inline">pygenomics</strong> to control <strong class="source-inline">SmartPCA</strong> and then parse the output. The code is typical for this kind of operation, and while you are invited to inspect it, it’s quite straightforward.</p>
<p>The <strong class="source-inline">parse</strong> function will return the PCA weights (which we will not use, but you should inspect), normalized weights, and then the principal components (usually up to PC <strong class="source-inline">10</strong>) per individual.</p>
<ol>
<li value="4">Then, we plot PC <strong class="source-inline">1</strong> and PC <strong class="source-inline">2</strong>, as shown in the following code:<p class="source-code">from genomics.popgen.pca import plot</p><p class="source-code">plot.render_pca(ind_comp, 1, 2, cluster=ind_pop)</p></li>
</ol>
<p>This will produce the following diagram. We will supply the plotting function and the population information retrieved from the metadata, which allows you to plot each population with a different color. The results are very similar to published results; we will find four groups. Most Asian populations are located at the top, the African populations are located on the right-hand side, and the European populations are located at the bottom. Two more admixed populations (<strong class="bold">GIH</strong> and <strong class="bold">MEX</strong>) are located in the middle:</p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<img alt="Figure 6.7 - PC 1 and PC 2 of the HapMap data, as produced by SmartPCA " height="479" src="image/B17942_06_7.jpg" width="899"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 - PC 1 and PC 2 of the HapMap data, as produced by SmartPCA</p>
<p class="callout-heading">Note</p>
<p class="callout">Note that PCA <a id="_idIndexMarker481"/>plots can be symmetrical in any axis across runs, as the signal does not matter. What matters is that the clusters should be the same and that the distances between individuals (and these clusters) should be similar.</p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor177"/>There’s more...</h2>
<p>An interesting question here is which method you should use – SmartPCA or scikit-learn, which we will use in <a href="B17942_10.xhtml#_idTextAnchor255"><em class="italic">Chapter 10</em></a>. The results are similar, so if you are performing your own analysis, you are free to choose. However, if you publish your results in a scientific journal, SmartPCA is probably a safer choice because it’s based on the published piece of software <a id="_idIndexMarker482"/>in the field of genetics; reviewers will probably prefer this.</p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor178"/>See also</h2>
<ul>
<li>The paper that probably popularized the use of PCA in genetics was Novembre et al.’s <em class="italic">Genes mirror geography within Europe</em> on <em class="italic">Nature</em>, where a PCA of Europeans mapped almost perfectly to a map of Europe. This can be found at <a href="http://www.nature.com/nature/journal/v456/n7218/abs/nature07331.xhtml">http://www.nature.com/nature/journal/v456/n7218/abs/nature07331.xhtml</a>. Note that there is nothing about PCA that assures it will map to geographical features (just check our PCA earlier).</li>
<li>The SmartPCA is described in Patterson et al.’s <em class="italic">Population Structure and Eigenanalysis</em>, <em class="italic">PLoS Genetics</em>, at <a href="http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190">http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020190</a>.</li>
<li>A discussion of the meaning of PCA can be found in McVean’s paper on <em class="italic">A Genealogical Interpretation of Principal Components Analysis</em>, <em class="italic">PLoS Genetics</em>, at <a href="http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000686">http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000686</a>.</li>
</ul>
<h1 id="_idParaDest-180"><a id="_idTextAnchor179"/>Investigating population structure with admixture</h1>
<p>A typical <a id="_idIndexMarker483"/>analysis in population <a id="_idIndexMarker484"/>genetics was the one popularized by the program structure (<a href="https://web.stanford.edu/group/pritchardlab/structure.xhtml">https://web.stanford.edu/group/pritchardlab/structure.xhtml</a>), which is used to study population <a id="_idIndexMarker485"/>structure. This type of software is used to <a id="_idIndexMarker486"/>infer how many populations exist (or how many ancestral populations generated the current population), and to identify potential migrants and admixed individuals. The structure was developed quite some time ago, when far fewer markers were genotyped (at that time, this was mostly a handful of microsatellites), and faster versions were developed, including <a id="_idIndexMarker487"/>one from the same laboratory called <strong class="source-inline">fastStructure</strong> (<a href="http://rajanil.github.io/fastStructure/">http://rajanil.github.io/fastStructure/</a>). Here, we will use Python to interface with a program of <a id="_idIndexMarker488"/>the same type that was developed at UCLA, called admixture (<a href="https://dalexander.github.io/admixture/download.xhtml">https://dalexander.github.io/admixture/download.xhtml</a>).</p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor180"/>Getting ready</h2>
<p>You will need to run the first recipe in order to use the <strong class="source-inline">hapmap10_auto_noofs_ld</strong> binary PLINK file. Again, we will use a 10 percent subsampling of autosomes that have been LD-pruned with no offspring.</p>
<p>As in the <a id="_idIndexMarker489"/>previous recipe, you will use <a id="_idIndexMarker490"/>the <strong class="source-inline">pygenomics</strong> library to help; you can find these code files at <a href="https://github.com/tiagoantao/pygenomics">https://github.com/tiagoantao/pygenomics</a>. You can install it with the following command:</p>
<p class="source-code">pip install pygenomics</p>
<p>In theory, for this recipe, you will need to download admixture (<a href="https://www.genetics.ucla.edu/software/admixture/">https://www.genetics.ucla.edu/software/admixture/</a>). However, in this case, I will provide the outputs of running admixture on the HapMap data that we will use, because running admixture takes a lot of time. You can either use the results available or run admixture yourself. There is a Notebook file for this in the <strong class="source-inline">Chapter06/Admixture.py</strong> recipe, but you will still need to run the recipe first.</p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor181"/>How to do it...</h2>
<p>Take a look at the following steps:</p>
<ol>
<li value="1">First, let’s define our <strong class="source-inline">k</strong> (a number of ancestral populations) range of interest, as follows:<p class="source-code">k_range = range(2, 10)  # 2..9</p></li>
<li>Let’s run admixture for all our <strong class="source-inline">k</strong> (alternatively, you can skip this step and use the example data provided):<p class="source-code">for k in k_range:</p><p class="source-code">    os.system('admixture --cv=10 hapmap10_auto_noofs_ld.bed %d &gt; admix.%d' % (k, k))</p></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This is the worst possible way of running admixture and will probably take more than 3 hours if you do it this way. This is because it will run all <strong class="source-inline">k</strong> from <strong class="source-inline">2</strong> to <strong class="source-inline">9</strong> in a sequence. There are two things that you can do to speed this up: use the multithreaded option (<strong class="source-inline">-j</strong>), which admixture provides, or run several applications in parallel. Here, I have to assume a worst-case scenario where you only have a single core and thread available, but you should be able to run this more efficiently by parallelizing. We will discuss this issue at length in <a href="B17942_11.xhtml#_idTextAnchor272"><em class="italic">Chapter 11</em></a>.</p>
<ol>
<li value="3">We will <a id="_idIndexMarker491"/>need the order <a id="_idIndexMarker492"/>of individuals in the PLINK file, as admixture outputs individual results in this order:<p class="source-code">f = open('hapmap10_auto_noofs_ld.fam')</p><p class="source-code">ind_order = []</p><p class="source-code">for l in f:</p><p class="source-code">    toks = l.rstrip().replace(' ', '\t').split('\t')</p><p class="source-code">    fam_id = toks[0]</p><p class="source-code">    ind_id = toks[1]</p><p class="source-code">    ind_order.append((fam_id, ind_id))</p><p class="source-code">f.close()</p></li>
<li>The cross-validation error gives a measure of the “best” <strong class="source-inline">k</strong>, as follows:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">CVs = []</p><p class="source-code">for k in k_range:</p><p class="source-code">    f = open('admix.%d' % k)</p><p class="source-code">    for l in f:</p><p class="source-code">        if l.find('CV error') &gt; -1:</p><p class="source-code">            CVs.append(float(l.rstrip().split(' ')[-1]))</p><p class="source-code">            break</p><p class="source-code">    f.close()</p><p class="source-code">fig = plt.figure(figsize=(16, 9))</p><p class="source-code">ax = fig.add_subplot(111)</p><p class="source-code">ax.set_title('Cross-Validation error')</p><p class="source-code">ax.set_xlabel('K')</p><p class="source-code">ax.plot(k_range, CVs)</p></li>
</ol>
<p>The following <a id="_idIndexMarker493"/>graph plots the <strong class="source-inline">CV</strong> between a <strong class="source-inline">K</strong> of <strong class="source-inline">2</strong> and <strong class="source-inline">9</strong>, the lower, the better. It should be clear from this <a id="_idIndexMarker494"/>graph that we should maybe run some more <strong class="source-inline">K</strong> (indeed, we have 11 populations; if not more, we should at least run up to 11), but due to computation costs, we stopped at <strong class="source-inline">9</strong>.</p>
<p>It would be a very technical debate on whether there is such thing as the “best” <strong class="source-inline">K</strong>. Modern scientific literature suggests that there may not be a “best” <strong class="source-inline">K</strong>; these results are worthy of some interpretation. I think it’s important that you are aware of this before you go ahead and interpret the <strong class="source-inline">K</strong> results:</p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<img alt="Figure 6.8 - The error by K " height="550" src="image/B17942_06_8.jpg" width="946"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 - The error by K</p>
<ol>
<li value="5">We will <a id="_idIndexMarker495"/>need the metadata for <a id="_idIndexMarker496"/>the population information:<p class="source-code">f = open('relationships_w_pops_041510.txt')</p><p class="source-code">pop_ind = defaultdict(list)</p><p class="source-code">f.readline() # header</p><p class="source-code">for l in f:</p><p class="source-code">   toks = l.rstrip().split('\t')</p><p class="source-code">   fam_id = toks[0]</p><p class="source-code">   ind_id = toks[1]</p><p class="source-code">   if (fam_id, ind_id) not in ind_order:</p><p class="source-code">      continue</p><p class="source-code">   mom = toks[2]</p><p class="source-code">   dad = toks[3]</p><p class="source-code">   if mom != '0' or dad != '0':</p><p class="source-code">      continue</p><p class="source-code"> pop = toks[-1]</p><p class="source-code"> pop_ind[pop].append((fam_id, ind_id))</p><p class="source-code">f.close()</p></li>
</ol>
<p>We will <a id="_idIndexMarker497"/>ignore individuals that are not in the PLINK file.</p>
<ol>
<li value="6">Let’s <a id="_idIndexMarker498"/>load the individual component, as follows:<p class="source-code">def load_Q(fname, ind_order):</p><p class="source-code">    ind_comps = {}</p><p class="source-code">    f = open(fname)</p><p class="source-code">    for i, l in enumerate(f):</p><p class="source-code">        comps = [float(x) for x in l.rstrip().split(' ')]</p><p class="source-code">        ind_comps[ind_order[i]] = comps</p><p class="source-code">    f.close()</p><p class="source-code">    return ind_comps</p><p class="source-code">comps = {}</p><p class="source-code">for k in k_range:</p><p class="source-code">    comps[k] = load_Q('hapmap10_auto_noofs_ld.%d.Q' % k, ind_order)</p></li>
</ol>
<p>Admixture produces a file with the ancestral component per individual (for an example, look at any of the generated <strong class="source-inline">Q</strong> files); there will be as many components as the number of <strong class="source-inline">k</strong> that you decided to study. Here, we will load the <strong class="source-inline">Q</strong> file for all <strong class="source-inline">k</strong> that we studied and store them in a dictionary where the individual ID is the key.</p>
<ol>
<li value="7">Then, we cluster individuals, as follows:<p class="source-code">from genomics.popgen.admix import cluster</p><p class="source-code">ordering = {}</p><p class="source-code">for k in k_range:</p><p class="source-code">    ordering[k] = cluster(comps[k], pop_ind)</p></li>
</ol>
<p>Remember <a id="_idIndexMarker499"/>that individuals <a id="_idIndexMarker500"/>were given components of ancestral populations by admixture; we would like to order them per their similarity in terms of ancestral components (not by their order in the PLINK file). This is not a trivial exercise and requires a clustering algorithm.</p>
<p>Furthermore, we do not want to order all of them; we want to order them in each population and then order each population accordingly.</p>
<p>For this purpose, I have some clustering code available at <a href="https://github.com/tiagoantao/pygenomics/blob/master/genomics/popgen/admix/__init__.py">https://github.com/tiagoantao/pygenomics/blob/master/genomics/popgen/admix/__init__.py</a>. This is far from perfect but allows you to perform some plotting that still looks reasonable. My code makes use of the SciPy clustering code. I suggest you take a look (by the way, it’s not very difficult to improve upon it).</p>
<ol>
<li value="8">With a sensible individual order, we can now plot the admixture:<p class="source-code">from genomics.popgen.admix import plot</p><p class="source-code">plot.single(comps[4], ordering[4])</p><p class="source-code">fig = plt.figure(figsize=(16, 9))</p><p class="source-code">plot.stacked(comps, ordering[7], fig)</p></li>
</ol>
<p>This will produce two charts; the second chart is shown in the following diagram (the first chart is actually a variation of the third admixture plot from the top).</p>
<p>The first figure of <strong class="source-inline">K</strong> = <strong class="source-inline">4</strong> requires the components per individual and their order. It will plot all individuals, ordered and split by population.</p>
<p>The second <a id="_idIndexMarker501"/>chart will perform a set of stacked plots of admixture from <strong class="source-inline">K</strong> = <strong class="source-inline">2</strong> to <strong class="source-inline">9</strong>. It requires a <strong class="source-inline">figure</strong> object (as the dimension of this figure can vary widely with the number of <a id="_idIndexMarker502"/>stacked admixtures that you require). The individual order will typically follow one of the <strong class="source-inline">K</strong> (we have chosen a <strong class="source-inline">K</strong> of <strong class="source-inline">7</strong> here).</p>
<p>Note that all <strong class="source-inline">K</strong> are worthy of some interpretation (for example, <strong class="source-inline">K</strong> = <strong class="source-inline">2</strong> separates the African population from others, and <strong class="source-inline">K</strong> = <strong class="source-inline">3</strong> separates the European population and shows the admixture of <strong class="bold">GIH</strong> and <strong class="bold">MEX</strong>):</p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<img alt="Figure 6.9 - A stacked admixture plot (between K of 2 and 9) for the HapMap example " height="626" src="image/B17942_06_9.jpg" width="1130"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 - A stacked admixture plot (between K of 2 and 9) for the HapMap example</p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor182"/>There’s more...</h2>
<p>Unfortunately, you cannot run a single instance of admixture to get a result. The best practice is to actually run 100 instances and get the one with the best log likelihood (which is reported in the admixture output). Obviously, I cannot ask you to run 100 instances <a id="_idIndexMarker503"/>for each of the 7 different <strong class="source-inline">K</strong> for this recipe (we are talking about two weeks of computation), but you will probably have <a id="_idIndexMarker504"/>to perform this if you want to have publishable results. A cluster (or at least a very good machine) is required to run this. You can use Python to go through outputs and select the best log likelihood. After selecting the result with the best log likelihood for each <strong class="source-inline">K</strong>, you can easily apply this recipe to plot the output.</p>
</div>
</div></body></html>