- en: 'Chapter 11: Data Visualization with PySpark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, from [*Chapter 1*](B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014)*,*
    *Distributed Computing Primer**,* through [*Chapter 9*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164),
    *Machine Learning Life Cycle Management*, you have learned how to ingest, integrate,
    and cleanse data, as well as how to make data conducive for analytics. You have
    also learned how to make use of clean data for practical business applications
    using data science and machine learning. This chapter will introduce you to the
    basics of deriving meaning out of data using data visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Importance of data visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for visualizing data using PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations for PySpark to pandas conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization is the process of graphically representing data using visual
    elements such as charts, graphs, and maps. Data visualization helps you understand
    patterns within data in a visual manner. In the big data world, with massive amounts
    of data, it is even more important to make use of data visualizations to derive
    meaning out of such data and present it in a simple and easy-to-understand way
    to business users; this helps them make data-driven decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be using Databricks Community Edition to run our code.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Community Edition can be accessed at [https://community.cloud.databricks.com](https://community.cloud.databricks.com).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code and data for this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter11](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter11).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importance of data visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data visualization is the process of translating data into a pictorial representation
    in the form of graphs, charts, or maps. This makes it easier for the human mind
    to comprehend complex information. Typically, data visualization is the final
    stage of business analytics and the first step of any data science process. Though
    there are professionals who deal solely with data visualizations, any data professional
    needs to be able to understand and produce data visualizations. They help convey
    complex patterns that are hidden within data in an easy-to-understand way to business
    users. Every business needs information for optimal performance, and data visualization
    helps businesses make easier data-driven decisions by representing relationships
    between datasets in a visual way and surfacing actionable insights. With the advent
    of big data, there has been an explosion of both structured and unstructured data,
    and it is difficult to make sense of it without the help of visual aids. Data
    visualization helps in accelerating the decision-making process by surfacing key
    business information and helps business users act on those insights quickly. Data
    visualization also aids the storytelling process by helping convey the right message
    to the right audience.
  prefs: []
  type: TYPE_NORMAL
- en: A data visualization can be a simple graph representing a single aspect of the
    current state of the business, a complex sales report, or a dashboard that gives
    a holistic view of an organization's performance. Data visualization tools are
    key to unlocking the power of data visualizations. We will explore the different
    types of data visualization tools that are available in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Types of data visualization tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data visualization tools provide us with an easier way to create data visualizations.
    They allow data analysts and data scientists to create data visualizations conveniently
    by providing a graphical user interface, database connections, and sometimes data
    manipulation tools in a single, unified interface. There are different types of
    data visualizations tools, and each serves a slightly different purpose. We will
    explore them in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Business intelligence tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Business intelligence** **(BI)** tools are typically enterprise-grade tools
    that help organizations track and visually represent their **Key Performance Indicators**
    (**KPIs**). BI tools typically include provisions for creating complex logical
    data models and contain data cleansing and integration mechanisms. BI tools also
    include connectors to a myriad of data sources and built-in data visualizations
    with drag-and-drop features to help business users quickly create data visualizations,
    operational and performance dashboards, and scorecards to track the performance
    of an individual department or the entire organization. The primary users of BI
    tools are business analysts and business executives involved in making tactical
    and strategic decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: BI tools traditionally use data warehouses as their data sources, but modern
    BI tools support RDMS, NoSQL databases, and data lakes as data sources. Some examples
    of prominent BI tools include **Tableau**, **Looker**, **Microsoft Power BI**,
    **SAP Business Objects**, **MicroStrategy**, **IBM Cognos**, and **Qlikview**,
    to name a few. BI tools can connect to Apache Spark and consume data stored in
    Spark SQL tables using an ODBC connection. These concepts will be explored in
    detail in [*Chapter 13*](B16736_13_Final_JM_ePub.xhtml#_idTextAnchor214), *Integrating
    External Tools with Spark SQL*. A class of data visualization tools with all the
    necessary data visualization and data connectivity components, minus any data
    processing capabilities such as Redash, also exist and they can also connect to
    Apache Spark via an ODBC connection.
  prefs: []
  type: TYPE_NORMAL
- en: Observability tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Observability is the process of constantly monitoring and understanding what's
    happening in highly distributed systems. Observability helps us understand what
    is slow or broken, as well as what needs to be fixed to improve performance. However,
    since modern cloud environments are dynamic and constantly increasing in scale
    and complexity, most problems are neither known nor monitored. Observability addresses
    common issues with modern cloud environments that are dynamic and ever-increasing
    in scale by enabling you to continuously monitor and surface any issues that might
    arise. Observability tools help businesses continuously monitor systems and applications
    and enable a business to receive actionable insights into system behavior, as
    well as predict outages or problems before they occur. Data visualization is an
    important component of observability tools; a few popular examples include Grafana
    and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Data teams are typically not responsible for monitoring and maintaining the
    health of data processing systems â€“ this is usually handled by specialists such
    as **DevOps** engineers. Apache Spark doesn't have any direct integrations with
    any observability tools out of the box, but it can be integrated with popular
    observability platforms such as **Prometheus** and **Grafana**. Apache Spark's
    integration with observability tools is outside the scope of this book, so we
    won't discuss this here.
  prefs: []
  type: TYPE_NORMAL
- en: Notebooks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Notebooks are interactive computing tools that are used to execute code, visualize
    results, and share insights. Notebooks are indispensable tools in the data science
    process and are becoming prominent in the entire data analytics development life
    cycle, as you have witnessed throughout this book. Notebooks are also excellent
    data visualization tools as they help you convert your Python or SQL code into
    easy-to-understand interactive data visualizations. Some notebooks, such as Databricks,
    Jupyter, and Zeppelin notebooks can also be used as standalone dashboards. The
    remainder of this chapter will focus on how notebooks can be used as data visualization
    tools when using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for visualizing data using PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a unified data processing engine and doesn't come out of the
    box with a graphical user interface, per se. As discussed in the previous sections,
    data that's been processed by Apache Spark can be stored in data warehouses and
    visualized using BI tools or natively visualized using notebooks. In this section,
    we will focus on how to leverage notebooks to interactively process and visualize
    data using PySpark. As we have done throughout this book, we will be making use
    of notebooks that come with **Databricks Community Edition**, though **Jupyter**
    and **Zeppelin** notebooks can also be used.
  prefs: []
  type: TYPE_NORMAL
- en: PySpark native data visualizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There aren't any data visualization libraries that can work with PySpark DataFrames
    natively. However, the notebook implementations of cloud-based Spark distributions
    such as Databricks and Qubole support natively visualizing Spark DataFrames using
    the built-in `display()` function. Let's see how we can use the `display()` function
    to visualize PySpark DataFrames in Databricks Community Edition.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the cleansed, integrated, and wrangled dataset that we produced
    toward the end of [*Chapter 6*](B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107),
    *Feature Engineering â€“ Extraction, Transformation, and Selection*, here, as shown
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code snippet, we read a table into a Spark DataFrame and selected
    the columns that we intend to visualize. Then, we called the `display()` method
    on the Spark DataFrame. The result is a grid display in the notebook, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 â€“ The grid widget'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 â€“ The grid widget
  prefs: []
  type: TYPE_NORMAL
- en: The previous screenshot shows the result of calling the `display()` function
    on a Spark DataFrame within a Databricks notebook. This way, any Spark DataFrame
    can be visualized in a tabular format within Databricks notebooks. The tabular
    grid supports sorting arbitrary columns. Databricks notebooks also support charts
    and graphs that can be used from within the notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Databricks's `display()` method supports all of Spark's programming APIs, including
    Python, Scala, R, and SQL. In addition, the `display()` method can also render
    Python pandas DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the same grid display and convert it into a graph by clicking on
    the graph icon and choosing the desired graph from the list, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 â€“ Graph options'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 â€“ Graph options
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the graph menu has multiple chart options, with the bar chart
    being the first on the list. If you choose the bar chart, plot options can be
    used to configure the chart''s key, value, and series grouping options. Similarly,
    we can use a line graph or a pie chart, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 â€“ Pie chart'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 â€“ Pie chart
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the `display()` function can be used to display various kinds of charts
    within the notebook and help configure various graph options. Databricks notebooks
    also support a rudimentary map widget that can visualize metrics on a world map,
    as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 â€“ World map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 â€“ World map
  prefs: []
  type: TYPE_NORMAL
- en: The previous screenshot shows metrics on a world map. Since our dataset only
    contains a few European countries, France and the UK have been shaded in on the
    map widget.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For this widget, the values should be either country codes in ISO 3166-1 alpha-3
    format ("GBR") or US state abbreviations ("TX").
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to basic bars and charts, Databricks notebooks also support scientific
    visualizations such as **scatter plots**, **histograms**, **quantile plots**,
    and **Q-Q** plots, as illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 â€“ Quantile plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.5 â€“ Quantile plot
  prefs: []
  type: TYPE_NORMAL
- en: A quantile plot, as illustrated in the previous figure, helps determine whether
    two datasets have a common distribution. Quantile plots are available in Databricks
    notebooks via the graph menu, and plot properties such as keys, values, and series
    groupings are available via the plot options menu.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the following code to make Databricks notebooks display images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code snippet uses Apache Spark''s built-in image data source to
    load images from a directory on persistent storage such as a data lake:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 â€“ Image data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 â€“ Image data
  prefs: []
  type: TYPE_NORMAL
- en: This image is rendered in a notebook using Databricks's `display()` function
    as it is capable of displaying image previews.
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks notebooks are also capable of rendering machine learning-specific
    visualizations such as `display()` function to visualize a **decision tree** model
    that we had trained, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 â€“ Decision tree model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.7 â€“ Decision tree model
  prefs: []
  type: TYPE_NORMAL
- en: The previous screenshot shows the decision tree model that we built using Spark
    ML, rendered in a Databricks notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'More information on rendering machine learning-specific visualizations using
    Databricks notebooks can be found in Databricks''s public documentation here:
    [https://docs.databricks.com/notebooks/visualizations/index.html#machine-learning-visualizations](https://docs.databricks.com/notebooks/visualizations/index.html#machine-learning-visualizations).'
  prefs: []
  type: TYPE_NORMAL
- en: Interactive visuals using JavaScript and HTML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Databricks notebooks also support `displayHTML()` function. You can pass any
    arbitrary HTML code to `displayHTML()` and have it rendered in a notebook, as
    shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet displays an arbitrary HTML hyperlink in a notebook.
    Other HTML elements such as paragraphs, headings, images, and more can also be
    used with the `displayHTML()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: HTML blocks such as hyperlinks, images, and tables can be used to make your
    notebooks more descriptive and interactive for end users and can aid in the storytelling
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, SVG graphics can also be rendered using the `displayHTML()` function,
    as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code renders an orange-colored, animated ellipse that fades in
    and out. Far more complex SVG graphics can also be rendered and data from a Spark
    DataFrame can be passed along. Similarly, the popular HTML and JavaScript-based
    visualization library can also be leveraged with Databricks notebooks, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 â€“ Word cloud using D3.js'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.8 â€“ Word cloud using D3.js
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have taken the `description` column from the `retail_sales` Delta table
    that we created during our data processing steps in the previous chapters, and
    then we extracted individual words from the item description column. Then, we
    rendered the words using a word cloud visualization by using HTML, CSS, and JavaScript.
    After, we used the popular D3.js JavaScript library to manipulate the HTML documents
    based on data. The code for this visualization can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/Chapter11/databricks-charts-graphs.py](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/Chapter11/databricks-charts-graphs.py).
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have seen some of the basic and statistical graphs that are available
    via the Databricks notebook interface, which can work natively with Spark DataFrames.
    However, sometimes, you may need some additional graphs and charts that aren't
    available within the notebook, or you may need more control over your graph. In
    these instances, popular visualization libraries for Python such as `matplotlib`,
    `plotly`, `seaborn`, `altair`, `bokeh`, and so on can be used with PySpark. We
    will explore some of these visualization libraries in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python data visualizations with PySpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you learned in the previous section, PySpark doesn't inherently have any
    visualization capabilities, but you can choose to use Databricks notebook capabilities
    to visualize data in Spark DataFrames. In situations where using Databricks notebooks
    is not possible, you can use popular Python-based visualizations libraries to
    visualize your data using any notebook interface that you are comfortable with.
    In this section, we will explore some prominent Python visualization libraries
    and how to use them for data visualization in Databricks notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Creating two-dimensional plots using Matplotlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`PyPI` repository using a package manager such as `pip`. The following code
    example shows how Matplotlib can be used with PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code snippet, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we imported the `pandas` and `matplotlib` libraries, assuming they are
    already installed in the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we generated a Spark DataFrame with the required columns using the online
    retail dataset that we have created during the data processing steps in the previous
    chapters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since Python-based visualization libraries cannot directly use Spark DataFrames,
    we converted the Spark DataFrame into a pandas DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we converted the quantity column into a numeric data type so that we could
    plot it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, we defined a plot on the pandas DataFrame using the `plot()` method
    of the Matplotlib library, specified the type of plot to be generated as a bar
    graph, and passed the x-axis and y-axis column names.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some notebook environments may require you to explicitly call the `display()`
    function for the plot to be displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This way, Matplotlib can be used with any Spark DataFrame if we convert it
    into a pandas DataFrame. The plot that was generated looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 â€“ Matplotlib visualization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.9 â€“ Matplotlib visualization
  prefs: []
  type: TYPE_NORMAL
- en: The previous graph depicts the number of items that have been sold over a certain
    period.
  prefs: []
  type: TYPE_NORMAL
- en: Scientific visualizations using Seaborn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`pip`. The following code sample shows how Seaborn can be used with PySpark
    DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code snippet, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we imported the `matplotlib` and `seaborn` libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we converted the Spark DataFrame, which contains a single column called
    `unit_price`, into a pandas DataFrame using the `toPandas()` PySpark function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we defined our plot dimensions using the `plot.figure()` Matplotlib method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we plotted a boxplot by invoking the `seaborn.boxplot()` method and
    passing the pandas DataFrame with a single column. The resultant plot is shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.10 â€“ Seaborn boxplot visualization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.10 â€“ Seaborn boxplot visualization
  prefs: []
  type: TYPE_NORMAL
- en: The previous screenshot depicts how the **unit_price** column can be distributed
    as a box plot using its minimum, first quartile, median, third quartile, and maximum
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive visualizations using Plotly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Plotly** is a JavaScript-based visualization library that enables Python
    users to create interactive web-based visualizations that can be displayed in
    notebooks or saved to standalone HTML files. Plotly comes pre-installed with Databricks
    and can be used like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code snippet, we did the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we imported the `matplotlib` and `seaborn` libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we converted the Spark DataFrame, along with the required columns, into
    a pandas DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we defined the Plotly plot parameters using the `plot.scatter()` method.
    This method configures a scatter plot with three dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we rendered the plot using the `fig.show()` method. The resultant
    plot is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.11 â€“ Plotly bubble chart visualization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.11 â€“ Plotly bubble chart visualization
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows a bubble graph that depicts three metrics along
    three dimensions. The plot is interactive, and information is provided when you
    hover your mouse over various parts of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Declarative visualizations using Altair
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Altair** is a declarative statistical visualization library for Python. Altair
    is based on an open source, declarative grammar engine called **Vega**. Altair
    also offers a concise visualization grammar that enables users to build a wide
    range of visualizations quickly. It can be installed using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous command installs Altair in the notebook''s local Python kernel
    and restarts it. Once Altair has been successfully installed, it can be invoked
    using the usual Python `import` statements, as shown in the following code sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code snippet, we imported the Altair and pandas libraries.
    Then, we selected the required columns from the Spark table and convert them into
    a pandas DataFrame. Once we have data in Python in a pandas DataFrame, Altair
    can be used to create a plot, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 â€“ Altair isotype visualization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.12 â€“ Altair isotype visualization
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure depicts an isotype visualization that shows the distribution
    of occupations by gender, across countries. Other open source libraries such as
    `bokeh`, `pygal`, and `leather` can also be used to visualize PySpark DataFrames.
    Bokeh is another popular data visualization library in Python that provides high-performance
    interactive charts and plots. Bokeh is based on JavaScript and HTML and unlike
    Matplotlib, it lets users create custom visualizations. Information on using Bokeh
    in Databricks notebooks can be found in Databricks's public documentation at [https://docs.databricks.com/notebooks/visualizations/bokeh.html#bokeh](https://docs.databricks.com/notebooks/visualizations/bokeh.html#bokeh).
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have learned how to use some of the popular visualizations that
    are available for Python with Spark DataFrames by converting PySpark DataFrames
    into pandas DataFrames. However, there are some performance considerations and
    limitations you must consider when converting PySpark DataFrames into pandas DataFrames.
    We will look at these in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for PySpark to pandas conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will introduce **pandas**, demonstrate the differences between
    pandas and PySpark, and the considerations that need to be kept in mind while
    converting datasets between PySpark and pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**pandas** is one of the most widely used open source data analysis libraries
    for Python. It contains a diverse set of utilities for processing, manipulating,
    cleaning, munging, and wrangling data. pandas is much easier to work with than
    Pythons lists, dictionaries, and loops. In some ways, pandas is like other statistical
    data analysis tools such as R or SPSS, which makes it very popular with data science
    and machine learning enthusiasts.'
  prefs: []
  type: TYPE_NORMAL
- en: The primary abstractions of pandas are **Series** and **DataFrames**, with the
    former essentially being a one-dimensional array and the latter a two-dimensional
    array. One of the fundamental differences between pandas and PySpark is that pandas
    represents its datasets as one- and two-dimensional **NumPy** arrays, while PySpark
    DataFrames are collections of **Row** and **Column** objects, based on Spark SQL.
    While pandas DataFrames can only be manipulated using pandas DSL, PySpark DataFrames
    can be manipulated using Spark's DataFrame DSL, as well as SQL. Owing to this
    difference, developers familiar with manipulating pandas might find PySpark to
    be different and may face a learning curve when working with the platform. The
    Apache Spark community realized this difficulty and launched a new open source
    project called Koalas. Koalas implements a pandas-like API on top of Spark DataFrames
    to try and overcome the previously mentioned difference between pandas and PySpark.
    More information on using Koalas will be presented in [*Chapter 10*](B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176),
    *Scaling Out Single-Node Machine Learning Using PySpark*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy is a Python package for scientific computing that provides a multi-dimensional
    array and a set of routines for fast operations on arrays. More information about
    NumPy can be found here: [https://numpy.org/doc/stable/user/whatisnumpy.html](https://numpy.org/doc/stable/user/whatisnumpy.html).'
  prefs: []
  type: TYPE_NORMAL
- en: The other fundamental difference, in the context of big data and processing
    massive amounts of data at big data scale, is that pandas was designed to process
    data on a single machine and PySpark, by design, is distributed and can process
    data on multiple machines in a massively parallel manner. This brings up an important
    limitation of pandas compared to PySpark, as well as some important considerations
    that the developer needs to keep in mind while converting from pandas into PySpark.
    We will look at these in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Converting from PySpark into pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The PySpark API comes with a handy utility function called `DataFrame.toPandas()`
    that converts PySpark DataFrames into pandas DataFrames. This function has been
    demonstrated throughout this chapter. If you recall our discussions from [*Chapter
    1*](B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014),*Distributed Computing Primer*,
    especially the *Spark''s cluster architecture* section, a Spark cluster consists
    of a **Driver** process and a set of executor processes on worker machines, with
    the Driver being responsible for compiling user code, passing it on to the workers,
    managing and communicating with the workers, and if required, aggregating and
    collecting data from the workers. The Spark workers are responsible for all the
    data processing tasks. However, pandas is not based on the distributed computing
    paradigm and works solely on a single computing machine. Thus, when you execute
    pandas code on a Spark cluster, it executes on the Driver or the Master node,
    as depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 â€“ PySpark architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_11_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.13 â€“ PySpark architecture
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, Python and `toPandas()` function is called on a Spark DataFrame,
    it collects rows from all the Spark workers and then creates a pandas DataFrame
    locally on the Driver inside the Python kernel.
  prefs: []
  type: TYPE_NORMAL
- en: The first issue with this is that `toPandas()` practically collects all the
    data from the workers and brings it back to the Driver. This may cause the Driver
    to run out of memory if the dataset being collected is too large. Another issue
    with this process is that by default, the **Row** objects of the Spark DataFrame
    are collected on the Driver as a **list** of **tuples**, and then converted to
    a pandas DataFrame. This ends up using a large amount of memory and sometimes
    even data that's twice the size of the Spark DataFrame being collected.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate some of the memory issues during PySpark to pandas conversion, Apache
    Arrow can be used. Apache Arrow is an in-memory, columnar data format that is
    similar to Spark's internal representation of datasets and is efficient at transferring
    data between the JVM and Python processes. Apache Arrow is not enabled by default
    in Spark and needs to be enabled by setting the `spark.sql.execution.arrow.enabled`
    Spark configuration to `true`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: PyArrow, the Python binding of Apache Arrow, is pre-installed on Databricks
    Runtime. However, you might need to install a version of PyArrow that's appropriate
    for the Spark and Python versions of your Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Arrow helps mitigate some of the memory issues that might arise from
    using `toPandas()`. Despite this optimization, the conversion operation still
    results in all records in the Spark DataFrame being collected by the Driver, so
    you should only perform the conversion on a small subset of the original data.
    Thus, by making use of the PyArrow format and taking care to sample down datasets,
    you can still use all the open source visualizations that are compatible with
    standard Python to visualize your PySpark DataFrames in a notebook environment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the importance of using data visualization
    to convey meaning from complex datasets in a simple way, as well as to easily
    surface patterns among data to business users. Various strategies for visualizing
    data with Spark were introduced. You also learned how to use data visualizations
    with PySpark natively using Databricks notebooks. We also looked at techniques
    for using plain Python visualization libraries to visualize data with Spark DataFrames.
    A few of the prominent open source visualization libraries, such as Matplotlib,
    Seaborn, Plotly, and Altair, were introduced, along with practical examples of
    their usage and code samples. Finally, you learned about the pitfalls of using
    plain Python visualizations with PySpark, the need for PySpark conversion, and
    some strategies to overcome these issues.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will cover the topic of connecting various BI and SQL analysis
    tools to Spark, which will help you perform ad hoc data analysis and build complex
    operational and performance dashboards.
  prefs: []
  type: TYPE_NORMAL
