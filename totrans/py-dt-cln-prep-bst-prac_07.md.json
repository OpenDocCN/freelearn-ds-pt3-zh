["```py\npip install pymongo==4.8.0\npip install pyarrow\npip install confluent_kafka\npip install psycopg2-binary==2.9.9\n```", "```py\nOS=$(uname)\n```", "```py\nif [ \"$OS\" == \"Darwin\" ]; then\n    echo \"Detected macOS. Installing PostgreSQL via Homebrew...\"\n    brew update\n    brew install postgresql\n    brew services start postgresql\n```", "```py\nUnsupported OS. Please install PostgreSQL manually.\n```", "```py\npsql postgres << EOF\nDO \\$\\$\nBEGIN\n    IF NOT EXISTS (\n        SELECT FROM pg_catalog.pg_user\n        WHERE usename = 'the_great_coder'\n    ) THEN\n        CREATE USER the_great_coder\n        WITH PASSWORD 'the_great_coder_again';\n    END IF;\nEND\n\\$\\$;\nEOF\npsql postgres << EOF\nCREATE DATABASE learn_sql2 OWNER the_great_coder;\nEOF\npsql postgres << EOF\n-- Grant privileges to the user on the database\nGRANT ALL PRIVILEGES ON DATABASE learn_sql2 TO the_great_coder;\nEOF\n```", "```py\n    cd chapter7\n    setup_postgres.sh script, as shown here:\n\n    ```", "```py\n\n    ```", "```py\n    chmod +x setup_postgres.sh\n    ```", "```py\n    ./setup_postgres.sh\n    ```", "```py\nPostgreSQL setup completed. Database and user created.\n```", "```py\n    import pandas as pd\n    import psycopg2\n    from psycopg2 import sql\n    ```", "```py\n    def table_exists(cursor, table_name):\n        cursor.execute(\n            sql.SQL(\"SELECT EXISTS ( \\\n                    SELECT 1 FROM information_schema.tables \\\n                    WHERE table_name = %s)\"),\n            [table_name]\n        )\n        return cursor.fetchone()[0]\n    ```", "```py\n    def create_table(cursor, table_name):\n        cursor.execute(\n            sql.SQL(\"\"\"\n                CREATE TABLE {} (\n                    id SERIAL PRIMARY KEY,\n                    name VARCHAR(255),\n                    age INT\n                )\n            \"\"\").format(sql.Identifier(table_name))\n        )\n    ```", "```py\n    def insert_data(cursor, table_name, data):\n        cursor.executemany(\n            sql.SQL(\"INSERT INTO {} (name, age) \\\n                    VALUES (%s, %s)\"\n            ).format(sql.Identifier(table_name)),\n            data\n        )\n    ```", "```py\n    def print_table_data(cursor, table_name):\n        cursor.execute(\n            sql.SQL(\n                \"SELECT * FROM {}\"\n            ).format(sql.Identifier(table_name))\n        )\n        rows = cursor.fetchall()\n        for row in rows:\n            print(row)\n    ```", "```py\n    data = {\n        'name': ['Alice', 'Bob', 'Charlie'],\n        'age': [25, 30, 22]\n    }\n    df = pd.DataFrame(data)\n    ```", "```py\n    db_params = {\n        'dbname': 'learn_sql',\n        'user': 'the_great_coder',\n        'password': 'the_great_coder_again',\n        'host': 'localhost',\n        'port': '5432'\n    }\n    conn = psycopg2.connect(**db_params)\n    cursor = conn.cursor()\n    ```", "```py\n    table_name = 'example_table'\n    if not table_exists(cursor, table_name):\n        create_table(cursor, table_name)\n    insert_data(cursor, table_name, df.values.tolist())\n    conn.commit()\n    print_table_data(cursor, table_name)\n    cursor.close()\n    conn.close()\n    ```", "```py\npython 1.postgressql.py\n```", "```py\npsql -h localhost -U the_great_coder -d learn_sql\n```", "```py\n\\dt\n```", "```py\n    brew services start mongodb-community@7.0\n    ```", "```py\n    brew services list\n    mongodb-community@7.0 started maria.zervou ~/Library/LaunchAgents/h\n    ```", "```py\n    no_sql_db:\n\n    ```", "```py\n    db.createCollection(\"best_collection_ever\")\n    ```", "```py\n    { ok: 1 }\n    ```", "```py\n\n    ```", "```py\n    from pymongo import MongoClient\n    ```", "```py\n    mongo_params = {\n        'host': 'localhost',\n        'port': 27017,\n        'username': 'your_mongo_username',\n        'password': 'your_mongo_password',\n        'authSource': 'your_auth_database'\n    }\n    ```", "```py\n    def collection_exists(db, collection_name):\n        return collection_name in db.list_collection_names()\n    ```", "```py\n    def create_collection(db, collection_name):\n        db.create_collection(collection_name)\n    ```", "```py\n    def insert_data(collection, data):\n        collection.insert_many(data)\n    ```", "```py\n    documents = [\n        {'name': 'Alice', 'age': 25},\n        {'name': 'Bob', 'age': 30},\n        {'name': 'Charlie', 'age': 22}\n    ]\n    ```", "```py\n    db_name = ' no_sql_db'\n    collection_name = 'best_collection_ever'\n    client = MongoClient(**mongo_params)\n    db = client[db_name]\n    ```", "```py\n    if not collection_exists(db, collection_name):\n        create_collection(db, collection_name)\n    ```", "```py\n    collection = db[collection_name]\n    insert_data(collection, documents)\n    ```", "```py\n    client.close()\n    ```", "```py\n{'_id': ObjectId('66d833ec27bc08e40e0537b4'), 'name': 'Alice', 'age': 25}\n{'_id': ObjectId('66d833ec27bc08e40e0537b5'), 'name': 'Bob', 'age': 30}\n{'_id': ObjectId('66d833ec27bc08e40e0537b6'), 'name': 'Charlie', 'age': 22}\n```", "```py\n{\n    \"person\": {\n        \"name\": \"John Doe\",\n        \"age\": 30,\n        \"address\": {\n            \"city\": \"New York\",\n            \"country\": \"USA\"\n        },\n        \"email\": [\"john.doe@email.com\", \"john@example.com\"]\n    }\n}\n```", "```py\n\\x16\\x00\\x00\\x00\n{\n    \"person\": {\n        \"name\": \"John Doe\",\n        \"age\": 30,\n        \"address\": {\n            \"city\": \"New York\",\n            \"country\": \"USA\"\n        },\n        \"email\": [\"john.doe@email.com\", \"john@example.com\"]\n    }\n}\\x00\n```", "```py\n    from google.cloud import bigquery\n    from google.cloud.bigquery import SchemaField\n    ```", "```py\n    client = bigquery.Client(project='your_project_id')\n    ```", "```py\n    dataset_name = 'your_dataset'\n    table_name = 'your_table'\n    ```", "```py\n    dataset_ref = client.dataset(dataset_name)\n    table_ref = dataset_ref.table(table_name)\n    table_exists = client.get_table(\n        table_ref, retry=3, timeout=30, max_results=None\n    ) is not None\n    ```", "```py\n    schema = [\n        SchemaField('column1', 'STRING', mode='REQUIRED'),\n        SchemaField('column2', 'INTEGER', mode='NULLABLE'),\n        # Add more fields as needed\n    ]\n    ```", "```py\n    if not table_exists:\n        table = bigquery.Table(table_ref, schema=schema)\n        client.create_table(table)\n    ```", "```py\n    rows_to_insert = [\n        ('value1', 1),\n        ('value2', 2),\n        ('value3', 3)\n    ]\n    ```", "```py\n    data_to_insert = [dict(zip([field.name for field in schema], row)) for row in rows_to_insert]\n    ```", "```py\n    errors = client.insert_rows(table, data_to_insert)\n    ```", "```py\n    print(f\"Errors occurred during data insertion: {errors}\")\n    ```", "```py\n    client.close()\n    ```", "```py\n    import pandas as pd\n    import pyarrow.parquet as pq\n    import boto3\n    from io import BytesIO\n    ```", "```py\n    data = {'Name': ['Alice', 'Bob', 'Charlie'],\n            'Age': [25, 30, 22],\n            'City': ['New York', 'San Francisco', 'Los Angeles']}\n    df = pd.DataFrame(data)\n    ```", "```py\n    parquet_buffer = BytesIO()\n    pq.write_table(pq.Table.from_pandas(df), parquet_buffer)\n    ```", "```py\n    aws_access_key_id = 'YOUR_ACCESS_KEY_ID'\n    aws_secret_access_key = 'YOUR_SECRET_ACCESS_KEY'\n    bucket_name = 'your-s3-bucket'\n    file_key = 'example_data.parquet'  # The key (path) of the file in S3\n    ```", "```py\n    s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n    ```", "```py\n    s3.put_object(Body=parquet_buffer.getvalue(), Bucket=bucket_name, Key=file_key)\n    ```", "```py\n    version: '3'\n    services:\n        zookeeper:\n            image: confluentinc/cp-zookeeper:latest\n            ports:\n                - \"2181:2181\"\n            environment:\n                ZOOKEEPER_CLIENT_PORT: 2181\n    ```", "```py\n    kafka:\n        image: confluentinc/cp-kafka:latest\n        ports:\n            - \"9092:9092\"\n        environment:\n            KAFKA_BROKER_ID: 1\n            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n            KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n    ```", "```py\n    docker-compose up –d\n    ```", "```py\n    [+] Running 3/3\n    ✔ Network setup_default        Created         0.0s\n    ✔ Container setup-kafka-1      Started         0.7s\n    ✔ Container setup-zookeeper-1  Started         0.6s\n    ```", "```py\n    from pymongo import MongoClient\n    from confluent_kafka import Producer\n    import json\n    ```", "```py\n    mongo_client = MongoClient('mongodb://localhost:27017')\n    db = mongo_client['no_sql_db']\n    collection = db['best_collection_ever']\n    ```", "```py\n    kafka_config = {\n        'bootstrap.servers': 'localhost:9092'\n    }\n    producer = Producer(kafka_config)\n    ```", "```py\n    def delivery_report(err, msg):\n        if err is not None:\n            print(f'Message delivery failed: {err}')\n        else:\n            print(f'Message delivered to {msg.topic()} [{msg.partition()}]')\n    ```", "```py\n        message = json.dumps(document, default=str)\n        producer.produce('mongodb_topic',\n                         alue=message.encode('utf-8'),\n                         callback=delivery_report)\n        producer.poll(0)\n    ```", "```py\n    producer.flush()\n    ```", "```py\nMessage delivered to mongodb_topic [0]\nMessage delivered to mongodb_topic [0]\nMessage delivered to mongodb_topic [0]\nMessage delivered to mongodb_topic [0]\nMessage delivered to mongodb_topic [0]\nMessage delivered to mongodb_topic [0]\nMessage delivered to mongodb_topic [0]\nMessage delivered to mongodb_topic [0]\n```", "```py\n    from confluent_kafka import Consumer, KafkaError\n    import json\n    import time\n    ```", "```py\n    consumer_config = {\n        'bootstrap.servers': 'localhost:9092',\n        'group.id': 'mongodb_consumer_group',\n        'auto.offset.reset': 'earliest'\n    }\n    ```", "```py\n    consumer = Consumer(consumer_config)\n    consumer.subscribe(['mongodb_topic'])\n    ```", "```py\n    run_duration = 10 # For example, 10 seconds\n    start_time = time.time()\n    print(\"Starting consumer...\")\n    ```", "```py\n    while True:\n        if time.time() - start_time > run_duration:\n            print(\"Time limit reached, shutting down consumer.\")\n            break\n        msg = consumer.poll(1.0)\n        if msg is None:\n            continue\n        if msg.error():\n            if msg.error().code() == KafkaError._PARTITION_EOF:\n                print('Reached end of partition')\n            else:\n                print(f'Error: {msg.error()}')\n        else:\n            document = json.loads(msg.value().decode('utf-8'))\n            print(f'Received document: {document}')\n    consumer.close()\n    print(\"Consumer closed.\")\n    ```", "```py\nStarting consumer...\nReceived document: {'_id': '66d833ec27bc08e40e0537b4', 'name': 'Alice', 'age': 25}\nReceived document: {'_id': '66d833ec27bc08e40e0537b5', 'name': 'Bob', 'age': 30}\nReceived document: {'_id': '66d833ec27bc08e40e0537b6', 'name': 'Charlie', 'age': 22}\nReceived document: {'_id': '66d835aa1798a2275cecaba8', 'name': 'Alice', 'age': 25, 'email': 'alice@example.com'}\nReceived document: {'_id': '66d835aa1798a2275cecaba9', 'name': 'Bob', 'age': 30, 'address': '123 Main St'}\nReceived document: {'_id': '66d835aa1798a2275cecabaa', 'name': 'Charlie', 'age': 22, 'hobbies': ['reading', 'gaming']}\nReceived document: {'_id': '66d835aa1798a2275cecabab', 'name': 'David', 'age': 40, 'email': 'david@example.com', 'address': '456 Elm St', 'active': True}\nReceived document: {'_id': '66d835aa1798a2275cecabac', 'name': 'Eve', 'age': 35, 'email': 'eve@example.com', 'phone': '555-1234'}\n```", "```py\n{\n  \"type\": \"record\",\n  \"name\": \"SensorData\",\n  \"fields\": [\n    {\"name\": \"sensor_id\", \"type\": \"int\"},\n    {\"name\": \"timestamp\", \"type\": \"long\"},\n    {\"name\": \"value\", \"type\": \"float\"},\n    {\"name\": \"status\", \"type\": \"string\"}\n  ]\n}\n```", "```py\nsyntax = \"proto3\";\nmessage SensorData {\n  int32 sensor_id = 1;\n  int64 timestamp = 2;\n  float value = 3;\n  string status = 4;\n}\n```", "```py\n    import os\n    import pandas as pd\n    import pyarrow as pa\n    import pyarrow.parquet as pq\n    from datetime import datetime\n    ```", "```py\n    data = {\n        \"timestamp\": [\"2022-01-01\", \"2022-01-01\", \"2022-01-02\"],\n        \"value\": [10, 15, 12]\n    }\n    ```", "```py\n    df = pd.DataFrame(data)\n    ```", "```py\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    ```", "```py\n    base_path = \" path_to_write_data\"\n    ```", "```py\n    for timestamp, group in df.groupby(df[\"timestamp\"].dt.date):\n    ```", "```py\n    os.makedirs(base_path, exist_ok=True)\n    partition_path = os.path.join(base_path, str(timestamp))\n    table = pa.Table.from_pandas(group)\n    pq.write_table(table, partition_path)\n    ```", "```py\n    base_directory = \"/geo_data\"\n    os.makedirs(base_directory, exist_ok=True)\n    ```", "```py\n    geo_data = {\"region\": [\"North\", \"South\", \"East\"],\n                \"value\": [10, 15, 12]}\n    geo_df = pd.DataFrame(geo_data)\n    for region, group in geo_df.groupby(\"region\"):\n    ```", "```py\n    region_path = os.path.join(base_directory, region)\n    ```", "```py\n    table = pa.Table.from_pandas(group)\n    pq.write_table(table, region_path)\n    ```", "```py\n    base_directory = \"/hybrid_data\"\n    ```", "```py\n    hybrid_data = {\n        \"timestamp\": [\"2022-01-01\", \"2022-01-01\", \"2022-01-02\"],\n        \"region\": [\"North\", \"South\", \"East\"],\n        \"value\": [10, 15, 12]}\n    hybrid_df = pd.DataFrame(hybrid_data)\n    for (timestamp, region), group in hybrid_df.groupby(\n        [\"timestamp\", \"region\"]):\n    ```", "```py\n    timestamp_path = os.path.join(base_directory, str(timestamp))\n    os.makedirs(timestamp_path, exist_ok=True)\n    timestamp_region_path = os.path.join(\n        base_directory, str(timestamp), str(region))\n    ```", "```py\n    table = pa.Table.from_pandas(group)\n    pq.write_table(table, timestamp_region_path)\n    ```"]