- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Profiling – Understanding Data Structure, Quality, and Distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data profiling** refers to scrutinizing, understanding, and validating datasets
    to learn more about their underlying structure, patterns, and quality. It is a
    critical step in data management and ingestion as it can enhance data quality
    and accuracy and ensure compliance with regulatory standards. In this chapter,
    you will learn how to perform profiling with different tools and how to change
    your tactics as the data volume increases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will deep dive into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data profiling with the pandas profiler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data validation with Great Expectations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing Great Expectations and the pandas profiler – when to use what
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to profile big data volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need to install a Python interpreter that can be
    downloaded and installed using the instructions given here: [https://www.python.org/downloads/](https://www.python.org/downloads/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all the code for the chapter in the following GitHub repository:
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have never heard of **data profiling** before starting this chapter,
    it is a comprehensive process that involves analyzing and examining data from
    various sources to gain insights into the structure, quality, and overall characteristics
    of a dataset. Let’s start by describing the main goals of data profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying goals of data profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data profiling helps us understand the structure and quality of the data. As
    a result, we can get a better idea of the best way to organize the different datasets,
    identify potential data integration challenges, assess data quality, and identify
    and address issues that may affect the reliability and trustworthiness of the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s deep dive into the three main goals of data profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Data structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the main goals of data profiling is to understand the data’s structure.
    This entails examining the data types, formats, and relationships between different
    data fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of a simple table structure. Consider a table named `Employee`
    that stores information about employees in a company:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `EmployeeID` | `FirstName` | `LastName` | `Position` | `Department` | `Salary`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `John` | `Doe` | `Software Eng` | `IT` | `75000` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `Jane` | `Smith` | `Data Analyst` | `Analytics` | `60000` |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `Bob` | `Johnson` | `Project Manager` | `Project Management` | `85000`
    |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s break this table down:'
  prefs: []
  type: TYPE_NORMAL
- en: '`EmployeeID`: A unique identifier for each employee'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FirstName` and `LastName` are columns storing the first and last names of
    employees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Position`: The job title or position of the employee'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Department`: The department in which the employee works'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Salary`: The salary of the employee'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This table structure is organized into rows and columns. Each row represents
    a specific employee, and each column represents a different attribute or piece
    of information about the employee. The table structure allows for easy querying,
    filtering, and joining of data. The values in each column adhere to a specific
    data type (e.g., integer, string, etc.), and relationships between tables can
    be established using keys.
  prefs: []
  type: TYPE_NORMAL
- en: This is a simplified example, but in real-world scenarios, tables can have more
    columns and complex relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Data quality** involves evaluating the overall reliability and trustworthiness
    of the data. Through data profiling, we can identify various data quality problems,
    including duplicate records, incorrect or inconsistent values, missing values,
    and outliers. By quantifying these issues, organizations gain an understanding
    of the extent to which the data can be trusted and relied upon for analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Data distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding data distribution within each field or column is another key objective
    of data profiling. By analyzing data distribution, organizations gain insights
    into patterns, frequencies, and anomalies present in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine that we are working for an e-commerce company and we are collecting
    daily sales revenue data. By examining the distribution, we can identify trends
    in sales:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Distribution of daily sales revenue](img/B19801_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Distribution of daily sales revenue
  prefs: []
  type: TYPE_NORMAL
- en: In this histogram, we can see that the sales data follows a normal distribution,
    indicating that data near the mean is more frequent in occurrence than data far
    from the mean. In this way, we can understand the mean daily sales we can expect
    on a regular day.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what challenges data profiling can deal with, let’s see
    the different ways you can go about performing data profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis options – profiler versus manual
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When performing **exploratory data analysis** (**EDA**), there are different
    approaches you can take to understand your data, including conducting manual analysis
    or using a profiler.
  prefs: []
  type: TYPE_NORMAL
- en: '**Manual EDA** involves writing custom code or using general-purpose data analysis
    libraries (e.g., pandas in Python) to explore the data. It gives you more flexibility
    and control over the analysis process. You can customize the analysis based on
    your specific requirements and questions. Manual EDA allows for more in-depth
    exploration, including custom calculations, feature engineering, and advanced
    visualizations. It can be beneficial when dealing with complex data or when you
    have specific domain knowledge that you want to apply to the analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: A **profiler** is a tool or library specifically designed for analyzing and
    summarizing data. It automates many EDA tasks and provides quick insights into
    the data’s structure, summary statistics, missing values, data types, and distributions.
    It can save you time by automating repetitive tasks and providing a comprehensive
    overview of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see more in detail when to use what:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Manual EDA** | **Data Profiling** |'
  prefs: []
  type: TYPE_TB
- en: '| **Pros** | Flexibility to explore data based on specific needs | Automated
    process for quick insights |'
  prefs: []
  type: TYPE_TB
- en: '|  | In-depth understanding of the data through custom code | Consistent and
    standardized analysis across datasets |'
  prefs: []
  type: TYPE_TB
- en: '|  | Greater control over analysis techniques and visuals | Automated visualizations
    and summary statistics |'
  prefs: []
  type: TYPE_TB
- en: '|  | Identification of data quality issues and anomalies |'
  prefs: []
  type: TYPE_TB
- en: '| **Cons** | A time-consuming process requiring manual effort and repetitive
    | Limited customization options in generated reports |'
  prefs: []
  type: TYPE_TB
- en: '|  | Higher likelihood of human errors or biases | May not capture complex
    relationships or patterns |'
  prefs: []
  type: TYPE_TB
- en: '|  | Lack of standardization across analysts or teams | Less flexibility compared
    to manual analysis |'
  prefs: []
  type: TYPE_TB
- en: '|  | **Manual EDA** | **Data Profiling** |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Reliance on predefined algorithms and techniques |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Comparison between Manual EDA versus using a profiler tool
  prefs: []
  type: TYPE_NORMAL
- en: As the data grows, manual EDA becomes increasingly time-consuming and prone
    to human error, leading to inconsistent results and potentially overlooked data
    issues. Manual efforts also lack scalability and reproducibility, making it difficult
    to handle large datasets and collaborate effectively. That is why for the rest
    of the chapter, we will focus on how you can use different profiling tools to
    perform EDA on the data; however, in practice, a combined approach is often implemented.
    We will also provide some insights on how to change your tools given the size
    of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling data with pandas’ ydata_profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s see an example in Python that showcases data profiling using the `ProfileReport`
    class from the `ydata-profiling` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with installing a few libraries first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the following code example, we will use the `iris` dataset from the `seaborn`
    library, which is an open source dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to read the dataset and perform some initial EDA with *minimal
    code*!
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by importing the libraries and loading the dataset directly from
    its URL using the `read_csv()` function from pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `iris` dataset from the `seaborn` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we’ll perform data profiling by creating a profile report using the `ProfileReport()`
    function from `pandas_profiling`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We’ll generate an HTML report using the `to_file()` method, which exports the
    profiling results to an HTML file for easy sharing and further analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Optionally, we can embed the report in the notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Writing the report to a JSON file is optional, but a best practice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s explore the results of the profiler one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first section in the profiling report is the **Overview** section. In the
    **Overview** section, you have multiple tabs, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Overview of pandas profiler results](img/B19801_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Overview of pandas profiler results
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Overview** tab of the profiler results, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of variables**: The iris dataset has five variables – sepal length,
    sepal width, petal length, petal width, and species'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of observations**: The dataset contains 150 rows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing cells**: No missing values are present in the iris dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duplicate rows**: There is one duplicate row'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, we have the **Alerts** tab, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The Alerts tab of the ydata_profiling profiler](img/B19801_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – The Alerts tab of the ydata_profiling profiler
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `sepal_length`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Numerical feature profiling](img/B19801_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Numerical feature profiling
  prefs: []
  type: TYPE_NORMAL
- en: In the `sepal_length` part of the profile page, we can get more details about
    the specific numeric feature. A similar analysis is performed for all the other
    numeric features in the dataset. We can see that this feature has 35 different
    values and there are no missing values in the dataset for this feature. All the
    values are positive, which makes sense as this feature represents the length of
    the sepal and these values can range from 4.3 to 7.9\. The histogram shows the
    distribution for the feature.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Categorical feature profiling](img/B19801_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Categorical feature profiling
  prefs: []
  type: TYPE_NORMAL
- en: In the `species` part of the profile page, we can get more details about the
    specific categorical feature. A similar analysis is performed for all the other
    categorical features in the dataset. We can see that this feature has three different
    values (`sectosa`, `versicolor`, and `virginica`) and there are no missing values
    in the data for this feature. From the graph, we can see that we have the same
    number of records for each value of the feature (50).
  prefs: []
  type: TYPE_NORMAL
- en: Interactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another section in the profiling report is an **Interactions** section, which
    visualizes the relationships and potential interactions between different columns
    in the dataset. These charts are particularly useful for identifying potential
    correlations or dependencies between variables presented as *scatter plots*.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure shows the interactions between different variables. This
    chart can be created for every different combination of *numeric* variables. Let’s
    see an example for petal length and petal width.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Interaction chart between petal length and petal width](img/B19801_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Interaction chart between petal length and petal width
  prefs: []
  type: TYPE_NORMAL
- en: In the interaction chart, we can see how one variable influences the other.
    For example, as the petal length increases, the petal width increases as well.
    So, there is a linear relationship between the two. Since there is a strong interaction
    between these two variables, it is a good idea to deep dive into this interaction
    and examine in more detail the correlation plots for this pair.
  prefs: []
  type: TYPE_NORMAL
- en: Correlations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **correlation matrix** also depicts the interactions between variables where
    each cell represents the relationship between two columns. The cells are color-coded
    based on the strength or type of interaction detected between the corresponding
    column pairs. This helps in identifying how strongly two variables are related.
    Positive correlations are typically shown in one color (e.g., blue), while negative
    correlations are shown in another (e.g., red), with the intensity of the color
    indicating the strength of the correlation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, there might be a positive correlation between petal length and
    petal width, indicating that as the length of the petal increases, so does the
    width.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Correlation chart between numeric variables](img/B19801_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Correlation chart between numeric variables
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the chart, the darker the color blue is, the stronger the
    correlation between the variables. Petal length and petal width have more than
    0.75 positive correlation, showing that as one increases the other increases too.
    This is something we need to be aware of before proceeding to any modeling exercise
    as we may not need to keep both variables in the dataset, as having one can predict
    the other. For instance, if two variables are highly correlated, you might drop
    one of them or create a new feature that encapsulates the information from both.
    In some cases, removing highly correlated features can lead to faster training
    times for some machine learning algorithms, as the algorithm doesn’t need to deal
    with redundant information; also, we can simplify models, making them easier to
    understand and less prone to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**High correlation threshold**: Set a threshold for high correlation (e.g.,
    0.8 or 0.9). Variables with correlation coefficients above this threshold are
    considered highly correlated.'
  prefs: []
  type: TYPE_NORMAL
- en: Missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another key aspect of data quality is **missing values**. It refers to the absence
    of data in specific entries or variables within a dataset. They can occur for
    various reasons, such as data entry errors, sensor malfunctions, or errors in
    the ingestion process. If ignored, it can lead to biased and inaccurate results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the percentage of non-missing values for each column
    in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Percentage of non-missing values in the data](img/B19801_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Percentage of non-missing values in the data
  prefs: []
  type: TYPE_NORMAL
- en: In the present example, we can see that all the values in the dataset are complete
    and that all the features have 150 non-null values. So, that is good news for
    us and we can proceed to the next check.
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate rows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **duplicate row** in a dataset refers to a row that is identical to another
    row in every column. This means that for every column in the dataset, the values
    in the duplicate row are the same as those in the row it duplicates. Surfacing
    the presence and extent of duplicate rows helps us quickly identify potential
    data quality issues. As we’ve said, duplicate rows can arise due to various reasons,
    such as data integration problems, incorrect deduplication processes, or simply
    the nature of the data collection process.
  prefs: []
  type: TYPE_NORMAL
- en: In a profiling report, we can see the duplicate rows under **Most frequently
    occurring**, where a sample of the duplicates in the dataset is presented.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Duplicate rows in the data](img/B19801_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Duplicate rows in the data
  prefs: []
  type: TYPE_NORMAL
- en: In general, to find the duplicate rows, you need to identify key columns or
    a combination of columns that should be unique. Typically, we identify duplicates
    based on all columns in the dataset. If there are duplicates, it indicates we
    have duplicate rows. There are only two duplicated rows in the dataset in our
    example, as shown in the preceding figure.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage of the analysis, we are not handling duplicates since our focus
    is on understanding the data’s structure and characteristics. However, we will
    need to investigate the nature and source of these duplicates. Given that they
    represent a small proportion of the data, we could simply drop one of each pair
    of identical rows.
  prefs: []
  type: TYPE_NORMAL
- en: Sample dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sampling refers to the process of selecting a subset of data from a larger dataset
    instead of working with the entire dataset. In the EDA step, we usually work on
    a sample of data as it can provide initial insights and help in formulating hypotheses
    before committing resources to a full analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Sample dataset](img/B19801_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Sample dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve understood how to build data profiles with the `ydata_profiling`
    library, let’s have a closer look at a very popular but similar profiler called
    the **pandas** **data profiler**.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling high volumes of data with the pandas data profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pandas profiling** is a powerful library for generating detailed reports
    on datasets. However, for large datasets, the profiling process can become time-consuming
    and memory-intensive. When dealing with large datasets, you may need to consider
    a few strategies to optimize the profiling process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sampling**: Instead of profiling the entire dataset, you can take a random
    sample of the data to generate the report. This can significantly reduce the computation
    time and memory requirements while still providing a representative overview of
    the dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Subset selection**: If you’re interested in specific columns or subsets of
    the dataset, you can select only those columns for profiling. This reduces the
    computational load and narrows down the focus to the variables of interest:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Configuring profiler options**: The pandas profiling library provides several
    configuration options that allow you to fine-tune the profiling process. You can
    adjust these options to limit the depth of analysis, reduce computations, or skip
    certain time-consuming tasks if they are not necessary for your analysis:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Parallel processing**: If your system supports parallel processing, you can
    leverage it to speed up the profiling process. By distributing the workload across
    multiple cores or machines, you can potentially reduce the time required for profiling
    large datasets:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Incremental profiling**: If your dataset is too large to fit in memory, you
    can consider performing incremental profiling by splitting the data into smaller
    chunks and profiling them individually. You can then combine the profiling results
    to get an overview of the entire dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Some of these strategies aim to optimize the profiling process for large datasets,
    but they may result in some loss of granularity and detail compared to profiling
    the entire dataset. It’s essential to strike a balance between computational efficiency
    and the level of insight required for your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The next tool we are going to review is usually used in data-engineering heavy
    workflows as it provides a lot of flexibility, automation, and easy integration
    with other tools.
  prefs: []
  type: TYPE_NORMAL
- en: Data validation with the Great Expectations library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Great Expectations** is an open source Python library that facilitates data
    validation and documentation. It provides a framework for defining, managing,
    and executing data quality checks, making it easier to ensure data integrity and
    reliability throughout the data pipeline. Quality checks can be executed at different
    stages of the data life cycle, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Quality checks at different stages of the data life cycle](img/B19801_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Quality checks at different stages of the data life cycle
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss each of the touch points in the data life cycle where quality
    checks can be applied, as illustrated in the preceding figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data entry**: During data entry or data collection, checks are conducted
    to ensure that the data is accurately captured and recorded. This can involve
    verifying the format, range, and type of data, as well as performing validation
    checks against predefined rules or standards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation**: If data undergoes any transformations or conversions,
    such as data cleansing or data normalization, quality checks are performed to
    validate the accuracy of the transformed data. This helps ensure that the data
    retains its integrity throughout the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data integration**: When combining data from different sources or systems,
    data quality checks are necessary to identify any inconsistencies or discrepancies.
    This may involve checking for duplicate records, resolving missing or mismatched
    data, and reconciling any conflicting information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consumption**: Prior to performing any data analysis or generating reports,
    it is essential to run data quality checks to ensure the integrity of the data.
    This involves validating the data against predefined criteria, checking for outliers
    or anomalies, and verifying the overall quality of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Great Expectations allows you to set Expectations or rules for your data and
    then validate your data against these Expectations at any point in the data life
    cycle. *Figure 3**.12* illustrates the features of this library in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Great Expectations process from data collection to data quality
    results](img/B19801_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Great Expectations process from data collection to data quality
    results
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, there are three main steps to be aware of when working with
    Great Expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: Bringing/collecting all the data you want to apply Expectations on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing the Expectations and applying them to the different data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enjoying the benefits of clean, high-quality, and trustworthy data coming to
    life
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will go through how to configure Great Expectations
    to validate the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Great Expectations for your project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can validate your data against the defined Expectations using Great Expectations.
    The library provides functions to execute these validations and identify any inconsistencies
    or issues in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to install the `great-expectations` library for data profiling.
    You can use the following command to install the library in any IDE or a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This should install the library. We are going to use the same dataset as before
    so that we can showcase the difference between the tools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by setting up your project. Open your terminal, navigate to the
    desired location where you want to set up your new project, and then set up a
    new folder by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will go into the newly created folder by typing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create some folders for our data and the code we will need to
    use to run our example. Make sure you are in the `great_expectations` directory
    and run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should have created the following project structure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Great Expectations project initialization](img/B19801_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Great Expectations project initialization
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll run the following command to initialize a new Great Expectations
    project. Make sure you are in the `great_expectations` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.14 – Great Expectations project initialization](img/B19801_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Great Expectations project initialization
  prefs: []
  type: TYPE_NORMAL
- en: 'Hit *Y* when prompted and Great Expectations will go ahead and build a project
    structure for us in the `great_expectations` folder. The folder structure will
    look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Great Expectations folder structure](img/B19801_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Great Expectations folder structure
  prefs: []
  type: TYPE_NORMAL
- en: 'The folder structure of Great Expectations follows a specific convention to
    organize the configuration, Expectations, and data documentation related to your
    data pipeline. Let’s learn a little more about the structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/uncommitted/`: This directory contains all the uncommitted configuration
    and validation files. It is where you define and modify Expectations, validations,
    and data documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/checkpoints/`: This directory stores checkpoint files, which hold the sets
    of Expectations to be validated against specific data batches. Checkpoints are
    useful for running validations on specific portions or subsets of your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/expectations/`: This directory holds the Expectation Suites and Expectation
    files. An Expectation Suite is a collection of related Expectations, while Expectation
    files contain individual Expectations. You can create subdirectories within this
    folder to organize your Expectations based on a data source or data asset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/plugins/`: This folder is used to store custom plugins and extensions that
    you may develop to extend the functionality of Great Expectations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`great_expectations.yml`: This configuration file stores the deployment settings
    for Great Expectations. It contains essential information and parameters that
    define how Great Expectations operates within your deployment environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve set up and initialized a Great Expectations project, let’s create
    our first data source using Great Expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Create your first Great Expectations data source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have created the project structure to create our Expectations. The
    next step is to get some data to build Expectations on. In order to retrieve the
    dataset, go to the repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03/great_expectations/code](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter03/great_expectations/code),
    get the `1.data_set_up.py` script, and save it under the `great_expectations/code/`
    folder. Now, let’s write some test data to our folder by running the following
    Python script: `great_expectations/code/1.data_set_up.py`. Here’s what the script
    looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In your terminal, in the `great_expectations/code/` directory, execute the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This script performs a simple task of loading the `iris` dataset from a remote
    source, from the `seaborn` library’s GitHub repository, using the pandas library.
    It then saves this dataset to a local file named `iris_data.csv` in the `great_expectations/data`
    directory. Finally, it prints a confirmation message to indicate that the file
    has been successfully saved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to tell Great Expectations which data we want to use to build
    Great Expectations and where to find this data. In your terminal, execute the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Great Expectations file configuration](img/B19801_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Great Expectations file configuration
  prefs: []
  type: TYPE_NORMAL
- en: Follow the steps in the terminal, as shown in *Figure 3**.16*, making sure you
    choose option `1` as we are going to work with files and not SQL databases. Since
    our datasets are small enough to fit in memory, we can manipulate them with pandas.
    So, we’ll choose option `1` again. It will then prompt you to enter the path to
    the dataset file and since we saved our dataset in the `data` folder, enter `../data`.
  prefs: []
  type: TYPE_NORMAL
- en: After this step, Great Expectations automatically creates a Jupyter Notebook
    for us to explore! This notebook is stored at the `great_expectations/gx/uncommitted/datasource_new.ipynb`
    path and after the execution, you can just delete it if you don’t want to maintain
    unnecessary code. The purpose of this notebook is to help you create a pandas
    data source configuration and avoid any manual mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s open the notebook, update `datasource_name`, as shown in the following
    screenshot, and execute all the cells in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Great Expectations – customizing data source name](img/B19801_03_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Great Expectations – customizing data source name
  prefs: []
  type: TYPE_NORMAL
- en: We can give it any name we want at this point, but to be consistent with the
    incoming data, let’s name it `iris_data`. From now on, when we refer to `iris_data`,
    we know we are working on the Expectations for the `iris` data source we created
    in the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining consistent and clear naming between Expectation validations and
    data sources enhances readability, reduces errors, and simplifies maintenance
    and debugging!
  prefs: []
  type: TYPE_NORMAL
- en: Creating your first Great Expectations suite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have declared which data source we want to build an Expectation
    for, let’s go ahead and build the first suite for our `iris` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your terminal and execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: There are multiple ways to create your Expectation Suite, as you can see from
    the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Great Expectations – options for creating your suite](img/B19801_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – Great Expectations – options for creating your suite
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore each of the options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Manually, without interacting with a sample batch of data (default)`: This
    approach involves manually defining Expectations and configuring the suite without
    directly interacting with a sample batch of data. Expectations are typically based
    on your knowledge of the data and the specific requirements of your project. You
    define Expectations by specifying conditions, ranges, patterns, and other criteria
    that you expect the data to meet. This approach requires a thorough understanding
    of the data and domain knowledge to define accurate Expectations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Interactively, with a sample batch of data`: In this approach, you load a
    small representative batch of data into Great Expectations and use it to interactively
    define Expectations. This allows you to visually inspect the data, identify patterns,
    and explore various data statistics. You can iteratively build and refine Expectations
    based on your observations and understanding of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Automatically, using a Data Assistant`: Great Expectations provides a Data
    Assistant feature that automatically suggests Expectations based on the data.
    The Data Assistant analyzes the data and generates a set of suggested Expectations,
    which you can review and customize. This approach can be helpful when you have
    limited knowledge about the data or want to quickly generate a starting point
    for your Expectations. You can leverage the suggested Expectations as a foundation
    and further refine them based on your domain knowledge and specific requirements.
    The Data Assistant accelerates the process of building a suite by automating the
    initial Expectation generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, we will use the third option to build the suite automatically.
    This functionality is similar to the one that pandas profiling offers and we have
    explored it before in the *Profiling data with pandas’ ydata_profiling* section.
    So, go ahead and choose option `3` in the terminal, as shown in *Figure 3**.19*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Great Expectations – the Data Assistant option](img/B19801_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Great Expectations – the Data Assistant option
  prefs: []
  type: TYPE_NORMAL
- en: 'As a next step, you will be asked to choose which data source you want to create
    the suite for, which is the output from the previous step. Type `1` for the `iris_data`
    source we built before and then input the name of the new Expectation Suite: `expect_iris`.'
  prefs: []
  type: TYPE_NORMAL
- en: After executing the preceding command, a new notebook will be created automatically
    at `great_expectations/gx/uncommitted/edit_expect_iris.ipynb`. Open and read the
    notebook to understand the logic of the code; in summary, this notebook helps
    you choose columns and other factors from the data that you care about and lets
    the profiler create some Expectations for you that you can adjust later.
  prefs: []
  type: TYPE_NORMAL
- en: You have the option to create Expectations for all the columns in your dataset
    or a subset of them, as shown in *Figure 3**.20*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Great Expectations – columns included in the Suite](img/B19801_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – Great Expectations – columns included in the Suite
  prefs: []
  type: TYPE_NORMAL
- en: You can add all the column names for which you do *not* want to create Expectations
    in the `exclude_column_name` list. For any columns not added to the list, `great_expectations`
    will build Expectations for you. In our case, we want to create Expectations for
    all the columns, so we will leave the list empty, as shown in *Figure 3**.21*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Great Expectations – excluding columns from the Suite](img/B19801_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – Great Expectations – excluding columns from the Suite
  prefs: []
  type: TYPE_NORMAL
- en: Remember to execute all the cells in the notebook and let’s have a look at all
    the different Expectations built automatically by `great_expectations` for us.
  prefs: []
  type: TYPE_NORMAL
- en: Great Expectations Suite report
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s have a look at the profiling created by `great_expectations`. As you can
    see from *Figure 3**.22*, 52 Expectations were created and all have successfully
    passed. We can monitor the success percentage in the **Overview** tab to get a
    quick idea of how many Expectations pass every time a new data feed is coming
    to your pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Report overview statistics](img/B19801_03_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Report overview statistics
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a closer look at what Expectations we are validating our data against.
    The first thing to consider is across-the-table or table-level Expectations, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Table-level expectations](img/B19801_03_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – Table-level expectations
  prefs: []
  type: TYPE_NORMAL
- en: These Expectations check if the columns in the dataset match a given set of
    column names and if the dataset has the expected number of columns. It can be
    useful for ensuring all expected columns are present in the incoming data. If
    the incoming data does not contain all the columns shown in the Expectations,
    then the process will fail.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Column-level Expectations](img/B19801_03_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – Column-level Expectations
  prefs: []
  type: TYPE_NORMAL
- en: The next set of Expectations is created for each of the columns in the table
    and we will refer to them as feature Expectations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Feature-level Expectations](img/B19801_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.25 – Feature-level Expectations
  prefs: []
  type: TYPE_NORMAL
- en: These Expectations are checked separately for each column, and they can contain
    min and max values for the feature, whether we accept null values in the column
    or not, and many others. Remember, up to this point, all the Expectations were
    built automatically by us using a tool that does not understand the business context
    of the data. So, remember to check the Expectations and update them based on the
    business understanding of the data, as we will show in the next part.
  prefs: []
  type: TYPE_NORMAL
- en: Manually edit Great Expectations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While automatically generated Expectations provide a good starting point, they
    may not be sufficient for production-ready data validation. At this stage, it
    is important to further refine and customize the suite. You have the option to
    edit the suite manually or interactively. In general, manual editing is preferred
    when you have a clear understanding of the expected data properties and want to
    define Expectations efficiently and precisely. Since we’ve already done a basic
    automatic profiling of the data, we will proceed with the manual editing approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the terminal and execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You will be prompted to choose how you want to update the suite, either manually
    or interactively. We will proceed with it manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon providing the necessary input, Great Expectations opens the Jupyter Notebook
    available at the following location: `great_expectations/gx/uncommitted/edit_expect_iris.ipynb`.
    The notebook includes a comprehensive display of all the Expectations that were
    automatically generated. This allows you to review and examine the Expectations
    in detail, providing you with a clear overview of the validation rules that Great
    Expectations has inferred from the data. Have a look at all the Expectations we
    created and update them as necessary. In case you don’t want to use notebooks,
    you can go open the `great_expectations/gx/expectations/expect_iris.json` file
    and update it there.'
  prefs: []
  type: TYPE_NORMAL
- en: Checkpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have established a connection to our training dataset and defined
    our Expectations based on the training data. The next step is to apply these Expectations
    to our new stream of data in order to validate the new dataset and make sure it
    passes the checks. So, we need to create the connection between the Great Expectation
    suite and the new data to validate. We can do this with a checkpoint. To achieve
    this, we will first mock some test data to apply the Expectations. You can find
    the script at the following location: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter03/great_expectations/code/2.mock_test_dataset.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter03/great_expectations/code/2.mock_test_dataset.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Save it under the `great_expectations/code/` folder. The script takes care of
    saving the test file in the required location, which is `great_expectations/data/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In your terminal, in the `great_expectations/code/` directory, execute the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s have a closer look at the code we just executed, starting with the import
    statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `iris` dataset from the `seaborn` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We will do some transformations that will fail the Expectations and, in this
    case, we will update the `sepal_length` values to `60`, which will break our Expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also rename columns to showcase the change in column names and by extension
    to the expected schema of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We will write DataFrame that will work as a new data feed to test our Expectations
    against:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to create a checkpoint that will execute the Great Expectation
    Suite we created on the test dataset. To initiate the checkpoint, you can run
    the following command in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon execution, Great Expectations automatically generates a Jupyter Notebook
    that provides valuable information about the checkpoint here: `/great_expectations/gx/uncommitted/edit_checkpoint_expect_iris_ckpnt.ipynb`.
    This includes details about the data to which the checkpoint will be applied.
    Before executing the notebook, we need to update the file name and point it to
    the test file, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Uncomment the last two lines and then execute all the cells of the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The preceding notebook will apply the checkpoint to the new dataset and create
    a report of all the Expectations that have passed or failed. Let’s see the results!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Expectations results](img/B19801_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.26 – Expectations results
  prefs: []
  type: TYPE_NORMAL
- en: As expected, our Expectations failed on the column names and on the petal width
    as it cannot find the right column names because of schema changes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27 – Expectations failures because of schema changes](img/B19801_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.27 – Expectations failures because of schema changes
  prefs: []
  type: TYPE_NORMAL
- en: It also alerted us about the `sepal_length` variable as all the values are unexpected
    and outside of the accepted range of values it has seen so far!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28 – Expectations failures because of out-of-range values](img/B19801_03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 – Expectations failures because of out-of-range values
  prefs: []
  type: TYPE_NORMAL
- en: Can you see how many problems it could save us from? If this data was not checked
    and had been ingested, the subsequent processes and integration pipelines would
    fail, and a lot of work would be needed to try and identify which process failed
    and why. In our case, we know exactly where the problem started, and we have a
    clear idea of what we need to do to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Checkpoints are designed to be reusable, so you can run the same checkpoint
    configuration against multiple batches of data as they arrive. This allows you
    to consistently validate incoming data against the same set of Expectations. Additionally,
    checkpoints can be enhanced with various actions, such as sending notifications,
    updating data documentation (Data Docs), or triggering downstream processes based
    on the validation results.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you are impressed by the automation that Great Expectations provides
    and you wish to see how you can migrate all the pandas profiling you’ve been doing
    so far to Great Expectations Suites, then we’ve got you covered. Just keep reading.
  prefs: []
  type: TYPE_NORMAL
- en: Using pandas profiler to build your Great Expectations Suite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pandas profiler has a functionality that allows you to build Expectation
    Suites out of a pandas profiling exercise. Let’s look at the following example
    `great_expectations/code/3.with_pandas_profiler.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In this code snippet, we have taken our data and have created a pandas profiling.
    Then, we obtained an Expectation Suite from the report created previously. We
    can use this suite to further validate and check another batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have reviewed different profiling tools and how they work. The next
    step is to get a better understanding of when to use which tool and where to start.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Great Expectations and pandas profiler – when to use what
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas profiling and Great Expectations are both valuable tools for data profiling
    and analysis, but they have different strengths and use cases. Here’s a comparison
    between the two tools.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Pandas Profiler** | **Great Expectations** |'
  prefs: []
  type: TYPE_TB
- en: '| **Data Exploration** | Provides quick insights and exploratory data summaries
    | Focuses on data validation and documentation |'
  prefs: []
  type: TYPE_TB
- en: '| **Data Validation** | Limited data validation capabilities | Advanced data
    validation with explicit Expectations and rules |'
  prefs: []
  type: TYPE_TB
- en: '| **Customization** | Limited customization options | Extensive customization
    for defining Expectations and rules |'
  prefs: []
  type: TYPE_TB
- en: '| **Learning Curve** | Relatively easy to use | A steeper learning curve for
    defining Expectations and configuration |'
  prefs: []
  type: TYPE_TB
- en: '| **Scalability** | Suitable for small- to medium-scale data | Scalable for
    big data environments with distributed processing |'
  prefs: []
  type: TYPE_TB
- en: '| **Visualizations** | Generates interactive visualizations | Focuses more
    on validating and documenting data rather than visuals |'
  prefs: []
  type: TYPE_TB
- en: '| **Use Case** | Quick data exploration and initial insights | Data quality
    control and enforcing data consistency |'
  prefs: []
  type: TYPE_TB
- en: Table 3.2 – Great Expectations and pandas profiler comparison
  prefs: []
  type: TYPE_NORMAL
- en: Pandas profiling is well suited for quick data exploration and initial insights,
    while Great Expectations excels in data validation, documentation, and enforcing
    data quality rules. Pandas profiling is more beginner-friendly and provides immediate
    insights, while Great Expectations offers more advanced customization options
    and scalability for larger datasets. The choice between the two depends on the
    specific requirements of the project and the level of data quality control needed.
  prefs: []
  type: TYPE_NORMAL
- en: As the volume of data increases, we need to make sure that the choice of tools
    we’ve made can scale as well. Let’s have a look at how we can do this with Great
    Expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Great Expectations and big data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While Great Expectations can be used effectively with smaller datasets, it
    also provides mechanisms to address the challenges associated with scaling data
    validation and documentation for big data environments. Here are some considerations
    for scaling Great Expectations as data size increases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed processing frameworks**: Great Expectations integrates seamlessly
    with popular distributed processing frameworks, such as Apache Spark. By leveraging
    the parallel processing capabilities of these frameworks, Great Expectations can
    distribute the data validation workload across a cluster, allowing for efficient
    processing and scalability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partitioning and sampling**: Great Expectations simplifies the process of
    partitioning and sampling large datasets and enhancing performances and scalability.
    Unlike the manual partitioning required in tools such as pandas profiling, Great
    Expectations automates the creation of data subsets or partitions for profiling
    and validation. This feature allows you to validate specific subsets or partitions
    of the data, rather than processing the entire dataset at once. By automating
    the partitioning process, Great Expectations streamlines the profiling workflow
    and eliminates the need for manual chunk creation, saving time and effort.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incremental validation**: Instead of revalidating the entire big dataset
    every time, Great Expectations supports incremental validation. This means that
    as new data is ingested or processed, only the relevant portions or changes need
    to be validated, reducing the overall validation time and effort. This is a great
    trick to reduce the time it takes to check the whole data and optimize for cost!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Caching and memoization**: Great Expectations incorporates caching and memoization
    techniques to optimize performance when repeatedly executing the same validations.
    This can be particularly beneficial when working with large datasets, as previously
    computed results can be stored and reused, minimizing redundant computations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud-based infrastructure**: Leveraging cloud-based infrastructure and services
    can enhance scalability for Great Expectations. By leveraging cloud computing
    platforms, such as AWS or Azure, you can dynamically scale resources to handle
    increased data volumes and processing demands'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient data storage**: Choosing appropriate data storage technologies
    optimized for big data, such as distributed file systems or columnar databases,
    can improve the performance and scalability of Great Expectations. These technologies
    are designed to handle large-scale data efficiently and provide faster access
    for validation and processing tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While Great Expectations offers scalability options, the specific scalability
    measures may depend on the underlying infrastructure, data storage systems, and
    distributed processing frameworks employed in your big data environment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter detailed how data profiling is crucial for ensuring the quality,
    integrity, and reliability of datasets. The process involves in-depth analysis
    to understand the structure, patterns, and potential issues within the data. For
    effective profiling, tools such as pandas profiling and Great Expectations offer
    powerful solutions. Pandas profiling automates the generation of comprehensive
    reports, providing valuable insights into data characteristics. Great Expectations,
    on the other hand, facilitates the creation of data quality Expectations and allows
    for systematic validation. While these tools excel in smaller datasets, scaling
    profiling to big data requires specialized approaches. Learning the tips and tricks,
    such as data sampling and parallel processing, enables efficient and scalable
    profiling on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on how to clean and manipulate data to make
    sure it is in the right format to pass Expectations and be successfully ingested.
  prefs: []
  type: TYPE_NORMAL
