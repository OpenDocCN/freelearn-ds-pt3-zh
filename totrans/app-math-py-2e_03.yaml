- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calculus and Differential Equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss various topics related to calculus. Calculus
    is the branch of mathematics that concerns the processes of differentiation and
    integration. Geometrically, the derivative of a function represents the gradient
    of the curve of the function, and the integral of a function represents the area
    below the curve of the function. Of course, these characterizations only hold
    in certain circumstances, but they provide a reasonable foundation for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by looking at calculus for a simple class of functions: polynomials.
    In the first recipe, we’ll create a class that represents a polynomial and define
    methods that differentiate and integrate the polynomial. Polynomials are convenient
    because the derivative or integral of a polynomial is again a polynomial. Then,
    we’ll use the `SymPy` package to perform symbolic differentiation and integration
    on more general functions. After that, we’ll look at methods for solving equations
    using the SciPy package. Then, we’ll turn our attention to numerical integration
    (quadrature) and solving differential equations. We’ll use the SciPy package to
    solve **ordinary differential equations** (**ODEs**) and systems of ODEs, and
    then use a finite difference scheme to solve a simple partial differential equation.
    Finally, we’ll use the **Fast Fourier transform** (**FFT**) to process a noisy
    signal and filter out the noise.'
  prefs: []
  type: TYPE_NORMAL
- en: The content of this chapter will help you solve problems that involve calculus,
    such as computing the solution to differential equations, which frequently arise
    when describing the physical world. We’ll also dip into calculus later in [*Chapter
    9*](B19085_09.xhtml#_idTextAnchor360) when we discuss optimization. Several optimization
    algorithms require some kind of knowledge of derivatives, including the backpropagation
    commonly used in **machine** **learning** (**ML**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with polynomials and calculus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiating and integrating symbolically using SymPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving equations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating functions numerically using SciPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving simple differential equations numerically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving systems of differential equations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving partial differential equations numerically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using discrete Fourier transforms for signal processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic differentiation and calculus using JAX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving differential equations using JAX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to the scientific Python packages NumPy and SciPy, we also need
    the SymPy, JAX, and `diffrax` packages. These can be installed using your favorite
    package manager, such as `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are different options for the way you install JAX. Please see the official
    documentation for more details: [https://github.com/google/jax#installation](https://github.com/google/jax#installation).'
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found in the `Chapter 03` folder of the GitHub
    repository at [https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2003](https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2003).
  prefs: []
  type: TYPE_NORMAL
- en: Primer on calculus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Calculus is the study of functions and the way that they change. There are
    two major processes in calculus: **differentiation** and **integration**. Differentiation
    takes a function and produces a new function—called the **derivative**—that is
    the *best linear approximation* at each point. (You may see this described as
    the **gradient** of the function. Integration is often described as *anti-differentiation*—indeed,
    differentiating the integral of a function does give back the original function—but
    is also an abstract description of the area between the graph of the function
    and the ![](img/Formula_03_001.png) axis, taking into account where the curve
    is above or below the axis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Abstractly, the derivative of a function ![](img/Formula_03_002.png) at a point
    ![](img/Formula_03_003.png) is defined as a limit (which we won’t describe here)
    of the quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is because this small number ![](img/Formula_03_005.png) becomes smaller
    and smaller. This is the *difference in* ![](img/Formula_03_006.png) divided by
    the *difference in* ![](img/Formula_03_007.png), which is why the derivative is
    sometimes written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are numerous rules for differentiating common function forms: for example,
    in the first recipe, we will see that the derivative of ![](img/Formula_03_009.png)
    is ![](img/Formula_03_010.png). The derivative of the exponential function ![](img/Formula_03_011.png)
    is, again, ![](img/Formula_03_012.png); the derivative of ![](img/Formula_03_013.png)
    is ![](img/Formula_03_014.png); and the derivative of ![](img/Formula_03_0141.png)
    is ![](img/Formula_03_016.png). These basic building blocks can be combined using
    the *product rule* and *chain rule*, and by the fact that derivatives of sums
    are sums of derivatives, to differentiate more complex functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In its indefinite form, integration is the opposite process of differentiation.
    In its definite form, the integral of a function ![](img/Formula_03_017.png) is
    the (signed) area that lies between the curve of ![](img/Formula_03_018.png) and
    the ![](img/Formula_03_019.png) axis—note that this is a simple number, not a
    function. The indefinite integral of ![](img/Formula_03_020.png) is usually written
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the derivative of this function is ![](img/Formula_03_022.png). The definite
    integral of ![](img/Formula_03_023.png) between ![](img/Formula_03_024.png) and
    ![](img/Formula_03_025.png) is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_03_027.png) is the indefinite integral of ![](img/Formula_03_028.png).
    We can, of course, define the indefinite integral abstractly, using limits of
    sums approximating the area below the curve, and then define the indefinite integral
    in terms of this abstract quantity. (We won’t go into detail here.) The most important
    thing to remember with indefinite integrals is the **constant** **of integration**.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several easily deduced indefinite integrals (*anti-derivatives*)
    that we can quickly deduce: the integral of ![](img/Formula_03_029.png) is ![](img/Formula_03_030.png)
    (this is what we would differentiate to get ![](img/Formula_03_031.png)); the
    integral of ![](img/Formula_03_032.png) is ![](img/Formula_03_033.png); the integral
    of ![](img/Formula_03_034.png) is ![](img/Formula_03_035.png); and the integral
    of ![](img/Formula_03_036.png) is ![](img/Formula_03_037.png). In all these examples,
    ![](img/Formula_03_038.png) is the constant of integration. We can combine these
    simple rules to integrate more interesting functions by using the techniques of
    integration by parts or integration by substitution (and a host of much more involved
    techniques that we won’t mention here).'
  prefs: []
  type: TYPE_NORMAL
- en: Working with polynomials and calculus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Polynomials are among the simplest functions in mathematics and are defined
    as a sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_039.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_03_040.png) represents a placeholder to be substituted
    (an indeterminate), and ![](img/Formula_03_041.png) is a number. Since polynomials
    are simple, they provide an excellent means for a brief introduction to calculus.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will define a simple class that represents a polynomial and
    write methods for this class to perform differentiation and integration.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are no additional packages required for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps describe how to create a class representing a polynomial,
    and implement differentiation and integration methods for this class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by defining a simple class to represent a polynomial:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have defined a basic class for a polynomial, we can move on to
    implement the differentiation and integration operations for this `Polynomial`
    class to illustrate how these operations change polynomials. We start with differentiation.
    We generate new coefficients by multiplying each element in the current list of
    coefficients without the first element. We use this new list of coefficients to
    create a new `Polynomial` instance that is returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To implement the integration method, we need to create a new list of coefficients
    containing the new constant (converted to a float for consistency) given by the
    argument. We then add to this list of coefficients the old coefficients divided
    by their new position in the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, to make sure these methods work as expected, we should test these
    two methods with a simple case. We can check this using a very simple polynomial,
    such as ![](img/Formula_03_042.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The derivative here is given the coefficients ![](img/Formula_03_043.png) and
    ![](img/Formula_03_044.png), which corresponds to the polynomial ![](img/Formula_03_045.png),
    which is indeed the derivative of ![](img/Formula_03_046.png). Similarly, the
    coefficients of the integral correspond to the polynomial ![](img/Formula_03_047.png),
    which is also correct (with constant of integration ![](img/Formula_03_048.png)).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polynomials offer an easy introduction to the basic operations of calculus,
    but it isn’t so easy to construct Python classes for other general classes of
    functions. That being said, polynomials are extremely useful because they are
    well understood and, perhaps more importantly, calculus for polynomials is very
    easy. For powers of a variable ![](img/Formula_03_049.png), the rule for differentiation
    is to multiply by the power and reduce the power by 1 so that ![](img/Formula_03_050.png)
    becomes ![](img/Formula_03_051.png), so our rule for differentiating a polynomial
    is to simply multiply each coefficient by its position and remove the first coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Integration is more complex since the integral of a function is not unique.
    We can add any constant to an integral and obtain a second integral. For powers
    of a variable ![](img/Formula_03_052.png), the rule for integration is to increase
    the power by 1 and divide by the new power so that ![](img/Formula_03_053.png)
    becomes ![](img/Formula_03_054.png). Therefore, to integrate a polynomial, we
    increase each power of ![](img/Formula_03_055.png) by 1 and divide the corresponding
    coefficient by the new power. Hence, our rule is to first insert the new constant
    of integration as the first element and divide each of the existing coefficients
    by its new position in the list.
  prefs: []
  type: TYPE_NORMAL
- en: The `Polynomial` class that we defined in the recipe is rather simplistic but
    represents the core idea. A polynomial is uniquely determined by its coefficients,
    which we can store as a list of numerical values. Differentiation and integration
    are operations that we can perform on this list of coefficients. We include a
    simple `__repr__` method to help with the display of `Polynomial` objects, and
    a `__call__` method to facilitate evaluation at specific numerical values. This
    is mostly to demonstrate the way that a polynomial is evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomials are useful for solving certain problems that involve evaluating
    a computationally expensive function. For such problems, we can sometimes use
    some kind of polynomial interpolation, where we *fit* a polynomial to another
    function, and then use the properties of polynomials to help solve the original
    problem. Evaluating a polynomial is much *cheaper* than the original function,
    so this can lead to dramatic improvements in speed. This usually comes at the
    cost of some accuracy. For example, Simpson’s rule for approximating the area
    under a curve approximates the curve by quadratic polynomials over intervals defined
    by three consecutive mesh points. The area below each quadratic polynomial can
    be calculated easily by integration.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polynomials have many more important roles in computational programming than
    simply demonstrating the effect of differentiation and integration. For this reason,
    a much richer `Polynomial` class is provided in the `numpy.polynomial` NumPy package.
    The NumPy `Polynomial` class, and the various derived subclasses, are useful in
    all kinds of numerical problems and support arithmetic operations as well as other
    methods. In particular, there are methods for fitting polynomials to collections
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy also provides classes, derived from `Polynomial`, that represent various
    special kinds of polynomials. For example, the `Legendre` class represents a specific
    system of polynomials called *Legendre polynomials*. Legendre polynomials are
    defined for ![](img/Formula_03_056.png) satisfying ![](img/Formula_03_057.png)
    and form an orthogonal system, which is important for applications such as numerical
    integration and the **finite element method** for solving partial differential
    equations. Legendre polynomials are defined using a recursive relation. We define
    them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_058.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, for each ![](img/Formula_03_059.png), we define the ![](img/Formula_03_060.png)th
    Legendre polynomial to satisfy the recurrence relation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_061.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are several other so-called *orthogonal (systems of) polynomials*, including
    *Laguerre polynomials*, *Chebyshev polynomials*, and *Hermite polynomials*.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Calculus is certainly well documented in mathematical texts, and there are many
    textbooks that cover the basic methods all the way to the deep theory. Orthogonal
    systems of polynomials are also well documented among numerical analysis texts.
  prefs: []
  type: TYPE_NORMAL
- en: Differentiating and integrating symbolically using SymPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At some point, you may have to differentiate a function that is not a simple
    polynomial, and you may need to do this in some kind of automated fashion—for
    example, if you are writing software for education. The Python scientific stack
    includes a package called SymPy, which allows us to create and manipulate symbolic
    mathematical expressions within Python. In particular, SymPy can perform differentiation
    and integration of symbolic functions, just like a mathematician.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create a symbolic function and then differentiate and
    integrate this function using the SymPy library.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike some of the other scientific Python packages, there does not seem to
    be a standard alias under which SymPy is imported in the literature. Instead,
    the documentation uses a star import at several points, which is not in line with
    the *PEP8* style guide. This is possibly to make the mathematical expressions
    more natural. We will simply import the module under its name `sympy`, to avoid
    any confusion with the `scipy` package’s standard abbreviation, `sp` (which is
    the natural choice for `sympy` too):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In this recipe, we will define a symbolic expression that represents the following
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_062.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Then, we will see how to symbolically differentiate and integrate this function.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Differentiating and integrating symbolically (as you would do by hand) is very
    easy using the SymPy package. Follow these steps to see how it is done:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once SymPy is imported, we define the symbols that will appear in our expressions.
    This is a Python object that has no particular value, just like a mathematical
    variable, but can be used in formulas and expressions to represent many different
    values simultaneously. For this recipe, we need only define a symbol for ![](img/Formula_03_063.png),
    since we will only require constant (literal) symbols and functions in addition
    to this. We use the `symbols` routine from `sympy` to define a new symbol. To
    keep the notation simple, we will name this new symbol `x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The symbols defined using the `symbols` function support all of the arithmetic
    operations, so we can construct the expression directly using the symbol `x` we
    just defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can use the symbolic calculus capabilities of SymPy to compute the
    derivative of `f`—that is, differentiate `f`. We do this using the `diff` routine
    in `sympy`, which differentiates a symbolic expression with respect to a specified
    symbol and returns an expression for the derivative. This is often not expressed
    in its simplest form, so we use the `sympy.simplify` routine to simplify the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can check whether the result of the symbolic differentiation using SymPy
    is correct, compared to the derivative computed by hand using the product rule,
    defined as a SymPy expression, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'SymPy equality tests whether two expressions are equal, but not whether they
    are symbolically equivalent. Therefore, we must first simplify the difference
    of the two statements we wish to test and test for equality to `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can integrate the derivative `fp` using SymPy by using the `integrate` function
    and check that this is again equal to `f`. It is a good idea to also provide the
    symbol with which the integration is to be performed by providing it as the second
    optional argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the result of integrating the derivative `fp` gives back the
    original function `f` (although we are technically missing the constant of integration
    ![](img/Formula_03_064.png)).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SymPy defines various classes to represent certain kinds of expressions. For
    example, symbols, represented by the `Symbol` class, are examples of *atomic expressions*.
    Expressions are built up in a similar way to how Python builds an abstract syntax
    tree from source code. These expression objects can then be manipulated using
    methods and standard arithmetic operations.
  prefs: []
  type: TYPE_NORMAL
- en: SymPy also defines standard mathematical functions that can operate on `Symbol`
    objects to create symbolic expressions. The most important feature is the ability
    to perform symbolic calculus—rather than the numerical calculus that we explore
    in the remainder of this chapter—and give exact (sometimes called *analytic*)
    solutions to calculus problems.
  prefs: []
  type: TYPE_NORMAL
- en: The `diff` routine from the SymPy package performs differentiation on these
    symbolic expressions. The result of this routine is usually not in its simplest
    form, which is why we used the `simplify` routine to simplify the derivative in
    the recipe. The `integrate` routine symbolically integrates a `scipy` expression
    with respect to a given symbol. (The `diff` routine also accepts a symbol argument
    that specifies the symbol for differentiating against.) This returns an expression
    whose derivative is the original expression. This routine does not add a constant
    of integration, which is good practice when doing integrals by hand.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SymPy can do much more than simple algebra and calculus. There are submodules
    for various areas of mathematics, such as number theory, geometry, and other discrete
    mathematics (such as combinatorics).
  prefs: []
  type: TYPE_NORMAL
- en: 'SymPy expressions (and functions) can be built into Python functions that can
    be applied to NumPy arrays. This is done using the `lambdify` routine from the
    `sympy.utilities` module. This converts a SymPy expression to a numerical expression
    that uses the NumPy equivalents of the SymPy standard functions to evaluate the
    expressions numerically. The result is similar to defining a Python Lambda, hence
    the name. For example, we could convert the function and derivative from this
    recipe into Python functions using this routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lambdify` routine takes two arguments. The first is the variables to be
    provided, `x` in the previous code block, and the second is the expression to
    be evaluated when this function is called. For example, we can evaluate the lambdified
    SymPy expressions defined previously as if they were ordinary Python functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can even evaluate these lambdified expressions on NumPy arrays (as usual,
    with NumPy imported as `np`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `lambdify` routine uses the Python `exec` routine to execute the code, so
    it should not be used with unsanitized input.
  prefs: []
  type: TYPE_NORMAL
- en: Solving equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many mathematical problems eventually reduce to solving an equation of the form
    ![](img/Formula_03_065.png), where ![](img/Formula_03_066.png) is a function of
    a single variable. Here, we try to find a value of ![](img/Formula_03_067.png)
    for which the equation holds. The values of ![](img/Formula_03_068.png) for which
    the equation holds are sometimes called *roots* of the equation. There are numerous
    algorithms for finding solutions to equations of this form. In this recipe, we
    will use the Newton-Raphson and secant methods to solve an equation of the form
    ![](img/Formula_03_069.png).
  prefs: []
  type: TYPE_NORMAL
- en: The Newton-Raphson method (Newton’s method) and the secant method are good,
    standard root-finding algorithms that can be applied in almost any situation.
    These are *iterative methods* that start with an approximation of the root and
    iteratively improve this approximation until it lies within a given tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate these techniques, we will use the function from the *Differentiating
    and integrating symbolically using SymPy* recipe, defined by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_070.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is defined for all real values of ![](img/Formula_03_071.png) and has exactly
    two roots, one at ![](img/Formula_03_072.png) and one at ![](img/Formula_03_073.png).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SciPy package contains routines for solving equations (among many other
    things). The root-finding routines can be found in the `optimize` module from
    the `scipy` package. As usual, we import NumPy as `np`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `optimize` package provides routines for numerical root finding. The following
    instructions describe how to use the `newton` routine from this module:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `optimize` module is not listed in the `scipy` namespace, so you must import
    it separately:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must define this function and its derivative in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The derivative of this function was computed in the previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For both the Newton-Raphson and secant methods, we use the `newton` routine
    from `optimize`. Both the secant method and the Newton-Raphson method require
    the function as the first argument and the first approximation, `x0`, as the second
    argument. To use the Newton-Raphson method, we must provide the derivative of
    ![](img/Formula_03_074.png), using the `fprime` keyword argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To use the secant method, only the function is needed, but we must provide
    the first two approximations for the root; the second is provided as the `x1`
    keyword argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Neither the Newton-Raphson nor the secant method is guaranteed to converge to
    a root. It is perfectly possible that the iterates of the method will simply cycle
    through a number of points (periodicity) or fluctuate wildly (chaos).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Newton-Raphson method for a function ![](img/Formula_03_075.png) with derivative
    ![](img/Formula_03_076.png) and initial approximation ![](img/Formula_03_077.png)
    is defined iteratively using this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_078.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For each integer, ![](img/Formula_03_079.png). Geometrically, this formula arises
    by considering the direction in which the gradient is negative (so, the function
    is decreasing) if ![](img/Formula_03_080.png) or positive (so, the function is
    increasing) if ![](img/Formula_03_081.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The secant method is based on the Newton-Raphson method, but replaces the first
    derivative with the following approximation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_082.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When ![](img/Formula_03_083.png) is sufficiently small, which occurs if the
    method is converging, then this is a good approximation. The price paid for not
    requiring the derivative of the function ![](img/Formula_03_084.png) is that we
    require an additional initial guess to start the method. The formula for the method
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_085.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Generally speaking, if either method is given an initial guess (guesses for
    the secant method) that is sufficiently close to a root, then the method will
    converge to that root. The Newton-Raphson method can also fail if the derivative
    is zero at one of the iterations, in which case the formula is not well defined.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The methods mentioned in this recipe are general-purpose methods, but there
    are others that may be faster or more accurate in some circumstances. Broadly
    speaking, root-finding algorithms fall into two categories: algorithms that use
    information about the function’s gradient at each iterate (Newton-Raphson, secant,
    Halley) and algorithms that require bounds on the location of a root (bisection
    method, Regula-Falsi, Brent). The algorithms discussed so far are of the first
    kind, and while generally quite fast, they may fail to converge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second kind of algorithm is those for which a root is known to exist within
    a specified interval ![](img/Formula_03_086.png). We can check whether a root
    lies within such an interval by checking that ![](img/Formula_03_087.png) and
    ![](img/Formula_03_088.png) have different signs—that is, one of ![](img/Formula_03_089.png)
    or ![](img/Formula_03_090.png) is true (provided, of course, that the function
    is *continuous*, which tends to be the case in practice). The most basic algorithm
    of this kind is the bisection algorithm, which repeatedly bisects the interval
    until a sufficiently good approximation to the root is found. The basic premise
    is to split the interval between ![](img/Formula_03_091.png) and ![](img/Formula_03_092.png)
    at the mid-point and select the interval in which the function changes sign. The
    algorithm repeats until the interval is very small. The following is a rudimentary
    implementation of this algorithm in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This method is guaranteed to converge since, at each step, the distance ![](img/Formula_03_093.png)
    is halved. However, it is possible that the method will require more iterations
    than Newton-Raphson or the secant method. A version of the bisection method can
    also be found in `optimize`. This version is implemented in C and is considerably
    more efficient than the version presented here, but the bisection method is not
    the fastest method in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Brent’s method is an improvement on the bisection method and is available in
    the `optimize` module as `brentq`. It uses a combination of bisection and interpolation
    to quickly find the root of an equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note that the techniques that involve bracketing (bisection,
    regula-falsi, Brent) cannot be used to find the root functions of a complex variable,
    whereas those techniques that do not use bracketing (Newton, secant, Halley) can.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, some equations are not quite of the form ![](img/Formula_03_094.png)
    but can still be solved using these techniques. This is done by rearranging the
    equation so that it is of the required form (renaming functions if necessary).
    This is usually not too difficult and simply requires moving any terms on the
    right-hand side over to the left-hand side. For example, if you wish to find the
    fixed points of a function—that is, when ![](img/Formula_03_095.png)—then we would
    apply the method to the related function given by ![](img/Formula_03_096.png).
  prefs: []
  type: TYPE_NORMAL
- en: Integrating functions numerically using SciPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Integration can be interpreted as the area that lies between a curve and the
    ![](img/Formula_03_097.png) axis, signed according to whether this area is above
    or below the axis. Some integrals cannot be computed directly using symbolic means,
    and instead, have to be approximated numerically. One classic example of this
    is the Gaussian error function, which was mentioned in the *Understanding basic
    mathematical functions* section in [*Chapter 1*](B19085_01.xhtml#_idTextAnchor014),
    *An Introduction to Basic Packages, Functions, and Concepts*. This is defined
    by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_098.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Furthermore, the integral that appears here cannot be evaluated symbolically.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to use numerical integration routines in the
    SciPy package to compute the integral of a function.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use the `scipy.integrate` module, which contains several routines for computing
    numerical integrals. We also import the NumPy library as `np`. We import this
    module as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps describe how to numerically integrate a function using
    SciPy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate the integral that appears in the definition of the error function
    at the value ![](img/Formula_03_099.png). For this, we need to define the integrand
    (the function that appears inside the integral) in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are two main routines in `scipy.integrate` for performing numerical integration
    (quadrature) that can be used. The first is the `quad` function, which uses QUADPACK
    to perform the integration, and the second is `quadrature`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `quad` routine is a general-purpose integration tool. It expects three
    arguments, which are the function to be integrated (`erf_integrand`), the lower
    limit (`-1.0`), and the upper limit (`1.0`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first returned value is the value of the integral, and the second is an
    estimate of the error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeating the computation with the `quadrature` routine, we get the following.
    The arguments are the same as for the `quad` routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is the same format as the code, with the value of the integral and
    then an estimate of the error. Notice that the error is larger for the `quadrature`
    routine. This is a result of the method terminating once the estimated error falls
    below a given tolerance, which can be modified when the routine is called.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most numerical integration techniques follow the same basic procedure. First,
    we choose points ![](img/Formula_03_100.png) for ![](img/Formula_03_101.png) in
    the region of integration, and then use these values and the values ![](img/Formula_03_102.png)
    to approximate the integral. For example, with the trapezium rule, we approximate
    the integral with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_103.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/Formula_03_104.png) and ![](img/Formula_03_105.png) is the (common)
    difference between adjacent ![](img/Formula_03_106.png) values, including the
    endpoints ![](img/Formula_03_107.png) and ![](img/Formula_03_108.png). This can
    be implemented in Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The algorithms used by `quad` and `quadrature` are far more sophisticated than
    this. Using this function to approximate the integral of `erf_integrand` using
    `trapezium` with 500 steps yields a result of 1.4936463036001209, which agrees
    with the approximations from the `quad` and `quadrature` routines to five decimal
    places.
  prefs: []
  type: TYPE_NORMAL
- en: The `quadrature` routine uses a fixed tolerance Gaussian quadrature, whereas
    the `quad` routine uses an adaptive algorithm implemented in the Fortran library
    QUADPACK routines. Timing both routines, we find that the `quad` routine is approximately
    five times faster than the `quadrature` routine for the problem described in the
    recipe. The `quad` routine executes in approximately 27 µs, averaging over 1 million
    executions, while the `quadrature` routine executes in approximately 134 µs. (Your
    results may differ depending on your system.) Generally speaking, you should use
    the quad method since it is both faster and more accurate unless you need the
    Gaussian quadrature implemented by `quadrature`.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The routines mentioned in this section require the integrand function to be
    known, which is not always the case. Instead, it might be the case that we know
    a number of pairs ![](img/Formula_03_109.png) with ![](img/Formula_03_110.png),
    but we don’t know the function ![](img/Formula_03_111.png) to evaluate at additional
    points. In this case, we can use one of the sampling quadrature techniques from
    `scipy.integrate`. If the number of known points is very large and all points
    are equally spaced, we can use Romberg integration for a good approximation of
    the integral. For this, we use the `romb` routine. Otherwise, we can use a variant
    of the trapezium rule (as shown previously) using the `trapz` routine, or Simpson’s
    rule using the `simps` routine.
  prefs: []
  type: TYPE_NORMAL
- en: Solving simple differential equations numerically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Differential equations arise in situations where a quantity evolves, usually
    over time, according to a given relationship. They are extremely common in engineering
    and physics, and appear quite naturally. One of the classic examples of a (very
    simple) differential equation is the law of cooling devised by Newton. The temperature
    of a body cools at a rate proportional to the current temperature. Mathematically,
    this means that we can write the derivative of the temperature ![](img/Formula_03_112.png)
    of the body at time ![](img/Formula_03_113.png) using the following differential
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_114.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/Formula_03_115.png) is a positive constant that determines the
    rate of cooling. This differential equation can be solved *analytically* by first
    *separating the variables* and then integrating and rearranging them. After performing
    this procedure, we obtain the general solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_116.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_03_117.png) is the initial temperature.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will solve a simple ODE numerically using the `solve_ivp`
    routine from SciPy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will demonstrate the technique for solving a differential equation numerically
    in Python using the cooling equation described previously since we can compute
    the true solution in this case. We take the initial temperature to be ![](img/Formula_03_118.png)
    and ![](img/Formula_03_119.png). Let’s also find the solution for ![](img/Formula_03_120.png)
    values between ![](img/Formula_03_121.png) and ![](img/Formula_03_122.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the NumPy library imported as `np`, the Matplotlib
    `pyplot` interface imported as `plt`, and the `integrate` module imported from
    SciPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'A general (first-order) differential equation has the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_123.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/Formula_03_124.png) is some function of ![](img/Formula_03_125.png)
    (the independent variable) and ![](img/Formula_03_126.png) (the dependent variable).
    In this formula, ![](img/Formula_03_127.png) is the dependent variable and ![](img/Formula_03_128.png).
    The routines for solving differential equations in the SciPy package require the
    function ![](img/Formula_03_129.png) and an initial value ![](img/Formula_03_130.png)and
    the range of ![](img/Formula_03_131.png) values where we need to compute the solution.
    To get started, we need to define our function ![](img/Formula_03_132.png) in
    Python and create a variables ![](img/Formula_03_133.png) and ![](img/Formula_03_134.png)
    range ready to be supplied to the SciPy routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to define the initial condition from which the solution should
    be found. For technical reasons, the initial ![](img/Formula_03_136.png) values
    must be specified as a one-dimensional NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Since, in this case, we already know the true solution, we can also define
    this in Python ready to compare to the numerical solution that we will compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see how to solve this initial value problem using SciPy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to solve a differential equation numerically and plot the
    solution along with the error:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `solve_ivp` routine from the `integrate` module in SciPy to solve
    the differential equation numerically. We add a parameter for the maximum step
    size, with a value of `0.1`, so that the solution is computed at a reasonable
    number of points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we extract the solution values from the `sol` object returned from the
    `solve_ivp` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we plot the solution on a set of axes, as follows. Since we are also
    going to plot the approximation error on the same figure, we create two subplots
    using the `subplots` routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This plots the solution on a set of axes displayed on the left-hand side of
    *Figure 3**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we need to compute the true solution at the points that we obtained
    from the `solve_ivp` routine, and then calculate the absolute value of the difference
    between the true and approximated solutions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, on the right-hand side of *Figure 3**.1*, we plot the error in the
    approximation with a logarithmic scale on the ![](img/Formula_03_137.png) axis.
    We can then plot this on the right-hand side with a logarithmic scale ![](img/Formula_03_1371.png)
    axis using the `semilogy` plot command, as we saw in [*Chapter 2*](B19085_02.xhtml#_idTextAnchor036),
    *Mathematical Plotting* *with Matplotlib*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The left-hand plot in *Figure 3**.1* shows decreasing temperature over time,
    while the right-hand plot shows that the error increases as we move away from
    the known value given by the initial condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Plot of the numerical solution to the cooling equation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Plot of the numerical solution to the cooling equation
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the right-hand side plot is on a logarithmic scale and, while the
    rate of increase looks fairly dramatic, the values involved are very small (of
    order ![](img/Formula_03_138.png)).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most methods for solving differential equations are *time-stepping* methods.
    The pairs ![](img/Formula_03_139.png) are generated by taking small ![](img/Formula_03_140.png)
    steps and approximating the value of the function ![](img/Formula_03_141.png).
    This is perhaps best illustrated by Euler’s method, which is the most basic time-stepping
    method. Fixing a small step size ![](img/Formula_03_142.png), we form the approximation
    at the ![](img/Formula_03_143.png)th step using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_144.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We start from the known initial value ![](img/Formula_03_145.png). We can easily
    write a Python routine that performs Euler’s method as follows (there are, of
    course, many different ways to implement Euler’s method; this is a very simple
    example).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we set up the method by creating lists that will store the ![](img/Formula_03_146.png)
    values and ![](img/Formula_03_150.png) values that we will return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Euler’s method continues until we hit the end of the ![](img/Formula_03_146.png)
    range. Here, we use a `while` loop to accomplish this. The body of the loop is
    very simple; we first increment a counter `i`, and then append the new ![](img/Formula_03_146.png)
    and ![](img/Formula_03_150.png) values to their respective lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The method used by the `solve_ivp` routine, by default, is the **Runge-Kutta-Fehlberg**
    (**RKF45**) method, which has the ability to adapt the step size to ensure that
    the error in the approximation stays within a given tolerance. This routine expects
    three positional arguments: the function ![](img/Formula_03_151.png), the ![](img/Formula_03_152.png)
    range on which the solution should be found, and the initial ![](img/Formula_03_153.png)
    value (![](img/Formula_03_154.png) in our example). Optional arguments can be
    provided to change the solver, the number of points to compute, and several other
    settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The function passed to the `solve_ivp` routine must have two arguments, as
    in the general differential equation described in the *Getting ready* section.
    The function can have additional arguments, which can be provided using the `args`
    keyword for the `solve_ivp` routine, but these must be positioned after the two
    necessary arguments. Comparing the `euler` routine we defined earlier to the `solve_ivp`
    routine, both with a (maximum) step size of 0.1, we find that the maximum true
    error between the `solve_ivp` solution is in the order of 10-11, whereas the `euler`
    solution only manages an error of 0.19\. The `euler` routine is working, but the
    step size is much too large to overcome the accumulating error. For comparison,
    *Figure 3**.2* is a plot of the solution and error as produced by Euler’s method.
    Compare *Figure 3**.2* to *Figure 3**.1*. Note the scale on the error plot is
    dramatically different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Plot of solution and error using Euler’s method with step size
    0.1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Plot of solution and error using Euler’s method with step size
    0.1
  prefs: []
  type: TYPE_NORMAL
- en: The `solve_ivp` routine returns a solution object that stores information about
    the solution that has been computed. Most important here are the `t` and `y` attributes,
    which contain the ![](img/Formula_03_155.png) values on which the solution ![](img/Formula_03_156.png)
    is computed and the solution ![](img/Formula_03_157.png) itself. We used these
    values to plot the solution we computed. The ![](img/Formula_03_157.png) values
    are stored in a NumPy array of shape `(n, N)`, where `n` is the number of components
    of the equation (here, 1), and `N` is the number of points computed. The ![](img/Formula_03_157.png)
    values held in `sol` are stored in a two-dimensional array, which in this case
    has one row and many columns. We use the slice `y[0, :]` to extract this first
    row as a one-dimensional array that can be used to plot the solution in step 4.
  prefs: []
  type: TYPE_NORMAL
- en: We use a logarithmically scaled ![](img/Formula_03_157.png) axis to plot the
    error because what is interesting there is the order of magnitude. Plotting it
    on a non-scaled ![](img/Formula_03_157.png) axis would give a line that is very
    close to the ![](img/Formula_03_162.png) axis, which doesn’t show the increase
    in the error as we move through the ![](img/Formula_03_163.png) values. The logarithmically
    scaled ![](img/Formula_03_157.png) axis shows this increase clearly.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `solve_ivp` routine is a convenient interface for a number of solvers for
    differential equations, the default being the RKF45 method. The different solvers
    have different strengths, but the RKF45 method is a good general-purpose solver.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more detailed instructions on how to add subplots to a figure in Matplotlib,
    see the *Adding subplots* recipe from [*Chapter 2*](B19085_02.xhtml#_idTextAnchor036),
    *Mathematical Plotting* *with Matplotlib*.
  prefs: []
  type: TYPE_NORMAL
- en: Solving systems of differential equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Differential equations sometimes occur in systems consisting of two or more
    interlinked differential equations. A classic example is a simple model of the
    populations of competing species. This is a simple model of competing species
    labeled ![](img/Formula_03_165.png) (the prey) and ![](img/Formula_03_166.png)
    (the predators) given by the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_167.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first equation dictates the growth of the prey species ![](img/Formula_03_169.png),
    which, without any predators, would be exponential growth. The second equation
    dictates the growth of the predator species ![](img/Formula_03_170.png), which,
    without any prey, would be exponential decay. Of course, these two equations are
    *coupled*; each population change depends on both populations. The predators consume
    the prey at a rate proportional to the product of their two populations, and the
    predators grow at a rate proportional to the relative abundance of prey (again
    the product of the two populations).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will analyze a simple system of differential equations and
    use the SciPy `integrate` module to obtain approximate solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tools for solving a system of differential equations using Python are the
    same as those for solving a single equation. We again use the `solve_ivp` routine
    from the `integrate` module in SciPy. However, this will only give us a predicted
    evolution over time with given starting populations. For this reason, we will
    also employ some plotting tools from Matplotlib to better understand the evolution.
    As usual, the NumPy library is imported as `np` and the Matplotlib `pyplot` interface
    is imported as `plt`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps walk us through how to analyze a simple system of differential
    equations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first task is to define a function that holds the system of equations.
    This function needs to take two arguments as for a single equation, except the
    dependent variable ![](img/Formula_03_171.png) (in the notation from the *Solving
    simple differential equations numerically* recipe) will now be an array with as
    many elements as there are equations. Here, there will be two elements. The function
    we need for the example system in this recipe is defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we have defined the system in Python, we can use the `quiver` routine from
    Matplotlib to produce a plot that will describe how the populations will evolve—given
    by the equations—at numerous starting populations. We first set up a grid of points
    on which we will plot this evolution. It is a good idea to choose a relatively
    small number of points for the `quiver` routine; otherwise, it becomes difficult
    to see details in the plot. For this example, we plot the population values between
    0 and 100:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we compute the values of the system at each of these pairs. Notice that
    neither equation in the system is time-dependent (they are autonomous); the time
    variable ![](img/Formula_03_172.png) is unimportant in the calculation. We supply
    the value `0` for the ![](img/Formula_03_172.png) argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `dp` and `dw` variables now hold the *direction* in which the population
    of ![](img/Formula_03_174.png) and ![](img/Formula_03_175.png) will evolve, respectively,
    if we started at each point in our grid. We can plot these directions together
    using the `quiver` routine from `matplotlib.pyplot`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plotting the result of these commands now gives us *Figure 3**.3*, which gives
    a *global* picture of how solutions evolve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – A quiver plot showing the population dynamics of two competing
    species'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – A quiver plot showing the population dynamics of two competing
    species
  prefs: []
  type: TYPE_NORMAL
- en: To understand a solution more specifically, we need some initial conditions
    so that we can use the `solve_ivp` routine described in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have two equations, our initial conditions will have two values. (Recall
    in the *Solving simple differential equations numerically* recipe, we saw that
    the initial condition provided to `solve_ivp` needs to be a NumPy array.) Let’s
    consider the initial values ![](img/Formula_03_176.png) and ![](img/Formula_03_177.png).
    We define these in a NumPy array, being careful to place them in the correct order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can use `solve_ivp` from the `scipy.integrate` module. We need to provide
    the `max_step` keyword argument to make sure that we have enough points in the
    solution to give a smooth solution curve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot this solution on our existing figure to show how this specific solution
    relates to the direction plot we have already produced. We also plot the initial
    condition at the same time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result of this is shown in *Figure 3**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Solution trajectory plotted over a quiver plot showing the general
    behavior'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Solution trajectory plotted over a quiver plot showing the general
    behavior
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the trajectory plotted is a closed loop. This means that the
    populations have a stable and periodic relationship. This is a common pattern
    when solving these equations.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The method used for a system of ODEs is exactly the same as for a single ODE.
    We start by writing the system of equations as a single vector differential equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_178.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This can then be solved using a time-stepping method as though ![](img/Formula_03_179.png)
    were a simple scalar value.
  prefs: []
  type: TYPE_NORMAL
- en: The technique of plotting the directional arrows on a plane using the `quiver`
    routine is a quick and easy way of learning how a system might evolve from a given
    state. The derivative of a function represents the gradient of the curve ![](img/Formula_03_180.png),
    and so a differential equation describes the gradient of the solution function
    at position ![](img/Formula_03_179.png) and time ![](img/Formula_03_182.png).
    A system of equations describes the gradient of separate solution functions at
    a given position ![](img/Formula_03_179.png) and time ![](img/Formula_03_182.png).
    Of course, the position is now a two-dimensional point, so when we plot the gradient
    at a point, we represent this as an arrow that starts at the point, in the direction
    of the gradient. The length of the arrow represents the size of the gradient;
    the longer the arrow, the *faster* the solution curve will move in that direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we plot the solution trajectory on top of this direction field, we can
    see that the curve (starting at the point) follows the direction indicated by
    the arrows. The behavior shown by the solution trajectory is a *limit cycle*,
    where the solution for each variable is periodic as the two species’ populations
    grow or decline. This description of the behavior is perhaps clearer if we plot
    each population against time, as seen in *Figure 3**.5*. What is not immediately
    obvious from *Figure 3**.4* is that the solution trajectory loops around several
    times, but this is clearly shown in *Figure 3**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Plots of populations P and W against time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Plots of populations P and W against time
  prefs: []
  type: TYPE_NORMAL
- en: The periodic relationship described previously is clear in *Figure 3**.5*. Moreover,
    we can see the lag between the peak populations of the two species. Species ![](img/Formula_03_185.png)
    experiences peak population approximately 0.3 time periods after species ![](img/Formula_03_186.png).
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The technique of analyzing a system of ODEs by plotting variables against one
    another, starting at various initial conditions, is called *phase space (plane)
    analysis*. In this recipe, we used the `quiver` plotting routine to quickly generate
    an approximation of the phase plane for a system of differential equations. By
    analyzing the phase plane of a system of differential equations, we can identify
    different local and global characteristics of the solution, such as limit cycles.
  prefs: []
  type: TYPE_NORMAL
- en: Solving partial differential equations numerically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Partial differential equations are differential equations that involve *partial
    derivatives* of functions in two or more variables, as opposed to *ordinary derivatives*
    in only a single variable. Partial differential equations are a vast topic, and
    could easily fill a series of books. A typical example of a partial differential
    equation is the (one-dimensional) *heat equation*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_187.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/Formula_03_188.png) is a positive constant and ![](img/Formula_03_189.png)
    is a function. The solution to this partial differential equation is a function
    ![](img/Formula_03_190.png), which represents the temperature of a rod, occupying
    the ![](img/Formula_03_191.png) range ![](img/Formula_03_192.png), at a given
    time ![](img/Formula_03_193.png). To keep things simple, we will take ![](img/Formula_03_194.png),
    which amounts to saying that no heating/cooling is applied to the system, ![](img/Formula_03_195.png),
    and ![](img/Formula_03_196.png). In practice, we can rescale the problem to fix
    the constant ![](img/Formula_03_197.png), so this is not a restrictive problem.
    In this example, we will use boundary conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_198.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These are equivalent to saying that the ends of the rod are held at the constant
    temperature 0\. We will also use the initial temperature profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This initial temperature profile describes a smooth curve between the values
    of 0 and 2 that peaks at a value of 3, which might be the result of heating the
    rod at the center to a temperature of 3.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to use a method called *finite differences*, where we divide the
    rod into a number of equal segments and the time range into a number of discrete
    steps. We then compute approximations for the solution at each of the segments
    and each time step.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use finite differences to solve a simple partial differential
    equation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the NumPy and Matplotlib packages, imported as
    `np` and `plt`, as usual. We also need to import the `mplot3d` module from `mpl_toolkits`
    since we will be producing a 3D plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: We will also need some modules from the SciPy package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following steps, we work through solving the heat equation using finite
    differences:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first create variables that represent the physical constraints of the
    system—the extent of the bar and the value of ![](img/Formula_03_200.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We first divide the ![](img/Formula_03_201.png) range into ![](img/Formula_03_202.png)
    equal intervals—we take ![](img/Formula_03_203.png) for this example—using ![](img/Formula_03_204.png)
    points. We can use the `linspace` routine from NumPy to generate these points.
    We also need the common length of each interval ![](img/Formula_03_205.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to set up the steps in the time direction. We take a slightly
    different approach here; we set the time step size ![](img/Formula_03_206.png)
    and the number of steps (implicitly making the assumption that we start at time
    0):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In order for the method to behave properly, we must have the following formula:![](img/Formula_03_207.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Otherwise, the system can become unstable. We store the left-hand side of this
    in a variable for use in step 5, and use an assertion to check that this inequality
    holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can construct a matrix that holds the coefficients from the finite
    difference scheme. To do this, we use the `diags` routine from the `scipy.sparse`
    module to create a sparse, tridiagonal matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a blank matrix that will hold the solution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to add the initial profile to the first row. The best way to do this
    is to create a function that holds the initial profile and store the result of
    evaluating this function on the `x` array in the matrix `u` that we just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can simply loop through each step, computing the next row of the matrix
    `u` by multiplying `A` and the previous row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, to visualize the solution we have just computed, we can plot the solution
    as a surface using Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this is the surface plot shown in *Figure 3**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6  -Numerical solution of the heat equation over the range'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 -Numerical solution of the heat equation over the range ![](img/Formula_03_208.png)
  prefs: []
  type: TYPE_NORMAL
- en: Along the ![](img/Formula_03_209.png) axis, we can see that the overall shape
    is similar to the shape of the initial profile but becomes flatter as time progresses.
    Along the ![](img/Formula_03_210.png) axis, the surface exhibits the exponential
    decay that is characteristic of cooling systems.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The finite difference method works by replacing each of the derivatives with
    a simple fraction that involves only the value of the function, which we can estimate.
    To implement this method, we first break down the spatial range and time range
    into a number of discrete intervals, separated by mesh points. This process is
    called *discretization*. Then, we use the differential equation and the initial
    conditions and boundary conditions to form successive approximations, in a manner
    very similar to the time-stepping methods used by the `solve_ivp` routine in the
    *Solving simple differential equations* *numerically* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve a partial differential equation such as the heat equation,
    we need at least three pieces of information. Usually, for the heat equation,
    this will come in the form of *boundary conditions* for the spatial dimension,
    which tell us what the behavior is at either end of the rod, and *initial conditions*
    for the time dimension, which is the initial temperature profile over the rod.
  prefs: []
  type: TYPE_NORMAL
- en: 'The finite difference scheme described previously is usually referred to as
    the **forward time cen** (**FTCS**) scheme, since we use the *forward finite difference*
    to estimate the time derivative and the *central finite difference* to estimate
    the (second-order) spatial derivative. The formula for the first-order finite
    difference approximation is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_211.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the second-order approximation is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting these approximations into the heat equation, and using the approximation
    ![](img/Formula_03_213.png) for the value of ![](img/Formula_03_214.png) after
    ![](img/Formula_03_215.png) time steps at the ![](img/Formula_03_216.png) spatial
    point, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_217.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be rearranged to obtain the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_218.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Roughly speaking, this equation says that the next temperature at a given point
    depends on the surrounding temperatures at the previous time. This also shows
    why the condition on the `r` value is necessary; if the condition does not hold,
    the middle term on the right-hand side will be negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write this system of equations in matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_219.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_03_220.png) is a vector containing the approximation ![](img/Formula_03_221.png)
    and matrix ![](img/Formula_03_222.png), which was defined in step 4\. This matrix
    is tridiagonal, which means the nonzero entries appear on, or adjacent to, the
    leading diagonal. We use the `diag` routine from the SciPy `sparse` module, which
    is a utility for defining these kinds of matrices. This is very similar to the
    process described in the *Solving equations* recipe of this chapter. The first
    and last rows of this matrix have zeros, except in the top left and bottom right,
    respectively, that represent the (non-changing) boundary conditions. The other
    rows have coefficients that are given by the finite difference approximations
    for the derivatives on either side of the differential equation. We first create
    diagonal entries and entries above and below the diagonal, and then we use the
    `diags` routine to create a sparse matrix. The matrix should have ![](img/Formula_03_223.png)
    rows and columns, to match the number of mesh points, and we set the data type
    as double-precision floats and **compressed sparse row** (**CSR**) format.
  prefs: []
  type: TYPE_NORMAL
- en: The initial profile gives us the vector ![](img/Formula_03_224.png), and from
    this first point, we can compute each subsequent time step by simply performing
    a matrix multiplication, as we saw in step 7.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The method we describe here is rather crude since the approximation can become
    unstable, as we mentioned, if the relative sizes of time steps and spatial steps
    are not carefully controlled. This method is *explicit* since each time step is
    computed explicitly using only information from the previous time step. There
    are also *implicit* methods, which give a system of equations that can be solved
    to obtain the next time step. Different schemes have different characteristics
    in terms of the stability of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the function ![](img/Formula_03_225.png) is not 0, we can easily accommodate
    this change by using the following assignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_226.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the function is suitably vectorized to make this formula valid. In terms
    of the code used to solve the problem, we need only include the definition of
    the function and then change the loop of the solution, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: Physically, this function represents an external heat source (or sink) at each
    point along the rod. This may change over time, which is why, in general, the
    function should have both ![](img/Formula_03_227.png) and ![](img/Formula_03_228.png)
    as arguments (though they need not both be used).
  prefs: []
  type: TYPE_NORMAL
- en: 'The boundary conditions we gave in this example represent the ends of the rod
    being kept at a constant temperature of 0\. These kinds of boundary conditions
    are sometimes called *Dirichlet* boundary conditions. There are also *Neumann*
    boundary conditions, where the derivative of the function ![](img/Formula_03_229.png)
    is given at the boundary. For example, we might have been given the following
    boundary conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_230.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This could be interpreted physically as the ends of the rod being insulated
    so that heat cannot escape through the endpoints. For such boundary conditions,
    we need to modify the matrix ![](img/Formula_03_231.png) slightly, but otherwise,
    the method remains the same. Indeed, inserting an imaginary ![](img/Formula_03_232.png)
    value to the left of the boundary and using the backward finite difference at
    the left-hand boundary (![](img/Formula_03_233.png)), we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_234.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this in the second-order finite difference approximation, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_235.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This means that the first row of our matrix should contain ![](img/Formula_03_236.png),
    then ![](img/Formula_03_237.png), followed by ![](img/Formula_03_238.png). Using
    a similar computation for the right-hand limit gives a similar final row of the
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: For more complex problems involving partial differential equations, it is probably
    more appropriate to use a *finite elements* solver. Finite element methods use
    a more sophisticated approach for computing solutions than partial differential
    equations, which are generally more flexible than the finite difference method
    we saw in this recipe. However, this comes at the cost of requiring more setup
    that relies on more advanced mathematical theory. On the other hand, there is
    a Python package for solving partial differential equations using finite element
    methods such as **FEniCS** ([fenicsproject.org](https://fenicsproject.org)). The
    advantage of using packages such as FEniCS is that they are usually tuned for
    performance, which is important when solving complex problems with high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The FEniCS documentation gives a good introduction to the finite element method
    and a number of examples of using the package to solve various classic partial
    differential equations. A more comprehensive introduction to the method and the
    theory is given in the following book: *Johnson, C.* (*2009*). *Numerical solution
    of partial differential equations by the finite element method*. *Mineola, N.Y.:*
    *Dover Publications*.'
  prefs: []
  type: TYPE_NORMAL
- en: For more details on how to produce three-dimensional surface plots using Matplotlib,
    see the *Surface and contour plots* recipe from [*Chapter 2*](B19085_02.xhtml#_idTextAnchor036),
    *Mathematical Plotting* *with Matplotlib*.
  prefs: []
  type: TYPE_NORMAL
- en: Using discrete Fourier transforms for signal processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most useful tools coming from calculus is the **Fourier transform**
    (**FT**). Roughly speaking, the FT changes the representation, in a reversible
    way, of certain functions. This change of representation is particularly useful
    in dealing with signals represented as a function of time. In this instance, the
    FT takes the signal and represents it as a function of frequency; we might describe
    this as transforming from signal space to frequency space. This can be used to
    identify the frequencies present in a signal for identification and other processing.
    In practice, we will usually have a discrete sample of a signal, so we have to
    use the **discrete Fourier transform** (**DFT**) to perform this kind of analysis.
    Fortunately, there is a computationally efficient algorithm, called the FFT, for
    applying the DFT to a sample.
  prefs: []
  type: TYPE_NORMAL
- en: We will follow a common process for filtering a noisy signal using the FFT.
    The first step is to apply the FFT and use the data to compute the **power spectral
    density** (**PSD**) of the signal. Then, we identify peaks and filter out the
    frequencies that do not contribute a sufficiently large amount to the signal.
    Then, we apply the inverse FFT to obtain the filtered signal.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we use the FFT to analyze a sample of a signal and identify
    the frequencies present and clean the noise from the signal.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will only need the NumPy and Matplotlib packages imported
    as `np` and `plt`, as usual. We will need an instance of the default random number
    generator, created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see how to use the DFT.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these instructions to use the FFT to process a noisy signal:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a function that will generate our underlying signal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create our sample signal by adding some Gaussian noise to the underlying
    signal. We also create an array that holds the true signal at the sample ![](img/Formula_03_239.png)
    values for convenience later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the `fft` module from NumPy to compute DFTs. We import this from NumPy
    before we start our analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To see what the noisy signal looks like, we can plot the sample signal points
    with the true signal superimposed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot created here is shown in *Figure 3**.7*. As we can see, the noisy
    signal does not bear much resemblance to the true signal (shown with the dashed
    line):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Noisy signal sample with true signal superimposed'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Noisy signal sample with true signal superimposed
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will use the DFT to extract the frequencies that are present in the
    sample signal. The `fft` routine in the `fft` module performs the DFT:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `fft` module provides a routine for constructing the appropriate frequency
    values called `fftfreq`. For convenience, we also generate an array containing
    the integers at which the positive frequencies occur:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, compute the PSD of the signal, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can plot the PSD of the signal for the positive frequencies and use
    this plot to identify frequencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result can be seen in *Figure 3**.8*. We can see in this diagram that there
    are spikes at roughly **4** and **7**, which are the frequencies of the signal
    that we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – PSD of a signal generated using the FFT'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – PSD of a signal generated using the FFT
  prefs: []
  type: TYPE_NORMAL
- en: 'We can identify these two frequencies to try to reconstruct the true signal
    from the noisy sample. All of the minor peaks that appear are not larger than
    2,000, so we can use this as a cut-off value for the filter. Let’s now extract
    from the list of all positive frequency indices the (hopefully 2) indices that
    correspond to the peaks above 2,000 in the PSD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a new, clean spectrum that contains only the frequencies that
    we have extracted from the noisy signal. We do this by creating an array that
    contains only 0, and then copying the value of `spectrum` from those indices that
    correspond to the filtered frequencies and the negatives thereof:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we use the inverse FFT (using the `ifft` routine) to transform this clean
    spectrum back to the time domain of the original sample. We take the real part
    using the `real` routine from NumPy to eliminate the erroneous imaginary parts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we plot this filtered signal over the true signal and compare the
    results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result of step 11 is shown in *Figure 3**.9*. We can see that the filtered
    signal closely matches the true signal, except for some small discrepancies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Filtered signal generated using FFTs superimposed over the true
    signal'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 – Filtered signal generated using FFTs superimposed over the true
    signal
  prefs: []
  type: TYPE_NORMAL
- en: We can see in *Figure 3**.9* that the filtered signal (dashed line) fits fairly
    closely over the true signal (lighter solid line). It captures most (but not all)
    of the oscillations of the true signal.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The FT of a function ![](img/Formula_03_240.png) is given by the following
    integral:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_241.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The DFT is given by the following integral:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_242.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the ![](img/Formula_03_243.png) values are the sample values as complex
    numbers. The DFT can be computed using the preceding formula, but in practice,
    this is not efficient. Computing using this formula is ![](img/Formula_03_244.png).
    The FFT algorithm improves the complexity to ![](img/Formula_03_245.png), which
    is significantly better. The book *Numerical Recipes* (full bibliographic details
    given in the *Further reading* section) gives a very good description of the FFT
    algorithm and the DFT.
  prefs: []
  type: TYPE_NORMAL
- en: We will apply the DFT to a sample generated from a known signal (with known
    frequency modes) so that we can see the connection between the results we obtain
    and the original signal. To keep this signal simple, we created a signal that
    has only two frequency components with values 4 and 7\. From this signal, we generated
    a sample that we analyzed. Because of the way the FFT works, it is best if the
    sample has a size that is a power of 2; if this isn’t the case, we can pad the
    sample with zero elements to make this the case. We add some Gaussian noise to
    the sample signal, which takes the form of a normally distributed random number.
  prefs: []
  type: TYPE_NORMAL
- en: The array returned by the `fft` routine contains ![](img/Formula_03_246.png)
    elements, where ![](img/Formula_03_247.png) is the sample size. The element that
    index 0 corresponds to is the 0 frequency or DC shift. The next ![](img/Formula_03_248.png)elements
    are the values corresponding to the positive frequencies, and the final ![](img/Formula_03_249.png)
    elements are the values corresponding to the negative frequencies. The actual
    values of the frequencies are determined by the number of sampled points ![](img/Formula_03_250.png)
    and the sample spacing, which, in this example, is stored in `sample_d`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PSD at the frequency ![](img/Formula_03_251.png) is given by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_252.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_03_253.png) represents the FT of the signal at frequency
    ![](img/Formula_03_254.png). The PSD measures the contribution of each frequency
    to the overall signal, which is why we see peaks at approximately 4 and 7\. Since
    Python indexing allows us to use negative indices for elements starting from the
    end of the sequence, we can use the positive index array to get both the positive
    and negative frequency elements from `spectrum`.
  prefs: []
  type: TYPE_NORMAL
- en: In step 9, we identified the indices of the two frequencies that peak above
    2,000 on the plot. The frequencies that correspond to these indices are 3.984375
    and 6.97265625, which are not exactly equal to 4 and 7 but are very close. The
    reason for this discrepancy is the fact that we have sampled a continuous signal
    using a finite number of points. (Using more points will, of course, yield better
    approximations.)
  prefs: []
  type: TYPE_NORMAL
- en: In step 11, we took the real part of the data returned from the inverse FFT.
    This is because, technically speaking, the FFT works with complex data. Since
    our data contained only real data, we expect that this new signal should also
    contain only real data. However, there will be some small errors made, meaning
    that the results are not totally real. We can remedy this by taking the real part
    of the inverse FFT. This is appropriate because we can see that the imaginary
    parts are very small.
  prefs: []
  type: TYPE_NORMAL
- en: We can see in *Figure 3**.9* that the filtered signal very closely matches the
    true signal, but not exactly. This is because, as mentioned previously, we are
    approximating a continuous signal with a relatively small sample.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Signal processing in a production setting would probably make use of a specialized
    package, such as the `signal` module from `scipy`, or some lower-level code or
    hardware to perform filtering or cleaning of a signal. This recipe should be taken
    as more of a demonstration of the use of FFT as a tool for working with data sampled
    from some kind of underlying periodic structure (the signal). FFTs are useful
    for solving partial differential equations, such as the heat equation seen in
    the *Solving partial differential equations* *numerically* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More information about random numbers and the normal distribution (Gaussian)
    can be found in [*Chapter 4*](B19085_04.xhtml#_idTextAnchor138), *Working with
    Randomness* *and Probability*.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic differentiation and calculus using JAX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JAX is a linear algebra and automatic differentiation framework developed by
    Google for ML. It combines the capabilities of **Autograd** and its **Accelerated
    Linear Algebra** (**XLA**) optimizing compiler for linear algebra and ML. In particular,
    it allows us to easily construct complex functions, with automatic gradient computation,
    that can be run on **Graphics Processing Units** (**GPUs**) or **Tensor Processing
    Units** (**TPUs**). On top of all of this, it is relatively simple to use. In
    this recipe, we see how to make use of the JAX **just-in-time** (**JIT**) compiler,
    get the gradient of a function, and make use of different computation devices.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we need the JAX package installed. We will make use of the
    Matplotlib package, with the `pyplot` interface imported as `plt` as usual. Since
    we’re going to plot a function of two variables, we also need to import the `mplot3d`
    module from the `mpl_toolkits` package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show how to define a JIT-compiled function using JAX, compute
    the gradient of this function, and use a GPU or TPU to perform calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import the parts of the JAX library that we will use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can define our function, with the `@jit` decorator applied to tell
    JAX to JIT compile this function where necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a grid and plot our function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in *Figure 3**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Plot of a function of two variables computed using JAX'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 – Plot of a function of two variables computed using JAX
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we use the `grad` function (and the `jit` decorator) to define two new
    functions that are the partial derivatives with respect to the first and second
    arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: 'To quickly check that these functions are working, we print the values of these
    functions at ![](img/Formula_03_255.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: 'To finish off, let’s plot the partial derivative with respect to ![](img/Formula_03_256.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: 'The partial derivative plot is shown in *Figure 3**.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Plot of the partial derivative of the function computed using
    autodiff in JAX'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 – Plot of the partial derivative of the function computed using
    autodiff in JAX
  prefs: []
  type: TYPE_NORMAL
- en: A quick check confirms that this is indeed a plot of the partial derivative
    with respect to ![](img/Formula_03_256.png) of the function ![](img/Formula_03_258.png).
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX is an interesting mix of a JIT compiler, with an emphasis on fast linear
    algebra operations, combined with the power of Autograd, with support for acceleration
    devices (and several other features that we don’t use here). The JIT compilation
    works by tracing the linear algebra operations performed on the JAX version of
    the NumPy library and constructing an intermediate representation of the function
    in a form that can be understood by the XLA compiler. For any of this to work,
    you need to make sure that you only use the NumPy module from JAX (`jax.numpy`)
    rather than the *real* NumPy. JAX also provides a version of the SciPy package.
  prefs: []
  type: TYPE_NORMAL
- en: 'One caveat of this approach is that the functions must be *pure*: they should
    not have side effects beyond the return value, and should not depend on any data
    not passed by arguments. It might still work if this is not the case, but you
    might get unexpected results—remember that the Python version of the function
    might only be executed once. Something else to consider is that, unlike NumPy
    arrays, JAX NumPy arrays cannot be updated in place using index notation and assignment.
    This, and several other current important caveats, are listed in the JAX documentation
    (refer to the following section, *See also…*).'
  prefs: []
  type: TYPE_NORMAL
- en: The `jit` decorator instructs JAX to construct compiled versions of the function
    where appropriate. It might actually produce several compiled versions depending
    on the types of arguments provided (for example, a different compiled function
    for scalar values versus array values).
  prefs: []
  type: TYPE_NORMAL
- en: The `grad` function takes a function and produces a new function that computes
    the derivative with respect to the input variable. If the function has more than
    one input variable, then this is the partial derivative with respect to the first
    argument. The second optional argument, `argnums`, is used to specify which derivatives
    to compute. In the recipe, we have a function of two variables and used the `grad(f,
    0)` and `grad(f, 1)` commands to get the functions representing the two partial
    derivatives of the `f` function.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the functions from `jax.numpy` have the same interface as from `numpy`—we
    see a few of these functions in the recipe. The difference is that JAX versions
    produce arrays that are stored correctly for the accelerator device if one is
    used. We can use these arrays in contexts that expect NumPy arrays, such as plotting
    functions, without any issues.
  prefs: []
  type: TYPE_NORMAL
- en: In step 5 of the recipe, we printed the value of the two partial derivatives.
    Notice that we used the values `1.` and `-1.`. It is important to note that using
    the integer equivalent `1` and `-1` would have failed because of the way JAX handles
    floating-point numbers. (Since most GPU devices do not handle double-precision
    floating-point numbers well, the default float type in JAX is `float32`.)
  prefs: []
  type: TYPE_NORMAL
- en: In step 6, we computed the derivative over the same region as the function.
    To do this, we had to flatten the ![](img/Formula_03_259.png) and ![](img/Formula_03_260.png)
    arrays and then use the `vmap` function to vectorize the `fx` derivative before
    reshaping the result. There is a complication in the way that `grad` works, which
    means that `fx` does not vectorize in the way we expect.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JAX is designed to scale well as needs change, so lots of the components are
    designed with concurrency in mind. For instance, the random numbers module provides
    a random number generator that is capable of splitting effectively so that computations
    can run concurrently without changing the outcome. This wouldn’t be possible,
    for example, with a Mersenne Twister random generator, which would potentially
    produce different answers depending on the number of threads used because it doesn’t
    *split* in a statistically sound way.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lots more information can be found in the JAX documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://jax.readthedocs.io/en/latest/](https://jax.readthedocs.io/en/latest/)'
  prefs: []
  type: TYPE_NORMAL
- en: Solving differential equations using JAX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JAX provides a set of tools for solving a wide array of problems. Solving differential
    equations—such as initial value problems described in the *Solving simple differential
    equations numerically* recipe—should be well within the capabilities of this library.
    The `diffrax` package provides various solvers for differential equations leveraging
    the power and convenience of JAX.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the earlier recipe, we solved a relatively simple first-order ODE. In this
    recipe, we’re going to solve a second-order ODE to illustrate the technique. A
    **second-order ODE** is a differential equation that involves both the first and
    second derivatives of a function. To keep things simple, we’re going to solve
    a *linear* second-order ODE of the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_261.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/Formula_03_262.png) is a function of ![](img/Formula_03_263.png)
    to be found. In particular, we’re going to solve the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_264.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The initial conditions are ![](img/Formula_03_265.png) and ![](img/Formula_03_266.png).
    (Note that this is a second-order differential equation, so we need two initial
    conditions.)
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can start to solve this equation, we need to do some pen-and-paper
    work to reduce the second-order equation to a system of first-order differential
    equations that can be solved numerically. To do this, we make a substitution ![](img/Formula_03_267.png)
    and ![](img/Formula_03_268.png). When we do this, we get a system like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_269.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We also get the initial conditions ![](img/Formula_03_270.png) and ![](img/Formula_03_271.png).
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we will need the `diffrax` package installed, along with JAX.
    As usual, we import the Matplotlib `pyplot` interface under the alias `plt`. We
    import `jax.numpy` under the alias `jnp` and the `diffrax` package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show how to use JAX and the `diffrax` library to solve
    a second-order linear differential equation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to set up our function that represents the system of first-order
    ODEs we constructed in the *Getting* *ready* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set up the `diffrax` environment that we will use to solve the equation.
    We’ll use the solver recommended in the `diffrax` *quickstart guide* – see the
    *See also* section below for more details. The setup is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we use the `diffeqsolve` routine from `diffrax` to solve the differential
    equation on the range ![](img/Formula_03_272.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have solved the equation, we need to extract the values for ![](img/Formula_03_273.png)
    from the `solution` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot the results on a new figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in *Figure 3**.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Numerical solution to a second-order linear ODE'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/3.12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Numerical solution to a second-order linear ODE
  prefs: []
  type: TYPE_NORMAL
- en: We can see that when ![](img/Formula_03_274.png) is close to ![](img/Formula_03_275.png),
    the solution is approximately linear, but later on, the solution becomes non-linear.
    (The ![](img/Formula_03_274.png) range might be too small to see the interesting
    behavior of this system.)
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`diffrax` is built on top of JAX and provides various solvers for differential
    equations. In the recipe, we used the Dormand-Prince 5(4) `Dopri5` solver class,
    which is another example of a Runge-Kutta method for solving ODEs similar to the
    Runge-Kutta-Fehlberg method we saw in an earlier recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, `diffrax` translates the ODE initial value problem into a
    `diffrax` able to solve other kinds of differential equations besides these simple
    ODEs shown here; one of the goals of the library is to provide tools for numerically
    solving **stochastic differential equations** (**SDEs**). Since it is based on
    JAX, it should be easy to integrate this into other JAX workflows. It also has
    support for backpropagation through various adjoint methods.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'More information about the `diffrax` library and the methods it contains can
    be found in the documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.kidger.site/diffrax](https://docs.kidger.site/diffrax'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Calculus is a very important part of every undergraduate mathematics course.
    There are a number of excellent textbooks on calculus, including the classic textbook
    by Spivak and the more comprehensive course by Adams and Essex:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Spivak, M.* (*2006*). *Calculus*. *3rd ed*. *Cambridge: Cambridge* *University
    Press*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adams, R.* and *Essex, C.* (*2018*). *Calculus: A Complete Course*. *9th ed*.
    *Don Mills,* *Ont: Pearson*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A good source for numerical differentiation and integration is the classic
    *Numerical Recipes* book, which gives a comprehensive description of how to solve
    many computational problems in C++, including a summary of the theory:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Press, W.*, *Teukolsky, S.*, *Vetterling, W*. and *Flannery, B.* (*2007*).
    *Numerical Recipes: The Art of Scientific Computing*. *3rd ed*. *Cambridge: Cambridge*
    *University Press*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
