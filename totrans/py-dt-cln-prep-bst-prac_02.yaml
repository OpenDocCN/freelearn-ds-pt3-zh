- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Importance of Data Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Did you know that data serves as the backbone of many important business decisions?
    Without accurate, complete, and consistent information, companies risk making
    faulty judgments that could potentially damage their reputation, client relationships,
    and business overall. Consistency issues across different datasets can cause confusion
    and prevent meaningful analysis from happening. Irrelevant or outdated data can
    misguide the judgment of decision-makers, resulting in suboptimal choices. On
    the other hand, building high-quality data products serves as a powerful asset,
    empowering organizations to make informed decisions, uncover valuable insights,
    identify trends, mitigate risks, and gain a competitive edge.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will dive deep into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why data quality is important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different dimensions to measure data quality in your data products
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impact of data silos on data quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code for the chapter in the following GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter02](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter02)'
  prefs: []
  type: TYPE_NORMAL
- en: Why data quality is important
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Allow me to unveil the reasons why data quality matters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accurate data can give you a competitive advantage**: Organizations depend
    on data to determine patterns, trends, preferences, and other vital aspects governing
    their ecosystem. If your data quality is subpar, the resulting analysis and conclusions
    may be skewed, resulting in wrong moves that could jeopardize your entire business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete data is the backbone of cost optimization**: Data forms the foundation
    of automation and optimization, which can drive up productivity and lower expenses
    when executed properly. Incomplete or low-quality data can cause bottlenecks and
    increase costs. Imagine countless man-hours wasted on fixing errors that would
    have been avoided if only there had been higher standards set for data entry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-notch data can lead to satisfied customers who stick around long term**:The
    heartbeat of every business depends on satisfied clients, whose loyalty can ensure
    sustained growth over time. Wrong data about customers might translate into personalized
    experiences that don’t match customer characteristics or even into inaccurate
    billings and unfulfilled requests. These disappointed customers may take their
    business elsewhere, leaving your company struggling to survive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compliant data is a requirement to avoid unwanted legal consequences**: Several
    industries must follow specific rules concerning data precision, safety, and privacy.
    Adherence requires a high level of data quality to satisfy strict guidelines and
    prevent legal penalties along with the potential loss of consumer confidence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**To avoid data silos, you need to trust your data**: When various entities
    within an enterprise must work together to leverage data, ensuring its integrity
    is essential. Incompatibility or discrepancies may obstruct cooperation and can
    hinder integration efforts and create data silos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality actually means trust**: Data quality directly impacts the trust
    and credibility stakeholders place in an organization. By maintaining high-quality
    data, organizations can foster trust among customers, partners, and investors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a clearer picture of why data quality is important, let’s move
    to the next section, where we will dive deep into the different dimensions of
    data quality.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensions of data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As emphasized earlier, superior data quality forms the foundation upon which
    informed decisions and strategic insights are built. With this in mind, let us
    now examine which **Key Performance Indicators** (**KPIs**) we could use to measure
    the data quality of our assets.
  prefs: []
  type: TYPE_NORMAL
- en: Completeness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Completeness measures the extent to which data is complete and lacks missing
    values or fields. KPIs can include metrics such as the percentage of missing data
    or missing data points per record.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will output the completeness percentages for each column
    in your dataset. A higher percentage indicates a higher level of completeness,
    while a lower percentage suggests more missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by importing the `pandas` library to work with the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a sample dataset with the following columns: `Name`, `Age`,
    `Gender`, and `City`. Some values are intentionally missing (represented as `None`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We’ll then use the `isnull()` function to identify missing values in each column
    and use the `sum()` function to count the total number of missing values for each
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we’ll calculate the completeness percentage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will print the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The completeness check shows the number of missing values for each column, and
    the completeness percentage indicates the proportion of missing values in each
    column relative to the total number of records. This output indicates that the
    `Name` and `Gender` columns have 100% completeness, whereas the `Age` and `City`
    columns have 80%.
  prefs: []
  type: TYPE_NORMAL
- en: Note – completeness percentage
  prefs: []
  type: TYPE_NORMAL
- en: The completeness percentage is calculated by dividing the count of missing values
    by the total number of records in the dataset and then multiplying by 100\. It
    represents the proportion of missing values relative to the size of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**The higher the percentage of completeness is,** **the better!**'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Accuracy assesses the correctness of data by comparing it to a trusted source
    or benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will output the accuracy percentage based on the comparison
    between the actual and expected values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by loading the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a sample dataset named `data` and a reference dataset named
    `reference_data`. Both datasets have the same structure (columns: `Name`, `Age`,
    `Gender`, and `City`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create two pandas DataFrames, named `df` and `reference_df`, using the sample
    data and the reference data, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create an `accuracy_check` variable and assign the result of the comparison
    between `df` and `reference_df` to it. This comparison is done using the `==`
    operator, which returns `True` for matching values and `False` for non-matching
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can compare the column with actual values to the column with expected values
    using the `==` operator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We then calculate the accuracy percentage by taking the mean of the `accuracy_check`
    DataFrame for each column. The `mean` operation treats `True` as `1` and `False`
    as `0`, so it effectively calculates the percentage of matching values in each
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy check shows `True` where the data matches the reference dataset
    and `False` where it doesn’t. The accuracy percentage indicates the proportion
    of matching values in each column relative to the total number of records. This
    output indicates that the `Age` column is the only one that needs some more attention
    in this case. All the others are 100% accurate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note – accuracy percentage
  prefs: []
  type: TYPE_NORMAL
- en: An accuracy percentage can be calculated by taking the mean (average) of the
    comparison results for all columns and multiplying by 100\. This represents the
    proportion of matching data values relative to the total number of data points.
  prefs: []
  type: TYPE_NORMAL
- en: The higher the percentage of accuracy is, the better!
  prefs: []
  type: TYPE_NORMAL
- en: Wondering how to build the ground truth dataset?
  prefs: []
  type: TYPE_NORMAL
- en: The ground truth must be representative of the task you are trying to solve.
    This means that depending on where you are in the data life cycle, the ground
    truth is built differently and plays a different role as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building a ground truth dataset is essential across various data personas,
    including data engineers, data analysts, and machine learning practitioners. For
    data engineers, ground truth is critical for data validation and testing. The
    good thing is that in most cases, data engineers can build the ground truth labels
    from historical data with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data validation rules**: Establish validation rules and constraints to verify
    the accuracy of the data as it flows through the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manual inspection**: Manually inspect data samples to identify inconsistencies
    or errors and create a dataset of validated and corrected data. This can serve
    as the ground truth dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data analysts rely on ground truth data to validate the accuracy of their findings,
    which can be acquired through expert annotations, historical data, and user feedback,
    ensuring that analytical insights reflect real-world phenomena:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Expert annotations**: If working with unstructured or text data, domain experts
    can manually annotate data samples with the correct labels or categories, serving
    as ground truth for analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Historical data**: Use historical data with well-documented accuracy to serve
    as ground truth. This can be valuable when analyzing trends, patterns, or historical
    events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Surveys and user feedback**: Collect data from surveys or user feedback to
    validate insights and conclusions drawn from the data. These can serve as qualitative
    ground truth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lastly, in the context of machine learning, ground truth data forms the backbone
    for model training and evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Manual labeling**: Annotate data samples manually to create a labeled dataset.
    This is common for tasks such as image classification, sentiment analysis, or
    object detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Crowdsourcing**: Use crowdsourcing platforms to collect annotations from
    multiple human workers, who collectively establish ground truth data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Existing datasets**: Many machine learning tasks benefit from established
    benchmark datasets that have been widely used in the research community. You can
    use and update these datasets to your needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain expert labels**: Consult domain experts to provide labels or annotations
    for data, especially when domain-specific knowledge is required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthetic data generation**: Create synthetic data with known ground truth
    labels to develop and test machine learning models. This is especially useful
    in the absence of real-world labeled data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of the persona, it is vital to create, maintain, and continually
    assess the quality of ground truth data, being mindful of potential biases and
    limitations. This is because it significantly influences the effectiveness of
    data engineering, analysis, and machine learning efforts.
  prefs: []
  type: TYPE_NORMAL
- en: Timeliness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Timeliness evaluates how quickly data is captured, processed, and made available
    for use. Timeliness KPIs may include metrics such as data latency (time elapsed
    between data capture and availability) or adherence to data refresh schedules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Measuring timeliness in data involves assessing the freshness or currency of
    the data with respect to a specific timeframe or event. Let’s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then generate a random dataset with timestamps and values. The timestamps
    are randomly distributed within a given time range to simulate real-world data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define a reference timestamp to compare the dataset’s timestamps to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We set a timeliness threshold of 30 minutes. Data with timestamps within 30
    minutes of the reference timestamp will be considered timely:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We calculate the timeliness for each record in the dataset by computing the
    time difference in minutes between the reference timestamp and each record’s timestamp.
    We also create a Boolean column to indicate whether the record is timely based
    on the threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we calculate the average timeliness of the dataset and display the
    results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This example provides a more realistic simulation of timeliness in a dataset
    with randomly generated timestamps and a timeliness threshold. *The average timeliness
    represents the average time deviation from the reference timestamp for* *the dataset*.
  prefs: []
  type: TYPE_NORMAL
- en: Good timeliness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A low average timeliness and a high percentage of timely records suggest that
    the dataset is current and aligns well with the reference timestamp. This is desirable
    in real-time applications or scenarios where up-to-date data is critical.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important consideration here is how to define the reference timestamp. The
    reference timestamp is the point in time to which the dataset’s timestamps are
    compared. It represents the expected or desired time for the data. For example:
    a record was created when a loyalty card was scanned at a retail store, and the
    reference time is when the record was logged in the database. So, we are calculating
    the time it took from the real creation of the event to a new entry to be stored
    in the database.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The smaller the reference threshold, the more *real-time* the application needs
    to be. On the other hand, the bigger the reference threshold is, the more time
    it takes to bring the data into your system (**batch application**). The choice
    between real-time and batch processing depends on the specific requirements of
    your application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time processing**: Choose real-time processing when immediate responses,
    low latency, and the ability to act on data as it arrives are crucial. It’s suitable
    for applications where time-sensitive decisions are made.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch processing**: Select batch processing when low latency is not a strict
    requirement, and you can tolerate some delay in data processing. Batch processing
    is often more cost-effective and suitable for tasks that can be scheduled and
    automated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How timeliness definitions change across different data personas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Timeliness is an essential aspect of data quality with applications across
    various roles, from data engineers to data analysts and machine learning practitioners.
    Here’s how each role can leverage timeliness in the real world:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data engineers**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data pipeline monitoring**: Data engineers can use timeliness as a key metric
    for monitoring data pipelines. They can set up automated alerts or checks to ensure
    that data is arriving on time, identifying, and addressing delays in data ingestion.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data validation**: Data engineers can incorporate timeliness checks as part
    of their data validation processes, ensuring that data meets specified timing
    criteria before it is used for downstream processes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data analysts**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time analytics**: Analysts in domains such as finance or e-commerce
    rely on real-time data to make informed decisions. They need to ensure that the
    data they analyze is up to date and reflects the current state of affairs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KPI monitoring**: Timeliness is essential in monitoring KPIs. Analysts use
    timely data to track and assess the performance of various business metrics.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine** **learning practitioners**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Timeliness plays a role in feature engineering for
    machine learning models. It’s important to keep features as up-to-date as possible
    because this has a direct impact on model training and scoring.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training and evaluation**: In real-time predictive models, model training
    and evaluation rely on timely data. Practitioners must ensure that the training
    data is current to build effective models or perform real-time inference.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept drift detection**: Timeliness is critical in detecting concept drift,
    which occurs when the relationships within the data change over time. Machine
    learning models need to adapt to these changes, and timely data is necessary to
    monitor and detect such drift.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some real applications of timeliness:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Finance**: In the financial sector, timeliness is crucial for stock trading,
    fraud detection, and risk management, where timely data can lead to better decisions
    and reduced risks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare**: Timeliness is vital for healthcare data, particularly in patient
    monitoring and real-time health data analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E-commerce**: Timely data is essential for e-commerce companies to monitor
    sales, customer behavior, and inventory in real-time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transportation and L=logistics**: In supply chain management and logistics,
    real-time tracking and timely data are essential for route optimization and inventory
    management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s move on to consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consistency measures the degree of consistency within the data and involves
    ensuring that data follows established rules, standards, and formats throughout
    a dataset. In more detail, we should check for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Same format**: Data should follow the same format, structure, and standards
    across all records or columns. This uniformity ensures that data can be easily
    processed and compared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adherence to standards**: Data should adhere to predefined rules, guidelines,
    naming conventions, and reference data. For example, if a dataset contains product
    names, consistency would require that all product names follow a standardized
    naming convention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data type and format**: Consistency checks include verifying that data types
    (e.g., text, numbers, and dates) and data formats (e.g., date formats and numerical
    representations) are consistent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s get a better understanding with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the `pandas` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then create a sample dataset with product information, including product
    names. In this example, we’ll check whether all product names start with `PROD`
    as per the naming convention:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s define the expected prefix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We check the consistency of the `ProductName` column by ensuring that all product
    names start with `PROD`. Inconsistent names will be flagged:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we calculate the percentage of consistent rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we display the results, including the dataset with the consistency
    check results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the final output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this specific dataset, three out of the five product names are consistent
    with the naming convention, resulting in an 80% consistency rate. The `Product003`
    entry is marked as inconsistent because it does not start with `PROD`.
  prefs: []
  type: TYPE_NORMAL
- en: This type of consistency check can be useful for ensuring that data adheres
    to specific standards or conventions, and the calculated percentage provides a
    quantitative measure of how many records meet the criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Higher consistency percentages imply more uniformity and conformity in the values
    within the respective columns. If there is a very low percentage, then we have
    a lot of distinct values in the dataset, which is not a mistake as long as we
    understand what the column represents and there is a meaning behind all the unique
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Are you wondering about what other consistency checks you could apply?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01.jpg)![](img/02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 2.1 – Different consistency check options
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss uniqueness next.
  prefs: []
  type: TYPE_NORMAL
- en: Uniqueness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Uniqueness measures the presence of unique values in a dataset. It can help
    identify anomalies such as duplicated keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code will output the validity results for each column, indicating whether
    the values in each column conform to the defined validity rules:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the `pandas` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then create a sample dataset containing email addresses. We want to check
    whether all email addresses in the dataset are unique:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We check the uniqueness of the `Email` column to ensure that no email address
    is duplicated in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we calculate the uniqueness percentage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we display the results, including the dataset with the uniqueness
    check results. Here’s the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This output indicates that the values in the dataset are all unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Uniqueness checks are commonly performed in various industries and use cases.
    Here are some common examples of uniqueness checks in real-world scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer IDs**: In a customer database, each customer should have a unique
    identifier (customer ID) to prevent duplicate customer records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product SKUs**: In inventory and e-commerce databases, each product should
    have a unique SKU to identify and manage products without duplication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Email Addresses**: Email addresses should be unique in a mailing list or
    user database to avoid sending duplicate messages or creating multiple accounts
    with the same email'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Employee IDs**: In HR databases, each employee typically has a unique employee
    ID to differentiate between employees and manage their records effectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vehicle Identification Numbers** (**VINs**): In the automotive industry,
    VINs are unique identifiers for each vehicle to track their history and ownership'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Barcodes and QR codes**: In retail and logistics, barcodes and QR codes provide
    unique identifiers for products, packages, and items for tracking and inventory
    management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usernames and user IDs**: In online platforms and applications, usernames
    and user IDs are unique to each user to distinguish them and manage user accounts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serial Numbers**: In manufacturing, products often have unique serial numbers
    to identify and track individual items'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transaction IDs**: In financial systems, each transaction is assigned a unique
    transaction ID to prevent duplication and ensure proper record-keeping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you have non-unique records in a dataset, it means that there are duplicate
    entries or records with identical key attributes (e.g., IDs, names, or email addresses)
    in the dataset. Non-unique records can lead to data quality issues and potentially
    cause errors in data analysis and reporting. To fix non-unique records in a dataset
    using Python, you can use various methods, including removing duplicates, aggregating
    data, or resolving conflicts based on your specific data and requirements. We’ll
    discuss those strategies in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Duplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Duplication assesses the presence of duplicate or redundant data within the
    dataset. Having duplicates in your data means that you have the same piece of
    information or record repeated multiple times within your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Example – customer database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine you work for a company with a customer database that tracks information
    about each customer, including their contact details, purchases, and interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Issue – duplicate customer records
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You discover that there are duplicate customer records in the database. These
    duplicates may have occurred due to data entry errors, system issues, or other
    reasons. For instance, a customer named John Smith has two separate records with
    slight variations in contact details. One record has his email address as `john.smith@email.com`,
    and the other has `jsmith@email.com`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is generally considered undesirable for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data accuracy**: When you have multiple copies of the same information, it
    becomes challenging to determine which version is correct or up-to-date. This
    can lead to data inconsistency and confusion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage efficiency**: Duplicated records consume unnecessary storage space.
    This is particularly important when dealing with large datasets, as it can lead
    to increased storage costs and longer data retrieval times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data integrity**: Duplicates can compromise data integrity. In situations
    where data relationships are critical, duplicated records can disrupt the integrity
    of your data model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient data processing**: Analyzing, querying, and processing datasets
    with fewer duplicates is more efficient. Data processing times are shorter, and
    results are more meaningful when you’re not dealing with repetitive information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data analysis**: When performing data analysis or running statistical models,
    duplicated records can skew results and lead to incorrect conclusions. Reducing
    duplicates is crucial for accurate and meaningful analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost savings**: Storing and managing duplicates incurs additional costs in
    terms of storage infrastructure and data management efforts. Eliminating duplicates
    can lead to cost savings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now imagine what happens if we extrapolate the same problem to a company that
    manages millions of customers. Could you see how expensive and confusing introducing
    duplicates could be?
  prefs: []
  type: TYPE_NORMAL
- en: 'While it’s generally a best practice to minimize duplicate records in a dataset,
    there are some specific scenarios where accepting or allowing duplicates might
    be a reasonable choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Scenario** | **Data representation** |'
  prefs: []
  type: TYPE_TB
- en: '| Customer order history | Each row represents a separate order made by a customer.
    Multiple rows with the same customer ID are allowed to show order history. |'
  prefs: []
  type: TYPE_TB
- en: '| Service requests | Records represent service requests, including multiple
    requests from the same customer or location over time. Duplicates are allowed
    to maintain a detailed history. |'
  prefs: []
  type: TYPE_TB
- en: '| Sensor data | Each row contains sensor readings, which can include multiple
    entries with the same data values over time. Duplicates are allowed for tracking
    each reading. |'
  prefs: []
  type: TYPE_TB
- en: '| Logging and audit trails | Log entries record events or actions, and some
    events may generate duplicate entries. Duplicates are preserved for detailed audit
    trails. |'
  prefs: []
  type: TYPE_TB
- en: '| User interaction data | Records capture user interactions with a website
    or application. Duplicates can represent repeated interactions for analysis of
    user behavior. |'
  prefs: []
  type: TYPE_TB
- en: '| Change history | Data versions or document changes result in multiple records,
    including duplicates that capture historical revisions. Duplicates are maintained
    for version history. |'
  prefs: []
  type: TYPE_TB
- en: Table 2.2 – Duplicated records scenarios
  prefs: []
  type: TYPE_NORMAL
- en: In these scenarios, allowing duplicates serves specific data management or analysis
    goals, such as preserving historical data, maintaining a record of changes, or
    capturing detailed user interactions. The data representation aligns with these
    objectives, and duplicates are *intentionally* retained to support these use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how to track duplicates with a code example. The code will output
    the number of duplicate records found in your dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a sample dataset with employee information. We’ll intentionally
    introduce duplicate employee IDs to demonstrate the identification of duplicates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We’ll use pandas to identify and mark duplicate records based on the `EmployeeID`
    column. The `duplicated()` function is used to create a Boolean mask, where `True`
    indicates a duplicated record:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `subset='EmployeeID'` argument specifies the column on which the duplication
    check is performed. `keep='first'` marks duplicates as `True` except for the first
    occurrence. You can change this parameter to `last` or `False` based on your requirements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We then create a new column called `''IsDuplicate''` in the DataFrame to indicate
    whether each record is a duplicate or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We calculate the percentage of duplicate records by dividing the number of
    duplicate records (those marked as `True` in the `''IsDuplicate''` column) by
    the total number of records and then multiplying by 100 to express it as a percentage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we display the dataset with the `IsDuplicate` column to see which
    records are duplicates. Here’s the final output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This output indicates that 28.57% of records are duplicated in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**The fewer duplicated records,** **the better!**'
  prefs: []
  type: TYPE_NORMAL
- en: The threshold for what is considered an acceptable or “good” level of duplicated
    records in a dataset can vary depending on the specific context and the goals
    of your data management or analysis. There is no one-size-fits-all answer to this
    question, as it depends on factors such as the type of data, the purpose of the
    dataset, and industry standards.
  prefs: []
  type: TYPE_NORMAL
- en: Data usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data usage assesses the extent to which data is effectively utilized within
    the organization. Data usage KPIs can include metrics such as data utilization
    rates, the number of data requests or queries, or user satisfaction surveys regarding
    data availability and quality.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario – corporate business intelligence dashboard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine a large corporation that relies on data-driven decision-making to optimize
    its operations, marketing strategies, and financial performance. The corporation
    has a centralized **Business Intelligence** (**BI**) dashboard that provides various
    data analytics and insights to different departments and teams. This dashboard
    is crucial for monitoring the company’s performance and making informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this scenario, assessing data usage metrics can be vital for optimizing
    the BI dashboard’s effectiveness and ensuring it meets the organization’s data
    needs. Here’s what we will track in the code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data utilization rates**: By tracking data utilization rates for different
    departments and teams, the organization can assess how often the dashboard is
    accessed and how extensively the data within it is used. For example, the marketing
    department might have a high data utilization rate, indicating a heavy reliance
    on the dashboard for campaign performance analysis. This metric can help identify
    areas of the organization where data-driven insights are most critical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of data requests or queries**: Monitoring the number of data requests
    or queries made by users provides insights into the volume of data analysis conducted
    through the dashboard. High data request numbers may indicate a strong appetite
    for data-driven decision-making. This metric can also help identify peak usage
    times and popular data sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User satisfaction scores**: Collecting user satisfaction scores through surveys
    can gauge how well the BI dashboard meets user expectations. A lower average user
    satisfaction score may signal that the dashboard’s features or user experience
    need improvement. Feedback from users can guide dashboard enhancements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Organization data utilization rate**: Calculating the overall data utilization
    rate for the entire organization helps assess the dashboard’s relevance and its
    effectiveness in serving the broader business goals. It also provides a benchmark
    to measure against in terms of data utilization improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the number of data requests for the last month, you would need
    to log the data requests of your application with the associated timestamps. Let’s
    see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the `random` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a function to simulate data usage metrics. In this function,
    we set the number of users in the organization to 500 users, but in a real-world
    scenario, you would replace this with the actual number of users in your organization.
    Let’s have a look at the following function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The main goal of this function is to simulate data utilization rates for each
    user in the organization. The `random.uniform(20, 90)` function generates a random
    floating-point number between 20 and 90\. We do this for each user, resulting
    in a list of utilization rates. Similarly, we simulate the number of data requests
    or queries made by each user. Here, we use `random.randint(1, 100)` to generate
    a random integer between 1 and 100 for each user, representing the number of data
    requests. Next, we calculate two organization-level metrics. The first is the
    average data utilization rate for the entire organization, and the second is the
    total number of data requests or queries across all users. We simulate user satisfaction
    scores using a scale from 1 to 5\. Each user receives a random satisfaction score.
    We calculate the average user satisfaction score for the entire organization based
    on the simulated satisfaction scores.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We call the `simulate_data_usage()` function to run the simulation and store
    the results in the `data_usage_metrics` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we display the simulated data usage metrics. The output is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Capturing the data usage of different data products is crucial for several
    reasons, especially in the context of organizations that rely on data for decision-making
    and operational effectiveness:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizing resources**: By understanding how data products are used, organizations
    can allocate resources effectively. This includes identifying which data sources
    are heavily utilized and which may be underused. It helps in optimizing data storage,
    processing, and infrastructure resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improving data quality**: Monitoring data usage can highlight data quality
    issues. For example, if certain data products are rarely accessed, it may indicate
    that the data quality is poor or that the data is no longer relevant. Capturing
    usage can trigger data quality improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identifying trends and patterns**: Data usage patterns can reveal insights
    into how data is consumed and what types of analyses or reports are most valuable
    to users. This input can inform data product development and enhancement strategies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost management**: Knowing which data products are in high demand helps manage
    data-related costs. It enables organizations to invest resources wisely and avoid
    unnecessary expenses on maintaining or storing less-used data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and compliance**: Tracking data usage is crucial for data security
    and compliance. Organizations can identify unauthorized access or unusual usage
    patterns that may indicate security breaches. It also helps in complying with
    data privacy regulations by demonstrating control over data access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User satisfaction**: Understanding how data products are used and whether
    they meet user needs is vital for user satisfaction. It allows organizations to
    tailor data products to user requirements, resulting in better user experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Capacity planning**: Capturing usage data helps in capacity planning for
    data infrastructure. It ensures that there is enough capacity to handle data traffic
    during peak usage periods, preventing performance bottlenecks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Return on Investment** (**ROI**) **measurement**: For organizations investing
    in data products, tracking usage is essential for measuring the ROI. It helps
    determine whether the resources spent on data collection, processing, and presentation
    are justified by their impact on decision-making and business outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s discuss data compliance.
  prefs: []
  type: TYPE_NORMAL
- en: Data compliance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data compliance evaluates the extent to which data adheres to regulatory requirements,
    industry standards, or internal data governance policies. Compliance KPIs may
    involve metrics such as the number of non-compliant data records, the percentage
    of data complying with specific regulations, or the results of data compliance
    audits. Data compliance is crucial for several important reasons, especially in
    today’s data-driven and highly regulated business environment as presented in
    the following table.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Consequence/challenge** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Legal and regulatory consequences | Non-compliance can lead to legal actions,
    fines, and penalties |'
  prefs: []
  type: TYPE_TB
- en: '| Reputational damage | Negative publicity and loss of trust among customers
    and stakeholders |'
  prefs: []
  type: TYPE_TB
- en: '| Financial impact | Costs associated with fines, legal fees, data breach notifications,
    and so on |'
  prefs: []
  type: TYPE_TB
- en: '| Data breaches | Increased risk of security breaches and unauthorized access
    |'
  prefs: []
  type: TYPE_TB
- en: '| Data quality issues | Inaccurate or incomplete data impacts decision-making
    and efficiency |'
  prefs: []
  type: TYPE_TB
- en: '| Loss of customers | Customers discontinuing relationships with non-compliant
    organizations |'
  prefs: []
  type: TYPE_TB
- en: '| Legal liability | Potential legal liability for individuals and organizations
    |'
  prefs: []
  type: TYPE_TB
- en: '| Additional monitoring and oversight | Imposition of stricter regulatory monitoring
    and oversight |'
  prefs: []
  type: TYPE_TB
- en: '| Difficulty in expanding internationally | Hindrance in global expansion due
    to international non-compliance |'
  prefs: []
  type: TYPE_TB
- en: Table 2.3 – Consequences of neglecting data compliance
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a Python example to illustrate a simplified scenario where we check
    the compliance of data records with specific regulations using randomly generated
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the `random` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a function to simulate a dataset with compliance checks for
    a given number of data records:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each data record consists of attributes such as `Age` and `Consent Given`,
    which are randomly generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define compliance rules for these attributes based on a simplified scenario
    where, for instance, individuals must be 18 or older to provide consent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We check compliance with specific regulations, and for each data record, we
    report whether it complies with `Age` and `Consent` requirements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We introduce a `compliant_count` variable to keep track of the number of compliant
    records:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Inside the loop that generates data records, we increment `compliant_count`
    whenever a record is compliant with the defined rules.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After generating all records, we calculate the percentage of compliant records
    as `(compliant_count / num_records) * 100` and store it in the `percentage_compliant`
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the number of records we would like to simulate and start simulating
    the compliance checks by calling our `simulate_data_compliance` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we display the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s a table summarizing popular compliance checks along with examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Compliance check** | **Description** **and examples** |'
  prefs: []
  type: TYPE_TB
- en: '| Data privacy compliance | Ensuring the protection of **Personally Identifiable
    Information** (**PII**); an example is the secure storage of customer names and
    addresses. |'
  prefs: []
  type: TYPE_TB
- en: '| GDPR compliance | Complying with the GDPR; an example is handling user data
    access and deletion requests. |'
  prefs: []
  type: TYPE_TB
- en: '| HIPAA compliance | Ensuring healthcare data protection in accordance with
    HIPAA; an example is secure handling of **Electronic Protected Health** **Information**
    (**ePHI**) |'
  prefs: []
  type: TYPE_TB
- en: '| PCI DSS compliance | Complying with the PCI DSS; an example is encrypting
    credit card information during payment processing |'
  prefs: []
  type: TYPE_TB
- en: '| Data retention compliance | Managing data retention periods and secure archiving
    or deletion |'
  prefs: []
  type: TYPE_TB
- en: '| Consent compliance | Verifying explicit user consent for data collection
    and processing; an example is opt-in consent for email marketing |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy and completeness compliance | Regularly checking and correcting
    data for accuracy and completeness |'
  prefs: []
  type: TYPE_TB
- en: '| Data classification and handling compliance | Labeling data by sensitivity
    and enforcing access controls; an example is classifying data as **Confidential**
    with restricted access. |'
  prefs: []
  type: TYPE_TB
- en: '| Data encryption compliance | Encrypting sensitive data in transit and at
    rest |'
  prefs: []
  type: TYPE_TB
- en: '| Access control compliance | Implementing role-based access control to restrict
    data access |'
  prefs: []
  type: TYPE_TB
- en: '| Auditing and logging compliance | Maintaining audit logs of data access and
    changes |'
  prefs: []
  type: TYPE_TB
- en: '| Data masking and anonymization Compliance | Protecting sensitive data through
    masking or anonymization |'
  prefs: []
  type: TYPE_TB
- en: '| Data life cycle management compliance | Managing data from creation to disposal
    in accordance with policies |'
  prefs: []
  type: TYPE_TB
- en: '| Data ethics and ethical compliance | Ensuring that data practices align with
    ethical standards |'
  prefs: []
  type: TYPE_TB
- en: '| Non-discrimination compliance | Avoiding discriminatory uses of data; an
    example is fair lending practices in financial services |'
  prefs: []
  type: TYPE_TB
- en: Table 2.4 – Key compliance checks
  prefs: []
  type: TYPE_NORMAL
- en: In practice, organizations may choose to calculate data quality KPIs, including
    completeness, on a daily, weekly, monthly, or quarterly basis. It is important
    to strike a balance between the frequency of measurement and the resources required
    to perform the assessments effectively. Regular monitoring and adjustments to
    the frequency of calculation can help ensure that data quality is continuously
    evaluated and maintained in accordance with business needs.
  prefs: []
  type: TYPE_NORMAL
- en: If the data has frequent updates, the monitoring metrics on the data should
    also be frequent. This ensures that any changes or updates to the data are captured
    in a timely manner and that the quality metrics remain up to date.
  prefs: []
  type: TYPE_NORMAL
- en: 'The more critical the data, the more frequent the update on the monitoring
    metrics should be. See here what critical data means:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Characteristic** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Vital for core operations | Essential for daily organizational functions
    |'
  prefs: []
  type: TYPE_TB
- en: '| Key to decision-making | Instrumental in strategic, tactical, and operational
    decisions |'
  prefs: []
  type: TYPE_TB
- en: '| High value and impact | Associated with significant financial value and operational
    impact |'
  prefs: []
  type: TYPE_TB
- en: '| Sensitive and confidential | Often includes sensitive and confidential information
    |'
  prefs: []
  type: TYPE_TB
- en: '| Business continuity and disaster recovery | Crucial for continuity planning
    and recovery measures |'
  prefs: []
  type: TYPE_TB
- en: '| Customer trust and satisfaction | Directly impacts trust and satisfaction
    |'
  prefs: []
  type: TYPE_TB
- en: '| Competitive advantage | May provide a competitive edge |'
  prefs: []
  type: TYPE_TB
- en: '| Strategic asset | Recognized as a strategic resource |'
  prefs: []
  type: TYPE_TB
- en: Table 2.5 – Critical data definition
  prefs: []
  type: TYPE_NORMAL
- en: If the data being measured for quality is vital for critical decision-making
    processes or sensitive operations, it may be necessary to calculate the quality
    KPIs on a more frequent basis.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to evaluate our data products against different quality
    KPIs, let’s see at which point in the data life cycle we need to apply those.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing quality controls throughout the data life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data quality should be a fundamental consideration throughout the entire life
    cycle of data. From data ingestion to utilization by downstream analytic teams,
    data undergoes various changes, and ensuring its quality at each step is paramount.
    Here is a diagram of the quality check life cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Quality check life cycle](img/B19801_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Quality check life cycle
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand more deeply what needs to happen at each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data entry/ingestion**: Validating the data sources and ensuring that data
    is captured accurately and consistently while entering the system can limit errors
    in the downstream processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data persona --> data engineer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data transformation**: By incorporating quality checks into the data transformation
    layer, organizations ensure that data remains reliable, accurate, and consistent
    throughout its journey from raw sources to its final destination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data persona --> data engineer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data integration**: When combining data from multiple sources or systems,
    data integration can introduce errors and inconsistencies. Applying quality checks
    at this level helps prevent data quality issues from propagating throughout the
    data ecosystem and supports a high level of confidence in the integrated data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data persona --> data engineer and data scientist
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data consumption**: Analytics and machine learning models heavily depend
    on the quality of input data. This is particularly crucial in today’s data-driven
    landscape, where the quality of data directly impacts an organization’s success
    and competitive advantage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data persona --> data scientist, analyst
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see in the preceding list, data flows in the system. Different teams
    collaborate on defining the quality metrics and applying the quality controls.
    Now, let’s see what would happen if there was no collaboration between the different
    teams.
  prefs: []
  type: TYPE_NORMAL
- en: Data silos and the impact on data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data silos, also known as isolated data repositories, are prevalent in many
    organizations today. Data silos refer to the practice of storing and managing
    data in isolated or disconnected systems or departments within an organization.
    These isolated data repositories have evolved over time, with various departments
    or business units maintaining the data separately and making it too complex to
    integrate. Organizations are increasingly aware of the limitations posed by data
    silos. They recognize that these silos hinder data-driven decision-making and
    operational efficiency. As a result, efforts to break down data silos and promote
    data integration and quality initiatives are on the rise, aimed at leveraging
    the full potential of data resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'These silos pose challenges in maintaining data quality across the dimensions
    we’ve already discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Not sharing data with the rest of the organization hinders its competitive
    advantage**: Data silos slow down decision-making by requiring employees to spend
    time searching for data from disparate sources, diverting their focus from gaining
    insights and taking action. Usually, data silos are associated with duplicate
    work as teams perform similar tasks independently, lacking efficient collaboration
    and information sharing. Conflicting interpretations of metrics frequently arise,
    causing confusion and disagreements among teams relying on different data sources.
    Misaligned assumptions and perspectives prevent progress and direction. Establishing
    clear communication guidelines and enforcing standardized methodologies is essential
    to align expectations and facilitate comprehension throughout the organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not sharing data with the rest of the organization is very costly**: Data
    silos increase costs due to the maintenance of multiple scattered systems across
    the organization. Maintaining these disparate systems requires dedicated resources,
    both in terms of personnel and infrastructure (such as redundant storage in multiple
    places). Retrieving relevant information becomes time-consuming due to scattered
    data repositories, resulting in delays. Manually combining data from different
    sources introduces potential errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s summarize what we’ve learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed the critical role of high-quality data, providing
    a solid foundation for analytics, machine learning, and informed decision-making.
    To ensure data quality, organizations implement a series of checks and measures
    at various stages of the data pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data entry/ingestion: Data sources are validated to ensure accurate and consistent
    data capture, primarily overseen by data engineers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data transformation: Quality checks are incorporated into the transformation
    layer to maintain data reliability and accuracy, typically managed by data engineers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data integration: Checks prevent data quality issues from propagating and support
    confidence in integrated data, involving data engineers and data scientists'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data consumption: Quality data input is vital for analytics and machine learning,
    impacting user trust and competitive advantage, and is driven by data scientists
    and analysts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These quality checks ensure that data adheres to defined standards, meets regulatory
    requirements, and is fit for its intended purpose. By implementing these checks,
    organizations maintain data accuracy, reliability, and transparency, facilitating
    better decision-making and ensuring data-driven success.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore how profiling tools can be used to continuously
    and automatically monitor data quality.
  prefs: []
  type: TYPE_NORMAL
