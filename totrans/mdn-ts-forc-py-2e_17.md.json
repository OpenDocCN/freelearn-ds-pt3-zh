["```py\nclass Attention(nn.Module, metaclass=ABCMeta):\n    def __init__(self, encoder_dim: int, decoder_dim: int):\n        super().__init__()\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim \n```", "```py\ndef forward(\n        self,\n        query: torch.Tensor,  # [batch_size, decoder_dim]\n        values: torch.Tensor,  # [batch_size, seq_length, encoder_dim]\n    ):\n        scores = self._get_scores(query, values)  # [batch_size, seq_length]\n        weights = torch.nn.functional.softmax(scores, dim=-1)\n        return (values*weights.unsqueeze(-1)).sum(dim=1)  # [batch_size, encoder_dim] \n```", "```py\nscores = (q @ v.transpose(1,2)) \n```", "```py\nscores = scores/math.sqrt(encoder_dim) \n```", "```py\nscores = (q @ self.W) @ v.transpose(1,2) \n```", "```py\nq = q.repeat(1, v.size(1), 1)  # [batch_size, seq_length, decoder_dim]\nscores = self.W_q(q) + self.W_v(v)  # [batch_size, seq_length, decoder_dim]\ntorch.tanh(scores) @ self.v  # [batch_size, seq_length] \n```", "```py\nself.W_q = torch.nn.Linear(self.decoder_dim, self.decoder_dim)\nself.W_v = torch.nn.Linear(self.encoder_dim, self.decoder_dim)\nself.v = torch.nn.Parameter(torch.FloatTensor(self.decoder_dim) \n```", "```py\nself.W = torch.nn.Linear(self.decoder_dim + self.encoder_dim, self.decoder_dim) \n```", "```py\nscores = self.W(\n            torch.cat([q, v], dim=-1)\n        )  # [batch_size, seq_length, decoder_dim] \n```", "```py\n01        y_hat = torch.zeros_like(y, device=y.device)\n02        dec_input = x[:, -1:, :]\n03        for i in range(y.size(1)):\n04            top_h = self._get_top_layer_hidden_state(h)\n05            context = self.attention(\n06                top_h.unsqueeze(1), o\n07            )\n08            dec_input = torch.cat((dec_input, context.unsqueeze(1)), dim=-1)\n09            out, h = self.decoder(dec_input, h)\n10            out = self.fc(out)\n11            y_hat[:, i, :] = out.squeeze(1)\n12            teacher_force = random.random() < self.hparams.teacher_forcing_ratio\n13            if teacher_force:\n14                dec_input = y[:, i, :].unsqueeze(1)\n15            else:\n16                dec_input = out \n```", "```py\nhidden_state[-1, :, :] \n```", "```py\ntorch.cat((hidden_state[-1, :, :], hidden_state[-2, :, :]), dim=-1) \n```", "```py\nself.input_projection = nn.Linear(\n            self.hparams.input_size, self.hparams.d_model, bias=False\n        ) \n```", "```py\nself.pos_encoder = PositionalEncoding(self.hparams.d_model) \n```", "```py\nself.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.hparams.d_model,\n            nhead=self.hparams.n_heads,\n            dropout=self.hparams.dropout,\n            dim_feedforward=self.hparams.d_model * self.hparams.ff_multiplier,\n            activation=self.hparams.activation,\n            batch_first=True,\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            self.encoder_layer, num_layers=self.hparams.n_layers\n        ) \n```", "```py\nself.decoder = nn.Sequential(nn.Linear(self.hparams.d_model, 100),\n            nn.ReLU(),\n            nn.Linear(100, self.hparams.multi_step_horizon)\n        ) \n```", "```py\nmask = self._generate_square_subsequent_mask(x.shape[1]).to(x.device) \n```", "```py\nmask = (torch.triu(torch.ones(5, 5)) == 1).transpose(0, 1) \n```", "```py\ntensor([[ True, False, False, False, False],\n        [ True,  True, False, False, False],\n        [ True,  True,  True, False, False],\n        [ True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True]]) \n```", "```py\nmask = (\n                mask.float()\n                .masked_fill(mask == 0, float(\"-inf\"))\n                .masked_fill(mask == 1, float(0.0))\n            ) \n```", "```py\n# Projecting input dimension to d_model\nx_ = self.input_projection(x)\n# Adding positional encoding\nx_ = self.pos_encoder(x_)\n# Encoding the input\nx_ = self.transformer_encoder(x_, mask)\n# Decoding the input\ny_hat = self.decoder(x_) \n```", "```py\ny = torch.cat([x[:, 1:, :], y], dim=1).squeeze(-1).unfold(1, y.size(1), 1) \n```", "```py\ny_hat = y_hat[:, -1, :].unsqueeze(1) \n```"]