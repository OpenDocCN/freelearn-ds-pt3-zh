- en: '*Chapter 2*: Working with Pandas DataFrames'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The time has come for us to begin our journey into the `pandas` universe. This
    chapter will get us comfortable working with some of the basic, yet powerful,
    operations we will be performing when conducting our data analyses with `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin with an introduction to the main `pandas`. Data structures provide
    us with a format for organizing, managing, and storing data. Knowledge of `pandas`
    data structures will prove infinitely helpful when it comes to troubleshooting
    or looking up how to perform an operation on the data. Keep in mind that these
    data structures are different from the standard Python data structures for a reason:
    they were created for specific analysis tasks. We must remember that a given method
    may only work on a certain data structure, so we need to be able to identify the
    best structure for the problem we are looking to solve.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will bring our first dataset into Python. We will learn how to collect
    data from an API, create `DataFrame` objects from other data structures in Python,
    read in files, and interact with databases. Initially, you may wonder why we would
    ever need to create a `DataFrame` object from other Python data structures; however,
    if we ever want to test something quickly, create our own data, pull data from
    an API, or repurpose Python code from another project, then we will find this
    knowledge indispensable. Finally, we will master ways to inspect, describe, filter,
    and summarize our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Pandas data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DataFrame objects from files, API requests, SQL queries, and other
    Python objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspecting DataFrame objects and calculating summary statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grabbing subsets of the data via selection, slicing, indexing, and filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding and removing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter materials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The files we will be working with in this chapter can be found in the GitHub
    repository at [https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_02](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_02).
    We will be working with earthquake data from the `data/` directory.
  prefs: []
  type: TYPE_NORMAL
- en: There are four CSV files and a SQLite database file, all of which will be used
    at different points throughout this chapter. The `earthquakes.csv` file contains
    data that's been pulled from the USGS API for September 18, 2018 through October
    13, 2018\. For our discussion of data structures, we will work with the `example_data.csv`
    file, which contains five rows and a subset of the columns from the `earthquakes.csv`
    file. The `tsunamis.csv` file is a subset of the data in the `earthquakes.csv`
    file for all earthquakes that were accompanied by tsunamis during the aforementioned
    date range. The `quakes.db` file contains a SQLite database with a single table
    for the tsunamis data. We will use this to learn how to read from and write to
    a database with `pandas`. Lastly, the `parsed.csv` file will be used for the end-of-chapter
    exercises, and we will also walk through the creation of it during this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The accompanying code for this chapter has been divided into six Jupyter Notebooks,
    which are numbered in the order they are to be used. They contain the code snippets
    we will be running throughout this chapter, along with the full output of any
    command that has to be trimmed for this text. Each time we are to switch notebooks,
    the text will indicate to do so.
  prefs: []
  type: TYPE_NORMAL
- en: In the `1-pandas_data_structures.ipynb` notebook, we will start learning about
    the main `pandas` data structures. Afterward, we will discuss the various ways
    to create `DataFrame` objects in the `2-creating_dataframes.ipynb` notebook. Our
    discussion on this topic will continue in the `3-making_dataframes_from_api_requests.ipynb`
    notebook, where we will explore the USGS API to gather data for use with `pandas`.
    After learning about how we can collect our data, we will begin to learn how to
    conduct `4-inspecting_dataframes.ipynb` notebook. Then, in the `5-subsetting_data.ipynb`
    notebook, we will discuss various ways to select and filter data. Finally, we
    will learn how to add and remove data in the `6-adding_and_removing_data.ipynb`
    notebook. Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python has several data structures already, such as tuples, lists, and dictionaries.
    Pandas provides two main structures to facilitate working with data: `Series`
    and `DataFrame`. The `Series` and `DataFrame` data structures each contain another
    `pandas` data structure, `Index`, that we must also be aware of. However, in order
    to understand these data structures, we need to first take a look at NumPy ([https://numpy.org/doc/stable/](https://numpy.org/doc/stable/)),
    which provides the n-dimensional arrays that `pandas` builds upon.'
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned data structures are implemented as Python `CapWords`, while
    objects are written in `snake_case`. (More Python style guidelines can be found
    at [https://www.python.org/dev/peps/pep-0008/](https://www.python.org/dev/peps/pep-0008/).)
  prefs: []
  type: TYPE_NORMAL
- en: We use a `pandas` function to read a CSV file into an object of the `DataFrame`
    class, but we use methods on our `DataFrame` objects to perform actions on them,
    such as dropping columns or calculating summary statistics. With `pandas`, we
    will often want to access the `pandas` object, such as dimensions, column names,
    data types, and whether it is empty.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For the remainder of this book, we will refer to `DataFrame` objects as dataframes,
    `Series` objects as series, and `Index` objects as index/indices, unless we are
    referring to the class itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, we will work in the `1-pandas_data_structures.ipynb` notebook.
    To begin, we will import `numpy` and use it to read the contents of the `example_data.csv`
    file into a `numpy.array` object. The data comes from the USGS API for earthquakes
    (source: [https://earthquake.usgs.gov/fdsnws/event/1/](https://earthquake.usgs.gov/fdsnws/event/1/)).
    Note that this is the only time we will use NumPy to read in a file and that this
    is being done for illustrative purposes only; the important part is to look at
    the way the data is represented with NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have our data in a NumPy array. Using the `shape` and `dtype` attributes,
    we can gather information about the dimensions of the array and the data types
    it contains, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of the entries in the array is a row from the CSV file. NumPy arrays contain
    a single data type (unlike lists, which allow mixed types); this allows for fast,
    vectorized operations. When we read in the data, we got an array of `numpy.void`
    objects, which are used to store flexible types. This is because NumPy had to
    store several different data types per row: four strings, a float, and an integer.
    Unfortunately, this means that we can''t take advantage of the performance improvements
    NumPy provides for single data type objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we want to find the maximum magnitudeâ€”we can use a `numpy.void` object.
    This makes a list, meaning that we can take the maximum using the `max()` function.
    We can use the `%%timeit` `%`) to see how long this implementation takes (times
    will vary):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that we should use a list comprehension whenever we would write a `for`
    loop with just a single line under it or want to run an operation against the
    members of some initial list. This is a rather simple list comprehension, but
    we can make them more complex with the addition of `if...else` statements. List
    comprehensions are an extremely powerful tool to have in our arsenal. More information
    can be found in the Python documentation at https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: '**IPython** ([https://ipython.readthedocs.io/en/stable/index.html](https://ipython.readthedocs.io/en/stable/index.html))
    provides an interactive shell for Python. Jupyter Notebooks are built on top of
    IPython. While knowledge of IPython is not required for this book, it can be helpful
    to be familiar with some of its functionality. IPython includes a tutorial in
    their documentation at [https://ipython.readthedocs.io/en/stable/interactive/](https://ipython.readthedocs.io/en/stable/interactive/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we create a NumPy array for each column instead, this operation is much
    easier (and more efficient) to perform. To do so, we will use a **dictionary comprehension**
    (https://www.python.org/dev/peps/pep-0274/) to make a dictionary where the keys
    are the column names and the values are NumPy arrays of the data. Again, the important
    part here is how the data is now represented using NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Grabbing the maximum magnitude is now simply a matter of selecting the `mag`
    key and calling the `max()` method on the NumPy array. This is nearly twice as
    fast as the list comprehension implementation, when dealing with just five entriesâ€”imagine
    how much worse the first attempt will perform on large datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this representation has other issues. Say we wanted to grab all the
    information for the earthquake with the maximum magnitude; how would we go about
    that? We need to find the index of the maximum, and then for each of the keys
    in the dictionary, grab that index. The result is now a NumPy array of strings
    (our numeric values were converted), and we are now in the format that we saw
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Consider how we would go about sorting the data by magnitude from smallest to
    largest. In the first representation, we would have to sort the rows by examining
    the third index. With the second representation, we would have to determine the
    order of the indices from the `mag` column, and then sort all the other arrays
    with those same indices. Clearly, working with several NumPy arrays containing
    different data types at once is a bit cumbersome; however, `pandas` builds on
    top of NumPy arrays to make this easier. Let's start our exploration of `pandas`
    with an overview of the `Series` data structure.
  prefs: []
  type: TYPE_NORMAL
- en: Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Series` class provides a data structure for arrays of a single type, just
    like the NumPy array. However, it comes with some additional functionality. This
    one-dimensional representation can be thought of as a column in a spreadsheet.
    We have a name for our column, and the data we hold in it is of the same type
    (since we are measuring the same variable):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the numbers on the left of the result; these correspond to the row number
    in the original dataset (offset by 1 since, in Python, we start counting at 0).
    These row numbers form the index, which we will discuss in the following section.
    Next to the row numbers, we have the actual value of the row, which, in this example,
    is a string indicating where the earthquake occurred. Notice that we have `dtype:
    object` next to the name of the `Series` object; this is telling us that the data
    type of `place` is `object`. A string will be classified as `object` in `pandas`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To access attributes of the `Series` object, we use attribute notation of the
    form `<object>.<attribute_name>`. The following are some common attributes we
    will access. Notice that `dtype` and `shape` are available, just as we saw with
    the NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 â€“ Commonly used series attributes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.1_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 â€“ Commonly used series attributes
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, `pandas` objects use NumPy arrays for their internal data
    representations. However, for some data types, `pandas` builds upon NumPy to create
    its own arrays (https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html).
    For this reason, depending on the data type, `values` can return either a `pandas.array`
    or a `numpy.array` object. Therefore, if we need to ensure we get a specific type
    back, it is recommended to use the `array` attribute or `to_numpy()` method, respectively,
    instead of `values`.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to bookmark the `pandas.Series` documentation ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html))
    for reference later. It contains more information on how to create a `Series`
    object, the full list of attributes and methods that are available, as well as
    a link to the source code. With this high-level introduction to the `Series` class,
    we are ready to move on to the `Index` class.
  prefs: []
  type: TYPE_NORMAL
- en: Index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The addition of the `Index` class makes the `Series` class significantly more
    powerful than a NumPy array. The `Index` class gives us row labels, which enable
    selection by row. Depending on the type, we can provide a row number, a date,
    or even a string to select our row. It plays a key role in identifying entries
    in the data and is used for a multitude of operations in `pandas`, as we will
    see throughout this book. We can access the index through the `index` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is a `RangeIndex` object. Its values start at `0` and end at
    `4`. The step of `1` indicates that the indices are all `1` apart, meaning that
    we have all the integers in that range. The default index class is `RangeIndex`;
    however, we can change the index, as we will discuss in [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*. Often, we will either work with an `Index` object
    of row numbers or date(time)s.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with `Series` objects, we can access the underlying data via the `values`
    attribute. Note that this `Index` object is built on top of a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the useful attributes of `Index` objects include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 â€“ Commonly used index attributes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.2_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 â€“ Commonly used index attributes
  prefs: []
  type: TYPE_NORMAL
- en: 'Both NumPy and `pandas` support arithmetic operations, which will be performed
    element-wise. NumPy will use the position in the array for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With `pandas`, this element-wise arithmetic is performed on matching index
    values. If we add a `Series` object with an index from `0` to `4` (stored in `x`)
    and another, `y`, from `1` to `5`, we will only get results were the indices align
    (`1` through `4`). In [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, we will discuss some ways to change and align the
    index so that we can perform these types of operations without losing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have had a primer on both the `Series` and `Index` classes, we are
    ready to learn about the `DataFrame` class. Note that more information on the
    `Index` class can be found in the respective documentation at [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html).
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the `Series` class, we essentially had columns of a spreadsheet, with
    the data all being of the same type. The `DataFrame` class builds upon the `Series`
    class and can have many columns, each with its own data type; we can think of
    it as representing the spreadsheet as a whole. We can turn either of the NumPy
    representations we built from the example data into a `DataFrame` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a dataframe of six series. Note the column before the `time`
    column; this is the `Index` object for the rows. When creating a `DataFrame` object,
    `pandas` aligns all the series to the same index. In this case, it is just the
    row number, but we could easily use the `time` column for this, which would enable
    some additional `pandas` features, as we will see in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 â€“ Our first dataframe'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.3_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 â€“ Our first dataframe
  prefs: []
  type: TYPE_NORMAL
- en: 'Our columns each have a single data type, but they don''t all share the same
    data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The values of the dataframe look very similar to the initial NumPy representation
    we had:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can access the column names via the `columns` attribute. Note that they
    are actually stored in an `Index` object as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are some commonly used dataframe attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 â€“ Commonly used dataframe attributes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.4_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 â€“ Commonly used dataframe attributes
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can also perform arithmetic on dataframes. For example, we can
    add `df` to itself, which will sum the numeric columns and concatenate the string
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Pandas will only perform the operation when both the index and column match.
    Here, `pandas` concatenated the string columns (`time`, `place`, `magType`, and
    `alert`) across dataframes. The numeric columns (`mag` and `tsunami`) were summed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 â€“ Adding dataframes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.5_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 â€“ Adding dataframes
  prefs: []
  type: TYPE_NORMAL
- en: More information on `DataFrame` objects and all the operations that can be performed
    directly on them is available in the official documentation at [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html);
    be sure to bookmark it for future reference. Now, we are ready to begin learning
    how to create `DataFrame` objects from a variety of sources.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pandas DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the data structures we will be working with, we can discuss
    the different ways we can create them. Before we dive into the code however, it's
    important to know how to get help right from Python. Should we ever find ourselves
    unsure of how to use something in Python, we can utilize the built-in `help()`
    function. We simply run `help()`, passing in the package, module, class, object,
    method, or function that we want to read the documentation on. We can, of course,
    look up the documentation online; however, in most cases, the `help()` will be
    equivalent to this since they are used to generate the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming we first ran `import pandas as pd`, we can run `help(pd)` to display
    information about the `pandas` package; `help(pd.DataFrame)` for all the methods
    and attributes of `DataFrame` objects (note we can also pass in a `DataFrame`
    object instead); and `help(pd.read_csv)` to learn more about the `pandas` function
    for reading CSV files into Python and how to use it. We can also try using the
    `dir()` function and the `__dict__` attribute, which will give us a list or dictionary
    of what's available, respectively; these might not be as useful as the `help()`
    function, though.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we can use `?` and `??` to get help, thanks to IPython, which
    is part of what makes Jupyter Notebooks so powerful. Unlike the `help()` function,
    we can use question marks by putting them after whatever we want to know more
    about, as if we were asking Python a question; for example, `pd.read_csv?` and
    `pd.read_csv??`. These three will yield slightly different outputs: `help()` will
    give us the docstring; `?` will give the docstring, plus some additional information,
    depending on what we are inquiring about; and `??` will give us even more information
    and, if possible, the source code behind it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now turn to the next notebook, `2-creating_dataframes.ipynb`, and import
    the packages we will need for the upcoming examples. We will be using `datetime`
    from the Python standard library, along with the third-party packages `numpy`
    and `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We have `pandas` package by referring to it with the alias we assign to be `pd`,
    which is the most common way of importing it. In fact, we can only refer to it
    as `pd`, since that is what we imported into the namespace. Packages need to be
    imported before we can use them; installation puts the files we need on our computer,
    but, in the interest of memory, Python won't load every installed package when
    we start it upâ€”just the ones we tell it to.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to begin using `pandas`. First, we will learn how to create
    `pandas` objects from other Python objects. Then, we will learn how to do so with
    flat files, tables in a database, and responses from API requests.
  prefs: []
  type: TYPE_NORMAL
- en: From a Python object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we cover all the ways we can create a `DataFrame` object from a Python
    object, we should learn how to make a `Series` object. Remember that a `Series`
    object is essentially a column in a `DataFrame` object, so, once we know this,
    it should be easy to understand how to create a `DataFrame` object. Say that we
    wanted to create a series of five random numbers between `0` and `1`. We could
    use NumPy to generate the random numbers as an array and create the series from
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: NumPy makes it very easy to generate numerical data. Aside from generating random
    numbers, we can use it to get evenly-spaced numbers in a certain range with the
    `np.linspace()` function; obtain a range of integers with the `np.arange()` function;
    sample from the standard normal with the `np.random.normal()` function; and easily
    create arrays of all zeros with the `np.zeros()` function and all ones with the
    `np.ones()` function. We will be using NumPy throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that the result is reproducible, we will set the seed here. The `Series`
    object with any list-like structure (such as NumPy arrays):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Making a `DataFrame` object is an extension of making a `Series` object; it
    will be composed of one or more series, and each will be distinctly named. This
    should remind us of dictionary-like structures in Python: the keys are the column
    names, and the values are the contents of the columns. Note that if we want to
    turn a single `Series` object into a `DataFrame` object, we can use its `to_frame()`
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: In computer science, a `__init__()` method. When we run `pd.Series()`, Python
    calls `pd.Series.__init__()`, which contains instructions for instantiating a
    new `Series` object. We will learn more about the `__init__()` method in [*Chapter
    7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial Analysis â€“ Bitcoin
    and the Stock Market*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since columns can all be different data types, let''s get a little fancy with
    this example. We are going to create a `DataFrame` object containing three columns,
    with five observations each:'
  prefs: []
  type: TYPE_NORMAL
- en: '`random`: Five random numbers between `0` and `1` as a NumPy array'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text`: A list of five strings or `None`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truth`: A list of five random Booleans'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also create a `DatetimeIndex` object with the `pd.date_range()` function.
    The index will contain five dates (`periods=5`), all one day apart (`freq='1D'`),
    ending with April 21, 2019 (`end`), and will be called `date`. Note that more
    information on the values the `pd.date_range()` function accepts for frequencies
    can be found at [https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).
  prefs: []
  type: TYPE_NORMAL
- en: 'All we have to do is package the columns in a dictionary using the desired
    column names as the keys and pass this in when we call the `pd.DataFrame()` constructor.
    The index gets passed as the `index` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: By convention, we use `_` to hold variables in a loop that we don't care about.
    Here, we use `range()` as a counter, and its values are unimportant. More information
    on the roles `_` plays in Python can be found at [https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc](https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc).
  prefs: []
  type: TYPE_NORMAL
- en: 'Having dates in the index makes it easy to select entries by date (or even
    in a date range), as we will see in [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 â€“ Creating a dataframe from a dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.6_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 â€“ Creating a dataframe from a dictionary
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where the data isn''t a dictionary, but rather a list of dictionaries,
    we can still use `pd.DataFrame()`. Data in this format is what we would expect
    when consuming from an API. Each entry in the list will be a dictionary, where
    the keys of the dictionary are the column names and the values of the dictionary
    are the values for that column at that index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a dataframe of three rows (one for each entry in the list) with
    two columns (one for each key in the dictionaries):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 â€“ Creating a dataframe from a list of dictionaries'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.7_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 â€“ Creating a dataframe from a list of dictionaries
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, `pd.DataFrame()` also works for lists of tuples. Note that we can
    also pass in the column names as a list through the `columns` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Each tuple is treated like a record and becomes a row in the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 â€“ Creating a dataframe from a list of tuples'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.8_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 â€“ Creating a dataframe from a list of tuples
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have the option of using `pd.DataFrame()` with NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This will have the effect of stacking each entry in the array as rows in a dataframe,
    giving us a result that's identical to *Figure 2.8*.
  prefs: []
  type: TYPE_NORMAL
- en: From a file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data we want to analyze will most often come from outside Python. In many
    cases, we may obtain a **data dump** from a database or website and bring it into
    Python to sift through it. A data dump gets its name from containing a large amount
    of data (possibly at a very granular level) and often not discriminating against
    any of it initially; for this reason, they can be unwieldy.
  prefs: []
  type: TYPE_NORMAL
- en: Often, these data dumps will come in the form of a text file (`.txt`) or a CSV
    file (`.csv`). Pandas provides many methods for reading in different types of
    files, so it is simply a matter of looking up the one that matches our file format.
    Our earthquake data is a CSV file; therefore, we use the `pd.read_csv()` function
    to read it in. However, we should always do an initial inspection of the file
    before attempting to read it in; this will inform us of whether we need to pass
    additional arguments, such as `sep` to specify the delimiter or `names` to provide
    the column names ourselves in the absence of a header row in the file.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '**Windows users**: Depending on your setup, the commands in the next few code
    blocks may not work. The notebook contains alternatives if you encounter issues.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can perform our due diligence directly in our Jupyter Notebook thanks to
    IPython, provided we prefix our commands with `!` to indicate they are to be run
    as shell commands. First, we should check how big the file is, both in terms of
    lines and in terms of bytes. To check the number of lines, we use the `wc` utility
    (word count) with the `â€“l` flag to count the number of lines. We have 9,333 rows
    in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s check the file''s size. For this task, we will use `ls` on the
    `data` directory. This will show us the list of files in that directory. We can
    add the `-lh` flag to get information about the files in a human-readable format.
    Finally, we send this output to the `grep` utility, which will help us isolate
    the files we want. This tells us that the `earthquakes.csv` file is 3.4 MB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that IPython also lets us capture the result of the command in a Python
    variable, so if we aren''t comfortable with pipes (`|`) or `grep`, we can do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s take a look at the top few rows to see if the file comes with headers.
    We will use the `head` utility and specify the number of rows with the `-n` flag.
    This tells us that the first row contains the headers for the data and that the
    data is delimited with commas (just because the file has the `.csv` extension
    does not mean it is comma-delimited):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note that we should also check the bottom rows to make sure there is no extraneous
    data that we will need to ignore by using the `tail` utility. This file is fine,
    so the result won't be reproduced here; however, the notebook contains the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we may be interested in seeing the column count in our data. While
    we could just count the fields in the first row of the result of `head`, we have
    the option of using the `awk` utility (for pattern scanning and processing) to
    count our columns. The `-F` flag allows us to specify the delimiter (a comma,
    in this case). Then, we specify what to do for each record in the file. We choose
    to print `NF`, which is a predefined variable whose value is the number of fields
    in the current record. Here, we say `exit` immediately after the print so that
    we print the number of fields in the first row of the file; then, we stop. This
    will look a little complicated, but this is by no means something we need to memorize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we know that the first line of the file contains headers and that the
    file is comma-separated, we can also count the columns by using `head` to get
    the headers and Python to parse them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The ability to run shell commands directly from our Jupyter Notebook dramatically
    streamlines our workflow. However, if we don't have past experience with the command
    line, it may be complicated to learn these commands initially. IPython has some
    helpful information on running shell commands in their documentation at [https://ipython.readthedocs.io/en/stable/interactive/reference.html#system-shell-access](https://ipython.readthedocs.io/en/stable/interactive/reference.html#system-shell-access).
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, we now know that the file is 3.4 MB and is comma-delimited with
    26 columns and 9,333 rows, with the first one being the header. This means that
    we can use the `pd.read_csv()` function with the defaults:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we aren''t limited to reading in data from files on our local machines;
    file paths can be URLs as well. As an example, let''s read in the same CSV file
    from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Pandas is usually very good at figuring out which options to use based on the
    input data, so we often won''t need to add arguments to this call; however, there
    are many options available should we need them, some of which include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 â€“ Helpful parameters when reading data from a file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.9_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.9 â€“ Helpful parameters when reading data from a file
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we will be working with CSV files; however, note that
    we can use the `read_excel()` function to read in Excel files, the `read_json()`
    function for `\t`), we can use the `read_csv()` function with the `sep` argument
    equal to the delimiter.
  prefs: []
  type: TYPE_NORMAL
- en: 'It would be remiss if we didn''t also learn how to save our dataframe to a
    file so that we can share it with others. To write our dataframe to a CSV file,
    we call its `to_csv()` method. We have to be careful here; if our dataframe''s
    index is just row numbers, we probably don''t want to write that to our file (it
    will have no meaning to the consumers of the data), but it is the default. We
    can write our data without the index by passing in `index=False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As with reading from files, `Series` and `DataFrame` objects have methods to
    write data to Excel (`to_excel()`) and JSON files (`to_json()`). Note that, while
    we use functions from `pandas` to read our data in, we must use methods to write
    our data; the reading functions create the `pandas` objects that we want to work
    with, but the writing methods are actions that we take using the `pandas` object.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The preceding file paths to read from and write to were `/home/myuser/learning/hands_on_pandas/data.csv`
    and our current directory is `/home/myuser/learning/hands_on_pandas`, then we
    can simply use the relative path of `data.csv` as the file path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas provides us with capabilities to read and write from many other data
    sources, including databases, which we will discuss next; pickle files (containing
    serialized Python objectsâ€”see the *Further reading* section for more information);
    and HTML pages. Be sure to check out the following resource in the `pandas` documentation
    for the full list of capabilities: [https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html).'
  prefs: []
  type: TYPE_NORMAL
- en: From a database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pandas can interact with SQLite databases without the need for us to install
    any additional packages; however, the SQLAlchemy package needs to be installed
    in order to interact with other database flavors. Interaction with a SQLite database
    can be achieved by opening a connection to the database using the `sqlite3` module
    in the Python standard library and then using either the `pd.read_sql()` function
    to query the database or the `to_sql()` method on a `DataFrame` object to write
    it to the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we read from a database, let''s write to one. We simply call `to_sql()`
    on our dataframe, telling it which table to write to, which database connection
    to use, and how to handle if the table already exists. There is already a SQLite
    database in the folder for this chapter in this book''s GitHub repository: `data/quakes.db`.
    Note that, to create a new database, we can change `''data/quakes.db''` to the
    path for the new database file. Let''s write the tsunami data from the `data/tsunamis.csv`
    file to a table in the database called `tsunamis`, replacing the table if it already
    exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Querying the database is just as easy as writing to it. Note this will require
    knowledge of `pandas` compares to SQL and [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, for some examples of how `pandas` actions relate
    to SQL statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s query our database for the full `tsunamis` table. When we write a SQL
    query, we first state the columns that we want to select, which in our case is
    all of them, so we write `"SELECT *"`. Next, we state the table to select the
    data from, which for us is `tsunamis`, so we add `"FROM tsunamis"`. This is our
    full query now (of course, it can get much more complicated than this). To actually
    query the database, we use `pd.read_sql()`, passing in our query and the database
    connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the tsunamis data in a dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 â€“ Reading data from a database'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.10_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 â€“ Reading data from a database
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The `connection` object we created in both code blocks is an example of a `with`
    statement, automatically handles cleanup after the code in the block executes
    (closing the connection, in this case). This makes cleanup easy and makes sure
    we don't leave any loose ends. Be sure to check out `contextlib` from the standard
    library for utilities using the `with` statement and context managers. The documentation
    is at [https://docs.python.org/3/library/contextlib.html](https://docs.python.org/3/library/contextlib.html).
  prefs: []
  type: TYPE_NORMAL
- en: From an API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can now easily create `Series` and `DataFrame` objects from data we have
    in Python or from files we obtain, but how can we get data from online resources,
    such as APIs? There is no guarantee that each data source will give us data in
    the same format, so we must remain flexible in our approach and be comfortable
    examining the data source to find the appropriate import method. In this section,
    we will request some earthquake data from the USGS API and see how we can make
    a dataframe out of the result. In [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, we will work with another API to gather weather
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, we will be working in the `3-making_dataframes_from_api_requests.ipynb`
    notebook, so we have to import the packages we need once again. As with the previous
    notebook, we need `pandas` and `datetime`, but we also need the `requests` package
    to make API requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will make a `GET` request to the USGS API for a JSON payload (a dictionary-like
    response containing the data that''s sent with a request or response) by specifying
    the format of `geojson`. We will ask for earthquake data for the last 30 days
    (we can use `dt.timedelta` to perform arithmetic on `datetime` objects). Note
    that we are using `yesterday` as the end of our date range, since the API won''t
    have complete information for today yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '`GET` is an HTTP method. This action tells the server we want to read some
    data. Different APIs may require that we use different methods to get the data;
    some will require a `POST` request, where we authenticate with the server. You
    can read more about API requests and HTTP methods at [https://nordicapis.com/ultimate-guide-to-all-9-standard-http-methods/](https://nordicapis.com/ultimate-guide-to-all-9-standard-http-methods/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we try to create a dataframe out of this, we should make sure that our
    request was successful. We can do this by checking the `status_code` attribute
    of the `response` object. A listing of status codes and their meanings can be
    found at [https://en.wikipedia.org/wiki/List_of_HTTP_status_codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).
    A `200` response will indicate that everything is OK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Our request was successful, so let''s see what the data we got looks like.
    We asked the API for a JSON payload, which is essentially a dictionary, so we
    can use dictionary methods on it to get more information about its structure.
    This is going to be a lot of data; hence, we don''t want to print it to the screen
    just to inspect it. We need to isolate the JSON payload from the HTTP response
    (stored in the `response` variable), and then look at the keys to view the main
    sections of the resulting data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inspect what kind of data we have as values for each of these keys;
    one of them will be the data we are after. The `metadata` portion tells us some
    information about our request. While this can certainly be useful, it isn''t what
    we are after right now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `features` key looks promising; if this does indeed contain all our data,
    we should check what type it is so that we don''t end up trying to print everything
    to the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This key contains a list, so let''s take a look at the first entry to see if
    this is the data we want. Note that the USGS data may be altered or added to for
    dates in the past as more information on the earthquakes comes to light, meaning
    that querying for the same date range may yield a different number of results
    later on. For this reason, the following is an example of what an entry looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This is definitely the data we are after, but do we need all of it? Upon closer
    inspection, we only really care about what is inside the `properties` dictionary.
    Now, we have a problem because we have a list of dictionaries where we only want
    a specific key from inside them. How can we pull this information out so that
    we can make our dataframe? We can use a list comprehension to isolate the `properties`
    section from each of the dictionaries in the `features` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we are ready to create our dataframe. Pandas knows how to handle data
    in this format already (a list of dictionaries), so all we have to do is pass
    in the data when we call `pd.DataFrame()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know how to create dataframes from a variety of sources, we can
    start learning how to work with them.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting a DataFrame object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing we should do when we read in our data is inspect it; we want
    to make sure that our dataframe isn't empty and that the rows look as we would
    expect. Our main goal is to verify that it was read in properly and that all the
    data is there; however, this initial inspection will also give us ideas with regard
    to where we should direct our data wrangling efforts. In this section, we will
    explore ways in which we can inspect our dataframes in the `4-inspecting_dataframes.ipynb`
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this is a new notebook, we must once again handle our setup. This time,
    we need to import `pandas` and `numpy`, as well as read in the CSV file with the
    earthquake data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Examining the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we want to make sure that we actually have data in our dataframe. We
    can check the `empty` attribute to find out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, so good; we have data. Next, we should check how much data we read
    in; we want to know the number of observations (rows) and the number of variables
    (columns) we have. For this task, we use the `shape` attribute. Our data contains
    9,332 observations of 26 variables, which matches our initial inspection of the
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use the `columns` attribute to see the names of the columns in
    our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Having a list of columns doesn't necessarily mean that we know what all of them
    mean. Especially in cases where our data comes from the Internet, be sure to read
    up on what the columns mean before drawing any conclusions. Information on the
    fields in the `geojson` format, including what each field in the JSON payload
    means (along with some example values), can be found on the USGS website at [https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php](https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php).
  prefs: []
  type: TYPE_NORMAL
- en: 'We know the dimensions of our data, but what does it actually look like? For
    this task, we can use the `head()` and `tail()` methods to look at the top and
    bottom rows, respectively. This will default to five rows, but we can change this
    by passing a different number to the method. Let''s take a look at the first few
    rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the first five rows we get using `head()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 â€“ Examining the top five rows of a dataframe'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.11_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 â€“ Examining the top five rows of a dataframe
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the last two rows, we use the `tail()` method and pass `2` as the number
    of rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 â€“ Examining the bottom two rows of a dataframe'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.12_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 â€“ Examining the bottom two rows of a dataframe
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: By default, when we print dataframes with many columns in a Jupyter Notebook,
    only a subset of them will be displayed. This is because `pandas` has a limit
    on the number of columns it will show. We can modify this behavior using `pd.set_option('display.max_columns',
    <new_value>)`. Consult the documentation at [https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html](https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html)
    for additional information. The notebook also contains a few example commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `dtypes` attribute to see the data types of the columns, which
    makes it easy to see when columns are being stored as the wrong type. (Remember
    that strings will be stored as `object`.) Here, the `time` column is stored as
    an integer, which is something we will learn how to fix in [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we can use the `info()` method to see how many non-null entries of
    each column we have and get information on our index. `pandas`, will typically
    be represented as `None` for objects and `NaN` (`float` or `integer` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: After this initial inspection, we know a lot about the structure of our data
    and can now begin to try and make sense of it.
  prefs: []
  type: TYPE_NORMAL
- en: Describing and summarizing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we''ve examined the structure of the `DataFrame` object we created
    from the earthquake data, but we don''t know anything about the data other than
    what a couple of rows look like. The next step is to calculate summary statistics,
    which will help us get to know our data better. Pandas provides several methods
    for easily doing so; one such method is `describe()`, which also works on `Series`
    objects if we are only interested in a particular column. Let''s get a summary
    of the numeric columns in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the 5-number summary, along with the count, mean, and standard
    deviation of the numeric columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 â€“ Calculating summary statistics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.13_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 â€“ Calculating summary statistics
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If we want different percentiles, we can pass them in with the `percentiles`
    argument. For example, if we wanted only the 5th and 95th percentiles, we would
    run `df.describe(percentiles=[0.05, 0.95])`. Note we will still get the 50th percentile
    back because that is the median.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, `describe()` won''t give us any information about the columns of
    type `object`, but we can either provide `include=''all''` as an argument or run
    it separately for the data of type `np.object`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'When describing non-numeric data, we still get the count of non-null occurrences
    (**count**); however, instead of the other summary statistics, we get the number
    of unique values (**unique**), the mode (**top**), and the number of times the
    mode was observed (**freq**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 â€“ Summary statistics for categorical columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.14_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.14 â€“ Summary statistics for categorical columns
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The `describe()` method only gives us summary statistics for non-null values.
    This means that, if we had 100 rows and half of our data was null, then the average
    would be calculated as the sum of the 50 non-null rows divided by 50.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easy to get a snapshot of our data using the `describe()` method, but
    sometimes, we just want a particular statistic, either for a specific column or
    for all the columns. Pandas makes this a cinch as well. The following table includes
    methods that will work for both `Series` and `DataFrame` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 â€“ Helpful calculation methods for series and dataframes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.15_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.15 â€“ Helpful calculation methods for series and dataframes
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Python makes it easy to count how many times something is `True`. Under the
    hood, `True` evaluates to `1` and `False` evaluates to `0`. Therefore, we can
    run the `sum()` method on a series of Booleans and get the count of `True` outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `Series` objects, we have some additional methods for describing our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`unique()`: Returns the distinct values of the column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value_counts()`: Returns a frequency table of the number of times each unique
    value in a given column appears, or, alternatively, the percentage of times each
    unique value appears when passed `normalize=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode()`: Returns the most common value of the column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consulting the USGS API documentation for the `alert` field (which can be found
    at [https://earthquake.usgs.gov/data/comcat/data-eventterms.php#alert](https://earthquake.usgs.gov/data/comcat/data-eventterms.php#alert))
    tells us that it can be `'green'`, `'yellow'`, `'orange'`, or `'red'` (when populated),
    and that it is the alert level from the `alert` column is a string of two unique
    values and that the most common value is `'green'`, with many null values. What
    is the other unique value, though?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we understand what this field means and the values we have in our
    data, we expect there to be far more `''green''` than `''red''`; we can check
    our intuition with a frequency table by using `value_counts()`. Notice that we
    only get counts for the non-null entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `Index` objects also have several methods that can help us describe
    and summarize our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 â€“ Helpful methods for the index'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.16_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16 â€“ Helpful methods for the index
  prefs: []
  type: TYPE_NORMAL
- en: When we used `unique()` and `value_counts()`, we got a preview of how to select
    subsets of our data. Now, let's go into more detail and cover selection, slicing,
    indexing, and filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Grabbing subsets of the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned how to work with and summarize the data as a whole;
    however, we will often be interested in performing operations and/or analyses
    on subsets of our data. There are many types of subsets we may look to isolate
    from our data, such as selecting only specific columns or rows as a whole or when
    a specific criterion is met. In order to obtain subsets of the data, we need to
    be familiar with selection, slicing, indexing, and filtering.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, we will work in the `5-subsetting_data.ipynb` notebook. Our
    setup is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Selecting columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we saw an example of column selection when we looked
    at the unique values in the `alert` column; we accessed the column as an attribute
    of the dataframe. Remember that a column is a `Series` object, so, for example,
    selecting the `mag` column in the earthquake data gives us the magnitudes of the
    earthquakes as a `Series` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Pandas provides us with a few ways to select columns. An alternative to using
    attribute notation to select a column is to access it with a dictionary-like notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We can also select columns using the `get()` method. This has the benefits of
    not raising an error if the column doesn't exist and allowing us to provide a
    backup valueâ€”the default is `None`. For example, if we call `df.get('event', False)`,
    it will return `False` since we don't have an `event` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we aren''t limited to selecting one column at a time. By passing
    a list to the dictionary lookup, we can select many columns, giving us a `DataFrame`
    object that is a subset of our original dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the full `mag` and `title` columns from the original dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 â€“ Selecting multiple columns of a dataframe'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.17_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.17 â€“ Selecting multiple columns of a dataframe
  prefs: []
  type: TYPE_NORMAL
- en: 'String methods are a very powerful way to select columns. For example, if we
    wanted to select all the columns that start with `mag`, along with the `title`
    and `time` columns, we would do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We get back a dataframe composed of the four columns that matched our criteria.
    Notice how the columns were returned in the order we requested, which is not the
    order they originally appeared in. This means that if we want to reorder our columns,
    all we have to do is select them in the order we want them to appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18 â€“ Selecting columns based on names'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.18_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.18 â€“ Selecting columns based on names
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break this example down. We used a list comprehension to go through
    each of the columns in the dataframe and only keep the ones whose names started
    with `mag`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we added this result to the other two columns we wanted to keep (`title`
    and `time`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we were able to use this list to run the actual column selection on
    the dataframe, resulting in the dataframe in *Figure 2.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: A complete list of string methods can be found in the Python 3 documentation
    at [https://docs.python.org/3/library/stdtypes.html#string-methods](https://docs.python.org/3/library/stdtypes.html#string-methods).
  prefs: []
  type: TYPE_NORMAL
- en: Slicing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we want to extract certain rows (slices) from our dataframe, we use `DataFrame`
    slicing works similarly to slicing with other Python objects, such as lists and
    tuples, with the first index being inclusive and the last index being exclusive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'When specifying a slice of `100:103`, we get back rows `100`, `101`, and `102`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 â€“ Slicing a dataframe to extract specific rows'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.19_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.19 â€“ Slicing a dataframe to extract specific rows
  prefs: []
  type: TYPE_NORMAL
- en: 'We can combine our row and column selections by using what is known as **chaining**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we selected the `title` and `time` columns for all the rows, and then
    we pulled out rows with indices `100`, `101`, and `102`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20 â€“ Selecting specific rows and columns with chaining'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.20_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.20 â€“ Selecting specific rows and columns with chaining
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, we selected the columns and then sliced the rows,
    but the order doesn''t matter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Note that we can slice on whatever is in our index; however, it would be difficult
    to determine the string or date after the last one we want, so with `pandas`,
    slicing dates and strings is different from integer slicing and is inclusive of
    both endpoints. Date slicing will work as long as the strings we provide can be
    parsed into a `datetime` object. In [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, we'll see some examples of this and also learn how
    to change what we use as the index, thus making this type of slicing possible.
  prefs: []
  type: TYPE_NORMAL
- en: If we decide to use chaining to update the values in our data, we will find
    `pandas` complaining that we aren't doing so correctly (even if it works). This
    is to warn us that setting data with a sequential selection may not give us the
    result we anticipate. (More information can be found at [https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy).)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s trigger this warning to understand it better. We will try to update
    the entries in the `title` column for a few earthquakes so that they''re in lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'As indicated by the warning, to be an effective `pandas` user, it''s not enough
    to know selection and slicingâ€”we must also master **indexing**. Since this is
    just a warning, our values have been updated, but this may not always be the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's discuss how to use indexing to set values properly.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pandas indexing operations provide us with a one-method way to select both
    the rows and the columns we want. We can use `loc[]` and `iloc[]` to subset our
    dataframe using label-based or integer-based lookups, respectively. A good way
    to remember the difference is to think of them as **loc**ation versus **i**nteger
    **loc**ation. For all indexing methods, we provide the row indexer first and then
    the column indexer, with a comma separating them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that by using `loc[]`, as indicated in the warning message, we no longer
    trigger any warnings from `pandas` for this operation. We also changed the end
    index from `113` to `112` because `loc[]` is inclusive of endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We can select all the rows (columns) if we use `:` as the row (column) indexer,
    just like with regular Python slicing. Let''s grab all the rows of the `title`
    column with `loc[]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We can select multiple rows and columns at the same time with `loc[]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'This leaves us with rows `10` through `15` for the `title` and `mag` columns
    only:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 â€“ Selecting specific rows and columns with indexing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.21_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.21 â€“ Selecting specific rows and columns with indexing
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen, when using `loc[]`, our end index is inclusive. This isn''t
    the case with `iloc[]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Observe how we had to provide a list of integers to select the same columns;
    these are the column numbers (starting from `0`). Using `iloc[]`, we lost the
    row at index `15`; this is because the integer slicing that `iloc[]` employs is
    exclusive of the end index, as with Python slicing syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.22 â€“ Selecting specific rows and columns by position'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.22_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.22 â€“ Selecting specific rows and columns by position
  prefs: []
  type: TYPE_NORMAL
- en: 'We aren''t limited to using the slicing syntax for the rows, though; columns
    work as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'By using slicing, we can easily grab adjacent rows and columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.23 â€“ Selecting ranges of adjacent rows and columns by position'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.23_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.23 â€“ Selecting ranges of adjacent rows and columns by position
  prefs: []
  type: TYPE_NORMAL
- en: 'When using `loc[]`, this slicing can be done on the column names as well. This
    gives us many ways to achieve the same result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'To look up scalar values, we use `at[]` and `iat[]`, which are faster. Let''s
    select the magnitude (the `mag` column) of the earthquake that was recorded in
    the row at index `10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The magnitude column has a column index of `8`; therefore, we can also look
    up the magnitude with `iat[]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: So far, we have seen how to get subsets of our data using row/column names and
    ranges, but how do we only take the data that meets some criteria? For this, we
    need to learn how to filter our data.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pandas gives us a few options for filtering our data, including `True`/`False`
    values; `pandas` can use this to select the appropriate rows/columns for us. There
    are endless possibilities for creating Boolean masksâ€”all we need is some code
    that returns one Boolean value for each row. For example, we can see which entries
    in the `mag` column had a magnitude greater than two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'While we can run this on the entire dataframe, it wouldn''t be too useful with
    our earthquake data since we have columns of various data types. However, we can
    use this strategy to get the subset of the data where the magnitude of the earthquake
    was greater than or equal to 7.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Our resulting dataframe has just two rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.24 â€“ Filtering with Boolean masks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.24_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.24 â€“ Filtering with Boolean masks
  prefs: []
  type: TYPE_NORMAL
- en: 'We got back a lot of columns we didn''t need, though. We could have chained
    a column selection to the end of the last code snippet; however, `loc[]` can handle
    Boolean masks as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The following dataframe has been filtered so that it only contains relevant
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.25 â€“ Indexing with Boolean masks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.25_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.25 â€“ Indexing with Boolean masks
  prefs: []
  type: TYPE_NORMAL
- en: 'We aren''t limited to just one criterion, either. Let''s grab the earthquakes
    with a red alert and a tsunami. To combine masks, we need to surround each of
    our conditions with parentheses and use the `&`) to require *both* to be true:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'There was only a single earthquake in the data that met our criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.26 â€“ Combining filters with AND'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.26_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.26 â€“ Combining filters with AND
  prefs: []
  type: TYPE_NORMAL
- en: 'If, instead, we want *at least one* of our conditions to be true, we can use
    the `|`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that this filter is much less restrictive since, while both conditions
    can be true, we only require that one of them is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.27 â€“ Combining filters with OR'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.27_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.27 â€“ Combining filters with OR
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: When creating Boolean masks, we must use bitwise operators (`&`, `|`, `~`) instead
    of logical operators (`and`, `or`, `not`). A good way to remember this is that
    we want a Boolean for each item in the series we are testing rather than a single
    Boolean. For example, with the earthquake data, if we want to select the rows
    where the magnitude is greater than 1.5, then we want one Boolean value for each
    row, indicating whether the row should be selected. In cases where we want a single
    value for the data, perhaps to summarize it, we can use `any()`/`all()` to condense
    a Boolean series into a single Boolean value that can be used with logical operators.
    We will work with the `any()` and `all()` methods in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous two examples, our conditions involved equality; however, we
    are by no means limited to this. Let''s select all the earthquakes in Alaska where
    we have a non-null value for the `alert` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'All the earthquakes in Alaska that have a value for `alert` are `green`, and
    some were accompanied by tsunamis, with the highest magnitude being 5.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.28 â€“ Creating Boolean masks with non-numeric columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.28_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.28 â€“ Creating Boolean masks with non-numeric columns
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break down how we got this. `Series` objects have some string methods
    that can be accessed via the `str` attribute. Using this, we can create a Boolean
    mask of all the rows where the `place` column contained the word `Alaska`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'To get all the rows where the `alert` column was not null, we used the `Series`
    object''s `notnull()` method (this works for `DataFrame` objects as well) to create
    a Boolean mask of all the rows where the `alert` column was not null:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `~`), also called `True` values `False` and vice versa. So, `df.alert.notnull()`
    and `~df.alert.isnull()`are equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, like we did previously, we combine the two conditions with the `&` operator
    to complete our mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Note that we aren't limited to checking if each row contains text; we can use
    regular expressions as well. `r` character outside the quotes; this lets Python
    know it is a `\`) characters in the string without Python thinking we are trying
    to escape the character immediately following it (such as when we use `\n` to
    mean a new line character instead of the letter `n`). This makes it perfect for
    use with regular expressions. The `re` module in the Python standard library ([https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html))
    handles regular expression operations; however, `pandas` lets us use regular expressions
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a regular expression, let''s select all the earthquakes in California
    that have magnitudes of at least 3.8\. We need to select entries in the `place`
    column that end in `CA` or `California` because the data isn''t consistent (we
    will look at how to fix this in the next section). The `$` character means *end*
    and `''CA$''` gives us entries that end in `CA`, so we can use `''CA|California$''`
    to get entries that end in either:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'There were only two earthquakes in California with magnitudes greater than
    3.8 during the time period we are studying:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.29 â€“ Filtering with regular expressions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.29_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.29 â€“ Filtering with regular expressions
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'Regular expressions are extremely powerful, but unfortunately, also difficult
    to get right. It is often helpful to grab some sample lines for parsing and use
    a website to test them. Note that regular expressions come in many flavors, so
    be sure to select Python. This website supports Python flavor regular expressions,
    and also provides a nice cheat sheet on the side: https://regex101.com/.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we want to get all earthquakes with magnitudes between 6.5 and 7.5?
    We could use two Boolean masksâ€”one to check for magnitudes greater than or equal
    to 6.5, and another to check for magnitudes less than or equal to 7.5â€”and then
    combine them with the `&` operator. Thankfully, `pandas` makes this type of mask
    much easier to create by providing us with the `between()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The result contains all the earthquakes with magnitudes in the range [6.5,
    7.5]â€”it''s inclusive of both ends by default, but we can pass in `inclusive=False`
    to change this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.30 â€“ Filtering using a range of values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.30_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.30 â€“ Filtering using a range of values
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `isin()` method to create a Boolean mask for values that match
    one of a list of values. This means that we don''t have to write one mask for
    each of the values that we could match and then use `|` to join them. Let''s utilize
    this to filter on the `magType` column, which indicates the measurement technique
    that was used to quantify the earthquake''s magnitude. We will take a look at
    earthquakes measured with either the `mw` or `mwb` magnitude type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'We have two earthquakes that were measured with the `mwb` magnitude type and
    four that were measured with the `mw` magnitude type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.31 â€“ Filtering using membership in a list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.31_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.31 â€“ Filtering using membership in a list
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have been filtering on specific values, but suppose we wanted to
    see all the data for the lowest-magnitude and highest-magnitude earthquakes. Rather
    than finding the minimum and maximum of the `mag` column first and then creating
    a Boolean mask, we can ask `pandas` to give us the index where these values occur,
    and easily filter to grab the full rows. We can use `idxmin()` and `idxmax()`
    for the indices of the minimum and maximum, respectively. Let''s grab the row
    numbers for the lowest-magnitude and highest-magnitude earthquakes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use these indices to grab the rows themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'The minimum magnitude earthquake occurred in Alaska and the highest magnitude
    earthquake occurred in Indonesia, accompanied by a tsunami. We will discuss the
    earthquake in Indonesia in [*Chapter 5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106),
    *Visualizing Data with Pandas and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.32 â€“ Filtering to isolate the rows containing the minimum and maximum
    of a column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.32_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.32 â€“ Filtering to isolate the rows containing the minimum and maximum
    of a column
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Be advised that the `filter()` method does not filter the data according to
    its values, as we did in this section; rather, it can be used to subset rows or
    columns based on their names. Examples with `DataFrame` and `Series` objects can
    be found in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Adding and removing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we frequently selected a subset of the columns, but
    if columns/rows aren't useful to us, we should just get rid of them. We also frequently
    selected data based on the value of the `mag` column; however, if we had made
    a new column holding the Boolean values for later selection, we would have only
    needed to calculate the mask once. Very rarely will we get data where we neither
    want to add nor remove something.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin adding and removing data, it''s important to understand that
    while most methods will return a new `DataFrame` object, some will be in-place
    and change our data. If we write a function where we pass in a dataframe and change
    it, it will change our original dataframe as well. Should we find ourselves in
    a situation where we don''t want to change the original data, but rather want
    to return a new copy of the data that has been modified, we must be sure to copy
    our dataframe before making any changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: By default, `df.copy()` makes a `deep=False`, we can obtain a **shallow copy**â€”changes
    to the shallow copy affect the original and vice versa. We will almost always
    want the deep copy, since we can change it without affecting the original. More
    information can be found in the documentation at [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s turn to the final notebook, `6-adding_and_removing_data.ipynb`,
    and get set up for the remainder of this chapter. We will once again be working
    with the earthquake data, but this time, we will only read in a subset of the
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Creating new data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating new columns can be achieved in the same fashion as variable assignment.
    For example, we can create a column to indicate the source of our data; since
    all our data came from the same source, we can take advantage of **broadcasting**
    to set every row of this column to the same value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'The new column is created to the right of the original columns, with a value
    of `USGS API` for every row:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.33 â€“ Adding a new column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.33_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.33 â€“ Adding a new column
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We cannot create the column with attribute notation (`df.source`) because the
    dataframe doesn't have that attribute yet, so we must use dictionary notation
    (`df['source']`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We aren''t limited to broadcasting one value to the entire column; we can have
    the column hold the result of Boolean logic or a mathematical equation. For example,
    if we had data on distance and time, we could create a speed column that is the
    result of dividing the distance column by the time column. With our earthquake
    data, let''s create a column that tells us whether the earthquake''s magnitude
    was negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the new column has been added to the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.34 â€“ Storing a Boolean mask in a new column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.34_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.34 â€“ Storing a Boolean mask in a new column
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we saw that the `place` column has some data consistency
    issuesâ€”we have multiple names for the same entity. In some cases, earthquakes
    occurring in California are marked as `CA` and as `California` in others. Needless
    to say, this is confusing and can easily cause issues for us if we don''t carefully
    inspect our data beforehand. For example, by just selecting `CA`, we miss out
    on 124 earthquakes marked as `California`. This isn''t the only place with an
    issue either (`Nevada` and `NV` are also both present). By using a regular expression
    to extract everything in the `place` column after the comma, we can see some of
    the issues firsthand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: If we want to treat countries and anything near them as a single entity, we
    have some additional work to do (see `Ecuador` and `Ecuador region`). In addition,
    our naive attempt at parsing the location by looking at the information after
    the comma appears to have failed; this is because, in some cases, we don't have
    a comma. We will need to change our approach to parsing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an `df.place.unique()`), we can simply look through and infer how to
    properly match up these names. Then, we can use the `replace()` method to replace
    patterns in the `place` column as we see fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can check the parsed places we are left with. Notice that there is
    arguably still more to fix here with `South Georgia and South Sandwich Islands`
    and `South Sandwich Islands`. We could address this with another call to `replace()`;
    however, this goes to show that entity recognition can be quite challenging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In practice, entity recognition can be an extremely difficult problem, where
    we may look to employ **natural language processing** (**NLP**) algorithms to
    help us. While this is well beyond the scope of this book, more information can
    be found at https://www.kdnuggets.com/2018/12/introduction-named-entity-recognition.html.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas also provides us with a way to make many new columns at once in one
    method call. With the `assign()` method, the arguments are the names of the columns
    we want to create (or overwrite), and the values are the data for the columns.
    Let''s create two new columns; one will tell us if the earthquake happened in
    California, and the other will tell us if it happened in Alaska. Rather than just
    show the first five entries (which are all in California), we will use `sample()`
    to randomly select five rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `assign()` doesn''t change our original dataframe; instead, it returns
    a new `DataFrame` object with these columns added. If we want to replace our original
    dataframe with this, we just use variable assignment to store the result of `assign()`
    in `df` (for example, `df = df.assign(...)`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.35 â€“ Creating multiple new columns at once'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.35_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.35 â€“ Creating multiple new columns at once
  prefs: []
  type: TYPE_NORMAL
- en: 'The `assign()` method also accepts `assign()` will pass the dataframe into
    the `lambda` function as `x`, and we can work from there. This makes it possible
    for us to use the columns we are creating in `assign()` to calculate others. For
    example, let''s once again create the `in_ca` and `in_alaska` columns, but this
    time also create a new column, `neither`, which is `True` if both `in_ca` and
    `in_alaska` are `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that `~` is the bitwise negation operator, so this allows us to create
    a column with the result of `NOT in_ca AND NOT in_alaska` per row:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.36 â€“ Creating multiple new columns at once with lambda functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.36_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.36 â€“ Creating multiple new columns at once with lambda functions
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: When working with `pandas`, it's crucial to get comfortable with `lambda` functions,
    as they can be used with much of the functionality available and will dramatically
    improve the quality and readability of the code. Throughout this book, we will
    see various places where `lambda` functions can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have seen how to add new columns, let''s take a look at adding
    new rows. Say we were working with two separate dataframes; one with earthquakes
    accompanied by tsunamis and the other with earthquakes without tsunamis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'If we wanted to look at earthquakes as a whole, we would want to concatenate
    the dataframes into a single one. To append rows to the bottom of our dataframe,
    we can either use `pd.concat()` or the `append()` method of the dataframe itself.
    The `concat()` function allows us to specify the axis that the operation will
    be performed alongâ€”`0` for appending rows to the bottom of the dataframe, and
    `1` for appending to the right of the last column with respect to the leftmost
    `pandas` object in the concatenation list. Let''s use `pd.concat()` with the default
    `axis` of `0` for rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the previous result is equivalent to running the `append()` method
    on the dataframe. This still returns a new `DataFrame` object, but it saves us
    from having to remember which axis is which, since `append()` is actually a wrapper
    around the `concat()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, we have been working with a subset of the columns from the CSV file,
    but suppose that we now want to work with some of the columns we ignored when
    we read in the data. Since we have added new columns in this notebook, we won''t
    want to read in the file and perform those operations again. Instead, we will
    concatenate along the columns (`axis=1`) to add back what we are missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the indices of the dataframes align, the additional columns are placed
    to the right of our original columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.37 â€“ Concatenating columns with matching indices'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.37_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.37 â€“ Concatenating columns with matching indices
  prefs: []
  type: TYPE_NORMAL
- en: 'The `concat()` function uses the index to determine how to concatenate the
    values. If they don''t align, this will generate additional rows because `pandas`
    won''t know how to align them. Say we forgot that our original dataframe had the
    row numbers as the index, and we read in the additional columns by setting the
    `time` column as the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Despite the additional columns containing data for the first two rows, `pandas`
    creates a new row for them because the index doesn''t match. In [*Chapter 3*](B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061),
    *Data Wrangling with Pandas*, we will see how to reset the index and set the index,
    both of which could resolve this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.38 â€“ Concatenating columns with mismatching indices'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.38_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.38 â€“ Concatenating columns with mismatching indices
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082), *Aggregating
    Pandas DataFrames*, we will discuss merging, which will also handle some of these
    issues when we're augmenting the columns in the dataframe. Often, we will use
    `concat()` or `append()` to add rows, but `merge()` or `join()` to add columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we want to concatenate the `tsunami` and `no_tsunami` dataframes, but the
    `no_tsunami` dataframe has an additional column (suppose we added a new column
    to it called `type`). The `join` parameter specifies how to handle any overlap
    in column names (when appending to the bottom) or in row names (when concatenating
    to the right). By default, this is `outer`, so we keep everything; however, if
    we use `inner`, we will only keep what they have in common:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the `type` column from the `no_tsunami` dataframe doesn''t show
    up because it wasn''t present in the `tsunami` dataframe. Take a look at the index,
    though; these were the row numbers from the original dataframe before we divided
    it into `tsunami` and `no_tsunami`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.39 â€“ Appending rows and keeping only shared columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.39_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.39 â€“ Appending rows and keeping only shared columns
  prefs: []
  type: TYPE_NORMAL
- en: 'If the index is not meaningful, we can also pass in `ignore_index` to get sequential
    values in the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'The index is now sequential, and the row numbers no longer match the original
    dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.40 â€“ Appending rows and resetting the index'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.40_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.40 â€“ Appending rows and resetting the index
  prefs: []
  type: TYPE_NORMAL
- en: 'Be sure to consult the `pandas` documentation for more information on the `concat()`
    function and other operations for combining data, which we will discuss in *Chapter
    4*, *Aggregating Pandas DataFrames*: http://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#concatenating-objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Deleting unwanted data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After adding that data to our dataframe, we can see the need to delete unwanted
    data. We need a way to undo our mistakes and get rid of data that we aren't going
    to use. Like adding data, we can use dictionary syntax to delete unwanted columns,
    just as we would when removing keys from a dictionary. Both `del df['<column_name>']`
    and `df.pop('<column_name>')` will work, provided that there is indeed a column
    with that name; otherwise, we will get a `KeyError`. The difference here is that
    while `del` removes it right away, `pop()` will return the column that we are
    removing. Remember that both of these operations will change our original dataframe,
    so use them with care.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use dictionary notation to delete the `source` column. Notice that it
    no longer appears in the result of `df.columns`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that if we aren''t sure whether the column exists, we should put our column
    deletion code in a `try...except` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Earlier, we created the `mag_negative` column for filtering our dataframe;
    however, we no longer want this column as part of our dataframe. We can use `pop()`
    to grab the series for the `mag_negative` column, which we can use as a Boolean
    mask later without having it in our dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a Boolean mask in the `mag_negative` variable that used to be a
    column in `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we used `pop()` to remove the `mag_negative` series rather than deleting
    it, we can still use it to filter our dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'This leaves us with the earthquakes that had negative magnitudes. Since we
    also called `head()`, we get back the first five such earthquakes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.41 â€“ Using a popped column as a Boolean mask'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.41_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.41 â€“ Using a popped column as a Boolean mask
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have a `drop()` method for removing multiple rows or columns
    either in-place (overwriting the original dataframe without having to reassign
    it) or returning a new `DataFrame` object. To remove rows, we pass the list of
    indices. Let''s remove the first two rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the index starts at `2` because we dropped `0` and `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.42 â€“ Dropping specific rows'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.42_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.42 â€“ Dropping specific rows
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, `drop()` assumes that we want to delete rows (`axis=0`). If we
    want to drop columns, we can either pass `axis=1` or specify our list of column
    names using the `columns` argument. Let''s delete some more columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'This drops all the columns that aren''t in the list we wanted to keep:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.43 â€“ Dropping specific columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_2.43_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.43 â€“ Dropping specific columns
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether we decide to pass `axis=1` to `drop()` or use the `columns` argument,
    our result will be equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, `drop()` will return a new `DataFrame` object; however, if we really
    want to remove the data from our original dataframe, we can pass in `inplace=True`,
    which will save us from having to reassign the result back to our dataframe. The
    result is the same as in *Figure 2.43*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Always be careful with in-place operations. In some cases, it may be possible
    to undo them; however, in others, it may require starting over from the beginning
    and recreating the dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use `pandas` for the data collection portion
    of data analysis and to describe our data with statistics, which will be helpful
    when we get to the drawing conclusions phase. We learned the main data structures
    of the `pandas` library, along with some of the operations we can perform on them.
    Next, we learned how to create `DataFrame` objects from a variety of sources,
    including flat files and API requests. Using earthquake data, we discussed how
    to summarize our data and calculate statistics from it. Subsequently, we addressed
    how to take subsets of data via selection, slicing, indexing, and filtering. Finally,
    we practiced adding and removing both columns and rows from our dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: These tasks also form the backbone of our `pandas` workflow and the foundation
    for the new topics we will cover in the next few chapters on data wrangling, aggregation,
    and data visualization. Be sure to complete the exercises provided in the next
    section before moving on.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the `data/parsed.csv` file and the material from this chapter, complete
    the following exercises to practice your `pandas` skills:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the 95th percentile of earthquake magnitude in Japan using the `mb` magnitude
    type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the percentage of earthquakes in Indonesia that were coupled with tsunamis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate summary statistics for earthquakes in Nevada.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a column indicating whether the earthquake happened in a country or US state
    that is on the Ring of Fire. Use Alaska, Antarctica (look for Antarctic), Bolivia,
    California, Canada, Chile, Costa Rica, Ecuador, Fiji, Guatemala, Indonesia, Japan,
    Kermadec Islands, Mexico (be careful not to select New Mexico), New Zealand, Peru,
    Philippines, Russia, Taiwan, Tonga, and Washington.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the number of earthquakes in the Ring of Fire locations and the number
    outside of them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the tsunami count along the Ring of Fire.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Those with an R and/or SQL background may find it helpful to see how the `pandas`
    syntax compares:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Comparison with R / R Libraries*: https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Comparison with SQL*: https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SQL Queries*: https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some resources on working with serialized data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pickle in Python: Object Serialization*: https://www.datacamp.com/community/tutorials/pickle-python-tutorial'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Read RData/RDS files into pandas.DataFrame objects (pyreader)*: https://github.com/ofajardo/pyreadr'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additional resources for working with APIs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Documentation for the requests package*: https://requests.readthedocs.io/en/master/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HTTP Methods*: https://restfulapi.net/http-methods/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HTTP Status Codes*: https://restfulapi.net/http-status-codes/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To learn more about regular expressions, consult the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mastering Python Regular Expressions by FÃ©lix LÃ³pez, VÃ­ctor Romero*: https://www.packtpub.com/application-development/mastering-python-regular-expressions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regular Expression Tutorial â€” Learn How to Use Regular Expressions*: https://www.regular-expressions.info/tutorial.html'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
