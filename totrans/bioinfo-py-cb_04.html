<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer025">
<h1 class="chapter-number" id="_idParaDest-69"><a id="_idTextAnchor068"/>3</h1>
<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Next-Generation Sequencing</h1>
<p><strong class="bold">Next-generation sequencing</strong> (<strong class="bold">NGS</strong>) is one of the fundamental technological developments of the century in life sciences. <strong class="bold">Whole-genome sequencing</strong> (<strong class="bold">WGS</strong>), <strong class="bold">restriction site-associated DNA sequencing</strong> (<strong class="bold">RAD-Seq</strong>), <strong class="bold">ribonucleic acid sequencing</strong> (<strong class="bold">RNA-Seq</strong>), <strong class="bold">chromatin immunoprecipitation sequencing</strong> (<strong class="bold">ChIP-Seq</strong>), and several other technologies are routinely used to investigate important biological problems. These are also called high-throughput sequencing technologies, and with good reason: they generate vast amounts of data that needs to be processed. NGS is the main reason that computational biology has become a big-data discipline. More than anything else, this is a field that requires strong bioinformatics techniques.</p>
<p>Here, we will not discuss each individual NGS technique <em class="italic">per se</em> (this would require a whole book of its own). We will use an existing WGS dataset—the 1,000 Genomes Project—to illustrate the most common steps necessary to analyze genomic data. The recipes presented here will be easily applicable to other genomic sequencing approaches. Some of them can also be used for transcriptomic analysis (for example, RNA-Seq). The recipes are also species-independent, so you will be able to apply them to any other species for which you have sequenced data. The biggest difference in processing data from different species is related to genome size, diversity, and the quality of the reference genome (if it exists for your species). These will not affect the automated Python part of NGS processing much. In any case, we will discuss different genomes in <a href="B17942_05.xhtml#_idTextAnchor122"><em class="italic">Chapter 5</em></a>, <em class="italic">Working with Genomes</em>.</p>
<p>As this is not an introductory book, you are expected to know at least what <strong class="bold">FASTA</strong> (<strong class="bold">FASTA</strong>), FASTQ, <strong class="bold">Binary Alignment Map</strong> (<strong class="bold">BAM</strong>), and <strong class="bold">Variant Call Format</strong> (<strong class="bold">VCF</strong>) files are. I will also make use of basic genomic terminology without introducing it (such as exomes, nonsynonymous mutations, and so on). You are required to be familiar with basic Python. We will leverage this knowledge to introduce the fundamental libraries in Python to perform NGS analysis. Here, we will follow the flow of a standard bioinformatics pipeline.</p>
<p>However, before we delve into real data from a real project, let’s get comfortable with accessing existing genomic databases and basic sequence processing—a simple start before the storm.</p>
<p>If you are running the content via Docker, you can use the <strong class="source-inline">tiagoantao/bioinformatics_ngs</strong> image. If you are using Anaconda Python, the required software for the chapter will be introduced in each recipe. </p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Accessing GenBank and moving around <strong class="bold">National Center for Biotechnology Information</strong> (<strong class="bold">NCBI</strong>) databases</li>
<li>Performing basic sequence analysis</li>
<li>Working with modern sequence formats</li>
<li>Working with alignment data</li>
<li>Extracting data from VCF files</li>
<li>Studying genome accessibility and filtering <strong class="bold">single-nucleotide polymorphism</strong> (<strong class="bold">SNP</strong>) data</li>
<li>Processing NGS data with HTSeq</li>
</ul>
<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Accessing GenBank and moving around NCBI databases</h1>
<p>Although you may <a id="_idIndexMarker142"/>have your own data to analyze, you will probably need existing genomic datasets. Here, we will look at how to access such databases from NCBI. We will not <a id="_idIndexMarker143"/>only discuss GenBank but also other databases from NCBI. Many people refer (wrongly) to the whole set of NCBI databases as GenBank, but NCBI includes the nucleotide database and many others—for example, PubMed.</p>
<p>As sequencing analysis is a long subject and this book targets intermediate to advanced users, we will not be very exhaustive with a topic that is, at its core, not very complicated.</p>
<p>Nonetheless, it’s a good warm-up for the more complex recipes that we will see at the end of this chapter.</p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor071"/>Getting ready</h2>
<p>We will use <a id="_idIndexMarker144"/>Biopython, which you installed in <a href="B17942_01.xhtml#_idTextAnchor020"><em class="italic">Chapter 1</em></a>, <em class="italic">Python and the Surrounding Software Ecology</em>. Biopython provides an interface to <strong class="source-inline">Entrez</strong>, the data <a id="_idIndexMarker145"/>retrieval system made available by NCBI.</p>
<p>This recipe is made available in the <strong class="source-inline">Chapter03/Accessing_Databases.py</strong> file.</p>
<p class="callout-heading">Tip</p>
<p class="callout">You will <a id="_idIndexMarker146"/>be accessing a live <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) from NCBI. Note that the performance of the system may vary during the day. Furthermore, you are expected to be a “good citizen” while using it. You will find some recommendations at <a href="https://www.ncbi.nlm.nih.gov/books/NBK25497/#chapter2.Usage_Guidelines_and_Requiremen">https://www.ncbi.nlm.nih.gov/books/NBK25497/#chapter2.Usage_Guidelines_and_Requiremen</a>. Notably, you are required to specify an email address with your query. You should try to avoid a large number of requests (100 or more) during peak times (between 9.00 a.m. and 5.00 p.m. American Eastern Time on weekdays), and do not post more than three queries per second (Biopython will take care of this for you). It’s not only good citizenship, but you risk getting blocked if you overuse NCBI’s servers (a good reason to give a real email address, because NCBI may try to contact you).</p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor072"/>How to do it...</h2>
<p>Now, let’s look <a id="_idIndexMarker147"/>at how we can search and fetch data <a id="_idIndexMarker148"/>from NCBI databases:</p>
<ol>
<li>We will start by importing the relevant module and configuring the email address:<p class="source-code">from Bio import Entrez, SeqIO</p><p class="source-code">Entrez.email = 'put@your.email.here'</p></li>
</ol>
<p>We will also import the module to process sequences. Do not forget to put in the correct email address.</p>
<ol>
<li value="2">We will <a id="_idIndexMarker149"/>now try to find the <strong class="bold">chloroquine resistance transporter</strong> (<strong class="bold">CRT</strong>) gene in <strong class="source-inline">Plasmodium falciparum</strong> (the parasite that causes the deadliest form of malaria) on the <strong class="source-inline">nucleotide</strong> database:<p class="source-code">handle = Entrez.esearch(db='nucleotide', term='CRT[Gene Name] AND "Plasmodium falciparum"[Organism]')</p><p class="source-code">rec_list = Entrez.read(handle)</p><p class="source-code">if int(rec_list['RetMax']) &lt; int(rec_list['Count']):</p><p class="source-code">    handle = Entrez.esearch(db='nucleotide', term='CRT[Gene Name] AND "Plasmodium falciparum"[Organism]', retmax=rec_list['Count'])</p><p class="source-code">    rec_list = Entrez.read(handle)</p></li>
</ol>
<p>We will search the <strong class="source-inline">nucleotide</strong> database for our gene and organism (for the syntax of <a id="_idIndexMarker150"/>the search string, check the NCBI website). Then, we will read the result that is returned. Note that the standard <a id="_idIndexMarker151"/>search will limit the number of record references to 20, so if you have more, you may want to repeat the query with an increased maximum limit. In our case, we will actually override the default limit with <strong class="source-inline">retmax</strong>. The <strong class="source-inline">Entrez</strong> system provides quite a few sophisticated ways to retrieve a large number of results (for more information, check the Biopython or NCBI Entrez documentation). Although you now have the <strong class="bold">identifiers</strong> (<strong class="bold">IDs</strong>) of all of the records, you still need to retrieve the records properly.</p>
<ol>
<li value="3">Now, let’s try to retrieve all of these records. The following query will download all matching nucleotide sequences from GenBank, which is 1,374 at the time of writing this book. You probably won’t want to do this all the time:<p class="source-code">id_list = rec_list['IdList']</p><p class="source-code">hdl = Entrez.efetch(db='nucleotide', id=id_list, rettype='gb')</p></li>
</ol>
<p>Well, in this case, go ahead and do it. However, be careful with this technique, because you will retrieve a large number of complete records, and some of them will have fairly large sequences inside. You risk downloading a lot of data (which would be a strain both on your side and on the NCBI servers).</p>
<p>There are several ways around this. One way is to make a more restrictive query and/or download just a few at a time and stop when you have found the one that you need. The precise strategy will depend on what you are trying to achieve. In any case, we will retrieve a list of records in the GenBank format (which includes sequences, plus a lot of interesting metadata).</p>
<ol>
<li value="4">Let’s read and parse the result:<p class="source-code">recs = list(SeqIO.parse(hdl, 'gb'))</p></li>
</ol>
<p>Note that <a id="_idIndexMarker152"/>we have converted an iterator (the result of <strong class="source-inline">SeqIO.parse</strong>) to a list. The advantage of doing this is that we can use the <a id="_idIndexMarker153"/>result as many times as we want (for example, iterate many times over), without repeating the query on the server. </p>
<p>This saves time, bandwidth, and server usage if you plan to iterate many times over. The disadvantage is that it will allocate memory for all records. This will not work for very large datasets; you might not want to do this conversion genome-wide as in <a href="B17942_05.xhtml#_idTextAnchor122"><em class="italic">Chapter 5</em></a>, <em class="italic">Working with Genomes</em>. We will return to this topic in the last part of this book. If you are doing interactive computing, you will probably prefer to have a list (so that you can analyze and experiment with it multiple times), but if you are developing a library, an iterator will probably be the best approach.</p>
<ol>
<li value="5">We will now just concentrate on a single record. This will only work if you used the exact same preceding query:<p class="source-code">for rec in recs:</p><p class="source-code">    if rec.name == 'KM288867':</p><p class="source-code">        break</p><p class="source-code"><strong class="bold">print(rec.name)</strong></p><p class="source-code"><strong class="bold">print(rec.description)</strong></p></li>
</ol>
<p>The <strong class="source-inline">rec</strong> variable now has our record of interest. The <strong class="source-inline">rec.description</strong> file will contain its human-readable description.</p>
<ol>
<li value="6">Now, let’s extract some sequence features that contain information such as <strong class="source-inline">gene</strong> products and <strong class="source-inline">exon</strong> positions on the sequence:<p class="source-code">for feature in rec.features:</p><p class="source-code">     if feature.type == 'gene':</p><p class="source-code">         print(feature.qualifiers['gene'])</p><p class="source-code">     elif feature.type == 'exon':</p><p class="source-code">         loc = feature.location</p><p class="source-code">         print(loc.start, loc.end, loc.strand)</p><p class="source-code">     else:</p><p class="source-code">         print('not processed:\n%s' % feature)</p></li>
</ol>
<p>If the <strong class="source-inline">feature.type</strong> value is <strong class="source-inline">gene</strong>, we will print its name, which will be in the <strong class="source-inline">qualifiers</strong> dictionary. We will also print all the locations of exons. Exons, as <a id="_idIndexMarker154"/>with all features, have locations in this <a id="_idIndexMarker155"/>sequence: a start, an end, and the strand from where they are read. While all the start and end positions for our exons are <strong class="source-inline">ExactPosition</strong>, note that Biopython supports many other types of positions. One type of position is <strong class="source-inline">BeforePosition</strong>, which specifies that a location point is before a certain sequence position. Another type of position is <strong class="source-inline">BetweenPosition</strong>, which gives the interval for a certain location start/end. There are quite a few more position types; these are just some examples.</p>
<p>Coordinates will be specified in such a way that you will be able to easily retrieve the sequence from a Python array with ranges, so generally, the start will be one before the value on the record, and the end will be equal. The issue of coordinate systems will be revisited in future recipes.</p>
<p>For other feature types, we simply print them. Note that Biopython will provide a human-readable version of the feature when you print it.</p>
<ol>
<li value="7">We will <a id="_idIndexMarker156"/>now look at the annotations on the record, which <a id="_idIndexMarker157"/>are mostly metadata that is not related to the sequence position:<p class="source-code">for name, value in rec.annotations.items():</p><p class="source-code">    print('%s=%s' % (name, value))</p></li>
</ol>
<p>Note that some values are not strings; they can be numbers or even lists (for example, the taxonomy annotation is a list).</p>
<ol>
<li value="8">Last but not least, you can access a fundamental piece of information—the sequence:<p class="source-code">print(len(rec.seq))</p></li>
</ol>
<p>The sequence object will be the main topic of our next recipe.</p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>There’s more...</h2>
<p>There are <a id="_idIndexMarker158"/>many more databases at NCBI. You will probably want to check the <strong class="bold">Sequence Read Archive</strong> (<strong class="bold">SRA</strong>) database (previously known as <strong class="bold">Short Read Archive</strong>) if you are working with NGS data. The SNP database contains information on SNPs, whereas the protein database has protein sequences, and so on. A full list <a id="_idIndexMarker159"/>of databases in Entrez is linked in the <em class="italic">See also</em> section of this recipe.</p>
<p>Another database that you probably already know about with regard to NCBI is PubMed, which includes a list of scientific and medical citations, abstracts, and even full texts. You can also access it via Biopython. Furthermore, GenBank records often contain links to PubMed. For example, we can perform this on our previous record, as shown here:</p>
<pre class="source-code">
from Bio import Medline
refs = rec.annotations['references']
for ref in refs:
    if ref.pubmed_id != '':
        print(ref.pubmed_id)
        handle = Entrez.efetch(db='pubmed', id=[ref.pubmed_id], rettype='medline', retmode='text')
        records = Medline.parse(handle)
        for med_rec in records:
            for k, v in med_rec.items():
                print('%s: %s' % (k, v))</pre>
<p>This will take all reference annotations, check whether they have a PubMed ID, and then access the PubMed database to retrieve the records, parse them, and then print them.</p>
<p>The output <a id="_idIndexMarker160"/>per record is a Python dictionary. Note that there are many references to external databases on a typical GenBank record.</p>
<p>Of course, there <a id="_idIndexMarker161"/>are many other biological databases outside <a id="_idIndexMarker162"/>NCBI, such as Ensembl (<a href="http://www.ensembl.org">http://www.ensembl.org</a>) and <strong class="bold">University of California, Santa Cruz</strong> (<strong class="bold">UCSC</strong>) Genome Bioinformatics (<a href="http://genome.ucsc.edu/">http://genome.ucsc.edu/</a>). The support for many of these databases in Python will vary a lot.</p>
<p>An introductory recipe on biological databases would not be complete without at least a passing <a id="_idIndexMarker163"/>reference to <strong class="bold">Basic Local Alignment Search Tool</strong> (<strong class="bold">BLAST</strong>). BLAST is an algorithm that assesses the similarity of sequences. NCBI provides a service that allows you to compare your sequence of interest against its own database. Of course, you can use your local BLAST database instead of using NCBI’s service. Biopython provides extensive support for this, but as this is too introductory, I will just refer you to the Biopython tutorial.</p>
<h2 id="_idParaDest-75"><a id="_idTextAnchor074"/>See also</h2>
<p>This additional information will also be useful:</p>
<ul>
<li>You can find <a id="_idIndexMarker164"/>more examples on the Biopython tutorial at <a href="http://biopython.org/DIST/docs/tutorial/Tutorial.xhtml">http://biopython.org/DIST/docs/tutorial/Tutorial.xhtml</a>.</li>
<li>A list of <a id="_idIndexMarker165"/>accessible NCBI databases can be found at <a href="http://www.ncbi.nlm.nih.gov/gquery/">http://www.ncbi.nlm.nih.gov/gquery/</a>.</li>
<li>A great <strong class="bold">question and answer</strong> (<strong class="bold">Q&amp;A</strong>) site where you can find help for your problems <a id="_idIndexMarker166"/>with databases and sequence analysis is Biostars (<a href="http://www.biostars.org">http://www.biostars.org</a>); you can use it for all of the content in this book, not just for this recipe.</li>
</ul>
<h1 id="_idParaDest-76"><a id="_idTextAnchor075"/>Performing basic sequence analysis</h1>
<p>We will now do some basic analysis of DNA sequences. We will work with FASTA files and do some <a id="_idIndexMarker167"/>manipulation, such as reverse complementing or transcription. As with the previous recipe, we will use Biopython, which you installed in <a href="B17942_01.xhtml#_idTextAnchor020"><em class="italic">Chapter 1</em></a>, <em class="italic">Python and the Surrounding Software Ecology</em>. These two recipes provide you with the necessary introductory building blocks with which we will perform all the modern NGS analysis and then genome processing in this chapter and <a href="B17942_05.xhtml#_idTextAnchor122"><em class="italic">Chapter 5</em></a>, <em class="italic">Working with Genomes</em>.</p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/>Getting ready</h2>
<p>The code for this recipe is available in <strong class="source-inline">Chapter03/Basic_Sequence_Processing.py</strong>. We will use the human <strong class="bold">lactase</strong> (<strong class="bold">LCT</strong>) gene as an example; you can get this using your knowledge from the previous recipe, by using the <strong class="source-inline">Entrez</strong> research interface:</p>
<pre class="source-code">
from Bio import Entrez, SeqIO, SeqRecord
Entrez.email = "your@email.here"
hdl = Entrez.efetch(db='nucleotide', id=['NM_002299'], rettype='gb') # Lactase gene
gb_rec = SeqIO.read(hdl, 'gb')</pre>
<p>Now that we have the GenBank record, let’s extract the gene sequence. The record has a bit more than that, but let’s get the precise location of the gene first:</p>
<pre class="source-code">
for feature in gb_rec.features:
    if feature.type == 'CDS':
        location = feature.location  # Note translation existing
cds = SeqRecord.SeqRecord(gb_rec.seq[location.start:location.end], 'NM_002299', description='LCT CDS only')</pre>
<p>Our example sequence is available on the Biopython sequence record.</p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>How to do it...</h2>
<p>Let’s take <a id="_idIndexMarker168"/>a look at the following steps:</p>
<ol>
<li value="1">As our sequence of interest is available in a Biopython sequence object, let’s start by saving it to a FASTA file on our local disk:<p class="source-code">from Bio import SeqIO</p><p class="source-code">w_hdl = open('example.fasta', 'w')</p><p class="source-code">SeqIO.write([cds], w_hdl, 'fasta')</p><p class="source-code">w_hdl.close()</p></li>
</ol>
<p>The <strong class="source-inline">SeqIO.write</strong> function takes a list of sequences to write (in our case, it’s just a single one). Be careful with this idiom. If you want to write many sequences (and you could easily write millions with NGS), do not use a list (as shown in the preceding code snippet) because this will allocate massive amounts of memory. Use either an iterator or the <strong class="source-inline">SeqIO.write</strong> function several times with a subset of the sequence on each write.</p>
<ol>
<li value="2">In most situations, you will actually have the sequence on the disk, so you will be interested in reading it:<p class="source-code">recs = SeqIO.parse('example.fasta', 'fasta')</p><p class="source-code">for rec in recs:</p><p class="source-code">    seq = rec.seq</p><p class="source-code">    print(rec.description)</p><p class="source-code">    print(seq[:10])</p></li>
</ol>
<p>Here, we are concerned with processing a single sequence, but FASTA files can contain multiple records. The Python idiom to perform this is quite easy. To read a FASTA file, you just use standard iteration techniques, as shown in the following code snippet. For our example, the preceding code will print the following output:</p>
<p class="source-code"><strong class="bold">NM_002299 LCT CDS only</strong></p>
<p class="source-code"><strong class="bold"> ATGGAGCTGT</strong></p>
<p>Note that we printed <strong class="source-inline">seq[:10]</strong>. The sequence object can use typical array slices to get part of a sequence.</p>
<ol>
<li value="3">As we now <a id="_idIndexMarker169"/>have an unambiguous DNA, we can transcribe it as follows:<p class="source-code">rna = seq.transcribe()</p><p class="source-code">print(rna)</p></li>
<li>Finally, we can translate our gene into a protein:<p class="source-code">prot = seq.translate()</p><p class="source-code">print(prot)</p></li>
</ol>
<p>Now, we have the amino acid sequence for our gene. </p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor078"/>There’s more...</h2>
<p>Much more can be said about the management of sequences in Biopython, but this is mostly introductory material that you can find in the Biopython tutorial. I think it’s important to give you a taste of sequence management, mostly for completion purposes. To support those of you who might have some experience in other fields of bioinformatics but are just starting with sequence analysis, there are, nonetheless, a few points that you should be aware of:</p>
<ul>
<li>When you perform an RNA translation to get your protein, be sure to use the correct genetic code. Even if you are working with “common” organisms (such as humans), remember that the mitochondrial genetic code is different.</li>
<li>Biopython’s <strong class="source-inline">Seq</strong> object is much more flexible than what’s shown here. For some good <a id="_idIndexMarker170"/>examples, refer to the Biopython tutorial. However, this recipe will be enough for the work we need to do with FASTQ files (see the next recipe).</li>
<li>To deal with strand-related issues, there are—as expected—sequence functions such as <strong class="source-inline">reverse_complement</strong>.</li>
<li>The GenBank record from which we started includes a lot of metadata information about the sequence, so be sure to explore it.</li>
</ul>
<h2 id="_idParaDest-80"><a id="_idTextAnchor079"/>See also</h2>
<ul>
<li>Genetic codes known <a id="_idIndexMarker171"/>to Biopython are the ones specified by NCBI, available at <a href="http://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi">http://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi</a>.</li>
<li>As in the previous recipe, the Biopython tutorial is your main port of call and is available at <a href="http://biopython.org/DIST/docs/tutorial/Tutorial.xhtml">http://biopython.org/DIST/docs/tutorial/Tutorial.xhtml</a>.</li>
<li>Be sure <a id="_idIndexMarker172"/>to also check the Biopython SeqIO page at <a href="http://biopython.org/wiki/SeqIO">http://biopython.org/wiki/SeqIO</a>.</li>
</ul>
<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Working with modern sequence formats</h1>
<p>Here, we will <a id="_idIndexMarker173"/>work with FASTQ files, the standard format output used by modern sequencers. You will learn how to work with quality scores per base and also consider variations in output coming from different sequencing machines and databases. This is the first recipe that will use real data (big data) from the human 1,000 Genomes Project. We will start with a brief description of the project.</p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor081"/>Getting ready</h2>
<p>The human 1,000 Genomes Project aims to catalog worldwide human genetic variation and takes advantage of modern sequencing technology to do WGS. This project makes all data publicly <a id="_idIndexMarker174"/>available, which includes output from sequencers, sequence alignments, and SNP calls, among many other artifacts. The name “1,000 Genomes” is actually a misnomer, because it currently includes more than 2,500 samples. These samples are divided into hundreds of populations, spanning <a id="_idIndexMarker175"/>the whole planet. We will mostly use <a id="_idIndexMarker176"/>data from four populations: <strong class="bold">African Yorubans</strong> (<strong class="bold">YRI</strong>), <strong class="bold">Utah Residents with Northern and Western European Ancestry</strong> (<strong class="bold">CEU</strong>), <strong class="bold">Japanese in Tokyo</strong> (<strong class="bold">JPT</strong>), and <strong class="bold">Han Chinese in Beijing</strong> (<strong class="bold">CHB</strong>). The <a id="_idIndexMarker177"/>reason we chose these specific <a id="_idIndexMarker178"/>populations is that they were the first ones that came from HapMap, an old project with similar goals. They used genotyping arrays to find out more about the quality of this subset. We will revisit the 1,000 Genomes and HapMap projects in <a href="B17942_06.xhtml#_idTextAnchor154"><em class="italic">Chapter 6</em></a>, <em class="italic">Population Genetics</em>.</p>
<p class="callout-heading">Tip</p>
<p class="callout">Next-generation datasets are generally very large. As we will be using real data, some of the files that you will download will be big. While I have tried to choose the smallest real examples possible, you will still need a good network connection and a considerably large amount of disk space. Waiting for the download will probably be your biggest hurdle in this recipe, but data management is a serious problem with NGS. In real life, you will need to budget time for data transfer, allocate disk space (which can be financially costly), and consider backup policies. The most common initial mistake with NGS is to think that these problems are trivial, but they are not. An operation such as copying a set of BAM files to a network, or even to your computer, will become a headache. Be prepared. After downloading large files, at the very least, you should check that the size is correct. Some <a id="_idIndexMarker179"/>databases offer <strong class="bold">Message Digest 5</strong> (<strong class="bold">MD5</strong>) checksums. You can compare these checksums with the ones on the files you downloaded by using tools such as md5sum.</p>
<p>The instructions to download the data are at the top of the notebook, as specified in the first cell of <strong class="source-inline">Chapter03/Working_with_FASTQ.py</strong>. This is a fairly small file (27 <strong class="bold">megabytes</strong> (<strong class="bold">MB</strong>)) and represents part of the sequenced data of a Yoruban female (<strong class="source-inline">NA18489</strong>). If you refer to the 1,000 Genomes Project, you will see that the vast majority of FASTQ files are much <a id="_idIndexMarker180"/>bigger (up to two orders of magnitude bigger).</p>
<p>The processing of FASTQ sequence files will mostly be performed using Biopython.</p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/>How to do it...</h2>
<p>Before we <a id="_idIndexMarker181"/>start coding, let’s take a look at the FASTQ file, in which you will have many records, as shown in the following code snippet:</p>
<pre class="source-code">
@SRR003258.1 30443AAXX:1:1:1053:1999 length=51
 ACCCCCCCCCACCCCCCCCCCCCCCCCCCCCCCCCCCACACACACCAACAC
 +
 =IIIIIIIII5IIIIIII&gt;IIII+GIIIIIIIIIIIIII(IIIII01&amp;III</pre>
<p><em class="italic">Line 1</em> starts with <strong class="source-inline">@</strong>, followed by a sequence ID and a description string. The description string will vary from a sequencer or a database source, but will normally be amenable to automated parsing.</p>
<p>The second line has the sequenced DNA, which is just like a FASTA file. The third line is a <strong class="source-inline">+</strong> sign, sometimes followed by the description line on the first line.</p>
<p>The fourth line contains quality values for each base that’s read on <em class="italic">line 2</em>. Each letter encodes <a id="_idIndexMarker182"/>a Phred quality score (<a href="http://en.wikipedia.org/wiki/Phred_quality_score">http://en.wikipedia.org/wiki/Phred_quality_score</a>), which assigns a probability of error to each read. This encoding can vary a bit among platforms. Be sure to check for this on your specific platform.</p>
<p>Let’s take a look at the following steps:</p>
<ol>
<li value="1">Let’s open the file:<p class="source-code">import gzip</p><p class="source-code">from Bio import SeqIO</p><p class="source-code">recs = SeqIO.parse(gzip.open('SRR003265.filt.fastq.gz'),'rt', encoding='utf-8'), 'fastq')</p><p class="source-code">rec = next(recs)</p><p class="source-code">print(rec.id, rec.description, rec.seq)</p><p class="source-code">print(rec.letter_annotations)</p></li>
</ol>
<p>We will open a <strong class="bold">GNU ZIP</strong> (<strong class="bold">GZIP</strong>) file so that we can use the Python <strong class="source-inline">gzip</strong> module. We will also specify the <strong class="source-inline">fastq</strong> format. Note that some variations in this format will impact the interpretation of the Phred quality scores. You may want to specify a slightly different format. Refer to <a href="http://biopython.org/wiki/SeqIO">http://biopython.org/wiki/SeqIO</a> for all formats.</p>
<p class="callout-heading">Tip</p>
<p class="callout">You should <a id="_idIndexMarker183"/>usually store your FASTQ files in a compressed format. Not only do you gain a lot of disk space, as these are text files, but you probably also gain some processing time. Although decompressing is a slow process, it can still be faster than reading a much bigger (uncompressed) file from a disk.</p>
<p>We print the standard fields and quality scores from the previous recipe into <strong class="source-inline">rec.letter_annotations</strong>. As long as we choose the correct parser, Biopython will convert all the Phred encoding letters to logarithmic scores, which we will use soon.</p>
<p>For now, <em class="italic">don’t</em> do this:</p>
<p class="source-code">recs = list(recs) # do not do it!</p>
<p>Although this might work with some FASTA files (and with this very small FASTQ file), if you do something such as this, you will allocate memory so that you can load the complete file in memory. With an average FASTQ file, this is the best way to crash your computer. As a rule, always iterate over your file. If you have to perform several operations over it, you have two main options. The first option is to perform a single iteration or all operations at once. The second option to is open a file several times and repeat the iteration.</p>
<ol>
<li value="2">Now, let’s take a look at the distribution of nucleotide reads:<p class="source-code">from collections import defaultdict</p><p class="source-code">recs = SeqIO.parse(gzip.open('SRR003265.filt.fastq.gz', 'rt', encoding='utf-8'), 'fastq')</p><p class="source-code">cnt = defaultdict(int)</p><p class="source-code">for rec in recs:</p><p class="source-code">    for letter in rec.seq:</p><p class="source-code">        cnt[letter] += 1</p><p class="source-code">tot = sum(cnt.values())</p><p class="source-code">for letter, cnt in cnt.items():</p><p class="source-code">    print('%s: %.2f %d' % (letter, 100. * cnt / tot, cnt))</p></li>
</ol>
<p>We will <a id="_idIndexMarker184"/>reopen the file and use <strong class="source-inline">defaultdict</strong> to maintain a count of nucleotide references in the FASTQ file. If you have never used this Python standard dictionary type, you may want to consider it because it removes the need to initialize dictionary entries, assuming default values for each type.</p>
<p class="callout-heading">Note</p>
<p class="callout">There is a residual number for <strong class="source-inline">N</strong> calls. These are calls in which a sequencer reports an unknown base. In our FASTQ file example, we have cheated a bit because we used a filtered file (the fraction of <strong class="source-inline">N</strong> calls will be quite low). Expect a much bigger number of <strong class="source-inline">N</strong> calls in a file that comes out of the sequencer unfiltered. In fact, you can even expect something more with regard to the spatial distribution of <strong class="source-inline">N</strong> calls.</p>
<ol>
<li value="3">Let’s plot the distribution of <strong class="source-inline">N</strong>s according to their read position:<p class="source-code">import seaborn as sns</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">recs = SeqIO.parse(gzip.open('SRR003265.filt.fastq.gz', 'rt', encoding='utf-8'), 'fastq')</p><p class="source-code">n_cnt = defaultdict(int)</p><p class="source-code">for rec in recs:</p><p class="source-code">    for i, letter in enumerate(rec.seq):</p><p class="source-code">        pos = i + 1</p><p class="source-code">        if letter == 'N':</p><p class="source-code">            n_cnt[pos] += 1</p><p class="source-code">seq_len = max(n_cnt.keys())</p><p class="source-code">positions = range(1, seq_len + 1)</p><p class="source-code">fig, ax = plt.subplots(figsize=(16,9))</p><p class="source-code">ax.plot(positions, [n_cnt[x] for x in positions])</p><p class="source-code">fig.suptitle('Number of N calls as a function of the distance from the start of the sequencer read')</p><p class="source-code">ax.set_xlim(1, seq_len)</p><p class="source-code">ax.set_xlabel('Read distance')</p><p class="source-code">ax.set_ylabel('Number of N Calls')</p></li>
</ol>
<p>We import the <strong class="source-inline">seaborn</strong> library. Although we do not use it explicitly at this point, this <a id="_idIndexMarker185"/>library has the advantage of making <strong class="source-inline">matplotlib</strong> plots look better, because it tweaks the default <strong class="source-inline">matplotlib</strong> style.</p>
<p>We then open the file to parse again (remember that you do not use a list but iterate again). We iterate through the file and get the position of any references to <strong class="source-inline">N</strong>. Then, we plot the distribution of <strong class="source-inline">N</strong>s as a function of the distance from the start of the sequence:</p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 3.1 – The number of N calls as a function of the distance from the start of the sequencer read " height="1324" src="image/B17942_03_001.jpg" width="1351"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – The number of N calls as a function of the distance from the start of the sequencer read</p>
<p>You will see <a id="_idIndexMarker186"/>that until position <strong class="source-inline">25</strong>, there are no errors. This is not what you will get from a typical sequencer output. Our example file is already filtered, and the 1,000 Genomes filtering rules enforce that no <strong class="source-inline">N</strong> calls can occur before position <strong class="source-inline">25</strong>.</p>
<p>While we cannot study the behavior of <strong class="source-inline">N</strong>s in this dataset before position <strong class="source-inline">25</strong> (feel free to use one of your own unfiltered FASTQ files with this code in order to see how <strong class="source-inline">N</strong>s distribute across the read position), we can see that after position <strong class="source-inline">25</strong>, the distribution is far from uniform. There is an important lesson here, which is that the quantity of uncalled bases is position-dependent. So, what about the quality of the reads?</p>
<ol>
<li value="4">Let’s study the distribution of Phred scores (that is, the quality of our reads):<p class="source-code">recs = SeqIO.parse(gzip.open('SRR003265.filt.fastq.gz', 'rt', encoding='utf-8'), 'fastq')</p><p class="source-code">cnt_qual = defaultdict(int)</p><p class="source-code">for rec in recs:</p><p class="source-code">    for i, qual in enumerate(rec.letter_annotations['phred_quality']):</p><p class="source-code">        if i &lt; 25:</p><p class="source-code">            continue</p><p class="source-code">        cnt_qual[qual] += 1</p><p class="source-code">tot = sum(cnt_qual.values())</p><p class="source-code">for qual, cnt in cnt_qual.items():</p><p class="source-code">    print('%d: %.2f %d' % (qual, 100. * cnt / tot, cnt))</p></li>
</ol>
<p>We will <a id="_idIndexMarker187"/>start by reopening the file (again) and initializing a default dictionary. We then get the <strong class="source-inline">phred_quality</strong> letter annotation, but we ignore sequencing positions that are up to 24 <strong class="bold">base pairs</strong> (<strong class="bold">bp</strong>) from the start (because of the filtering of our FASTQ file, if you have an unfiltered file, you may want to drop this rule). We add the quality score to our default dictionary and, finally, print it.</p>
<p class="callout-heading">Note</p>
<p class="callout">As a short reminder, the <a id="_idIndexMarker188"/>Phred quality score is a logarithmic representation of the probability of an accurate call. This probability is given as <img alt="" height="52" src="image/B17942_03_009.png" width="96"/>. So, a <em class="italic">Q</em> of 10 represents 90 percent call accuracy, 20 represents 99 percent call accuracy, and 30 will be 99.9 percent. For our file, the maximum accuracy will be 99.99 percent (40). In some cases, values of 60 are possible (99.9999 percent accuracy).</p>
<ol>
<li value="5">More <a id="_idIndexMarker189"/>interestingly, we can plot the distribution of qualities according to their read position:<p class="source-code">recs = SeqIO.parse(gzip.open('SRR003265.filt.fastq.gz', 'rt', encoding='utf-8'), 'fastq')</p><p class="source-code">qual_pos = defaultdict(list)</p><p class="source-code">for rec in recs:</p><p class="source-code">    for i, qual in enumerate(rec.letter_annotations['phred_quality']):</p><p class="source-code">        if i &lt; 25 or qual == 40:</p><p class="source-code">           continue</p><p class="source-code">        pos = i + 1</p><p class="source-code">        qual_pos[pos].append(qual)</p><p class="source-code">vps = []</p><p class="source-code">poses = list(qual_pos.keys())</p><p class="source-code">poses.sort()</p><p class="source-code">for pos in poses:</p><p class="source-code">    vps.append(qual_pos[pos])</p><p class="source-code">fig, ax = plt.subplots(figsize=(16,9))</p><p class="source-code">sns.boxplot(data=vps, ax=ax)</p><p class="source-code">ax.set_xticklabels([str(x) for x in range(26, max(qual_pos.keys()) + 1)])</p><p class="source-code">ax.set_xlabel('Read distance')</p><p class="source-code">ax.set_ylabel('PHRED score')</p><p class="source-code">fig.suptitle('Distribution of PHRED scores as a function of read distance')</p></li>
</ol>
<p>In this case, we will ignore both positions sequenced as <strong class="source-inline">25</strong> bp from the start (again, remove this rule if you have unfiltered sequencer data) and the maximum quality score for this file (<strong class="source-inline">40</strong>). However, in your case, you can consider starting your plotting analysis with the maximum. You may want to check the maximum possible value for your sequencer hardware. Generally, as most calls can be performed with maximum quality, you may want to remove them if you are trying to understand where quality problems lie.</p>
<p>Note that <a id="_idIndexMarker190"/>we are using the <strong class="source-inline">boxplot</strong> function of <strong class="source-inline">seaborn</strong>; we are only using this because the output looks slightly better than the standard <strong class="source-inline">boxplot</strong> function of <strong class="source-inline">matplotlib</strong>. If you prefer not to depend on <strong class="source-inline">seaborn</strong>, just use the stock <strong class="source-inline">matplotlib</strong> function. In this case, you will call <strong class="source-inline">ax.boxplot(vps)</strong> instead of <strong class="source-inline">sns.boxplot(data=vps, ax=ax)</strong>.</p>
<p>As expected, the distribution is not uniform, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 3.2 – The distribution of Phred scores as a function of the distance from the start of the sequencer read " height="1618" src="image/B17942_03_002.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – The distribution of Phred scores as a function of the distance from the start of the sequencer read</p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/>There’s more...</h2>
<p>Although it’s impossible to discuss all the variations of output coming from sequencer files, paired-end reads are worth mentioning because they are common and require a different <a id="_idIndexMarker191"/>processing approach. With paired-end sequencing, both ends of a DNA fragment are sequenced with a gap in the middle (called the insert). In this case, two files will be produced: <strong class="source-inline">X_1.FASTQ</strong> and <strong class="source-inline">X_2.FASTQ</strong>. Both files will have the same order and the exact same number of sequences. The first sequence will be in <strong class="source-inline">X_1</strong> pairs with the first sequence of <strong class="source-inline">X_2</strong>, and so on. With regard to the programming technique, if you want to keep the pairing information, you might perform something such as this:</p>
<pre class="source-code">
f1 = gzip.open('X_1.filt.fastq.gz', 'rt, enconding='utf-8')
f2 = gzip.open('X_2.filt.fastq.gz', 'rt, enconding='utf-8')
recs1 = SeqIO.parse(f1, 'fastq')
recs2 = SeqIO.parse(f2, 'fastq')
cnt = 0
for rec1, rec2 in zip(recs1, recs2):
    cnt +=1
print('Number of pairs: %d' % cnt)</pre>
<p>The preceding code reads all pairs in order and just counts the number of pairs. You will probably want to do something more, but this exposes a dialect that is based on the Python <strong class="source-inline">zip</strong> function that allows you to iterate through both files simultaneously. Remember to replace <strong class="source-inline">X</strong> with your <strong class="source-inline">FASTQ</strong> prefix.</p>
<p>Finally, if you are sequencing human genomes, you may want to use sequencing data from Complete Genomics. In this case, read the <em class="italic">There’s more…</em> section in the next recipe, where we briefly discuss Complete Genomics data.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor084"/>See also</h2>
<p>Here are some links with more information:</p>
<ul>
<li>The Wikipedia <a id="_idIndexMarker192"/>page on the FASTQ format is quite informative (<a href="http://en.wikipedia.org/wiki/FASTQ_format">http://en.wikipedia.org/wiki/FASTQ_format</a>). </li>
<li>You can <a id="_idIndexMarker193"/>find more information on the 1,000 Genomes Project at <a href="http://www.1000genomes.org/">http://www.1000genomes.org/</a>.</li>
<li>Information about the Phred quality score can be found at <a href="http://en.wikipedia.org/wiki/Phred_quality_score">http://en.wikipedia.org/wiki/Phred_quality_score</a>.</li>
<li>Illumina provides a good introduction page to paired-end reads at <a href="https://www.illumina.com/science/technology/next-generation-sequencing/paired-end-vs-single-read-sequencing.xhtml">https://www.illumina.com/science/technology/next-generation-sequencing/paired-end-vs-single-read-sequencing.xhtml</a>.</li>
<li>The <em class="italic">Computational methods for discovering structural variation with next-generation sequencing</em> paper from Medvedev et al. on nature methods (<a href="http://www.nature.com/nmeth/journal/v6/n11s/abs/nmeth.1374.xhtml">http://www.nature.com/nmeth/journal/v6/n11s/abs/nmeth.1374.xhtml</a>); note that this is not open access.</li>
</ul>
<h1 id="_idParaDest-86"><a id="_idTextAnchor085"/>Working with alignment data</h1>
<p>After you receive <a id="_idIndexMarker194"/>your data from the sequencer, you will normally use <a id="_idIndexMarker195"/>a tool such as <strong class="bold">Burrows-Wheeler Aligner</strong> (<strong class="source-inline">bwa</strong>) to align your sequences to a reference genome. Most users will have a reference genome for their species. You can read more on reference genomes in <a href="B17942_05.xhtml#_idTextAnchor122"><em class="italic">Chapter 5</em></a>, <em class="italic">Working with Genomes</em>.</p>
<p>The most common representation for aligned data is the <strong class="bold">Sequence Alignment Map</strong> (<strong class="bold">SAM</strong>) format. Due <a id="_idIndexMarker196"/>to the massive size of most of these files, you will probably work with its compressed version (BAM). The compressed format is indexable for extremely fast random access (for example, to speedily find alignments to a certain part of a chromosome). Note that you will need to have an index for your BAM file, which is normally created by the <strong class="source-inline">tabix</strong> utility of SAMtools. SAMtools is probably the most widely used tool for manipulating SAM/BAM files.</p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>Getting ready</h2>
<p>As discussed in the previous recipe, we will use data from the 1,000 Genomes Project. We will use the exome alignment for chromosome 20 of female <strong class="source-inline">NA18489</strong>. This is just 312 MB. The whole-exome alignment for this individual is 14.2 <strong class="bold">gigabytes</strong> (<strong class="bold">GB</strong>), and the whole genome alignment (at low coverage of 4x) is 40.1 GB. This data is a paired end with reads of 76 bp. This is common nowadays, but slightly more complex to process. We will take this into account. If your data is not paired, just simplify the following recipe appropriately.</p>
<p>The cell at the top of <strong class="source-inline">Chapter03/Working_with_BAM.py</strong> will download the data for you. The files you will want are <strong class="source-inline">NA18490_20_exome.bam</strong> and <strong class="source-inline">NA18490_20_exome.bam.bai</strong>.</p>
<p>We will use <strong class="source-inline">pysam</strong>, a Python wrapper to the SAMtools C API. You can install it with the following command:</p>
<p class="source-code">conda install –c bioconda pysam</p>
<p>OK—let’s get started.</p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor087"/>How to do it...</h2>
<p>Before you start <a id="_idIndexMarker197"/>coding, note that you can inspect the BAM file using <strong class="source-inline">samtools view -h</strong> (this is if you have SAMtools installed, which we recommend, even <a id="_idIndexMarker198"/>if you use the <strong class="bold">Genome Analysis Toolkit</strong> (<strong class="bold">GATK</strong>) or something else for variant calling). We suggest that you take a look at the header file and the first few records. The SAM format is too complex to be described here. There is plenty of information on the internet about it; nonetheless, sometimes, there’s some really interesting information buried in these header files.</p>
<p class="callout-heading">Tip</p>
<p class="callout">One of the most complex operations in NGS is to generate good alignment files from raw sequence data. This not only calls the aligner but also cleans up data. Now, in the <strong class="source-inline">@PG</strong> headers of high-quality BAM files, you will find the actual command lines used for most—if not all—of the procedures used to generate this BAM file. In our example BAM file, you will find all the information needed to run bwa, SAMtools, GATK IndelRealigner, and the Picard application suite to clean up data. Remember that while you can generate BAM files easily, the programs after it will be quite picky in terms of the correctness of the BAM input. For instance, if you use GATK’s variant caller to generate genotype calls, the files will have to be extensively cleaned. The header of other BAM files can thus provide you with the best way to generate yours. A final recommendation is that if you do not work with human data, try to find good BAMs for your species, because the parameters of a given program may be slightly different. Also, if you use something other than the WGS data, check for similar types of sequencing data.</p>
<p>Let’s take <a id="_idIndexMarker199"/>a look at the following steps:</p>
<ol>
<li value="1">Let’s inspect the header files:<p class="source-code">import pysam</p><p class="source-code">bam = pysam.AlignmentFile('NA18489.chrom20.ILLUMINA.bwa.YRI.exome.20121211.bam', 'rb')</p><p class="source-code">headers = bam.header</p><p class="source-code">for record_type, records in headers.items():</p><p class="source-code">    print (record_type)</p><p class="source-code">    for i, record in enumerate(records):</p><p class="source-code">        if type(record) == dict:</p><p class="source-code">            print('\t%d' % (i + 1))</p><p class="source-code">            for field, value in record.items():</p><p class="source-code">                print('\t\t%s\t%s' % (field, value))</p><p class="source-code">        else:</p><p class="source-code">            print('\t\t%s' % record)</p></li>
</ol>
<p>The header is represented as a dictionary (where the key is <strong class="source-inline">record_type</strong>). As there can be several instances of the same <strong class="source-inline">record_type</strong>, the value of the dictionary is a list (where each element is—again—a dictionary, or sometimes a string containing tag/value pairs).</p>
<ol>
<li value="2">We will now inspect a single record. The amount of data per record is quite complex. Here, we will focus on some of the fundamental fields for paired-end reads. Check the SAM file specification and the <strong class="source-inline">pysam</strong> API documentation for more details:<p class="source-code">for rec in bam:</p><p class="source-code">    if rec.cigarstring.find('M') &gt; -1 and rec.cigarstring.find('S') &gt; -1 and not rec.is_unmapped and not rec.mate_is_unmapped:</p><p class="source-code">    break</p><p class="source-code">print(rec.query_name, rec.reference_id, bam.getrname(rec.reference_id), rec.reference_start, rec.reference_end)</p><p class="source-code">print(rec.cigarstring)</p><p class="source-code">print(rec.query_alignment_start, rec.query_alignment_end, rec.query_alignment_length)</p><p class="source-code">print(rec.next_reference_id, rec.next_reference_start,rec.template_length)</p><p class="source-code">print(rec.is_paired, rec.is_proper_pair, rec.is_unmapped, rec.mapping_quality)</p><p class="source-code">print(rec.query_qualities)</p><p class="source-code">print(rec.query_alignment_qualities)</p><p class="source-code">print(rec.query_sequence)</p></li>
</ol>
<p>Note that <a id="_idIndexMarker200"/>the BAM file object can be iterated <a id="_idIndexMarker201"/>over its records. We will transverse it until we find a record whose <strong class="bold">Concise Idiosyncratic Gapped Alignment Report</strong> (<strong class="bold">CIGAR</strong>) string contains a match and a soft clip.</p>
<p>The CIGAR string gives an indication of the alignment of individual bases. The clipped part of the sequence is the part that the aligner failed to align (but is not removed from the sequence). We will also want the read, its mate ID, and position (of the pair, as we have paired-end reads) that was mapped to the reference genome.</p>
<p>First, we print the query template name, followed by the reference ID. The reference ID is <a id="_idIndexMarker202"/>a pointer to the name of the sequence on the given references on the lookup table of references. An example will make this clear. For all records on this BAM file, the reference ID is <strong class="source-inline">19</strong> (a non-informative number), but if you apply <strong class="source-inline">bam.getrname(19)</strong>, you will get <strong class="source-inline">20</strong>, which is the name of the chromosome. So, do not confuse the reference ID (in this case, <strong class="source-inline">19</strong>) with the name of the chromosome (<strong class="source-inline">20</strong>). This is then followed by the reference start and reference end. <strong class="source-inline">pysam</strong> is 0-based, not 1-based, so be careful when you convert coordinates to other libraries. You will notice that <a id="_idIndexMarker203"/>the start and end for this case are 59,996 and 60,048, which means an alignment of 52 bases. Why are there only 52 bases when the read size is 76 (remember—the read size used in this BAM file)? The answer can be found on the CIGAR string, which in our case will be <strong class="source-inline">52M24S</strong>, which is a 52-bases match, followed by 24 bases that were soft-clipped.</p>
<p>Then, we print where the alignment starts and ends and calculate its length. By the way, you can compute this by looking at the CIGAR string. It starts at 0 (as the first part of the read is mapped) and ends at 52. The length is 76 again.</p>
<p>Now, we query the mate (something that you will only do if you have paired-end reads). We get its reference ID (as shown in the previous code snippet), its start position, and a measure of the distance between both pairs. This measure of distance only makes sense if both mates are mapped to the same chromosome.</p>
<p>We then plot the Phred score (refer to the previous recipe, <em class="italic">Working with modern sequence formats</em>, on Phred scores) for the sequence, and then only for the aligned part. Finally, we print the sequence (don’t forget to do this!). This is the complete sequence, not the clipped one (of course, you can use the preceding coordinates to clip).</p>
<ol>
<li value="3">Now, let’s plot the distribution of the successfully mapped positions in a subset of sequences in the BAM file:<p class="source-code">import seaborn as sns</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">counts = [0] * 76</p><p class="source-code">for n, rec in enumerate(bam.fetch('20', 0, 10000000)):</p><p class="source-code">    for i in range(rec.query_alignment_start, rec.query_alignment_end):</p><p class="source-code">        counts[i] += 1</p><p class="source-code">freqs = [x / (n + 1.) for x in counts]</p><p class="source-code">fig, ax = plt.subplots(figsize=(16,9))</p><p class="source-code">ax.plot(range(1, 77), freqs)</p><p class="source-code">ax.set_xlabel('Read distance')</p><p class="source-code">ax.set_ylabel('PHRED score')</p><p class="source-code">fig.suptitle('Percentage of mapped calls as a function of the position from the start of the sequencer read')</p></li>
</ol>
<p>We will <a id="_idIndexMarker204"/>start by initializing an array to keep the count for the entire <strong class="source-inline">76</strong> positions. Note that we then fetch only the records for chromosome 20 between positions 0 and 10 <strong class="bold">megabase pairs</strong> (<strong class="bold">Mbp</strong>). We will just use a small part of the chromosome here. It’s fundamental to have an index (generated by <strong class="source-inline">tabix</strong>) for these kinds of fetch operations; the speed of execution will be completely different.</p>
<p>We traverse all records in the 10 Mbp boundary. For each boundary, we get the alignment start and end and increase the counter of mappability among the positions that were aligned. Finally, we convert this into frequencies, and then plot it, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<img alt="Figure 3.3 – The percentage of mapped calls as a function of the position from the start of the sequencer read " height="686" src="image/B17942_03_003.jpg" width="1228"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – The percentage of mapped calls as a function of the position from the start of the sequencer read</p>
<p>It’s quite clear that the distribution of mappability is far from being uniform; it’s worse at the extremes, with a drop in the middle.</p>
<ol>
<li value="4">Finally, let’s <a id="_idIndexMarker205"/>get the distribution of Phred scores across the mapped part of the reads. As you may suspect, this is probably not going to be uniform:<p class="source-code">from collections import defaultdict</p><p class="source-code">import numpy as np</p><p class="source-code">phreds = defaultdict(list)</p><p class="source-code">for rec in bam.fetch('20', 0, None):</p><p class="source-code">    for i in range(rec.query_alignment_start, rec.query_alignment_end):</p><p class="source-code">        phreds[i].append(rec.query_qualities[i])</p><p class="source-code">maxs = [max(phreds[i]) for i in range(76)]</p><p class="source-code">tops = [np.percentile(phreds[i], 95) for i in range(76)]</p><p class="source-code">medians = [np.percentile(phreds[i], 50) for i in range(76)]</p><p class="source-code">bottoms = [np.percentile(phreds[i], 5) for i in range(76)]</p><p class="source-code">medians_fig = [x - y for x, y in zip(medians, bottoms)]</p><p class="source-code">tops_fig = [x - y for x, y in zip(tops, medians)]</p><p class="source-code">maxs_fig = [x - y for x, y in zip(maxs, tops)]</p><p class="source-code">fig, ax = plt.subplots(figsize=(16,9))</p><p class="source-code">ax.stackplot(range(1, 77), (bottoms, medians_fig,tops_fig))</p><p class="source-code">ax.plot(range(1, 77), maxs, 'k-')</p><p class="source-code">ax.set_xlabel('Read distance')</p><p class="source-code">ax.set_ylabel('PHRED score')</p><p class="source-code">fig.suptitle('Distribution of PHRED scores as a function of the position in the read')</p></li>
</ol>
<p>Here, we again use default dictionaries that allow you to use a bit of initialization code. We now fetch from start to end and create a list of Phred scores in a dictionary whose index is the relative position in the sequence read.</p>
<p>We then <a id="_idIndexMarker206"/>use NumPy to calculate the 95th, 50th (median), and 5th percentiles, along with the maximum of quality scores per position. For most computational biology analyses, having a statistical summarized view of the data is quite common. So, you’re probably already familiar with not only percentile calculations, but also with other Pythonic ways to calculate means, standard deviations, maximums, and minimums.</p>
<p>Finally, we will perform a stacked plot of the distribution of Phred scores per position. Due to the way <strong class="source-inline">matplotlib</strong> expects stacks, we have to subtract the value of the lower percentile from the one before with the <strong class="source-inline">stackplot</strong> call. We can use the list for the bottom percentiles, but we have to correct the median and the top, as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 3.4 – The distribution of Phred scores as a function of the position in the read; the bottom blue color spans from 0 to the 5th percentile; the green color up to the median, red to the 95th percentile, and purple to the maximum " height="684" src="image/B17942_03_004.jpg" width="1233"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – The distribution of Phred scores as a function of the position in the read; the bottom blue color spans from 0 to the 5th percentile; the green color up to the median, red to the 95th percentile, and purple to the maximum</p>
<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>There’s more...</h2>
<p>Although we will <a id="_idIndexMarker207"/>discuss data filtering in the <em class="italic">Studying genome accessibility and filtering SNP data</em> recipe of this chapter, it’s not our objective to explain the SAM format in detail or give a detailed course in data filtering. This task would require a book of its own, but with the basics of <strong class="source-inline">pysam</strong>, you can navigate through SAM/BAM files. However, in the last recipe of this chapter, we will take a look at extracting genome-wide metrics from BAM files (via annotations on VCF files that represent metrics of BAM files) for the purpose of understanding the overall quality of our dataset.</p>
<p>You will probably have very large data files to work with. It’s possible that some BAM processing will take too much time. One of the first approaches to reducing the computation time is subsampling. For example, if you subsample at 10 percent, you ignore 9 records out of 10. For many tasks, such as some of the analysis done for the quality assessment of BAM files, subsampling at 10 percent (or even 1 percent) will be enough to get the gist of the quality of the file.</p>
<p>If you use <a id="_idIndexMarker208"/>human data, you may have your data sequenced at Complete Genomics. In this case, the alignment files will be different. Although Complete Genomics provides tools to convert to standard formats, you might be served better if you use its own data.</p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor089"/>See also</h2>
<p>Additional information can be found at the following links:</p>
<ul>
<li>The SAM/BAM <a id="_idIndexMarker209"/>format is described at <a href="http://samtools.github.io/hts-specs/SAMv1.pdf">http://samtools.github.io/hts-specs/SAMv1.pdf</a>.</li>
<li>You can find <a id="_idIndexMarker210"/>an introductory explanation of the SAM format on the Abecasis group wiki page at <a href="http://genome.sph.umich.edu/wiki/SAM">http://genome.sph.umich.edu/wiki/SAM</a>.</li>
<li>If you really need to get complex statistics from BAM files, Alistair Miles’ <strong class="source-inline">pysamstats</strong> library <a id="_idIndexMarker211"/>is your port of call, at <a href="https://github.com/alimanfoo/pysamstats">https://github.com/alimanfoo/pysamstats</a>.</li>
<li>To convert <a id="_idIndexMarker212"/>your raw sequence data to alignment data, you will need an aligner; the most widely used is bwa (<a href="http://bio-bwa.sourceforge.net/">http://bio-bwa.sourceforge.net/</a>).</li>
<li>Picard (surely a reference to <em class="italic">Star Trek: The Next Generation</em>) is the most commonly used tool to clean up BAM files; refer to <a href="http://broadinstitute.github.io/picard/">http://broadinstitute.github.io/picard/</a>.</li>
<li>The technical <a id="_idIndexMarker213"/>forum for sequence analysis is called <em class="italic">SEQanswers </em>(<a href="http://seqanswers.com/">http://seqanswers.com/</a>).</li>
<li>I would like to repeat the recommendation on Biostars here (which is referred to in the previous recipe, <em class="italic">Working with modern sequence formats</em>); it’s a treasure trove of information and has a very friendly community, at <a href="http://www.biostars.org/">http://www.biostars.org/</a>.</li>
<li>If you have the Complete Genomics data, take a look at the <strong class="bold">frequently asked questions</strong> (<strong class="bold">FAQs</strong>) at <a href="http://www.completegenomics.com/customer-support/faqs/">http://www.completegenomics.com/customer-support/faqs/</a>.</li>
</ul>
<h1 id="_idParaDest-91"><a id="_idTextAnchor090"/>Extracting data from VCF files</h1>
<p>After running a <a id="_idIndexMarker214"/>genotype caller (for example, GATK or SAMtools), you <a id="_idIndexMarker215"/>will have a VCF file reporting <a id="_idIndexMarker216"/>on genomic variations, such as SNPs, <strong class="bold">insertions/deletions</strong> (<strong class="bold">INDELs</strong>), <strong class="bold">copy number variations</strong> (<strong class="bold">CNVs</strong>), and so on. In this <a id="_idIndexMarker217"/>recipe, we will discuss VCF processing with the <strong class="source-inline">cyvcf2</strong> module. </p>
<h2 id="_idParaDest-92"><a id="_idTextAnchor091"/>Getting ready</h2>
<p>While NGS is all about big data, there is a limit to how much I can ask you to download as a dataset for this book. I believe that 2 to 20 GB of data for a tutorial is asking too much. While the 1,000 Genomes VCF files with realistic annotations are in this OOM, we will want to work with much less data here. Fortunately, the bioinformatics community has <a id="_idIndexMarker218"/>developed tools to allow for the partial download of data. As part of the SAMtools/<strong class="source-inline">htslib</strong> package (<a href="http://www.htslib.org/">http://www.htslib.org/</a>), you can download <strong class="source-inline">tabix</strong> and <strong class="source-inline">bgzip</strong>, which will take care of data management. On the command line, perform the following operation:</p>
<pre class="source-code">
<strong class="bold">tabix -fh ftp://ftp-</strong>
<strong class="bold">trace.ncbi.nih.gov/1000genomes/ftp/release/20130502/supporting/vcf_with_sample_level_annotation/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5_extra_anno.20130502.genotypes.vcf.gz 22:1-17000000 | bgzip -c &gt; genotypes.vcf.gz</strong>
<strong class="bold">tabix -p vcf genotypes.vcf.gz</strong></pre>
<p>The first line will partially download the VCF file for chromosome 22 (up to 17 Mbp) of the 1,000 Genomes Project. Then, <strong class="source-inline">bgzip</strong> will compress it.</p>
<p>The second line will create an index, which we will need for direct access to a section of the genome. As usual, you have the code to do this in a notebook (the <strong class="source-inline">Chapter03/Working_with_VCF.py</strong> file).</p>
<p>You will need to install <strong class="source-inline">cyvcf2</strong>:</p>
<p class="source-code">conda install –c bioconda cyvcf2</p>
<p class="callout-heading">Tip</p>
<p class="callout">If you have conflict resolution problems, you can try using <strong class="source-inline">pip</strong> instead. This is a last-resort solution that you will find yourself doing with <strong class="source-inline">conda</strong>, as it is incapable of resolving package dependencies, something that happens more often than not. You can execute <strong class="source-inline">pip install cyvcf2</strong>.</p>
<h2 id="_idParaDest-93"><a id="_idTextAnchor092"/>How to do it...</h2>
<p>Take a <a id="_idIndexMarker219"/>look at the following steps:</p>
<ol>
<li value="1">Let’s start <a id="_idIndexMarker220"/>by inspecting the information that we can get per record:<p class="source-code">from cyvcf2 import VCF</p><p class="source-code">v = VCF('genotypes.vcf.gz')</p><p class="source-code">rec = next(v)</p><p class="source-code">print('Variant Level information')</p><p class="source-code">info = rec.INFO</p><p class="source-code">for info in rec.INFO:</p><p class="source-code">    print(info)</p><p class="source-code">print('Sample Level information')</p><p class="source-code">for fmt in rec.FORMAT:</p><p class="source-code">    print(fmt)</p></li>
</ol>
<p>We start by inspecting the annotations that are available for each record (remember that each record encodes a variant, such as SNP, CNV, INDELs, and so on, and the state of that variant per sample). At the variant (record) level, we find <strong class="source-inline">AC</strong>—the total number of <strong class="source-inline">ALT</strong> alleles in called genotypes, <strong class="source-inline">AF</strong>—the estimated allele frequency, <strong class="source-inline">NS</strong>—the number of samples with data, <strong class="source-inline">AN</strong>—the total number of alleles in called genotypes, and <strong class="source-inline">DP</strong>—the total read depth. There are others, but they are mostly specific to the 1,000 Genomes Project (here, we will try <a id="_idIndexMarker221"/>to be as general as possible). Your own dataset <a id="_idIndexMarker222"/>may have more annotations (or none of these).</p>
<p>At the sample level, there are only two annotations in this file: <strong class="source-inline">GT</strong>—genotype, and <strong class="source-inline">DP</strong>—the per-sample read depth. You have the per-variant (total) read depth and the per-sample read depth; be sure not to confuse both.</p>
<ol>
<li value="2">Now that we know what information is available, let’s inspect a single VCF record:<p class="source-code">v = VCF('genotypes.vcf.gz')</p><p class="source-code">samples = v.samples</p><p class="source-code">print(len(samples))</p><p class="source-code">variant = next(v)</p><p class="source-code">print(variant.CHROM, variant.POS, variant.ID, variant.REF, variant.ALT, variant.QUAL, variant.FILTER)</p><p class="source-code">print(variant.INFO)</p><p class="source-code">print(variant.FORMAT)</p><p class="source-code">print(variant.is_snp)</p><p class="source-code">str_alleles = variant.gt_bases[0]</p><p class="source-code">alleles = variant.genotypes[0][0:2]</p><p class="source-code">is_phased = variant.genotypes[0][2]</p><p class="source-code">print(str_alleles, alleles, is_phased)</p><p class="source-code">print(variant.format('DP')[0])</p></li>
</ol>
<p>We will start by retrieving the standard information: the chromosome, position, ID, reference base (typically just one) and alternative bases (you can have more than one, but it’s not uncommon as a first filtering approach to only accept a single <strong class="source-inline">ALT</strong>, for example, only accept biallelic SNPs), quality (as you might expect, Phred-scaled), and filter status. Regarding the filter status, remember <a id="_idIndexMarker223"/>that whatever the VCF file says, you may <a id="_idIndexMarker224"/>still want to apply extra filters (as in the next recipe, <em class="italic">Studying genome accessibility and filtering SNP data</em>).</p>
<p>We then print the additional variant-level information (<strong class="source-inline">AC</strong>, <strong class="source-inline">AS</strong>, <strong class="source-inline">AF</strong>, <strong class="source-inline">AN</strong>, <strong class="source-inline">DP</strong>, and so on), followed by the sample format (in this case, <strong class="source-inline">DP</strong> and <strong class="source-inline">GT</strong>). Finally, we count the number of samples and inspect a single sample to check whether it was called for this variant. Also, the reported alleles, heterozygosity, and phasing status (this dataset happens to be phased, which is not that common) are included.</p>
<ol>
<li value="3">Let’s check the type of variant and the number of nonbiallelic SNPs in a single pass:<p class="source-code">from collections import defaultdict</p><p class="source-code">f = VCF('genotypes.vcf.gz')</p><p class="source-code">my_type = defaultdict(int)</p><p class="source-code">num_alts = defaultdict(int)</p><p class="source-code">for variant in f:</p><p class="source-code">    my_type[variant.var_type, variant.var_subtype] += 1</p><p class="source-code">    if variant.var_type == 'snp':</p><p class="source-code">        num_alts[len(variant.ALT)] += 1</p><p class="source-code">print(my_type)</p></li>
</ol>
<p>We will now use the now-common Python default dictionary. We find that this dataset has INDELs, CNVs, and—of course—SNPs (roughly two-thirds being transitions with one-third transversions). There is a residual number (79) of triallelic SNPs.</p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor093"/>There’s more...</h2>
<p>The purpose <a id="_idIndexMarker225"/>of this recipe is to get you up to speed with the <strong class="source-inline">cyvcf2</strong> module. At this stage, you should be comfortable with the API. We will not <a id="_idIndexMarker226"/>spend too much time on usage details because this will be the main purpose of the next recipe: using the VCF module to study the quality of your variant calls.</p>
<p>While <strong class="source-inline">cyvcf2</strong> is quite fast, it can still take a lot of time to process text-based VCF files. There are two main strategies for dealing with this problem. One strategy is parallel processing, which we will discuss in the last chapter, <a href="B17942_09.xhtml#_idTextAnchor237"><em class="italic">Chapter 9</em></a>, <em class="italic">Bioinformatics Pipelines</em>. The second strategy is to convert to a more efficient format; we will provide an example of this in <a href="B17942_06.xhtml#_idTextAnchor154"><em class="italic">Chapter 6</em></a>, <em class="italic">Population Genetics</em>. Note that VCF developers <a id="_idIndexMarker227"/>are working on a <strong class="bold">Binary Variant Call Format</strong> (<strong class="bold">BCF</strong>) version to deal with parts of these problems (<a href="http://www.1000genomes.org/wiki/analysis/variant-call-format/bcf-binary-vcf-version-2">http://www.1000genomes.org/wiki/analysis/variant-call-format/bcf-binary-vcf-version-2</a>).</p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>See also</h2>
<p>Some useful links are as follows:</p>
<ul>
<li>The specification for VCF is available at <a href="http://samtools.github.io/hts-specs/VCFv4.2.pdf">http://samtools.github.io/hts-specs/VCFv4.2.pdf</a>.</li>
<li>GATK is one of the most widely used variant callers; check out <a href="https://www.broadinstitute.org/gatk/">https://www.broadinstitute.org/gatk/</a> for details.</li>
<li>SAMtools and <strong class="source-inline">htslib</strong> are used for variant calling and SAM/BAM management; check out <a href="http://htslib.org">http://htslib.org</a> for details.</li>
</ul>
<h1 id="_idParaDest-96"><a id="_idTextAnchor095"/>Studying genome accessibility and filtering SNP data</h1>
<p>While the previous recipes were focused on giving an overview of Python libraries to deal with <a id="_idIndexMarker228"/>alignment and variant call data, in this recipe, we will concentrate on actually using them with a clear purpose in mind.</p>
<p>If you are <a id="_idIndexMarker229"/>using NGS data, chances are that your most important file to analyze is a VCF file, which is produced by a genotype caller such as SAMtools, <strong class="source-inline">mpileup</strong>, or GATK. The quality of your VCF calls may need to be assessed and filtered. Here, we will put in place a framework to filter SNP data. Rather than giving you filtering rules (an impossible task to be performed in a general way), we will give you procedures to assess the quality of your data. With this, you can devise your own filters.</p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor096"/>Getting ready</h2>
<p>In the best-case scenario, you have a VCF file with proper filters applied. If this is the case, you can just go ahead and use your file. Note that all VCF files will have a <strong class="source-inline">FILTER</strong> column, but this might not mean that all of the proper filters were applied. You have to be sure that your data is properly filtered.</p>
<p>In the second case, which is one of the most common, your file will have unfiltered data, but you’ll have enough annotations and can apply hard filters (there is no need for programmatic filtering). If you have a GATK annotated file, refer to <a href="http://gatkforums.broadinstitute.org/discussion/2806/howto-apply-hard-filters-to-a-call-set">http://gatkforums.broadinstitute.org/discussion/2806/howto-apply-hard-filters-to-a-call-set</a>.</p>
<p>In the third case, you have a VCF file that has all the annotations that you need, but you may want to apply more flexible filters (for example, “if read depth &gt; 20, accept if mapping quality &gt; 30; otherwise, accept if mapping quality &gt; 40”).</p>
<p>In the fourth case, your VCF file does not have all the necessary annotations and you have to revisit your BAM files (or even other sources of information). In this case, the best solution is to find whatever extra information you can and create a new VCF file with the required annotations. Some genotype callers (such as GATK) allow you to specify which annotations you want; you may also want to use extra programs to provide <a id="_idIndexMarker230"/>more annotations. For example, <strong class="bold">SnpEff</strong> (<a href="http://snpeff.sourceforge.net/">http://snpeff.sourceforge.net/</a>) will annotate your SNPs with predictions of their effect (for example, if they are in exons, are they coding or non-coding?).</p>
<p>It’s impossible to provide a clear-cut recipe, as it will vary with your type of sequencing data, your species of study, and your tolerance to errors, among other variables. What we can do is provide a set of typical analyses that is done for high-quality filtering.</p>
<p>In this recipe, we will not use data from the human 1,000 Genomes Project. We want <em class="italic">dirty</em>, unfiltered data that has a lot of common annotations that can be used to filter it. We will use data from the Anopheles gambiae 1,000 Genomes Project (Anopheles is a mosquito vector involved in the transmission of a parasite that causes malaria), which makes filtered and unfiltered data available. You can find more information on this project at <a href="http://www.malariagen.net/projects/vector/ag1000g">http://www.malariagen.net/projects/vector/ag1000g</a>.</p>
<p>We will get <a id="_idIndexMarker231"/>a part of the centromere of chromosome <strong class="source-inline">3L</strong> for around 100 mosquitoes, which is followed by a part somewhere in the middle <a id="_idIndexMarker232"/>of this chromosome (and index both):</p>
<pre class="source-code">
<strong class="bold">tabix -fh ftp://ngs.sanger.ac.uk/production/ag1000g/phase1/preview/ag1000g.AC.phase1.AR1.vcf.gz 3L:1-200000 |bgzip -c &gt; centro.vcf.gz</strong>
<strong class="bold">tabix -fh ftp://ngs.sanger.ac.uk/production/ag1000g/phase1/preview/ag1000g.AC.phase1.AR1.vcf.gz 3L:21000001-21200000 |bgzip -c &gt; standard.vcf.gz</strong>
<strong class="bold">tabix -p vcf centro.vcf.gz</strong>
<strong class="bold">tabix -p vcf standard.vcf.gz</strong></pre>
<p>If the links do not work, be sure to check <a href="https://github.com/PacktPublishing/Bioinformatics-with-Python-Cookbook-third-edition/blob/main/Datasets.py">https://github.com/PacktPublishing/Bioinformatics-with-Python-Cookbook-third-edition/blob/main/Datasets.py</a> for updates. As usual, the code for downloading this data is available in the <strong class="source-inline">Chapter02/Filtering_SNPs.ipynb</strong> notebook.</p>
<p>Finally, a word of warning on this recipe: the level of Python here will be slightly more complicated than usual. The more general code we write, the easier it will be for you to reuse it for your specific case. We will use functional programming techniques (<strong class="source-inline">lambda</strong> functions) and the <strong class="source-inline">partial</strong> function application extensively.</p>
<h2 id="_idParaDest-98"><a id="_idTextAnchor097"/>How to do it...</h2>
<p>Take a look <a id="_idIndexMarker233"/>at the following steps:</p>
<ol>
<li value="1">Let’s start <a id="_idIndexMarker234"/>by plotting the distribution of variants across the genome in both files:<p class="source-code">from collections import defaultdict</p><p class="source-code">import functools</p><p class="source-code">import numpy as np</p><p class="source-code">import seaborn as sns</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from cyvcf2 import VCF</p><p class="source-code">def do_window(recs, size, fun):</p><p class="source-code">    start = None</p><p class="source-code">    win_res = []</p><p class="source-code">    for rec in recs:</p><p class="source-code">        if not rec.is_snp or len(rec.ALT) &gt; 1:</p><p class="source-code">            continue</p><p class="source-code">        if start is None:</p><p class="source-code">            start = rec.POS</p><p class="source-code">        my_win = 1 + (rec.POS - start) // size</p><p class="source-code">        while len(win_res) &lt; my_win:</p><p class="source-code">            win_res.append([])</p><p class="source-code">        win_res[my_win - 1].extend(fun(rec))</p><p class="source-code">    return win_res</p><p class="source-code">wins = {}</p><p class="source-code">size = 2000</p><p class="source-code">names = ['centro.vcf.gz', 'standard.vcf.gz']</p><p class="source-code">for name in names:</p><p class="source-code"> recs = VCF(name)</p><p class="source-code"> wins[name] = do_window(recs, size, lambda x: [1])</p></li>
</ol>
<p>We will start by performing the required imports (as usual, remember to remove the <a id="_idIndexMarker235"/>first line if you are not using the IPython Notebook). Before I explain the function, note what we are doing.</p>
<p>For both files, we will <a id="_idIndexMarker236"/>compute windowed statistics. We will divide our file, which includes 200,000 bp of data, into windows of size 2,000 (100 windows). Every time we find a biallelic SNP, we will add a 1 to the list related to this window in the <strong class="source-inline">window</strong> function.</p>
<p>The <strong class="source-inline">window</strong> function will take a VCF record (a <strong class="source-inline">rec.is_snp</strong> SNP that is not biallelic len <strong class="source-inline">(rec.ALT) == 1</strong>), determine the window where that record belongs (by performing an integer division of <strong class="source-inline">rec.POS</strong> by size), and extend the list of results of that window by the function passed to it as the <strong class="source-inline">fun</strong> parameter (which, in our case, is just a 1).</p>
<p>So, now we have a list of 100 elements (each representing 2,000 bp). Each element will be another list that will have a 1 for each biallelic SNP found.</p>
<p>So, if you have 200 SNPs in the first 2,000 bp, the first element of the list will have 200 ones.</p>
<ol>
<li value="2">Let’s continue, as follows:<p class="source-code">def apply_win_funs(wins, funs):</p><p class="source-code">    fun_results = []</p><p class="source-code">    for win in wins:</p><p class="source-code">        my_funs = {}</p><p class="source-code">        for name, fun in funs.items():</p><p class="source-code">            try:</p><p class="source-code">                my_funs[name] = fun(win)</p><p class="source-code">            except:</p><p class="source-code">                my_funs[name] = None</p><p class="source-code">        fun_results.append(my_funs)</p><p class="source-code">    return fun_results</p><p class="source-code">stats = {}</p><p class="source-code">fig, ax = plt.subplots(figsize=(16, 9))</p><p class="source-code">for name, nwins in wins.items():</p><p class="source-code">    stats[name] = apply_win_funs(nwins, {'sum': sum})</p><p class="source-code">    x_lim = [i * size for i in range(len(stats[name]))]</p><p class="source-code">    ax.plot(x_lim, [x['sum'] for x in stats[name]], label=name)</p><p class="source-code">ax.legend()</p><p class="source-code">ax.set_xlabel('Genomic location in the downloaded segment')</p><p class="source-code">ax.set_ylabel('Number of variant sites (bi-allelic SNPs)')</p><p class="source-code">fig.suptitle('Number of bi-allelic SNPs along the genome', fontsize='xx-large')</p></li>
</ol>
<p>Here, we <a id="_idIndexMarker237"/>perform a plot that contains statistical <a id="_idIndexMarker238"/>information for each of our 100 windows. <strong class="source-inline">apply_win_funs</strong> will calculate a set of statistics for every window. In this case, it will sum all the numbers in the window. Remember that every time we find an SNP, we add 1 to the window list. This means that if we have 200 SNPs, we will have 200 ones; hence, summing them will return 200.</p>
<p>So, we are able to compute the number of SNPs per window in an apparently convoluted way. Why we perform things with this strategy will become <a id="_idIndexMarker239"/>apparent soon. However, for now, let’s check <a id="_idIndexMarker240"/>the result of this computation for both files, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 3.5 – The number of biallelic SNP distributed windows of 2,000 bp in size for an area of 200 kilobase pairs (kbp) near the centromere (orange), and in the middle of the chromosome (blue); both areas come from chromosome 3L for circa 100 Ugandan mosquitoes from the Anopheles 1,000 Genomes Project " height="706" src="image/B17942_03_005.jpg" width="1272"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – The number of biallelic SNP distributed windows of 2,000 bp in size for an area of 200 kilobase pairs (kbp) near the centromere (orange), and in the middle of the chromosome (blue); both areas come from chromosome 3L for circa 100 Ugandan mosquitoes from the Anopheles 1,000 Genomes Project</p>
<p class="callout-heading">Tip</p>
<p class="callout">Note that the amount of SNPs in the centromere is smaller than in the middle of the chromosome. This is expected because calling variants in chromosomes is more difficult than calling in the middle. Also, there is probably less genomic diversity in centromeres. If you are used to humans or other mammals, you will find the density of variants obnoxiously high—that’s mosquitoes for you!</p>
<ol>
<li value="3">Let’s take a look at the sample-level annotation. We will inspect mapping quality zero (refer to <a href="https://www.broadinstitute.org/gatk/guide/tooldocs/org_broadinstitute_gatk_tools_walkers_annotator_MappingQualityZeroBySample.php">https://www.broadinstitute.org/gatk/guide/tooldocs/org_broadinstitute_gatk_tools_walkers_annotator_MappingQualityZeroBySample.php</a> for details), which is a measure of how well sequences involved in calling this variant map clearly to this position. Note that there is also an <strong class="source-inline">MQ0</strong> annotation at the variant level:<p class="source-code">mq0_wins = {}</p><p class="source-code">size = 5000</p><p class="source-code">def get_sample(rec, annot, my_type):</p><p class="source-code">    return [v for v in rec.format(annot) if v &gt; np.iinfo(my_type).min]</p><p class="source-code">for vcf_name in vcf_names:</p><p class="source-code">    recs = vcf.Reader(filename=vcf_name)</p><p class="source-code">    mq0_wins[vcf_name] = do_window(recs, size, functools.partial(get_sample, annot='MQ0', my_type=np.int32))</p></li>
</ol>
<p>Start inspecting <a id="_idIndexMarker241"/>this by looking at the last <strong class="source-inline">for</strong>; we will <a id="_idIndexMarker242"/>perform a windowed analysis by reading the <strong class="source-inline">MQ0</strong> annotation from each record. We perform this by calling the <strong class="source-inline">get_sample</strong> function, which will return our preferred annotation (in this case, <strong class="source-inline">MQ0</strong>) that has been cast with a certain type (<strong class="source-inline">my_type=np.int32</strong>). We use the <strong class="source-inline">partial</strong> application function here. Python allows you to specify some parameters of a function and wait for other parameters to be specified later. Note that the most complicated thing here is the functional programming style. Also, note that it makes it very easy to compute other sample-level annotations. Just replace <strong class="source-inline">MQ0</strong> with <strong class="source-inline">AB</strong>, <strong class="source-inline">AD</strong>, <strong class="source-inline">GQ</strong>, and so on. You immediately have a computation for that annotation. If the annotation is not of the integer type, no problem; just adapt <strong class="source-inline">my_type</strong>. It’s a difficult programming style if you are not used to it, but you will reap its benefits very soon.</p>
<ol>
<li value="4">Now, let’s print <a id="_idIndexMarker243"/>the median and the top 75 <a id="_idIndexMarker244"/>percentile for each window (in this case, with a size of 5,000):<p class="source-code">stats = {}</p><p class="source-code">colors = ['b', 'g']</p><p class="source-code">i = 0</p><p class="source-code">fig, ax = plt.subplots(figsize=(16, 9))</p><p class="source-code">for name, nwins in mq0_wins.items():</p><p class="source-code">    stats[name] = apply_win_funs(nwins, {'median':np.median, '75': functools.partial(np.percentile, q=75)})</p><p class="source-code">    x_lim = [j * size for j in range(len(stats[name]))]</p><p class="source-code">    ax.plot(x_lim, [x['median'] for x in stats[name]], label=name, color=colors[i])</p><p class="source-code">    ax.plot(x_lim, [x['75'] for x in stats[name]], '--', color=colors[i])</p><p class="source-code">    i += 1</p><p class="source-code">ax.legend()</p><p class="source-code">ax.set_xlabel('Genomic location in the downloaded segment')</p><p class="source-code">ax.set_ylabel('MQ0')</p><p class="source-code">fig.suptitle('Distribution of MQ0 along the genome', fontsize='xx-large')</p></li>
</ol>
<p>Note that we now have two different statistics on <strong class="source-inline">apply_win_funs</strong> (percentile and median). Again, we pass functions as parameters (<strong class="source-inline">np.median</strong> and <strong class="source-inline">np.percentile</strong>), with <strong class="source-inline">partial</strong> function application done on <strong class="source-inline">np.percentile</strong>. The result looks like this:</p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 3.6 – Median (continuous line) and 75th percentile (dashed) of MQ0 of sample SNPs distributed on windows of 5,000 bp in size for an area of 200 kbp near the centromere (blue) and in the middle of chromosome (green); both areas come from chromosome 3L for circa 100 Ugandan mosquitoes from the Anopheles 1,000 Genomes Project " height="571" src="image/B17942_03_006.jpg" width="898"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Median (continuous line) and 75th percentile (dashed) of MQ0 of sample SNPs distributed on windows of 5,000 bp in size for an area of 200 kbp near the centromere (blue) and in the middle of chromosome (green); both areas come from chromosome 3L for circa 100 Ugandan mosquitoes from the Anopheles 1,000 Genomes Project</p>
<p>For the <strong class="source-inline">standard.vcf.gz</strong> file, the median <strong class="source-inline">MQ0</strong> is <strong class="source-inline">0</strong> (it’s plotted at the very bottom and is almost unseen). This is good as it suggests that most sequences involved <a id="_idIndexMarker245"/>in the calling of variants map clearly to this <a id="_idIndexMarker246"/>area of the genome. For the <strong class="source-inline">centro.vcf.gz</strong> file, <strong class="source-inline">MQ0</strong> is of poor quality. Furthermore, there are areas where the genotype caller cannot find any variants at all (hence the incomplete chart).</p>
<ol>
<li value="5">Let’s compare heterozygosity with <strong class="bold">DP</strong>, the sample-level annotation. Here, we will plot the fraction of heterozygosity calls as a function of the <strong class="bold">sample read depth</strong> (<strong class="bold">DP</strong>) for every <a id="_idIndexMarker247"/>SNP. First, we will explain the result and then the code that generates it.</li>
</ol>
<p>The following screenshot shows the fraction of calls that are heterozygous at a certain depth:</p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<img alt="Figure 3.7 – The continuous line represents the fraction of heterozygosite calls computed at a certain depth; in orange is the centromeric area; in blue is the “standard” area; the dashed lines represent the number of sample calls per depth; both areas come from chromosome 3L for circa 100 Ugandan mosquitoes from the Anopheles 1,000 Genomes Project " height="726" src="image/B17942_03_007.jpg" width="1308"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – The continuous line represents the fraction of heterozygosite calls computed at a certain depth; in orange is the centromeric area; in blue is the “standard” area; the dashed lines represent the number of sample calls per depth; both areas come from chromosome 3L for circa 100 Ugandan mosquitoes from the Anopheles 1,000 Genomes Project</p>
<p>In the preceding <a id="_idIndexMarker248"/>screenshot, there are two considerations to take into account. At a very low depth, the fraction of heterozygote <a id="_idIndexMarker249"/>calls is biased—in this case, lower. This makes sense, as the number of reads per position does not allow you to make a correct estimate of the presence of both alleles in a sample. Therefore, you should not trust calls at very low depth.</p>
<p>As expected, the number of calls in the centromere is way lower than outside it. The distribution of SNPs outside the centromere follows a common pattern that you can expect in many datasets.</p>
<p>The <a id="_idIndexMarker250"/>code for <a id="_idIndexMarker251"/>this is presented here:</p>
<p class="source-code">def get_sample_relation(recs, f1, f2):</p>
<p class="source-code">    rel = defaultdict(int)</p>
<p class="source-code">    for rec in recs:</p>
<p class="source-code">        if not rec.is_snp:</p>
<p class="source-code">             continue</p>
<p class="source-code">        for pos in range(len(rec.genotypes)):</p>
<p class="source-code">            v1 = f1(rec, pos)</p>
<p class="source-code">            v2 = f2(rec, pos)</p>
<p class="source-code">            if v1 is None or v2 == np.iinfo(type(v2)).min:</p>
<p class="source-code">                continue  # We ignore Nones</p>
<p class="source-code">            rel[(v1, v2)] += 1</p>
<p class="source-code">            # careful with the size, floats: round?</p>
<p class="source-code">        #break</p>
<p class="source-code">    return rel get_sample_relation(recs, f1, f2):</p>
<p class="source-code">rels = {}</p>
<p class="source-code">for vcf_name in vcf_names:</p>
<p class="source-code">    recs = VCF(filename=vcf_name)</p>
<p class="source-code">    rels[vcf_name] = get_sample_relation(</p>
<p class="source-code">        recs,</p>
<p class="source-code">        lambda rec, pos: 1 if rec.genotypes[pos][0] != rec.genotypes[pos][1] else 0,</p>
<p class="source-code">        lambda rec, pos: rec.format('DP')[pos][0])</p>
<p>Start by looking for the <strong class="source-inline">for</strong> loop. Again, we use functional programming; the <strong class="source-inline">get_sample_relation</strong> function will traverse all SNP records and apply two functional parameters. The first parameter determines heterozygosity, whereas the second parameter acquires the sample <strong class="source-inline">DP</strong> (remember that there is also a variant of <strong class="source-inline">DP</strong>).</p>
<p>Now, since the code is as complex as it is, I opted for a naive data structure to be returned by <strong class="source-inline">get_sample_relation</strong>: a dictionary where the key is a pair of results (in this case, heterozygosity and <strong class="source-inline">DP</strong>) and the sum of SNPs that share both values. There are more elegant data structures with different trade-offs. For this, there are SciPy sparse matrices, pandas DataFrames, or you may want to consider PyTables. The fundamental point here is to have a framework that is general <a id="_idIndexMarker252"/>enough to compute relationships <a id="_idIndexMarker253"/>between a couple of sample annotations.</p>
<p>Also, be careful with the dimension space of several annotations. For example, if your annotation is of the float type, you might have to round it (if not, the size of your data structure might become too big).</p>
<ol>
<li value="6">Now, let’s take a look at the plotting code. Let’s perform this in two parts. Here is part one:<p class="source-code">def plot_hz_rel(dps, ax, ax2, name, rel):</p><p class="source-code">    frac_hz = []</p><p class="source-code">    cnt_dp = []</p><p class="source-code">    for dp in dps:</p><p class="source-code">        hz = 0.0</p><p class="source-code">        cnt = 0</p><p class="source-code">        for khz, kdp in rel.keys():</p><p class="source-code">            if kdp != dp:</p><p class="source-code">                continue</p><p class="source-code">            cnt += rel[(khz, dp)]</p><p class="source-code">            if khz == 1:</p><p class="source-code">                hz += rel[(khz, dp)]</p><p class="source-code">        frac_hz.append(hz / cnt)</p><p class="source-code">        cnt_dp.append(cnt)</p><p class="source-code">    ax.plot(dps, frac_hz, label=name)</p><p class="source-code">    ax2.plot(dps, cnt_dp, '--', label=name)</p></li>
</ol>
<p>This function will take a data structure, as generated by <strong class="source-inline">get_sample_relation</strong>, expecting that the first parameter of the key tuple is the <a id="_idIndexMarker254"/>heterozygosity state (<strong class="source-inline">0</strong>=homozygote, <strong class="source-inline">1</strong>=heterozygote) and the second parameter is <strong class="source-inline">DP</strong>. With this, it will generate <a id="_idIndexMarker255"/>two lines: one with the fraction of samples (which are heterozygotes at a certain depth) and the other with the SNP count.</p>
<ol>
<li value="7">Now, let’s call this function:<p class="source-code">fig, ax = plt.subplots(figsize=(16, 9))</p><p class="source-code">ax2 = ax.twinx()</p><p class="source-code">for name, rel in rels.items():</p><p class="source-code">    dps = list(set([x[1] for x in rel.keys()]))</p><p class="source-code">dps.sort()</p><p class="source-code">plot_hz_rel(dps, ax, ax2, name, rel)</p><p class="source-code">ax.set_xlim(0, 75)</p><p class="source-code">ax.set_ylim(0, 0.2)</p><p class="source-code">ax2.set_ylabel('Quantity of calls')</p><p class="source-code">ax.set_ylabel('Fraction of Heterozygote calls')</p><p class="source-code">ax.set_xlabel('Sample Read Depth (DP)')</p><p class="source-code">ax.legend()</p><p class="source-code">fig.suptitle('Number of calls per depth and fraction of calls which are Hz', fontsize='xx-large')</p></li>
</ol>
<p>Here, we will use two axes. On the left-hand side, we will have the fraction of heterozygous SNPs. On the right-hand side, we will have the number of SNPs. We then call <strong class="source-inline">plot_hz_rel</strong> for both data files. The rest is standard <strong class="source-inline">matplotlib</strong> code.</p>
<ol>
<li value="8">Finally, let’s <a id="_idIndexMarker256"/>compare the <strong class="source-inline">DP</strong> variant with a categorical <a id="_idIndexMarker257"/>variant-level annotation (<strong class="source-inline">EFF</strong>). <strong class="source-inline">EFF</strong> is provided by SnpEff and tells us (among many other things) the type of SNP (for example, intergenic, intronic, coding synonymous, and coding nonsynonymous). The Anopheles dataset provides this useful annotation. Let’s start by extracting variant-level annotations and the functional programming style:<p class="source-code">def get_variant_relation(recs, f1, f2):</p><p class="source-code">    rel = defaultdict(int)</p><p class="source-code">    for rec in recs:</p><p class="source-code">        if not rec.is_snp:</p><p class="source-code">            continue</p><p class="source-code">    try:</p><p class="source-code">        v1 = f1(rec)</p><p class="source-code">        v2 = f2(rec)</p><p class="source-code">        if v1 is None or v2 is None:</p><p class="source-code">            continue # We ignore Nones</p><p class="source-code">        rel[(v1, v2)] += 1</p><p class="source-code">    except:</p><p class="source-code">        pass</p><p class="source-code">    return rel</p></li>
</ol>
<p>The programming style here is similar to <strong class="source-inline">get_sample_relation</strong>, but we will not delve into any samples. Now, we define the type of effect that we’ll work with <a id="_idIndexMarker258"/>and convert its effect into an integer (as this will allow us to use it as an index—for example, matrices). Now, think about <a id="_idIndexMarker259"/>coding a categorical variable:</p>
<p class="source-code">accepted_eff = ['INTERGENIC', 'INTRON', 'NON_SYNONYMOUS_CODING', 'SYNONYMOUS_CODING']</p>
<p class="source-code">def eff_to_int(rec):</p>
<p class="source-code">    try:</p>
<p class="source-code">        annot = rec.INFO['EFF']</p>
<p class="source-code">        master_type = annot.split('(')[0]</p>
<p class="source-code">        return accepted_eff.index(master_type)</p>
<p class="source-code">    except ValueError:</p>
<p class="source-code">        return len(accepted_eff)</p>
<ol>
<li value="9">We will now traverse the file; the style should be clear to you now:<p class="source-code">eff_mq0s = {}</p><p class="source-code">for vcf_name in vcf_names:</p><p class="source-code">    recs = VCF(filename=vcf_name)</p><p class="source-code">    eff_mq0s[vcf_name] = get_variant_relation(recs, lambda r: eff_to_int(r), lambda r: int(r.INFO['DP']))</p></li>
<li>Finally, we plot the distribution of <strong class="source-inline">DP</strong> using the SNP effect:<p class="source-code">fig, ax = plt.subplots(figsize=(16,9))</p><p class="source-code">vcf_name = 'standard.vcf.gz'</p><p class="source-code">bp_vals = [[] for x in range(len(accepted_eff) + 1)]</p><p class="source-code">for k, cnt in eff_mq0s[vcf_name].items():</p><p class="source-code">    my_eff, mq0 = k</p><p class="source-code">    bp_vals[my_eff].extend([mq0] * cnt)</p><p class="source-code">sns.boxplot(data=bp_vals, sym='', ax=ax)</p><p class="source-code">ax.set_xticklabels(accepted_eff + ['OTHER'])</p><p class="source-code">ax.set_ylabel('DP (variant)')</p><p class="source-code">fig.suptitle('Distribution of variant DP per SNP type', fontsize='xx-large')</p></li>
</ol>
<p>Here, we just print a <strong class="source-inline">boxplot</strong> for the non-centromeric file, as shown in the following diagram. The <a id="_idIndexMarker260"/>results are as expected: SNPs in <a id="_idIndexMarker261"/>coding areas will probably have more depth because they are in more complex regions that are easier to call than intergenic SNPs:</p>
<div>
<div class="IMG---Figure" id="_idContainer024">
<img alt="Figure 3.8 – Boxplot for the distribution of variant read depth across different SNP effects " height="745" src="image/B17942_03_008.jpg" width="1322"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Boxplot for the distribution of variant read depth across different SNP effects</p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/>There’s more...</h2>
<p>The whole issue of filtering SNPs and other genome features will need a book of its own. This approach will depend on the type of sequencing data that you have, the number of samples, and potential extra information (for example, a pedigree among samples).</p>
<p>This recipe is <a id="_idIndexMarker262"/>very complex as it is, but parts of it are profoundly <a id="_idIndexMarker263"/>naive (there is a limit regarding the complexity that I can force on you in a simple recipe). For example, the window code does not support overlapping windows. Also, data structures are simplistic. However, I hope that they give you an idea of the general strategy to process genomic, high-throughput sequencing data. You can read more in <a href="B17942_04.xhtml#_idTextAnchor104"><em class="italic">Chapter 4</em></a>, <em class="italic">Advanced NGS Processing</em>.</p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/>See also</h2>
<p>More information can be found via these links:</p>
<ul>
<li>There are many filtering rules, but I would like to draw your attention to the need for reasonably good coverage (clearly above 10x). Refer to <em class="italic">Meynert et al.</em>, <em class="italic">Variant detection sensitivity and biases in whole genome and exome sequencing</em>, at <a href="http://www.biomedcentral.com/1471-2105/15/247/">http://www.biomedcentral.com/1471-2105/15/247/</a>.</li>
<li><strong class="source-inline">bcbio-nextgen</strong> is a Python-based pipeline for high-throughput sequencing analysis and is worth checking out (<a href="https://bcbio-nextgen.readthedocs.org">https://bcbio-nextgen.readthedocs.org</a>).</li>
</ul>
<h1 id="_idParaDest-101"><a id="_idTextAnchor100"/>Processing NGS data with HTSeq</h1>
<p>HTSeq (<a href="https://htseq.readthedocs.io">https://htseq.readthedocs.io</a>) is an <a id="_idIndexMarker264"/>alternative library that’s used for processing NGS data. Most <a id="_idIndexMarker265"/>of the functionality <a id="_idIndexMarker266"/>made available by HTSeq is actually available in other libraries covered in this book, but you should be aware of it as an alternative <a id="_idIndexMarker267"/>way of processing NGS data. HTSeq supports, among others, FASTA, FASTQ, SAM (via <strong class="source-inline">pysam</strong>), VCF, <strong class="bold">General Feature Format</strong> (<strong class="bold">GFF</strong>), and <strong class="bold">Browser Extensible Data</strong> (<strong class="bold">BED</strong>) file formats. It also includes a set of <a id="_idIndexMarker268"/>abstractions for processing (mapped) genomic data, encompassing concepts such as genomic positions and intervals or alignments. A complete examination of the features of this library is beyond our scope, so we will concentrate on a small subset of features. We will take this opportunity to also introduce the BED file format.</p>
<p>The BED format allows for the specification of features for annotations’ tracks. It has many uses, but it’s common to load BED files into genome browsers to visualize features. Each line includes information about at least the position (chromosome, start, and end) and also optional fields such as name or strand. Full details about the format can be found at <a href="https://genome.ucsc.edu/FAQ/FAQformat.xhtml#format1">https://genome.ucsc.edu/FAQ/FAQformat.xhtml#format1</a>.</p>
<h2 id="_idParaDest-102"><a id="_idTextAnchor101"/>Getting ready</h2>
<p>Our simple <a id="_idIndexMarker269"/>example will use data from the region <a id="_idIndexMarker270"/>where the LCT gene is located in the human genome. The LCT gene codifies lactase, an enzyme involved in the digestion of lactose.</p>
<p>We will take this information from Ensembl. Go to <a href="http://uswest.ensembl.org/Homo_sapiens/Gene/Summary?db=core;g=ENSG00000115850">http://uswest.ensembl.org/Homo_sapiens/Gene/Summary?db=core;g=ENSG00000115850</a> and choose <strong class="bold">Export data</strong>. The <strong class="bold">Output</strong> format should be <strong class="bold">BED Format</strong>. <strong class="bold">Gene information</strong> should be selected (you can choose more if you want). For convenience, a downloaded file called <strong class="source-inline">LCT.bed</strong> is available in the <strong class="source-inline">Chapter03</strong> directory.</p>
<p>The notebook for this code is called <strong class="source-inline">Chapter03/Processing_BED_with_HTSeq.py</strong>.</p>
<p>Take a look at the file before we start. An example of a few lines of this file is provided here:</p>
<pre class="source-code">
<strong class="bold">track name=gene description="Gene information"</strong>
<strong class="bold"> 2       135836529       135837180       ENSE00002202258 0       -</strong>
<strong class="bold"> 2       135833110       135833190       ENSE00001660765 0       -</strong>
<strong class="bold"> 2       135789570       135789798       NM_002299.2.16  0       -</strong>
<strong class="bold"> 2       135787844       135788544       NM_002299.2.17  0       -</strong>
<strong class="bold"> 2       135836529       135837169       CCDS2178.117    0       -</strong>
<strong class="bold"> 2       135833110       135833190       CCDS2178.116    0       -</strong></pre>
<p>The fourth <a id="_idIndexMarker271"/>column is the feature name. This will vary <a id="_idIndexMarker272"/>widely from file to file, and you will have to check it each and every time. However, in our case, it seems apparent that we have <a id="_idIndexMarker273"/>Ensembl exons (<strong class="source-inline">ENSE</strong>...), GenBank records (<strong class="source-inline">NM</strong>_...), and coding region information (<strong class="source-inline">CCDS</strong>) from the <strong class="bold">Consensus Coding Sequence</strong> (<strong class="bold">CCDS</strong>) database (<a href="https://www.ncbi.nlm.nih.gov/CCDS/CcdsBrowse.cgi">https://www.ncbi.nlm.nih.gov/CCDS/CcdsBrowse.cgi</a>).</p>
<p>You will need to install HTSeq:</p>
<p class="source-code">conda install –c bioconda htseq</p>
<p>Now, we can begin.</p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor102"/>How to do it...</h2>
<p>Take a look at the following steps:</p>
<ol>
<li value="1">We will start by setting up a reader for our file. Remember that this file has already been supplied to you, and should be in your current work directory:<p class="source-code">from collections import defaultdict</p><p class="source-code">import re</p><p class="source-code">import HTSeq</p><p class="source-code">lct_bed = HTSeq.BED_Reader('LCT.bed')</p></li>
<li>We are now going to extract all the types of features via their name:<p class="source-code">feature_types = defaultdict(int)</p><p class="source-code">for rec in lct_bed:</p><p class="source-code">    last_rec = rec</p><p class="source-code">    feature_types[re.search('([A-Z]+)', rec.name).group(0)] += 1</p><p class="source-code">print(feature_types)</p></li>
</ol>
<p>Remember that this code is specific to our example. You will have to adapt it to your case.</p>
<p class="callout-heading">Tip</p>
<p class="callout">You will find <a id="_idIndexMarker274"/>that the preceding code uses a <strong class="bold">regular expression</strong> (<strong class="bold">regex</strong>). Be careful with regexes, as they tend to generate read-only code that is difficult to maintain. You might have better alternatives. In any case, regexes exist, and you will find them from time to time.</p>
<p>The <a id="_idIndexMarker275"/>output for our case looks like this:</p>
<p class="source-code"><strong class="bold">defaultdict(&lt;class 'int'&gt;, {'ENSE': 27, 'NM': 17, 'CCDS': 17})</strong></p>
<ol>
<li value="3">We <a id="_idIndexMarker276"/>stored the last record so that we can inspect it:<p class="source-code">print(last_rec)</p><p class="source-code">print(last_rec.name)</p><p class="source-code">print(type(last_rec))</p><p class="source-code">interval = last_rec.iv</p><p class="source-code">print(interval)</p><p class="source-code">print(type(interval))</p></li>
</ol>
<p>There are many fields available, most notably <strong class="source-inline">name</strong> and <strong class="source-inline">interval</strong>. For the preceding code, the output looks like this:</p>
<p class="source-code"><strong class="bold">&lt;GenomicFeature: BED line 'CCDS2178.11' at 2: 135788543 -&gt; 135788322 (strand '-')&gt;</strong></p>
<p class="source-code"><strong class="bold"> CCDS2178.11</strong></p>
<p class="source-code"><strong class="bold"> &lt;class 'HTSeq.GenomicFeature'&gt;</strong></p>
<p class="source-code"><strong class="bold"> 2:[135788323,135788544)/-</strong></p>
<p class="source-code"><strong class="bold"> &lt;class 'HTSeq._HTSeq.GenomicInterval'&gt;</strong></p>
<ol>
<li value="4">Let’s <a id="_idIndexMarker277"/>dig deeper into the interval:<p class="source-code">print(interval.chrom, interval.start, interval.end)</p><p class="source-code">print(interval.strand)</p><p class="source-code">print(interval.length)</p><p class="source-code">print(interval.start_d)</p><p class="source-code">print(interval.start_as_pos)</p><p class="source-code">print(type(interval.start_as_pos))</p></li>
</ol>
<p>The <a id="_idIndexMarker278"/>output looks like this:</p>
<p class="source-code"><strong class="bold">2 135788323 135788544</strong></p>
<p class="source-code"><strong class="bold"> -</strong></p>
<p class="source-code"><strong class="bold"> 221</strong></p>
<p class="source-code"><strong class="bold"> 135788543</strong></p>
<p class="source-code"><strong class="bold"> 2:135788323/-</strong></p>
<p class="source-code"><strong class="bold"> &lt;class 'HTSeq._HTSeq.GenomicPosition'&gt;</strong></p>
<p>Note the genomic position (chromosome, start, and end). The most complex issue is how to deal with the strand. If the feature is coded in the negative strand, you have to be careful with processing. HTSeq offers the <strong class="source-inline">start_d</strong> and <strong class="source-inline">end_d</strong> fields to help you with this (that is, they will be reversed with regard to the start and end if the strand is negative).</p>
<p>Finally, let’s extract some statistics from our coding regions (CCDS records). We will use <a id="_idIndexMarker279"/>CCDS since it’s probably better <a id="_idIndexMarker280"/>than the curated database here:</p>
<p class="source-code">exon_start = None</p>
<p class="source-code">exon_end = None</p>
<p class="source-code">sizes = []</p>
<p class="source-code">for rec in lct_bed:</p>
<p class="source-code">    if not rec.name.startswith('CCDS'):</p>
<p class="source-code">        continue</p>
<p class="source-code">    interval = rec.iv</p>
<p class="source-code">    exon_start = min(interval.start, exon_start or interval.start)</p>
<p class="source-code">    exon_end = max(interval.length, exon_end or interval.end)</p>
<p class="source-code">    sizes.append(interval.length)</p>
<p class="source-code">sizes.sort()</p>
<p class="source-code">print("Num exons: %d / Begin: %d / End %d" % (len(sizes), exon_start, exon_end))</p>
<p class="source-code">print("Smaller exon: %d / Larger exon: %d / Mean size: %.1f" % (sizes[0], sizes[-1], sum(sizes)/len(sizes)))</p>
<p>The output should be self-explanatory:</p>
<p class="source-code"><strong class="bold">Num exons: 17 / Begin: 135788323 / End 135837169</strong></p>
<p class="source-code"><strong class="bold"> Smaller exon: 79 / Larger exon: 1551 / Mean size: 340.2</strong></p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor103"/>There’s more...</h2>
<p>The BED format can be a bit more complex than this. Furthermore, the preceding code is based on quite specific premises with regard to the contents of our file. However, this example should be enough to get you started. Even at its worst, the BED format is not very complicated.</p>
<p>HTSeq has <a id="_idIndexMarker281"/>much more functionality than this, but <a id="_idIndexMarker282"/>this recipe is mostly provided as a starting point for the whole package. HTSeq has functionality that can be used as an alternative to most of the recipes that we’ve covered thus far.</p>
</div>
</div></body></html>