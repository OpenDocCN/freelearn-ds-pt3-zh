<html><head></head><body>
<div epub:type="chapter" id="_idContainer076">
<h1 class="chapter-number" id="_idParaDest-103"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-104"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.2.1">Data Preparation</span></h1>
<p><span class="koboSpan" id="kobo.3.1">So far, we have covered the foundations of time series and Apache Spark and the full lifecycle of a time series analysis project. </span><span class="koboSpan" id="kobo.3.2">In this chapter, we delve into the critical steps of organizing, cleaning, and transforming time series data for effective analysis. </span><span class="koboSpan" id="kobo.3.3">It covers techniques for handling missing values, dealing with outliers, and structuring data to suit Spark’s distributed computing model. </span><span class="koboSpan" id="kobo.3.4">This information is invaluable as it equips you with the skills to ensure data quality and compatibility with Spark, laying a robust foundation for accurate and efficient time series analysis. </span><span class="koboSpan" id="kobo.3.5">Proper data preparation enhances the reliability of subsequent analytical processes, making this chapter an essential prerequisite to derive meaningful insights from time-dependent datasets </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">using Spark.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">We’re going to cover the following main topics in </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Data ingestion </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">and persistence</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Data quality checks </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">and cleaning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.11.1">Transformations</span></span></li>
</ul>
<h1 id="_idParaDest-105"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.12.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.13.1">Hands-on coding is predominant in this chapter, covering the common data preparation steps of a time series analysis project. </span><span class="koboSpan" id="kobo.13.2">The code for this chapter can be found in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.14.1">ch5</span></strong><span class="koboSpan" id="kobo.15.1"> folder of the book’s GitHub repository at </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">this URL:</span></span></p>
<p><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch5"><span class="No-Break"><span class="koboSpan" id="kobo.17.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch5</span></span></a></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.18.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.19.1">The code will be used with the Databricks Community Edition, as per the approach explained in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.20.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.21.1"> and </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">this chapter.</span></span></p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.23.1">Data ingestion and persistence</span></h1>
<p><span class="koboSpan" id="kobo.24.1">In this</span><a id="_idIndexMarker464"/><span class="koboSpan" id="kobo.25.1"> first section, we will cover the methods of getting time series data from sources and persisting the dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">to storage.</span></span></p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.27.1">Ingestion</span></h2>
<p><span class="koboSpan" id="kobo.28.1">Ingestion is the </span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.29.1">process by which data is retrieved from a source system for further processing and analysis. </span><span class="koboSpan" id="kobo.29.2">This process can be executed in batches to ingest a large amount of data as a one-off on demand or scheduled to run automatically at regular intervals, such as every night. </span><span class="koboSpan" id="kobo.29.3">Alternatively, if the data is available from the source system on a continual basis and is required as such, the other ingestion method is </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">structured streaming.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.31.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.32.1">We can technically code the ingestion process as structured streaming and configure it to run at triggered intervals. </span><span class="koboSpan" id="kobo.32.2">This gives the flexibility to adjust to changing business requirements on data freshness without having to redevelop the </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">ingestion process.</span></span></p>
<p><span class="koboSpan" id="kobo.34.1">In this chapter, we will focus on batch ingestion, the most common method today. </span><span class="koboSpan" id="kobo.34.2">We will also briefly discuss structured streaming, which is quickly gaining adoption and has even overtaken batch ingestion in </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">some organizations.</span></span></p>
<h3><span class="koboSpan" id="kobo.36.1">Batch ingestion</span></h3>
<p><span class="koboSpan" id="kobo.37.1">Batch ingestion </span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.38.1">is usually done from file storage or from </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">a database.</span></span></p>
<h4><span class="koboSpan" id="kobo.40.1">From file storage</span></h4>
<p><span class="koboSpan" id="kobo.41.1">As we </span><a id="_idIndexMarker467"/><span class="koboSpan" id="kobo.42.1">saw in the hands-on sections of the previous chapters, reading from a file is a frequently used batch ingestion method. </span><span class="koboSpan" id="kobo.42.2">This is done as follows with Apache Spark, </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.44.1">spark.read()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.46.1">
df = spark.read.csv("file_path", header=True, sep=";", inferSchema=True)</span></pre> <p><span class="koboSpan" id="kobo.47.1">With this</span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.48.1"> example, we are reading a CSV-formatted file from a </span><strong class="source-inline"><span class="koboSpan" id="kobo.49.1">file_path</span></strong><span class="koboSpan" id="kobo.50.1"> storage location. </span><span class="koboSpan" id="kobo.50.2">The header is present in this file as the first line. </span><span class="koboSpan" id="kobo.50.3">The different columns are separated with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.51.1">;</span></strong><span class="koboSpan" id="kobo.52.1"> character. </span><span class="koboSpan" id="kobo.52.2">We want Spark to find out the data columns and types present in the file, as specified </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.54.1">inferSchema</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.56.1">This example is based on the code in </span><strong class="source-inline"><span class="koboSpan" id="kobo.57.1">ts-spark_ch5_1.dbc</span></strong><span class="koboSpan" id="kobo.58.1">, which we can import from the GitHub location for </span><a href="B18568_05.xhtml#_idTextAnchor103"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.59.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.60.1">, mentioned in the </span><em class="italic"><span class="koboSpan" id="kobo.61.1">Technical requirements</span></em><span class="koboSpan" id="kobo.62.1"> section, into Databricks Community Edition, as per the approach explained in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.63.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.64.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">The code URL </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">is </span></span><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch5/ts-spark_ch5_1.dbc"><span class="No-Break"><span class="koboSpan" id="kobo.67.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch5/ts-spark_ch5_1.dbc</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.68.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.69.1">The ingested data can then be further processed and analyzed, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.70.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.71.1">.1</span></em><span class="koboSpan" id="kobo.72.1">, based on the code example provided for </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">this chapter.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<span class="koboSpan" id="kobo.74.1"><img alt="" src="image/B18568_05_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.75.1">Figure 5.1: Viewing the ingested data</span></p>
<p><span class="koboSpan" id="kobo.76.1">When reading files, it is also possible to read multiple files from a storage folder by proving the folder location instead of a specific file location. </span><span class="koboSpan" id="kobo.76.2">This is a common ingestion pattern for files. </span><span class="koboSpan" id="kobo.76.3">Another frequently used feature is to provide a filter (</span><strong class="source-inline"><span class="koboSpan" id="kobo.77.1">pathGlobFilter</span></strong><span class="koboSpan" id="kobo.78.1">) to only include filenames matching </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">a pattern.</span></span></p>
<p><span class="koboSpan" id="kobo.80.1">There are </span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.81.1">many other options for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.82.1">spark.read</span></strong><span class="koboSpan" id="kobo.83.1"> command, depending on the source being read. </span><span class="koboSpan" id="kobo.83.2">The following Apache Spark documentation on data sources details </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">these options:</span></span></p>
<p><a href="https://spark.apache.org/docs/latest/sql-data-sources.html"><span class="No-Break"><span class="koboSpan" id="kobo.85.1">https://spark.apache.org/docs/latest/sql-data-sources.html</span></span></a></p>
<h4><span class="koboSpan" id="kobo.86.1">From a database</span></h4>
<p><span class="koboSpan" id="kobo.87.1">Another</span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.88.1"> frequently used type of source is a relational database. </span><span class="koboSpan" id="kobo.88.2">An example for reading from </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">PostgreSQL follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.90.1">
df = </span><strong class="bold"><span class="koboSpan" id="kobo.91.1">spark.read</span></strong><span class="koboSpan" id="kobo.92.1"> \
    .format("</span><strong class="bold"><span class="koboSpan" id="kobo.93.1">jdbc</span></strong><span class="koboSpan" id="kobo.94.1">") \
    .option("url", "jdbc:postgresql:dbserver") \
    .option("dbtable", "schema.tablename") \
    .option("user", "username") \
    .option("password", "password") \
    .load()</span></pre> <p><span class="koboSpan" id="kobo.95.1">This is further detailed in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">documentation: </span></span><a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html"><span class="No-Break"><span class="koboSpan" id="kobo.97.1">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</span></span></a></p>
<p><span class="koboSpan" id="kobo.98.1">Data from specialized time series databases, such as QuestDB, can be ingested in a similar way, as </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">shown here:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.100.1">
df = spark.read.format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:8812/questdb") \
    .option("driver", "org.postgresql.Driver") \
    .option("user", "admin") \
    .option("password", "quest") \
    .option("dbtable", "timeseries_table") \
    .load()</span></pre> <p><span class="koboSpan" id="kobo.101.1">This is further </span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.102.1">detailed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">following documentation:</span></span></p>
<p><a href="https://questdb.io/blog/integrate-apache-spark-questdb-time-series-analytics/"><span class="No-Break"><span class="koboSpan" id="kobo.104.1">https://questdb.io/blog/integrate-apache-spark-questdb-time-series-analytics/</span></span></a></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.105.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.106.1">You will need to include the JDBC driver for the particular database on the Spark classpath. </span><span class="koboSpan" id="kobo.106.2">The previously referenced documentation </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">explains this.</span></span></p>
<h3><span class="koboSpan" id="kobo.108.1">Structured Streaming</span></h3>
<p><span class="koboSpan" id="kobo.109.1">In the</span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.110.1"> case of event-driven or near-real-time processing with</span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.111.1"> Apache Spark, time</span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.112.1"> series data can be ingested from streaming sources such as Apache Kafka, Amazon Kinesis, Google Cloud Pub/Sub, and Azure Event Hubs. </span><span class="koboSpan" id="kobo.112.2">This typically involves setting up Spark Structured Streaming with the corresponding connectors for </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">the source.</span></span></p>
<p><span class="koboSpan" id="kobo.114.1">The following example shows how to ingest data from Apache Kafka </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">using Spark:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.116.1">
df = spark \
    .</span><strong class="bold"><span class="koboSpan" id="kobo.117.1">readStream</span></strong><span class="koboSpan" id="kobo.118.1"> \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2") \
    .option("subscribe", "topic1") \
    .load()</span></pre> <p><span class="koboSpan" id="kobo.119.1">The Apache Spark documentation provides further details on reading from </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">streaming sources:</span></span></p>
<p><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources"><span class="No-Break"><span class="koboSpan" id="kobo.121.1">https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources</span></span></a></p>
<p><span class="koboSpan" id="kobo.122.1">Once the data has been ingested, the next step is to persist it to storage for further processing, as we will </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">see next.</span></span></p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.124.1">Persistence</span></h2>
<p><span class="koboSpan" id="kobo.125.1">Data is </span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.126.1">typically persisted to files on disk or to databases. </span><span class="koboSpan" id="kobo.126.2">With Apache Spark, a proven solution for files is </span><a id="_idIndexMarker476"/><span class="koboSpan" id="kobo.127.1">Delta Lake, an open source </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">storage protocol.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.129.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.130.1">Apache Iceberg is </span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.131.1">another common open source </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">storage protocol.</span></span></p>
<p><span class="koboSpan" id="kobo.133.1">Delta provides ACID transactions to Apache Spark and big data workloads, effectively combining the best of file and database storage, in what </span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.134.1">is called a </span><strong class="bold"><span class="koboSpan" id="kobo.135.1">lakehouse</span></strong><span class="koboSpan" id="kobo.136.1"> (merge of the terms </span><em class="italic"><span class="koboSpan" id="kobo.137.1">data lake</span></em><span class="koboSpan" id="kobo.138.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.139.1">data warehouse</span></em><span class="koboSpan" id="kobo.140.1">). </span><span class="koboSpan" id="kobo.140.2">Built on top of the Parquet file format, Delta provides capabilities such as schema enforcement, data versioning, and </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">time travel.</span></span></p>
<p><span class="koboSpan" id="kobo.142.1">Here’s an example of how you can persist time series data in Delta storage format using Apache Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">in Python:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.144.1">
df.</span><strong class="bold"><span class="koboSpan" id="kobo.145.1">write</span></strong><span class="koboSpan" id="kobo.146.1">.</span><strong class="bold"><span class="koboSpan" id="kobo.147.1">format</span></strong><span class="koboSpan" id="kobo.148.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.149.1">"delta"</span></strong><span class="koboSpan" id="kobo.150.1">).mode(</span><strong class="bold"><span class="koboSpan" id="kobo.151.1">"overwrite"</span></strong><span class="koboSpan" id="kobo.152.1">).option(</span><strong class="bold"><span class="koboSpan" id="kobo.153.1">"path"</span></strong><span class="koboSpan" id="kobo.154.1">, delta_table_path).saveAsTable(table_name)</span></pre> <p><span class="koboSpan" id="kobo.155.1">With this example, we are writing in Delta format to a </span><strong class="source-inline"><span class="koboSpan" id="kobo.156.1">delta_table_path</span></strong><span class="koboSpan" id="kobo.157.1"> storage location. </span><span class="koboSpan" id="kobo.157.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.158.1">overwrite</span></strong><span class="koboSpan" id="kobo.159.1"> mode means that existing data at this location will be overwritten. </span><span class="koboSpan" id="kobo.159.2">With Delta format, the data is written as a table that is given the name specified </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.161.1">table_name</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.163.1">This example is based on the code in </span><strong class="source-inline"><span class="koboSpan" id="kobo.164.1">ts-spark_ch5_1.dbc</span></strong><span class="koboSpan" id="kobo.165.1">, which we imported in the earlier section on </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">batch ingestion.</span></span></p>
<p><span class="koboSpan" id="kobo.167.1">There are many other options for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.168.1">spark.write</span></strong><span class="koboSpan" id="kobo.169.1"> command, depending on the destination being written to. </span><span class="koboSpan" id="kobo.169.2">The following Apache Spark documentation on saving details </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">these options:</span></span></p>
<p><a href="https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#saving-to-persistent-tables"><span class="No-Break"><span class="koboSpan" id="kobo.171.1">https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#saving-to-persistent-tables</span></span></a></p>
<p><span class="koboSpan" id="kobo.172.1">When the data is persisted in Delta format, in addition to the data, metadata is also stored together to disk. </span><span class="koboSpan" id="kobo.172.2">This can be retrieved with the </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.174.1">
# Load the Delta table as a DeltaTable object
delta_table = DeltaTable.forPath(spark, delta_table_path)
# Details on the Delta table
print("Delta table details:")
delta_table.</span><strong class="bold"><span class="koboSpan" id="kobo.175.1">detail</span></strong><span class="koboSpan" id="kobo.176.1">().display()</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.177.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.178.1">In the code example, we did not have to install Delta as it is already installed when using the Databricks Community Edition. </span><span class="koboSpan" id="kobo.178.2">You will need to install the Delta packages if you are using another Apache Spark environment where Delta is not pre-installed. </span><span class="koboSpan" id="kobo.178.3">You can find the instructions </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">here: </span></span><a href="https://docs.delta.io/latest/quick-start.html"><span class="No-Break"><span class="koboSpan" id="kobo.180.1">https://docs.delta.io/latest/quick-start.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.181.1">.</span></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.182.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.183.1">.2</span></em><span class="koboSpan" id="kobo.184.1"> shows some of the metadata such as location and </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">creation date.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<span class="koboSpan" id="kobo.186.1"><img alt="" src="image/B18568_05_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.187.1">Figure 5.2: Metadata for the Delta table</span></p>
<p><span class="koboSpan" id="kobo.188.1">Once data</span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.189.1"> has been persisted, it can be read from storage as needed at a later stage for querying and analysis. </span><span class="koboSpan" id="kobo.189.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.190.1">spark.read</span></strong><span class="koboSpan" id="kobo.191.1"> command can be used here as well, as per the </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">following example:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.193.1">
spark.read.load(delta_table_path).display()</span></pre> <p><span class="koboSpan" id="kobo.194.1">The Delta table storage location, </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">delta_table_path</span></strong><span class="koboSpan" id="kobo.196.1">, is passed to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">load</span></strong><span class="koboSpan" id="kobo.198.1"> command, which retrieves the stored table from the </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">disk storage.</span></span></p>
<p><span class="koboSpan" id="kobo.200.1">As mentioned earlier, Spark can also write to a database, among other destinations. </span><span class="koboSpan" id="kobo.200.2">The following example shows how to write to a </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">PostgreSQL database.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.202.1">
jdbcDF.</span><strong class="bold"><span class="koboSpan" id="kobo.203.1">write</span></strong><span class="koboSpan" id="kobo.204.1"> \
    .format("</span><strong class="bold"><span class="koboSpan" id="kobo.205.1">jdbc</span></strong><span class="koboSpan" id="kobo.206.1">") \
    .option("url", "jdbc:postgresql:dbserver") \
    .option("dbtable", "schema.tablename") \
    .option("user", "username") \
    .option("password", "password") \
    .save()</span></pre> <p><span class="koboSpan" id="kobo.207.1">This is further detailed in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">documentation: </span></span><a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html"><span class="No-Break"><span class="koboSpan" id="kobo.209.1">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</span></span></a></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.210.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.211.1">You will need to include the JDBC driver for the particular database on the Spark classpath. </span><span class="koboSpan" id="kobo.211.2">The previously referenced documentation </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">explains this.</span></span></p>
<p><span class="koboSpan" id="kobo.213.1">As seen in this section, persistence</span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.214.1"> allows longer-term storage and retrieval. </span><span class="koboSpan" id="kobo.214.2">Delta also stores different versions of the data whenever it changes, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">investigate next.</span></span></p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.216.1">Versioning</span></h2>
<p><span class="koboSpan" id="kobo.217.1">Data versioning is</span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.218.1"> one of the key features provided by Delta Lake, allowing you to keep track of changes made to your data over time. </span><span class="koboSpan" id="kobo.218.2">This storage of different versions is done in an optimal way to minimize </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">storage footprint.</span></span></p>
<p><span class="koboSpan" id="kobo.220.1">With a record of different versions, Delta enables a</span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.221.1"> functionality called </span><strong class="bold"><span class="koboSpan" id="kobo.222.1">time travel</span></strong><span class="koboSpan" id="kobo.223.1">. </span><span class="koboSpan" id="kobo.223.2">With this, you can query data at specific versions or timestamps, revert to previous versions, and perform time travel queries. </span><span class="koboSpan" id="kobo.223.3">This is also useful from a reproducibility point of view, whereby we can go back to the specific version of data used previously, even if it has since changed, to audit, review, and redo </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">an analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.225.1">The code provided in this chapter has an example of using versioning and time travel. </span><span class="koboSpan" id="kobo.225.2">The following extract shows how to read a specific version of the Delta table. </span><strong class="source-inline"><span class="koboSpan" id="kobo.226.1">version_as_of</span></strong><span class="koboSpan" id="kobo.227.1"> is an integer representing the </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">version number:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.229.1">
df_ = spark.read.</span><strong class="bold"><span class="koboSpan" id="kobo.230.1">format</span></strong><span class="koboSpan" id="kobo.231.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.232.1">"delta"</span></strong><span class="koboSpan" id="kobo.233.1">).option(</span><strong class="bold"><span class="koboSpan" id="kobo.234.1">"versionAsOf"</span></strong><span class="koboSpan" id="kobo.235.1">, version_as_of).load(delta_table_path)</span></pre> <p><span class="koboSpan" id="kobo.236.1">It is also possible to read a specific version based on the timestamp, as per the following code extract, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">timestamp_as_of</span></strong><span class="koboSpan" id="kobo.238.1"> represents the timestamp of the version </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">of interest:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.240.1">
df_ = spark.read.</span><strong class="bold"><span class="koboSpan" id="kobo.241.1">format</span></strong><span class="koboSpan" id="kobo.242.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.243.1">"delta"</span></strong><span class="koboSpan" id="kobo.244.1">).option(</span><strong class="bold"><span class="koboSpan" id="kobo.245.1">"timestampAsOf"</span></strong><span class="koboSpan" id="kobo.246.1">, timestamp_as_of).load(delta_table_path)</span></pre> <p><span class="koboSpan" id="kobo.247.1">As changes are made to the Delta table, the metadata about the different versions is stored in a Delta table history log. </span><span class="koboSpan" id="kobo.247.2">The history can be read with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">history</span></strong><span class="koboSpan" id="kobo.249.1"> command, </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">as follows:</span></span></p>
<pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.251.1">print</span></strong><span class="koboSpan" id="kobo.252.1">(f"Delta table history - after modification:")
delta_table.</span><strong class="bold"><span class="koboSpan" id="kobo.253.1">history</span></strong><span class="koboSpan" id="kobo.254.1">().display()</span></pre> <p><span class="koboSpan" id="kobo.255.1">An example of output from the history is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.256.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.257.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<span class="koboSpan" id="kobo.259.1"><img alt="" src="image/B18568_05_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.260.1">Figure 5.3: Delta table versions</span></p>
<p><span class="koboSpan" id="kobo.261.1">Finally, it is </span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.262.1">possible to restore the Delta table back to a previous version with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.263.1">restoreToVersion</span></strong><span class="koboSpan" id="kobo.264.1"> command, overwriting the latest version, as per </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.266.1">
delta_table.</span><strong class="bold"><span class="koboSpan" id="kobo.267.1">restoreToVersion</span></strong><span class="koboSpan" id="kobo.268.1">(latest_version)</span></pre> <p><span class="koboSpan" id="kobo.269.1">You can also find more information on time </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">travel here:</span></span></p>
<p><a href="https://delta.io/blog/2023-02-01-delta-lake-time-travel/"><span class="No-Break"><span class="koboSpan" id="kobo.271.1">https://delta.io/blog/2023-02-01-delta-lake-time-travel/</span></span></a></p>
<p><span class="koboSpan" id="kobo.272.1">This concludes the section on ingestion and persistence. </span><span class="koboSpan" id="kobo.272.2">We will now move on to verify and clean </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">the data.</span></span></p>
<h1 id="_idParaDest-110"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.274.1">Data quality checks, cleaning, and transformation</span></h1>
<p><span class="koboSpan" id="kobo.275.1">Once the data has been ingested from source systems to a storage location from which we can access it, we will need to ensure that it is of usable quality and, if not, do the necessary cleaning </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">and transformation.</span></span></p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.277.1">Data quality checks</span></h2>
<p><span class="koboSpan" id="kobo.278.1">The outcome of </span><a id="_idIndexMarker484"/><span class="koboSpan" id="kobo.279.1">any analysis done with the data can be only as good as the data, making data quality checks an important </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">next step.</span></span></p>
<h3><span class="koboSpan" id="kobo.281.1">Consistency, accuracy, and completeness</span></h3>
<p><span class="koboSpan" id="kobo.282.1">Data quality checks for consistency, accuracy, and completeness are essential to ensure the reliability of your data. </span><span class="koboSpan" id="kobo.282.2">With its powerful tools for data processing and analysis, Apache Spark is suitable for implementing these checks. </span><span class="koboSpan" id="kobo.282.3">The following are examples of how you can perform data quality checks for consistency, accuracy, and completeness using Apache Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">in Python.</span></span></p>
<h4><span class="koboSpan" id="kobo.284.1">Consistency check</span></h4>
<p><span class="koboSpan" id="kobo.285.1">In the </span><a id="_idIndexMarker485"/><span class="koboSpan" id="kobo.286.1">following consistency test example, we are counting the number of records for </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">each date:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.288.1">
# Example consistency check: Check if a column has consistent values
consistency_check_result = df.groupBy("Date").count().orderBy("count")
print(f"Data consistency result:")
consistency_check_result.display()</span></pre> <p><span class="koboSpan" id="kobo.289.1">As per </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.290.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.291.1">.4</span></em><span class="koboSpan" id="kobo.292.1">, this simple check shows that some dates do not consistently have the same number of records, which can indicate missing values for </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">some dates.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<span class="koboSpan" id="kobo.294.1"><img alt="" src="image/B18568_05_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.295.1">Figure 5.4: Consistency check results</span></p>
<h4><span class="koboSpan" id="kobo.296.1">Accuracy check</span></h4>
<p><span class="koboSpan" id="kobo.297.1">In the</span><a id="_idIndexMarker486"/><span class="koboSpan" id="kobo.298.1"> accuracy test example, we want to verify the accuracy of </span><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.300.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.302.1">
# Example accuracy check:
# Check if values in a column meet certain criteria
accuracy_check_expression = "Global_active_power &lt; 0 OR Global_active_power &gt; 10"
# Check
accuracy_check_result = df.filter(accuracy_check_expression)
accuracy_check_result_count = accuracy_check_result.count()
if accuracy_check_result_count == 0:
    print(f"Data meets accuracy check - !({accuracy_check_expression}).")
else:
    print(f"Data fails accuracy check - {accuracy_check_expression} - count {accuracy_check_result_count}:")
    accuracy_check_result.display()</span></pre> <p><span class="koboSpan" id="kobo.303.1">As per </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.304.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.305.1">.5</span></em><span class="koboSpan" id="kobo.306.1">, this </span><a id="_idIndexMarker487"/><span class="koboSpan" id="kobo.307.1">check shows that in two cases, </span><strong class="source-inline"><span class="koboSpan" id="kobo.308.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.309.1"> is outside of the accuracy criteria that we have defined for this check. </span><span class="koboSpan" id="kobo.309.2">This indicates that either these values are wrong or that they are correct but are now going beyond the previously known ranges that we have used to define the criteria. </span><span class="koboSpan" id="kobo.309.3">We must update the criteria in this </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">latter case.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<span class="koboSpan" id="kobo.311.1"><img alt="" src="image/B18568_05_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.312.1">Figure 5.5: Accuracy check results</span></p>
<h4><span class="koboSpan" id="kobo.313.1">Completeness check</span></h4>
<p><span class="koboSpan" id="kobo.314.1">In the </span><a id="_idIndexMarker488"/><span class="koboSpan" id="kobo.315.1">completeness test example, we want to verify whether </span><strong class="source-inline"><span class="koboSpan" id="kobo.316.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.317.1"> has </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">null values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.319.1">
# Example completeness check: Check for null values in a column
completeness_check_expression = "Global_active_power is NULL"
# Check
completeness_check_result = df.filter(
    completeness_check_expression)
completeness_check_result_count = completeness_check_result.count()
if completeness_check_result_count == 0:
    print(f"Data meets completeness check - !({completeness_check_expression})")
else:
    print(f"Data fails completeness check - {completeness_check_expression} - count {completeness_check_result_count}:")
    completeness_check_result.display()</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.320.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.321.1">The consistency check example presented earlier can also be used </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">for completeness.</span></span></p>
<p><span class="koboSpan" id="kobo.323.1">These </span><a id="_idIndexMarker489"/><span class="koboSpan" id="kobo.324.1">examples show basic data quality checks for consistency, accuracy, and completeness using Apache Spark. </span><span class="koboSpan" id="kobo.324.2">These checks can be extended and integrated into your data pipelines for more comprehensive data </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">quality assurance.</span></span></p>
<h3><span class="koboSpan" id="kobo.326.1">Data quality framework</span></h3>
<p><span class="koboSpan" id="kobo.327.1">To</span><a id="_idIndexMarker490"/><span class="koboSpan" id="kobo.328.1"> better manage the suite of tests, it is recommended that a framework such as </span><em class="italic"><span class="koboSpan" id="kobo.329.1">Great Expectations</span></em><span class="koboSpan" id="kobo.330.1"> be used for data quality checks. </span><span class="koboSpan" id="kobo.330.2">You can find more information </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">here: </span></span><a href="https://github.com/great-expectations/great_expectations"><span class="No-Break"><span class="koboSpan" id="kobo.332.1">https://github.com/great-expectations/great_expectations</span></span></a></p>
<p><span class="koboSpan" id="kobo.333.1">We will cover another framework approach with the integration of data quality in the Delta Live Tables pipeline, and monitoring and alerting in </span><a href="B18568_10.xhtml#_idTextAnchor190"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.334.1">Chapter 10</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.335.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.336.1">Once the data quality has been tested, the next step is to clean </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">the data.</span></span></p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor112"/><span class="koboSpan" id="kobo.338.1">Data cleaning</span></h2>
<p><span class="koboSpan" id="kobo.339.1">The </span><a id="_idIndexMarker491"/><span class="koboSpan" id="kobo.340.1">previous step of data quality checks indicates the issues with the data that need to be corrected, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">now address.</span></span></p>
<h3><span class="koboSpan" id="kobo.342.1">Missing values</span></h3>
<p><span class="koboSpan" id="kobo.343.1">Apache Spark</span><a id="_idIndexMarker492"/><span class="koboSpan" id="kobo.344.1"> offers various methods to handle missing values in time series data. </span><span class="koboSpan" id="kobo.344.2">The following examples show how you can clean time series data for missing values using Apache Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">in Python.</span></span></p>
<h4><span class="koboSpan" id="kobo.346.1">Forward filling</span></h4>
<p><span class="koboSpan" id="kobo.347.1">The </span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.348.1">forward filling method to handle missing values replaces the missing values with the previous known value, with the values sorted in chronological order based on their timestamp. </span><span class="koboSpan" id="kobo.348.2">In the following code example, missing values for </span><strong class="source-inline"><span class="koboSpan" id="kobo.349.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.350.1"> are replaced in this way. </span><span class="koboSpan" id="kobo.350.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.351.1">Window.rowsBetween</span></strong><span class="koboSpan" id="kobo.352.1"> function in the following case goes from the first record to the current one. </span><span class="koboSpan" id="kobo.352.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">last</span></strong><span class="koboSpan" id="kobo.354.1"> function then finds the last non-null value within that window. </span><span class="koboSpan" id="kobo.354.2">As the window slides over all the records, all the missing values are replaced with the last </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">known value:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.356.1">
from pyspark.sql import functions as F
from pyspark.sql import Window
# Example: Handling missing values by forward filling
# "timestamp" column is ordered chronologically
df = spark.sql(
    f"select timestamp, </span><strong class="bold"><span class="koboSpan" id="kobo.357.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.358.1"> from {table_name} order by timestamp"
)
window = </span><strong class="bold"><span class="koboSpan" id="kobo.359.1">Window.rowsBetween</span></strong><span class="koboSpan" id="kobo.360.1">(float('-inf'),0)
filled_df = df.withColumn(
    "filled_Global_active_power",
    F.</span><strong class="bold"><span class="koboSpan" id="kobo.361.1">last</span></strong><span class="koboSpan" id="kobo.362.1">(df['Global_active_power'],
           ignorenulls=True).</span><strong class="bold"><span class="koboSpan" id="kobo.363.1">over</span></strong><span class="koboSpan" id="kobo.364.1">(window))
# Display updated values
filled_df.filter(
    "timestamp BETWEEN '2008-11-10 17:58:00' AND'2008-11-10 18:17:00'"
).display()</span></pre> <p><span class="koboSpan" id="kobo.365.1">The </span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.366.1">result of forward filling can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.367.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.368.1">.6</span></em><span class="koboSpan" id="kobo.369.1">, where the filled values are shown in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">filled_Global_active_power</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.371.1"> column.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<span class="koboSpan" id="kobo.372.1"><img alt="" src="image/B18568_05_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.373.1">Figure 5.6: Forward filling</span></p>
<p><span class="koboSpan" id="kobo.374.1">Forward filling works well when the last known value is a good indication of the next value, such as for a slow-changing value. </span><span class="koboSpan" id="kobo.374.2">It is not a good method when the value can change suddenly or when there </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">is seasonality.</span></span></p>
<h4><span class="koboSpan" id="kobo.376.1">Backward filling</span></h4>
<p><span class="koboSpan" id="kobo.377.1">The</span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.378.1"> backward filling method to handle missing values replaces the missing values with the next known value, with the values sorted in chronological order based on their timestamp. </span><span class="koboSpan" id="kobo.378.2">In the following code example, missing values for </span><strong class="source-inline"><span class="koboSpan" id="kobo.379.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.380.1"> are replaced in this way. </span><span class="koboSpan" id="kobo.380.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">Window.rowsBetween</span></strong><span class="koboSpan" id="kobo.382.1"> function in the following case goes from the current one to the last record. </span><span class="koboSpan" id="kobo.382.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.383.1">first</span></strong><span class="koboSpan" id="kobo.384.1"> function then finds the next non-null value within that window. </span><span class="koboSpan" id="kobo.384.2">As the window slides over all the records, all the missing values are replaced with the next </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">known value:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.386.1">
from pyspark.sql import functions as F
from pyspark.sql import Window
# Example: Handling missing values by backward filling
# "timestamp" column is ordered chronologically
df = spark.sql(
    f"select timestamp, </span><strong class="bold"><span class="koboSpan" id="kobo.387.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.388.1"> from {table_name} order by timestamp"
)
window = </span><strong class="bold"><span class="koboSpan" id="kobo.389.1">Window.rowsBetween</span></strong><span class="koboSpan" id="kobo.390.1">(0,float('inf'))
filled_df = df.withColumn(
    "filled_Global_active_power",
    F.first(df['Global_active_power'],
            ignorenulls=True).</span><strong class="bold"><span class="koboSpan" id="kobo.391.1">over</span></strong><span class="koboSpan" id="kobo.392.1">(window))
# Display updated values
filled_df.filter(
    "timestamp BETWEEN '2008-11-10 17:58:00' AND'2008-11-10 18:17:00'"
).display()</span></pre> <p><span class="koboSpan" id="kobo.393.1">The result</span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.394.1"> of backward filling can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.395.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.396.1">.7</span></em><span class="koboSpan" id="kobo.397.1">, where the filled values are shown in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.398.1">filled_Global_active_power</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.399.1"> column.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<span class="koboSpan" id="kobo.400.1"><img alt="" src="image/B18568_05_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.401.1">Figure 5.7: Backward filling</span></p>
<p><span class="koboSpan" id="kobo.402.1">Backward filling works well when the next known value can reasonably indicate the previous value, such as with slow-changing data or when collecting data retrospectively with gaps in the past. </span><span class="koboSpan" id="kobo.402.2">However, it is not suitable for analyzing causality or </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">leading indicators.</span></span></p>
<h4><span class="koboSpan" id="kobo.404.1">Interpolation</span></h4>
<p><span class="koboSpan" id="kobo.405.1">The</span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.406.1"> interpolation method to handle missing values replaces the missing values with a combination, such as the average, of the previous and next non-missing values, with the values sorted in chronological order based on </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">their timestamp.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.408.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.409.1">There are several different interpolation calculation methods, including linear, polynomial, and spline interpolation. </span><span class="koboSpan" id="kobo.409.2">The average method used here is a simple form of </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">linear interpolation.</span></span></p>
<p><span class="koboSpan" id="kobo.411.1">In the following code example, missing values for </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.413.1"> are replaced in this way. </span><span class="koboSpan" id="kobo.413.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">Window.rowsBetween</span></strong><span class="koboSpan" id="kobo.415.1"> function, used twice, in the following case, goes from the first record to the current one for </span><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">windowF</span></strong><span class="koboSpan" id="kobo.417.1">, and from the current one to the last record for </span><strong class="source-inline"><span class="koboSpan" id="kobo.418.1">windowB</span></strong><span class="koboSpan" id="kobo.419.1">. </span><span class="koboSpan" id="kobo.419.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.420.1">last</span></strong><span class="koboSpan" id="kobo.421.1"> function then finds the previous non-null value within </span><strong class="source-inline"><span class="koboSpan" id="kobo.422.1">windowF</span></strong><span class="koboSpan" id="kobo.423.1">, while the </span><strong class="source-inline"><span class="koboSpan" id="kobo.424.1">first</span></strong><span class="koboSpan" id="kobo.425.1"> function finds the next non-null value within </span><strong class="source-inline"><span class="koboSpan" id="kobo.426.1">windowB</span></strong><span class="koboSpan" id="kobo.427.1">. </span><span class="koboSpan" id="kobo.427.2">These two non-null values are averaged. </span><span class="koboSpan" id="kobo.427.3">As the window slides over all the records, all the missing values are replaced by the </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">averaged value:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.429.1">
from pyspark.sql import Window
# Example: Handling missing values by backward filling
# "timestamp" column is ordered chronologically
df = spark.sql(
    f"select timestamp, </span><strong class="bold"><span class="koboSpan" id="kobo.430.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.431.1"> from {table_name} order by timestamp"
)
</span><strong class="bold"><span class="koboSpan" id="kobo.432.1">windowF</span></strong><span class="koboSpan" id="kobo.433.1"> = </span><strong class="bold"><span class="koboSpan" id="kobo.434.1">Window.rowsBetween</span></strong><span class="koboSpan" id="kobo.435.1">(float('-inf'),0)
</span><strong class="bold"><span class="koboSpan" id="kobo.436.1">windowB</span></strong><span class="koboSpan" id="kobo.437.1"> = </span><strong class="bold"><span class="koboSpan" id="kobo.438.1">Window.rowsBetween</span></strong><span class="koboSpan" id="kobo.439.1">(0,float('inf'))
filled_df = df.withColumn(
    "filled_Global_active_power", (F.last(
        df['Global_active_power'], ignorenulls=True
    ).</span><strong class="bold"><span class="koboSpan" id="kobo.440.1">over</span></strong><span class="koboSpan" id="kobo.441.1">(windowF) + F.first(
        df['Global_active_power'], ignorenulls=True
    ).</span><strong class="bold"><span class="koboSpan" id="kobo.442.1">over</span></strong><span class="koboSpan" id="kobo.443.1">(windowB))/2)
# Display updated values
filled_df.filter(
    "timestamp BETWEEN '2008-11-10 17:58:00' AND'2008-11-10 18:17:00'"
).display()</span></pre> <p><span class="koboSpan" id="kobo.444.1">The result </span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.445.1">of interpolation can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.446.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.447.1">.8</span></em><span class="koboSpan" id="kobo.448.1">, where the filled values are shown in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">filled_Global_active_power</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.450.1"> column.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<span class="koboSpan" id="kobo.451.1"><img alt="" src="image/B18568_05_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.452.1">Figure 5.8: Interpolation</span></p>
<p><span class="koboSpan" id="kobo.453.1">Interpolation </span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.454.1">works well for slow-changing values, when there is a predictable cyclical pattern, or when there is a small gap in data. </span><span class="koboSpan" id="kobo.454.2">It is not a good method when the value can change suddenly, is discrete, or when there is a large gap </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">in data.</span></span></p>
<p><span class="koboSpan" id="kobo.456.1">Of the three methods shown for handling missing values, the appropriate method to use is based on the characteristics of your time series data and the requirements of </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">your analysis.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.458.1">Data leakage</span></p>
<p class="callout"><span class="koboSpan" id="kobo.459.1">Note that the </span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.460.1">backward filling and interpolation methods can leak future data across the boundaries of training, validation, and test data splits. </span><span class="koboSpan" id="kobo.460.2">Use these methods within the splits, and not across, or use forward filling if this is going to be </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">an issue.</span></span></p>
<h3><span class="koboSpan" id="kobo.462.1">Duplicates</span></h3>
<p><span class="koboSpan" id="kobo.463.1">The presence of </span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.464.1">duplicate values in time series data can skew analysis and lead to incorrect results. </span><span class="koboSpan" id="kobo.464.2">Apache Spark has functions to efficiently remove duplicate values. </span><span class="koboSpan" id="kobo.464.3">In the following example, we clean time series data for duplicate values using Apache Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">in Python.</span></span></p>
<p><span class="koboSpan" id="kobo.466.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">dropDuplicates</span></strong><span class="koboSpan" id="kobo.468.1"> function removes duplicates by comparing all columns by default and only considers a row to be a duplicate if all the columns match those of one or more other rows. </span><span class="koboSpan" id="kobo.468.2">This will not work if we have multiple rows with, say, the same </span><strong class="source-inline"><span class="koboSpan" id="kobo.469.1">timestamp</span></strong><span class="koboSpan" id="kobo.470.1"> column value but different values in one or more other columns. </span><span class="koboSpan" id="kobo.470.2">In this case, we can pass a subset of one or more columns as a parameter to be used to identify the duplicates, as opposed to using all </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">the columns.</span></span></p>
<p><span class="koboSpan" id="kobo.472.1">In the most common cases, we want to have one and only one row of values for each timestamp and consider the other rows with the same timestamp to be duplicates. </span><span class="koboSpan" id="kobo.472.2">Passing the timestamp as the subset parameter to </span><strong class="source-inline"><span class="koboSpan" id="kobo.473.1">dropDuplicates</span></strong><span class="koboSpan" id="kobo.474.1"> will remove all the other rows having the same timestamp value, as we will see in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">code example:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.476.1">
# Example: Remove duplicate rows based on all columns
print(f"With duplicates - count: {df.count()}")
cleaned_df = df.</span><strong class="bold"><span class="koboSpan" id="kobo.477.1">dropDuplicates</span></strong><span class="koboSpan" id="kobo.478.1">()
print(f"Without duplicates - count: {cleaned_df.count()}")
# Example: Remove duplicate rows based on selected columns
# Assuming "timestamp" is the column to identify duplicates
cleaned_df = df.dropDuplicates(["timestamp"])
print(f"Without duplicates timestamp - count: {cleaned_df.count()}")</span></pre> <p><span class="koboSpan" id="kobo.479.1">Depending on your dataset and use case, you can choose the appropriate method based on the columns that uniquely identify duplicates in your time </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">series data.</span></span></p>
<h3><span class="koboSpan" id="kobo.481.1">Outliers</span></h3>
<p><span class="koboSpan" id="kobo.482.1">The</span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.483.1"> detection and handling of outliers in time series data is crucial to ensure the accuracy of analysis and modeling. </span><span class="koboSpan" id="kobo.483.2">Apache Spark provides various functions</span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.484.1"> to detect and handle outliers efficiently. </span><span class="koboSpan" id="kobo.484.2">The following example shows how to clean time series data for outliers using Apache Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">in Python.</span></span></p>
<p><span class="koboSpan" id="kobo.486.1">The z-score method </span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.487.1">used is based on how far the data point is from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.488.1">mean</span></strong><span class="koboSpan" id="kobo.489.1"> relative to the standard deviation, </span><strong class="source-inline"><span class="koboSpan" id="kobo.490.1">stddev</span></strong><span class="koboSpan" id="kobo.491.1">. </span><span class="koboSpan" id="kobo.491.2">The parametrizable threshold value, </span><strong class="source-inline"><span class="koboSpan" id="kobo.492.1">z_score_threshold</span></strong><span class="koboSpan" id="kobo.493.1">, then specifies beyond which z-score value the data point is considered an outlier. </span><span class="koboSpan" id="kobo.493.2">A high threshold will allow more data points in, while a low threshold will flag </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">more outliers:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.495.1">
from pyspark.sql import functions as F
# Example: Detect outliers using z-score
# Compute z-score for each value in the "value" column
mean_value = df.select(F.</span><strong class="bold"><span class="koboSpan" id="kobo.496.1">mean</span></strong><span class="koboSpan" id="kobo.497.1">(
    "Global_active_power")).collect()[0][0]
stddev_value = df.select(F.</span><strong class="bold"><span class="koboSpan" id="kobo.498.1">stddev</span></strong><span class="koboSpan" id="kobo.499.1">(
    "Global_active_power")).collect()[0][0]
z_score_threshold = 5  # Adjust the threshold as needed
df_with_z_score = df.withColumn("z_score", (F.col(
    "Global_active_power") - mean_value) / stddev_value)
# Filter out rows where z-score exceeds the threshold
outliers = df_with_z_score.</span><strong class="bold"><span class="koboSpan" id="kobo.500.1">filter</span></strong><span class="koboSpan" id="kobo.501.1">(~F.col("z_score").between(
    -z_score_threshold, z_score_threshold))
cleaned_df = df_with_z_score.filter(F.col("z_score").between(
    -z_score_threshold, z_score_threshold))
# Mark as outliers
df_with_outlier = df_with_z_score.withColumn(
    "_outlier",
    F.when(
        (F.col("z_score") &lt; -z_score_threshold) |
        (F.col("z_score") &gt; z_score_threshold), 1
    ).otherwise(0))
print(f"With outliers - count: {df.count()}")
print(f"Global_active_power - mean: {mean_value}, stddev_value: {stddev_value}, z_score_threshold: {z_score_threshold}")
print(f"Without outliers - count: {cleaned_df.count()}")
print(f"Outliers - count: {outliers.count()}")
print("Outliers:")
outliers.display()</span></pre> <p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.502.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.503.1">.9</span></em><span class="koboSpan" id="kobo.504.1"> shows </span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.505.1">the outcome of the outlier detection based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">z-score chosen.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<span class="koboSpan" id="kobo.507.1"><img alt="" src="image/B18568_05_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.508.1">Figure 5.9: Outlier detection</span></p>
<p><span class="koboSpan" id="kobo.509.1">Beyond this</span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.510.1"> example, the choice of z-score threshold and outlier detection techniques is based on the data characteristics and requirements of the </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">use case.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.512.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.513.1">Outliers can be</span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.514.1"> indicative of one or more anomalies in the source system that generated the measurement, or data processing or transmission issues post source system. </span><span class="koboSpan" id="kobo.514.2">The identification of outliers flags the requirement to further investigate the source system and the data transmission chain to find the </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">root cause.</span></span></p>
<p><span class="koboSpan" id="kobo.516.1">After cleaning the data based on the issues identified with the data quality checks, other transformations, which we will look at next, are required to get the data into the right shape for the analytics algorithm </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">to work.</span></span></p>
<h2 id="_idParaDest-113"><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.518.1">Transformations</span></h2>
<p><span class="koboSpan" id="kobo.519.1">In this section, we</span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.520.1"> will look at examples of normalizing and standardizing, and touch briefly on </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">stationary transformation.</span></span></p>
<h3><span class="koboSpan" id="kobo.522.1">Normalizing</span></h3>
<p><span class="koboSpan" id="kobo.523.1">Normalizing time</span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.524.1"> series data ensures that features are on a similar scale, which can improve the performance of machine learning algorithms while facilitating analysis. </span><span class="koboSpan" id="kobo.524.2">Apache Spark provides various functions for normalization. </span><span class="koboSpan" id="kobo.524.3">The following example shows how to normalize time series data using Apache Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">in Python.</span></span></p>
<p><span class="koboSpan" id="kobo.526.1">The</span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.527.1"> min-max normalization technique is used to scale the data points relative to the min-max range. </span><span class="koboSpan" id="kobo.527.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">min</span></strong><span class="koboSpan" id="kobo.529.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.530.1">max</span></strong><span class="koboSpan" id="kobo.531.1"> values are calculated first. </span><span class="koboSpan" id="kobo.531.2">This brings the value to the range of </span><strong class="source-inline"><span class="koboSpan" id="kobo.532.1">0</span></strong><span class="koboSpan" id="kobo.533.1"> for </span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.534.1">the minimum value and </span><strong class="source-inline"><span class="koboSpan" id="kobo.535.1">1</span></strong><span class="koboSpan" id="kobo.536.1"> for the </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">maximum value:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.538.1">
from pyspark.sql import functions as F
# Define the columns to normalize (e.g., "value" column)
columns_to_normalize = ["Global_active_power"]
# Compute the minimum and maximum values for each column to normalize
min_max_values = df.select(
    [F.</span><strong class="bold"><span class="koboSpan" id="kobo.539.1">min</span></strong><span class="koboSpan" id="kobo.540.1">(F.col(column)).alias(f"min_{column}")
     for column in columns_to_normalize] +
    [F.</span><strong class="bold"><span class="koboSpan" id="kobo.541.1">max</span></strong><span class="koboSpan" id="kobo.542.1">(F.col(column)).alias(f"max_{column}")
     for column in columns_to_normalize]
).collect()[0]
# Normalize the data using min-max normalization
for column in columns_to_normalize:
    min_value = min_max_values[f"min_{column}"]
    max_value = min_max_values[f"max_{column}"]
    df = df.withColumn(
        f"normalized_{column}",
        (F.col(column) - min_value) / (max_value - min_value))
print(f"Normalized - {columns_to_normalize}:")
df.display()</span></pre> <p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.543.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.544.1">.10</span></em><span class="koboSpan" id="kobo.545.1"> shows the outcome of normalizing the example time </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">series data.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<span class="koboSpan" id="kobo.547.1"><img alt="" src="image/B18568_05_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.548.1">Figure 5.10: Normalizing</span></p>
<p><span class="koboSpan" id="kobo.549.1">Depending on </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.550.1">the specific requirements and data characteristics, the normalization method can be adjusted with the use of other techniques such as z-score normalization and decimal scaling, in addition to the min-max technique used in </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">the example.</span></span></p>
<h3><span class="koboSpan" id="kobo.552.1">Standardizing</span></h3>
<p><span class="koboSpan" id="kobo.553.1">Standardizing time </span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.554.1">series data ensures that features are on a similar scale, which can improve the performance of machine learning algorithms while facilitating analysis. </span><span class="koboSpan" id="kobo.554.2">This method transforms the data such that it has a mean of </span><strong class="source-inline"><span class="koboSpan" id="kobo.555.1">0</span></strong><span class="koboSpan" id="kobo.556.1"> and a standard deviation of </span><strong class="source-inline"><span class="koboSpan" id="kobo.557.1">1</span></strong><span class="koboSpan" id="kobo.558.1">. </span><span class="koboSpan" id="kobo.558.2">Apache Spark provides various functions for standardization. </span><span class="koboSpan" id="kobo.558.3">The following example shows how to standardize time series data using Apache Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">in Python.</span></span></p>
<p><span class="koboSpan" id="kobo.560.1">This example uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.561.1">log</span></strong><span class="koboSpan" id="kobo.562.1"> values to account for the skewness of the data. </span><span class="koboSpan" id="kobo.562.2">First, </span><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">mean</span></strong><span class="koboSpan" id="kobo.564.1"> and  </span><strong class="source-inline"><span class="koboSpan" id="kobo.565.1">stddev</span></strong><span class="koboSpan" id="kobo.566.1"> are calculated. </span><span class="koboSpan" id="kobo.566.2">These values are then used in the formula </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">to standardize:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.568.1">
from pyspark.sql import functions as F
# Define the columns to standardize (e.g., "value" column)
columns_to_standardize = ["Global_active_power"]
# Compute the mean and standard deviation for each column to 
# standardize
mean_stddev_values = df.select(
    [F.</span><strong class="bold"><span class="koboSpan" id="kobo.569.1">mean</span></strong><span class="koboSpan" id="kobo.570.1">(F.</span><strong class="bold"><span class="koboSpan" id="kobo.571.1">log</span></strong><span class="koboSpan" id="kobo.572.1">(F.col(column))).alias(f"mean_{column}")
     for column in columns_to_standardize] +
    [F.</span><strong class="bold"><span class="koboSpan" id="kobo.573.1">stddev</span></strong><span class="koboSpan" id="kobo.574.1">(F.log(F.col(column))).alias(f"stddev_{column}")
     for column in columns_to_standardize]
).collect()[0]
# Standardize the data using z-score standardization
for column in columns_to_standardize:
    mean_value = mean_stddev_values[f"mean_{column}"]
    stddev_value = mean_stddev_values[f"stddev_{column}"]
    df = df.withColumn(
        f"standardized_{column}",
        (F.log(F.col(column)) - mean_value) / stddev_value
    )
print(f"Standardized - {columns_to_standardize}:")
df.display()</span></pre> <p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.575.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.576.1">.11</span></em><span class="koboSpan" id="kobo.577.1"> shows</span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.578.1"> the outcome of standardizing the example time </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">series data.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<span class="koboSpan" id="kobo.580.1"><img alt="" src="image/B18568_05_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.581.1">Figure 5.11: Standardizing</span></p>
<p><span class="koboSpan" id="kobo.582.1">The standardization </span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.583.1">method can be adjusted depending on the specific requirements and </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">data characteristics.</span></span></p>
<h3><span class="koboSpan" id="kobo.585.1">Stationary</span></h3>
<p><span class="koboSpan" id="kobo.586.1">In </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.587.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.588.1">, we</span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.589.1"> discussed the requirement of stationary time series for some analysis methods. </span><span class="koboSpan" id="kobo.589.2">Making time series data stationery involves removing trends and seasonality, which we will cover in the </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">following chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.591.1">This concludes the section on testing for data quality and then cleaning and transforming time series data. </span><span class="koboSpan" id="kobo.591.2">We will cover the scalability considerations in data preparation when we discuss feature engineering in </span><a href="B18568_08.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.592.1">Chapter 8</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.593.1">.</span></span></p>
<h1 id="_idParaDest-114"><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.594.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.595.1">In conclusion, this chapter focused on the critical steps of organizing, cleaning, and transforming time series data for effective analysis. </span><span class="koboSpan" id="kobo.595.2">We have covered data preparation techniques using Apache Spark for ingestion, persistence, data quality checks, cleaning, and transformations. </span><span class="koboSpan" id="kobo.595.3">We looked at code examples for, among others, handling missing values and duplicates, addressing outliers, and normalizing data. </span><span class="koboSpan" id="kobo.595.4">This has set the stage for an accurate and efficient analytical process using Apache Spark. </span><span class="koboSpan" id="kobo.595.5">Proper data preparation significantly enhances the reliability of subsequent analytical processes, which is what we will progress toward in the </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">next chapter.</span></span></p>
<h1 id="_idParaDest-115"><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.597.1">Join our community on Discord</span></h1>
<p><span class="koboSpan" id="kobo.598.1">Join our community’s Discord space for discussions with the authors and </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">other readers:</span></span></p>
<p><a href="https://packt.link/ds"><span class="No-Break"><span class="koboSpan" id="kobo.600.1">https://packt.link/ds</span></span></a></p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<span class="koboSpan" id="kobo.601.1"><img alt="" src="image/ds_(1).jpg"/></span>
</div>
</div>
</div>
</body></html>