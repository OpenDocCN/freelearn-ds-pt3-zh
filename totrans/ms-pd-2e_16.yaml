- en: Data Case Studies Using pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have covered the extensive functionalities of pandas. We'll try to
    implement these functionalities in some case studies. These case studies will
    give us an overview of the use of each functionality and help us determine the
    pivotal points in handling a DataFrame. Moreover, the step-by-step approach of
    the case studies helps us to deepen our understanding of the pandas functions.
    This chapter is equipped with practical examples along with code snippets to ensure
    that, by the end, you understand the pandas approach to solving the DataFrame
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following case studies:'
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end exploratory data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web scraping with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-to-end exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploratory data analysis refers to the critical process of understanding the
    quirks of data—the outliers, the columns containing the most relevant information,
    and determining the relationship between the variables using statistics and graphical
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following DataFrame to perform exploratory data analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the DataFrame loaded in Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1432bfa2-441d-4f22-90e2-78bd09b48ece.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame loaded in Jupyter Notebook
  prefs: []
  type: TYPE_NORMAL
- en: Data overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding DataFrame is the customer data of an automobile servicing firm.
    They basically provide services to their clients on a periodic basis. Each row
    in the DataFrame corresponds to a unique customer. Hence, it is customer-level
    data. Here is an observation from the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/547abf7c-ab9b-4b1c-9a65-48b8df8ff4c9.png)'
  prefs: []
  type: TYPE_IMG
- en: The shape of the DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: We can observe that the data contains 27,002 records and 26 characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start exploratory data analysis on any data, it is advised to know
    as much about the data as possible—the column names and their corresponding data
    types, whether they contain null values or not (and if so, how many), and so on.
    The following screenshot shows some of the basic information about the DataFrame
    obtained using the `info` function in pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9f4f4d1-c447-449f-9f7a-c6d9865e95fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic information about the DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: Using the `info()` function, we can see that the data only has float and integer
    values. Also, none of the columns has null/missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `describe()` function in pandas is used to obtain various summary statistics
    of all the numeric columns. This function returns the count, mean, standard deviation,
    minimum and maximum values, and the quantiles of all the numeric columns. The
    following table shows the description of the data obtained using the `describe`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9f15dcf-e64c-4c09-a5c2-535d7daa7c6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Describing the Data
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you a have dataset with many variables, a good way to check correlations
    among columns is by visualizing the correlation matrix as a heatmap. We can identify
    and remove those that are highly correlated, thereby simplifying our analysis.
    The visualization can be achieved using the `seaborn` library in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c159842-b5f9-4c9e-aa06-4184d9218eed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following will be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa3c9dde-f91f-42dd-b039-69e9b4a08718.png)'
  prefs: []
  type: TYPE_IMG
- en: Correlation heatmap of the DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe the following in the preceding heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: '`soldBy` and `days_old` are highly negatively correlated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age_median` and `income_median` are positively correlated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, we can derive the correlation between different sets of variables.
    Hence, based on the correlation results, we can minimize the number of independent
    features by selecting only the important features.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from selecting the useful features, we also need to extract significant
    variables from the existing ones. This method is called **feature extraction**.
    In the current example, a new feature called `new_tenure` has been extracted from
    the existing variables. This variable gives us the amount of time that a customer
    has stayed with the firm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following DataFrame shows the newly extracted variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b70a40ca-73ab-42af-9ac9-8c6d120f2d31.png)'
  prefs: []
  type: TYPE_IMG
- en: The DataFrame with the newly extracted variables
  prefs: []
  type: TYPE_NORMAL
- en: Data aggregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned earlier, the data presented is customer-level data. It would be
    more feasible and easy to perform analysis on aggregated data, which in this case
    is a region. To start with, we need to understand how the customers are spread
    across each region. Hence, we are going to use the `groupby` function to find
    the number of customers in each zip code. The snippet and its output are shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21e6a21a-c125-4c22-9b88-0f16a5a4ea47.png)'
  prefs: []
  type: TYPE_IMG
- en: Aggregating the data based on zip codes
  prefs: []
  type: TYPE_NORMAL
- en: This gives the first 10 zip codes that have the maximum number of customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can convert our client-level data into zip-level data using aggregation.
    After grouping the values, we also have to make sure that we remove the NAs. The
    following code can be used to perform aggregation on the entire DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the aggregated DataFrame after removing the NAs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dddd3a12-a571-4769-b85e-e945a13804d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Aggregated DataFrame after removing the NAs
  prefs: []
  type: TYPE_NORMAL
- en: '`data_clean` will become the cleaned version of our sample DataFrame, which
    will be passed to a model for further analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web scraping deals with extracting large amounts of data from websites in either
    structured or unstructured forms. For example, a website might have some data
    already present in an HTML table element or as a CSV file. This is an example
    of structured data on website. But, in most cases, the required information would
    be scattered across the content of the web page. Web scraping helps collect these
    data and store it in a structured form. There are different ways to scrape websites
    such as online services, APIs, or writing your own code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some important notes about web scraping:'
  prefs: []
  type: TYPE_NORMAL
- en: Read through the website's terms and conditions to understand how you can legally
    use the data. Most sites prohibit you from using the data for commercial purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure you are not downloading data at a rapid rate because this may break
    the website. You may potentially be blocked from the site as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web scraping using pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python provides different libraries for scraping:'
  prefs: []
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BeautifulSoup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scrapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we'll see how to scrape data by leveraging the power of pandas
    and BeautifulSoup. To start with, pandas is sufficient to extract structured data
    from a website without the help of BeautifulSoup. In the earlier sections, we
    learned about loading data from different formats (`.csv`, .`xlsx`, and `.xls`)
    in Python. Similar to these, pandas has a separate function for loading tabular
    data from an HTML file. To read an HTML file, a pandas DataFrame looks for a tag.
    That tag is called a `<td> </td>` tag. This tag is used to define a table in HTML.
  prefs: []
  type: TYPE_NORMAL
- en: pandas uses `read_html()` to read the HTML document. This function loads all
    the structured data from the URL into the Python environment. So, whenever you
    pass an HTML to pandas and expect it to output a nice-looking DataFrame, make
    sure the HTML page has a table in it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can try this function on a sample URL ([https://www.bseindia.com/static/members/TFEquity.aspx](https://www.bseindia.com/static/members/TFEquity.aspx)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48da0222-119d-4f5d-8fc8-b24649577cab.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample web page
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding web page contains multiple tables. Using pandas, we can extract
    all the tables, which will be stored inside a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd6d4277-e11c-432b-a1dc-91e58c062567.png)'
  prefs: []
  type: TYPE_IMG
- en: List containing multiple DataFrames
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, the second table from the web page is being extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e61565b-b000-45ad-a4e3-7e34cc00fe32.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of the web page and the pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'After cleaning, the extracted DataFrame is an exact replica of what is available
    on the website:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54c4560d-d7c0-42f2-b174-f2a9d83c5e35.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame after cleaning
  prefs: []
  type: TYPE_NORMAL
- en: With proper indexing, all the tables from a web page can be extracted using
    the `read_html` function.
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping using BeautifulSoup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BeautifulSoup is a Python library ([https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/)) for
    pulling data out of HTML and XML files. It provides ways of navigating, accessing,
    searching, and modifying the HTML content of a web page. It is important to understand
    the basics of HTML in order to successfully scrape a web page. To parse the content,
    the first thing that we need to do is to figure out where we can locate the links
    to the files we want to download inside the multiple levels of HTML tags. Simply
    put, there is a lot of code on a web page, and we want to find the relevant pieces
    of code that contains our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the website, right-click and click on Inspect. This allows you to see the
    raw code behind the site. Once you''ve clicked on Inspect, you should see the
    following console pop up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73f85c88-ed30-433d-80e5-8a12cd1bb2fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspect menu of a browser
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the table that we are referring to is wrapped in a tag called **table**.
    Each row will be present between `<tr>` tags. Similarly, each cell will be present
    between `<td>` tags. Understanding these basic differences makes it easier to
    extract the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68b0881a-d34a-45e9-a21d-97f13641eb39.png)'
  prefs: []
  type: TYPE_IMG
- en: Importing libraries
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we request the URL with the `requests` library. If the access was successful,
    you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9fc1dc8-0f62-4f70-82c6-7a12fc6be37b.png)'
  prefs: []
  type: TYPE_IMG
- en: Successful response from a website
  prefs: []
  type: TYPE_NORMAL
- en: 'We then parse `html` with `BeautifulSoup` so that we can work with a nicer,
    nested `BeautifulSoup` data structure. With a little knowledge of HTML tags, the
    parsed content can be easily converted into a DataFrame using a `for` loop and
    a pandas DataFrame. The biggest advantage of using BeautifulSoup is that it can
    even extract data from unstructured sources that can be molded into a table by
    the supported libraries, whereas the `read_html` function of pandas will only
    work with structured sources. Hence, based on the requirement, we have used `BeautifulSoup`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35619365-12c4-4da0-9066-c80ceb661ea1.png)'
  prefs: []
  type: TYPE_IMG
- en: Extracted DataFrame using BeautifulSoup
  prefs: []
  type: TYPE_NORMAL
- en: Data validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data validation is the process of examining the quality of data to ensure it
    is both correct and useful for performing analysis. It uses routines, often called
    **validation rules**, that check for the genuineness of the data that is input
    to the models. In the age of big data, where vast caches of information are generated
    by computers and other forms of technology that contribute to the quantity of
    data being produced, it would be incompetent to use such data if it lacks quality,
    highlighting the importance of data validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case study, we are going to consider two DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: Test DataFrame (from a flat file)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation DataFrame (from MongoDB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation routines are performed on the test DataFrame, keeping its counterpart
    as the reference.
  prefs: []
  type: TYPE_NORMAL
- en: Data overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The datasets considered here are part of the **Learning Management System**
    (**LMS**) data. They project information pertaining to student enrolment, tracking,
    reporting, and delivery of educational courses. Let''s load the test DataFrame from
    the flat file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1ea73e9-25bd-41f9-a6b4-7cbee992b41f.png)'
  prefs: []
  type: TYPE_IMG
- en: Loading test DataFrame from flat file
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pymongo` library is used to connect MongoDB to Python. Generally, MongoDB
    listens on port `27017`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac1e34df-1652-4b6d-804e-80f1f5cadc37.png)'
  prefs: []
  type: TYPE_IMG
- en: MongoDB connection from Python
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the connection parameters in the following screenshot. Since the
    database is installed locally, we are connecting to it via localhost. The name
    of the loaded database is `lms_db`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/683b1c50-4e42-4bf8-a2c1-7dfa8b8ac53b.png)'
  prefs: []
  type: TYPE_IMG
- en: Reading data from MongoDB
  prefs: []
  type: TYPE_NORMAL
- en: Structured databases versus unstructured databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since MongoDB falls under the category of unstructured databases, the terminology
    used widely differs from its structured counterparts, such as MySQL and PostgreSQL.
    The following table presents various SQL terminology and concepts and the corresponding
    MongoDB terminology and concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **SQL terms/concepts** | **MongoDB terms/concepts** |'
  prefs: []
  type: TYPE_TB
- en: '| database | Database ([h](https://docs.mongodb.com/manual/reference/glossary/#term-database)[ttps://docs.mongodb.com/manual/reference/glossary/#term-database](https://docs.mongodb.com/manual/reference/glossary/#term-database)[)](https://docs.mongodb.com/manual/reference/glossary/#term-database)
    |'
  prefs: []
  type: TYPE_TB
- en: '| table | Collection ([https://docs.mongodb.com/manual/reference/glossary/#term-collection](https://docs.mongodb.com/manual/reference/glossary/#term-collection))
    |'
  prefs: []
  type: TYPE_TB
- en: '| row | Document or BSON document |'
  prefs: []
  type: TYPE_TB
- en: '| column | Field ([https://docs.mongodb.com/manual/reference/glossary/#term-field](https://docs.mongodb.com/manual/reference/glossary/#term-field))
    |'
  prefs: []
  type: TYPE_TB
- en: '| index | Index ([https://docs.mongodb.com/manual/reference/glossary/#term-index](https://docs.mongodb.com/manual/reference/glossary/#term-index))
    |'
  prefs: []
  type: TYPE_TB
- en: '| table joins | `$lookup`, embedded documents ([https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/#pipe._S_lookup](https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/#pipe._S_lookup))
    |'
  prefs: []
  type: TYPE_TB
- en: '| primary key | Primary key ([https://docs.mongodb.com/manual/reference/glossary/#term-primary-key](https://docs.mongodb.com/manual/reference/glossary/#term-primary-key))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Specify any unique column or column combination as primary key. | In MongoDB,
    the primary key is automatically set to the `_id` field. ([https://docs.mongodb.com/manual/reference/glossary/#term-id](https://docs.mongodb.com/manual/reference/glossary/#term-id))
    |'
  prefs: []
  type: TYPE_TB
- en: '| aggregation (example group by) | Aggregation pipeline |'
  prefs: []
  type: TYPE_TB
- en: '| transactions | Transactions ([https://docs.mongodb.com/manual/core/transactions/](https://docs.mongodb.com/manual/core/transactions/))
    |'
  prefs: []
  type: TYPE_TB
- en: Comparative view of SQL MongoDB terminologies
  prefs: []
  type: TYPE_NORMAL
- en: Validating data types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A data type is a property of a variable that Python uses to understand how to
    store and manipulate data. For instance, a program needs to understand that variables
    storing 5 and 10 are numeric to be able to add them and get 15, or that the variables
    storing `cat` and `hat` are strings so that they could be concatenated (added)
    together to get `cathat`. Hence it becomes a preliminary and cardinal property
    of any pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'A user-defined comparison function can be used to validate the data types of
    the test DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e612fdcc-3078-43d5-affc-f33a871cc54c.png)'
  prefs: []
  type: TYPE_IMG
- en: Validating data types of test DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: '`File1` and `File2` correspond to the test and validation datasets respectively.
    It is evident from the output that all the data types of the test DataFrame match
    those of the validation DataFrame. If there is a mismatch, the output will display
    the number of columns that are inconsistent.'
  prefs: []
  type: TYPE_NORMAL
- en: Validating dimensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A DataFrame is a two-dimensional data structure where data is presented in
    a tabular manner, much like a relational database table, in rows and columns.
    One of the basic methods to check whether the test and validation datasets are
    matching is to equate the number of rows and columns. If the shapes of the DataFrames
    do not match, it becomes clearly evident that the test DataFrame is different
    from the validation one. The following is a screenshot that shows how to validate
    dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10056043-7eb1-4884-9719-17df84ac20dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Validating dimensions
  prefs: []
  type: TYPE_NORMAL
- en: Validating individual entries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the first two test cases are satisfied, it becomes highly important to
    scan individual entries to find spurious data. The validation process in the preceding
    figure describes the difference between a value obtained from a data collection
    process and the true value. As the amount of data increases, validating entries
    becomes difficult. This effect can be diminished by efficiently utilizing pandas.
    In the following example, individual entries have been scanned using both looping
    (the brute force method) and pandas indexing.
  prefs: []
  type: TYPE_NORMAL
- en: Using pandas indexing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following screenshot shows how to validate cells using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/229fbaa0-4d36-4ead-92c6-a1bb93f15028.png)'
  prefs: []
  type: TYPE_IMG
- en: Validating cells using pandas indexing
  prefs: []
  type: TYPE_NORMAL
- en: Using loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following screenshot shows how to validate cells by using loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3e02c99-beba-4d35-8f5b-f062d92522d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Validating cells using loops
  prefs: []
  type: TYPE_NORMAL
- en: The results were highly encouraging when we used pandas indexing. It took only
    0.53 seconds to validate a DataFrame with 200,000 rows and 15 columns, whereas
    the same validation routine took more than 7 minutes to complete using loops.
    Therefore it is always recommended to leverage the power of pandas and avoid iterative
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: pandas is useful for a lot of ancillary data activities, such as exploratory
    data analysis, validating the sanctity (such as the data type or count) of data
    between two data sources, and structuring and shaping data obtained from another
    source, such as scraping a website or a database. In this chapter, we dealt with
    some case studies on these topics. A data scientist performs these activities
    on a day-to-day basis, and this chapter should give a flavor of what it is like
    to perform them on a real dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss the architecture and code structure of
    the pandas library. This will help us develop an exhaustive understanding of the
    functionalities of the library and enable us to do better troubleshooting.
  prefs: []
  type: TYPE_NORMAL
