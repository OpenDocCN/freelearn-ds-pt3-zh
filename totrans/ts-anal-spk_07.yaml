- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building and Testing Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having covered the data preparation and exploratory data analysis stages of
    time series analysis, we will now direct our focus to constructing predictive
    models for time series data. We will cover the diverse types of models and how
    to decide which one to choose. We will also learn how to train, tune, and evaluate
    models.
  prefs: []
  type: TYPE_NORMAL
- en: The concepts covered in this chapter will act as a practical guide to model
    development, providing essential building blocks for effective time series models
    and facilitating accurate predictions and insightful analyses. We will factor
    in common execution constraints faced in real-life projects and conclude with
    a comparison of the outcome of the different models to solve a forecasting problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Model selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for this chapter, which will be covered in the *Development and testing*
    section, can be found in the `ch7` folder of the book’s GitHub repository at this
    URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch7](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch7).'
  prefs: []
  type: TYPE_NORMAL
- en: Model selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step before developing a time series analysis model is to select which
    model to use. As discussed in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016),
    one of the key challenges of time series analysis is using the right model. This
    choice impacts, among other things, the accuracy, reliability, efficiency, and
    scalability of the analysis. This, in turn, ensures that the analysis leads to
    better-informed decisions and more effective outcomes while being scientifically
    robust and practically useful.
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of models, each with its own characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Types of models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Time series analysis models can be categorized into statistical, classical
    Machine Learning (ML), and Deep Learning (DL) models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical models** for time series analysis are based on statistical theories
    with assumptions about the characteristics of the time series, such as linearity
    and stationarity. Examples of classical models include **Autoregressive Moving
    Average** (**ARIMA**), **Seasonal Autoregressive Integrated Moving Average Exogenous**
    (**SARIMAX**), **Exponential Smoothing** (**ETS**), **Generalized Autoregressive
    Conditional Heteroskedasticity** (**GARCH**), and state-space models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classical Machine Learning models** for time series analysis use algorithms
    that can learn from data without explicit programming. These models can handle
    non-linear relationships. However, they often require more data for training compared
    to classical models. Examples of Machine Learning models include linear regression,
    **Support Vector Machines** (**SVMs**), **k-Nearest Neighbors** (**kNN**), random
    forests, and gradient boosting machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Learning models** use neural networks with multiple layers to learn
    complex patterns in time series data. These models can handle non-linear relationships
    and long-term dependencies. They, however, require large datasets for training
    and significant computational resources. Examples of Deep Learning models include
    **Long Short-Term Memory (LSTM) Networks**, **Convolutional Neural Networks**
    (**CNNs**), **Temporal Convolutional Networks** (**TCNs**), transformers, and
    autoencoders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Learning and Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning is a subset of Machine Learning that uses deep neural networks.
    As is common practice, we are using the term classical Machine Learning here to
    refer to approaches and models that are not neural-network-based. The term Deep
    Learning is used for approaches and models using neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the preceding categories and models has distinct characteristics and
    approaches that determine their applicability, which we will explore next.
  prefs: []
  type: TYPE_NORMAL
- en: Selection criteria
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When to use which model is based on several criteria. We touched on this briefly
    in the section on using the right model in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016)
    and when initially discussing model selection in [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087).
    The applicability of a model to solve a time series analysis problem is dependent
    on factors such as the objectives of the analysis, the characteristics of the
    data, and the computation power and time available.
  prefs: []
  type: TYPE_NORMAL
- en: We will now dive deep into the details of these and other important factors
    for model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Types of use cases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time series analysis broadly falls into use cases for forecasting, classification,
    and anomaly detection, as discussed in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044).
    We will briefly recap these types of use cases here, highlighting the frequently
    used models. We will go into further detail in the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Forecasting**’s goal is to predict future values based on patterns learned
    by the model from past values. As presented in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044),
    forecasting can be single or multi-steps, based on a single (**univariate**) or
    multiple (**multivariate**) time series. Commonly used models such as ARIMA, SARIMA,
    and **Exponential Smoothing** (**ETS**) are chosen for their simplicity while
    giving strong performance in forecasting tasks. LSTM and Prophet, introduced in
    previous examples in the book, are preferred for more complex forecasting requirements
    where they can be more effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pattern recognition** and **classification** are used to identify and understand
    patterns and classify time series accordingly. Commonly used models are based
    on decomposition methods, such as **Seasonal-Trend decomposition using LOESS**
    (**STL**) and **Multiple STL** (**MSTL**), and Fourier analysis. We spent some
    time on decomposition in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016) and [*Chapter
    6*](B18568_06.xhtml#_idTextAnchor116). We briefly discussed Fourier analysis in
    [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044), in addition to distance-based
    approaches, shapelets analysis, ensemble methods and Deep Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection** aims to identify outliers or anomalies in the time series.
    As presented in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044), this detection
    can be based on univariate or multivariate series and point, collective, or contextual
    analysis. What is initially flagged as an anomaly can turn out to be a novelty,
    in the sense of a new non-problematic pattern. Commonly used models are based
    on their capabilities for residual analysis, such as ARIMA. Machine learning models
    are frequently used as well, such as Isolation Forest or, when there is a high
    percentage of anomalies, specialized methods such as **Seasonal Hybrid Extreme
    Studentized Deviate** (**SH-ESD**). We saw a code example in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044)
    of Isolation Forest for anomaly detection, in addition to discussing supervised,
    unsupervised, semi-supervised, and hybrid approaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another model selection criterion, which we will look at next, is the statistical
    nature of the time series.
  prefs: []
  type: TYPE_NORMAL
- en: Nature of time series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The nature of the time series, that is, its statistical properties, influences
    the choice of model. Models are researched and developed to work well, if at all,
    based on specific assumptions about the nature of the time series, which then
    determines their applicability. We will focus in this section on applicability
    and skip definitions, assuming that, by now, you are familiar with the terms we
    will use in this section, based on the introduction in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016)
    and code examples in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stationary** time series can be modeled with ARIMA, which assumes stationarity.
    An example of a stationary time series is the daily percentage returns of a stock
    over a 3-year period. Assuming no significant structural changes in the market,
    stock returns tend to fluctuate around a stable mean with consistent variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-stationary time series can be converted to stationary, for example, by differencing,
    as seen in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116). The differenced series
    can then be used with such models. Alternatively, use Prophet, or Machine Learning
    models for non-stationary series. An example of a non-stationary time series is
    the monthly unemployment rate, which possibly has a trend, and cyclical patterns
    related to economic conditions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Seasonal** time series require models that handle seasonality, such as SARIMA,
    ETS, Prophet, or Machine Learning models. We have seen this in action with the
    coding example in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044) to forecast
    temperature using Prophet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trends** in time series can impact the performance of certain models, such
    as ARIMA. In this case, similarly to stationarity, we can remove the trend component
    by differencing, as per the code example in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116).
    ARIMA can then be used. Alternatively, use models that can handle trends, such
    as trend models, ETS, Prophet, or Machine Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volatility** in time series can be handled with models such as **Generalized
    Autoregressive Conditional Heteroskedasticity** (**GARCH**), **Stochastic Volatility
    GARCH** (**SV-GARCH**), or Machine Learning. Common use cases for these models
    are forecasting and risk management in highly volatile financial markets and other
    domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity** of the relationship in the data means that linear models such
    as ARIMA are suitable. An example of a linear time series is daily temperature,
    where today’s temperature can be predicted by a linear combination of the temperatures
    from the previous two days plus some random error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of non-linear patterns, Machine Learning models with neural networks
    are preferable. An example of a non-linear time series is if a stock price follows
    one relationship if below a certain threshold (say 100) and follows a different
    relationship if it’s above that threshold.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The volume and frequency of data to analyze, discussed next, is another property
    of time series that influences model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Volume and frequency of data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The volume and frequency of data impact the computational power required and
    the duration of the analysis. The combination of these factors determines the
    choice of model to use. We will discuss volume and frequency here, and the other
    two factors in the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Small datasets** can be analyzed with statistical models such as ARIMA and
    ETS. These are simple models that work well with smaller datasets. An example
    of a small dataset is the daily sales for a store over the past few years.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Large datasets** are a good match for Machine Learning models such as gradient
    boosting and LSTM. This works in both ways: in terms of processing capability
    and scalability of ML models for large datasets, and the substantial amount of
    data needed for model training to avoid overfitting. ML models can learn complex
    patterns present in large datasets at the cost of more computational resources.
    Examples of large datasets are minute-by-minute stock prices or sensor data over,
    say, the past 5 years.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we will see in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151), we can scale
    models to large datasets by using the distributed computing capabilities of Apache
    Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-frequency** time series, such as daily, weekly, monthly, quarterly, or
    annual, are usually small in size. As discussed before about small datasets, ARIMA
    and ETS are usually good choices for such datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-frequency** time series are likely to have rapid changes, noise, volatility,
    and heteroskedasticity, which can be handled with models such as GARCH, often
    used for financial time series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the analysis is required at a lower frequency than the data arrival rate,
    the high-frequency series can be converted to low frequency by resampling and
    aggregation, as discussed in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116).
    Resampling decreases the size of the dataset while smoothing out the noise and
    volatility. This opens the possibility of using models suited for low-frequency
    time series, as discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Diminishing value of high-frequency data
  prefs: []
  type: TYPE_NORMAL
- en: We discussed frequency here as pertaining to the time interval between consecutive
    data points in the time series, also referred to as granularity. Another consideration
    for high-frequency data is the requirement that the analysis also be done at high
    frequency. This is due to the quickly diminishing value of high-frequency data
    over time. Consider how real-time stock tick changes are critical at the moment
    they occur but become less relevant just a few hours later. In this scenario,
    the model must be capable of performing extremely rapid computations, potentially
    in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Higher volume and frequency of data require more computational resources, which
    we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Computational constraints
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Like any other project, time series analysis occurs within a budget. This means
    that the amount of resources available, including the computing power to execute
    the analysis process, is constrained. At the same time, we know that higher volume
    and frequency of data require more computational resources. We also must factor
    in how fast the analysis needs to be completed for the outcome to be useful. With
    these constraints in mind, let’s investigate the choice of model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Limited computation** resources mean that we may have to consider a combination
    of dataset size reduction, with resampling, and simpler models such as ARIMA or
    ETS. Machine learning models, while better at detecting complex patterns and with
    larger datasets, usually require more computation resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast analysis** requires using faster models for training and prediction.
    Models such as ARIMA or ETS are, again, good candidates for smaller datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If fast analysis is required for a large dataset, options include the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scaling out using the distributed processing of Apache Spark clusters on large
    datasets, which we will cover in the next chapter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Resampling to convert to a smaller dataset size, with the use of simpler models
    such as ARIMA or ETS.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Machine Learning models with the following caveats. The training and tuning
    stage will be slower for larger datasets. The prediction speed can be improved
    by using more computation resources, which of course comes at a higher cost. Note
    that training, tuning, and prediction speed can also be improved by using the
    distributed processing of Apache Spark, as we will see in the next chapter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost of compute resources** is another important factor that may limit the
    use of compute-intensive models. While the simpler statistical models can run
    on cheaper standard resources, Deep Learning models may require more expensive
    GPUs on high-performance hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After considering how computational requirements influence the choice of models,
    we will now consider how model accuracy, complexity, and interpretability determine
    which model to use.
  prefs: []
  type: TYPE_NORMAL
- en: Model accuracy, complexity, and interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some of the other factors that are considered for model selection are model
    accuracy, complexity, and interpretability:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model accuracy** is wrongly seen as the determining factor for model selection
    in many cases. Accuracy has been presented at the end of the list of selection
    criteria on purpose to highlight the importance of considering other factors as
    well. The best model is not always the most accurate one. It is the one that delivers
    the most ROI for the use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When high accuracy is needed, especially in forecasting, more complex models
    such as SARIMAX or Deep Learning may be necessary. Hyperparameter tuning is used
    as part of the development process to further improve accuracy, but this comes
    at the cost of additional computations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Complexity** and **interpretability** usually conflict. The need for higher
    accuracy leads to the use of more complex models, which are then harder to interpret
    and often referred to as black boxes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If interpretability is crucial, prefer simpler models such as ARIMA or ETS,
    which have the added benefit of lower compute requirements. Tree-based models
    such as GBM or **Tree-Based Pipelines for Time Series** (**TSPi**) offer a good
    balance of accuracy and compute requirement, while simpler tree-based models offer
    interpretability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the data exhibits complex patterns and high accuracy is crucial, there may
    not be many options, and we may have to use complex models, with a trade-off on
    compute and interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of model selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To conclude on model selection, there are a few points worth noting:'
  prefs: []
  type: TYPE_NORMAL
- en: Statistical models such as ARIMA are based on assumptions about the nature of
    the time series, requiring statistical tests and possibly additional pre-processing
    to convert the series before using the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prophet and Machine Learning models are more broadly applicable but have additional
    complexity and compute requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The models mentioned in this section are provided as examples applicable to
    the criteria discussed. Other models, from a growing list of publicly available
    models and approaches, can and should be tested. Finding the best model is an
    experimentation and iterative process, dependent on one’s context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have seen in this section on selection criteria, several factors influence
    the choice of models and determine which ones to invest more effort in. Which
    factors are most important depends on the project context and the use case. The
    best model to choose is the one resulting in the highest ROI, requiring a trade-off
    between the different factors discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, with the models selected, we are ready to move on to the next
    development step, which is to train the model on our time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Development and testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will compare forecasting performance across different categories
    of models: statistical, classical Machine Learning, and Deep Learning. We will
    use six different models: SARIMA, LightGBM, LSTM, NBEATS, NHITS, and NeuralProphet.
    These models are chosen for their wide and proven adoption and ease of access
    and use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will proceed with the following constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: Use of the default model hyperparameters whenever possible for comparison and
    minimize tuning to a few cases, which will be explained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete execution, from data loading to model training, testing, and forecasting,
    will be limited to under 15 minutes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computing resource used will also be constrained to the Databricks Community
    Edition compute as per *Figure 7**.1*, with 15.3 GB of memory and 2 CPU cores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18568_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Databricks Community Edition compute resource'
  prefs: []
  type: TYPE_NORMAL
- en: We all commonly face time and resource constraints in our real-life projects.
    This section also aims to give you the tools to work within these limits.
  prefs: []
  type: TYPE_NORMAL
- en: Single-threaded, multi-threaded, and clustering
  prefs: []
  type: TYPE_NORMAL
- en: We will use `Pandas` and `NumPy` in the code examples in this chapter. `Pandas`
    is single-threaded in terms of the use of a CPU core. `NumPy` is multi-threaded
    by default, so it makes use of multiple CPU cores in parallel. Both are bound
    to a single machine and do not leverage the multi-machine Spark clustering capability.
    We will address this limitation in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151),
    which covers scaling. As a lot of the existing code examples, you will find use
    `Pandas` and `NumPy`, it is important to start with these libraries as a foundation.
    We will then, in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151), cover how to
    convert the single-machine code to leverage Spark clustering capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The time series data that will be used for this section is an extended version
    of that used in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044) for the energy
    consumption of a household. We will use the same time series for all the models
    we develop in the rest of this chapter. The dataset is in `ts-spark_ch7_ds1_25mb.csv`
    in the `ch7` folder. As this is a new dataset, we will go through the steps of
    exploring the data as part of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we want to check the stationarity, seasonality, and autocorrelation
    in the dataset. This is a crucial step in our understanding of the nature of the
    time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this section is in `ts-spark_ch7_1e_sarima_comm.dbc`. We import
    the code into Databricks Community Edition, as per the approach explained in the
    *Hands-on: Loading and visualizing time series* section of [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code URL is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc)'
  prefs: []
  type: TYPE_NORMAL
- en: The first part of the code loads and prepares the data. We will not go into
    the details of this part here as we already covered data preparation in [*Chapter
    5*](B18568_05.xhtml#_idTextAnchor103), and you can refer to the code in the notebook.
    The data exploration part is, however, pertinent to this chapter, so let’s explore
    this further next, starting with the stationarity check.
  prefs: []
  type: TYPE_NORMAL
- en: Stationarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can check whether the energy consumption time series is stationary by running
    the **Augmented Dickey-Fuller** (**ADF**) test on the data with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following ADF statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As the ADF statistic is less than the critical values and the p-value is less
    than 0.05, we can conclude that the time series is stationary.
  prefs: []
  type: TYPE_NORMAL
- en: Seasonality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can check on the seasonality with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This gives the seasonal decomposition in *Figure 7**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Seasonal decomposition'
  prefs: []
  type: TYPE_NORMAL
- en: As the pattern repeats every 24 hours, we can conclude that the time series
    has a daily seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: Autocorrelation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can check on the autocorrelation and partial autocorrelation with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This gives the autocorrelation plot in *Figure 7**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Autocorrelation (y axis) at different lags (x axis)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the high autocorrelation at the lower lag values, including lag
    1, and at lag 12, as well as the effect of seasonality at lag 24\. This makes
    sense when we consider the following typical patterns of energy consumption in
    a household:'
  prefs: []
  type: TYPE_NORMAL
- en: Moments of active energy use, for example for cooking, washing, or use of the
    television, are likely to go over an hour (lag 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mornings and evenings (lag 12) are usually peaks in activity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daily routines mean that we have similar periods of activities every 24 hours
    (lag 24)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18568_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Partial autocorrelation'
  prefs: []
  type: TYPE_NORMAL
- en: The PACF plot shows high partial autocorrelation at lag 1 and noticeable partial
    autocorrelation around lag 10 and lag 23\. This is in line with the typical patterns
    of energy consumption in a household we mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical model – SARIMA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first model we will cover is SARIMA, which extends the ARIMA model by incorporating
    seasonal components. While ARIMA models address aspects such as autocorrelation,
    differencing for stationarity, and moving averages, SARIMA adds the handling of
    seasonal patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this section is in `ts-spark_ch7_1e_sarima_comm.dbc`. We import
    the code into Databricks Community Edition, as per the approach explained in the
    *Hands-on: Loading and visualizing time series* section of [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code URL is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc)'
  prefs: []
  type: TYPE_NORMAL
- en: Development and tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For model development, we separated the last 48 hours of the dataset from the
    training data with the following code. This will be used for testing afterward.
    We use the rest for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will discuss two methods combining training and tuning to train the model
    and find the best parameters: `auto_arima` and `ParameterGrid`.'
  prefs: []
  type: TYPE_NORMAL
- en: Auto ARIMA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the auto ARIMA approach, we want to automatically find the model parameters
    that minimize the `pmdarima` library to demonstrate the auto ARIMA approach. As
    this is a compute-intensive operation, and we want to keep to the time (15 minutes)
    and resource (Databricks Community Edition) constraints explained previously,
    we will limit the dataset to the last `300` data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to use `pmdarima` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code output shows the step-by-step search for the parameters
    minimizing the AIC. This will be the best set of model parameters to use with
    the ARIMA model to forecast this household’s energy consumption:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that while this is the best set of model parameters, we may find, given
    the time and resource constraints, that we may be able to find a better model
    with a longer run of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: ParameterGrid
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the `ParameterGrid` approach, we will sweep one by one through a list of
    parameter combinations to find the model parameters that minimize the AIC.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to use `ParameterGrid` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: While both auto ARIMA and `ParamaeterGrid` are similar in terms of minimizing
    AIC, auto ARIMA is much simpler to use with only 1 line of code.
  prefs: []
  type: TYPE_NORMAL
- en: After the SARIMA model is trained, we will next test the model forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use the model to forecast the test dataset with the `predict` function,
    one period at a time, updating the model with the actual value after every time
    forecast. This iterative approach converts single-step forecasting in `forecast_step`
    into multi-step forecasting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can then plot the forecast against the actual values in *Figures 7.5* and
    *7.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: SARIMA Forecast vs Actuals (training and testing)'
  prefs: []
  type: TYPE_NORMAL
- en: We zoom in on the testing period in *Figure 7**.6* for a visual comparison of
    the forecast and actuals.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: SARIMA Forecast vs Actuals (zoom on test data)'
  prefs: []
  type: TYPE_NORMAL
- en: While visualizing the graphs gives us an idea of the forecasting capability
    of the model, we need quantifiable metrics on how good the model is. These metrics
    will also allow us to compare forecasting accuracy with other models.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several metrics available for time series forecasting. We will show
    the use of the following three in this chapter, to highlight how different metrics
    serve different objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Squared Error** (**MSE**) measures the average squared differences between
    the forecasted (F) and actual (A) values. It works well when we want to penalize
    large errors. However, it is sensitive to outliers because the squaring of errors
    gives importance to large discrepancies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>M</mi><mi>S</mi><mi>E</mi><mo
    mathvariant="italic">=</mo><mfrac><mn mathvariant="italic">1</mn><mi>n</mi></mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>t</mi><mo
    mathvariant="italic">=</mo><mn mathvariant="italic">1</mn></mrow><mi>n</mi></msubsup><msup><mrow><mo>(</mo><msub><mi>F</mi><mi>t</mi></msub><mo
    mathvariant="italic">−</mo><msub><mi>A</mi><mi>t</mi></msub><mo>)</mo></mrow><mn
    mathvariant="italic">2</mn></msup></mrow></mrow></mrow></math>](img/2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Symmetric Mean Absolute Percentage Error** (**SMAPE**) is the average of
    the absolute differences between forecasted (F) and actual (A) values. It is expressed
    as a percentage over half of the sum of absolute values of actual and forecasted
    values. SMAPE adjusts to the scale of the data, making it suitable for comparisons
    across different datasets. Due to its symmetric scaling, it is less sensitive
    to extreme values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>S</mi><mi>M</mi><mi>A</mi><mi>P</mi><mi>E</mi><mo
    mathvariant="italic">=</mo><mfrac><mrow><mn mathvariant="italic">100</mn><mi>%</mi></mrow><mi>n</mi></mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>t</mi><mo
    mathvariant="italic">=</mo><mn mathvariant="italic">1</mn></mrow><mi>n</mi></msubsup><mfrac><mrow><mo>|</mo><msub><mi>F</mi><mi>t</mi></msub><mo
    mathvariant="italic">−</mo><msub><mi>A</mi><mi>t</mi></msub><mo>|</mo></mrow><mrow><mo>(</mo><mo>|</mo><msub><mi>A</mi><mi>t</mi></msub><mo>|</mo><mo
    mathvariant="italic">+</mo><mo>|</mo><msub><mi>F</mi><mi>t</mi></msub><mo>|</mo><mo>)</mo><mo
    mathvariant="italic">/</mo><mn mathvariant="italic">2</mn></mrow></mfrac></mrow></mrow></mrow></math>](img/3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Weighted Absolute Percentage Error** (**WAPE**) is a normalized measure of
    error, weighing the absolute errors by the actual values. It works well when dealing
    with data of varying magnitudes but is sensitive to high-value errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>W</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mi
    mathvariant="normal">%</mml:mi></mml:math>](img/4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will see two different approaches to metrics calculation: metrics calculation
    functions included in the model library, and a separate dedicated metrics calculation
    library.'
  prefs: []
  type: TYPE_NORMAL
- en: Metric functions from the model library
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this approach, we want to use the functions for metrics calculations already
    included in the model library. We will use the `sklearn` and `pmdarima` libraries
    for the metric calculations to demonstrate this in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Separate metrics library
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this second approach for metrics calculation, we use the `SeqMetrics` library,
    as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This library also provides a nice visualization of all the metrics calculated,
    as in *Figures 7.7* and *7.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: SeqMetrics display of WAPE'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: SeqMetrics display of SMAPE'
  prefs: []
  type: TYPE_NORMAL
- en: After training and testing our first model, we can move on to the next model,
    which is a classical Machine Learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Classical Machine Learning model – LightGBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second model we will cover is **Light Gradient Boosting Machine** (**LightGBM**),
    which is a free open source gradient boosting model. It is based on the tree learning
    algorithm, designed to be efficient and distributed.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this section is in `ts-spark_ch7_1e_lgbm_comm.dbc`. We import the
    code into Databricks Community Edition, as per the approach explained in [*Chapter
    1*](B18568_01.xhtml#_idTextAnchor016).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code URL is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_lgbm_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_lgbm_comm.dbc)'
  prefs: []
  type: TYPE_NORMAL
- en: Development and tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For model development, we separated the last 48 hours of the dataset from the
    training data with the following code. This will be used for testing afterward.
    We use the rest for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `GridSearchCV` method to find the best parameters for the `LGBMRegressor`
    model. `TimeSeriesSplit` is used to split the training dataset for cross-validation,
    respecting the time series nature of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We find the following best parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the training dataset, this will be the best set of parameters to use
    with the LightGBM model to forecast this household’s energy consumption. We can
    then train the final model with these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: After the LightGBM model is trained, we will test the model forecasting next.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use the model to forecast the test dataset with the `predict` function.
    Note that in this case, we have not had the need to use iterative multi-step forecasting
    code. We have instead used the lag values as input features to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can then plot the forecast against the actual values in *Figures 7.8* and
    *7.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: LightGBM Forecast vs Actuals (training and testing)'
  prefs: []
  type: TYPE_NORMAL
- en: We zoom in on the testing period in *Figure 7**.9* for a visual comparison of
    the forecast and actuals.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: LightGBM Forecast vs Actuals (zoom on test data)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the forecast and actuals, we can then measure the SMAPE and WAPE,
    getting the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have trained and tested statistical and classical Machine Learning
    models, we can move on to a third type of model, which is a Deep Learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning model – NeuralProphet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The third model we will cover is NeuralProphet, which is a free open source
    Deep Learning model inspired by Prophet, which we used in previous chapters, and
    AR-Net. NeuralProphet is built on PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this section is in `ts-spark_ch7_1e_nprophet_comm.dbc`. We import
    the code into Databricks Community Edition, as per the approach explained in [*Chapter
    1*](B18568_01.xhtml#_idTextAnchor016).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code URL is as follows: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_nprophet_comm.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_nprophet_comm.dbc)'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that the notebook for this example requires Databricks compute DBR 13.3
    LTS ML.
  prefs: []
  type: TYPE_NORMAL
- en: Development
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We instantiate a `NeuralProphet` model, specifying with `n_lag` that we want
    to use the past 24 hours for the forecasting. We then train (the `fit` method)
    the model on the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With these two lines of code sufficient to train the model, we will next test
    the model forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before using the model to forecast the test dataset, we need to prepare the
    data for NeuralProphet, similar to how we did previously for Prophet. The required
    format is to have a `ds` column for the date/time and `y` for the forecasting
    target. We can then use the `predict` method. Note that in this case, we have
    not had the need to use iterative multi-step forecasting code. With the lag of
    24 specified as a parameter in the previous code section, NeuralProphet uses a
    sliding window of the past 24 values to forecast the next values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We plot the forecast against the actual values in *Figures 7.12* and *7.13*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: NeuralProphet Forecast vs Actuals (training and testing)'
  prefs: []
  type: TYPE_NORMAL
- en: We zoom in on the testing period in *Figure 7**.13* for a visual comparison
    of the forecast and actuals.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: NeuralProphet Forecast vs Actuals (zoom on test data)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the forecast and actuals, we can then measure the SMAPE and WAPE,
    getting the following values as a measurement of the accuracy of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We will use these metrics to compare the different models we have used in this
    chapter, in the later *Model* *comparison* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have trained and tested each type of model: a statistical, a classical
    Machine Learning, and a Deep Learning model. Other examples of commonly used models
    for time series are provided in the book’s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prophet: `ts-spark_ch7_1e_prophet_comm.dbc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LSTM: `ts-spark_ch7_1e_lstm_comm1-cpu.dbc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NBEATS and NHITS: `ts-spark_ch7_1e_nbeats-nhits_comm.dbc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We encourage you to explore these further.
  prefs: []
  type: TYPE_NORMAL
- en: Having a working model is great but is not sufficient. We also need to be able
    to explain the model we are working with. We will cover this next.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Explainability is a key requirement in many cases, such as for financial and
    regulated industries. We will look at how to do this now using a widely used method
    called **Shapley Additive Explanations** (**SHAP**) to explain how the different
    features of the dataset contributed to the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `TreeExplainer` function of the `shap` library on the final
    model from the *Classical Machine Learning model – LightGBM* section to compute
    the SHAP values, which will give us the impact of each feature on the model output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can then plot the feature importance in *Figure 7**.10*. As expected from
    the data exploration we did in the earlier section, lag 1 and lag 24 are the features
    contributing the most to the forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: SHAP – feature importance'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can go further in the analysis by focusing on a specific forecast with the
    following code, where we want to explain the forecasting for the first value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We can see in *Figure 7**.11* the relative contributions of the features, again
    with pre-dominance of lag 1 and 24, and to a lesser extent lag 12\. This is coherent
    with our analysis in the *Data exploration* section, where we established the
    pertinence of these lags in forecasting the energy consumption of a household.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: SHAP – feature importance (first observation)'
  prefs: []
  type: TYPE_NORMAL
- en: Model comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before concluding this chapter, we will compare all the models we have tested
    based on the metrics we measured and the code execution time. The results are
    shown in *Table 7.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Type** | **SMAPE** | **WAPE** | **Training** | **Tuning** |
    **Testing** | **Total incl. data** **prep.** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| NeuralProphet | DL/Mixed | 41.19 | 0.35 | 60s | - | 1s | 90s |'
  prefs: []
  type: TYPE_TB
- en: '| LightGBM | Classical ML | 41.46 | 0.39 | 60s | Included | Included | 137s
    |'
  prefs: []
  type: TYPE_TB
- en: '| SARIMA | Statistical | 43.78 | 0.42 | Included | 420s | 180s | 662s |'
  prefs: []
  type: TYPE_TB
- en: '| Prophet | Statistical/Mixed | 47.60 | 0.41 | 2s | - | 1s | 70s |'
  prefs: []
  type: TYPE_TB
- en: '| NHITS | DL | 54.43 | 0.47 | 35s | - | Included | 433s |'
  prefs: []
  type: TYPE_TB
- en: '| NBEATS | DL | 54.91 | 0.48 | 35s | - | Included | 433s |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM | DL | 55.08 | 0.48 | 722s | - | 4s | 794s |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.1: Model results comparison'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few observations on the model accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: NeuralProphet and LightGBM gave the best forecasting accuracy with both the
    SMAPE and WAPE metrics. SARIMA was not very far behind.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Deep Learning models, NBEATS, NHITS, and LSTM, did not have good forecasting
    accuracy when used as single-input models. We encourage you to explore further
    how they can be improved with multiple inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is in regard to execution time:'
  prefs: []
  type: TYPE_NORMAL
- en: In all the cases, we kept within the constraint of a total execution of 900s
    (15 minutes) with the 2 CPU cores on a single-node Databricks Community Edition
    cluster. This worked with the 25 MB dataset. We will see in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151)
    how to scale for larger datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prophet, NBEATS, and NHITS had the best execution time, with NeuralProphet and
    LightGBM coming after, still within 1 minute for training, tuning, and testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SARIMA had a relatively high execution time, even if we limited the dataset
    to the last 300 observations. This was due to the Auto ARIMA algorithm searching
    for the best hyperparameter, and then the multi-step iterative forecasting code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM had the longest execution time, which can be explained by the use of CPUs
    instead of GPUs, which are much faster for Deep Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall conclusion from this model comparison is that NeuralProphet and
    LightGBM are the best choices for the dataset we used, with minimal tuning, and
    for the compute and execution time constraint that we set.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have focused on the core topic of this book, which is the
    development of models for time series analysis, more specifically for forecasting.
    Starting with a review of the different types of models, we then looked at the
    important criteria guiding the choice of the right model to use. In the second
    part of the chapter, we put into practice the development and testing of several
    models, which we then compared on accuracy and execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will expand on a topic where Apache Spark shines: scaling
    time series analysis to big data.'
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/ds](https://packt.link/ds)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ds_(1).jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Part 3: Scaling to Production and Beyond'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last part, we will cover the considerations and practical examples of
    scaling and bringing to production the solutions covered in *Part 2*. We then
    conclude the book with techniques to go further with Apache Spark and time series
    analysis. This guides you to using Databricks and generative AI as part of your
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18568_08.xhtml#_idTextAnchor151), *Going at Scale*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18568_09.xhtml#_idTextAnchor169), *Going to Production*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18568_10.xhtml#_idTextAnchor190), *Going Further with Apache
    Spark*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B18568_11.xhtml#_idTextAnchor211), *Recent Developments in Time
    Series Analysis*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
