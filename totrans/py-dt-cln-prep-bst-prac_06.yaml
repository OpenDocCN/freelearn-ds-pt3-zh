- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Grouping, Aggregation, Filtering, and Applying Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data grouping** and **aggregation** are fundamental techniques in data cleaning
    and preprocessing, serving several critical purposes. Firstly, they enable the
    summarization of large datasets, transforming extensive raw data into concise,
    meaningful summaries that facilitate analysis and insight derivation. Additionally,
    aggregation helps manage missing or noisy data by smoothing out inconsistencies
    and filling gaps with combined data points. These techniques also contribute to
    reducing data volume, enhancing processing efficiency, and creating valuable features
    for further analysis or machine learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: The main components of data grouping and aggregation include group keys, which
    define how data is segmented; aggregation functions, which perform operations
    such as summing, averaging, counting, and more; and output columns, which display
    the group keys and aggregated values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following main points:'
  prefs: []
  type: TYPE_NORMAL
- en: Grouping data using one or multiple keys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying aggregate functions on grouped data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying functions on grouped data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the code for the chapter in the following GitHub repository: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter06](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter06).'
  prefs: []
  type: TYPE_NORMAL
- en: Grouping data using one or multiple keys
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In pandas, grouping data is a fundamental operation that involves splitting
    data into groups based on one or more keys and then performing operations within
    each group. Grouping is often used in data analysis to gain insights and perform
    aggregate calculations on subsets of data. Let’s dive deeper into grouping data
    and provide examples to illustrate their usage. The code for this section can
    be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/2.groupby_full_example.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/2.groupby_full_example.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Grouping data using one key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Grouping data with pandas using one key is a common operation for data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To group data using one key, we use the `groupby()` method of a DataFrame and
    specify the column that we want to use as the key for grouping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After grouping, you typically want to perform some aggregation. Common aggregation
    functions include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`grouped.sum()`: This calculates the sum of all numeric columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`grouped.mean()`: This calculates the average (arithmetic mean)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`grouped.count()`: This counts the number of non-null values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`grouped.agg([''sum'', ''mean'', ''count''])`: This applies multiple aggregation
    functions at once: `sum`, `mean`, and `count`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s present a common use case on which to apply our learnings. Let’s pretend
    we are working for an electronics retail company and we need to analyze the sales
    data for the different products. A sample of the data is presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In data analysis, certain columns are typically candidates for grouping due
    to their *categorical* nature. These columns often represent categories, classifications,
    or time-related segments that make sense to aggregate data around:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Category columns**: Columns that represent distinct groups or types within
    the data. Examples include product categories, user types, or service types. These
    columns help in understanding the performance or behavior of each group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Geographical columns**: Columns that denote geographical divisions, such
    as country, region, city, or store location. These are useful for regional performance
    analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temporal columns**: Columns representing time-related information, such as
    year, quarter, month, week, or day. Grouping by these columns helps in trend analysis
    over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Demographic columns**: Columns that describe demographic attributes, such
    as age group, gender, or income level. These are useful for segmenting data based
    on population characteristics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transaction-related columns**: Columns related to the nature of transactions,
    such as transaction type, payment method, or order status. These help in understanding
    different aspects of transactional data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given the data we have in our example, the candidate columns for grouping are
    `Category`, `Subcategory`, and `Region`. `Date` could also be a candidate if we
    had multiple records per day and we wanted to calculate the number of sales per
    day. In our case, our manager asked us to report on the total number of sales
    (sales volume) for each category. Let’s see how to calculate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code example, we group the data by the `Category` column, sum the `Sales`
    column for each category, and reset the index. The resulting DataFrame is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have seen how to group data by a single key, let’s add more complexity
    by grouping the data by `Category` and `Region`.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping data using multiple keys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Grouping by multiple keys allows for a more granular and detailed examination
    of the data. This approach helps uncover insights that may be hidden when only
    using a single key, offering a deeper understanding of relationships and patterns
    within the dataset. In our example, grouping by both `Region` and `Category` allows
    the company to see not only the overall sales performance but also how different
    categories perform in each region. This helps in identifying which products are
    popular in specific regions and tailoring marketing strategies accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To group data using multiple keys, we pass a list of column names to the `groupby()`
    method. Pandas will create groups based on *unique combinations* of values from
    these columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this piece of code, we group the data by both the `Category` and `Region`
    columns, and then we perform the aggregation by summing the `Sales` column for
    each group. Finally, we reset the index. Let’s see the output from this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With just a line of code, we have managed to summarize and present all the sales
    for each `Category` and `Region` value, making our manager very happy. Now, let’s
    have a look at some best practices when working with groupby statements.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for grouping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When grouping data in pandas, there are several things to consider to ensure
    accurate results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing data**: Be aware of missing data in the columns used for grouping.
    Pandas will *exclude* rows with missing data from the grouped result, which can
    affect the final calculations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultiIndex`: When grouping by multiple columns, pandas returns a hierarchical
    index (`MultiIndex`). Be familiar when working with `MultiIndex` and consider
    resetting the index if needed as we have been doing for simplicity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Order of operations**: The order in which you perform groupings and aggregations
    *can affect the results*. Be mindful of the sequence in which you apply grouping
    and aggregation functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grouping large datasets**: For large datasets, grouping can be memory intensive.
    Consider using techniques such as chunking or parallel processing to manage memory
    usage and computation time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our managing team saw the efficiency of the groupby operations we performed,
    and they asked us for a more detailed summary of sales! With multiple keys in
    place, we can further enhance our analysis by applying multiple aggregation functions
    to the `Sales` column. This will give us a more detailed summary of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Applying aggregate functions on grouped data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In pandas, after grouping data using the `groupby()` method, you can apply aggregate
    functions to perform calculations on the grouped data. **Aggregate functions**
    are used to summarize or compute statistics for each group, resulting in a new
    DataFrame or Series. Let’s dive deeper into applying aggregate functions on grouped
    data and provide examples to illustrate their usage.
  prefs: []
  type: TYPE_NORMAL
- en: Basic aggregate functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have touched base on the basic aggregation function in the first section
    as you cannot perform groupby without an aggregation function. In this section,
    we will expand a bit more on what each function does and when should we use each
    one, starting by presenting all the available functions in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Aggregation** **function** | **Description** | **When** **to use** | **Code
    example** |'
  prefs: []
  type: TYPE_TB
- en: '| `sum` | Adds up all values in a group | When you need the total value for
    each group.**Example**: Total sales per category. | `df.groupby(''Category'')[''Sales''].sum()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `mean` | Calculates the average of values in a group | When you need the
    average value for each group.**Example**: Average sales per region. | `df.groupby(''Category'')[''Sales''].mean()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `count` | Counts the number of non-null values in a group | When you need
    to know the number of occurrences in each group.**Example**: Number of sales transactions
    per sub-category. | `df.groupby(''Category'')[''Sales''].count()` |'
  prefs: []
  type: TYPE_TB
- en: '| `min` | Finds the minimum value in a group | When you need the smallest value
    in each group.**Example**: Minimum sales value per region. | `df.groupby(''Category'')[''Sales''].min()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Aggregation** **function** | **Description** | **When** **to use** | **Code
    example** |'
  prefs: []
  type: TYPE_TB
- en: '| `max` | Finds the maximum value in a group | When you need the largest value
    in each group.**Example**: Maximum sales value per category. | `df.groupby(''Category'')[''Sales''].max()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `median` | Finds the median value in a group | When you need the middle value
    in a sorted list of numbers. **Example**: Median sales value per category. | `df.groupby(''Category'')[''Sales''].median()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `std`(Standard Deviation) | Measures the spread of values in a group | When
    you need to understand the variation in values.**Example**: Standard deviation
    of sales per region. | `df.groupby(''Category'')[''Sales''].std()` |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Summary table of the basic aggregation functions
  prefs: []
  type: TYPE_NORMAL
- en: 'You can call each of these functions one by one or all together, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This calculates the number of sales per category, as we’ve learned, and it’s
    sufficient if this is the only aggregate information you want to extract from
    the dataset. However, if you find yourself being asked to produce multiple sale
    aggregates for the different product categories, a more efficient way is to perform
    all the aggregates at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we apply multiple aggregation functions (`sum` and `mean`) to
    the `Sales` column. The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can add as many aggregations as we want in the group by clause.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve been really efficient with calculating all the different metrics our managing
    team asked for, and as a result, they are now keen to understand the sales metrics
    and the number of unique sub-category sales per region and category. Let’s do
    that next.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced aggregation with multiple columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand the sales metrics and the number of unique sub-category sales
    per region and category, we can group additional columns and apply multiple aggregations
    to both the `Sales` and `Subcategory` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we group the DataFrame by `Category` and `Region` and we perform
    a couple of aggregations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''Sales'': [''sum'', ''mean'', ''count'']` calculates the total sales, average
    sales, and number of transactions (count of rows) for each group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''Sub-Category'': ''nunique''` calculates the number of unique sub-categories
    within each group of `Category` and `Region`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The summarized results are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you may be wondering, what have we learned by making these calculations?
    Let me answer that! Total sales, average sales, and transaction count were calculated
    to understand the financial performance across different category-region combinations.
    Additionally, the `Sub-Category` unique count revealed crucial aspects of our
    product distribution strategy. This analysis serves multiple purposes: it provides
    insights into the diversity of products within each category-region segment, for
    example, in the context of our data, knowing the number of unique products (sub-categories)
    sold in each region under different categories provides insights into the market
    segmentation and product assortment strategies. It also aids in assessing market
    penetration by highlighting regions with a broader product offering and supports
    strategic decisions in product portfolio management, including expansion and inventory
    strategies tailored to regional preferences.'
  prefs: []
  type: TYPE_NORMAL
- en: Standard aggregation functions, such as sum, mean, and count, provide fundamental
    statistics. However, custom functions allow you to calculate metrics that are
    specific to your business needs or analysis goals. For example, calculating the
    range or coefficient of variation in sales data can reveal insights into the distribution
    and variability of sales within different groups. As you can imagine, we were
    asked to implement these custom metrics, which we’ll do next.
  prefs: []
  type: TYPE_NORMAL
- en: Applying custom aggregate functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Custom functions are valuable when the aggregation requires complex calculations
    that go beyond simple statistics. You can use them when you need to calculate
    metrics that are unique to your analysis objectives or business context. For example,
    in sales analysis, you might want to compute profit margins, customer lifetime
    value, or churn rates, which are not typically available through standard aggregation
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to our example and build the metrics we were asked about: For
    each region, we want to calculate the sales range and the variability of sales.
    Let’s have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a function that calculates the range (difference between max and
    min) of sales:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create a function that computes the coefficient of variation of sales,
    which measures the relative variability in relation to the mean:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `df` DataFrame is then grouped by `Region`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Sales: [''sum'', ''mean'', ''count'', range_sales, coefficient_of_variation]`
    calculates the total sales, average sales, transaction count, sales range, and
    coefficient of variation using the custom functions. `''Sub-Category'':''nunique''`
    counts the number of unique sub-categories within each group. Then, we reset the
    index to flatten the `df` DataFrame and make it easier to work with.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we rename the aggregated columns for clarity and better readability
    of the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s print the final DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final DataFrame is presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Let’s spend some time understanding the sales variability for each region. The
    range of sales within each region can reveal the **spread** or **difference**
    between the highest and lowest sales figures. For instance, a wide range may indicate
    significant variability in consumer demand or sales performance across different
    regions. The coefficient of variation helps to standardize the variability of
    sales relative to their average. A higher coefficient suggests greater relative
    variability, which may prompt further investigation into factors influencing sales
    fluctuations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: I hope it’s clear to you that you can build any function that you want as a
    custom aggregate function as long as it computes a *single* aggregation result
    from an input series of values. The function should also return a single scalar
    value, which is the result of the aggregation for that group.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s have a look at some best practices when working with aggregate functions.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for aggregate functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When working with aggregate functions in pandas, there are several things to
    consider to ensure accurate results:'
  prefs: []
  type: TYPE_NORMAL
- en: Write efficient custom functions that minimize computational overhead, especially
    when working with large datasets. Avoid unnecessary loops or operations that can
    slow down processing time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly document the logic and purpose of your custom aggregation functions.
    This helps in maintaining and sharing code within your team or organization, ensuring
    transparency and reproducibility of analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate the accuracy of custom aggregation functions by comparing results with
    known benchmarks or manual calculations. This step is crucial to ensure that your
    custom metrics are reliable and correctly implemented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In pandas, when using the `.agg()` method with `groupby`, the aggregation function
    you define should *ideally* return a single scalar value for each column it operates
    on within each group. However, there are scenarios where you might want to return
    multiple values or perform more complex operations. While the pandas `.agg()`
    method expects scalar values, you can achieve more complex aggregations by using
    custom functions that return tuples or lists. However, this requires careful handling
    and often isn’t straightforward within pandas’ native aggregation framework. For
    more complex scenarios where you need to return multiple values or perform intricate
    calculations, we can use `apply()` instead of `agg()`, which is more flexible,
    as we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using the apply function on grouped data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `apply()` function in Pandas is a powerful method used to apply a custom
    function along an axis of a DataFrame or Series. It is highly versatile and can
    be used in various scenarios to manipulate data, compute complex aggregations,
    or transform data based on custom logic. The `apply()` function can be used to
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply functions row-wise or column-wise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply functions to groups of data when used in conjunction with `groupby()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will focus on using the `apply` function on groups of
    data by first grouping on the column we want and then performing the `apply` operation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Using the `apply` function without `groupby` allows you to apply a function
    across either rows or columns of a DataFrame directly. This is useful when you
    need to perform row-wise or column-wise operations that don’t require grouping
    the data. Apply the same learnings and just skip the group by clause.
  prefs: []
  type: TYPE_NORMAL
- en: When using the `apply` function in pandas, `axis=0` (default) applies the function
    to each column, while `axis=1` applies it to each row. Let’s go a little bit deeper
    on this.
  prefs: []
  type: TYPE_NORMAL
- en: '`axis=0` applies the function along the *rows*. In other words, it processes
    each *column* independently. This is typically used when you want to aggregate
    data column-wise (e.g., summing up values in each column), as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Apply() with axis=0](img/B19801_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Apply() with axis=0
  prefs: []
  type: TYPE_NORMAL
- en: 'If we go back to our use case, the managing team wants to understand more about
    the actual quantity sold for the products per category apart from the sum of dollars
    in sales we achieved. Our example gets more and more complex; so, it’s a good
    idea to implement that with `apply()`. Let’s see this with a code example that
    can also be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/3.apply_axis0.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/3.apply_axis0.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s extend our DataFrame to add the `Quantity` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then convert the `Date` column to datetime format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s define a custom function to compute multiple statistics for `Sales`
    and `Quantity`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The custom function (`compute_statistics`) now computes multiple statistics
    (sum, mean, std, CV) for both the `Sales` and `Quantity` columns within each group
    defined by `Category`. For each category group (series), it calculates the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Sum_Sales`: The sum of sales'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Mean_Sales`: The mean of sales'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Std_Sales`: The standard deviation of sales'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CV_Sales`: The `Sum_Quantity`: Sum of quantities'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Mean_Quantity`: Mean of quantities'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Std_Quantity`: Standard deviation of quantities'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CV_Quantity`: The CV of quantities'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, it returns a pandas Series with these computed statistics, indexed
    appropriately.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we’ll perform a groupby operation on `Category` and apply our custom
    function to compute statistics of `Sales` and `Quantity`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use `apply()` in conjunction with `groupby(''Category'')` to apply the `compute_statistics`
    function to each group of sales data defined by the `Category` column. The function
    operates on the entire group (series), allowing the computation of statistics
    for both the `Sales` and `Quantity` columns simultaneously. Finally, `reset_index()`
    is used to flatten the resulting DataFrame, providing a structured output with
    category-wise statistics for both columns. Let’s have a look at the final DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B19801_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By grouping the data by `Category`, we can analyze sales and quantity metrics
    at the category level, which helps us understand how different types of products
    (electronics, furniture, clothing) perform in terms of sales and quantity. As
    we can see from the presented results, `Furniture` is the key income generator
    as it has the highest `Sum_Sales` and `Mean_Sales`, indicating a category with
    popular or high-value products. Categories with lower `CV_Sales` and `CV_Quantity`
    values, such as `Clothing`, are more consistent in sales and quantity, suggesting
    stable demand or predictable sales patterns whereas categories with higher variability
    (`Std_Sales` and `Std_Quantity`) may indicate products with fluctuating sales
    or seasonal demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is great in terms of data analysis, but now, we get asked to come up with
    some strategic decisions related to product assortment, pricing strategies, and
    marketing initiatives. Let’s be a little bit more creative at this point:'
  prefs: []
  type: TYPE_NORMAL
- en: Categories with high `Sum_Sales` values and stable metrics (`CV_Sales`, `CV_Quantity`)
    are prime candidates for expanding product lines or investing in marketing efforts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categories with high variability (`Std_Sales`, `Std_Quantity`) may require dynamic
    pricing strategies or seasonal promotions to optimize sales
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use the `Mean_Sales` and `Mean_Quantity` values to identify categories
    with potential for growth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using the `apply()` function in pandas without specifying the axis parameter,
    the default behavior is `axis=0`. This means that the function will be applied
    to each column (i.e., it will process each column independently). This is what
    we have applied in the example code provided earlier. Depending on your specific
    use case, adjust `apply()` to operate row-wise (`axis=1`) or column-wise (`axis=0`)
    as needed. Next, let’s focus on `axis=1`.
  prefs: []
  type: TYPE_NORMAL
- en: '`axis=1` applies the function along the columns, so it processes each row independently.
    This is typically used when you want to perform row-wise operations (e.g., calculating
    a custom metric for each row).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Apply() with axis=1](img/B19801_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Apply() with axis=1
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying functions row-wise allows for row-level transformations and calculations.
    Let’s see a code example with `axis=1`. Let’s start by defining a function to
    be applied across columns (`axis=1`). The code can be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/4.apply_axis1.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/4.apply_axis1.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `row_summary` function takes a single row of a DataFrame as input and returns
    a summary of that row’s data. The input for the function is key to understanding
    that is a single row of the DataFrame, passed as a pandas Series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `total_sales_quantity` variable will store the sum of `Sales` and `Quantity`
    for the row. The `sales_quantity_ratio` variable will store the ratio of `Sales`
    to `Quantity` for the row, or `np.nan` if the quantity is zero, providing insight
    into the sales efficiency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We apply the function row-wise (`axis=1`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will produce a new `df_row_summary` DataFrame where each row corresponds
    to the calculated values for `total_sales_quantity` and `sales_quantity_ratio`
    for the original rows in `df`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we group by `Category` to calculate metrics per category:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s see the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `total_sales_quantity` metric provides a simple yet effective measure of
    overall sales performance per transaction, helping to understand the combined
    impact of the number of items sold (`Quantity`) and their sales value (`Sales`).
    By analyzing `total_sales_quantity`, we can identify transactions with high combined
    sales and quantity, which might indicate popular product categories or successful
    sales strategies. Conversely, it also helps recognize low-performing transactions,
    thereby guiding inventory management and promotional adjustments to improve sales
    efficiency and product performance. This dual insight aids in strategic decision-making
    to optimize sales and inventory management.
  prefs: []
  type: TYPE_NORMAL
- en: The `sales_quantity_ratio` metric provides valuable insights into the efficiency
    of sales per unit quantity, revealing how effectively products convert quantity
    into revenue. This metric is crucial for assessing the value derived from each
    unit sold. With this, we can identify products that generate high revenue per
    unit, indicating high-value items that may warrant prioritized marketing efforts.
    Conversely, it helps uncover products with a low revenue per unit, signaling potential
    areas for price adjustments, targeted promotions, or re-evaluation within the
    product portfolio to optimize profitability and sales performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Whenever feasible, prefer using vectorized operations (built-in pandas methods
    or NumPy functions) over `apply` for performance reasons. Vectorized operations
    are generally faster because they leverage optimized C code under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: The concepts and techniques we have explored so far directly lead to the importance
    of filtering in data cleaning. Once we have applied transformations or aggregated
    data, filtering allows us to focus on specific subsets of data that are relevant
    to our analysis or meet certain conditions. For example, after calculating sales
    performance metrics, such as `Total_Sales_Quantity` and `Sales_Quantity_Ratio`,
    across different product categories, filtering can help identify categories or
    products that require further investigation, such as those with unusually high
    or low performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Data filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data filtering** is a fundamental operation in data manipulation that involves
    selecting a subset of data based on specified conditions or criteria. It is used
    to extract relevant information from a larger dataset, exclude unwanted data points,
    or focus on specific segments that are of interest for analysis or reporting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we filter the DataFrame to only include rows where
    the `Quantity` column is greater than `10`. This operation selects products that
    have sold more than 10 units, focusing our analysis on potentially high-performing
    products. [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/5.simple_filtering.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/5.simple_filtering.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s have a look at the resulting DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Moving beyond simple filters allows us to identify electronic products that
    satisfy more complex conditions, as we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple criteria for filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Filtering may involve complex conditions, such as combining logical `AND` and
    `OR` operations, or using nested conditions. Let’s say that the management team
    wants us to identify high-value electronics products (`sales > 1000`) with relatively
    low sales quantities (`quantity <` `30`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can do this with multiple filtering criteria (the code can
    be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/6.advanced_filtering.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter06/6.advanced_filtering.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we define a filter condition that filters rows where the sales
    are greater than `1000` and the quantity is less than `30`. Let’s have a look
    at the resulting DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Filtering is a straightforward operation to implement, but let’s explore some
    best practices to optimize its effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s explore the best practices that can enhance the effectiveness of filtering
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly define the filtering criteria based on the analysis goals. Use conditions
    that are specific and relevant to the insights you want to derive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize built-in filter functions provided by data manipulation libraries, such
    as pandas in Python or SQL queries in databases. These functions are optimized
    for performance and ease of use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that the filtering criteria do not exclude important data points that
    might be valuable for analysis. Validate the results to confirm they align with
    the expected outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document the filtering criteria and steps applied to maintain transparency and
    facilitate reproducibility of the analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As datasets grow, filtering becomes essential to manage and extract insights
    efficiently. Operations that involve processing large volumes of data can become
    prohibitively slow without effective filtering strategies. Filtering helps in
    optimizing resource utilization, such as memory and processing power, by reducing
    the amount of data that needs to be stored and processed at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: Performance considerations as data grows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s have a look at things to keep in mind as data grows:'
  prefs: []
  type: TYPE_NORMAL
- en: Filtering operations optimize query execution by reducing the number of rows
    or columns that need to be processed, leading to faster response times for data
    queries and analyses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large datasets consume significant memory and storage resources. Filtering reduces
    the amount of data held in memory or stored on disk, improving efficiency, and
    reducing operational costs associated with data storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now summarize the learnings from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve explored some powerful techniques, such as grouping,
    aggregation, and applying custom functions. These methods are essential for summarizing
    and transforming data, enabling deeper insights into datasets. We’ve learned how
    to efficiently group data by categorical variables, such as `Category` and `Region`,
    and apply aggregate functions, such as sum, mean, and custom metrics to derive
    meaningful summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we deep-dived into the versatility of `apply` functions, which
    allow for row-wise or column-wise custom computations. Best practices, such as
    optimizing function efficiency, handling missing values, and understanding performance
    implications, were emphasized to ensure effective data processing. Finally, we
    discussed the strategic application of filters to refine datasets based on specific
    criteria, enhancing data analysis precision.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss designing and optimizing data write operations
    to efficiently store the transformed and cleaned data.
  prefs: []
  type: TYPE_NORMAL
