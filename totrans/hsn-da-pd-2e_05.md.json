["```py\n>>> import matplotlib.pyplot as plt\n>>> import pandas as pd\n>>> wide_df = \\\n...     pd.read_csv('data/wide_data.csv', parse_dates=['date'])\n>>> long_df = pd.read_csv(\n...     'data/long_data.csv', \n...     usecols=['date', 'datatype', 'value'], \n...     parse_dates=['date']\n... )[['date', 'datatype', 'value']] # sort columns\n```", "```py\n>>> wide_df.head(6)\n```", "```py\n>>> wide_df.describe(include='all', datetime_is_numeric=True)\n```", "```py\n>>> wide_df.plot(\n...     x='date', y=['TMAX', 'TMIN', 'TOBS'], figsize=(15, 5),\n...     title='Temperature in NYC in October 2018' \n... ).set_ylabel('Temperature in Celsius')\n>>> plt.show()\n```", "```py\n>>> long_df.head(6)\n```", "```py\n>>> long_df.describe(include='all', datetime_is_numeric=True)\n```", "```py\n>>> import seaborn as sns\n>>> sns.set(rc={'figure.figsize': (15, 5)}, style='white')\n>>> ax = sns.lineplot(\n...     data=long_df, x='date', y='value', hue='datatype'\n... )\n>>> ax.set_ylabel('Temperature in Celsius')\n>>> ax.set_title('Temperature in NYC in October 2018')\n>>> plt.show()\n```", "```py\n>>> sns.set(\n...     rc={'figure.figsize': (20, 10)},\n...     style='white', font_scale=2 \n... )\n>>> g = sns.FacetGrid(long_df, col='datatype', height=10)\n>>> g = g.map(plt.plot, 'date', 'value')\n>>> g.set_titles(size=25)\n>>> g.set_xticklabels(rotation=45)\n>>> plt.show()\n```", "```py\n>>> import requests\n>>> def make_request(endpoint, payload=None):\n...     \"\"\"\n...     Make a request to a specific endpoint on the \n...     weather API passing headers and optional payload.\n...     Parameters:\n...         - endpoint: The endpoint of the API you want to \n...                     make a GET request to.\n...         - payload: A dictionary of data to pass along \n...                    with the request.\n...     \n...     Returns:\n...         A response object.\n...     \"\"\"\n...     return requests.get(\n...         'https://www.ncdc.noaa.gov/cdo-web/'\n...         f'api/v2/{endpoint}',\n...         headers={'token': 'PASTE_YOUR_TOKEN_HERE'},\n...         params=payload\n...     )\n```", "```py\n>>> response = \\\n...     make_request('datasets', {'startdate': '2018-10-01'})\n```", "```py\n>>> response.status_code\n200\n>>> response.ok\nTrue\n```", "```py\n>>> payload = response.json()\n>>> payload.keys()\ndict_keys(['metadata', 'results'])\n```", "```py\n>>> payload['metadata']\n{'resultset': {'offset': 1, 'count': 11, 'limit': 25}}\n```", "```py\n>>> payload['results'][0].keys()\ndict_keys(['uid', 'mindate', 'maxdate', 'name', \n           'datacoverage', 'id'])\n```", "```py\n>>> [(data['id'], data['name']) for data in payload['results']]\n[('GHCND', 'Daily Summaries'),\n ('GSOM', 'Global Summary of the Month'),\n ('GSOY', 'Global Summary of the Year'),\n ('NEXRAD2', 'Weather Radar (Level II)'),\n ('NEXRAD3', 'Weather Radar (Level III)'),\n ('NORMAL_ANN', 'Normals Annual/Seasonal'),\n ('NORMAL_DLY', 'Normals Daily'),\n ('NORMAL_HLY', 'Normals Hourly'),\n ('NORMAL_MLY', 'Normals Monthly'),\n ('PRECIP_15', 'Precipitation 15 Minute'),\n ('PRECIP_HLY', 'Precipitation Hourly')]\n```", "```py\n>>> response = make_request(\n...     'datacategories', payload={'datasetid': 'GHCND'}\n... )\n>>> response.status_code\n200\n>>> response.json()['results']\n[{'name': 'Evaporation', 'id': 'EVAP'},\n {'name': 'Land', 'id': 'LAND'},\n {'name': 'Precipitation', 'id': 'PRCP'},\n {'name': 'Sky cover & clouds', 'id': 'SKY'},\n {'name': 'Sunshine', 'id': 'SUN'},\n {'name': 'Air Temperature', 'id': 'TEMP'},\n {'name': 'Water', 'id': 'WATER'},\n {'name': 'Wind', 'id': 'WIND'},\n {'name': 'Weather Type', 'id': 'WXTYPE'}]\n```", "```py\n>>> response = make_request(\n...     'datatypes', \n...     payload={'datacategoryid': 'TEMP', 'limit': 100}\n... )\n>>> response.status_code\n200\n>>> [(datatype['id'], datatype['name'])\n...  for datatype in response.json()['results']]\n[('CDSD', 'Cooling Degree Days Season to Date'),\n ...,\n ('TAVG', 'Average Temperature.'),\n ('TMAX', 'Maximum temperature'),\n ('TMIN', 'Minimum temperature'),\n ('TOBS', 'Temperature at the time of observation')]\n```", "```py\n>>> response = make_request(\n...     'locationcategories', payload={'datasetid': 'GHCND'}\n... )\n>>> response.status_code\n200\n```", "```py\n>>> import pprint\n>>> pprint.pprint(response.json())\n{'metadata': {\n     'resultset': {'count': 12, 'limit': 25, 'offset': 1}},\n 'results': [{'id': 'CITY', 'name': 'City'},\n             {'id': 'CLIM_DIV', 'name': 'Climate Division'},\n             {'id': 'CLIM_REG', 'name': 'Climate Region'},\n             {'id': 'CNTRY', 'name': 'Country'},\n             {'id': 'CNTY', 'name': 'County'},\n             ...,\n             {'id': 'ST', 'name': 'State'},\n             {'id': 'US_TERR', 'name': 'US Territory'},\n             {'id': 'ZIP', 'name': 'Zip Code'}]}\n```", "```py\n>>> def get_item(name, what, endpoint, start=1, end=None):\n...     \"\"\"\n...     Grab the JSON payload using binary search.\n... \n...     Parameters:\n...         - name: The item to look for.\n...         - what: Dictionary specifying what item `name` is.\n...         - endpoint: Where to look for the item.\n...         - start: The position to start at. We don't need\n...           to touch this, but the function will manipulate\n...           this with recursion.\n...         - end: The last position of the items. Used to \n...           find the midpoint, but like `start` this is not \n...           something we need to worry about.\n... \n...     Returns: Dictionary of the information for the item \n...              if found, otherwise an empty dictionary.\n...     \"\"\"\n...     # find the midpoint to cut the data in half each time \n...     mid = (start + (end or 1)) // 2\n...     \n...     # lowercase the name so this is not case-sensitive\n...     name = name.lower()\n...     # define the payload we will send with each request\n...     payload = {\n...         'datasetid': 'GHCND', 'sortfield': 'name',\n...         'offset': mid, # we'll change the offset each time\n...         'limit': 1 # we only want one value back\n...     }\n...     \n...     # make request adding additional filters from `what`\n...     response = make_request(endpoint, {**payload, **what})\n...     \n...     if response.ok:\n...         payload = response.json()\n...     \n...         # if ok, grab the end index from the response \n...         # metadata the first time through\n...         end = end or \\\n...             payload['metadata']['resultset']['count']\n...         \n...         # grab the lowercase version of the current name\n...         current_name = \\\n...             payload['results'][0]['name'].lower()  \n...\n...         # if what we are searching for is in the current \n...         # name, we have found our item\n...         if name in current_name:\n...             # return the found item\n...             return payload['results'][0] \n...         else:\n...             if start >= end: \n...                 # if start index is greater than or equal\n...                 # to end index, we couldn't find it\n...                 return {}\n...             elif name < current_name:\n...                 # name comes before the current name in the \n...                 # alphabet => search further to the left\n...                 return get_item(name, what, endpoint, \n...                                 start, mid - 1)\n...             elif name > current_name:\n...                 # name comes after the current name in the \n...                 # alphabet => search further to the right\n...                 return get_item(name, what, endpoint,\n...                                 mid + 1, end) \n...     else:\n...         # response wasn't ok, use code to determine why\n...         print('Response not OK, '\n...               f'status: {response.status_code}')\n```", "```py\n>>> nyc = get_item(\n...     'New York', {'locationcategoryid': 'CITY'}, 'locations'\n... )\n>>> nyc\n{'mindate': '1869-01-01',\n 'maxdate': '2021-01-14',\n 'name': 'New York, NY US',\n 'datacoverage': 1,\n 'id': 'CITY:US360019'}\n```", "```py\n>>> central_park = get_item(\n...     'NY City Central Park',\n...     {'locationid': nyc['id']}, 'stations'\n... )\n>>> central_park\n{'elevation': 42.7,\n 'mindate': '1869-01-01',\n 'maxdate': '2020-01-13',\n 'latitude': 40.77898,\n 'name': 'NY CITY CENTRAL PARK, NY US',\n 'datacoverage': 1,\n 'id': 'GHCND:USW00094728',\n 'elevationUnit': 'METERS',\n 'longitude': -73.96925}\n```", "```py\n>>> response = make_request(\n...     'data', \n...     {'datasetid': 'GHCND',\n...      'stationid': central_park['id'],\n...      'locationid': nyc['id'],\n...      'startdate': '2018-10-01',\n...      'enddate': '2018-10-31',\n...      'datatypeid': ['TAVG', 'TMAX', 'TMIN'],\n...      'units': 'metric',\n...      'limit': 1000}\n... )\n>>> response.status_code\n200\n```", "```py\n>>> import pandas as pd\n>>> df = pd.DataFrame(response.json()['results'])\n>>> df.head()\n```", "```py\n>>> df.datatype.unique()\narray(['TMAX', 'TMIN'], dtype=object)\n>>> if get_item(\n...     'NY City Central Park', \n...     {'locationid': nyc['id'], 'datatypeid': 'TAVG'}, \n...     'stations'\n... ):\n...     print('Found!')\nFound!\n```", "```py\n>>> import pandas as pd\n>>> df = pd.read_csv('data/nyc_temperatures.csv')\n>>> df.head()\n```", "```py\n>>> df.columns\nIndex(['date', 'datatype', 'station', 'attributes', 'value'],\n      dtype='object')\n```", "```py\n>>> df.rename(\n...     columns={'value': 'temp_C', 'attributes': 'flags'},\n...     inplace=True\n... )\n```", "```py\n>>> df.columns\nIndex(['date', 'datatype', 'station', 'flags', 'temp_C'], \n      dtype='object')\n```", "```py\n>>> df.rename(str.upper, axis='columns').columns\nIndex(['DATE', 'DATATYPE', 'STATION', 'FLAGS', 'TEMP_C'], \n      dtype='object')\n```", "```py\n>>> df.dtypes\ndate         object\ndatatype     object\nstation      object\nflags        object \ntemp_C      float64\ndtype: object\n```", "```py\n>>> df.loc[:,'date'] = pd.to_datetime(df.date)\n>>> df.dtypes \ndate        datetime64[ns] \ndatatype            object\nstation             object\nflags               object \ntemp_C             float64\ndtype: object\n```", "```py\n>>> df.date.describe(datetime_is_numeric=True)\ncount                     93\nmean     2018-10-16 00:00:00\nmin      2018-10-01 00:00:00\n25%      2018-10-08 00:00:00\n50%      2018-10-16 00:00:00\n75%      2018-10-24 00:00:00\nmax      2018-10-31 00:00:00\nName: date, dtype: object\n```", "```py\n>>> pd.date_range(start='2018-10-25', periods=2, freq='D')\\\n...     .tz_localize('EST')\nDatetimeIndex(['2018-10-25 00:00:00-05:00', \n               '2018-10-26 00:00:00-05:00'], \n              dtype='datetime64[ns, EST]', freq=None)\n```", "```py\n>>> eastern = pd.read_csv(\n...     'data/nyc_temperatures.csv',\n...     index_col='date', parse_dates=True\n... ).tz_localize('EST')\n>>> eastern.head()\n```", "```py\n>>> eastern.tz_convert('UTC').head()\n```", "```py\n>>> eastern.tz_localize(None).to_period('M').index\nPeriodIndex(['2018-10', '2018-10', ..., '2018-10', '2018-10'],\n            dtype='period[M]', name='date', freq='M')\n```", "```py\n>>> eastern.tz_localize(None)\\\n...     .to_period('M').to_timestamp().index\nDatetimeIndex(['2018-10-01', '2018-10-01', '2018-10-01', ...,\n               '2018-10-01', '2018-10-01', '2018-10-01'],\n              dtype='datetime64[ns]', name='date', freq=None)\n```", "```py\n>>> df = pd.read_csv('data/nyc_temperatures.csv').rename(\n...     columns={'value': 'temp_C', 'attributes': 'flags'}\n... )\n>>> new_df = df.assign(\n...     date=pd.to_datetime(df.date),\n...     temp_F=(df.temp_C * 9/5) + 32\n... )\n>>> new_df.dtypes\ndate        datetime64[ns] \ndatatype            object\nstation             object\nflags               object \ntemp_C             float64\ntemp_F             float64\ndtype: object\n>>> new_df.head()\n```", "```py\n>>> df = df.assign(\n...     date=lambda x: pd.to_datetime(x.date),\n...     temp_C_whole=lambda x: x.temp_C.astype('int'),\n...     temp_F=lambda x: (x.temp_C * 9/5) + 32,\n...     temp_F_whole=lambda x: x.temp_F.astype('int')\n... )\n>>> df.head()\n```", "```py\n>>> df_with_categories = df.assign(\n...     station=df.station.astype('category'),\n...     datatype=df.datatype.astype('category')\n... )\n>>> df_with_categories.dtypes \ndate            datetime64[ns]\ndatatype              category\nstation               category\nflags                   object\ntemp_C                 float64\ntemp_C_whole             int64\ntemp_F                 float64\ntemp_F_whole             int64\ndtype: object\n>>> df_with_categories.describe(include='category')\n```", "```py\n>>> pd.Categorical(\n...     ['med', 'med', 'low', 'high'], \n...     categories=['low', 'med', 'high'], \n...     ordered=True\n... )\n['med', 'med', 'low', 'high'] \nCategories (3, object): ['low' < 'med' < 'high']\n```", "```py\n>>> df[df.datatype == 'TMAX']\\\n...     .sort_values(by='temp_C', ascending=False).head(10)\n```", "```py\n>>> df[df.datatype == 'TMAX'].sort_values(\n...     by=['temp_C', 'date'], ascending=[False, True] \n... ).head(10)\n```", "```py\n>>> df[df.datatype == 'TAVG'].nlargest(n=10, columns='temp_C')\n```", "```py\n>>> df.sample(5, random_state=0).index\nInt64Index([2, 30, 55, 16, 13], dtype='int64')\n>>> df.sample(5, random_state=0).sort_index().index\nInt64Index([2, 13, 16, 30, 55], dtype='int64')\n```", "```py\n>>> df.sort_index(axis=1).head()\n```", "```py\n>>> df.equals(df.sort_values(by='temp_C'))\nFalse\n>>> df.equals(df.sort_values(by='temp_C').sort_index())\nTrue\n```", "```py\n>>> df.set_index('date', inplace=True)\n>>> df.head()\n```", "```py\n>>> df['2018-10-11':'2018-10-12']\n```", "```py\n>>> df['2018-10-11':'2018-10-12'].reset_index()\n```", "```py\n>>> sp = pd.read_csv(\n...     'data/sp500.csv', index_col='date', parse_dates=True\n... ).drop(columns=['adj_close']) # not using this column\n```", "```py\n>>> sp.head(10)\\\n...     .assign(day_of_week=lambda x: x.index.day_name())\n```", "```py\n>>> bitcoin = pd.read_csv(\n...     'data/bitcoin.csv', index_col='date', parse_dates=True\n... ).drop(columns=['market_cap'])\n```", "```py\n# every day's closing price = S&P 500 close + Bitcoin close\n# (same for other metrics)\n>>> portfolio = pd.concat([sp, bitcoin], sort=False)\\\n...     .groupby(level='date').sum()\n>>> portfolio.head(10).assign(\n...     day_of_week=lambda x: x.index.day_name()\n... )\n```", "```py\n>>> import matplotlib.pyplot as plt # module for plotting\n>>> from matplotlib.ticker import StrMethodFormatter \n# plot the closing price from Q4 2017 through Q2 2018\n>>> ax = portfolio['2017-Q4':'2018-Q2'].plot(\n...     y='close', figsize=(15, 5), legend=False,\n...     title='Bitcoin + S&P 500 value without accounting '\n...           'for different indices'\n... )\n# formatting\n>>> ax.set_ylabel('price')\n>>> ax.yaxis\\\n...     .set_major_formatter(StrMethodFormatter('${x:,.0f}'))\n>>> for spine in ['top', 'right']:\n...     ax.spines[spine].set_visible(False)\n# show the plot\n>>> plt.show()\n```", "```py\n>>> sp.reindex(bitcoin.index, method='ffill').head(10)\\\n...     .assign(day_of_week=lambda x: x.index.day_name())\n```", "```py\nnp.where(boolean condition, value if True, value if False)\n```", "```py\n>>> import numpy as np\n>>> sp_reindexed = sp.reindex(bitcoin.index).assign(\n...     # volume is 0 when the market is closed\n...     volume=lambda x: x.volume.fillna(0),\n...     # carry this forward\n...     close=lambda x: x.close.fillna(method='ffill'),\n...     # take the closing price if these aren't available\n...     open=lambda x: \\\n...         np.where(x.open.isnull(), x.close, x.open),\n...     high=lambda x: \\\n...         np.where(x.high.isnull(), x.close, x.high),\n...     low=lambda x: np.where(x.low.isnull(), x.close, x.low)\n... )\n>>> sp_reindexed.head(10).assign(\n...     day_of_week=lambda x: x.index.day_name()\n... )\n```", "```py\n# every day's closing price = S&P 500 close adjusted for\n# market closure + Bitcoin close (same for other metrics)\n>>> fixed_portfolio = sp_reindexed + bitcoin\n# plot the reindexed portfolio's close (Q4 2017 - Q2 2018)\n>>> ax = fixed_portfolio['2017-Q4':'2018-Q2'].plot(\n...     y='close', figsize=(15, 5), linewidth=2, \n...     label='reindexed portfolio of S&P 500 + Bitcoin', \n...     title='Reindexed portfolio vs.' \n...           'portfolio with mismatched indices'\n... )\n# add line for original portfolio for comparison\n>>> portfolio['2017-Q4':'2018-Q2'].plot(\n...     y='close', ax=ax, linestyle='--',\n...     label='portfolio of S&P 500 + Bitcoin w/o reindexing' \n... ) \n# formatting\n>>> ax.set_ylabel('price')\n>>> ax.yaxis\\\n...     .set_major_formatter(StrMethodFormatter('${x:,.0f}'))\n>>> for spine in ['top', 'right']:\n...     ax.spines[spine].set_visible(False)\n# show the plot\n>>> plt.show() \n```", "```py\n>>> import pandas as pd\n>>> long_df = pd.read_csv(\n...     'data/long_data.csv',\n...     usecols=['date', 'datatype', 'value']\n... ).rename(columns={'value': 'temp_C'}).assign(\n...     date=lambda x: pd.to_datetime(x.date),\n...     temp_F=lambda x: (x.temp_C * 9/5) + 32\n... )\n```", "```py\n>>> long_df.set_index('date').head(6).T\n```", "```py\n>>> pivoted_df = long_df.pivot(\n...     index='date', columns='datatype', values='temp_C'\n... )\n>>> pivoted_df.head()\n```", "```py\n>>> pivoted_df.describe()\n```", "```py\n>>> pivoted_df = long_df.pivot(\n...     index='date', columns='datatype',\n...     values=['temp_C', 'temp_F']\n... )\n>>> pivoted_df.head()\n```", "```py\n>>> pivoted_df['temp_F']['TMIN'].head()\ndate\n2018-10-01    48.02\n2018-10-02    57.02\n2018-10-03    60.08\n2018-10-04    53.06\n2018-10-05    53.06\nName: TMIN, dtype: float64\n```", "```py\n>>> multi_index_df = long_df.set_index(['date', 'datatype'])\n>>> multi_index_df.head().index\nMultiIndex([('2018-10-01', 'TMAX'),\n            ('2018-10-01', 'TMIN'),\n            ('2018-10-01', 'TOBS'),\n            ('2018-10-02', 'TMAX'),\n            ('2018-10-02', 'TMIN')],\n           names=['date', 'datatype'])\n>>> multi_index_df.head()\n```", "```py\n>>> unstacked_df = multi_index_df.unstack()\n>>> unstacked_df.head()\n```", "```py\n>>> extra_data = long_df.append([{\n...     'datatype': 'TAVG', \n...     'date': '2018-10-01', \n...     'temp_C': 10, \n...     'temp_F': 50\n... }]).set_index(['date', 'datatype']).sort_index()\n>>> extra_data['2018-10-01':'2018-10-02']\n```", "```py\n>>> extra_data.unstack().head()\n```", "```py\n>>> extra_data.unstack(fill_value=-40).head()\n```", "```py\n>>> wide_df = pd.read_csv('data/wide_data.csv')\n>>> wide_df.head()\n```", "```py\n>>> melted_df = wide_df.melt(\n...     id_vars='date', value_vars=['TMAX', 'TMIN', 'TOBS'], \n...     value_name='temp_C', var_name='measurement'\n... )\n>>> melted_df.head()\n```", "```py\n>>> wide_df.set_index('date', inplace=True)\n>>> stacked_series = wide_df.stack() # put datatypes in index\n>>> stacked_series.head()\ndate          \n2018-10-01  TMAX    21.1\n            TMIN     8.9\n            TOBS    13.9\n2018-10-02  TMAX    23.9\n            TMIN    13.9\ndtype: float64\n```", "```py\n>>> stacked_df = stacked_series.to_frame('values')\n>>> stacked_df.head()\n```", "```py\n>>> stacked_df.head().index\nMultiIndex([('2018-10-01', 'TMAX'),\n            ('2018-10-01', 'TMIN'),\n            ('2018-10-01', 'TOBS'),\n            ('2018-10-02', 'TMAX'),\n            ('2018-10-02', 'TMIN')],\n           names=['date', None])\n```", "```py\n>>> stacked_df.index\\\n...     .set_names(['date', 'datatype'], inplace=True)\n>>> stacked_df.index.names\nFrozenList(['date', 'datatype'])\n```", "```py\n>>> import pandas as pd\n>>> df = pd.read_csv('data/dirty_data.csv')\n```", "```py\n>>> df.head()\n```", "```py\n>>> df.describe()\n```", "```py\n>>> df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 765 entries, 0 to 764\nData columns (total 10 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   date               765 non-null    object \n 1   station            765 non-null    object \n 2   PRCP               765 non-null    float64\n 3   SNOW               577 non-null    float64\n 4   SNWD               577 non-null    float64\n 5   TMAX               765 non-null    float64\n 6   TMIN               765 non-null    float64\n 7   TOBS               398 non-null    float64\n 8   WESF               11 non-null     float64\n 9   inclement_weather  408 non-null    object \ndtypes: float64(7), object(3)\nmemory usage: 59.9+ KB\n```", "```py\n>>> contain_nulls = df[\n...     df.SNOW.isna() | df.SNWD.isna() | df.TOBS.isna() \n...     | df.WESF.isna() | df.inclement_weather.isna()\n... ]\n>>> contain_nulls.shape[0]\n765\n>>> contain_nulls.head(10)\n```", "```py\n>>> import numpy as np\n>>> df[df.inclement_weather == 'NaN'].shape[0] # doesn't work\n0\n>>> df[df.inclement_weather == np.nan].shape[0] # doesn't work\n0\n```", "```py\n>>> df[df.inclement_weather.isna()].shape[0] # works\n357\n```", "```py\n>>> df[df.SNWD.isin([-np.inf, np.inf])].shape[0]\n577\n```", "```py\n>>> def get_inf_count(df):\n...     \"\"\"Find the number of inf/-inf values per column\"\"\"\n...     return {\n...         col: df[\n...             df[col].isin([np.inf, -np.inf])\n...         ].shape[0] for col in df.columns\n...     }\n```", "```py\n>>> get_inf_count(df)\n{'date': 0, 'station': 0, 'PRCP': 0, 'SNOW': 0, 'SNWD': 577,\n 'TMAX': 0, 'TMIN': 0, 'TOBS': 0, 'WESF': 0,\n 'inclement_weather': 0}\n```", "```py\n>>> pd.DataFrame({\n...     'np.inf Snow Depth':\n...         df[df.SNWD == np.inf].SNOW.describe(),\n...     '-np.inf Snow Depth': \n...         df[df.SNWD == -np.inf].SNOW.describe()\n... }).T\n```", "```py\n>>> df.describe(include='object')\n```", "```py\n>>> df[df.duplicated()].shape[0]\n284\n```", "```py\n>>> df[df.duplicated(keep=False)].shape[0] \n482\n```", "```py\n>>> df[df.duplicated(['date', 'station'])].shape[0]\n284\n```", "```py\n>>> df[df.duplicated()].head()\n```", "```py\n>>> df[df.WESF.notna()].station.unique()\narray(['?'], dtype=object)\n```", "```py\n    >>> df.date = pd.to_datetime(df.date)\n    ```", "```py\n    >>> station_qm_wesf = df[df.station == '?']\\\n    ...     .drop_duplicates('date').set_index('date').WESF\n    ```", "```py\n    >>> df.sort_values(\n    ...     'station', ascending=False, inplace=True\n    ... )\n    ```", "```py\n    >>> df_deduped = df.drop_duplicates('date')\n    ```", "```py\n    >>> df_deduped = df_deduped.drop(columns='station')\\\n    ...     .set_index('date').sort_index()\n    ```", "```py\n    >>> df_deduped = df_deduped.assign(WESF= \n    ...     lambda x: x.WESF.combine_first(station_qm_wesf)\n    ... )\n    ```", "```py\n>>> df_deduped.shape\n(324, 8)\n>>> df_deduped.head()\n```", "```py\n>>> df_deduped.dropna().shape\n(4, 8)\n```", "```py\n>>> df_deduped.dropna(how='all').shape # default is 'any'\n(324, 8)\n```", "```py\n>>> df_deduped.dropna(\n...     how='all', subset=['inclement_weather', 'SNOW', 'SNWD']\n... ).shape\n(293, 8)\n```", "```py\n>>> df_deduped.dropna(\n...     axis='columns', \n...     thresh=df_deduped.shape[0] * .75 # 75% of rows\n... ).columns\nIndex(['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'TOBS',\n       'inclement_weather'],\n      dtype='object')\n```", "```py\n>>> df_deduped.loc[:,'WESF'].fillna(0, inplace=True)\n>>> df_deduped.head()\n```", "```py\n>>> df_deduped = df_deduped.assign(\n...     TMAX=lambda x: x.TMAX.replace(5505, np.nan), \n...     TMIN=lambda x: x.TMIN.replace(-40, np.nan) \n... )\n```", "```py\n>>> df_deduped.assign(\n...     TMAX=lambda x: x.TMAX.fillna(method='ffill'),\n...     TMIN=lambda x: x.TMIN.fillna(method='ffill')\n... ).head()\n```", "```py\n>>> df_deduped.assign(\n...     SNWD=lambda x: np.nan_to_num(x.SNWD)\n... ).head()\n```", "```py\n>>> df_deduped.assign(\n...     SNWD=lambda x: x.SNWD.clip(0, x.SNOW)\n... ).head()\n```", "```py\n>>> df_deduped.assign(\n...     TMAX=lambda x: x.TMAX.fillna(x.TMAX.median()),\n...     TMIN=lambda x: x.TMIN.fillna(x.TMIN.median()),\n...     # average of TMAX and TMIN\n...     TOBS=lambda x: x.TOBS.fillna((x.TMAX + x.TMIN) / 2)\n... ).head()\n```", "```py\n>>> df_deduped.apply(lambda x:\n...     # Rolling 7-day median (covered in chapter 4).\n...     # we set min_periods (# of periods required for\n...     # calculation) to 0 so we always get a result\n...     x.fillna(x.rolling(7, min_periods=0).median())\n... ).head(10)\n```", "```py\n>>> df_deduped.reindex(\n...     pd.date_range('2018-01-01', '2018-12-31', freq='D')\n... ).apply(lambda x: x.interpolate()).head(10)\n```"]