["```py\n# Define the ModelConfig\nfrom lightgbm import LGBMRegressor\nmodel_config = ModelConfig(\n    model=LGBMRegressor(random_state=42),\n    name=\"Global LightGBM Baseline\",\n    # LGBM is not sensitive to normalized data\n    normalize=False,\n    # LGBM can handle missing values\n    fill_missing=False,\n)\n# Get train and test data\ntrain_features, train_target, train_original_target = feat_config.get_X_y(\n    train_df, categorical=True, exogenous=False\n)\ntest_features, test_target, test_original_target = feat_config.get_X_y(\n    test_df, categorical=True, exogenous=False\n) \n```", "```py\ny_pred, feat_df = train_model(\n        model_config,\n        _feat_config,\n        missing_value_config,\n        train_features,\n        train_target,\n        test_features,\n    )\nagg_metrics, eval_metrics_df = evaluate_forecast(\n    y_pred, test_target, train_target, model_config\n) \n```", "```py\nfrom category_encoders import CountEncoder\nfrom lightgbm import LGBMRegressor\n#Define which columns names are categorical features\ncat_encoder = CountEncoder(cols=cat_features)\nmodel_config = ModelConfig(\n    model=LGBMRegressor(random_state=42),\n    name=\"Global LightGBM with Meta Features (CountEncoder)\",\n    # LGBM is not sensitive to normalized data\n    normalize=False,\n    # LGBM can handle missing values\n    fill_missing=False,\n    # Turn on categorical encoding\n    encode_categorical=True,\n    # Pass the categorical encoder to be used\n    categorical_encoder=cat_encoder\n) \n```", "```py\nfrom category_encoders import TargetEncoder\ncat_encoder = TargetEncoder(cols=cat_features) \n```", "```py\nfrom lightgbm import LGBMRegressor\nmodel_config = ModelConfig(\n    model=LGBMRegressor(random_state=42),\n    name=\"Global LightGBM with Meta Features (NativeLGBM)\",\n    # LGBM is not sensitive to normalized data\n    normalize=False,\n    # LGBM can handle missing values\n    fill_missing=False,\n    # We are using inbuilt categorical feature handling\n    encode_categorical=False,\n)\n# Training the model and passing in fit_kwargs\ny_pred, feat_df = train_model(\n    model_config,\n    _feat_config,\n    missing_value_config,\n    train_features,\n    train_target,\n    test_features,\n    fit_kwargs=dict(categorical_feature=cat_features),\n) \n```", "```py\nfrom sklearn.model_selection import ParameterGrid\ngrid_params = {\n    \"num_leaves\": [16, 31, 63],\n    \"objective\": [\"regression\", \"regression_l1\", \"huber\"],\n    \"random_state\": [42],\n    \"colsample_bytree\": [0.5, 0.8, 1.0],\n}\nparameter_space = list(ParameterGrid(grid_params)) \n```", "```py\nscores = []\nfor p in tqdm(parameter_space, desc=\"Performing Grid Search\"):\n    _model_config = ModelConfig(\n        model=LGBMRegressor(**p, verbose=-1),\n        name=\"Global Meta LightGBM Tuning\",\n        # LGBM is not sensitive to normalized data\n        normalize=False,\n        # LGBM can handle missing values\n        fill_missing=False,\n    )\n    y_pred, feat_df = train_model(\n        _model_config,\n        _feat_config,\n        missing_value_config,\n        train_features,\n        train_target,\n        test_features,\n        fit_kwargs=dict(categorical_feature=cat_features),\n    )\n    scores.append(ts_utils.mae(\n                test_target['energy_consumption'], y_pred\n            )) \n```", "```py\nimport scipy\nfrom sklearn.model_selection import ParameterSampler\nrandom_search_params = {\n    # A uniform distribution between 10 and 100, but only integers\n    \"num_leaves\": scipy.stats.randint(10,100),\n    # A list of categorical string values\n    \"objective\": [\"regression\", \"regression_l1\", \"huber\"],\n    \"random_state\": [42],\n    # List of floating point numbers between 0.3 and 1.0 with a resolution of 0.05\n    \"colsample_bytree\": np.arange(0.3,1.0,0.05),\n    # List of floating point numbers between 0 and 10 with a resolution of 0.1\n    \"lambda_l1\":np.arange(0,10,0.1),\n    # List of floating point numbers between 0 and 10 with a resolution of 0.1\n    \"lambda_l2\":np.arange(0,10,0.1)\n}\n# Sampling from the search space number of iterations times\nparameter_space = list(ParameterSampler(random_search_params, n_iter=27, random_state=42)) \n```", "```py\ndef objective(trial):\n    params = {\n        # Sample an integer between 10 and 100\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 100),\n        # Sample a categorical value from the list provided\n        \"objective\": trial.suggest_categorical(\n            \"objective\", [\"regression\", \"regression_l1\", \"huber\"]\n        ),\n        \"random_state\": [42],\n        # Sample from a uniform distribution between 0.3 and 1.0\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.3, 1.0),\n        # Sample from a uniform distribution between 0 and 10\n        \"lambda_l1\": trial.suggest_uniform(\"lambda_l1\", 0, 10),\n        # Sample from a uniform distribution between 0 and 10\n        \"lambda_l2\": trial.suggest_uniform(\"lambda_l2\", 0, 10),\n    }\n    _model_config = ModelConfig(\n        # Use the sampled params to initialize the model\n        model=LGBMRegressor(**params, verbose=-1),\n        name=\"Global Meta LightGBM Tuning\",\n        # LGBM is not sensitive to normalized data\n        normalize=False,\n        # LGBM can handle missing values\n        fill_missing=False,\n    )\n    y_pred, feat_df = train_model(\n        _model_config,\n        _feat_config,\n        missing_value_config,\n        train_features,\n        train_target,\n        test_features,\n        fit_kwargs=dict(categorical_feature=cat_features),\n    )\n    # Return the MAE metric as the value\n    return ts_utils.mae(test_target[\"energy_consumption\"], y_pred) \n```", "```py\n# Create a study\nstudy = optuna.create_study(direction=\"minimize\", sampler=sampler)\n# Start the optimization run\nstudy.optimize(objective, n_trials=27, show_progress_bar=True) \n```", "```py\n# Define a function which splits a list into n partitions\ndef partition (list_in, n):\n    random.shuffle(list_in)\n    return [list_in[i::n] for i in range(n)]\n# split the unique LCLids into partitions\npartitions = partition(train_df.LCLid.cat.categories.tolist(), 3) \n```", "```py\nimport tsfel\ncfg = tsfel.get_features_by_domain(\"statistical\")\ncfg = {**cfg, **tsfel.get_features_by_domain(\"temporal\")}\nuniq_ids = train_df.LCLid.cat.categories\nstat_df = []\nfor id_ in tqdm(uniq_ids, desc=\"Calculating features for all households\"):\n    ts = train_df.loc[train_df.LCLid==id_, \"energy_consumption\"]\n    res = tsfel.time_series_features_extractor(cfg, ts, verbose=False)\n    res['LCLid'] = id_\n    stat_df.append(res)\nstat_df = pd.concat(stat_df).set_index(\"LCLid\") \n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom src.utils.data_utils import replace_array_in_dataframe\nfrom sklearn.manifold import TSNE #T-Distributed Stochastic Neighbor Embedding\n# Standardizing to make distance calculation fair\nX_std = replace_array_in_dataframe(stat_df, StandardScaler().fit_transform(stat_df))\n#Non-Linear Dimensionality Reduction\ntsne = TSNE(n_components=2, perplexity=50, learning_rate=\"auto\", init=\"pca\", random_state=42, metric=\"cosine\", square_distances=True)\nX_tsne = tsne.fit_transform(X_std.values)\n# Clustering reduced dimensions into 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=42).fit(X_tsne)\ncluster_df = pd.Series(kmeans.labels_, index=X_std.index) \n```"]