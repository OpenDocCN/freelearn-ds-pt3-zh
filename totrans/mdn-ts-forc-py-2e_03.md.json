["```py\n# Skipping first row cause it doesn't have any data\ndf = pd.read_excel(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00247/data_akbilgic.xlsx\", skiprows=1) \n```", "```py\n>>> pd.to_datetime(\"13-4-1987\").strftime(\"%d, %B %Y\")\n'13, April 1987' \n```", "```py\n>>> pd.to_datetime(\"4-1-1987\").strftime(\"%d, %B %Y\")\n'01, April 1987' \n```", "```py\n>>> pd.to_datetime(\"4-1-1987\", dayfirst=True).strftime(\"%d, %B %Y\")\n'04, January 1987' \n```", "```py\n>>> pd.to_datetime(\"4|1|1987\", format=\"%d|%m|%Y\").strftime(\"%d, %B %Y\")\n'04, January 1987' \n```", "```py\ndf['date'] = pd.to_datetime(df['date'], yearfirst=True) \n```", "```py\n>>> df.date.min(),df.date.max()\n(Timestamp('2009-01-05 00:00:00'), Timestamp('2011-02-22 00:00:00')) \n```", "```py\n>>> print(f\"\"\"\n     Date: {df.date.iloc[0]}\n     Day of year: {df.date.dt.day_of_year.iloc[0]}\n     Day of week: {df.date.dt.dayofweek.iloc[0]}\n     Month: {df.date.dt.month.iloc[0]}\n     Month Name: {df.date.dt.month_name().iloc[0]}\n     Quarter: {df.date.dt.quarter.iloc[0]}\n     Year: {df.date.dt.year.iloc[0]}\n     ISO Week: {df.date.dt.isocalendar().week.iloc[0]}\n     \"\"\")\nDate: 2009-01-05 00:00:00\nDay of year: 5\nDay of week: 0\nMonth: 1\nMonth Name: January\nQuarter: 1\nYear: 2009\nISO Week: 2 \n```", "```py\n# Setting the index as the datetime column\ndf.set_index(\"date\", inplace=True)\n# Select all data after 2010-01-04(inclusive)\ndf[\"2010-01-04\":]\n# Select all data between 2010-01-04 and 2010-02-06(exclusive)\ndf[\"2010-01-04\": \"2010-02-06\"]\n# Select data 2010 and before\ndf[: \"2010\"]\n# Select data between 2010-01 and 2010-06(both including)\ndf[\"2010-01\": \"2010-06\"] \n```", "```py\n# Specifying start and end dates with frequency\npd.date_range(start=\"2018-01-20\", end=\"2018-01-23\", freq=\"D\").astype(str).tolist()\n# Output: ['2018-01-20', '2018-01-21', '2018-01-22', '2018-01-23']\n# Specifying start and number of periods to generate in the given frequency\npd.date_range(start=\"2018-01-20\", periods=4, freq=\"D\").astype(str).tolist()\n# Output: ['2018-01-20', '2018-01-21', '2018-01-22', '2018-01-23']\n# Generating a date sequence with every 2 days\npd.date_range(start=\"2018-01-20\", periods=4, freq=\"2D\").astype(str).tolist()\n# Output: ['2018-01-20', '2018-01-22', '2018-01-24', '2018-01-26']\n# Generating a date sequence every month. By default it starts with Month end\npd.date_range(start=\"2018-01-20\", periods=4, freq=\"M\").astype(str).tolist()\n# Output: ['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30']\n# Generating a date sequence every month, but month start\npd.date_range(start=\"2018-01-20\", periods=4, freq=\"MS\").astype(str).tolist()\n# Output: ['2018-02-01', '2018-03-01', '2018-04-01', '2018-05-01'] \n```", "```py\n# Add four days to the date range\n(pd.date_range(start=\"2018-01-20\", end=\"2018-01-23\", freq=\"D\") + pd.Timedelta(4, unit=\"D\")).astype(str).tolist()\n# Output: ['2018-01-24', '2018-01-25', '2018-01-26', '2018-01-27']\n# Add four weeks to the date range\n(pd.date_range(start=\"2018-01-20\", end=\"2018-01-23\", freq=\"D\") + pd.Timedelta(4, unit=\"W\")).astype(str).tolist()\n# Output: ['2018-02-17', '2018-02-18', '2018-02-19', '2018-02-20'] \n```", "```py\n    df['pm2_5_1_hr'].ffill() \n    ```", "```py\n    df['pm2_5_1_hr'].bfill() \n    ```", "```py\n    df['pm2_5_1_hr'].fillna(df['pm2_5_1_hr'].mean()) \n    ```", "```py\n    df['pm2_5_1_hr'].interpolate(method=\"linear\") \n    ```", "```py\n    df['pm2_5_1_hr'].interpolate(method=\"nearest\") \n    ```", "```py\n    df['pm2_5_1_hr'].interpolate(method=\"spline\", order=2)\n    df['pm2_5_1_hr'].interpolate(method=\"polynomial\", order=5) \n    ```", "```py\nblock_df = pd.read_parquet(\"data/london_smart_meters/preprocessed/london_smart_meters_merged_block_0-7.parquet\") \n```", "```py\n#Converting to expanded form\nexp_block_df = compact_to_expanded(block_df[block_df.file==\"block_7\"], timeseries_col = 'energy_consumption',\nstatic_cols = [\"frequency\", \"series_length\", \"stdorToU\", \"Acorn\", \"Acorn_grouped\", \"file\"],\ntime_varying_cols = ['holidays', 'visibility', 'windBearing', 'temperature', 'dewPoint',\n       'pressure', 'apparentTemperature', 'windSpeed', 'precipType', 'icon',\n       'humidity', 'summary'],\nts_identifier = \"LCLid\") \n```", "```py\n# Pivot the data to set the index as the datetime and the different time series along the columns\nplot_df = pd.pivot_table(exp_block_df, index=\"timestamp\", columns=\"LCLid\", values=\"energy_consumption\")\n# Generate Plot. Since we have a datetime index, we can mention the frequency to decide what do we want on the X axis\nmsno.matrix(plot_df, freq=\"M\") \n```", "```py\n# Taking a single time series from the block\nts_df = exp_block_df[exp_block_df.LCLid==\"MAC000193\"].set_index(\"timestamp\")\nmsno.matrix(ts_df[\"2012-09-30\": \"2012-10-31\"], freq=\"D\") \n```", "```py\n# The dates between which we are nulling out the time series\nwindow = slice(\"2012-10-07\", \"2012-10-08\")\n# Creating a new column and artificially creating missing values\nts_df['energy_consumption_missing'] = ts_df.energy_consumption\nts_df.loc[window, \"energy_consumption_missing\"] = np.nan \n```", "```py\n#Shifting 48 steps to get previous day\nts_df[\"prev_day\"] = ts_df['energy_consumption'].shift(48)\n#Using the shifted column to fill missing\nts_df['prev_day_imputed'] =  ts_df['energy_consumption_missing']\nts_df.loc[null_mask,\"prev_day_imputed\"] = ts_df.loc[null_mask,\"prev_day\"]\nmae = mean_absolute_error(ts_df.loc[window, \"prev_day_imputed\"], ts_df.loc[window, \"energy_consumption\"]) \n```", "```py\n#Create a column with the Hour from timestamp\nts_df[\"hour\"] = ts_df.index.hour\n#Calculate hourly average consumption\nhourly_profile = ts_df.groupby(['hour'])['energy_consumption'].mean().reset_index()\nhourly_profile.rename(columns={\"energy_consumption\": \"hourly_profile\"}, inplace=True)\n#Saving the index because it gets lost in merge\nidx = ts_df.index\n#Merge the hourly profile dataframe to ts dataframe\nts_df = ts_df.merge(hourly_profile, on=['hour'], how='left', validate=\"many_to_one\")\nts_df.index = idx\n#Using the hourly profile to fill missing\nts_df['hourly_profile_imputed'] = ts_df['energy_consumption_missing']\nts_df.loc[null_mask,\"hourly_profile_imputed\"] = ts_df.loc[null_mask,\"hourly_profile\"]\nmae = mean_absolute_error(ts_df.loc[window, \"hourly_profile_imputed\"], ts_df.loc[window, \"energy_consumption\"]) \n```", "```py\n#Create a column with the weekday from timestamp\nts_df[\"weekday\"] = ts_df.index.weekday\n#Calculate weekday-hourly average consumption\nday_hourly_profile = ts_df.groupby(['weekday','hour'])['energy_consumption'].mean().reset_index()\nday_hourly_profile.rename(columns={\"energy_consumption\": \"day_hourly_profile\"}, inplace=True)\n#Saving the index because it gets lost in merge\nidx = ts_df.index\n#Merge the day-hourly profile dataframe to ts dataframe\nts_df = ts_df.merge(day_hourly_profile, on=['weekday', 'hour'], how='left', validate=\"many_to_one\")\nts_df.index = idx\n#Using the day-hourly profile to fill missing\nts_df['day_hourly_profile_imputed'] = ts_df['energy_consumption_missing']\nts_df.loc[null_mask,\"day_hourly_profile_imputed\"] = ts_df.loc[null_mask,\"day_hourly_profile\"]\nmae = mean_absolute_error(ts_df.loc[window, \"day_hourly_profile_imputed\"], ts_df.loc[window, \"energy_consumption\"]) \n```", "```py\nfrom src.imputation.interpolation import SeasonalInterpolation\n# Seasonal interpolation using 48*7 as the seasonal period.\nrecovered_matrix_seas_interp_weekday_half_hour = SeasonalInterpolation(seasonal_period=48*7,decomposition_strategy=\"additive\", interpolation_strategy=\"spline\", interpolation_args={\"order\":3}, min_value=0).fit_transform(ts_df.energy_consumption_missing.values.reshape(-1,1))\nts_df['seas_interp_weekday_half_hour_imputed'] = recovered_matrix_seas_interp_weekday_half_hour \n```"]