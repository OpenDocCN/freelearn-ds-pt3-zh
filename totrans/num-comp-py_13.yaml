- en: Grouping for Aggregation, Filtration, and Transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most fundamental tasks during a data analysis involves splitting
    data into independent groups before performing a calculation on each group. This
    methodology has been around for quite some time but has more recently been referred
    to as **split-apply-combine**. This chapter covers the powerful `groupby` method,
    which allows you to group your data in any way imaginable and apply any type of
    function independently to each group before returning a single dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Hadley Wickham coined the term **split-apply-combine** to describe the common
    data analysis pattern of breaking up data into independent manageable chunks,
    independently applying functions to these chunks, and then combining the results
    back together. More details can be found in his paper ([http://bit.ly/2isFuL9](http://www.stat.wvu.edu/~jharner/courses/stat623/docs/plyrJSS.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get started with the recipes, we will need to know just a little
    terminology. All basic groupby operations have **grouping columns**, and each
    unique combination of values in these columns represents an independent grouping
    of the data. The syntax looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The result of this operation returns a groupby object. It is this groupby object
    that will be the engine that drives all the calculations for this entire chapter.
    Pandas actually does very little when creating this groupby object, merely validating
    that grouping is possible. You will have to chain methods on this groupby object
    in order to unleash its powers.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, the result of the operation will either be a `DataFrameGroupBy`
    or `SeriesGroupBy` but for simplicity, it will be referred to as the groupby object
    for the entire chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining an aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping and aggregating with multiple columns and functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing the MultiIndex after grouping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing an aggregation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing aggregating functions with `*args` and `**kwargs`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the `groupby` object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering for states with a minority majority
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming through a weight loss bet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating weighted mean SAT scores per state with apply
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping by continuous variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counting the total number of flights between cities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the longest streak of on-time flights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining an aggregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common use of the `groupby` method is to perform an aggregation. What
    actually is an aggregation? In our data analysis world, an aggregation takes place
    when a sequence of many inputs get summarized or combined into a single value
    output. For example, summing up all the values of a column or finding its maximum
    are common aggregations applied on a single sequence of data. An aggregation simply
    takes many values and converts them down to a single value.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the grouping columns defined during the introduction, most aggregations
    have two other components, the **aggregating columns** and **aggregating functions**.
    The aggregating columns are those whose values will be aggregated. The aggregating
    functions define how the aggregation takes place. Major aggregation functions
    include `sum`, `min`, `max`, `mean`, `count`, `variance`, `std`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we examine the flights dataset and perform the simplest possible
    aggregation involving only a single grouping column, a single aggregating column,
    and a single aggregating function. We will find the average arrival delay for
    each airline. Pandas has quite a few different syntaxes to produce an aggregation
    and this recipe covers them.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Read in the flights dataset, and define the grouping columns (`AIRLINE`), aggregating
    columns (`ARR_DELAY`), and aggregating functions (`mean`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/50f4d53d-fcb0-4e12-9a69-0d39d5ea04d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Place the grouping column in the `groupby` method and then call the `agg` method
    with a dictionary pairing the aggregating column with its aggregating function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fec1f726-3a08-44eb-97aa-a134f638a217.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, you may place the aggregating column in the indexing operator
    and then pass the aggregating function as a string to `agg`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The string names used in the previous step are a convenience pandas offers
    you to refer to a particular aggregation function. You can pass any aggregating
    function directly to the `agg` method such as the NumPy `mean` function. The output
    is the same as the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s possible to skip the `agg` method altogether in this case and use the
    `mean` method directly. This output is also the same as step 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The syntax for the `groupby` method is not as straightforward as other methods.
    Let's intercept the chain of methods in step 2 by storing the result of the `groupby`
    method as its own variable
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A completely new intermediate object is first produced with its own distinct
    attributes and methods. No calculations take place at this stage. Pandas merely
    validates the grouping columns. This groupby object has an `agg` method to perform
    aggregations. One of the ways to use this method is to pass it a dictionary mapping
    the aggregating column to the aggregating function, as done in step 2.
  prefs: []
  type: TYPE_NORMAL
- en: There are several different flavors of syntax that produce a similar result,
    with step 3 showing an alternative. Instead of identifying the aggregating column
    in the dictionary, place it inside the indexing operator just as if you were selecting
    it as a column from a DataFrame. The function string name is then passed as a
    scalar to the `agg` method.
  prefs: []
  type: TYPE_NORMAL
- en: You may pass any aggregating function to the `agg` method. Pandas allows you
    to use the string names for simplicity but you may also explicitly call an aggregating
    function as done in step 4\. NumPy provides many functions that aggregate values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5 shows one last syntax flavor. When you are only applying a single aggregating
    function as in this example, you can often call it directly as a method on the
    groupby object itself without `agg`. Not all aggregation functions have a method
    equivalent but many basic ones do. The following is a list of several aggregating
    functions that may be passed as a string to `agg` or chained directly as a method
    to the groupby object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you do not use an aggregating function with `agg`, pandas raises an exception.
    For instance, let''s see what happens when we apply the square root function to
    each group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas official documentation on *Aggregation* ([http://bit.ly/2iuf1Nc](http://pandas.pydata.org/pandas-docs/stable/groupby.html#aggregation))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping and aggregating with multiple columns and functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is possible to do grouping and aggregating with multiple columns. The syntax
    is only slightly different than it is for grouping and aggregating with a single
    column. As usual with any kind of grouping operation, it helps to identify the
    three components: the grouping columns, aggregating columns, and aggregating functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we showcase the flexibility of the `groupby` DataFrame method
    by answering the following queries:'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the number of cancelled flights for every airline per weekday
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the number and percentage of cancelled and diverted flights for every
    airline per weekday
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each origin and destination, finding the total number of flights, the number
    and percentage of cancelled flights, and the average and variance of the airtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Read in the flights dataset, and answer the first query by defining the grouping
    columns (`AIRLINE, WEEKDAY`), the aggregating column (`CANCELLED`), and the aggregating
    function (`sum`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Answer the second query by using a list for each pair of grouping and aggregating
    columns. Also, use a list for the aggregating functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5f4fb3bf-8d5c-43ec-adfa-712255f9496e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Answer the third query using a dictionary in the `agg` method to map specific
    aggregating columns to specific aggregating functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/14de92a8-0578-44fd-93d4-149460e03eef.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To group by multiple columns as in step 1, we pass a list of the string names
    to the `groupby` method. Each unique combination of `AIRLINE` and `WEEKDAY` forms
    an independent group. Within each of these groups, the sum of the cancelled flights
    is found and then returned as a Series.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2, again groups by both `AIRLINE` and `WEEKDAY`, but this time aggregates
    two columns. It applies each of the two aggregation functions, `sum` and `mean`,
    to each column resulting in four returned columns per group.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 goes even further, and uses a dictionary to map specific aggregating
    columns to different aggregating functions. Notice that the `size` aggregating
    function returns the total number of rows per group. This is different than the
    `count` aggregating function, which returns the number of non-missing values per
    group.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a few main flavors of syntax that you will encounter when performing
    an aggregation. The following four blocks of pseudocode summarize the main ways
    you can perform an aggregation with the `groupby` method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `agg` with a dictionary is the most flexible and allows you to specify
    the aggregating function for each column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `agg` with a list of aggregating functions applies each of the functions
    to each of the aggregating columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Directly using a method following the aggregating columns instead of `agg`,
    applies just that method to each aggregating column. This way does not allow for
    multiple aggregating functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do not specify the aggregating columns, then the aggregating method
    will be applied to all the non-grouping columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding four code blocks it is possible to substitute a string for
    any of the lists when grouping or aggregating by a single column.
  prefs: []
  type: TYPE_NORMAL
- en: Removing the MultiIndex after grouping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inevitably, when using `groupby`, you will likely create a MultiIndex in the
    columns or rows or both. DataFrames with MultiIndexes are more difficult to navigate
    and occasionally have confusing column names as well.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we perform an aggregation with the `groupby` method to create
    a DataFrame with a MultiIndex for the rows and columns and then manipulate it
    so that the index is a single level and the column names are descriptive.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Read in the flights dataset; write a statement to find the total and average
    miles flown; and the maximum and minimum arrival delay for each airline for each
    weekday:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/000fc6a3-3bb0-4fd1-8c48-339fdbbc16ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Both the rows and columns are labeled by a MultiIndex with two levels. Let''s
    squash it down to just a single level. To address the columns, we use the MultiIndex
    method, `get_level_values`. Let''s display the output of each level and then concatenate
    both levels before setting it as the new column values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/776af29d-5e5c-4c22-8d65-e75693161af6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Return the row labels to a single level with `reset_index`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d2a0fb0c-945c-478e-a7a7-8835ff925b68.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using the `agg` method to perform an aggregation on multiple columns, pandas
    creates an index object with two levels. The aggregating columns become the top
    level and the aggregating functions become the bottom level. Pandas displays MultiIndex
    levels differently than single-level columns. Except for the **innermost** levels,
    repeated index values do not get displayed on the screen. You can inspect the
    DataFrame from step 1 to verify this. For instance, the `DIST` column shows up
    only once but it refers to both of the first two columns.
  prefs: []
  type: TYPE_NORMAL
- en: The innermost MultiIndex level is the one closest to the data. This would be
    the bottom-most column level and the right-most index level.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 defines new columns by first retrieving the underlying values of each
    of the levels with the MultiIndex method `get_level_values.` This method accepts
    an integer identifying the index level. They are numbered beginning with zero
    from the top/left. Indexes support vectorized operations, so we concatenate both
    levels together with a separating underscore. We assign these new values to the
    `columns` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we make both index levels as columns with `reset_index`. We could
    have concatenated the levels together like we did in step 2, but it makes more
    sense to keep them as separate columns.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, at the end of a groupby operation, pandas puts all of the grouping
    columns in the index. The `as_index` parameter in the `groupby` method can be
    set to `False` to avoid this behavior. You can chain the `reset_index` method
    after grouping to get the same effect as done in step 3\. Let''s see an example
    of this by finding the average distance traveled per flight from each airline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a196e45e-4c13-4ed3-9efe-5d64e7dbc121.png)'
  prefs: []
  type: TYPE_IMG
- en: Take a look at the order of the airlines in the previous result. By default,
    pandas sorts the grouping columns. The `sort` parameter exists within the `groupby`
    method and is defaulted to `True`. You may set it to `False` to keep the order
    of the grouping columns the same as how they are encountered in the dataset. You
    also get a small performance improvement by not sorting your data.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing an aggregation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas provides a number of the most common aggregation functions for you to
    use with the groupby object. At some point, you will need to write your own customized
    user-defined functions that don't exist in pandas or NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use the college dataset to calculate the mean and standard
    deviation of the undergraduate student population per state. We then use this
    information to find the maximum number of standard deviations from the mean that
    any single population value is per state.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Read in the college dataset, and find the mean and standard deviation of the
    undergraduate population by state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/da651e10-a471-434f-a5e1-e5e0f7ae4307.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This output isn''t quite what we desire. We are not looking for the mean and
    standard deviations of the entire group but the maximum number of standard deviations
    away from the mean for any one institution. In order to calculate this, we need
    to subtract the mean undergraduate population by state from each institution''s
    undergraduate population and then divide by the standard deviation. This standardizes
    the undergraduate population for each group. We can then take the maximum of the
    absolute value of these scores to find the one that is farthest away from the
    mean. Pandas does not provide a function capable of doing this. Instead, we will
    need to create a custom function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After defining the function, pass it directly to the `agg` method to complete
    the aggregation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There does not exist a predefined pandas function to calculate the maximum number
    of standard deviations away from the mean. We were forced to construct a customized
    function in step 2\. Notice that this custom function `max_deviation` accepts
    a single parameter, `s`. Looking ahead at step 3, you will notice that the function
    name is placed inside the `agg` method without directly being called. Nowhere
    is the parameter `s` explicitly passed to `max_deviation`. Instead, pandas implicitly
    passes the `UGDS` column as a Series to `max_deviation`.
  prefs: []
  type: TYPE_NORMAL
- en: The `max_deviation` function is called once for each group. As `s` is a Series,
    all normal Series methods are available. It subtracts the mean of that particular
    grouping from each of the values in the group before dividing by the standard
    deviation in a process called **standardization**.
  prefs: []
  type: TYPE_NORMAL
- en: Standardization is a common statistical procedure to understand how greatly
    individual values vary from the mean. For a normal distribution, 99.7% of the
    data lies within three standard deviations of the mean.
  prefs: []
  type: TYPE_NORMAL
- en: As we are interested in absolute deviation from the mean, we take the absolute
    value from all the standardized scores and return the maximum. The `agg` method
    necessitates that a single scalar value must be returned from our custom function, or
    else an exception will be raised. Pandas defaults to using the sample standard
    deviation which is undefined for any groups with just a single value. For instance,
    the state abbreviation *AS* (American Samoa) has a missing value returned as it
    has only a single institution in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is possible to apply our customized function to multiple aggregating columns.
    We simply add more column names to the indexing operator. The `max_deviation`
    function only works with numeric columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/aba24502-a192-411c-bc6b-61e69a37abab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also use your customized aggregation function along with the prebuilt
    functions. The following does this and groups by state and religious affiliation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e89efc68-2bf0-4338-9180-1c2b0efbbc56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that pandas uses the name of the function as the name for the returned
    column. You can change the column name directly with the rename method or you
    can modify the special function attribute `__name__`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/25d346b9-cf76-4eb2-a6db-d0fa8725a4d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Customizing aggregating functions with *args and **kwargs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When writing your own user-defined customized aggregation function, pandas
    implicitly passes it each of the aggregating columns one at a time as a Series.
    Occasionally, you will need to pass more arguments to your function than just
    the Series itself. To do so, you need to be aware of Python''s ability to pass
    an arbitrary number of arguments to functions. Let''s take a look at the signature
    of the groupby object''s `agg` method with help from the `inspect` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The argument `*args` allow you to pass an arbitrary number of non-keyword arguments
    to your customized aggregation function. Similarly, `**kwargs` allows you to pass
    an arbitrary number of keyword arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we build a customized function for the college dataset that
    finds the percentage of schools by state and religious affiliation that have an
    undergraduate population between two values.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define a function that returns the percentage of schools with an undergraduate
    population between 1,000 and 3,000:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate this percentage grouping by state and religious affiliation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This function works fine but it doesn''t give the user any flexibility to choose
    the lower and upper bound. Let''s create a new function that allows the user to
    define these bounds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass this new function to the `agg` method along with lower and upper bounds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Step 1 creates a function that doesn't accept any extra arguments. The upper
    and lower bounds must be hardcoded into the function itself, which isn't very
    flexible. Step 2 shows the results of this aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: We create a more flexible function in step 3 that allows users to define both
    the lower and upper bounds dynamically. Step 4 is where the magic of `*args` and
    `**kwargs` come into play. In this particular example, we pass two non-keyword
    arguments, 1,000 and 10,000, to the `agg` method. Pandas passes these two arguments
    respectively to the `low` and `high` parameters of `pct_between`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few ways we could achieve the same result in step 4\. We could
    have explicitly used the parameter names with the following command to produce
    the same result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The order of the keyword arguments doesn''t matter as long as they come after
    the function name. Further still, we can mix non-keyword and keyword arguments
    as long as the keyword arguments come last:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: For ease of understanding, it's probably best to include all the parameter names
    in the order that they are defined in the function signature.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, when `agg` is called, all the non-keyword arguments get collected
    into a tuple named `args` and all the keyword arguments get collected into a dictionary
    named `kwargs`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unfortunately, pandas does not have a direct way to use these additional arguments
    when using multiple aggregation functions together. For example, if you wish to
    aggregate using the `pct_between` and `mean` functions, you will get the following
    exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Pandas is incapable of understanding that the extra arguments need to be passed
    to `pct_between`. In order to use our custom function with other built-in functions
    and even other custom functions, we can define a special type of nested function
    called a **closure**. We can use a generic closure to build all of our customized
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0185d893-b8f3-47d9-847b-ea7838685247.png)'
  prefs: []
  type: TYPE_IMG
- en: The `make_agg_func` function acts as a factory to create customized aggregation
    functions. It accepts the customized aggregation function that you already built
    (`pct_between` in this case), a `name` argument, and an arbitrary number of extra
    arguments. It returns a function with the extra arguments already set. For instance,
    `my_agg1` is a specific customized aggregating function that finds the percentage
    of schools with an undergraduate population between one and three thousand. The
    extra arguments (`*args` and `**kwargs`) specify an exact set of parameters for
    your customized function (`pct_between` in this case). The `name` parameter is
    very important and must be unique each time `make_agg_func` is called. It will
    eventually be used to rename the aggregated column.
  prefs: []
  type: TYPE_NORMAL
- en: A closure is a function that contains a function inside of it (a nested function)
    and returns this nested function. This nested function must refer to variables
    in the scope of the outer function in order to be a closure. In this example,
    `make_agg_func` is the outer function and returns the nested function `wrapper`,
    which accesses the variables `func`, `args`, and `kwargs` from the outer function.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Arbitrary Argument Lists* from the official Python documentation ([http://bit.ly/2vumbTE](https://docs.python.org/3/tutorial/controlflow.html#arbitrary-argument-lists))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A tutorial on *Python Closures* ([http://bit.ly/2xFdYga](http://www.geeksforgeeks.org/python-closures/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the groupby object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The immediate result from using the `groupby` method on a DataFrame will be
    a groupby object. Usually, we continue operating on this object to do aggregations
    or transformations without ever saving it to a variable. One of the primary purposes
    of examining this groupby object is to inspect individual groups.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we examine the groupby object itself by directly calling methods
    on it as well as iterating through each of its groups.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get started by grouping the state and religious affiliation columns
    from the college dataset, saving the result to a variable and confirming its type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `dir` function to discover all its available functionality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the number of groups with the `ngroups` attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To find the uniquely identifying labels for each group, look in the `groups`
    attribute, which contains a dictionary of each unique group mapped to all the
    corresponding index labels of that group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve a single group with the `get_group` method by passing it a tuple of
    an exact group label. For example, to get all the religiously affiliated schools
    in the state of Florida, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/eacc01ed-8742-4326-b264-9d7ef642eb55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You may want to take a peek at each individual group. This is possible because
    groupby objects are iterable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/abafebc3-f01c-4644-8ff4-dd569a6c390b.png)'
  prefs: []
  type: TYPE_IMG
- en: You can also call the head method on your groupby object to get the first rows
    of each group together in a single DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/72cd2228-11a4-4845-90d3-401a079270ee.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Step 1 formally creates our groupby object. It is useful to display all the
    public attributes and methods to reveal all the possible functionality as was
    done in step 2\. Each group is uniquely identified by a tuple containing a unique
    combination of the values in the grouping columns. Pandas allows you to select
    a specific group as a DataFrame with the `get_group` method shown in step 5.
  prefs: []
  type: TYPE_NORMAL
- en: It is rare that you will need to iterate through your groups and in general,
    you should avoid doing so if necessary, as it can be quite slow. Occasionally,
    you will have no other choice. When iterating through a groupby object, you are
    given a tuple containing the group name and the DataFrame without the grouping
    columns. This tuple is unpacked into the variables `name` and `group` in the for-loop
    in step 6.
  prefs: []
  type: TYPE_NORMAL
- en: One interesting thing you can do while iterating through your groups is to display
    a few of the rows from each group directly in the notebook. To do this, you can
    either use the print function or the `display` function from the `IPython.display`
    module. Using the `print` function results in DataFrames that are in plain text
    without any nice HTML formatting. Using the `display` function will produce DataFrames
    in their normal easy-to-read format.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several useful methods that were not explored from the list in step
    2\. Take for instance the `nth` method, which, when given a list of integers,
    selects those specific rows from each group. For example, the following operation
    selects the first and last rows from each group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/35964ec0-13c2-4fc8-a4e1-2c49eeeb7735.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Official documentation of the `display` function from IPython ([http://bit.ly/2iAIogC](http://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering for states with a minority majority
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](3b938362-1f65-406c-ba9d-3bf735543ca8.xhtml), *Selecting Subsets
    of Data*, we marked every row as `True` or `False` before filtering out the `False`
    rows. In a similar fashion, it is possible to mark entire groups of data as either
    `True` or `False` before filtering out the `False` groups. To do this, we first
    form groups with the `groupby` method and then apply the `filter` method. The
    `filter` method accepts a function that must return either `True` or `False` to
    indicate whether a group is kept or not.
  prefs: []
  type: TYPE_NORMAL
- en: This `filter` method applied after a call to the `groupby` method is completely
    different than the DataFrame `filter` method.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use the college dataset to find all the states that have
    more non-white undergraduate students than white. As this is a dataset from the
    US, whites form the majority and therefore, we are looking for states with a minority
    majority.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Read in the college dataset, group by state, and display the total number of
    groups. This should equal the number of unique states retrieved from the `nunique`
    Series method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `grouped` variable has a `filter` method, which accepts a custom function
    that determines whether a group is kept or not. The custom function gets implicitly
    passed a DataFrame of the current group and is required to return a boolean. Let''s
    define a function that calculates the total percentage of minority students and
    returns `True` if this percentage is greater than a user-defined threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `filter` method passed with the `check_minority` function and a threshold
    of 50% to find all states that have a minority majority:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3f8be7b3-9a86-4710-b751-fbe63a2b8cd5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just looking at the output may not be indicative of what actually happened.
    The DataFrame starts with state Arizona (AZ) and not Alaska (AK) so we can visually
    confirm that something changed. Let''s compare the `shape` of this filtered DataFrame
    with the original. Looking at the results, about 60% of the rows have been filtered,
    and only 20 states remain that have a minority majority:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe takes a look at the total population of all the institutions on
    a state-by-state basis. The goal is to keep all the rows from the states, as a
    whole, that have a minority majority. This requires us to group our data by state,
    which is done in step 1\. We find that there are 59 independent groups.
  prefs: []
  type: TYPE_NORMAL
- en: The `filter` groupby method either keeps all the rows in a group or filters
    them out. It does not change the number of columns. The `filter` groupby method
    performs this gatekeeping through a user-defined function, for example, `check_minority`
    in this recipe. A very important aspect to filter is that it passes the entire
    DataFrame for that particular group to the user-defined function and returns a
    single boolean for each group.
  prefs: []
  type: TYPE_NORMAL
- en: Inside of the `check_minority` function, the percentage and the total number
    of non-white students for each institution are first calculated and then the total
    number of all students is found. Finally, the percentage of non-white students
    for the entire state is checked against the given threshold, which produces a
    boolean.
  prefs: []
  type: TYPE_NORMAL
- en: The final result is a DataFrame with the same columns as the original but with
    the rows from the states that don't meet the threshold filtered out. As it is
    possible that the head of the filtered DataFrame is the same as the original,
    you need to do some inspection to ensure that the operation completed successfully.
    We verify this by checking the number of rows and number of unique states.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our function, `check_minority`, is flexible and accepts a parameter to lower
    or raise the percentage of minority threshold. Let''s check the shape and number
    of unique states for a couple of other thresholds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas official documentation on *Filtration* ([http://bit.ly/2xGUoA7](https://pandas.pydata.org/pandas-docs/stable/groupby.html#filtration))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming through a weight loss bet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One method to increase motivation to lose weight is to make a bet with someone
    else. The scenario in this recipe will track weight loss from two individuals
    over the course of a four-month period and determine a winner.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use simulated data from two individuals to track the percentage
    of weight loss over the course of four months. At the end of each month, a winner
    will be declared based on the individual who lost the highest percentage of body
    weight for that month. To track weight loss, we group our data by month and person,
    then call the `transform` method to find the percentage weight loss at each week
    from the start of the month.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Read in the raw weight_loss dataset, and examine the first month of data from
    the two people, `Amy` and `Bob`. There are a total of four weigh-ins per month:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5efff218-dbf7-48a9-b831-f6052ddb4fef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To determine the winner for each month, we only need to compare weight loss
    from the first week to the last week of each month. But, if we wanted to have
    weekly updates, we can also calculate weight loss from the current week to the
    first week of each month.  Let''s create a function that is capable of providing
    weekly updates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Let's test out this function for Bob during the month of January.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: You should ignore the index values in the last output. 0, 2, 4 and 6 simply
    refer to the original row labels of the DataFrame and have no relation to the
    week.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the first week, Bob lost 1% of his body weight. He continued losing weight
    during the second week but made no progress during the last week.  We can apply
    this function to every single combination of person and week to get the weight
    loss per week in relation to the first week of the month. To do this, we need
    to group our data by `Name` and `Month` , and then use the `transform` method
    to apply this custom function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The `transform` method must return an object with the same number of rows as
    the calling DataFrame. Let''s append this result to our original DataFrame as
    a new column. To help shorten the output, we will select Bob''s first two months
    of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/998f0234-1f22-4853-8f8c-3c1576a352ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that the percentage weight loss resets after the new month. With this
    new column, we can manually determine a winner but let''s see if we can find a
    way to do this automatically. As the only week that matters is the last week,
    let''s select week 4:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5024a731-2dbe-4fc1-9e27-d76065bb7859.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This narrows down the weeks but still doesn''t automatically find out the winner
    of each month. Let''s reshape this data with the `pivot` method so that Bob''s
    and Amy''s percent weight loss is side-by-side for each month:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/efdc2ed3-5b3b-4a62-b399-7da0b37d2cec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This output makes it clearer who has won each month, but we can still go a
    couple steps farther. NumPy has a vectorized if-then-else function called `where`,
    which can map a Series or array of booleans to other values. Let''s create a column
    for the name of the winner and highlight the winning percentage for each month:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/452143c1-f8f8-4aa6-a8e2-98749075c3b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Use the `value_counts` method to return the final score as the number of months
    won:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this recipe, the `query` method is used to filter data instead of
    boolean indexing. Refer to the *Improving readability of Boolean indexing with
    the query method* recipe from [Chapter 11](9f721370-ae04-4425-aab9-d525335b96b3.xhtml),
    *Boolean Indexing*, for more information*.*
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to find the percentage weight loss for each month for each person.
    One way to accomplish this task is to calculate each week's weight loss relative
    to the start of each month. This specific task is perfectly suited to the `transform`
    groupby method. The `transform` method accepts a function as its one required
    parameter. This function gets implicitly passed each non-grouping column (or only
    the columns specified in the indexing operator as was done in this recipe with
    `Weight`). It must return a sequence of values the same length as the passed group
    or else an exception will be raised. In essence, all values from the original
    DataFrame are transforming. No aggregation or filtration takes place.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 creates a function that subtracts the first value of the passed Series
    from all of its values and then divides this result by the first value. This calculates
    the percent loss (or gain) relative to the first value. In step 3 we test this
    function on one person during one month.
  prefs: []
  type: TYPE_NORMAL
- en: In step 4, we use this function in the same manner over every combination of
    person and week. In some literal sense, we are *transforming* the `Weight` column
    into the percentage of weight lost for the current week. The first month of data
    is outputted for each person. Pandas returns the new data as a Series. This Series
    isn't all that useful by itself and makes more sense appended to the original
    DataFrame as a new column. We complete this operation in step 5.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the winner, only week 4 of each month is necessary. We could stop
    here and manually determine the winner but pandas supplies us functionality to
    automate this. The `pivot` function in step 7 reshapes our dataset by pivoting
    the unique values of one column into new column names. The `index` parameter is
    used for the column that you do not want to pivot. The column passed to the `values`
    parameter gets tiled over each unique combination of the columns in the `index`
    and `columns` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The `pivot` method only works if there is just a single occurrence of each unique
    combination of the columns in the `index` and `columns` parameters. If there is
    more than one unique combination, an exception will be raised. You can use the
    `pivot_table` method in that situation which allows you to aggregate multiple
    values together.
  prefs: []
  type: TYPE_NORMAL
- en: After pivoting, we utilize the highly effective and fast NumPy `where` function,
    whose first argument is a condition that produces a Series of booleans. `True`
    values get mapped to *Amy* and `False` values get mapped to *Bob.* We highlight
    the winner of each month and tally the final score with the `value_counts` method.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Take a look at the DataFrame output from step 7\. Did you notice that the months
    are in alphabetical and not chronological order? Pandas unfortunately, in this
    case at least, orders the months for us alphabetically. We can solve this issue
    by changing the data type of `Month` to a categorical variable. Categorical variables
    map all the values of each column to an integer. We can choose this mapping to
    be the normal chronological order for the months. Pandas uses this underlying
    integer mapping during the `pivot` method to order the months chronologically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0c4324b4-8516-4239-bba6-9cd5b45d1893.png)'
  prefs: []
  type: TYPE_IMG
- en: To convert the `Month` column, use the `Categorical` constructor. Pass it the
    original column as a Series and a unique sequence of all the categories in the
    desired order to the `categories` parameter. As the `Month` column is already
    in chronological order, we can simply use the `unique` method, which preserves
    order to get the array that we desire. In general, to sort columns of object data
    type by something other than alphabetical, convert them to categorical.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas official documentation on `groupby` *Transformation* ([http://bit.ly/2vBkpA7](http://pandas.pydata.org/pandas-docs/stable/groupby.html#transformation))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy official documentation on the `where` function ([http://bit.ly/2weT21l](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating weighted mean SAT scores per state with apply
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The groupby object has four methods that accept a function (or functions) to perform
    a calculation on each group. These four methods are `agg`, `filter`, `transform`,
    and `apply`. Each of the first three of these methods has a very specific output
    that the function must return. `agg` must return a scalar value, `filter` must
    return a boolean, and `transform` must return a Series with the same length as
    the passed group. The `apply` method, however, may return a scalar value, a Series,
    or even a DataFrame of any shape, therefore making it very flexible. It is also
    called only once per group, which contrasts with `transform` and `agg` that get
    called once for each non-grouping column. The `apply` method's ability to return
    a single object when operating on multiple columns at the same time makes the
    calculation in this recipe possible.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we calculate the weighted average of both the math and verbal
    SAT scores per state from the college dataset. We weight the scores by the population
    of undergraduate students per school.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Read in the college dataset, and drop any rows that have missing values in
    either the `UGDS`, `SATMTMID`, or `SATVRMID` columns. We must have non-missing
    values for each of these three columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The vast majority of institutions do not have data for our three required columns,
    but this is still more than enough data to continue. Next, create a user-defined
    function to calculate the weighted average of just the SAT math scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Group by state and pass this function to the `apply` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We successfully returned a scalar value for each group. Let''s take a small
    detour and see what the outcome would have been by passing the same function to
    the `agg` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d02a23f5-aa1b-4a02-b5e6-5449ba33f6fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `weighted_math_average` function gets applied to each non-aggregating column
    in the DataFrame. If you try and limit the columns to just `SATMTMID`, you will
    get an error as you won''t have access to `UGDS`. So, the best way to complete
    operations that act on multiple columns is with `apply`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'A nice feature of `apply` is that you can create multiple new columns by returning
    a Series. The index of this returned Series will be the new column names. Let''s
    modify our function to calculate the weighted and arithmetic average for both
    SAT scores along with the count of the number of institutions from each group.
    We return these five values in a Series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3267d64e-37a3-4e90-9c9f-2aabdf1b0d26.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order for this recipe to complete properly, we need to first filter for institutions
    that do not have missing values for `UGDS`, `SATMTMID`, and `SATVRMID`. By default,
    the `dropna` method drops rows that have one or more missing values. We must use
    the `subset` parameter to limit the columns it looks at for missing values.
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, we define a function that calculates the weighted average for just
    the `SATMTMID` column. The weighted average differs from an arithmetic mean in
    that each value is multiplied by some weight. This quantity is then summed and
    divided by the sum of the weights. In this case, our weight is the undergraduate
    student population.
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we pass this function to the `apply` method. Our function `weighted_math_average`
    gets passed a DataFrame of all the original columns for each group. It returns
    a single scalar value, the weighted average of `SATMTMID`. At this point, you
    might think that this calculation is possible using the `agg` method. Directly
    replacing `apply` with `agg` does not work as `agg` returns a value for each of
    its aggregating columns.
  prefs: []
  type: TYPE_NORMAL
- en: It actually is possible to use `agg` indirectly by precomputing the multiplication
    of `UGDS` and `SATMTMID`.
  prefs: []
  type: TYPE_NORMAL
- en: Step 6 really shows the versatility of `apply`. We build a new function that
    calculates the weighted and arithmetic average of both SAT columns as well as
    the number of rows for each group. In order for `apply` to create multiple columns,
    you must return a Series. The index values are used as column names in the resulting
    DataFrame. You can return as many values as you want with this method.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the `OrderedDict` class was imported from the `collections` module,
    which is part of the standard library. This ordered dictionary is used to store
    the data. A normal Python dictionary could not have been used to store the data
    since it does not preserve insertion order.
  prefs: []
  type: TYPE_NORMAL
- en: The constructor, `pd.Series`, does have an index parameter that you can use
    to specify order but using an `OrderedDict` is cleaner.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we returned a single row as a Series for each group. It''s
    possible to return any number of rows and columns for each group by returning
    a DataFrame. In addition to finding just the arithmetic and weighted means, let''s
    also find the geometric and harmonic means of both SAT columns and return the
    results as a DataFrame with rows as the name of the type of mean and columns as
    the SAT type. To ease the burden on us, we use the NumPy function `average` to
    compute the weighted average and the SciPy functions `gmean` and `hmean` for geometric
    and harmonic means:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e4bf0560-11c6-4599-b1d5-e8f47cd2a215.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas official documentation of the `apply` groupby method ([http://bit.ly/2wmG9ki](http://pandas.pydata.org/pandas-docs/stable/groupby.html#flexible-apply))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python official documentation of the `OrderedDict` class ([http://bit.ly/2xwtUCa](https://docs.python.org/3/library/collections.html#collections.OrderedDict))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SciPy official documentation of its stats module ([http://bit.ly/2wHtQ4L](https://docs.scipy.org/doc/scipy/reference/stats.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping by continuous variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When grouping in pandas, you typically use columns with discrete repeating values.
    If there are no repeated values, then grouping would be pointless as there would
    only be one row per group. Continuous numeric columns typically have few repeated
    values and are generally not used to form groups. However, if we can transform
    columns with continuous values into a discrete column by placing each value into
    a bin, rounding them, or using some other mapping, then grouping with them makes
    sense.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the flights dataset to discover the distribution
    of airlines for different travel distances. This allows us, for example, to find
    the airline that makes the most flights between 500 and 1,000 miles. To accomplish
    this, we use the pandas `cut` function to discretize the distance of each flight
    flown.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Read in the flights dataset, and output the first five rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ed49e291-ded0-4c48-9451-d0f0cf9c47f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we want to find the distribution of airlines over a range of distances,
    we need to place the values of the `DIST` column into discrete bins. Let''s use
    the pandas `cut` function to split the data into five bins:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'An ordered categorical Series is created. To help get an idea of what happened,
    let''s count the values of each category:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cuts` Series can now be used to form groups. Pandas allows you to form
    groups in any way you wish. Pass the `cuts` Series to the `groupby` method and
    then call the `value_counts` method on the `AIRLINE` column to find the distribution
    for each distance group. Notice that SkyWest (*OO*) makes up 33% of flights less
    than 200 miles but only 16% of those between 200 and 500 miles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 2, the `cut` function places each value of the `DIST` column into one
    of five bins. The bins are created by a sequence of six numbers defining the edges.
    You always need one more edge than the number of bins. You can pass the `bins`
    parameter an integer, which automatically creates that number of equal-width bins.
    Negative infinity and positive infinity objects are available in NumPy and ensure
    that all values get placed in a bin. If you have values that are outside of the
    bin edges, they will be made missing and not be placed in a bin.
  prefs: []
  type: TYPE_NORMAL
- en: The `cuts` variable is now a Series of five ordered categories. It has all the
    normal Series methods and in step 3, the `value_counts` method is used to get
    a sense of its distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Very interestingly, pandas allows you to pass the `groupby` method any object.
    This means that you are able to form groups from something completely unrelated
    to the current DataFrame. Here, we group by the values in the `cuts` variable.
    For each grouping, we find the percentage of flights per airline with `value_counts` by
    setting `normalize` to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: Some interesting insights can be drawn from this result. Looking at the full
    result, SkyWest is the leading airline for under 200 miles but has no flights
    over 2,000 miles. In contrast, American Airlines has the fifth highest total for
    flights under 200 miles but has by far the most flights between 1,000 and 2,000
    miles.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can find more results when grouping by the `cuts` variable. For instance,
    we can find the 25th, 50th, and 75th percentile airtime for each distance grouping.
    As airtime is in minutes, we can divide by 60 to get hours:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this information to create informative string labels when using
    the `cut` function. These labels replace the interval notation. We can also chain
    the `unstack` method which transposes the inner index level to column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a44ef2f0-739b-4970-abba-c3e147aa0031.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas official documentation on the `cut` function ([http://bit.ly/2whcUkJ](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to [Chapter 14](a833ecb6-8487-4648-8632-860640490e01.xhtml), *Re**structuring
    Data into a Tidy For**m*, for many more recipes with unstack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counting the total number of flights between cities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the flights dataset, we have data on the origin and destination airport.
    It is trivial to count the number of flights originating in Houston and landing
    in Atlanta, for instance. What is more difficult is counting the total number
    of flights between the two cities, regardless of which one is the origin or destination.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we count the total number of flights between two cities regardless
    of which one is the origin or destination. To accomplish this, we sort the origin
    and destination airports alphabetically so that each combination of airports always
    occurs in the same order. We can then use this new column arrangement to form
    groups and then to count.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Read in the flights dataset, and find the total number of flights between each
    origin and destination airport:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the total number of flights between Houston (*IAH*) and Atlanta (*ATL*)
    in both directions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'We could simply sum these two numbers together to find the total flights between
    the cities but there is a more efficient and automated solution that can work
    for all flights. Let''s independently sort the origin and destination cities for
    each row in alphabetical order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/bacd986b-9e18-46ee-b46c-caf49b2b8385.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that each row has been independently sorted, the column names are not correct.
    Let''s rename them to something more generic and then again find the total number
    of flights between all cities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s select all the flights between Atlanta and Houston and verify that it
    matches the sum of the values in step 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'If we try and select flights with Houston followed by Atlanta, we get an error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we form groups by the origin and destination airport columns and
    then apply the `size` method to the groupby object, which simply returns the total
    number of rows for each group. Notice that we could have passed the string `size`
    to the `agg` method to achieve the same result. In step 2, the total number of
    flights for each direction between Atlanta and Houston are selected. The Series
    `flights_count` has a MultiIndex with two levels. One way to select rows from
    a MultiIndex is to pass the `loc` indexing operator a tuple of exact level values.
    Here, we actually select two different rows, `('ATL', 'HOU')` and `('HOU', 'ATL')`.
    We use a list of tuples to do this correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 is the most pertinent step in the recipe. We would like to have just
    one label for all flights between Atlanta and Houston and so far we have two.
    If we alphabetically sort each combination of origin and destination airports,
    we would then have a single label for flights between airports. To do this, we
    use the DataFrame `apply` method. This is different from the groupby `apply` method.
    No groups are formed in step 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DataFrame `apply` method must be passed a function. In this case, it''s
    the built-in `sorted` function. By default, this function gets applied to each
    column as a Series. We can change the direction of computation by using `axis=1`
    (or `axis=''index''`). The `sorted` function has each row of data passed to it
    implicitly as a Series. It returns a list of sorted airport codes. Here is an
    example of passing the first row as a Series to the sorted function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: The `apply` method iterates over all rows using `sorted` in this exact manner.
    After completion of this operation, each row is independently sorted. The column
    names are now meaningless. We rename the column names in the next step and then
    perform the same grouping and aggregating as was done in step 2\. This time, all
    flights between Atlanta and Houston fall under the same label.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might be wondering why we can't use the simpler `sort_values` Series method.
    This method does not sort independently and instead, preserves the row or column
    as a single record as one would expect while doing a data analysis. Step 3 is
    a very expensive operation and takes several seconds to complete. There are only
    about 60,000 rows so this solution would not scale well to larger data. Calling
    the
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 is a very expensive operation and takes several seconds to complete.
    There are only about 60,000 rows so this solution would not scale well to larger
    data. Calling the `apply` method with `axis=1` is one of the least performant
    operations in all of pandas. Internally, pandas loops over each row and does not
    provide any speed boosts from NumPy. If possible, avoid using `apply` with `axis=1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get a massive speed increase with the NumPy `sort` function. Let''s
    go ahead and use this function and analyze its output. By default, it sorts each
    row independently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'A two-dimensional NumPy array is returned. NumPy does not easily do grouping
    operations so let''s use the DataFrame constructor to create a new DataFrame and
    check whether it equals the `flights_sorted` DataFrame from step 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'As the DataFrames are the same, you can replace step 3 with the previous faster
    sorting routine. Let''s time the difference between each of the different sorting
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: The NumPy solution is an astounding 700 times faster than using `apply` with
    pandas.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NumPy official documentation on the `sort` function ([http://bit.ly/2vtRt0M](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sort.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the longest streak of on-time flights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important metrics for airlines is their on-time flight performance.
    The Federal Aviation Administration considers a flight delayed when it arrives
    at least 15 minutes later than its scheduled arrival time. Pandas has direct methods
    to calculate the total and percentage of on-time flights per airline. While these
    basic summary statistics are an important metric, there are other non-trivial
    calculations that are interesting, such as finding the length of consecutive on-time
    flights for each airline at each of its origin airports.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we find the longest consecutive streak of on-time flights for
    each airline at each origin airport. This requires each value in a column to be
    aware of the value immediately following it. We make clever use of the `diff`
    and `cumsum` methods in order to find streaks before applying this methodology
    to each of the groups.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get started with the actual flights dataset, let''s practice counting
    streaks of ones with a small sample Series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Our final representation of the streaks of ones will be a Series of the same
    length as the original with an independent count beginning from one for each streak.
    To get started, let''s use the `cumsum` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now accumulated all the ones going down the Series. Let''s multiply
    this Series by the original:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'We have only non-zero values where we originally had ones. This result is fairly
    close to what we desire. We just need to restart each streak at one instead of
    where the cumulative sum left off. Let''s chain the `diff` method, which subtracts
    the previous value from the current:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'A negative value represents the end of a streak. We need to propagate the negative
    values down the Series and use them to subtract away the excess accumulation from
    step 2\. To do this, we will make all non-negative values missing with the `where`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now propagate these values down with the `ffill` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can add this Series back to `s1` to clear out the excess accumulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a working consecutive streak finder, we can find the longest
    streak per airline and origin airport. Let''s read in the flights dataset and
    create a column to represent on-time arrival:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6e668d00-e72b-4851-a2ab-dfe2ecbd2739.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Use our logic from the first seven steps to define a function that returns
    the maximum streak of ones for a given Series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the maximum streak of on-time arrivals per airline and origin airport
    along with the total number of flights and percentage of on-time arrivals. First,
    sort the day of the year and scheduled departure time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3bd814fd-2141-4ccc-9b1b-334f35c85196.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding streaks in the data is not a straightforward operation in pandas and
    requires methods that look ahead or behind, such as `diff` or `shift`, or those
    that remember their current state, such as `cumsum`. The final result from the
    first seven steps is a Series the same length as the original that keeps track
    of all consecutive ones. Throughout these steps, we use the `mul` and `add` methods
    instead of their operator equivalents (`*`) and (`+`). In my opinion, this allows
    for a slightly cleaner progression of calculations from left to right. You, of
    course, can replace these with the actual operators.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we would like to tell pandas to apply the `cumsum` method to the start
    of each streak and reset itself after the end of each one. It takes many steps
    to convey this message to pandas. Step 2 accumulates all the ones in the Series
    as a whole. The rest of the steps slowly remove any excess accumulation. In order
    to identify this excess accumulation, we need to find the end of each streak and
    subtract this value from the beginning of the next streak.
  prefs: []
  type: TYPE_NORMAL
- en: To find the end of each streak, we cleverly make all values not part of the
    streak zero by multiplying `s1` by the original Series of zeros and ones in step
    3\. The first zero following a non-zero, marks the end of a streak. That's good,
    but again, we need to eliminate the excess accumulation. Knowing where the streak
    ends doesn't exactly get us there.
  prefs: []
  type: TYPE_NORMAL
- en: In step 4, we use the `diff` method to find this excess. The `diff` method takes
    the difference between the current value and any value located at a set number
    of rows away from it. By default, the difference between the current and the immediately
    preceding value is returned.
  prefs: []
  type: TYPE_NORMAL
- en: Only negative values are meaningful in step 4\. Those are the ones immediately
    following the end of a streak. These values need to be propagated down until the
    end of the following streak. To eliminate (make missing) all the values we don't
    care about, we use the `where` method, which takes a Series of conditionals of
    the same size as the calling Series. By default, all the `True` values remain
    the same, while the `False` values become missing. The `where` method allows you
    to use the calling Series as part of the conditional by taking a function as its
    first parameter. An anonymous function is used, which gets passed the calling
    Series implicitly and checks whether each value is less than zero. The result
    of step 5 is a Series where only the negative values are preserved with the rest
    changed to missing.
  prefs: []
  type: TYPE_NORMAL
- en: The `ffill` method in step 6 replaces missing values with the last non-missing
    value going forward/down a Series. As the first three values don't follow a non-missing
    value, they remain missing. We finally have our Series that removes the excess
    accumulation. We add our accumulation Series to the result of step 6 to get the
    streaks all beginning from zero. The `add` method allows us to replace the missing
    values with the `fill_value` parameter. This completes the process of finding
    streaks of ones in the dataset. When doing complex logic like this, it is a good
    idea to use a small dataset where you know what the final output will be. It would
    be quite a difficult task to start at step 8 and build this streak-finding logic
    while grouping.
  prefs: []
  type: TYPE_NORMAL
- en: In step 8, we create the `ON_TIME` column. One item of note is that the cancelled
    flights have missing values for `ARR_DELAY`, which do not pass the boolean condition
    and therefore result in a zero for the `ON_TIME` column. Canceled flights are
    treated the same as delayed.
  prefs: []
  type: TYPE_NORMAL
- en: Step 9 turns our logic from the first seven steps into a function and chains
    the `max` method to return the longest streak. As our function returns a single
    value, it is formally an aggregating function and can be passed to the `agg` method
    as done in step 10\. To ensure that we are looking at actual consecutive flights,
    we use the `sort_values` method to sort by date and scheduled departure time.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have found the longest streaks of on-time arrivals, we can easily
    find the opposite--the longest streak of delayed arrivals. The following function
    returns two rows for each group passed to it. The first row is the start of the
    streak, and the last row is the end of the streak. Each row contains the month
    and day that the streak started/ended, along with the total streak length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2f18b944-147e-4743-aa76-2e1ee9800842.png)'
  prefs: []
  type: TYPE_IMG
- en: As we are using the `apply` groupby method, a DataFrame of each group is passed
    to the `max_delay_streak` function. Inside this function, the index of the DataFrame
    is dropped and replaced by a `RangeIndex` in order for us to easily find the first
    and last row of the streak. The `ON_TIME` column is inverted and then the same
    logic is used to find streaks of delayed flights. The index of the first and last
    rows of the streak are stored as variables. These indexes are then used to select
    the month and day when the streaks ended. We use a DataFrame to return our results.
    We label and name the index to make the final result clearer.
  prefs: []
  type: TYPE_NORMAL
- en: Our final results show the longest delayed streaks accompanied by the first
    and last date. Let's investigate to see if we can find out why these delays happened.
    Inclement weather is a common reason for delayed or canceled flights. Looking
    at the first row, American Airlines (AA) started a streak of 38 delayed flights
    in a row from the Dallas Fort-Worth (DFW) airport beginning February 26 until
    March 1 of 2015\. Looking at historical weather data from February 27, 2015, two
    inches of snow fell, which was a record for that day ([http://bit.ly/2iLGsCg](http://bit.ly/2iLGsCg)).
    This was a major weather event for DFW and caused massive problems for the entire
    city ([http://bit.ly/2wmsHPj](http://bit.ly/2wmsHPj)). Notice that DFW makes another
    appearance as the third longest streak but this time a few days earlier and for
    a different airline.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas official documentation of `ffill` ( [http://bit.ly/2gn5zGU](http://bit.ly/2gn5zGU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
