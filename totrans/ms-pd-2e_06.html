<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">I/Os of Different Data Formats with pandas</h1>
                </header>
            
            <article>
                
<p class="chapter-content"><span><span>A</span></span> data scientist has to work on data that comes from a variety of sources and hence in a variety of formats. The most common are the ubiquitous spreadsheets, Excel sheets, and <kbd>CSV</kbd> and text files. But there are many others, such as <kbd>URL</kbd>, <kbd>API</kbd>, <kbd>JSON</kbd>, <kbd>XML</kbd>, <kbd>HDF</kbd>, <kbd>Feather</kbd><span> and so on, depending on where it is being accessed. In this chapter, we will cover the following topics among others:</span></p>
<ul>
<li class="chapter-content">Data sources and pandas methods</li>
<li>CSV and TXT</li>
<li>URL and S3</li>
<li>JSON</li>
<li>Reading HDF formats</li>
</ul>
<p>Let's get started!</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data sources and pandas methods</h1>
                </header>
            
            <article>
                
<p class="chapter-content">The data sources for a data science project can be clubbed into the following categories:</p>
<ul>
<li><strong>Databases</strong>: Most of the CRM, ERP, and other business operations tools store data in a database. Depending on the volume, velocity, and variety, it can be a traditional or NoSQL database. To connect to most of the popular databases, we need <kbd>JDBC/ODBC</kbd> drivers from Python. Fortunately, there are such drivers available for all the popular databases. Working with data in such databases involves making a connection through Python to these databases, querying the data through Python, and then manipulating it using pandas. We will look at an example of how to do this later in this chapter.</li>
<li><strong>Web services</strong>: Many of the business operations tools, especially <strong>Software as a Services</strong> (<strong><span>SaaS</span></strong>) tools, make their data accessible through <strong>Application Programming Interfaces</strong> (<span><strong>APIs</strong>) </span>instead of a database. This reduces the infrastructure cost of hosting a database permanently. Instead, data is made available as a service, as and when required. An <kbd>API</kbd> call can be made through Python, which returns packets of data in formats such as <kbd>JSON</kbd> or <kbd>XML</kbd>. This data is parsed and then manipulated using pandas for further usage.</li>
<li><strong>Data files</strong>: A lot of data for prototyping data science models comes as data files. One example of data being stored as a physical file is the data from IoT sensors <span>– </span>more often than not, the data from these sensors is stored in a flat file, a <kbd>.txt</kbd> file, or a <kbd>.csv</kbd> file. Another source for a data file is the sample data that's been extracted from a database and stored in such files. The output of many data science and machine learning algorithms are also stored in such files, such as CSV, Excel, and <kbd>.txt</kbd> files. Another example is that the trained weight matrices of a deep learning neural network model can be stored as an HDF file.</li>
<li><strong>Web and document scraping</strong>: Two other sources of data are the tables and text present on web pages. This data is gleaned from these pages using Python packages such as BeautifulSoup and Scrapy and are put into a data file or database to be used further. The tables and data that are present in another non-data format file, such as PDF or Docs, are also a major source of data. This is then extracted using Python packages such as Tesseract and Tabula-py.</li>
</ul>
<p class="chapter-content">In this chapter, we will look at how to read and write data to and from these formats/sources using pandas and ancillary libraries. We will also discuss a little bit about these formats, their utilities, and various operations that can be performed on them.</p>
<p>The following is a summary of the read and write methods in Python for some of the data formats we are going to discuss in this chapter: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8abf637c-9d15-45c5-b8e7-fdef295dd9d5.png" style="width:49.42em;height:21.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"> Reader and writer methods in pandas for different types of data file formats and their sources</div>
<p>The section headers mean that we are dealing with I/O operations of that file type in that section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CSV and TXT</h1>
                </header>
            
            <article>
                
<p>CSV stands for comma-separated values, which means that the comma is the default delimiter for these files. However, they accept other delimiters as well.</p>
<p>CSVs are made of columns and rows and the cell value is arranged in a tabular format. They can come with or without column names and row indices. The primary reasons for a CSV file's existence include manually gathered data, data that's been extracted and downloaded from a database, a direct download from a tool or website, web scraping, and the result of running a data science algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading CSV and TXT files</h1>
                </header>
            
            <article>
                
<p><kbd>read_csv</kbd> is the go-to method for reading CSV files in <kbd>pandas</kbd>. It can also be used to read <kbd>txt</kbd> files. The syntax of using <kbd>read_csv</kbd> is shown in the following code:</p>
<pre>pd.read_csv(filepath, sep=', ', dtype=None, header=None, names=None, skiprows=None, index_col=None, skip_blank_lines=TRUE, na_filter=TRUE)</pre>
<p>The parameters of the <kbd>read_csv</kbd> method are as follows:</p>
<ul>
<li><kbd>filepath</kbd>: A string or filename with or without a filepath.</li>
<li><kbd>dtype</kbd>: Can be passed as a dictionary containing name and type as a key-value pair. Specifies the data type of the column name. Generally, pandas guesses the type of columns based on the first few rows.</li>
<li><kbd>header</kbd>: True/False. This specifies whether the first row in the data is a header or not.</li>
<li><kbd>names</kbd>: List. Specifies column names for all the columns of a dataset.</li>
<li><kbd>skiprows</kbd>: List. Skip certain rows of data by specifying row indices.</li>
<li><kbd>index_col</kbd>: Series/List. Specifies the column that can work as a row number/identifier.</li>
<li><kbd>skip_blank_lines</kbd>: True/False. Specifies whether to skip blank lines or not.</li>
<li><kbd>na_filter</kbd>: True/False. Specifies whether to filter NA values or not.</li>
<li><kbd>usecols</kbd>: List. Returns the subset of data with columns in the passed list.</li>
</ul>
<p>The <kbd>read_csv</kbd> method returns a DataFrame. The following are some examples of reading files using the <kbd>read_csv</kbd> method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading a CSV file</h1>
                </header>
            
            <article>
                
<p>We can read a CSV file by using the following code: </p>
<pre>import pandas as pd<br/>import os<br/>os.chdir(' ')<br/>data=pd.read_csv('Hospital Cost.csv')</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Specifying column names for a dataset</h1>
                </header>
            
            <article>
                
<p>The following code will specify the column names for a dataset:</p>
<pre>column_names=pd.read_csv('Customer Churn Columns.csv')<br/>column_names_list=column_names['Column Names'].tolist()<br/>data=pd.read_csv('Customer Churn Model.txt',header=None,names=column_names_list)</pre>
<p>Note that the column names are read from a file and then converted into a list to be passed to the names parameter in <kbd>read_csv</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading from a string of data</h1>
                </header>
            
            <article>
                
<p><span>Here's how we can use <kbd>read_csv</kbd> to create a DataFrame from a list of strings:</span></p>
<pre>from io import StringIO<br/>data = 'col1,col2,col3\na,b,1\na,b,2\nc,d,3\nc,e,4\ng,f,5\ne,z,6'<br/>pd.read_csv(StringIO(data))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Skipping certain rows</h1>
                </header>
            
            <article>
                
<p>We can also skip certain rows. Let's say that we only want the rows whose indices are multiples of 3: </p>
<pre>from io import StringIO<br/>data = 'col1,col2,col3\na,b,1\na,b,2\nc,d,3\nc,e,4\ng,f,5\ne,z,6'<br/>pd.read_csv(StringIO(data),skiprows=lambda x: x % 3 != 0)</pre>
<p>We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1810 image-border" src="assets/7e5b5fa6-3e3b-4995-a8db-66caaf0727eb.png" style="width:18.83em;height:14.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Demonstration of using the skiprows parameter in read_csv. The right-hand panel shows the data that's been filtered through skiprows (keeping only rows with row numbers that are multiples of 3)</div>
<p>The left-hand side diagram shows the resultant DataFrame without skipping any row, while the right-hand side shows the same DataFrame after filtering the rows whose indices are not multiples of 3. Note that this method considers the real index (3rd and 6th from the top, starting from 1) and not the Python index (starting from 0) for filtering the rows based on their index.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Row index</h1>
                </header>
            
            <article>
                
<p>If a file has one more column of data than the number of column names, the first column will be used as the DataFrame's row names:</p>
<pre>data = 'a,b,c\n4,apple,bat,5.7\n8,orange,cow,10'<br/>pd.read_csv(StringIO(data), index_col=0)</pre>
<p>We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8e787128-6e43-4df6-a46d-728f1e129fb9.png" style="width:11.58em;height:6.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The column with values but no corresponding column name is used as a row index.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading a text file</h1>
                </header>
            
            <article>
                
<p><kbd>read_csv</kbd> can help read text files as well. Often, data is stored in <kbd>.txt</kbd> files with different kinds of delimiters. The <kbd>sep</kbd> parameter can be used to specify the delimiter of a particular file, as shown in the following code:</p>
<pre>data=pd.read_csv('Tab Customer Churn Model.txt',sep='/t')</pre>
<p>The preceding file has <kbd>Tab</kbd> as a delimiter, which is specified using the <kbd>sep</kbd> parameter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Subsetting while reading</h1>
                </header>
            
            <article>
                
<p>Only a selected list of columns can be subsetted and loaded using the <kbd>usecols</kbd> parameter while reading: </p>
<pre>data=pd.read_csv('Tab Customer Churn Model.txt',sep='/t',usecols=[1,3,5])<br/>data=pd.read_csv('Tab Customer Churn Model.txt',sep='/t',usecols=['VMail Plan','Area Code'])</pre>
<p>Numeric lists, as well as explicit lists with column names, can be used. Numeric indexing follows Python indexing, that is, starting from 0.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading thousand format numbers as numbers</h1>
                </header>
            
            <article>
                
<p>If a dataset contains a numeric column that has thousand numbers formatted by a comma or any other delimiter, the default data type for such a column is a string or object. The problem is that it is actually a numeric field and it needs to be read as a numeric field to be used further:</p>
<pre>pd.read_csv('tmp.txt',sep='|')</pre>
<p>We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a1ae239f-3e96-45a6-85a3-47f6f43ba54d.png" style="width:16.25em;height:7.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Data with a level column with thousand format numbers</div>
<pre>data.level.dtype returns dtype('O')</pre>
<p>To overcome this problem, the <kbd>thousands</kbd> parameter can be used while reading:</p>
<pre>pd.read_csv('tmp.txt',sep='|',thousands=',')<br/>data.level.dtype now returns dtype('int64')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Indexing and multi-indexing</h1>
                </header>
            
            <article>
                
<p><kbd>index_col</kbd> can be used to specify one column to provide row indices. A list of columns can be passed as indices, which leads to multi-indexing. Let's look at an example:</p>
<pre>pd.read_csv('mindex.txt')<br/>pd.read_csv('mindex.txt',index_col=[0,1])</pre>
<p class="mce-root"/>
<p>Take a look at the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1832 image-border" src="assets/c22b0911-c5a3-40af-9b87-92c6b3161b6d.png" style="width:23.75em;height:25.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Single index (left) and multi-index (right) on the same data</div>
<p>This kind of multi-indexing makes it easy to subset based on either an index or both:</p>
<pre>data.loc[1977]<br/>data.loc[(1977,'A')]</pre>
<p>We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1812 image-border" src="assets/4752a1fa-6e39-4f44-80b3-529f1880ba9e.png" style="width:21.50em;height:9.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Subsetting multi-indexed data using one index (left) and both indices (right)</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading large files in chunks</h1>
                </header>
            
            <article>
                
<p>Reading a large file in memory at once may consume the entire RAM of the computer and may cause it to throw an error. In such cases, it becomes pertinent to divide the data into chunks. These chunks can then be read sequentially and processed. This is achieved by using the <kbd>chunksize</kbd> parameter in <kbd>read_csv</kbd>.</p>
<p>The resulting chunks can be iterated over using a for loop. In the following code, we are printing the shape of the chunks:</p>
<pre>for chunks in pd.read_csv('Chunk.txt',chunksize=500):<br/>    print(chunks.shape)</pre>
<p>These chunks can then be concatenated to each other using the <kbd>concat</kbd> method:</p>
<pre>data=pd.read_csv('Chunk.txt',chunksize=500)<br/>data=pd.concat(data,ignore_index=True)<br/>print(data.shape)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling delimiter characters in column data</h1>
                </header>
            
            <article>
                
<p>Sometimes, a column separate character is present as part of the data in one of the columns. This leads to incorrectly parsing data as this would split the column that was supposed to be read as one into two. To avoid such a situation, a quote character should be used around the data in the specified columns. This quote character forces <kbd>read_csv</kbd> to ignore the delimiter for the data that's present inside the quote character and not break it into two pieces.</p>
<p>The quote characters can be specified using the <kbd>quotechar</kbd> argument of <kbd>read_csv</kbd>. For example, consider the following dataset. Here, white space is used as a delimiter and double quotes have been used as a grouping element:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c343c737-1b11-46a8-9dd4-f8dbb4d2f424.png" style="width:30.00em;height:3.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"> Usage of quotechar keyword 1—input dataset</div>
<p>To parse this, we would use the following code:</p>
<pre>d1 = <br/>pd.read_csv('t1.txt',index_col=0, delim_whitespace=True,quotechar="\"")<br/>d1.head()</pre>
<p class="mce-root"/>
<p>We would get the following output:</p>
<p class="CDPAlignCenter CDPAlign">  <img src="assets/5e92e0f9-5111-4605-bc3a-9110692a106d.png" style="width:20.33em;height:9.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Usage of quotechar keyword 2—output dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing to a CSV</h1>
                </header>
            
            <article>
                
<p>A DataFrame is an in-memory object. Often, DataFrames need to be saved as physical files for later use. In such cases, the DataFrames can be written as a <kbd>CSV</kbd> or <kbd>TXT</kbd> file.</p>
<p>Let's create a synthesized DataFrame using random numbers:</p>
<pre class="mce-root">import numpy as np<br/>import pandas as pd<br/>a=['Male','Female']<br/>b=['Rich','Poor','Middle Class']<br/>gender=[]<br/>seb=[]<br/><br/>for i in range(1,101):<br/>    gender.append(np.random.choice(a))<br/>    seb.append(np.random.choice(b))<br/>    height=30*np.random.randn(100)+155<br/>    weight=20*np.random.randn(100)+60<br/>    age=10*np.random.randn(100)+35<br/>    income=1500*np.random.randn(100)+15000<br/><br/>df=pd.DataFrame({'Gender':gender,'Height':height,'Weight':weight,'Age':age,'Income':income,'Socio-Eco':seb})</pre>
<p>This can be written to a <kbd>.csv</kbd> or <kbd>.txt</kbd> file using the <kbd>to_csv</kbd> method, as shown in the following code:</p>
<pre>df.to_csv('data.csv')<br/>df.to_csv('data.txt')</pre>
<p>These files would be written to the current working directory.</p>
<p>A delimiter of choice can be provided while writing to a file:</p>
<pre>df.to_csv('data.csv',sep='|')<br/>df.to_csv('data.txt',sep='/')</pre>
<p>There are many other useful options available, such as the following: </p>
<ul>
<li><kbd>index</kbd>: True/False. Indicates whether we should have row indices or not.</li>
<li><kbd>index_label</kbd>: String/Column name. Column to be used as a row index.</li>
<li><kbd>header</kbd>: True/False. Specifies whether to write the column names.</li>
<li><kbd>na_rep</kbd>: String. A string representation for missing values.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Excel</h1>
                </header>
            
            <article>
                
<p>Excel files are similar to <kbd>CSV</kbd> files but are different in the sense that they can have multiple sheets, formatted data and tables, charts, and formulas. In many cases, reading data from Excel files is required.</p>
<p><kbd>xlrd</kbd> is the package of choice while working with Excel sheets. Some of the major functionalities of the <kbd>xlrd</kbd> package are summarized in the following table:</p>
<table style="border-collapse: collapse" class="table" border="1">
<tbody>
<tr>
<td>
<p><strong>Code snippet</strong></p>
</td>
<td>
<p><strong>Goal achieved</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>import xlrd</kbd></p>
</td>
<td>
<p>Importing the xlrd library</p>
</td>
</tr>
<tr>
<td>
<p><kbd>book=xlrd.open_workbook('SRS Career.xlsx')</kbd></p>
</td>
<td>
<p>Reading the Excel workbook</p>
</td>
</tr>
<tr>
<td>
<p><kbd>n=book.nsheets</kbd></p>
</td>
<td>
<p>Finding the number of sheets in a workbook</p>
</td>
</tr>
<tr>
<td>
<p><kbd>book.sheet_names()</kbd></p>
</td>
<td>
<p>Finding the names of sheets in a workbook</p>
</td>
</tr>
<tr>
<td>
<p><kbd>last_sheet=book.sheet_by_index(n-1)</kbd></p>
</td>
<td>
<p>Reading the sheets by sheet index</p>
</td>
</tr>
<tr>
<td>
<p><kbd>last_sheet.row_values(0)</kbd></p>
</td>
<td>
<p>Getting the first row of a sheet</p>
</td>
</tr>
<tr>
<td>
<p><kbd>last_sheet.cell(0,0)</kbd></p>
</td>
<td>
<p>Getting the first cell of the sheet</p>
</td>
</tr>
<tr>
<td>
<p><kbd>last_sheet.row_slice(rowx=0,start_colx=1,end_colx=5)</kbd></p>
</td>
<td>
<p>Getting the 1<sup>st</sup> to the 5<sup>th</sup> columns of the first row</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">URL and S3</h1>
                </header>
            
            <article>
                
<p>Sometimes, the data is directly available as a URL. In such cases, <kbd>read_csv</kbd> can be directly used to read from these URLs:</p>
<pre>pd.read_csv('http://bit.ly/2cLzoxH').head()</pre>
<p>Alternatively, to work with URLs in order to get data, we can use a couple of Python packages that we haven't used so far, such as <kbd>.csv</kbd> and <kbd>.urllib</kbd>. It would suffice to know that <kbd>.csv</kbd> provides a range of methods for handling <kbd>.csv</kbd> files and that <kbd>urllib</kbd> is used to navigate to and access information from the URL. Here is how we can do this:</p>
<pre>import csv 
import urllib2 
 
url='http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' 
response=urllib2.urlopen(url) 
cr=csv.reader(response) 
 
for rows in cr: 
   print rows 
 </pre>
<p><kbd>AWS S3</kbd> is a popular file-sharing and storage repository on the web. Many enterprises store their business operations data as files on S3, which needs to be read and processed directly or be moved to a database. Python allows us to directly read files from S3, as shown in the following code.</p>
<p>Python 3.4 and above use the <kbd>s3fs</kbd> package in addition to pandas to read files directly from S3. An AWS config file needs to be placed in the current working directory. The bucket name, as well as the path and filename, need to be passed for reading:</p>
<pre>import os 
import pandas as pd 
from s3fs.core import S3FileSystem 
 
os.environ['AWS_CONFIG_FILE'] = 'aws_config.ini' 
 
s3 = S3FileSystem(anon=False) 
key = 'path\to\your-csv.csv' 
bucket = 'your-bucket-name' 
 
df = pd.read_csv(s3.open('{}/{}'.format(bucket, key), 
                         mode='rb') 
                 ) </pre>
<p>A DataFrame can be written to a CSV file and saved directly in S3 as follows: </p>
<pre> import s3fs 
 
bytes_to_write = df.to_csv(None).encode() 
fs = s3fs.S3FileSystem(key=key, secret=secret) 
with fs.open('s3://bucket/path/to/file.csv', 'wb') as f: 
    f.write(bytes_to_write) 
 </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">HTML</h1>
                </header>
            
            <article>
                
<p>HTML is the popular file format for creating and wrapping web elements and pages. Sometimes, tabular data is stored in a file. In such cases, the <kbd>read_html</kbd> method is directly used to read such data. This function parses table elements from HTML files and reads the tables as DataFrames:</p>
<pre>pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html') </pre>
<p>You can find all of the table elements containing a particular match word by using the following code:</p>
<pre> match = 'Malta National Bank' 
df_list = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html', match=match) </pre>
<p>A DataFrame can be converted into an HTML table element so that it can be placed into an HTML file like so:</p>
<pre>data=pd.read_csv('http://bit.ly/2cLzoxH')<br/>print(data.to_html())</pre>
<p>We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/99e747e1-7272-4977-8aad-2b2550d77cb7.png" style="width:18.00em;height:19.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">HTML table element created from a DataFrame</div>
<p>A selected list of columns can be filtered and converted into HTML like so:</p>
<pre> print(data.to_html(columns=['country','year'])) </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing to an HTML file</h1>
                </header>
            
            <article>
                
<p>The HTML file can be saved as a physical file like so: </p>
<pre>data=pd.read_csv('http://bit.ly/2cLzoxH') 
print(data.to_html('test.html')) </pre>
<p>Take a look at the following screenshot:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/6f0d904f-7480-42a1-a071-7acbf2a42ad1.png" style="width:32.00em;height:19.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Subsetting multi-indexed data using one index (left) and both indices (right)</div>
<p class="mce-root">The row and column names are bold by default. This can be changed with the following code: </p>
<pre> data.to_html('test.html',bold_rows=False) </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">JSON</h1>
                </header>
            
            <article>
                
<p><kbd>JSON</kbd> is a popular dictionary-like, key-value pair-based data structure that's suitable for exposing data as APIs from SaaS tools. <kbd>address</kbd>, <kbd>postalCode</kbd>, <kbd>state</kbd>, <kbd>streetAddress</kbd>, <kbd>age</kbd>, <kbd>firstName</kbd>, <kbd>lastName</kbd>, and <kbd>phoneNumber</kbd> are keys whose values are shown to the right of them. <kbd>JSON</kbd> files can be nested (the values of a key are JSON) as well. Here, <kbd>address</kbd> has nested values: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b14ed272-db57-4fba-9a57-9c9cea728544.png" style="width:30.83em;height:8.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Example of JSON data (dictionary; key-value pairs)</div>
<p>DataFrames can be converted into JSON using <kbd>to_json</kbd>:</p>
<pre>import numpy as np 
pd.DataFrame(np.random.randn(5, 2), columns=list('AB')).to_json() "</pre>
<p class="mce-root">Take a look at the following screenshot:</p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><img src="assets/502b72f1-18c4-4307-ab86-8fd00f5b27d8.png"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Converting a DataFrame into JSON format</div>
<p>While converting the DataFrame into a JSON file, the orientation can be set.</p>
<p>If we want to keep the column name as the primary index and the row indices as the secondary index, then we can choose the orientation to be <kbd>columns</kbd>:</p>
<pre class="mce-root">dfjo = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),columns=list('ABC'), index=list('xyz')) 
dfjo.to_json(orient="columns")</pre>
<p>We receive the following output:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/016ceada-6767-443e-8442-efbebb525bb9.png" style="width:35.08em;height:1.67em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Converting a DataFrame into JSON with the column orientation </div>
<p class="mce-root">If we want to keep the row indices as the primary index and the column names as the secondary index, then we can choose the orientation to be <kbd>index</kbd>: </p>
<pre class="mce-root">dfjo = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),columns=list('ABC'), index=list('xyz')) 
dfjo.to_json(orient="index")</pre>
<p>We receive the following output:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/9f1817dc-3dee-4536-8d37-613c445ee872.png" style="width:34.25em;height:1.92em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Converting a DataFrame into JSON with the index orientation</div>
<p class="mce-root">Another option is to convert a DataFrame into an array of JSONs. This is useful while passing data to a visualization library, like so:</p>
<pre class="mce-root">d3.js. dfjo = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),columns=list('ABC'), index=list('xyz')) dfjo.to_json(orient="records")</pre>
<p>We receive the following output:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/3f25c39c-f779-4804-8e0e-e03ee72f54ea.png" style="width:27.75em;height:1.50em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Converting a DataFrame into JSON with the records orientation</div>
<p class="mce-root">We can also contain the bare-bones values as a list of values, without any row or column index: </p>
<pre class="mce-root">dfjo = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),columns=list('ABC'), index=list('xyz')) dfjo.to_json(orient="values") </pre>
<p><span>We receive the following output:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/b3ea3f67-3fe1-4968-bfd3-302fcc8198d0.png" style="width:11.50em;height:1.17em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Converting a DataFrame into JSON with the values orientation</div>
<p class="mce-root">Finally, we can also orient the converted JSON in order to separate the row indices, column names, and data values:</p>
<pre class="mce-root">dfjo = pd.DataFrame(dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),columns=list('ABC'), index=list('xyz')) dfjo.to_json(orient="split")</pre>
<p><span>We receive the following output:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/69ac8ae1-0481-40fa-a39f-dcbc19b84268.png" style="width:38.92em;height:1.83em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Converting a DataFrame into JSON with the split orientation</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing a JSON to a file</h1>
                </header>
            
            <article>
                
<p class="mce-root">JSON can be written to physical files like so:</p>
<pre class="mce-root">import json with open('jsonex.txt','w') as outfile: json.dump(dfjo.to_json(orient="columns"), outfile)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading a JSON</h1>
                </header>
            
            <article>
                
<p class="mce-root"><kbd>json_loads</kbd> is used to read a physical file containing JSONs:</p>
<pre class="mce-root">f=open('usagov_bitly.txt','r').readline() json.loads(f) </pre>
<p>We get the following output:</p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><img src="assets/04b51282-c183-492b-a784-6eeeec586db7.png"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">First record in a list of JSONs</div>
<p class="mce-root CDPAlignLeft CDPAlign">The files can be read one JSON at a time using the <kbd>open</kbd> and <kbd>readline</kbd> methods: </p>
<pre class="mce-root CDPAlignLeft CDPAlign">records=[] f=open('usagov_bitly.txt','r') for i in range(1000): fiterline=f.readline() d=json.loads(fiterline) records.append(d) f.close()</pre>
<p class="mce-root CDPAlignLeft CDPAlign">Now, <kbd>records</kbd> contains a list of JSONs from which all the values of a particular key can be pulled out. For example, here, we are  pulling out all the <kbd>latlong</kbd> (<kbd>'ll'</kbd> column) wherever it has a non-zero value: </p>
<pre class="mce-root CDPAlignLeft CDPAlign">latlong=[rec['ll'] for rec in records if 'll' in rec]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing JSON to a DataFrame</h1>
                </header>
            
            <article>
                
<p class="mce-root">A list of JSON objects can be converted into a DataFrame (much like a dictionary can). The records element we created previously is a list of JSONs (we can check this by using <kbd>records[0:3]</kbd> or <kbd>type(records)</kbd>):</p>
<pre class="mce-root">df=pd.DataFrame(records) <br/>df.head() <br/>df['tz'].value_counts() </pre>
<p class="mce-root">In the last line, we are trying to find the count of different time zones contained in the <kbd>'tz'</kbd> column.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Subsetting a JSON</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's have a look at a new JSON file:</p>
<pre class="mce-root"> with open('product.json') as json_string: <br/>    d=json.load(json_string) <br/>d </pre>
<p>We get the following output:</p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><img src="assets/2d6b3b39-b011-49be-a8ed-edfd9191c8e3.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Loading a JSON file with several degrees of nesting</div>
<p class="mce-root">This is a JSON with several degrees of nesting. The <kbd>hits</kbd> key contains a JSON as a value whose key value is <kbd>hits</kbd>. The value of this JSON is a list containing another JSON.</p>
<p class="mce-root">Let's say that we want to find out the score value from this JSON:</p>
<pre class="mce-root CDPAlignLeft CDPAlign"> d['hits']['hits'][0]['_score'] </pre>
<p class="mce-root CDPAlignLeft CDPAlign">Similarly, the image URL can be found as follows:</p>
<pre class="mce-root CDPAlignLeft CDPAlign"> d['hits']['hits'][0]['_source']['r']['wayfair']['image_url']</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looping over JSON keys</h1>
                </header>
            
            <article>
                
<p class="mce-root">JSON data can be looped on its keys and values:</p>
<pre class="mce-root">for keys,values in d['hits']['hits'][0].items(): <br/>print(keys)</pre>
<p>We receive the following output:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/5f5e5349-afc0-410e-845e-10fe1314ea0e.png" style="width:4.42em;height:5.42em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Printing keys of the loaded JSON by looping over the keys and values</div>
<p>We can print both the keys and values together as well.</p>
<pre class="mce-root">for keys,values in d['hits']['hits'][0].items():<br/> print(keys,values) </pre>
<p>We receive the following output:</p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><img src="assets/bb4bd0e2-2b88-4120-a6e4-1bc8d5ef1f16.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Printing keys and values of the loaded JSON by looping over the keys and values</div>
<p>Now, we will look at how to use reading and writing operations with exotic file formats. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading HDF formats</h1>
                </header>
            
            <article>
                
<p>The <strong>Hierarchical Data Format</strong> (<strong>HDF</strong>) is efficient in handling large and complex data models. The versatility and flexibility of HDF in data storage make it a sought after format for storing scientific data. In fact, HDF was selected as the standard data and information system by NASA, for use in the Earth Observing System. HDF5 is the current technological suite used by the HDF file format and replaced the older HDF4.</p>
<p>The following are some unique features of HDF5:</p>
<ul>
<li>HDF5 has no set limits regarding file size and the objects in the file.</li>
<li>HDF5 can group and link objects in the file, thereby facilitating as a supportive mechanism for complex relationships and dependencies in data.</li>
<li>HDF5 also supports metadata.</li>
<li>While accommodating a variety of predefined and user-defined data types, HDF5 also has the ability to store and share data type descriptions in HDF files.</li>
</ul>
<p>For efficiency in the data transfer process, HDF5 incorporates Standard (Posix), Parallel, and Network I/O file drivers. Additional file drivers can also be developed and integrated with HDF5 for any custom data transfer and storage requirements. HDF5 makes data storage more optimized through techniques such as compression, extensibility, and chunking. Being able to perform data transformations, make changes to data types, and select subsets of data during data transfer makes the reading and writing processes efficient.</p>
<p>Now, let's read an HDF file using pandas:</p>
<pre>pd.read_hdf('stat_df.h5','table')</pre>
<p>We receive the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d925a312-cffd-4dd2-8e60-44d2d1fad99c.png" style="width:25.42em;height:13.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Output of read_hdf</div>
<p>A subset of the data could be extracted during the reading process using the index argument:</p>
<pre>pd.read_hdf('stat_df.h5', 'table', where=['index&gt;=2'])</pre>
<p>We receive the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/743debf2-53b8-4f10-b761-dada60221e07.png" style="width:23.75em;height:8.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Output of read_hdf with indexing</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading feather files</h1>
                </header>
            
            <article>
                
<p>The feather format is a binary file format for storing data that makes use of Apache Arrow, an in-memory columnar data structure. It was developed by Wes Mckinney and Hadley Wickham, chief scientists at RStudio as an initiative for a data sharing infrastructure across Python and R. The columnar serialization of data in feather files makes way for efficient read and write operations, making it far faster than CSV and JSON files where storage is record-wise.</p>
<p>Feather files have the following features:</p>
<ul>
<li>Fast I/O operations.</li>
<li>Feather files can be read and written in languages other than R or Python, such as Julia and Scala.</li>
<li>They have compatibility with all pandas datatypes, such as Datetime and Categorical.</li>
</ul>
<p>Feather currently supports the following datatypes:</p>
<ul>
<li>All numeric datatypes</li>
<li>Logical</li>
<li>Timestamps</li>
<li>Categorical</li>
<li>UTF-8 encoded strings</li>
<li>Binary</li>
</ul>
<p>Since feather is merely a simplistic version of Arrow, it has several caveats associated with it. The following are some limitations of using a feather file:</p>
<ul>
<li>Not recommended for long-term data storage as their stability between versions cannot be guaranteed.</li>
<li>Any index or multi-index, other than the default indexing scheme, is not supported in Feather format.</li>
<li>Python data types such as Period are not supported.</li>
<li>Duplicates in column names are not supported.</li>
</ul>
<p>Reading a feather file in pandas is done like so:</p>
<pre>pd.read_feather("sample.feather")</pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b96d3b4f-a50f-4a52-91c6-a4165aaf499b.png" style="width:22.42em;height:12.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Output of read_feather</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading parquet files</h1>
                </header>
            
            <article>
                
<p>Apache Parquet is another file format that makes use of columnar compression for efficient read and write operations. It was designed to be compatible with big data ecosystems such as Hadoop and can handle nested data structures and sparsely populated columns. Though the parquet and feather formats share a similar base, parquet has a better compression routine than feather. The compressed file is smaller in parquet than it is in feather. Columns with similar data types use the same encoding for compression. The use of different encoding schemes for the compression of parquet makes it efficient. Just like feather, parquet is a binary file format that can work well with all pandas data types and is supported across several languages. Parquet can be used for the long-term storage of data.</p>
<p>The following are some limitations of the parquet file format:</p>
<ul>
<li>While parquet can accept multi-level indices, it requires that the index level name is in string format.</li>
<li>Python data types such as Period are not supported.</li>
<li>Duplicates in column names are not supported.</li>
<li>When Categorical objects are serialized in a parquet file, they are deserialized as an object datatype.</li>
</ul>
<p>Serialization or deserialization of parquet files t in pandas can take place in either of the <kbd>pyarrow</kbd> and <kbd>fastparquet</kbd> engines. These two engines have different dependencies. Pyarrow does not support Timedelta.</p>
<p>Let's read a parquet file using the <kbd>pyarrow</kbd> engine:</p>
<pre>pd.read_parquet("sample.paraquet",engine='pyarrow')</pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1a4f82f0-be20-408d-805a-3cb1e7160638.png" style="width:25.25em;height:12.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Output of read_parquet</div>
<p>Parquet allows us to select columns when reading a file, which saves time:</p>
<pre>pd.read_parquet("sample.paraquet",engine='pyarrow',columns=["First_Name","Score"])</pre>
<p>The same works for the <kbd>fastparquet</kbd> engine as well:</p>
<pre>pd.read_parquet("sample.paraquet",engine='fastparquet')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading a SQL file</h1>
                </header>
            
            <article>
                
<p>Interacting with a SQL database through pandas requires the sqlalchemy dependency to be installed.</p>
<p>First, let's define the engine from which connection parameters can be obtained:</p>
<pre>engine = create_engine('sqlite:///:memory:')</pre>
<p>Now, let's read the <kbd>data_sql</kbd> table from the SQL database:</p>
<pre>with engine.connect() as conn, conn.begin():<br/>    print(pd.read_sql_table('data_sql', conn))</pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e991f948-810a-4d79-9894-cd22d844b593.png" style="width:25.42em;height:6.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Output of read_sql_table</div>
<p>The <kbd>read_sql_table()</kbd> function reads an entire table for the given table name. A specific column can be set as the index when reading: </p>
<pre>pd.read_sql_table('data_sql', engine, index_col='index')</pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/357a68c7-d791-454a-8dfe-fdff32a5188c.png" style="width:22.08em;height:11.58em;"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Output of read_sql_table with indexing</div>
<p>The columns argument lets us choose specific columns when reading data by passing the column names as a list. Any date columns can be parsed into a specific format during the read process, as shown in the following code:</p>
<pre>pd.read_sql_table('data_sql', engine, parse_dates={'Entry_date': '%Y-%m-<em>%d</em>'})<br/>pd.read_sql_table('data_sql', engine, parse_dates={'Entry_date': {'format': '%Y-%m-<em>%d</em> %H:%M:%S'}})</pre>
<p>The <kbd>schema</kbd> argument in this function helps specify the schema from which the table is to be extracted.</p>
<p>Instead of reading the entire table, it is also possible to use a SQL query to get data in the necessary format. We can do this with the <kbd>read_sql_query()</kbd> function:</p>
<pre>pd.read_sql_query("SELECT Last_name, Score FROM data_sql", engine)</pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5276913a-81d8-4b1e-a138-bf3e32b90d67.png" style="width:13.17em;height:12.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Output of read_sql_query</div>
<p>To run the <kbd>INSERT</kbd> and <kbd>CREATE</kbd> queries, which do not return any output, the <kbd>sql.execute()</kbd> function can be used. This requires an <kbd>sql</kbd> file of <kbd>pandas.io</kbd> to be imported:</p>
<pre>from pandas.io import sql<br/>sql.execute("INSERT INTO tablename VALUES (90, 100, 171)", engine)</pre>
<p>With a <kbd>sqlite</kbd> database, the connection to the engine has to be defined as follows so that it can be used in the <kbd>read_sql_table()</kbd> or <kbd>read_sql_query()</kbd> functions. The <kbd>sqlite</kbd> module must be imported prior to this:</p>
<pre>import sqlite3<br/>conn = sqlite3.connect(':memory:')</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading a SAS/Stata file</h1>
                </header>
            
            <article>
                
<p>Pandas can read two file formats from SAS <span>– </span>SAS xports (<kbd>.XPT</kbd>) and SAS data files (<kbd>.sas7bdat</kbd>).</p>
<p>The <kbd>read_sas()</kbd> function helps read SAS files. Here, a SAS data file has been read and displayed as a pandas dataframe:</p>
<pre>df = pd.read_sas('sample.sas7bdat')<br/>df</pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/83cb9f2b-0d6c-428a-bde7-f4e3ebaa063e.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Output of read_sas</div>
<p>The <kbd>chunksize</kbd> and <kbd>iterator</kbd> arguments help in reading the SAS file in groups of the same size. If the SAS data file that was used earlier is read with a chunksize of 10, then the 51 records will be divided into six groups, as shown in the following code:</p>
<pre>rdr = pd.read_sas('sample.sas7bdat', chunksize=10)<br/>for chunk in rdr:<br/>print(chunk.shape)</pre>
<p>Take a look at the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b71ebc09-1929-443e-bdf7-1a75cf72d09c.png" style="width:10.42em;height:7.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Output of read_sas with chunksize</div>
<p>However, these SAS files cannot be written using pandas.</p>
<p>Pandas also provides support for reading and writing files that have been generated from Stata. Stata only supports limited datatypes: <kbd>int8</kbd>, <kbd>int16</kbd>, <kbd>int32</kbd>, <kbd>float32</kbd>, <kbd>float64</kbd>, and strings with a length less than 244. When writing a Stata data file through pandas, type conversion is applied wherever applicable.</p>
<p>Let's read a Stata datafile using pandas:</p>
<pre>df = pd.read_stata('sample.dta')<br/>df</pre>
<p><span>Take a look at the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a760ba3a-248a-4b8d-9d52-6bb2a95e589c.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Output of read_stata</div>
<p>The <kbd>read_stata()</kbd> function also has <kbd>chunksize</kbd> and <kbd>iterator</kbd> arguments to read data in smaller groups. The following arguments are the available <kbd>stata</kbd> reader functions: </p>
<ul>
<li><kbd>convert_categoricals</kbd>: Converts a suitable column into a categorical data type</li>
<li><kbd>index_col</kbd>: Identifies the column to be defined as an index</li>
<li><kbd>convert_missing</kbd>: Specifies whether to represent missing values as NaN or with a Stata missing value object</li>
<li><kbd>columns</kbd>: Columns to select from the dataset</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading from Google BigQuery</h1>
                </header>
            
            <article>
                
<p>BigQuery is an extremely powerful data warehousing solution provided by Google. Pandas can directly connect to BigQuery and bring your data to a Python environment for further analysis.</p>
<p>The following is an example of reading a dataset from BigQuery:</p>
<pre>pd.read_gbq("SELECT urban_area_code, geo_code, name, area_type, area_land_meters <br/>FROM `bigquery-public-data.utility_us.us_cities_area` LIMIT 5", project_id, dialect = "standard")</pre>
<p>Take a look at the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7131995c-654c-4c9f-a9a1-74c5225ac87a.png" style="width:39.75em;height:12.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Output of read_gbq</div>
<p>The <kbd>read_gbq()</kbd> function accepts the query and the Google Cloud project-id (which serves as a key) so that it can access the database and bring out the data. The dialect argument takes care of the SQL syntax to be used: BigQuery's legacy SQL dialect or the standard SQL dialect. In addition, there are arguments that allow the index column to be set (<kbd>index_col</kbd>), columns to be reordered (<kbd>col_order</kbd>), and reauthentication to be enabled (<kbd>reauth</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading from a clipboard</h1>
                </header>
            
            <article>
                
<p>This is a rather interesting feature in pandas. Any tabular data that has been copied onto the clipboard can be read as a DataFrame in pandas.</p>
<p>Let's copy the following tabular data with the usual <em>ctrl + C</em> keyboard command: </p>
<table style="border-collapse: collapse;width: 859px;height: 251px" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td style="width: 52.4653px" class="CDPAlignCenter CDPAlign"/>
<td style="width: 234.688px" class="CDPAlignCenter CDPAlign">
<p><strong>Gender</strong></p>
</td>
<td style="width: 386.91px" class="CDPAlignCenter CDPAlign">
<p><strong>Entry_Date</strong></p>
</td>
<td style="width: 171.354px" class="CDPAlignCenter CDPAlign">
<p><strong>Flag</strong></p>
</td>
</tr>
<tr>
<td style="width: 52.4653px" class="CDPAlignCenter CDPAlign">
<p><strong>A</strong></p>
</td>
<td style="width: 234.688px" class="CDPAlignCenter CDPAlign">
<p>M</p>
</td>
<td style="width: 386.91px" class="CDPAlignCenter CDPAlign">
<p>2012-01-19</p>
</td>
<td style="width: 171.354px" class="CDPAlignCenter CDPAlign">
<p>True</p>
</td>
</tr>
<tr>
<td style="width: 52.4653px" class="CDPAlignCenter CDPAlign">
<p><strong>B</strong></p>
</td>
<td style="width: 234.688px" class="CDPAlignCenter CDPAlign">
<p>F</p>
</td>
<td style="width: 386.91px" class="CDPAlignCenter CDPAlign">
<p>2012-12-30</p>
</td>
<td style="width: 171.354px" class="CDPAlignCenter CDPAlign">
<p>False</p>
</td>
</tr>
<tr>
<td style="width: 52.4653px" class="CDPAlignCenter CDPAlign">
<p><strong>C</strong></p>
</td>
<td style="width: 234.688px" class="CDPAlignCenter CDPAlign">
<p>M</p>
</td>
<td style="width: 386.91px" class="CDPAlignCenter CDPAlign">
<p>2012-05-05</p>
</td>
<td style="width: 171.354px" class="CDPAlignCenter CDPAlign">
<p>False</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Calling the <kbd>read_clipboard()</kbd> function makes this data available as a pandas DataFrame: </p>
<pre>pd.read_clipboard()</pre>
<p>Take a look at the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9479c71d-b380-484b-9e00-c217d19a956f.png" style="width:13.58em;height:7.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Output of read_clipboard</div>
<p>This function also recognizes the <strong>Flag</strong> column as a bool data type by default and assigns the unnamed column to be the index:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8ab745a9-7b80-4af3-bd88-83f89e40fe90.png" style="width:11.58em;height:5.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"> Data types after reading the clipboard</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing sparse data</h1>
                </header>
            
            <article>
                
<p>Sparse data refers to data structures such as arrays, series, DataFrames, and panels in which there is a very high proportion of missing data or NaNs.</p>
<p>Let's create a sparse DataFrame:</p>
<pre>df = pd.DataFrame(np.random.randn(100, 3))<br/>df.iloc[:95] = np.nan</pre>
<p>This DataFrame has NaNs in 95% of the records. The memory usage of this data can be estimated with the following code:</p>
<pre>df.memory_usage()</pre>
<p><span>Take a look at the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/395b1fb1-ccac-4920-9d60-8879499c651a.png" style="width:7.58em;height:6.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Memory usage of a DataFrame with 95% NaNs</div>
<p>As we can see, each element consumes 8 bytes of data, irrespective of whether it is actual data or a NaN. Pandas offers a memory-efficient solution for handling sparse data, as depicted in the following code: </p>
<pre>sparse_df = df.to_sparse()<br/>sparse_df.memory_usage()</pre>
<p><span>Take a look at the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/006a584e-c19f-4aa2-b459-408b6d75f790.png" style="width:7.33em;height:6.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Memory usage of sparse data</div>
<p>Now, the memory usage has come down, with memory not being allotted to NaNs. This can also be implemented by defining a <kbd>fill_value</kbd> instead of NaN: </p>
<pre>df.fillna(0).to_sparse(fill_value = 0)<br/>df.fillna(0).to_sparse(fill_value = 0).memory_usage()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>Take a look at the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e92501ca-3ad0-4c92-9cdb-c15e3cfb8a0c.png" style="width:6.83em;height:6.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Memory usage of sparse data after filling in the values</div>
<p>The sparse data can also be converted back into the original dense form, as shown in the following code: </p>
<pre>sparse_df.to_dense()</pre>
<p>This way of handling sparse data can be applied in a similar way to series, panels, and arrays.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing JSON objects to a file</h1>
                </header>
            
            <article>
                
<p>The <kbd>to_json()</kbd> function allows any DataFrame object to be converted into a JSON string or written to a JSON file if the file path is specified: </p>
<pre>df = pd.DataFrame({"Col1": [1, 2, 3, 4, 5], "Col2": ["A", "B", "B", "A", "C"], "Col3": [True, False, False, True, True]})<br/>df.to_json()</pre>
<p><span>Take a look at the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3093bb79-a18a-40aa-9758-d5a8d340405b.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">JSON output</div>
<p>The orientation of the data in the JSON can be altered. The <kbd>to_json()</kbd> function has an orient argument which can be set for the following modes: columns, index, record, value, and split. Columns is the default setting for orientation: </p>
<pre>df.to_json(orient="columns")</pre>
<p><span>Take a look at the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a98a5f64-dc5b-44bd-8e85-be398a6fc207.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">JSON output <span>– </span>column orientation</div>
<p>Orienting along the index acts like a transpose of the former case with a reversal of row and column indices in the JSON dictionary: </p>
<pre>df.to_json(orient="index")</pre>
<p><span>Take a look at the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b257e514-0a3d-4c8e-b9cb-4e236929305f.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">JSON output <span>–</span> index orientation</div>
<p> </p>
<p>Setting orient as records creates a JSON structure where each record or row from the original DataFrame retains its structural form: </p>
<pre>df.to_json(orient="records")</pre>
<p>Take a look at the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2238dc0e-150a-451d-866b-651b27862cb4.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">JSON output <span>–</span> records orientation</div>
<p>When the orient option is set to values, both row indices and column indices vanish from the picture: </p>
<pre>df.to_json(orient="values")</pre>
<p><span>Take a look at the following output:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/a0d7aa85-911e-4556-94e7-6db105f4765f.png" style="width:35.00em;height:1.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">JSON output <span>– </span>values orientation</div>
<p class="mce-root"/>
<p>The split orientation defines a JSON made up of entities such as column, index, and data: </p>
<pre>df.to_json(orient="split")</pre>
<p><span>Take a look at the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c909c169-02bf-4ebc-a2e7-40fb9626bad0.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">JSON output—split orientation</div>
<p>Setting orient to table brings out aspects such as schema and field: </p>
<pre>df.to_json(orient="table")</pre>
<p><span>Take a look at the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4214d637-f8be-4ec1-9a61-40762c7b7e15.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">JSON output—table orientation</div>
<p>The <kbd>date_format</kbd> argument of <kbd>to_json()</kbd> allows timestamps in the DataFrame to be converted into either <kbd>epoch</kbd> format or <kbd>iso</kbd> format.</p>
<p>An unsupported datatype such as <kbd>complex</kbd> can be handled by specifying the type conversion to be followed through the <kbd>default_handler</kbd> argument.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Serialization/deserialization</h1>
                </header>
            
            <article>
                
<p>Serialization is the process of translating <a href="https://en.wikipedia.org/wiki/Data_structure">data structures</a> or <a href="https://en.wikipedia.org/wiki/Object_(computer_science)">object</a> state into a format that can be stored (for example, in a <a href="https://en.wikipedia.org/wiki/Computer_file">file</a> or memory <a href="https://en.wikipedia.org/wiki/Data_buffer">buffer</a>) or transmitted (for example, across a <a href="https://en.wikipedia.org/wiki/Computer_network">network</a> connection link) and reconstructed later (possibly in a different computer environment).<a href="https://en.wikipedia.org/wiki/Serialization#cite_note-1">[1]</a> When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object.</p>
<p>Data structures such as JSON, arrays, DataFrames, and Series sometimes need to be stored as physical files or transmitted over a network. These serializations can be understood as a dump of data where data can be stored in any format (text, CSV, and so on) or structure but all the important data points can be recreated by loading/deserializing them.</p>
<p>Some examples of this are storing the parameters of the trained model object of a statistical model. This serialized file containing trained parameters can be loaded and the testing data can be passed through it for prediction. This is a popular method that's used to put statistical models to use.</p>
<p>Other uses of serialized data formats include transferring data through wires, storing objects in databases or HDDs, to make remote procedure calls, and to detect changes in time-varying data.</p>
<p>Let's create a sample DataFrame to understand the serialization of various file formats supported by Pandas: </p>
<pre>df = pd.DataFrame({"First_Name":["Mike","Val","George","Chris","Benjamin"],<br/>"Last_name":["K.","K.","C.","B.","A."],<br/>"Entry_date":pd.to_datetime(["June 23,1989","June 16,1995","June 20,1997","June 25,2005","March 25,2016"],format= "%B %d,%Y"),<br/>"Score":np.random.random(5)})<br/>df</pre>
<p>Take a look at the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/20da6e5b-781d-4b37-9945-e38614c41d24.png" style="width:22.08em;height:11.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"> DataFrame for serialization</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing to exotic file types</h1>
                </header>
            
            <article>
                
<p>There are various formats that a data structure or object can be stored in. Let's go over a few of them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">to_pickle()</h1>
                </header>
            
            <article>
                
<p>When a Python object is pickled, it gets saved to disk. Pickling serializes the object first, before writing it. It involves converting objects such as lists, Dicts, DataFrames, and trained machine learning models into a character stream.</p>
<p>Let's convert the DataFrame we defined earlier into pickle format: </p>
<pre>df.to_pickle('pickle_filename.pkl')</pre>
<p>It is also possible to compress pickle files before they are written. Compression schemes such as <kbd>gzip</kbd>, <kbd>bz2</kbd>, and <kbd>xz</kbd> are supported: </p>
<pre>df.to_pickle("pickle_filename.compress", compression="gzip")</pre>
<p>By default, the compression type is inferred from the extension that's provided: </p>
<pre>df.to_pickle("pickle_filename.gz")</pre>
<p>The <kbd>read_pickle()</kbd> function will deserialize the <kbd>pickle</kbd> file. Zip compression is only supported for reading a single file and not for writing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">to_parquet()</h1>
                </header>
            
            <article>
                
<p>As we discussed in the <em>Reading parquet files</em> section, two engines can be used for deserialization as well: </p>
<pre>df.to_parquet('sample_pyarrow.parquet', engine='pyarrow')<br/>df.to_parquet('sample_fastparquet.parquet', engine='fastparquet')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">to_hdf()</h1>
                </header>
            
            <article>
                
<p>An HDF file is like a dictionary and it can store multiple objects. The <kbd>to_hdf()</kbd> function converts a Pandas object into an HDF file: </p>
<pre>df.to_hdf('store.h5', append = True, format='table')</pre>
<p>When all the columns of a row are NaNs, they are not automatically dropped. This can be done by setting the <kbd>dropna</kbd> argument to <kbd>True</kbd> when writing to HDF.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">to_sql()</h1>
                </header>
            
            <article>
                
<p>With support from the <kbd>sqlalchemy</kbd> package, data can be transferred to databases through pandas: </p>
<pre>from sqlalchemy import create_engine<br/>engine = create_engine('sqlite:///:memory:')<br/>df.to_sql('data_sql',engine)</pre>
<p>Data can also be pushed iteratively in batches by using the <kbd>chunksize</kbd> argument.</p>
<p>The data type of any column can also be changed when pushing to the database, as shown in the following code:</p>
<pre>from sqlalchemy.types import String<br/>df.to_sql('data_dtype', engine, dtype={'Score': String})</pre>
<p>The <kbd>Timedelta</kbd> datatype, which is not supported across databases, is converted into its equivalent integral value in nanoseconds before being stored in the database.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">to_feather()</h1>
                </header>
            
            <article>
                
<p>Serializing a pandas object into feather format just requires the <kbd>to_feather()</kbd> function to be called: </p>
<pre>df.to_feather('sample.feather')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">to_html()</h1>
                </header>
            
            <article>
                
<p>The <kbd>to_html()</kbd> function converts a DataFrame into a raw HTML format:</p>
<pre>df.to_html()</pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/91c4cda7-34b2-4cd7-bb92-f9548df02cf8.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">DataFrame in HTML format</div>
<p>A horde of options in the <kbd>to_html()</kbd> function allow the raw HTML to be enriched. Being able to select columns and control escape sequences is possible through the use of the <kbd>columns</kbd> and <kbd>escape</kbd> arguments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">to_msgpack()</h1>
                </header>
            
            <article>
                
<p>Msgpack offers fast and efficient binary serialization.</p>
<p>A single object can be directly converted into <kbd>msgpack</kbd> format like so:</p>
<pre>df.to_msgpack("sample.msg")</pre>
<p>If we have multiple objects, they can be serialized into a single <kbd>msgpack</kbd> file like so:</p>
<pre>arr = np.random.randint(1,10,7)<br/>lst = [10,20,40,60,60]<br/>strg = "Data"<br/>pd.to_msgpack("msg_all.msg",df,arr,lst,strg)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">to_latex()</h1>
                </header>
            
            <article>
                
<p>The <kbd>to_latex()</kbd> function takes a DataFrame and converts it into an aesthetic tabular structure that's compatible with <kbd>latex</kbd> documents: </p>
<pre>df.to_latex()</pre>
<p>Take a look at the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f3f596fb-88e6-4ca5-99d7-6ef2497ef3a2.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">DataFrame in latex format</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">to_stata()</h1>
                </header>
            
            <article>
                
<p>Pandas can help with creating <kbd>stata</kbd> data files with the <kbd>.dta</kbd> extension, as shown in the following code: </p>
<pre>df.to_stata('stata_df.dta')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">to_clipboard()</h1>
                </header>
            
            <article>
                
<p>The <kbd>to_clipboard()</kbd> function transfers a DataFrame from a Python environment to the clipboard. From the clipboard, the object can be pasted elsewhere through the use of the <em>ctrl</em> + <em>V</em> keyboard command:</p>
<pre>df.to_clipboard()</pre>
<p>This DataFrame can also be sent to the clipboard in a format that's more compatible with CSV like so:</p>
<pre>df.to_clipboard(excel=True,sep=",")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GeoPandas</h1>
                </header>
            
            <article>
                
<p>GeoPandas is a Python package written on top of pandas that's used to work with geospatial data. It is designed to work with existing tools, such as desktop GIS, geospatial databases, web maps, and Python data tools.</p>
<p>GeoPandas allows you to easily perform operations in Python that would otherwise require a spatial database such as PostGIS.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is geospatial data?</h1>
                </header>
            
            <article>
                
<p>Spatial data, geospatial data, GIS data, and geodata are the names for numeric data that identifies the geographical location of a physical object such as a building, street, town, city, country, and so on according to a geographic coordinate system<strong>.</strong><span> </span>Apart from the geographical location, geospatial data often also stores socioeconomic data, transaction data, and so on for each location.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installation and dependencies</h1>
                </header>
            
            <article>
                
<p>GeoPandas can be installed through pip or Anaconda, or directly through GitHub. The most common ways are through <kbd>pip</kbd> and Anaconda through a Terminal window:</p>
<pre>pip install geopandas 
conda install -c conda-forge geopandas </pre>
<p>GeoPandas depends on the following Python libraries:</p>
<ul>
<li><kbd>pandas</kbd></li>
<li><kbd>numpy</kbd></li>
<li><kbd>shapely</kbd></li>
<li><kbd>fiona</kbd></li>
<li><kbd>pyproj</kbd></li>
<li><kbd>six</kbd></li>
<li><kbd>rtree</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with GeoPandas</h1>
                </header>
            
            <article>
                
<p>We can use the GeoPandas library to read many GIS file formats (relying on the <kbd>fiona</kbd> library, which is an interface to GDAL/OGR) using the <kbd>geopandas.read_file</kbd> function.</p>
<p>Data can be read through shapefiles as well. In this section, we will look at an example of working with GeoPandas. We will explain how to read a shapefile that contains geospatial data, performing aggregations on it, sorting it, and finally plotting the required Geo DataFrame.</p>
<p>Use the following code to call in the required prerequisites libraries:</p>
<pre>    import pandas as pd
    import geopandas
    import matplotlib as plt</pre>
<p>Use the following code to read a shapefile that has geospatial information data:</p>
<pre>countries = geopandas.read_file("ne_110m_admin_0_countries.shp") </pre>
<p>Use the following code to access the first five rows of the dataset, just like we do with pandas:</p>
<pre>countries.head(5) </pre>
<p>The preceding code snippets result in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1855 image-border" src="assets/9ce0578f-c959-4a75-bd6b-e3c778dcbd1f.png" style="width:89.00em;height:33.75em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">A geospatial shape file read as a DataFrame</div>
<p>Let's plot a quick basic visualization of the data:</p>
<pre>countries.plot() </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1856 image-border" src="assets/5db74b7f-59a8-46b4-bed9-66be5731c66d.png" style="width:27.00em;height:15.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The countries in the shapefile plotted on a map</div>
<p>To check the data type of our geospatial data, we can use the following code:</p>
<pre>type(countries) </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1857 image-border" src="assets/b7a1f2d4-33c6-46b5-b852-2b0e93eb46b6.png" style="width:21.25em;height:3.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Asserting that the data type of the countries shapefile is a GeoDataFrame after conversion</div>
<p>Here, we can see that the DataFrame is a GeoDataFrame. Now, let's discuss what a GeoDataFrame is.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GeoDataFrames</h1>
                </header>
            
            <article>
                
<p>A GeoDataFrame contains a geospatial dataset. It is just like a pandas DataFrame but with some additional functionality for working with geospatial data. This additional functionality is as follows:</p>
<ul>
<li>A <kbd>.geometry</kbd> attribute that always returns the column that includes geometry information (returning a GeoSeries). The column name itself does not necessarily need to be <kbd>.geometry</kbd>, but it will always be accessible as the <kbd>.geometry</kbd> attribute.</li>
<li>It has some extra methods for working with spatial data (area, distance, buffer, intersection, and so on), all of which we will look at in later chapters.</li>
</ul>
<p>GeoDataFrame is still a DataFrame, so we have all the functionalities that we have available for DataFrames. We can perform aggregation, sorting, filtering, and so on in GeoDataFrames as well.</p>
<p>Use the following code to perform a simple aggregation with GeoPandas:</p>
<pre>countries['POP_EST'].mean()  </pre>
<p><kbd>POP_EST</kbd> is a column in the <kbd>countries</kbd> GeoDataFrame and is of the numeric type. This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1858 image-border" src="assets/c9bc4306-c07b-4ef9-af33-5110ae805ff2.png" style="width:17.17em;height:3.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"> Aggregating a numeric column in a GeoDataFrame shows that it works exactly the same way as a normal DataFrame</div>
<p>Alternatively, we can use boolean filtering to select a subset of the DataFrame based on a condition:</p>
<pre>africa = countries[countries['CONTINENT'] == 'Africa'] </pre>
<p>Now, we will try to plot the filtered GeoDataFrame by using the <kbd>plot()</kbd> function:</p>
<pre>africa.plot() </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1859 image-border" src="assets/1812f806-00e9-48b4-83f9-553a54086457.png" style="width:26.58em;height:17.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Subsetting one continent and plotting it for better visibility</div>
<p>GeoPandas also helps in converting an ordinary DataFrame into a GeoDataFrame, provided that you have <kbd>Latitude</kbd> and <kbd>Longitude</kbd> coordinates. Let's take a look at this.</p>
<p>Let's assume that we have a simple <span>DataFrame</span>, like this:</p>
<pre>df = pd.DataFrame( 
{'City': ['Buenos Aires', 'Brasilia', 'Santiago', 'Bogota', 'Caracas'], 
'Country': ['Argentina', 'Brazil', 'Chile', 'Colombia', 'Venezuela'], 
'Latitude': [-34.58, -15.78, -33.45, 4.60, 10.48], 
'Longitude': [-58.66, -47.91, -70.66, -74.08, -66.86]}) </pre>
<p>The preceding code results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1860 image-border" src="assets/33331910-d469-4863-9e25-20be419b7a83.png" style="width:22.92em;height:12.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Creating a normal DataFrame of country capitals with latitude and longitude</div>
<p>Let's append a new column called <kbd>'Coordinates'</kbd> which concatenates the latitude and longitude columns:</p>
<pre>   df['Coordinates']  = list(zip(df.Longitude, df.Latitude)) </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1861 image-border" src="assets/d86862f8-a170-4390-97fd-193d8b46b2bb.png" style="width:31.50em;height:12.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">A normal DataFrame with latitude and longitude zipped in one column</div>
<p><span class="n">Using the <kbd>Point</kbd> function from the shapely package, we can correctly identify these as positional coordinates or point tuple parameters, which are the vertebra of a GeoDataFrame:</span></p>
<pre><span class="n">from shapely.geometry import Point</span> 
   df['Coordinates'] = df['Coordinates'].apply(Point) </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1862 image-border" src="assets/86e05119-7fff-4636-80a6-b3214ef99f72.png" style="width:31.33em;height:12.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Converting the zipped latitude and longitude into a point so that it is usable by geopandas for converting it into a GeoDataFrame</div>
<p>Now that everything is in place, let's convert this to a GeoDataFrame:</p>
<pre>gdf = geopandas.GeoDataFrame(df, geometry='Coordinates') </pre>
<p>Let's print the type as <kbd>gdf</kbd> to see its GeodataFrame type:</p>
<pre>   type(gdf) </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1863 image-border" src="assets/ecdd1986-6e59-4459-9087-39a7fe552661.png" style="width:20.33em;height:3.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Asserting that the type of the newly created DataFrame is a GeoDataFrame</div>
<p>This has given us a basic idea about GeoPandas and how it works. Its wings are so widespread that you can glide across the various features of it and get benefits from it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Open source APIs – Quandl</h1>
                </header>
            
            <article>
                
<p style="font-weight: 400">Python can be used to fetch data from open source and commercial APIs. We can use it to fetch data in several formats. Some of them output data in JSON format, some in XML, and some in tabular formats such as CSV and DataFrames. Once converted into DataFrames, this data is generally processed in pandas.</p>
<p style="font-weight: 400">In this section, we will look at an example of fetching data from the Quandl API, which is an open source API that contains data on a variety of topics such as financial, economic, and alternative data. You can have a look at this famous data repository here: <a href="https://www.quandl.com/">https://www.quandl.com/</a>.<a href="https://www.quandl.com/"/></p>
<p>An <kbd>api</kbd> key is an application programming interface that acts as a mediator between a developer or any other user who wishes to access the data within the website using a computer code. An <kbd>api</kbd> key is a piece of code that identifies the user and their associated account.</p>
<p>To get started with this example, you will need to sign up for a Quandl account, which is free. Post signup, the API key can be found under the Account setting options, which will be available under the dropdown of the profile image.</p>
<p>Python has made it easier to work with the Quandl API by providing us with a package that can be used to interact with the latest version of the Quandl data repository. This package is compatible with Python version 2.7 and above.</p>
<p>First things first, you need to install the Quandl package through <kbd>pip</kbd> or <kbd>conda</kbd> using the following command:</p>
<pre><strong>pip install quandl</strong>
<strong>conda install quandl</strong></pre>
<p>You can fetch any dataset you wish to use. Here, I am using the Brazilian Real Futures, July 2022 dataset for illustration purposes. You will need to find the data code for the dataset you want to download. This can be obtained from the Quandl website and is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1255 image-border" src="assets/6b21ddba-b76c-4576-b7e6-109183f6a825.png" style="width:41.75em;height:17.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Finding the data code for a dataset on the Quandl website</div>
<p>Now, let's look at how we can use the Quandl API to fetch the data we want:</p>
<pre>    # import quandl into your code
    import quandl
    
    # setting your api key
    quandl.ApiConfig.api_key = "[YOUR API KEY]"
    
    # BCCME/L6N2022 is the code for the dataset and you can see it on the right below the table
    data = quandl.get("BCCME/L6N2022", start_date="2015-12-31", end_date="2019-06-23")
    
    data.head()  </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1256 image-border" src="assets/3b2db996-2bea-4c2c-9b41-18c247cd38f2.png" style="width:25.75em;height:8.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Brazilian Real Futures data fetched via the Quandl API</div>
<p>The API can also be used to fetch a subset of data and not all of it at once. For example, here, we have filtered the data for a given date range:</p>
<pre>    transform =  quandl.get("BCCME/L6N2022", start_date="2015-12-31", end_date="2019-06-23",transformation='diff')
    transform.head()</pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1257 image-border" src="assets/4377abc8-7f48-40fc-bd88-b142a4cb1b36.png" style="width:27.92em;height:9.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Brazilian Real Futures data fetched via the Quandl API with a date range filter</div>
<p>Once the data has been read, pandas can be used to perform all the transformations on the data, which will be helpful for further analysis.</p>
<p>Datasets can also be downloaded by providing the URLs of the dataset. This can be checked by downloading a file that will list the available datasets. Let's try downloading a file from the URL through Python's <kbd>urllib</kbd> package rather than following the <kbd>Quandl</kbd> package method.</p>
<p>To get the URL that can be used in this method, follow these steps:</p>
<ol>
<li>Click on the dataset header/link (marked in the red box):</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1258 image-border" src="assets/27f37797-063f-4bb8-bdd5-858dc4fb0b66.png" style="width:62.17em;height:17.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Data topic link to get more details about the topic</div>
<ol start="2">
<li>Clicking on the link will take you to the next page, where you would see these options. Select API under the Usage option, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1259 image-border" src="assets/3805a13b-b080-48cf-b9c3-48710a3fbe5f.png" style="width:29.17em;height:7.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Data topic documentation</div>
<ol start="3">
<li>After this selection, you should scroll down a bit to find the following URLs, which can be used in the code to fetch the data:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1260 image-border" src="assets/bd0bf04a-881e-4eb0-af08-610d5189a8f4.png" style="width:31.17em;height:15.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">API links for Quandl data that were obtained from the Data topic documentation</div>
<ol start="4">
<li>Since one topic may contain multiple datasets, the topic is downloaded as a <kbd>.zip</kbd> file containing all the datasets. It provides a metadata table, as well as the details (including the dataset key) of each dataset:</li>
</ol>
<pre>    import urllib.request
    print('Beginning file download with urllib...')
    url = 'https://www.quandl.com/api/v3/databases/EOD/metadata?api_key=[YOUR API KEY]'
    urllib.request.urlretrieve(url,'file location.zip')  
    
    # We will read the zip file contents through the zipfile package.
    import zipfile
    archive = zipfile.ZipFile('[Name of the zip file].zip', 'r')
    
    # lists the contents of the zip file
    archive.namelist()
    ['EOD_metadata.csv']
    df = pd.read_csv(archive.open('EOD_metadata.csv'))    
    df.head()
  </pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1261 image-border" src="assets/507b5a15-cb3c-4309-99b6-531fd6a82d48.png" style="width:67.50em;height:11.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"> Output of the metadata table of the downloaded data topic</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">read_sql_query</h1>
                </header>
            
            <article>
                
<p>Python supports a lot of database operations using libraries such as <kbd>psycopg2</kbd> and <kbd>sqlalchemy</kbd>. Both of them are quite comprehensive and useful when working with databases from a Python interface. However, they have their own paraphernalia, which sometimes gets too much information for simple querying tasks. Fortunately, there is a hidden gem in pandas called<span> </span><kbd>read_sql_query</kbd><span> </span>method. It does the following:</p>
<ul>
<li>Runs simple queries involving select, where, and so on.</li>
<li>Runs all the queries that return a table or its subset in a tabular form.</li>
<li>Can't use the INSERT, UPDATE, and DELETE statements.</li>
<li>The output is a DataFrame and hence all the pandas methods can be used for further data processing.</li>
</ul>
<p>Let's look at how we can make use of this method. To illustrate this, we will insert a dataset as a table into a database. To do this, you will need to install a PostgreSQL or SQL database to your local directory. If you already have a database set up, you can ignore the table creation process and jump to the queries process.</p>
<p>Let's download the World Happiness 2019 dataset from Kaggle, push it to a <kbd>db</kbd>, and perform various DB and pandas operations on it:</p>
<pre>    import pandas as pd
    df = pd.read_csv('F:/world-happiness-report-2019.csv')
    df.head()</pre>
<p>The following data shows us the World Happiness report as a DataFrame:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1262 image-border" src="assets/b3bac349-5720-4957-9446-3d44cee6a066.png" style="width:70.42em;height:13.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">World Happiness report as a DataFrame</div>
<p>Since we are going to directly create a table from the DataFame that we generated previously, it is necessary to change the column name to postgresql since it does not support column names with spaces:</p>
<pre>    # rename Country (region) to region
    df= df.rename(columns={'Country (region)':'region'})
    
    # push the dataframe to postgresql using sqlalchemy
    # Syntax:engine = db.create_engine('dialect+driver://user:pass@host:port/db')
    
    from sqlalchemy import create_engine
    engine = create_engine('postgresql://postgres:1128@127.0.0.1:5433/postgres')
    df.to_sql('happy', engine,index=False)      </pre>
<p>In order to use<span> the </span><kbd>read_sql_query</kbd><span> </span>method, we need to make a connection with the database using either<span> </span><kbd>psycopg2</kbd><span> </span>or<span> </span><kbd>sqlalchemy</kbd>. Once the connection has been established,<span> </span><kbd>read_sql_query</kbd> can be used in its full form:</p>
<pre>    <strong>import psycopg2</strong>
    <strong>try:</strong>
    <strong>   connection = psycopg2.connect(user="[db-user_name]",</strong>
    <strong>                                  password="[db-pwd]",</strong>
    <strong>                                  host="127.0.0.1",</strong>
    <strong>                                  port="5433",</strong>
    <strong>                                  database="postgres")</strong>
    
    <strong>   happy= pd.read_sql_query("select * from happy;",con=connection).head()</strong>      </pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1263 image-border" src="assets/97849629-087f-4e41-a3f4-2c00306967da.png" style="width:75.08em;height:12.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">World Happiness report data as a DataFrame queried from the table in the PostgreSQL DB</div>
<p><span>Take a look at the following code. This helps in running a SQL query.</span></p>
<pre><strong>posgrt40 = pd.read_sql_query('select * from happy where "Positive affect" &gt; 40;',con=connection).head()</strong>
    <strong>      </strong>  </pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1264 image-border" src="assets/ce2363d9-2dc4-43f9-9fc5-31592f4d4844.png" style="width:75.08em;height:13.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">World Happiness report data with filters as a DataFrame queried from the table in the PostgreSQL DB</div>
<pre>except (Exception, psycopg2.Error) as error :<br/>print ("Error while fetching data from PostgreSQL", error)  </pre>
<p>The <kbd>pd.read_sql_query()</kbd> method returns the table as a DataFrame rather than requiring the programmer to intervene and convert the data into the necessary format.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pandas plotting</h1>
                </header>
            
            <article>
                
<p>A picture is worth a thousand words. This is why graphs are commonly used to visually illustrate relationships in data. The purpose of a graph is to present data that is too numerous or complicated to be described adequately in terms of text and in less space. With Python's<span> </span><strong>plotting function</strong>, it takes far less than a few words of code to create a production-quality graphic.</p>
<p>We will begin by installing the necessary packages:</p>
<pre>import pandas as pd 
import numpy as np </pre>
<p>We are using the <kbd>mtcars</kbd> data here to explain the plots:</p>
<pre>mtcars = pd.DataFrame({ 
        'mpg':[21,21,22.8,21.4,18.7,18.1,18.3,24.4,22.8,19.2], 
        'cyl':[6,6,4,6,8,6,8,4,4,4], 
        'disp':[160,160,108,258,360,225,360,146.7,140.8,167.7], 
  'hp':[110,110,93,110,175,105,245,62,95,123],    
'category':['SUV','Sedan','Sedan','Hatchback','SUV','Sedan','SUV','Hatchback','SUV','Sedan'] 
        }) 
mtcars </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1265 image-border" src="assets/2d0b263d-0e95-49e8-9a1b-f32841cfce3b.png" style="width:28.33em;height:19.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">mtcars DataFrame</div>
<p>Let's discuss the various plots in <kbd>pandas.plotting</kbd> in detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Andrews curves</h1>
                </header>
            
            <article>
                
<p>Andrews curve is a method that's used to visualize multidimensional data. It does this by mapping each observation onto a function. Here, each color that's used represents a class and we can easily note that the lines that represent samples from the same class have similar curves. This curve is very useful in analyzing time series and signal data.</p>
<p>Basically, each data point is sent through a Fourier transform according to the Fourier function. Each line in the following chart represents a separate data point. It can be plotted using the snippet below.</p>
<pre>andrew = pd.plotting.andrews_curves(mtcars,'Category') </pre>
<p><span>The following is the graph:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1267 image-border" src="assets/c372bbe4-daf0-4f94-946e-9706f991bbf0.png" style="width:24.42em;height:15.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Andrews curve plot</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parallel plot</h1>
                </header>
            
            <article>
                
<p>Parallel plots are best used when we need to compare many variables for each point and to understand the relationship between them, for example, if you need to compare an array of variables with the same attributes but differing values (for example, comparing motorcycle specs across different models).</p>
<p>Each connected line represents one data point. The vertical lines represent the columns or variables whose values have been plotted for each data point. The inflection point (marked in red) represents the values of those variables for those points. A parallel chart can be plotted very easily using pandas, as shown in the following code:</p>
<pre>parallel = pd.plotting.parallel_coordinates(mtcars,'Category') </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1268 image-border" src="assets/d63564e2-d01a-4300-8c17-66bc6ddb8dd5.png" style="width:25.00em;height:15.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Parallel plot</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Radviz plots</h1>
                </header>
            
            <article>
                
<p>Radviz plots allow for the exploration of multi-task classification problems. It displays data of three or more variables in a two-dimensional projection. This plot is like a circle with data points inside it. The variables are present around the perimeter of the circle.</p>
<p class="mce-root"/>
<p>The position of each point is determined by the values of all the variable values that make it. An imaginary circle is created and the variables are placed on this circle. The points are placed within the perimeter of the circle. The exact position of the point is determined by the position where the force that's exerted on it by each variable sums to zero. The force that's applied by each variable can be thought of as a spring force and is governed by Hook's law (F = kx):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1269 image-border" src="assets/29eadd53-789c-4924-b7c2-96032063ac2c.png" style="width:36.50em;height:37.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Radviz plot explanation</div>
<p>The plot above can be obtained by running the snippet below.</p>
<pre>rad_viz = pd.plotting.radviz(mtcars, 'Category') </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1270 image-border" src="assets/0d7e97fb-4d6c-44ee-a265-6e41c7fd895c.png" style="width:24.17em;height:15.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"> Radviz plot</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scatter matrix plot</h1>
                </header>
            
            <article>
                
<p>A scatter matrix plot consists of several plots of variables, all of which are present in a matrix format. Basically, a 2 x 2 matrix of variables is created where each cell represents a combination of two variables. Then, a scatter plot is generated for each combination. It can be used to determine the correlation between variables. It is used in a lot of dimension reduction cases:</p>
<pre>scatter = pd.plotting.scatter_matrix(mtcars,alpha = 0.5) </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1271 image-border" src="assets/e32e1878-bc5c-4c40-a6f0-19eeb6f9c660.png" style="width:32.58em;height:22.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Scatter matrix plot</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Lag plot</h1>
                </header>
            
            <article>
                
<p>A lag plot is a special type of scatter plot with variables (X, X-lagged, and so on).</p>
<p>X -lagged is the variable that's derived from X with a time lag. The graph is plotted among two variables and the plot is used to determine the randomness, model suitability, outliers, and serial correlation in the data <span>– </span>especially time series data:</p>
<pre>s = pd.Series(np.random.uniform(size=100)) </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1272 image-border" src="assets/68f06f9e-8f57-4809-b3a3-1bd8cf53baf8.png" style="width:22.83em;height:15.67em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Lag plot</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bootstrap plot</h1>
                </header>
            
            <article>
                
<p>A bootstrap plot is used to determine the uncertainty of statistics such as mean, median, midrange, and so on. It relies on the random sampling method with replacement. Calculating a statistic by randomly sampling from the same data multiple times and then averaging the individual result from each sample is called bootstrapping. A bootstrapping plot basically plots all the resultant values that were obtained from each random sample. It calculates the mean, median, and mode for all the samples and plots them as bar and line charts.</p>
<p>A random sample is selected from the data and the process is repeated a specified number of times to obtain the required metrics. The resulting plot that's obtained is a bootstrap plot:</p>
<pre>fig = pd.plotting.bootstrap_plot(s) </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1273 image-border" src="assets/fdd433ce-4e14-47e1-ad5a-40d2c899fec3.png" style="width:31.33em;height:22.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"> Lag plot</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">pandas-datareader</h1>
                </header>
            
            <article>
                
<p>We can use pandas to not only read data from local CSV or text files but also from various popular remote data sources such as Yahoo Finance, World Bank, and so on. Without any support from pandas, this would have been tedious and we would have to resort to web scraping. This simple and powerful functionality is provided through the<span> </span><kbd>pandas-datareader</kbd>.</p>
<p>It provides us with a direct way of connecting through various data sources from the comfort of the pandas ecosystem without having to delve into the complexity of HTML/JavaScript code where data is enmeshed. These data sources can be accessed by providing the source name and data code. Only a subset of the data can be obtained.</p>
<p class="mce-root"/>
<p>Let's delve deeper and see how we can use it:</p>
<ol>
<li>Install<span> </span><kbd>pandas-datareader</kbd> through <kbd>pip</kbd> using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install pandas-datareader</strong>  </pre>
<p style="padding-left: 60px">You can also install it through <kbd>conda</kbd> using the following  set of commands</p>
<ol start="2">
<li>First, we have to add <kbd>conda-forge</kbd> to our channel:</li>
</ol>
<pre style="padding-left: 60px"><strong>conda config --add channels conda-forge</strong>  </pre>
<ol start="3">
<li>After enabling<span> </span><kbd>pandas-datareader</kbd><span>, it </span>can be installed with the following code:</li>
</ol>
<pre style="padding-left: 60px"><strong>conda install pandas-datareader </strong> </pre>
<p>Now, let's get hands-on with some of the remote data sources and perform various functions provided by the pandas library to get an idea of how it works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Yahoo Finance</h1>
                </header>
            
            <article>
                
<p>If you are interested in knowing about the trends of the business world and have a thirst to get updates about <a href="https://money.howstuffworks.com/personal-finance/financial-planning/stocks.htm">stocks</a><span> </span>and<span> </span><a href="https://money.howstuffworks.com/personal-finance/budgeting/bonds.htm">bonds</a>, or if you are the one who has invested in them, then you may crave an update that occurs every minute. Google Finance is a financial website that was developed by Google that's made this straightforward by providing the required information and also letting us customize our needs according to our interests.</p>
<p>Google's API became less reliable during 2017 and has become highly deprecated because of the unavailability of a stable replacement due to large breaks in the API.</p>
<p>An alternative to it is<span> </span><strong>Yahoo</strong><span> </span>Finance, which is similar to Google Finance, and is popular among users for its robust data and consistency.</p>
<p><span>Now let's use <kbd>pandas-datareader</kbd> to get information related to stocks, mutual funds, and anything related to finance using the Google Finance API.</span></p>
<pre>    import pandas as pd
    from pandas_datareader import data
    symbols=['AAPL','GOOGL','FB','TWTR']
    # initializing a dataframe
    get_data = pd.DataFrame()
    stock = pd.DataFrame()
    
    
    for ticker in symbols:
       get_data = get_data.append(data.DataReader(ticker, 
                           start='2015-1-1', 
                           end='2019-6-23', data_source='yahoo'))
    
       for line in get_data:
            get_data['symbol'] = ticker
    
       stock = stock.append(get_data)
       get_data = pd.DataFrame()
    
    stock.head()
  </pre>
<p><span>The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1274 image-border" src="assets/8c88926d-7f47-4230-b091-596d14be8e2e.png" style="width:43.25em;height:13.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Date filtered stock data for Apple, Google, Facebook, and Twitter</div>
<pre>stock.describe() </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1275 image-border" src="assets/0845aa70-e0bd-489d-9f36-6bfc68a14f7c.png" style="width:40.08em;height:16.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Summary statistics of the stock data</div>
<p class="mce-root"/>
<p>Take a look at the following code:</p>
<pre> # get the list of column names
    cols = [ col for col in stock.columns]
    cols                      </pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1276 image-border" src="assets/f1b34b5d-8aca-4956-9ebb-42e4a4861bb4.png" style="width:39.25em;height:2.17em;"/></p>
<p>Take a look at the following code:</p>
<pre> # returns the symbol of the highest traded value among the symbols
 stock.loc[stock['High']==stock['High'].max(), 'High']</pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1277 image-border" src="assets/d74262cf-8abb-48c4-8a9b-f2c91efecc78.png" style="width:15.50em;height:3.92em;"/></p>
<p>The preceding code will return a DataFrame that gives you full details about the stock prices for each and every day between the two dates for <kbd>Apple[AAPL]</kbd>, <kbd>Google[GOOGL]</kbd>, <kbd>FB[FB]</kbd>, and <kbd>Twitter[TWTR]</kbd>.</p>
<p>It is important to get to know your data before performing any kind of analysis. Please take the following things into account:</p>
<ul>
<li>High is the highest price that the stock was traded for on that particular date.</li>
<li>Low is the lowest price that the stock was traded for on that particular date.</li>
<li>Open is the price that the stock was when the date started.</li>
<li>Close is the price that the stock was when the market closed for that date.</li>
<li>Volume is the number of physical shares that were traded for that particular stock.</li>
<li>Adj Close is the price that the stock was after the market closed.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">World Bank</h1>
                </header>
            
            <article>
                
<p>The World Bank is an organization that provides financial advice and helps various nations in terms of their economical state. It also provides a variety of data, including time series, geospatial, financial data, and so on, all of which will be helpful for analysis.</p>
<p>Before we start fetching data from the World Bank website, we must sign up to it. This allows us to get the indicator code for the dataset that we want to download. Signing up for the World Bank is free and doesn't take much time.</p>
<p>For the purpose of this example, I have used the World Development Indicators dataset. You can choose any dataset of your choice and start working on it:</p>
<pre>from pandas_datareader import wb  </pre>
<p>To get the indicator code, select the <span class="packt_screen">Databank</span> tab and choose <span class="packt_screen">Metadata Glossary</span> from the list that's displayed on the left-hand pane. You can find the indicator below each dataset, on the left-hand side of the panel (marked in red in the attached screenshot):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1278 image-border" src="assets/cbf5065e-9268-4764-aff2-898397b13765.png" style="width:81.25em;height:16.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"> Indicator code for the dataset, which is required for fetching data</div>
<pre><strong>dat = wb.download(indicator='FP.CPI.TOTL.ZG', start=2005, end=2019)</strong>  </pre>
<p>The DataFrame is returned in a multi-index row format:</p>
<pre><strong>dat.head()</strong>  </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1279 image-border" src="assets/d8bb985a-5374-4ce0-a358-e528b1090bd5.png" style="width:12.00em;height:10.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Multi-indexed DataFrame output of the World Bank indicator data</div>
<p>Now, let's display the data for one particular country, like so:</p>
<pre>    <strong>dat.loc['Canada']</strong>
  </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1280 image-border" src="assets/35220b81-cb9a-4a59-a420-71a2dcadc30c.png" style="width:8.83em;height:23.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Multi-indexed DataFrame output of the World Bank indicator data for one country. It becomes single indexed.</div>
<p>We can also return the price inflation data related to only one particular year for a particular country, like so:</p>
<pre>    <strong>dat.loc['Canada'].loc['2015']</strong>
  </pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1281 image-border" src="assets/40efc2f1-f406-4a72-aa41-2ec6a597c839.png" style="width:13.00em;height:2.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"> Data subsetted by both indices</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">After reading this chapter, the following points have been observed:</p>
<ul>
<li><kbd>pandas</kbd> provides powerful methods so that we can read from and write to a variety of data structures and a variety of sources.</li>
<li>The <kbd>read_csv</kbd> method in pandas can be used for reading CSV files, TXT files, and tables. This method has a multitude of arguments in order to specify delimiters, which rows to skip while reading, reading a file in smaller chunks, and so on.</li>
<li>pandas can be used to read data directly from URLs or S3.</li>
<li>DataFrames can be converted into JSON and vice versa. JSON can be stored in text files that can be read.</li>
<li>JSONs have dictionary-like structures that can be nested an infinite number of times. This nested data can be subsetted just like a dictionary with keys.</li>
<li>Pandas provide methods so that we can read data from the HD5, HTML, SAS, SQL, parquet, feather, and Google BigQuery data formats.</li>
<li>Serialization helps in dumping data structures or objects to physical files, storing them in a database, or transmitting them through a message.</li>
</ul>
<p>In the next chapter, we will <span>learn how to access and select data from panda data structures. We will also look in detail at basic indexing and label-, integer-, and mixed indexing.</span></p>


            </article>

            
        </section>
    </body></html>