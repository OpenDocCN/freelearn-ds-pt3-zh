- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Group By
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most fundamental tasks during data analysis involves splitting data
    into independent groups before performing a calculation on each group. This methodology
    has been around for quite some time, but has more recently been referred to as
    *split-apply-combine*.
  prefs: []
  type: TYPE_NORMAL
- en: Within the *apply* step of the *split-apply-combine* paradigm, it is additionally
    helpful to know whether we are trying to perform a *reduction* (also referred
    to as an aggregation) or a *transformation*. The former reduces the values in
    a group down to *one value* whereas the latter attempts to maintain the shape
    of the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, here is what *split-apply-combine* looks like for a reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Split-apply-combine paradigm for a reduction'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the same paradigm for a *transformation*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Split-apply-combine paradigm for a transformation'
  prefs: []
  type: TYPE_NORMAL
- en: In pandas, the `pd.DataFrame.groupby` method is responsible for splitting, applying
    a function of your choice, and combining the results back together for you as
    an end user.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Group by basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping and calculating multiple columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Group by apply
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Window operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the highest rated movies by year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the best hitter in baseball across years
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Group by basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: True mastery of the pandas group by mechanisms is a powerful skill for any data
    analyst. With pandas, you can easily summarize data, find patterns within different
    groups, and compare groups to one another. The number of algorithms you can apply
    alongside a group by are endless in theory, giving you as an analyst tons of flexibility
    to explore your data.
  prefs: []
  type: TYPE_NORMAL
- en: In this first recipe, we are going to start with a very simple summation against
    different groups in an intentionally small dataset. While this example is overly
    simplistic, a solid theoretical understanding of how group by works is important
    as you look toward real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get familiarized with how group by works in code, let’s create some sample
    data that matches our starting point in *Figures 8.1* and *8.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `pd.DataFrame` has two distinct groups: `group_a` and `group_b`. As you
    can see, the `group_a` rows are associated with `value` data of `0` and `2`, whereas
    the `group_b` rows are associated with `value` data of `1`, `3`, and `5`. Summing
    the values within each `group` should therefore yield a result of `2` and `9`,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To express this with pandas, you are going to use the `pd.DataFrame.groupby`
    method, which accepts as an argument the group name(s). In our case, this is the
    `group` column. This technically returns a `pd.core.groupby.DataFrameGroupBy`
    object that exposes a `pd.core.groupby.DataFrameGroupBy.sum` method for summation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Don’t worry if you find the method name `pd.core.groupby.DataFrameGroupBy.sum`
    to be verbose; it is, but you will never have to write it out by hand. We are
    going to refer to it here by its technical name for the sake of completeness,
    but as an end user, you will always follow the form you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is what you will follow to get your `pd.core.groupby.DataFrameGroupBy`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: By default, `pd.core.groupby.DataFrameGroupBy.sum` is considered an *aggregation*,
    so each group is *reduced* down to a single row during the *apply* phase of *split-apply-combine*,
    much like we see in *Figure 8.1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of calling `pd.core.groupby.DataFrameGroupBy.sum` directly, we could
    have alternatively used the `pd.core.groupby.DataFrameGroupBy.agg` method, providing
    it with the argument of `"sum"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The explicitness of `pd.core.groupby.DataFrameGroupBy.agg` is useful when compared
    side by side with the `pd.core.groupby.DataFrameGroupBy.transform` method, which
    will perform a *transformation* (see *Figure 8.2* again) instead of a *reduction*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`pd.core.groupby.DataFrameGroupBy.transform` guarantees to return a like-indexed
    object to the caller, which makes it ideal for performing calculations like `%
    of group`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When applying a reduction algorithm, `pd.DataFrame.groupby` will take the unique
    values of the group(s) and use them to form a new row `pd.Index` (or `pd.MultiIndex`,
    in the case of multiple groups). If you would prefer not to have the grouped labels
    create a new index, keeping them as columns instead, you can pass `as_index=False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You should also note that the name of any non-grouping columns will not be
    altered when performing a group by operation. For example, even though we start
    with a `pd.DataFrame` containing a column named *value*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The fact that we then group by the `group` column and sum the `value` column
    does not change its name in the result; it is still just `value`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be confusing or ambiguous if you apply other algorithms to your groups,
    like `min`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Our column is still just called `value`, even though in one instance, we are
    taking the *sum of value* and in the other instance, we are taking the *min of
    value*.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there is a way to control this by using the `pd.NamedAgg` class.
    When calling `pd.core.groupby.DataFrameGroupBy.agg`, you can provide keyword arguments
    where each argument key dictates the desired column name and the argument value
    is a `pd.NamedAgg`, which dictates the aggregation as well as the original column
    it is applied to.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if we wanted to apply a `sum` aggregation to our `value` column,
    and have the result shown as `sum_of_value`, we could write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although this recipe focused mainly on summation, pandas offers quite a few
    other built-in *reduction* algorithms that can be applied to a `pd.core.groupby.DataFrameGroupBy`
    object, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `any` | `all` | `sum` | `prod` |'
  prefs: []
  type: TYPE_TB
- en: '| `idxmin` | `idxmax` | `min` | `max` |'
  prefs: []
  type: TYPE_TB
- en: '| `mean` | `median` | `var` | `std` |'
  prefs: []
  type: TYPE_TB
- en: '| `sem` | `skew` | `first` | `last` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.1: Commonly used GroupBy reduction algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, there are some built-in *transformation* functions that you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '| cumprod | cumsum | cummin |'
  prefs: []
  type: TYPE_TB
- en: '| cummax | rank |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8.2: Commonly used GroupBy transformation algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: 'Functionally, there is no difference between calling these functions directly
    as methods of `pd.core.groupby.DataFrameGroupBy` versus providing them as an argument
    to `pd.core.groupby.DataFrameGroupBy.agg` or `pd.core.groupby.DataFrameGroupBy.transform`.
    You will get the same performance and result by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet will yield the same results as this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You could argue that the latter approach signals a clearer intent, especially
    considering that `max` can be used as a transformation just as well as an aggregation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In practice, both styles are commonplace, so you should be familiar with the
    different approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping and calculating multiple columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the basics down, let’s take a look at a `pd.DataFrame` that
    contains more columns of data. Generally, your `pd.DataFrame` objects will contain
    many columns with potentially different data types, so knowing how to select and
    work with them all through the context of `pd.core.groupby.DataFrameGroupBy` is
    important.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a `pd.DataFrame` that shows the `sales` and `returns` of a hypothetical
    `widget` across different `region` and `month` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate the total `sales` and `returns` for each `widget`, your first
    attempt at doing so may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'While `sales` and `returns` look good, the `region` and `month` columns also
    ended up being summed, using the same summation logic that Python would when working
    with strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, this default behavior is usually undesirable. I personally find
    it rare to ever want strings to be concatenated like this, and when dealing with
    large `pd.DataFrame` objects, it can be prohibitively expensive to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to avoid this issue is to be more explicit about the columns you would
    like to aggregate by selecting them after the `df.groupby("widget")` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you could reach for the `pd.NamedAgg` class we introduced back
    in the *Group by basics* recipe. Though more verbose, the use of `pd.NamedAgg`
    gives you the benefit of being able to rename the columns you would like to see
    in the output (i.e., instead of `sales`, you may want to see `sales_total`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Another feature of `pd.core.groupby.DataFrameGroupBy` worth reviewing here
    is its ability to deal with multiple `group` arguments. By providing a list, you
    can expand your grouping to cover both `widget` and `region`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'With `pd.core.groupby.DataFrameGroupBy.agg`, there is no limitation on how
    many functions can be applied. For instance, if you want to see the `sum`, `min`,
    and `mean` of `sales` and `returns` within each `widget` and `region`, you could
    simply write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the built-in reduction functions and transformation functions that work
    out of the box with a group by are useful, there may still be times when you need
    to roll with your own custom function. This can be particularly useful when you
    find an algorithm to be `good enough` for what you are attempting in your local
    analysis, but when it may be difficult to generalize to all use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'A commonly requested function in pandas that is not provided out of the box
    with a group by is `mode`, even though there is a `pd.Series.mode` method. With
    `pd.Series.mode`, the type returned is always a `pd.Series`, regardless of whether
    there is only one value that appears most frequently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This is true even if there are two or more elements that appear most frequently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Given that there is a `pd.Series.mode`, why does pandas not offer a similar
    function when doing a group by? From a pandas developer perspective, the reason
    is simple; there is no single way to interpret what a group by should return.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s think through this in more detail with the following example, where `group_a`
    contains two values that appear with the same frequency (42 and 555), whereas
    `group_b` only contains the value 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The question we need to answer is *what should the mode return for group_a?*
    One possible solution would be to return a list (or any Python sequence) that
    holds both 42 and 555\. The downside to this approach is that your returned dtype
    would be `object`, the pitfalls of which we covered back in *Chapter 3*, *Data
    Types*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: A second expectation would be for pandas to just *choose one* of the values.
    Of course, this begs the question as to *how* pandas should make that decision
    – would the value 42 or 555 be more appropriate for `group_a` and how can that
    be determined in a general case?
  prefs: []
  type: TYPE_NORMAL
- en: 'A third expectation would be to return something where the label `group_a`
    appears twice in the resulting row index after aggregation. However, no other
    group by aggregations work this way, so we would be introducing new and potentially
    unexpected behavior by reducing to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Rather than trying to solve for all of these expectations and codify it as part
    of the API, pandas leaves it entirely up to you how you would like to implement
    a `mode` function, as long as you adhere to the expectations that aggregations
    reduce to a single value per group. This eliminates the third expectation we just
    outlined as a possibility, at least until we talk about **Group by** apply later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To that end, if we wanted to roll with our own custom mode functions, they
    may end up looking something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Since these are both aggregations, we can use them in the context of a `pd.core.groupby.DataFrameGroupBy.agg`
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Group by apply
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During our discussion on algorithms and how to apply them back in *Chapter 5*,
    *Algorithms and How to Apply Them*, we came across the Apply function, which is
    both powerful and terrifying at the same time. An equivalent function for group
    by exists as `pd.core.groupby.DataFrameGroupBy.apply` with all of the same caveats.
    Generally, this function is overused, and you should opt for `pd.core.groupby.DataFrameGroupBy.agg`
    or `pd.core.groupby.DataFrameGroupBy.transform` instead. However, for the cases
    where you don’t really want an *aggregation* or a *transformation*, but something
    in between, using `apply` is your only option.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, `pd.core.groupby.DataFrameGroupBy.apply` should only be used as a
    last resort. It can produce sometimes ambiguous behavior and is rather prone to
    breakage across releases of pandas.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the *There’s more…* section of the previous recipe, we mentioned how it
    is not possible to start with a `pd.DataFrame` of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'And to produce the following output, using a custom `mode` algorithm supplied
    to `pd.core.groupby.DataFrameGroupBy.agg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The reason for this is straightforward; an aggregation expects you to reduce
    to a single value per group label. Repeating the label `group_a` twice in the
    output is a non-starter for an aggregation. Similarly, a transformation would
    expect you to produce a result that shares the same row index as the calling `pd.DataFrame`,
    which is not what we are after either.
  prefs: []
  type: TYPE_NORMAL
- en: '`pd.core.groupby.DataFrameGroupBy.apply` is the in-between method that can
    get us closer to the desired result, which you can see in the following code.
    As a technical aside, the `include_groups=False` argument is passed to suppress
    any deprecation warnings about behavior in pandas 2.2\. In subsequent versions,
    you may not need this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to note that we annotated the parameter of the `mode_for_apply`
    function as a `pd.DataFrame`. With aggregations and transformations, user-defined
    functions receive just a single `pd.Series` of data at a time, but with apply,
    you get an entire `pd.DataFrame`. For a more detailed look at what is going on,
    you can add `print` statements to the user-defined function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Essentially, `pd.core.groupby.DataFrameGroupBy.apply` passes a `pd.DataFrame`
    of data to the user-defined function, excluding the column(s) that are used for
    grouping. From there, it will look at the return type of the user-defined function
    and try to infer the best possible output shape it can. In this particular instance,
    because our `mode_for_apply` function returns a `pd.Series`, `pd.core.groupby.DataFrameGroupBy.apply`
    has determined that the best output shape should have a `pd.MultiIndex`, where
    the first level of the index is the group value and the second level contains
    the row index from the `pd.Series` returned by the `mode_for_apply` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where `pd.core.groupby.DataFrameGroupBy.apply` gets overused is in the fact
    that it can change its shape to look like an aggregation when it detects that
    the functions it applies reduce to a single value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: It is a trap to use it in this way, however. Even if it can infer a reasonable
    shape for some outputs, the rules for how it determines that are implementation
    details, for which you pay a performance penalty or run the risk of code breakage
    across pandas releases. If you know your functions will reduce to a single value,
    always opt for `pd.core.groupby.DataFrameGroupBy.agg` in lieu of `pd.core.groupby.DataFrameGroupBy.apply`,
    leaving the latter only for extreme use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Window operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Window operations allow you to calculate values over a sliding partition (or
    “window”) of values. Commonly, these operations are used to calculate things like
    “rolling 90-day average,” but they are flexible enough to extend to any algorithm
    of your choosing.
  prefs: []
  type: TYPE_NORMAL
- en: While not technically a group by operation, window operations are included here
    as they share a similar API and work with “groups” of data. The only difference
    to a group by call is that, instead of forming groups from unique value sets,
    a window operation creates its group by iterating over each value of a pandas
    object and looking at a particular number of preceding (and sometimes following)
    values.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get a feel for how window operations work, let’s start with a simple `pd.Series`
    where each element is an increasing power of 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The first type of window operation you will come across is the “rolling window,”
    accessed via the `pd.Series.rolling` method. When calling this method, you need
    to tell pandas the desired size of your window *n*. The pandas library starts
    at each element and looks backward *n-1* records to form the “window”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'You may notice that we started with a `pd.Int64Dtype()` but ended up with a
    `float64` type after the rolling window operation. Unfortunately, the pandas window
    operations do not work well with the pandas extension system in at least version
    2.2 (see issue #50449), so for the time being, we need to cast the result back
    into the proper data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: So, what is going on here? Essentially, you can think of a rolling window operation
    as iterating through the `pd.Series` values. While doing so, it looks backward
    to try and collect enough values to fulfill the desired window size, which we
    have specified as 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'After collecting two elements in each window, pandas will apply the specified
    aggregation function (in our case, summation). The result of that aggregation
    in each window is then used to piece back together the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Rolling window with sum aggregation'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of our very first record, which cannot form a window with two elements,
    pandas returns a missing value. If you want the rolling calculation to just sum
    up as many elements as it can, even if the window size cannot be reached, you
    can pass an argument to `min_periods=` that dictates the minimum number of elements
    within each window required to perform the aggregation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: By default, rolling window operations look backward to try and fulfill your
    window size requirements. You can also “center” them instead so that pandas looks
    both forward and backward.
  prefs: []
  type: TYPE_NORMAL
- en: 'The effect of this is better seen with an odd window size. Note the difference
    when we expand our call so far with a window size of `3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Compared to the same call with an argument of `center=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Instead of looking at the current and preceding two values, usage of `center=True`
    tells pandas to take the current value, one prior, and one following to form a
    window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another type of window function is the “expanding window”, which looks at all
    prior values encountered. The syntax for that is straightforward; simply replace
    your call to `pd.Series.rolling` with `pd.Series.expanding` and follow that up
    with your desired aggregation function. An expanding summation is similar to the
    `pd.Series.cumsum` method you have seen before, so for demonstration purposes,
    let’s pick a different aggregation function, like `mean`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Visually represented, an expanding window calculation looks as follows (for
    brevity, not all of the `pd.Series` elements are shown):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Expanding window with mean aggregation'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Chapter 9*, *Temporal Data Types and Algorithms*, we will dive deeper into
    some of the very nice features pandas can offer when dealing with temporal data.
    Before we get there, it is worth noting that group by and rolling/expanding window
    functions work very naturally with such data, allowing you to concisely perform
    calculations like, “N day moving averages” “year-to-date X,” “quarter-to-date
    X,” etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how that works, let’s take another look at the Nvidia stock performance
    dataset we started with back in *Chapter 5*, *Algorithms and How to Apply Them*,
    originally as part of the *Calculating a trailing stop order price* recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'With rolling window functions, we can easily add 30, 60, and 90-day moving
    averages. A subsequent call to `pd.DataFrame.plot` also makes this easy to visualize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B31091_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For “year-to-date” and “quarter-to-date” calculations, we can use a combination
    of group by and expanding window functions. For “year-to-date” min, max, and mean
    close values, we can start by forming a group by object to split our data into
    yearly buckets, and from there, we can make a call to `.expanding()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pd.Grouper(freq="YS")` takes our row index, which contains datetimes,
    and groups them by the start of the year within which they fall. After the grouping,
    the call to `.expanding()` performs the min/max aggregations, only looking as
    far back as the start of each year. The effects of this are once again easier
    to see with a visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B31091_08_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For a more granular view, you can calculate the expanding min/max close prices
    per quarter by changing the `freq=` argument from `YS` to `QS` in `pd.Grouper`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B31091_08_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A `MS` `freq=` argument gets you down to the monthly level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B31091_08_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the highest rated movies by year
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most basic and common operations to perform during data analysis
    is to select rows containing the largest value of some column within a group.
    Applied to our movie dataset, this could mean finding the highest-rated film of
    each year or the highest-grossing film by content rating. To accomplish these
    tasks, we need to sort the groups as well as the column used to rank each member
    of the group, and then extract the highest member of each group.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will find the highest-rated film of each year using a combination
    of `pd.DataFrame.sort_values` and `pd.DataFrame.drop_duplicates`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start by reading in the movie dataset and slim it down to just the three columns
    we care about: `movie_title`, `title_year`, and `imdb_score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the `title_year` column gets interpreted as a floating point
    value, but years should always be whole numbers. We could correct that by assigning
    the proper data type directly to our column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we could have passed the desired data type as the `dtype=` argument
    in `pd.read_csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: With our data cleansing out of the way, we can now turn our focus to answering
    the question of “what is the highest rated movie each year?”. There are a few
    ways we can calculate this, but let’s start with the approach you see most commonly.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you perform a group by in pandas, the order in which rows appear in the
    original `pd.DataFrame` is respected as rows are bucketed into different groups.
    Knowing this, many users will answer this question by first sorting their dataset
    across `title_year` and `imdb_score`. After the sort, you can group by the `title_year`
    column, select just the `movie_title` column, and chain in a call to `pd.DataFrameGroupBy.last`
    to select the last value from each group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'A slightly more succinct approach can be had if you use `pd.DataFrameGroupBy.idxmax`,
    which selects the row index value corresponding to the highest movie rating each
    year. This would require you to set the index to the `movie_title` up front:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Our results appear mostly the same, although we can see that the two approaches
    disagreed on what the highest rated movie was in the years 2012 and 2014\. A closer
    look at these titles reveals the root cause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: In case of a tie, each method has its own way of choosing a value. Neither approach
    is right or wrong per se, but if you wanted finer control over that, you would
    have to reach for **Group by apply**.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume we wanted to aggregate the values so that when there is no tie,
    we get back a string, but in case of a tie we get a sequence of strings. To do
    this, you should define a function that accepts a `pd.DataFrame`. This `pd.DataFrame`
    will contain the values associated with each unique grouping column, which is
    `title_year` in our case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the body of the function, you can figure out what the top movie rating
    is, find all movies with that rating, and return back either a single movie title
    (when there are no ties) or a set of movies (in case of a tie):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Comparing the best hitter in baseball across years
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Finding the baseball* *players best at…* recipe back in *Chapter 5*,
    *Algorithms and How to Apply Them*, we worked with a dataset that had already
    aggregated the performance of players from the years 2020-2023\. However, comparing
    players based on their performance across multiple years is rather difficult.
    Even on a year-to-year basis, statistics that appear elite one year can be considered
    just “very good” in other years. The reasons for the variation in statistics across
    years can be debated, but likely come down to some combination of strategy, equipment,
    weather, and just pure statistical chance.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we are going to work with a more granular dataset that goes
    down to the game level. From there, we are going to aggregate the data up to a
    yearly summary, and from there calculate a common baseball statistic known as
    the *batting average*.
  prefs: []
  type: TYPE_NORMAL
- en: For those unfamiliar, a batting average is calculated by taking the number of
    *hits* a player produces (i.e., how many times they swung a bat at a baseball,
    and reached base as a result) as a percentage of their total *at bats* (i.e.,
    how many times they came to bat, excluding *walks*).
  prefs: []
  type: TYPE_NORMAL
- en: So what constitutes a good batting average? As you will see, the answer to that
    question is a moving target, having shifted even within the past twenty years.
    In the early 2000s, a batting average between .260-.270 (i.e., getting a hit in
    26%-27% of at bats) was considered middle of the road for professionals. Within
    recent years, that number has fallen somewhere in the range of .240-.250.
  prefs: []
  type: TYPE_NORMAL
- en: As such, to try and compare the *best hitters* from each year to one another,
    we cannot solely look at the batting average. A league-leading batting average
    of .325 in a year when the league itself averaged .240 is likely more impressive
    than a league-leading batting average of .330 in a year where the overall league
    averaged around .260.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once again, we are going to use data collected from `retrosheet.org`, with
    the following legal disclaimer:'
  prefs: []
  type: TYPE_NORMAL
- en: The information used here was obtained free of charge from and is copyrighted
    by Retrosheet. Interested parties may contact Retrosheet at [www.retrosheet.org](https://www.retrosheet.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe we are going to use “box score” summaries from every regular
    season game played in the years 2000-2023:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'A box score summarizes the performance of every player in a *game*. We could
    therefore single in on a particular game that was played in Baltimore on April
    10, 2015, and see how batters performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'In that game alone we see a total of 75 at bats (*ab*), 29 hits (*h*) and two
    home runs (*hr*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'With a basic understanding of what a box score is and what it shows, let’s
    turn our focus toward calculating the batting average every player produces each
    year. The individual player is notated in the `id` column of our dataset, and
    since we want to see the batting average over the course of an entire season,
    we can use the combination of `year` and `id` as our argument to `pd.DataFrame.groupby`.
    Afterward, we can apply a summation to the at bats (`ab`) and hits (`h`) columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'To turn those totals into a batting average, we can chain in a division using
    `pd.DataFrame.assign`. After that, a call to `pd.DataFrame.drop` will let us solely
    focus on the batting average, dropping the `total_ab` and `total_h` columns we
    no longer need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: Before we continue, we have to consider some data quality issues that may arise
    when calculating averages. Over the course of a baseball season, teams may use
    players who only appear in very niche situations, yielding a low number of plate
    appearances. In some instances, a batter may not even register an “at bat” for
    the season, so using that as a divisor has a chance of dividing by 0, which will
    produce `NaN`. In cases where a batter has a non-zero amount of at bats on the
    season, but still has relatively few, a small sample size can severely skew their
    batting average.
  prefs: []
  type: TYPE_NORMAL
- en: 'Major League Baseball has strict rules for determining how many plate appearances
    it takes for a batter to qualify for records within a given year. Without following
    the rule exactly, and without having to calculate plate appearances in our dataset,
    we can proxy this by setting a requirement of at least 400 at bats over the course
    of a season:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'We can summarize this further by finding the average and maximum `batting_average`
    per season, and we can even use `pd.core.groupby.DataFrameGroupBy.idxmax` to identify
    the player who achieved the best average:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the mean batting average fluctuates each year, with those numbers
    having been higher back toward the year 2000\. In the year 2005, the mean batting
    average was .277, with the best hitter (lee-d002, or Derrek Lee) having hit .335\.
    The best hitter in 2019 (andet001, or Tim Anderson) also averaged .335, but the
    overall league was down around .269\. Therefore, a strong argument could be made
    that Tim Anderson’s 2019 season was more impressive than Derrek Lee’s 2005 season,
    at least through the lens of batting average.
  prefs: []
  type: TYPE_NORMAL
- en: While taking the mean can be useful, it doesn’t tell the full story of what
    goes on within a given season. We would probably like to get a better feel for
    the overall distribution of batting averages across each season, for which a visualization
    is in order. The violin plot we discovered back in the *Plotting movie ratings
    by decade with seaborn* recipe can help us understand this in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'First let’s set up our seaborn import, and have Matplotlib draw plots as soon
    as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will want to make a few considerations for seaborn. Seaborn does not
    make use of `pd.MultiIndex`, so we are going to move our index values to columns
    with a call to `pd.DataFrame.reset_index`. Additionally, seaborn can easily misinterpret
    discrete *year* values like 2000, 2001, 2002, and so on for a continuous range,
    which we can solve by turning that column into a categorical data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pd.CategoricalDtype` we want to construct is also ideally ordered, so
    that pandas can ensure the year 2000 is followed by 2001, which is followed by
    2002, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '23 years of data on a single plot may take up a lot of space, so let’s just
    look at the years 2000-2009 first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B31091_08_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We intentionally made the call to `plt.subplots()` and used `ax.set_xlim(0.15,
    0.4)` so that the x-axis would not change when plotting the remaining years:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B31091_08_10.png)'
  prefs: []
  type: TYPE_IMG
- en: While some years show skew in the data (e.g., 2014 skewing right and 2018 skewing
    left), we can generally imagine the distribution of this data as an approximation
    of a normal distribution. Therefore, to try and better compare the peak performances
    across different years, we can use a technique whereby we *normalize* data within
    each season. Rather than thinking in terms of absolute batting averages like .250,
    we instead think of how far beyond the norm within a season a batter’s performance
    is.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, we can use Z-score normalization, which would appear as
    follows when mathematically represented:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_08_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, `![](img/B31091_08_002.png)` is the mean and `![](img/B31091_08_003.png)`
    is the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating this in pandas is rather trivial; all we need to do is define our
    custom `normalize` function and use that as an argument to `pd.core.groupby.DataFrameGroupBy.transform`
    to assign each combination of year and player their normalized batting average.
    Using that in subsequent group by operations allows us to better compare the peak
    performance each year across different years:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: According to this analysis, the 2023 season by Luis Arráez is the most impressive
    batting average performance since the year 2000\. His `league_max_avg` achieved
    that year may appear as the lowest out of our top five, but so was the `league_mean_avg`
    in 2023.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from this recipe, effective use of pandas’ Group By functionality
    allows you to more fairly evaluate records within different groups. Our example
    used professional baseball players within a season, but that same methodology
    could be extended to evaluate users within different age groups, products within
    different product lines, stocks within different sectors, and so on. Simply put,
    the possibilities for exploring your data with group by are endless!
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/pandas](https://packt.link/pandas)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5040900042138312.png)'
  prefs: []
  type: TYPE_IMG
