<html><head></head><body>
		<div id="_idContainer087">
			<h1 id="_idParaDest-176" class="chapter-number"><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.1.1">9</span></h1>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.2.1">Normalization and Standardization</span></h1>
			<p><span class="koboSpan" id="kobo.3.1">Feature scaling, normalization, and standardization are essential preprocessing steps that help ensure that machine learning models can effectively learn from data. </span><span class="koboSpan" id="kobo.3.2">These techniques address issues related to numerical stability, algorithm convergence, model performance, and more, ultimately contributing to better, more reliable results in data analysis and machine </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">learning tasks.</span></span></p>
			<p><span class="koboSpan" id="kobo.5.1">In this chapter, we will dive deep into the </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">following topics:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.7.1">Scaling features to </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">a range</span></span></li>
				<li><span class="No-Break"><span class="koboSpan" id="kobo.9.1">Z-score scaling</span></span></li>
				<li><span class="No-Break"><span class="koboSpan" id="kobo.10.1">Robust scaling</span></span></li>
			</ul>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.11.1">Technical requirements</span></h1>
			<p><span class="koboSpan" id="kobo.12.1">You can find all the code for this chapter </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter09"><span class="No-Break"><span class="koboSpan" id="kobo.14.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter09</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.15.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.16.1">The different code files follow the names of the different sections of </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">the chapters.</span></span></p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor216"/><span class="koboSpan" id="kobo.18.1">Scaling features to a range</span></h1>
			<p><span class="koboSpan" id="kobo.19.1">Feature scaling is a</span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.20.1"> preprocessing technique in machine learning that rescales the </span><em class="italic"><span class="koboSpan" id="kobo.21.1">range</span></em><span class="koboSpan" id="kobo.22.1"> of independent variables or features of a dataset. </span><span class="koboSpan" id="kobo.22.2">It’s used to ensure that all features contribute equally to the model training process by bringing them to a common scale. </span><span class="koboSpan" id="kobo.22.3">Feature scaling is particularly important for algorithms that are sensitive to the scale of input features, such as k-nearest neighbors and gradient descent-based </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">optimization algorithms.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.24.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.25.1">When scaling features, we’re changing the range of the distribution of </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">the data.</span></span></p>
			<p><span class="koboSpan" id="kobo.27.1">Let’s present an example to </span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.28.1">make the concept of feature scaling easier to grasp. </span><span class="koboSpan" id="kobo.28.2">Let’s suppose you’re working on a machine learning project to predict housing prices based on various features of the houses, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">the following:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.30.1">Square footage (in </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">square feet)</span></span></li>
				<li><span class="koboSpan" id="kobo.32.1">Distance to the nearest school (</span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">in miles)</span></span></li>
				<li><span class="koboSpan" id="kobo.34.1">Distance to the nearest public transportation </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">stop (miles)</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.36.1">Now, let’s discuss why feature scaling is important in this context. </span><span class="koboSpan" id="kobo.36.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.37.1">Square footage</span></strong><span class="koboSpan" id="kobo.38.1"> feature could range from hundreds to thousands of square feet. </span><span class="koboSpan" id="kobo.38.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.39.1">Distance to the nearest school</span></strong><span class="koboSpan" id="kobo.40.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.41.1">Distance to the nearest public transportation stop</span></strong><span class="koboSpan" id="kobo.42.1"> features might range from a fraction of a mile to several miles. </span><span class="koboSpan" id="kobo.42.2">If you don’t scale these features, the algorithm may give excessive importance to the larger values, making square footage the dominant factor in predicting house prices. </span><span class="koboSpan" id="kobo.42.3">Features such as Distance to the nearest school might be </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">unfairly downplayed.</span></span></p>
			<p><span class="koboSpan" id="kobo.44.1">For all the sections in this chapter, we will use the above example to showcase the different scaling methods. </span><span class="koboSpan" id="kobo.44.2">Let’s go through the data creation for this example; the code can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter09/min_max_scaling.py"><span class="No-Break"><span class="koboSpan" id="kobo.46.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter09/min_max_scaling.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.47.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.48.1">Let’s start by importing the </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">required libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.50.1">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler</span></pre></li>				<li><span class="koboSpan" id="kobo.51.1">Next, we will create a dataset with features related to </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">housing prices:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.53.1">
np.random.seed(42)
num_samples = 100</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.54.1">We’ll create the following features that affect the price of </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">a house:</span></span></p><ul><li><span class="koboSpan" id="kobo.56.1">Square footage in </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">square feet:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.58.1">square_footage = np.random.uniform(500, 5000, num_samples)</span></pre></li><li><span class="koboSpan" id="kobo.59.1">Distance to the nearest school </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">in miles:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.61.1">distance_to_school = np.random.uniform(0.1, 5, num_samples)</span></pre></li><li><span class="koboSpan" id="kobo.62.1">Commute distance to work </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">in miles:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.64.1">commute_distance = np.random.exponential(5, num_samples)</span></pre></li><li><span class="koboSpan" id="kobo.65.1">Traffic density (</span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">skewed feature):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.67.1">traffic_density = np.random.exponential(2, num_samples)</span></pre></li></ul></li>				<li><span class="koboSpan" id="kobo.68.1">Then, we </span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.69.1">create a DataFrame that holds all </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">the features:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.71.1">
data = pd.DataFrame({
        'Square_Footage': square_footage,
        'Distance_to_School': distance_to_school,
        'Commute_Distance': commute_distance,
        'Traffic_Density': traffic_density
})</span></pre></li>				<li><span class="koboSpan" id="kobo.72.1">Finally, we plot the </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">original distributions:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.74.1">
plt.figure(figsize=(12, 8))</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.75.1">You can see the original distributions of the </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">data here:</span></span></p></li>			</ol>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<span class="koboSpan" id="kobo.77.1"><img src="image/B19801_09_1.jpg" alt="Figure 9.1 – Distribution of house pricing prediction use case"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.78.1">Figure 9.1 – Distribution of house pricing prediction use case</span></p>
			<ol>
				<li value="5"><span class="koboSpan" id="kobo.79.1">Let’s display </span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.80.1">the statistics of </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">our dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.82.1">
print("Original Dataset Statistics:")
print(data.describe())</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.83.1">This will print the </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">following output:</span></span></p></li>			</ol>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<span class="koboSpan" id="kobo.85.1"><img src="image/B19801_09_2.jpg" alt="Figure 9.2 – Original dataset statistics"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.86.1">Figure 9.2 – Original dataset statistics</span></p>
			<p><span class="koboSpan" id="kobo.87.1">Now let’s discuss one of the most common methods for scaling – </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.88.1">min-max scaling</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">.</span></span></p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor217"/><span class="koboSpan" id="kobo.90.1">Min-max scaling</span></h1>
			<p><span class="koboSpan" id="kobo.91.1">Min-max scaling, also</span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.92.1"> known as </span><strong class="bold"><span class="koboSpan" id="kobo.93.1">normalization</span></strong><span class="koboSpan" id="kobo.94.1">, scales the values of a variable </span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.95.1">to a specific range, typically between 0 and 1. </span><span class="koboSpan" id="kobo.95.2">Min-max scaling is useful when you want to ensure that all values in a variable fall within a standardized range, making them directly comparable. </span><span class="koboSpan" id="kobo.95.3">It is commonly employed when the distribution of the variable is not assumed to </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">be normal.</span></span></p>
			<p><span class="koboSpan" id="kobo.97.1">Let’s have a look at the formula for calculating </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">min-max scaling:</span></span></p>
			<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.99.1">X</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.100.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.101.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.102.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.103.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.104.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.105.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.106.1">d</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.107.1">=</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.108.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.109.1">X</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.110.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.111.1">X</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.112.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.113.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.114.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.115.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.116.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.117.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.118.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.119.1">X</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.120.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.121.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.122.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.123.1">x</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.124.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.125.1">X</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.126.1">_</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.127.1">m</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.128.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.129.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.130.1">)</span></span></span></p>
			<p><span class="koboSpan" id="kobo.131.1">As you can see from the formula, min-max scaling preserves the relative ordering of values but </span><em class="italic"><span class="koboSpan" id="kobo.132.1">compresses them into a specific range</span></em><span class="koboSpan" id="kobo.133.1">. </span><span class="koboSpan" id="kobo.133.2">One thing to note here is that it is </span><em class="italic"><span class="koboSpan" id="kobo.134.1">not a way to deal with outliers</span></em><span class="koboSpan" id="kobo.135.1"> and if outliers exist in the data, these extreme values can disproportionately influence the scaling. </span><span class="koboSpan" id="kobo.135.2">So, it is a good practice to deal with outliers first and then proceed to the scaling </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">of features.</span></span></p>
			<p><span class="koboSpan" id="kobo.137.1">Scaling to a specific range is a suitable approach when the following conditions </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">are satisfied:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.139.1">You have prior knowledge of the approximate upper and lower bounds of </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">your data</span></span></li>
				<li><span class="koboSpan" id="kobo.141.1">Your data follows a relatively uniform or bell-shaped distribution across </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">that range</span></span></li>
				<li><span class="koboSpan" id="kobo.143.1">Your chosen machine learning algorithm or model benefits from having features within a specific range, typically </span><strong class="source-inline"><span class="koboSpan" id="kobo.144.1">[0, 1]</span></strong><span class="koboSpan" id="kobo.145.1"> or any other </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">desired range</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.147.1">A classic example of this scenario is age. </span><span class="koboSpan" id="kobo.147.2">Age values typically span from 0 to 90 and the entire range includes a significant number of individuals. </span><span class="koboSpan" id="kobo.147.3">However, scaling something such as income is not really recommended as there are a limited number of individuals with exceptionally high incomes. </span><span class="koboSpan" id="kobo.147.4">If you were to apply linear scaling to income, the upper limit of the scale would become exceedingly high, and most data points would be concentrated in a small segment of the scale, leading to loss of information and </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">skewed representations.</span></span></p>
			<p><span class="koboSpan" id="kobo.149.1">Let’s have a look at the code for </span><em class="italic"><span class="koboSpan" id="kobo.150.1">the house pricing prediction use case</span></em><span class="koboSpan" id="kobo.151.1"> we discussed previously to understand how a min-max scaler can transform </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">the data:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.153.1">First, we’ll scale the data </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.155.1">MinMaxScaler()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.157.1">
scaler = MinMaxScaler()
data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)</span></pre></li>				<li><span class="koboSpan" id="kobo.158.1">We can </span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.159.1">display the dataset statistics after scaling using the </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.161.1">
print("\nDataset Statistics After Scaling:")
print(data_scaled.describe())</span></pre></li>				<li><span class="koboSpan" id="kobo.162.1">Let’s plot and observe the distributions </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">after scaling:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.164.1">
plt.figure(figsize=(12, 8))</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.165.1">Let’s have a look at the modified data distributions after </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">the scaling:</span></span></p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<span class="koboSpan" id="kobo.167.1"><img src="image/B19801_09_3.jpg" alt="Figure 9.3 – Distribution of house pricing prediction use case after min-max scaling"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.168.1">Figure 9.3 – Distribution of house pricing prediction use case after min-max scaling</span></p>
			<p><span class="koboSpan" id="kobo.169.1">We applied min-max scaling to transform each column into a standardized range between 0 and 1. </span><span class="koboSpan" id="kobo.169.2">The shape of the original feature distributions remained the same after scaling because min-max scaling maintains the relative distances between data points. </span><span class="koboSpan" id="kobo.169.3">The scaling</span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.170.1"> had a normalization effect on the data, bringing all features to a common scale. </span><span class="koboSpan" id="kobo.170.2">This is important when features have different units or ranges, preventing one feature from </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">dominating others.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.172.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.173.1">If your dataset is sparse (contains many zero values), min-max scaling may not be appropriate, as it can result in loss of information. </span><span class="koboSpan" id="kobo.173.2">Alternative methods such as </span><strong class="bold"><span class="koboSpan" id="kobo.174.1">MaxAbsScaler</span></strong><span class="koboSpan" id="kobo.175.1"> or</span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.176.1"> robust scalers may </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">be considered.</span></span></p>
			<p><span class="koboSpan" id="kobo.178.1">In the following section, we will discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">z-score scaling.</span></span></p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor218"/><span class="koboSpan" id="kobo.180.1">Z-score scaling</span></h1>
			<p><span class="koboSpan" id="kobo.181.1">Z-score scaling, also</span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.182.1"> known </span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.183.1">as standardization, is applied when you want to transform your data to have a mean of 0 and a standard deviation of 1. </span><span class="koboSpan" id="kobo.183.2">Z-score scaling is widely used in statistical analysis and machine learning, especially when algorithms such as</span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.184.1"> k-means clustering or </span><strong class="bold"><span class="koboSpan" id="kobo.185.1">Principal Component Analysis</span></strong><span class="koboSpan" id="kobo.186.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.187.1">PCA</span></strong><span class="koboSpan" id="kobo.188.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">are employed.</span></span></p>
			<p><span class="koboSpan" id="kobo.190.1">Here is the formula </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">for z-score:</span></span></p>
			<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.192.1">X</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.193.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.194.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.195.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.196.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.197.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.198.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.199.1">d</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.200.1">=</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.201.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.202.1">X</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.203.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.204.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.205.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.206.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.207.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.208.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.209.1">X</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.210.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.211.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.212.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.213.1">s</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.214.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.215.1">d</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.216.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.217.1">X</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.218.1">)</span></span></span></p>
			<p><span class="koboSpan" id="kobo.219.1">Let’s continue with the house pricing prediction use case to showcase the z-score scaling. </span><span class="koboSpan" id="kobo.219.2">The code can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter09/zscaler.py"><span class="No-Break"><span class="koboSpan" id="kobo.221.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter09/zscaler.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.222.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.223.1">We first perform </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">z-score scaling:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.225.1">
data_zscore = (data - data.mean()) / data.std()</span></pre></li>				<li><span class="koboSpan" id="kobo.226.1">Then, we print the </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">dataset statistics:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.228.1">
print("\nDataset Statistics after Z-score Scaling:")
print(data_zscore.describe())</span></pre></li>				<li><span class="koboSpan" id="kobo.229.1">Finally, we visualize </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">the distributions:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.231.1">
data_zscore.hist(figsize=(12, 10), bins=20, color='green', alpha=0.7)
plt.suptitle('Data Distributions after Z-score Scaling')
plt.show()</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.232.1">Let’s have a look at </span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.233.1">the modified data distributions after </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">the scaling:</span></span></p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<span class="koboSpan" id="kobo.235.1"><img src="image/B19801_09_4.jpg" alt="Figure 9.4 – Distribution of house pricing prediction use case after Z-scaling"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.236.1">Figure 9.4 – Distribution of house pricing prediction use case after Z-scaling</span></p>
			<p><span class="koboSpan" id="kobo.237.1">The mean of each feature after scaling is very close to 0, which is expected in z-score scaling. </span><span class="koboSpan" id="kobo.237.2">The data is now centered around 0. </span><span class="koboSpan" id="kobo.237.3">The standard deviation for each feature is now approximately 1, making the scales comparable. </span><span class="koboSpan" id="kobo.237.4">The minimum and maximum values have been transformed, maintaining the relative distribution </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">of data.</span></span></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor219"/><span class="koboSpan" id="kobo.239.1">When to use Z-score scaling</span></h2>
			<p><span class="koboSpan" id="kobo.240.1">Let’s also discuss </span><a id="_idIndexMarker713"/><span class="koboSpan" id="kobo.241.1">some considerations regarding when to use </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">z-score scaling:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.243.1">Z-score scaling assumes that the data is approximately normally distributed, or at least symmetrically distributed, around a central mean value. </span><span class="koboSpan" id="kobo.243.2">If your data is highly skewed or has a non-standard distribution, standardization may not be as effective in making the data more Gaussian. </span><span class="koboSpan" id="kobo.243.3">As you can see, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">Commute_Distance</span></strong><span class="koboSpan" id="kobo.245.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">Traffic_Density</span></strong><span class="koboSpan" id="kobo.247.1"> features are skewed and the z-score scaling was not that successful as the data is not centered around the mean compared to the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">the features.</span></span></li>
				<li><span class="koboSpan" id="kobo.249.1">This is most applicable to numerical features rather than categorical or ordinal ones. </span><span class="koboSpan" id="kobo.249.2">Ensure that the data you are standardizing is quantitative </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">in nature.</span></span></li>
				<li><span class="koboSpan" id="kobo.251.1">Extreme outliers can have a significant impact on the mean and standard deviation, which are used in z-score scaling. </span><span class="koboSpan" id="kobo.251.2">It’s important to address outliers before standardization, as they can distort the </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">scaling effect.</span></span></li>
				<li><span class="koboSpan" id="kobo.253.1">Z-score scaling assumes a linear relationship between variables. </span><span class="koboSpan" id="kobo.253.2">If the underlying relationship is non-linear, other scaling methods or transformations may be </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">more appropriate.</span></span></li>
				<li><span class="koboSpan" id="kobo.255.1">Z-score scaling assumes that the variables are independent or at least not highly correlated. </span><span class="koboSpan" id="kobo.255.2">If variables are highly correlated, standardization may not provide additional benefits, and correlation structure should be </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">considered separately.</span></span></li>
				<li><span class="koboSpan" id="kobo.257.1"> Z-score scaling alters the original units of the data, which can affect the interpretability of results. </span><span class="koboSpan" id="kobo.257.2">Consider whether maintaining the original units is essential for </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">your analysis.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.259.1">For small datasets, the impact of z-score scaling can be more pronounced. </span><span class="koboSpan" id="kobo.259.2">Be cautious when applying standardization to very small datasets, as it can potentially overemphasize the effects </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">of outliers.</span></span></p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor220"/><span class="koboSpan" id="kobo.261.1">Robust scaling</span></h1>
			<p><span class="koboSpan" id="kobo.262.1">Robust scaling, also known as </span><strong class="bold"><span class="koboSpan" id="kobo.263.1">robust standardization</span></strong><span class="koboSpan" id="kobo.264.1">, is a method of feature scaling that is </span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.265.1">particularly </span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.266.1">useful when dealing with datasets containing outliers. </span><span class="koboSpan" id="kobo.266.2">Unlike min-max scaling and z-score scaling, which can be sensitive to outliers, robust scaling is designed to be robust in the presence of extreme values. </span><span class="koboSpan" id="kobo.266.3">It is especially beneficial when you want to normalize or standardize features while minimizing the impact of extreme values. </span><span class="koboSpan" id="kobo.266.4">Robust scaling is also suitable for datasets where the features do not follow a normal distribution and may have skewness or </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">heavy tails.</span></span></p>
			<p><span class="koboSpan" id="kobo.268.1">Here is the formula for </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">robust scaling:</span></span></p>
			<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.270.1">X</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.271.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.272.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.273.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.274.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.275.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.276.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.277.1">d</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.278.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.279.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.280.1">X</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.281.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.282.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.283.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.284.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.285.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.286.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.287.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.288.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.289.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.290.1">I</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.291.1">Q</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.292.1">R</span></span></span></p>
			<p><span class="koboSpan" id="kobo.293.1">Subtracting the median and dividing </span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.294.1">by the </span><strong class="bold"><span class="koboSpan" id="kobo.295.1">Interquartile Range</span></strong><span class="koboSpan" id="kobo.296.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.297.1">IQR</span></strong><span class="koboSpan" id="kobo.298.1">)in the scaling process normalizes the data by centering it around the median and scaling it based on the spread represented by the IQR. </span><span class="koboSpan" id="kobo.298.2">This normalization helps to mitigate the impact of extreme values, making the scaled values more representative of the </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">overall distribution.</span></span></p>
			<p><span class="koboSpan" id="kobo.300.1">As we have already discussed in the previous chapter, the median being the middle value when the data is ordered makes robust scaling less sensitive to extreme values or outliers than other scaling methods that rely on the mean. </span><span class="koboSpan" id="kobo.300.2">In addition, the IQR, representing the range between the first quartile (Q1) and the third quartile (Q3), is also robust against outliers. </span><span class="koboSpan" id="kobo.300.3">Unlike the full range or standard deviation, the IQR focuses on the middle 50% of the data, making it less affected by </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">extreme values.</span></span></p>
			<p><span class="koboSpan" id="kobo.302.1">Here’s an example of how to apply robust scaling using </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">Python code:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.304.1">
robust_scaler = RobustScaler()
data_scaled = robust_scaler.fit_transform(data)</span></pre>			<p><span class="koboSpan" id="kobo.305.1">Let’s have a look at the modified data distributions after </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">the scaling:</span></span></p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<span class="koboSpan" id="kobo.307.1"><img src="image/B19801_09_5.jpg" alt="Figure 9.5 – Distribution of house pricing prediction use case after robust scaling"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.308.1">Figure 9.5 – Distribution of house pricing prediction use case after robust scaling</span></p>
			<p><span class="koboSpan" id="kobo.309.1">After the robust </span><a id="_idIndexMarker717"/><span class="koboSpan" id="kobo.310.1">scaling exercise, we can observe that the central tendency of the data has shifted, and that the mean of each feature is now closer to zero. </span><span class="koboSpan" id="kobo.310.2">This is because the robust scaling process subtracts the median. </span><span class="koboSpan" id="kobo.310.3">As for the spread of the data, it changed as a result of dividing by the IQR. </span><span class="koboSpan" id="kobo.310.4">The variability in each feature is now represented in a more consistent manner, robust to outliers. </span><span class="koboSpan" id="kobo.310.5">The range of values for each feature is now compressed, particularly for features with larger initial ranges, preventing dominance by features with </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">extreme values.</span></span></p>
			<p><span class="koboSpan" id="kobo.312.1">To close the chapter, we have created a summary table presenting all the techniques discussed so far, including information on when to use them, as well as the advantages and disadvantages </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">of each.</span></span></p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor221"/><span class="koboSpan" id="kobo.314.1">Comparison between methods</span></h1>
			<p><span class="koboSpan" id="kobo.315.1">This chart</span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.316.1"> provides guidance on which scaling technique is suitable for various </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">data scenarios.</span></span></p>
			<table id="table001-6" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.318.1">Scaling method</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.319.1">When </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.320.1">to use</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.321.1">Pros</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.322.1">Cons</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.323.1">Min-max scaling</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.324.1">Features have a clear, </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">known range</span></span></p>
							<p><span class="koboSpan" id="kobo.326.1">Normal distribution </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">is assumed</span></span></p>
							<p><span class="koboSpan" id="kobo.328.1">Data does not </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">contain outliers</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.330.1">Simple and easy </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">to understand</span></span></p>
							<p><span class="koboSpan" id="kobo.332.1">Preserves relative </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">relationships</span></span></p>
							<p><span class="No-Break"><span class="koboSpan" id="kobo.334.1">Memory efficient</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.335.1">Sensitive </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">to outliers</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.337.1">Z-score scaling</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.338.1">Data follows a </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">normal distribution</span></span></p>
							<p><span class="koboSpan" id="kobo.340.1">No strong assumptions about </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">the range</span></span></p>
							<p><span class="koboSpan" id="kobo.342.1">Handling outliers is not </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">a priority</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.344.1">Standardizes data to zero mean and deviation </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">of 1</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.346.1">Sensitive </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">to outliers</span></span></p>
							<p><span class="koboSpan" id="kobo.348.1">May not be suitable for </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">skewed data</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.350.1">Robust scaling</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.351.1">Data </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">contains outliers</span></span></p>
							<p><span class="koboSpan" id="kobo.353.1">Skewed distributions or </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">non-normal data</span></span></p>
							<p><span class="koboSpan" id="kobo.355.1">Equalizing </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">feature contributions</span></span></p>
							<p><span class="koboSpan" id="kobo.357.1">Resilience to varying </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">feature variances</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.359.1">Less sensitive </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">to outliers</span></span></p>
							<p><span class="koboSpan" id="kobo.361.1">Preserves central tendency </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">and spread</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.363.1">Computationally </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">more expensive</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.365.1">Table 9.1 – Comparing different scaling techniques</span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.366.1">Computational complexity</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.367.1">Min-max scaling tends to be more memory-efficient, especially when dealing with large datasets. </span><span class="koboSpan" id="kobo.367.2">This is because min-max scaling only involves scaling the values based on the minimum and maximum values of each feature, and the computation is </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">relatively straightforward.</span></span></p>
			<p class="callout"><span class="koboSpan" id="kobo.369.1">On the other hand, z-score scaling and robust scaling both require additional calculations such as mean, standard deviation (for z-score scaling), median, and interquartile range (for robust scaling), which may involve more memory usage. </span><span class="koboSpan" id="kobo.369.2">The computational complexity and memory requirements of z-score scaling and robust scaling can become more pronounced, particularly when dealing with </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">large datasets.</span></span></p>
			<p><span class="koboSpan" id="kobo.371.1">Finally, let’s summarize our learning for this chapter and get inspired for the </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">next one.</span></span></p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor222"/><span class="koboSpan" id="kobo.373.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.374.1">In this chapter, we explored three common methods for scaling numerical features: min-max scaling, z-score scaling, and robust scaling. </span><span class="koboSpan" id="kobo.374.2">Min-max scaling transforms data to a specific range, making it suitable for algorithms sensitive to feature magnitudes. </span><span class="koboSpan" id="kobo.374.3">Z-score scaling standardizes data to zero mean and unit variance, providing a standardized distribution. </span><span class="koboSpan" id="kobo.374.4">Robust scaling, robust to outliers, employs the median and interquartile range, making it suitable for datasets with skewed distributions or outliers. </span><span class="koboSpan" id="kobo.374.5">We also went through different considerations to keep in mind while deciding on the best approach for your </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">use case.</span></span></p>
			<p><span class="koboSpan" id="kobo.376.1">Moving forward, we’ll shift our focus to handling categorical features in the </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">next chapter.</span></span></p>
		</div>
	</body></html>