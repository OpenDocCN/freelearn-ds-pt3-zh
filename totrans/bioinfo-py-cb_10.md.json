["```py\n    docker pull bgruening/galaxy-stable:20.09\n    ```", "```py\n    docker run -d -v YOUR_DIRECTORY:/export -p 8080:80 -p 8021:21 bgruening/galaxy-stable:20.09\n    ```", "```py\nrest_protocol: http\nserver: localhost\nrest_port: 8080\nsftp_port: 8022\nuser: admin\npassword: password\napi_key: fakekey\n```", "```py\nimport base64\nimport getpass\nfrom io import StringIO\nimport os\nfrom ruamel.yaml import YAML\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\npassword = getpass.getpass('Please enter the password:').encode()\nsalt = os.urandom(16)\nkdf = PBKDF2HMAC(algorithm=hashes.SHA256(), length=32, salt=salt,\n                 iterations=100000, backend=default_backend())\nkey = base64.urlsafe_b64encode(kdf.derive(password))\nfernet = Fernet(key)\nwith open('salt', 'wb') as w:\n    w.write(salt)\nyaml = YAML()\ncontent = yaml.load(open('galaxy.yaml', 'rt', encoding='utf-8'))\nprint(type(content), content)\noutput = StringIO()\nyaml.dump(content, output)\nprint ('Encrypting:\\n%s' % output.getvalue())\nenc_output = fernet.encrypt(output.getvalue().encode())\nwith open('galaxy.yaml.enc', 'wb') as w:\n    w.write(enc_output) \n```", "```py\n    import base64\n    from collections import defaultdict\n    import getpass\n    import pprint\n    import warnings\n    from ruamel.yaml import YAML\n    from cryptography.fernet import Fernet\n    from cryptography.hazmat.backends import default_backend\n    from cryptography.hazmat.primitives import hashes\n    from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n    import pandas as pd\n    Import paramiko\n    from bioblend.galaxy import GalaxyInstance\n    pp = pprint.PrettyPrinter()\n    warnings.filterwarnings('ignore')\n    # explain above, and warn\n    with open('galaxy.yaml.enc', 'rb') as f:\n        enc_conf = f.read()\n    password = getpass.getpass('Please enter the password:').encode()\n    with open('salt', 'rb') as f:\n        salt = f.read()\n    kdf = PBKDF2HMAC(algorithm=hashes.SHA256(), length=32, salt=salt,\n                     iterations=100000, backend=default_backend())\n    key = base64.urlsafe_b64encode(kdf.derive(password))\n    fernet = Fernet(key)\n    yaml = YAML()\n    conf = yaml.load(fernet.decrypt(enc_conf).decode())\n    ```", "```py\n    server = conf['server']\n    rest_protocol = conf['rest_protocol']\n    rest_port = conf['rest_port']\n    user = conf['user']\n    password = conf['password']\n    ftp_port = int(conf['ftp_port'])\n    api_key = conf['api_key']\n    rest_url = '%s://%s:%d' % (rest_protocol, server, rest_port)\n    history_name = 'bioinf_example'\n    ```", "```py\n    gi = GalaxyInstance(url=rest_url, key=api_key)\n    gi.verify = False\n    ```", "```py\n    histories = gi.histories\n    print('Existing histories:')\n    for history in histories.get_histories():\n        if history['name'] == history_name:\n            histories.delete_history(history['id'])\n        print('  - ' + history['name'])\n    print()\n    ```", "```py\n    ds_history = histories.create_history(history_name)\n    ```", "```py\n    print('Uploading file')\n    transport = paramiko.Transport((server, sftp_port))\n    transport.connect(None, user, password)\n    sftp = paramiko.SFTPClient.from_transport(transport)\n    sftp.put('LCT.bed', 'LCT.bed')\n    sftp.close()\n    transport.close()\n    ```", "```py\n    gi.tools.upload_from_ftp('LCT.bed', ds_history['id'])\n    ```", "```py\n    def summarize_contents(contents):\n     summary = defaultdict(list)\n     for item in contents:\n     summary['íd'].append(item['id'])\n     summary['híd'].append(item['hid'])\n     summary['name'].append(item['name'])\n     summary['type'].append(item['type'])\n     summary['extension'].append(item['extension'])\n     return pd.DataFrame.from_dict(summary)\n    print('History contents:')\n    pd_contents = summarize_contents(contents)\n    print(pd_contents)\n    print()\n    ```", "```py\n                 íd  híd     name  type extension\n0  f2db41e1fa331b3e    1  LCT.bed  file      auto\n```", "```py\n    print('Metadata for LCT.bed')\n    bed_ds = contents[0]\n    pp.pprint(bed_ds)\n    print()\n    ```", "```py\n{'create_time': '2018-11-28T21:27:28.952118',\n 'dataset_id': 'f2db41e1fa331b3e',\n 'deleted': False,\n 'extension': 'auto',\n 'hid': 1,\n 'history_content_type': 'dataset',\n 'history_id': 'f2db41e1fa331b3e',\n 'id': 'f2db41e1fa331b3e',\n 'name': 'LCT.bed',\n 'purged': False,\n 'state': 'queued',\n 'tags': [],\n 'type': 'file',\n 'type_id': 'dataset-f2db41e1fa331b3e',\n 'update_time': '2018-11-28T21:27:29.149933',\n 'url': '/api/histories/f2db41e1fa331b3e/contents/f2db41e1fa331b3e',\n 'visible': True}\n```", "```py\n    print('Metadata about all tools')\n    all_tools = gi.tools.get_tools()\n    pp.pprint(all_tools)\n    print()\n    ```", "```py\n    bed2gff = gi.tools.get_tools(name='Convert BED to GFF')[0]\n    print(\"Converter metadata:\")\n    pp.pprint(gi.tools.show_tool(bed2gff['id'], io_details=True, link_details=True))\n    print()\n    ```", "```py\n{'config_file': '/galaxy-central/lib/galaxy/datatypes/converters/bed_to_gff_converter.xml',\n 'id': 'CONVERTER_bed_to_gff_0',\n 'inputs': [{'argument': None,\n             'edam': {'edam_data': ['data_3002'],\n                      'edam_formats': ['format_3003']},\n             'extensions': ['bed'],\n             'label': 'Choose BED file',\n             'multiple': False,\n             'name': 'input1',\n             'optional': False,\n             'type': 'data',\n             'value': None}],\n 'labels': [],\n 'link': '/tool_runner?tool_id=CONVERTER_bed_to_gff_0',\n 'min_width': -1,\n 'model_class': 'Tool',\n 'name': 'Convert BED to GFF',\n 'outputs': [{'edam_data': 'data_1255',\n              'edam_format': 'format_2305',\n              'format': 'gff',\n              'hidden': False,\n              'model_class': 'ToolOutput',\n              'name': 'output1'}],\n 'panel_section_id': None,\n 'panel_section_name': None,\n 'target': 'galaxy_main',\n 'version': '2.0.0'}\n```", "```py\n    def dataset_to_param(dataset):\n        return dict(src='hda', id=dataset['id'])\n    tool_inputs = {\n        'input1': dataset_to_param(bed_ds)\n        }\n    gi.tools.run_tool(ds_history['id'], bed2gff['id'], tool_inputs=tool_inputs)\n    ```", "```py\n    from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider \n    HTTP = HTTPRemoteProvider()\n    download_root = \"https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3\"\n    remote_hapmap_map = f\"{download_root}/plink_format/hapmap3_r3_b36_fwd.consensus.qc.poly.map.gz\"\n    remote_hapmap_ped = f\"{download_root}/plink_format/hapmap3_r3_b36_fwd.consensus.qc.poly.ped.gz\"\n    remote_hapmap_rel = f\"{download_root}/relationships_w_pops_041510.txt\"\n\n    rule plink_download:\n        input:\n            map=HTTP.remote(remote_hapmap_map, keep_local=True),\n            ped=HTTP.remote(remote_hapmap_ped, keep_local=True),\n            rel=HTTP.remote(remote_hapmap_rel, keep_local=True)\n\n        output:\n            map=\"scratch/hapmap.map.gz\",\n            ped=\"scratch/hapmap.ped.gz\",\n            rel=\"data/relationships.txt\"\n\n        shell:\n            \"mv {input.map} {output.map};\"\n            \"mv {input.ped} {output.ped};\"\n            \"mv {input.rel} {output.rel}\"\n    ```", "```py\n    snakemake -c1 data/relationships.txt\n    ```", "```py\nBuilding DAG of jobs...\nUsing shell: /usr/bin/bash\nProvided cores: 1 (use --cores to define parallelism)\nRules claiming more threads will be scaled down.\nJob stats:\njob               count    min threads    max threads\n--------------  -------  -------------  -------------\nplink_download        1              1              1\ntotal                 1              1              1\n\nSelect jobs to execute...\n\n[Mon Jun 13 18:54:26 2022]\nrule plink_download:\n    input: ftp.ncbi.nlm.nih.gov/hapmap/ge [...]\n    output: [..], data/relationships.txt\n    jobid: 0\n    reason: Missing output files: data/relationships.txt\n    resources: tmpdir=/tmp\n\nDownloading from remote: [...]relationships_w_pops_041510.txt\nFinished download.\n[...]\nFinished job 0.\n1 of 1 steps (100%) done\n```", "```py\n    PLINKEXTS = ['ped', 'map']\n    rule uncompress_plink:\n        input:\n            \"scratch/hapmap.{plinkext}.gz\"\n        output:\n            \"data/hapmap.{plinkext}\"\n        shell:\n            \"gzip -dc {input} > {output}\"\n    ```", "```py\n    rule subsample_1p:\n        input:\n            \"data/hapmap.ped\",\n            \"data/hapmap.map\"\n\n        output:\n            \"data/hapmap1.ped\",\n            \"data/hapmap1.map\"\n\n        run:\n            shell(f\"plink2 --pedmap {input[0][:-4]} --out {output[0][:-4]} --thin 0.01 --geno 0.1 --export ped\")\n    ```", "```py\n    rule plink_pca:\n        input:\n            \"data/hapmap1.ped\",\n            \"data/hapmap1.map\"\n        output:\n            \"data/hapmap1.eigenvec\",\n            \"data/hapmap1.eigenval\"\n        shell:\n            \"plink2 --pca --file data/hapmap1 -out data/hapmap1\"\n    ```", "```py\n    snakemake --dag data/hapmap1.eigenvec | dot -Tsvg > bio.svg\n    ```", "```py\n    rule plot_pca:\n        input:\n            \"data/hapmap1.eigenvec\",\n            \"data/hapmap1.eigenval\"\n\n        output:\n            \"pca.png\"\n\n        script:\n            \"./plot_pca.py\"\n    ```", "```py\n    import pandas as pd\n\n    eigen_fname = snakemake.input[0] if snakemake.input[0].endswith('eigenvec') else snakemake.input[1]\n    pca_df = pd.read_csv(eigen_fname, sep='\\t') \n    ax = pca_df.plot.scatter(x=2, y=3, figsize=(16, 9))\n    ax.figure.savefig(snakemake.output[0])\n    ```", "```py\n    nextflow.enable.dsl=2\n    download_root = \"https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3\"\n     process plink_download {\n      output:\n      path 'hapmap.map.gz'\n      path 'hapmap.ped.gz'\n      script:\n      \"\"\"\n      wget $download_root/plink_format/hapmap3_r3_b36_fwd.consensus.qc.poly.map.gz -O hapmap.map.gz\n      wget $download_root/plink_format/hapmap3_r3_b36_fwd.consensus.qc.poly.ped.gz -O hapmap.ped.gz\n       \"\"\"\n    }\n    ```", "```py\n    process uncompress_plink {\n      publishDir 'data', glob: '*', mode: 'copy'\n\n      input:\n      path mapgz\n      path pedgz\n\n      output:\n      path 'hapmap.map'\n      path 'hapmap.ped'\n\n      script:\n      \"\"\"\n      gzip -dc $mapgz > hapmap.map\n      gzip -dc $pedgz > hapmap.ped\n      \"\"\"\n    }\n    ```", "```py\n    workflow {\n        plink_download | uncompress_plink\n    }\n    ```", "```py\n    nextflow run pipeline.nf -resume\n    ```", "```py\n    workflow {\n        ped_file = file('data/hapmap.ped')\n        map_file = file('data/hapmap.map')\n        if (!ped_file.exists() | !map_file.exists()) {\n            plink_download | uncompress_plink\n        }\n    }\n    ```", "```py\n    process subsample_1p {\n      input:\n      path 'hapmap.map'\n      path 'hapmap.ped'\n\n      output:\n      path 'hapmap1.map'\n      path 'hapmap1.ped'\n\n      script:\n      \"\"\"\n      plink2 --pedmap hapmap --out hapmap1 --thin 0.01 --geno 0.1 --export ped\n      \"\"\"\n    }\n    ```", "```py\n    process plink_pca {\n      input:\n      path 'hapmap.map'\n      path 'hapmap.ped'\n      output:\n      path 'hapmap.eigenvec'\n      path 'hapmap.eigenval'\n       script:\n      \"\"\"\n      plink2 --pca --pedmap hapmap -out hapmap\n      \"\"\"\n    }\n    ```", "```py\n    process plot_pca {\n      publishDir '.', glob: '*', mode: 'copy'\n\n      input:\n      path 'hapmap.eigenvec'\n      path 'hapmap.eigenval'\n\n      output:\n      path 'pca.png'\n\n      script:\n      \"\"\"\n      #!/usr/bin/env python\n      import pandas as pd\n\n      pca_df = pd.read_csv('hapmap.eigenvec', sep='\\t') \n      ax = pca_df.plot.scatter(x=2, y=3, figsize=(16, 9))\n      ax.figure.savefig('pca.png')\n      \"\"\"\n    }\n    ```", "```py\nworkflow {\n    ped_file = file('data/hapmap.ped')\n    map_file = file('data/hapmap.map')\n    if (!ped_file.exists() | !map_file.exists()) {\n        plink_download | uncompress_plink | subsample_1p | plink_pca | plot_pca\n    }\n    else {\n        subsample_1p(\n            Channel.fromPath('data/hapmap.map'),\n            Channel.fromPath('data/hapmap.ped')) | plink_pca | plot_pca\n    }\n}\n```", "```py\n    nextflow run pipeline.nf -with-report report/report.xhtml\n    ```"]