- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cleaning Messy Data and Data Manipulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll dive into the strategies of **data manipulation**, focusing
    on efficient techniques to clean and fix messy datasets. We’ll remove irrelevant
    columns, systematically address inconsistent data types, and fix dates and times.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Renaming columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing irrelevant or redundant columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixing data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with dates and times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code for this chapter in the following GitHub link: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter04](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter04).'
  prefs: []
  type: TYPE_NORMAL
- en: Each file is named according to the respective sections covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Renaming columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Renaming columns with more descriptive and meaningful names makes it easier
    to understand the content and purpose of each column. Clear and intuitive column
    names enhance the interpretability of the dataset, especially when sharing or
    collaborating with others.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand all the concepts introduced in this chapter, we will use
    a scenario across the chapter. Let’s consider an e-commerce company that wants
    to analyze customer purchase data to optimize its marketing strategies. The dataset
    includes information about customer transactions, such as purchase amount, payment
    method, and timestamp of the transactions. However, the dataset is messy and requires
    cleaning and manipulation to derive meaningful insights.
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of the features is presented in the following figure. To build
    the following statistic charts, execute the file at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter04/1.descriptive_stats.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter04/1.descriptive_stats.py).
    The data and the following charts are created automatically once you run this
    script.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Distribution of features before any data transformation](img/B19801_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Distribution of features before any data transformation
  prefs: []
  type: TYPE_NORMAL
- en: 'We have five columns in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CustomerID`: A unique identifier for each customer. In this example, customer
    IDs range from `1` to `11`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ProductName`: This represents the name of the purchased product. In the dataset,
    three products are considered: `Product_A`, `Product_B`, and `Product_C`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PurchaseAmount`: This indicates the amount spent by the customer on a particular
    product. The amounts are in an arbitrary currency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PaymentMethod`: This describes the method used by the customer to make the
    purchase. Payment methods include `Card`, `PayPal`, `Cash`, and `Bank Transfer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Timestamp`: This represents the date and time when the purchase occurred.
    It is formatted as a `datetime` object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first thing we are going to check and update are the column names. Let’s
    start with this in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Renaming a single column
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, the e-commerce company has decided to rebrand its products, necessitating
    a change in the column names related to product information. We’ll start by renaming
    a single column, and then we’ll further rename multiple columns to align with
    the rebranding initiative. For the renaming example, go to the file at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter04/2.rename_columns.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter04/2.rename_columns.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at how we can rename one column in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `inplace=True` argument is an optional parameter in pandas DataFrame methods
    that allows you to modify the DataFrame directly without creating a new DataFrame
    object.
  prefs: []
  type: TYPE_NORMAL
- en: When `inplace` is set to `True`, the DataFrame is modified in place, meaning
    the changes are applied to the original DataFrame object. This can be useful when
    you want to update or modify the DataFrame without assigning the modified DataFrame
    to a new variable.
  prefs: []
  type: TYPE_NORMAL
- en: If `inplace=True` is not specified or set to `False` (which is the default behavior),
    the DataFrame methods return a new modified DataFrame object, leaving the original
    DataFrame unchanged. In such cases, you need to assign the modified DataFrame
    to a new variable to store the changes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that using `inplace=True` can be a destructive operation
    since it modifies the original DataFrame directly. Therefore, it’s recommended
    to use it with caution and ensure that you have a backup of the original DataFrame
    if needed. If you have a large dataset, modifying it in place can help conserve
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will rename multiple columns to align with the rebranding
    initiative.
  prefs: []
  type: TYPE_NORMAL
- en: Renaming all columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following a rebranding initiative, the company decided to rename `OldProductName`
    as `NewProductName` and `PurchaseAmount` as `NewPurchaseAmount` to align with
    the updated product names. This code demonstrates how to rename multiple columns
    at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to rename columns in a DataFrame and need to ensure a smooth and
    error-free process, we can add error handling. For example, ensure that the columns
    you intend to rename actually exist in the DataFrame. If a column is misspelled
    or does not exist, the renaming operation will raise an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that the new column names do not already exist in the DataFrame to avoid
    overwriting existing columns.
  prefs: []
  type: TYPE_NORMAL
- en: Renaming a column is one of the simplest things we can do to make our data cleaner
    and easier to understand. The next thing we usually do is keep only the columns
    we need or care about, as we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Removing irrelevant or redundant columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large datasets often contain numerous columns, some of which may be irrelevant
    to the specific analyses or tasks at hand. By eliminating these columns, we can
    get some significant benefits. Firstly, storage requirements are dramatically
    reduced, leading to cost savings and more efficient use of resources. Additionally,
    the streamlined dataset results in faster query performance, optimized memory
    usage, and expedited processing times for complex analyses. This not only improves
    the overall efficiency of data processing tasks but also facilitates easier management
    and maintenance of large datasets. Furthermore, in cloud-based environments, where
    storage costs are a factor, the removal of unnecessary columns directly contributes
    to cost efficiency. So, let’s have a look at how we can drop columns in an efficient
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the e-commerce dataset we presented earlier, we have collected information
    about customer purchases. However, as your analysis focuses on product-related
    metrics and customer behavior, certain columns, such as `CustomerID` and `Timestamp`,
    may be considered irrelevant to the current analysis. The goal is to streamline
    the dataset by dropping these columns. You can follow along with this example
    using this Python script [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter04/3.dropping_columns.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter04/3.dropping_columns.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you have a look at the dataset, the column before the deletion was
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After dropping the two columns, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Python, by default, is case-sensitive. This means that `ColumnName` and `columnname`
    are considered different.
  prefs: []
  type: TYPE_NORMAL
- en: 'We successfully removed the unnecessary columns, as demonstrated earlier. To
    further assess memory efficiency, we can calculate the memory consumption of the
    DataFrame both before and after the column deletion. The following code provides
    a Python example of how to calculate the memory used by the DataFrame before and
    after the drop of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The initial memory usage of the DataFrame was approximately 0.00054 megabytes,
    and after dropping columns, it reduced to around 0.00037 megabytes. The achieved
    reduction in memory usage showcases an optimization of nearly 31%.
  prefs: []
  type: TYPE_NORMAL
- en: While this example involves a small dataset, the principles of memory efficiency
    hold significant implications when extrapolated to big data scenarios. In large-scale
    datasets, the impact of removing unnecessary columns becomes even more pronounced.
  prefs: []
  type: TYPE_NORMAL
- en: 'To underline the significance of the operation, consider a scenario with a
    substantial dataset. Initially, the dataset size was 100,000 megabytes, and after
    the removal of unnecessary columns, it was reduced to 69,000 megabytes. To execute
    the same workload, the initial option would be utilizing an AWS EC2 instance of
    type `r7g.4xlarge` with an hourly rate of $1.0064 and memory of 128 GiB, as we
    need 100 gigabytes of memory to load just the dataset. However, by reducing the
    dataset size to 61 gigabytes, an alternative, more cost-effective option is available,
    employing an `r7g.2xlarge` instance at $0.5032 per hour and memory of 64 GiB.
    In the context of a five-minute workload runtime, the cost associated with the
    operation before dropping the data was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The solution became approximately 50% more cost-effective after dropping unnecessary
    columns. This represents the cost savings achieved by optimizing the dataset and
    utilizing a more suitable AWS instance type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplicity of this example underscores a crucial message:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Streamline your data operations by focusing on what is truly essential and
    let this simplicity* *drive cost-effectiveness.*'
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning from dropping columns to fixing inconsistent data types involves
    addressing the quality and integrity of the remaining columns in your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with inconsistent and incorrect data types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working with a DataFrame, it’s important to ensure that each column has
    the correct data type. Inconsistent or incorrect data types can lead to errors
    in analysis, unexpected behavior, and difficulties in performing operations. Let’s
    review how you can handle such situations. You can find the code for this example
    here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter04/4.data_types.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter04/4.data_types.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inspecting the data types of each column in the data is an essential step in
    identifying any inconsistencies or incorrect data types. The `dtypes` attribute
    of a DataFrame provides information about the data types of each column. Let’s
    check the data types of the columns in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The types are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Inspecting the data types allows you to understand the current representation
    of the data and determine if any data type conversions or transformations are
    needed for further analysis or data cleaning tasks. In the following sections,
    we will perform different type transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Columnar type transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the data world, various `astype` method is your friend. The most common type
    transformations that you should be comfortable with are presented in the next
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Converting to numeric types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In pandas, the `astype()` function is used to convert a column to a specified
    numeric data type. For example, to convert a column named `PurchaseAmount` to
    an integer type, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see how we can turn columns into strings.
  prefs: []
  type: TYPE_NORMAL
- en: Converting to string types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use the `astype()` function to convert a column into a string type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see how we can turn columns into categorical types.
  prefs: []
  type: TYPE_NORMAL
- en: Converting to categorical types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **categorical type** refers to a data type that represents categorical or
    discrete variables. Categorical variables can take on a limited, and usually fixed,
    number of distinct categories or levels. These variables often represent qualitative
    data or attributes with no inherent order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The last transformation we will discuss is the Boolean.
  prefs: []
  type: TYPE_NORMAL
- en: Converting to Boolean types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A `True`/`False`) values based on certain conditions or criteria. This transformation
    is often used to create binary indicators or flags, making it easier to work with
    and analyze data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code part checks whether each element in the `ProductName` column
    contains the substring `Dive`. It returns a Boolean Series where each element
    is `True` if the condition is met and `False` otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `astype('bool')` method is used to explicitly cast the data type of the
    `HasDive` column to Boolean.
  prefs: []
  type: TYPE_NORMAL
- en: Things to be aware of when using astype(bool)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re experiencing a situation where all values are being converted to
    `True`, it could be due to one of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. `True` in a Boolean context, `.astype(bool)` will convert all non-zero values
    to `True`. In such cases, consider if the column contains unexpected or unintended
    non-zero values.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. `True` when using `.astype(bool)`. Check if there are missing values present
    in the column and consider how you want to handle them. You may need to fill in
    or drop missing values before the conversion.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section of this chapter, we’ll discuss how to handle dates and times.
  prefs: []
  type: TYPE_NORMAL
- en: Working with dates and times
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you have data that includes information about when things happened –
    being able to understand and handle that time-related data is key for making sense
    of patterns and trends. It’s not just about understanding when things happened,
    but also about making it easier to visualize and tell stories with your data.
    Whether you’re diving into trends over time, filtering data for specific periods,
    or making predictions with machine learning, being good with dates and times is
    key to unlocking valuable insights from datasets that involve the dimension of
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand why dealing with dates and time is so important, the
    next step is learning how to grab that time-related info and make it work for
    us.
  prefs: []
  type: TYPE_NORMAL
- en: Importing and parsing date and time data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python provides several main functions to parse dates, depending on the format
    of the input date string and the desired output. Let’s discuss the commonly used
    functions for parsing dates.
  prefs: []
  type: TYPE_NORMAL
- en: pd.to_datetime() from the pandas library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This function is specifically designed for parsing date strings within pandas
    DataFrames or Series, but it can also be used independently. It is suitable when
    working with tabular data and allows handling multiple date formats simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `format` parameter specifies the expected format of the input string. In
    this example, `%Y` represents the four-digit year, `%m` represents the month,
    `%d` represents the day, `%H` represents the hour, `%M` represents the minute,
    and `%S` represents the second.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations
  prefs: []
  type: TYPE_NORMAL
- en: If your dataset contains missing or inconsistent timestamp values, consider
    using the `errors` parameter. For example, `errors='coerce'` will replace parsing
    errors with **Not a Time** (**NaT**) values.
  prefs: []
  type: TYPE_NORMAL
- en: While `pd.to_datetime` is efficient, it may have performance implications for
    large datasets. For improved performance, consider using the `infer_datetime_format=True`
    parameter to automatically infer the format (works well for standard formats).
    When `infer_datetime_format` is set to `True`, and `parse_dates` is enabled, Pandas
    will try to automatically deduce the format of datetime strings in the columns.
    If successful, it switches to a more efficient parsing method, potentially boosting
    parsing speed by 5-10 times in certain scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: If your data involves different time zones, consider using the `utc` and `tz`
    parameters to handle **Coordinated Universal Time** (**UTC**) conversion and time
    zone localization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce another method, `strftime`. This method
    allows for the customization of datetime values, enabling the creation of specific
    and readable representations of time.
  prefs: []
  type: TYPE_NORMAL
- en: strftime() from the datetime module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This function is used to parse a date string into a datetime object based on
    *a specified format string*. It is suitable when you have a known date format
    and want precise control over the parsing process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting DataFrame is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The format is controlled by format specifiers, each starting with a percent
    (`%`) character, representing different components of the date and time (e.g.,
    `%Y` for the year, `%m` for the month, `%d` for the day, `%H` for the hour, `%M`
    for the minute, `%S` for the second, etc.). A full list of format specifiers can
    be found in the Python documentation: [https://strftime.org/](https://strftime.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the rigid structure required by `strftime`, `dateutil.parser.parse()`
    excels in interpreting a wide range of date and time representations, offering
    a dynamic solution for parsing diverse datetime strings, as we will see in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: dateutil.parser.parse() from the dateutil library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This function provides a flexible approach to parse date strings, *automatically
    inferring* the format based on the input. It is useful when dealing with a variety
    of date formats or when the format is unknown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: One thing to note about this method is that the parser can infer and *handle
    time zone information*, making it convenient for working with data originating
    from different time zones.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, instead of treating dates and times, we shift our approach
    to splitting them into individual parts, such as days, months, and years.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting components from dates and times
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can extract specific components of a datetime object, such as year, month,
    day, hour, minute, or second, using the attributes provided by the datetime module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `.dt` accessor, we can extract the day, month, and year components
    from the `Timestamp` column and create new columns, `Day`, `Month`, and `Year`,
    as presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Extracting components is useful in the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal analysis**: If your analysis involves patterns or trends that vary
    across days, months, or years, extracting these components facilitates a more
    focused exploration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grouping and aggregation**: When grouping data based on temporal patterns,
    extracting components allows for easy aggregation and summarization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time series analysis**: For time series analysis, breaking down datetime
    values into components is essential for understanding seasonality and trends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving on to calculate time differences and durations elevates our exploration
    of temporal data by introducing a dynamic dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating time differences and durations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When calculating the time difference between two datetime objects using subtraction,
    you harness the inherent capability of Python’s `datetime` library to produce
    a `timedelta` object. This object encapsulates the duration between the two timestamps,
    providing a comprehensive representation of the temporal gap in terms of days,
    hours, minutes, and seconds. The code for this section can be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter04/8.time_deltas.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter04/8.time_deltas.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This pandas function, `.diff()`, calculates the difference between each element
    and the previous element in the `Timestamp` column. It effectively computes the
    time elapsed since the previous timestamp for each row.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the first line, this computes the difference between each element
    and the following element in the `Timestamp` column. It calculates the time duration
    until the next timestamp. The negative sign is applied to reverse the sign of
    the time differences. This is done to get a positive representation of the time
    until the next purchase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how the time delta is depicted in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are wondering when it is a good idea to consider adding some time differences
    in your data workflow, then read the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time-based analysis**: Calculating time differences allows for analyzing
    the duration between events or timestamps. It helps quantify the time taken for
    different processes, activities, or intervals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance measurement**: By measuring the duration of tasks or events,
    you can evaluate performance metrics, such as response time, processing time,
    or time taken to complete an operation. This information can guide optimization
    efforts and identify areas for improvement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event sequencing**: By comparing timestamps, you can determine the chronological
    order in which events occurred. This sequencing helps you understand the relationships
    between events and their dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service-level agreement (SLA) monitoring**: Time differences are useful for
    SLA monitoring. By comparing timestamps related to SLA metrics, such as response
    time or resolution time, you can ensure compliance with agreed-upon service levels.
    Monitoring time differences helps identify SLA breaches and take appropriate actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `.diff()` method in pandas is primarily used to compute the difference between
    *consecutive* elements in a Series or DataFrame. While it’s straightforward to
    compute first-order differences (i.e., differences between adjacent elements),
    there are additional considerations and variations to explore.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying time intervals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can customize `.diff()` to compute the difference between elements at a
    specific *time interval*. This is achieved by passing the `periods` parameter
    to specify the number of elements to shift:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s observe the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see,`.diff(periods=2)` calculated the difference between each timestamp
    and the two positions before it. The `periods` parameter allows you to specify
    the number of elements to shift when computing the difference. In this case, it
    is `periods=2`, but you can assign to it any value that makes sense for your use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `.diff()` method introduces a NaN for the first 2 elements when used with
    `diff(periods=2)` since there are no previous elements to calculate the difference
    from. You can handle or fill in these missing values based on your specific use
    case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s observe the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `fillna(0)` replaced the NaN values with `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on from time differences and durations to time zones and daylight saving
    time, we’ll now address the nuances of handling temporal data across different
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: Handling time zones and daylight saving time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Handling time zones is key when dealing with data that spans multiple geographical
    regions or when accurate time representation is crucial. Time zones help standardize
    time across different locations, considering the offset from UTC due to geographic
    boundaries and daylight-saving time adjustments. In our example dataset, we’ll
    demonstrate how to handle time zones using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We localized the timestamps to a specific time zone, in this case, `'UTC'`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We then converted the localized timestamps to a different time zone, in this
    case, `''America/New_York''`. Let’s observe the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Curious about the significance of managing time zones? Let’s understand why
    it matters:'
  prefs: []
  type: TYPE_NORMAL
- en: When working with data collected from different time zones, it is essential
    to handle time zones to ensure accurate analysis and interpretation. Without proper
    time zone handling, the analysis might be skewed due to inconsistencies in time
    representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For applications that require precise time representation, such as financial
    transactions, log entries, or event tracking, handling time zones becomes crucial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When integrating data from various sources or merging datasets, handling time
    zones becomes necessary to align timestamps accurately. This ensures the correct
    chronological ordering of events and prevents inconsistencies in time-based analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are developing applications or services that serve users across different
    time zones, handling time zones is crucial for providing accurate and relevant
    information to users based on their local time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations
  prefs: []
  type: TYPE_NORMAL
- en: Time zone handling should be implemented consistently throughout the data processing
    pipeline to avoid inconsistencies or errors.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize the learnings from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was about the techniques for cleaning and manipulating data. Beginning
    with the challenges of messy data, we covered the removal of irrelevant columns
    and the handling of inconsistent data types. Practical use cases were demonstrated
    with an e-commerce dataset, showcasing Python code for effective data transformations.
    The importance of dropping unnecessary columns was emphasized, highlighting potential
    cost reductions and memory efficiency gains, particularly for big data. Data type
    transformations, including numeric, string, categorical, and Boolean conversions,
    were illustrated with practical examples. The chapter then explored intricate
    aspects of working with dates and times, showcasing methods such as `pd.to_datetime()`,
    `strftime`, and `dateutil.parser.parse()`.
  prefs: []
  type: TYPE_NORMAL
- en: As we wrap up this chapter, it lays a solid foundation for the upcoming one
    in which data merging and transformations will be discussed.
  prefs: []
  type: TYPE_NORMAL
