- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pandas Ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the pandas library offers an impressive array of features, its popularity
    owes much to the vast amount of third-party libraries that work with it in a complementary
    fashion. We cannot hope to cover all of those libraries in this chapter, nor can
    we even dive too deep into how any individual library works. However, just knowing
    these tools exist and understanding what they offer can serve as a great inspiration
    for future learning.
  prefs: []
  type: TYPE_NORMAL
- en: While pandas is an amazing tool, it has its flaws, which we have tried to highlight
    throughout this book; pandas cannot hope to solve every analytical problem there
    is. I strongly encourage you to get familiar with the tools outlined in this chapter
    and to also refer to the pandas ecosystem documentation ([https://pandas.pydata.org/about/](https://pandas.pydata.org/about/))
    when looking for new and specialized tools.
  prefs: []
  type: TYPE_NORMAL
- en: As a technical note on this chapter, it is possible that these code blocks may
    break or change behavior as new releases of the libraries are released. While
    we went to great lengths throughout this book to try and write pandas code that
    is “future-proof”, it becomes more difficult to guarantee that as we write about
    third party dependencies (and their dependencies). If you encounter any issues
    running the code in this chapter, be sure to reference the `requirements.txt`
    file provided alongside the code samples with this book. That file will contain
    a list of dependencies and versions that are known to work with this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Foundational libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other DataFrame libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foundational libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like many open source libraries, pandas builds functionality on top of other
    foundational libraries, letting them manage lower-level details while pandas offers
    more user-friendly functionality. If you find yourself wanting to dive deeper
    into technical details beyond what you learn with pandas, these are the libraries
    you’ll want to focus on.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NumPy labels itself as the *fundamental package for scientific computing with
    Python*, and it is the library on top of which pandas was originally built. NumPy
    is actually an *n*-dimensional library, so you are not limited to two-dimensional
    data like we get with a `pd.DataFrame` (pandas actually used to offer 3-d and
    4-d panel structures, but they are now long gone).
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this book, we have shown you how to construct pandas objects from
    NumPy objects, as you can see in the following `pd.DataFrame` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'However, you can also create NumPy arrays from `pd.DataFrame` objects by using
    the `pd.DataFrame.to_numpy` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Many NumPy functions accept a `pd.DataFrame` as an argument and will still
    even return a `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The main thing to keep in mind with NumPy is that its interoperability with
    pandas will degrade the moment you need missing values in non-floating point types,
    or more generally when you try to use data types that are neither integral nor
    floating point.
  prefs: []
  type: TYPE_NORMAL
- en: The exact rules for this are too complicated to list in this book, but generally,
    I would advise against ever calling `pd.Series.to_numpy` or `pd.DataFrame.to_numpy`
    for anything other than floating point and integral data.
  prefs: []
  type: TYPE_NORMAL
- en: PyArrow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The other main library that pandas is built on top of is Apache Arrow, which
    labels itself as a *cross-language development platform for in-memory analytics*.
    Started by Wes McKinney (the creator of pandas) and announced in his influential
    Apache Arrow and the *10 Things I Hate About pandas* post ([https://wesmckinney.com/blog/apache-arrow-pandas-internals/](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)),
    the Apache Arrow project defines the memory layout for one-dimensional data structures
    in a way that allows different languages, programs, and libraries to work with
    the same data. In addition to defining these structures, the Apache Arrow project
    offers a vast suite of tooling for libraries to implement the Apache Arrow specifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'An implementation of Apache Arrow in Python, PyArrow, has been used in particular
    instances throughout this book. While pandas does not expose a method to convert
    a `pd.DataFrame` into PyArrow, the PyArrow library offers a `pa.Table.from_pandas`
    method for that exact purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'PyArrow similarly offers a `pa.Table.to_pandas` method to get you from a `pa.Table`
    into a `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Generally, PyArrow is considered a lower-level library than pandas. It mostly
    aims to serve other library authors more than it does general users looking for
    a DataFrame library, so, unless you are authoring a library, you may not often
    need to convert to PyArrow from a `pd.DataFrame`. However, as the Apache Arrow
    ecosystem grows, the fact that pandas and PyArrow can interoperate opens up a
    world of integration opportunities for pandas with many other analytical libraries
    and databases.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Oftentimes, you will find yourself provided with a dataset that you know very
    little about. Throughout this book, we’ve shown ways to manually sift through
    data, but there are also tools out there that can help automate potentially tedious
    tasks and help you grasp the data in a shorter amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: YData Profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: YData Profiling bills itself as the “*leading package for data profiling, that
    automates and standardizes the generation of detailed reports, complete with statistics
    and visualizations*.” While we discovered how to manually explore data back in
    the chapter on visualization, this package can be used as a quick-start to automatically
    generate many useful reports and features.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare this to some of the work we did in those chapters, let’s take another
    look at the vehicles dataset. For now, we are just going to pick a small subset
    of columns to keep our YData Profiling minimal; for large datasets, the performance
    can often degrade:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: YData Profiling allows you to easily create a profile report, which contains
    many common visualizations and helps describe the columns you are working with
    in your `pd.DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This book was written using `ydata_profiling` version 4.9.0\. To create the
    profile report, simply run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If running code within a Jupyter notebook, you can see the output of this directly
    within the notebook with a call to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are not using Jupyter, you can alternatively export that profile to
    a local HTML file and open it from there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When looking at the profile, the first thing you will see is a high-level **Overview**
    section that lists the number of cells with missing data, number of duplicate
    rows, etc.:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31091_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: Overview provided by YData Profiling'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each column from your `pd.DataFrame` will be detailed. In the case of a column
    with continuous values, YData Profiling will create a histogram for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a graph  Description automatically generated](img/B31091_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: Histogram generated by YData Profiling'
  prefs: []
  type: TYPE_NORMAL
- en: 'For categorical variables, the tool will generate a word cloud visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31091_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: Word cloud generated by YData Profiling'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how your continuous variables may or may not be correlated, the
    profile contains a very concise heat map that colors each pair accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a color chart  Description automatically generated](img/B31091_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Heat map generated by YData Profiling'
  prefs: []
  type: TYPE_NORMAL
- en: While you still will likely need to dive further into your datasets than what
    this library provides, it can be a great starting point and can help automate
    the generation of otherwise tedious plots.
  prefs: []
  type: TYPE_NORMAL
- en: Data validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The “garbage in, garbage out” principle in computing says that no matter how
    great your code may be, if you start with poor-quality data, your analysis will
    yield poor-quality results. All too often, data practitioners struggle with issues
    like unexpected missing data, duplicate values, and broken relationships between
    modeling entities.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are tools to help you automate both the data that is input
    to and output from your models, which ensures trust in the work that you are performing.
    In this recipe, we are going to look at Great Expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Great Expectations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This book was written using Great Expectations version 1.0.2\. To get started,
    let’s once again look at our vehicles dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: There are a few different ways to use Great Expectations, not all of which can
    be documented in this cookbook. For the sake of having a self-contained example,
    we are going to set up and process all of our expectations in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we are going to import the `great_expectations` library and create
    a `context` for our tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the context, you can create a data source and a data asset. For non-DataFrame
    sources like SQL, the data source would typically contain connection credentials,
    but with the `pd.DataFrame` residing in memory there is less work to do. The data
    asset is a grouping mechanism for results. Here we are just creating one data
    asset, but in real-life use cases you may decide that you want multiple assets
    to store and organize the validation results that Great Expectations outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, you can create a batch definition within Great Expectations. For
    non-DataFrame sources, the batch definition would tell the library how to retrieve
    data from the source. In the case of pandas, the batch definition will simply
    retrieve all of the data from the associated `pd.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you can start to make assertions about the data. For instance,
    you can use Great Expectations to ensure that a column does not contain any null
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'That same expectation applied to the `cylinders` column will not be successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: For brevity, we have only shown you how to set expectations around nullability,
    but there is an entire Expectations Gallery at [https://greatexpectations.io/expectations/](https://greatexpectations.io/expectations/)
    you can use for other assertions. Great Expectations also works with other tools
    like Spark, PostgreSQL, etc., so you can apply your expectations at many different
    points in your data transformation pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back in *Chapter 6*, *Visualization*, we discussed at length visualization using
    matplotlib, and we even discussed using Seaborn for advanced plots. These tools
    are great for generating static charts, but when you want to add some level of
    interactivity, you will need to opt for other libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe, we are going to load the same data from the vehicles dataset
    we used back in our *Scatter plots* recipe from *Chapter 6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Plotly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by looking at Plotly, which can be used to create visualizations
    with a high degree of interactivity, making it a popular choice within Jupyter
    notebooks. To use it, simply pass `plotly` as the `backend=` argument to `pd.DataFrame.plot`.
    We are also going to add a `hover_data=` argument, which Plotly can use to add
    labels to each data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If you inspect this in a Jupyter notebook or HTML page, you will see that you
    can hover over any data point to reveal more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: Hovering over a data point with Plotly'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can even select an area of the chart to zoom into the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31091_11_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: Zooming in with Plotly'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, Plotly is very easy to use with the same pandas API you have
    seen throughout this book. If you desire interactivity with your plots, it is
    a great tool to make use of.
  prefs: []
  type: TYPE_NORMAL
- en: PyGWalker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the plotting code you have seen so far is declarative in nature; i.e.,
    you tell pandas that you want a bar, line, scatter plot, etc., and pandas generates
    that for you. However, many users may prefer having a more “free-form” tool for
    exploration, where they can just drag and drop elements to make charts on the
    fly.
  prefs: []
  type: TYPE_NORMAL
- en: 'If that is what you are after, then you will want to take a look at the PyGWalker
    library. With a very succinct API, you can generate an interactive tool within
    a Jupyter notebook, with which you can drag and drop different elements to generate
    various charts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B31091_11_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: PyGWalker within a Jupyter notebook'
  prefs: []
  type: TYPE_NORMAL
- en: Data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While pandas offers some built-in statistical algorithms, it cannot hope to
    cover all of the statistical and machine learning algorithms that are used in
    the domain of data science. Fortunately, however, many of the libraries that do
    specialize further in data science offer very tight integrations with pandas,
    letting you move data from one library to the next rather seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: scikit-learn is a popular machine learning library that can help with both supervised
    and unsupervised learning. The scikit-learn library offers an impressive array
    of algorithms for classification, prediction, and clustering tasks, while also
    providing tools to pre-process and cleanse your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We cannot hope to cover all of these features, but for the sake of showcasing
    something, let’s once again load the vehicles dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s assume that we want to create an algorithm to predict the combined
    mileage a vehicle will achieve, inferring it from other attributes in the data.
    Since mileage is a continuous variable, we can opt for a linear regression model
    to make our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear regression model we are going to work with will want to use features
    that are also numeric. While there are ways we could artificially convert some
    of our non-numeric data into numeric (e.g., using the technique from the *One-hot
    encoding with pd.get_dummies* recipe back in *Chapter 5*, *Algorithms and How
    to Apply Them*), we are just going to ignore any non-numeric columns for now.
    The linear regression model is also unable to handle missing data. We know from
    the *Exploring continuous* *data* recipe from *Chapter 6* that this dataset has
    two continuous variables with missing data. While we could try to interpolate
    those values, we are again going to take the simple route in this example and
    just drop them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The scikit-learn model will need to know the *features* we want to use for
    prediction (commonly notated as `X`) and the target variable we are trying to
    predict (commonly notated as `y`). It is also a good practice to split the data
    into training and testing datasets, which we can do with the `train_test_split`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'With our data in this form, we can go ahead and train the linear regression
    model and then apply it to our test data to generate predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have predictions from our test dataset, we can compare them back
    to the actual values we withheld as part of testing. This is a good way to measure
    how accurate the model is that we fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different ways to manage model accuracy, but for now, we can
    opt for the commonly used and relatively simple `mean_squared_error`, which scikit-learn
    also provides as a convenience function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are interested in knowing more, I highly recommend you read through
    the documentation and examples on the scikit-learn website, or check out books
    like *Machine Learning with PyTorch and Scikit-Learn: Develop machine learning
    and deep learning models with Python* ([https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312](https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312)).'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For another great machine learning library, let’s now turn our attention to
    XGBoost, which implements algorithms using Gradient boosting. XGBoost is extremely
    performant, scales well, scores well in machine learning competitions, and pairs
    well with data that is stored in a `pd.DataFrame`. If you are already familiar
    with scikit-learn, the API it uses will feel familiar.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost can be used for both classification and regression. Since we just performed
    a regression analysis with scikit-learn, let’s now work through a classification
    example where we try to predict the make of a vehicle from the numeric features
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The vehicles dataset we are working with has 144 different makes. For our analysis,
    we are going to just pick a small subset of consumer brands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, we are going to split our data into features (`X`) and a target
    variable (`y`). For the purposes of the machine learning algorithm, we also need
    to convert our target variable into categorical data type, so that the algorithm
    can predict values like `0`, `1`, `2`, etc instead of `"Dodge," "Toyota," "Volvo,"`
    etc.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'With that out of the way, we can once again use the `train_test_split` function
    from scikit-learn to create training and testing data. Note that we are using
    `pd.Series.cat.codes` to use the numeric value assigned to our categorical data
    type, rather than the string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can import the `XGBClassifier` from XGBoost, train it on our data,
    and apply it to our test features to generate predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the predictions, we can validate how many of them matched
    the target variables included as part of our testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we are only scratching the surface of what you can do with a library
    like XGBoost. There are many different ways to tweak your model to improve accuracy,
    prevent over-/underfitting, optimize for a different outcome, etc. For users wanting
    to learn more about this great library, I advise checking out the XGBoost documentation
    or books like *Hands-On* *Gradient Boosting with XGBoost and scikit-learn*.
  prefs: []
  type: TYPE_NORMAL
- en: Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Database knowledge is an important tool in the toolkit of any data practitioner.
    While pandas is a great tool for single-machine, in-memory computations, databases
    offer a very complementary set of analytical tools that can help with the storage
    and distribution of analytical processes.
  prefs: []
  type: TYPE_NORMAL
- en: Back in *Chapter 4*, *The pandas I/O System*, we walked through how to transfer
    data between pandas and theoretically any database. However, a relatively more
    recent database called DuckDB is worth some extra consideration, as it allows
    you to even more seamlessly bridge the worlds of dataframes and databases together.
  prefs: []
  type: TYPE_NORMAL
- en: DuckDB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DuckDB is a lightweight database system that offers a zero-copy integration
    with Apache Arrow, a technology that also underpins efficient data sharing and
    usage with pandas. It is extremely lightweight and, unlike most database systems,
    can be easily embedded into other tools or processes. Most importantly, DuckDB
    is optimized for analytical workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'DuckDB makes it easy to query data in your `pd.DataFrame` using SQL. Let’s
    see this in action by loading the vehicles dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'By passing a `CREATE TABLE` statement to `duckdb.sql`, you can load the data
    from the `pd.DataFrame` into a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the table is created, you can query from it with SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to convert your results back to a `pd.DataFrame`, you use the `.df`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: For a deeper dive into DuckDB, I strongly advise checking out the DuckDB documentation
    and, for a greater understanding of where it fits in the grand scheme of databases,
    the *Why DuckDB* article ([https://duckdb.org/why_duckdb](https://duckdb.org/why_duckdb)).
    Generally, DuckDB’s focus is on single-user analytics, but if you are interested
    in a shared, cloud-based data warehouse, you may also want to look at MotherDuck
    ([https://motherduck.com/](https://motherduck.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Other DataFrame libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Soon after pandas was developed, it became the de facto DataFrame library in
    the Python space. Since then, many new DataFrame libraries have been developed
    in the space, which all aim to address some of the shortcomings of pandas while
    introducing their own novel design decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Ibis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ibis is yet another amazing analytics tool created by Wes McKinney, the creator
    of pandas. At a high level, Ibis is a DataFrame “frontend” that gives you one
    generic API through which you can query multiple “backends.”
  prefs: []
  type: TYPE_NORMAL
- en: 'To help understand what that means, it is worth contrasting that with the design
    approach of pandas. In pandas, the API or “frontend” for a group by and a sum
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'In Ibis, a similar expression would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: While the API exposed to the user may not be all that different, the similarities
    between Ibis and pandas stop there. Ibis does not dictate how you store the data
    you are querying; it can be stored in BigQuery, DuckDB, MySQL, PostgreSQL, etc.,
    and it can be even stored in another DataFrame library like pandas. Beyond the
    storage, Ibis does not dictate how summation should be performed; instead, it
    leaves it to an execution engine. Many SQL databases have their own execution
    engine, but others may defer to third-party libraries like Apache DataFusion ([https://datafusion.apache.org/](https://datafusion.apache.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a `pd.DataFrame` through Ibis, you will need to wrap it with the `ibis.memtable`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'With that out of the way, you can then start to query the data just as you
    would with pandas but using the Ibis API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: It is worth noting that the preceding code does not actually return a result.
    Unlike pandas, which executes all of the operations you give it *eagerly*, Ibis
    collects all of the expressions you want and waits to perform execution until
    explicitly required. This practice is commonly called *deferred* or *lazy* execution.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of deferring is that Ibis can find ways to optimize the query
    that you are telling it to perform. Our query is asking Ibis to find all rows
    where the make is Honda and then select a few columns, but it might be faster
    for the underlying database to select the columns first and then perform the filter.
    How that works is abstracted from the end user; users are just required to tell
    Ibis what they want and Ibis takes care of how to retrieve that data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To materialize this back into a `pd.DataFrame`, you can chain in a call to
    `.to_pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'However, you are not required to return a `pd.DataFrame`. If you wanted a PyArrow
    table instead, you could opt for `.to_pyarrow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: For more information on Ibis, be sure to check out the Ibis documentation. There
    is even an Ibis tutorial aimed specifically at users coming from pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Dask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another popular library that has a history closely tied to pandas is Dask. Dask
    is a framework that provides a similar API to the `pd.DataFrame` but scales its
    usage to parallel computations and datasets that exceed the amount of memory available
    on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wanted to convert our vehicles dataset to a Dask DataFrame, we can use
    the `dask.dataframe.from_pandas` function with a `npartitions=` argument that
    controls how to divide up the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: By splitting your DataFrame into different partitions, Dask allows you to perform
    computations against each partition in parallel, which can help immensely with
    performance and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Much like Ibis, Dask performs calculations lazily. If you want to force a calculation,
    you will want to call the `.compute` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'To go from a Dask DataFrame back to pandas, simply call `ddf.compute`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Polars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polars is a newcomer to the DataFrame space and has developed impressive features
    and a dedicated following in a very short amount of time. The Polars library is
    Apache Arrow native, so it has a much cleaner type system and consistent missing
    value handling than what pandas offers today (for the history of the pandas type
    system and all of its flaws, be sure to give *Chapter 3*, *Data Types*, a good
    read).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to a simpler and cleaner type system, Polars can scale to datasets
    that are larger than memory, and it even offers a lazy execution engine coupled
    with a query optimizer that can make it easier to write performant, scalable code.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a naive conversion from pandas to Polars, you can use `polars.from_pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'For lazy execution, you will want to try out the `pl.LazyFrame`, which can
    take the `pd.DataFrame` directly as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Much like we saw with Ibis, the lazy execution engine of Polars can take care
    of optimizing the best path for doing a filter and select. To execute the plan,
    you will need to chain in a call to `pl.LazyFrame.collect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'If you would like to convert back to pandas from Polars, both the `pl.DataFrame`
    and `pl.LazyFrame` offer a `.to_pandas` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: For a more detailed look at Polars and all of the great things it has to offer,
    I suggest checking out the *Polars Cookbook* ([https://www.packtpub.com/en-us/product/polars-cookbook-9781805121152](https://www.packtpub.com/en-us/product/polars-cookbook-9781805121152)).
  prefs: []
  type: TYPE_NORMAL
- en: cuDF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have a Nvidia device and the CUDA toolkit available to you, you may
    also be interested in cuDF. In theory, cuDF is a “drop-in” replacement for pandas;
    as long as you have the right hardware and tooling, it will take your pandas expressions
    and run them on your GPU, simply by importing cuDF before pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Given the power of modern GPUs compared to CPUs, this library can offer users
    a significant performance boost without having to change the way code is written.
    For the right users with the right hardware, that type of out-of-the-box performance
    boost can be invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/pandas](https://packt.link/pandas)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5040900042138312.png)'
  prefs: []
  type: TYPE_IMG
- en: Leave a Review!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for purchasing this book from Packt Publishing—we hope you enjoyed
    it! Your feedback is invaluable and helps us improve and grow. Please take a moment
    to leave an [Amazon review](Chapter_11.xhtml); it will only take a minute, but
    it makes a big difference for readers like you.
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code below to receive a free ebook of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1474021820358918656.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://packt.link/NzOWQ](https://packt.link/NzOWQ)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/New_Packt_Logo1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[packt.com](https://www.packt.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  prefs: []
  type: TYPE_NORMAL
- en: Why subscribe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve your learning with Skill Plans built especially for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a free eBook or video every month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully searchable for easy access to vital information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy and paste, print, and bookmark content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At [www.packt.com](https://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  prefs: []
  type: TYPE_NORMAL
- en: Other Books You May Enjoy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/978-1-80181-931-2.png)](https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine Learning with PyTorch and Scikit-Learn**'
  prefs: []
  type: TYPE_NORMAL
- en: Sebastian Raschka
  prefs: []
  type: TYPE_NORMAL
- en: Yuxi (Hayden) Liu
  prefs: []
  type: TYPE_NORMAL
- en: Vahid Mirjalili
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 978-1-80181-931-2'
  prefs: []
  type: TYPE_NORMAL
- en: Explore frameworks, models, and techniques for machines to learn from data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use scikit-learn for machine learning and PyTorch for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train machine learning classifiers on images, text, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build and train neural networks, transformers, and boosting algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discover best practices for evaluating and tuning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict continuous target outcomes using regression analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dig deeper into textual and social media data using sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/978-1-80323-291-1.png)](https://www.packtpub.com/en-us/product/deep-learning-with-tensorflow-and-keras-3rd-edition-9781803232911)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep Learning with TensorFlow and Keras, Third Edition**'
  prefs: []
  type: TYPE_NORMAL
- en: Amita Kapoor
  prefs: []
  type: TYPE_NORMAL
- en: Antonio Gulli
  prefs: []
  type: TYPE_NORMAL
- en: Sujit Pal
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 978-1-80323-291-1'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to use the popular GNNs with TensorFlow to carry out graph mining
    tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discover the world of transformers, from pretraining to fine-tuning to evaluating
    them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply self-supervised learning to natural language processing, computer vision,
    and audio signal processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine probabilistic and deep learning models using TensorFlow Probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train your models on the cloud and put TF to work in real environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build machine learning and deep learning systems with TensorFlow 2.x and the
    Keras API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/978-1-78934-641-1.png)](https://www.packtpub.com/en-us/product/machine-learning-for-algorithmic-trading-9781839217715)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine Learning for Algorithmic Trading, Second Edition**'
  prefs: []
  type: TYPE_NORMAL
- en: Stefan Jansen
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 978-1-83921-771-5'
  prefs: []
  type: TYPE_NORMAL
- en: Leverage market, fundamental, and alternative text and image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Research and evaluate alpha factors using statistics, Alphalens, and SHAP values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement machine learning techniques to solve investment and trading problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backtest and evaluate trading strategies based on machine learning using Zipline
    and Backtrader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize portfolio risk and performance analysis using pandas, NumPy, and pyfolio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a pairs trading strategy based on cointegration for US equities and ETFs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train a gradient boosting model to predict intraday returns using AlgoSeek s
    high-quality trades and quotes data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packt is searching for authors like you
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](https://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/pandas](Other_Books_You_May_Enjoy.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5040900042138312.png)'
  prefs: []
  type: TYPE_IMG
