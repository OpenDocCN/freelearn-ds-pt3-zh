<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer119">
    <h1 class="chapterNumber">8</h1>
    <h1 class="chapterTitle" id="_idParaDest-93">Deploying Streamlit Apps with Hugging Face and Heroku</h1>
    <p class="normal">In <em class="chapterRef">Chapter 5</em>, <em class="italic">Deploying Streamlit with Streamlit Community Cloud</em>, we learned how to deploy our Streamlit applications with Streamlit Community Cloud. Streamlit Community Cloud is quick, easy, and very effective for most applications. However, it does not have unlimited free computing resources available and is limited to 1 GB of RAM per deployed app. If we want to have an app that uses more resources than that, we do not have that option.</p>
    <p class="normal">This leads me to the other aspect to consider—the integration of Streamlit with Snowflake. The paid Streamlit offering is now within the Snowflake ecosystem. Though it might seem like a constraint, note that Snowflake enjoys immense popularity for a reason. If your company already uses Snowflake, this could be a great advantage to you. However, if you do not already use Snowflake, this chapter provides you with a couple other excellent options for deploying your resource-intensive or security-constrained applications.</p>
    <p class="normal">When Streamlit first was launched, and also when this book was first launched in the Fall of 2021, the deployment options available were sparse. Often the best option was to rent out server space from Amazon Web Services or Azure and set up all the configuration yourself. Thankfully with the massive success of the library the deployment options are much improved. This chapter will focus on three main sections:</p>
    <ul>
      <li class="bulletList">Choosing between Streamlit Community Cloud, Hugging Face, and Heroku</li>
      <li class="bulletList">Deploying Streamlit apps on Hugging Face</li>
      <li class="bulletList">Deploying Streamlit apps on Heroku</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-94">Technical requirements</h1>
    <p class="normal">Here is a list of installments required for this chapter:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Heroku account</strong>: Heroku is<a id="_idIndexMarker305"/> a popular platform that data scientists and software engineers use to host their applications, models, and <strong class="keyWord">Application Programming Interfaces (APIs</strong>), and <a id="_idIndexMarker306"/>is owned by Salesforce. To<a id="_idIndexMarker307"/> get a Heroku account, please head over to <a href="https://signup.heroku.com"><span class="url">https://signup.heroku.com</span></a> to make your free account.</li>
      <li class="bulletList"><strong class="keyWord">Heroku Command-Line Interface</strong> (<strong class="keyWord">CLI</strong>): To use Heroku effectively, we will need to download the<a id="_idIndexMarker308"/> Heroku CLI, which will allow us to run Heroku commands. To download<a id="_idIndexMarker309"/> this, please follow the instructions listed here: <a href="https://devcenter.heroku.com/articles/heroku-cli"><span class="url">https://devcenter.heroku.com/articles/heroku-cli</span></a>.</li>
      <li class="bulletList"><strong class="keyWord">Hugging Face account</strong>: Hugging Face<a id="_idIndexMarker310"/> is a wonderful machine learning-focused platform, which we used in <em class="chapterRef">Chapter 4</em>, <em class="italic">Machine Learning and AI with Streamlit</em>; to<a id="_idIndexMarker311"/> create an account head over to <a href="https://huggingface.co/join"><span class="url">https://huggingface.co/join</span></a>.</li>
    </ul>
    <p class="normal">Now that we have the requirements, let’s begin!</p>
    <h1 class="heading-1" id="_idParaDest-95">Choosing between Streamlit Community Cloud, Hugging Face, and Heroku</h1>
    <p class="normal">At a high level, whenever we are trying to deploy our Streamlit application such that users on the internet can see our applications, what we are really doing is renting a computer owned by someone else and giving that computer a set of instructions to start up our application. Choosing which platform to use is difficult to know how to do without either having a background in deploying systems or without trying each option out first, but there are a few heuristics that should help you out.</p>
    <p class="normal">The two most important factors for this decision are the flexibility of the system and the time it takes to get up and running. Note that these two factors are often directly traded off with one another. If you are using <a id="_idIndexMarker312"/>Streamlit Community Cloud, you cannot say “I want this to run this on GPUs with 30 GiB of memory,” but in return, you get a wildly simple process where you can simply point Streamlit Community Cloud to your GitHub repository, and it will take care of all the other little decisions that need to be made. On the other hand, Hugging Face and Heroku<a id="_idIndexMarker313"/> give you more flexibility through paid options but take a bit more time to set up (as you will find out!).</p>
    <p class="normal">In short, if you’re working with a<a id="_idIndexMarker314"/> platform already (Snowflake, Hugging Face, or Heroku), you should just work with the platform you’re already on. If you aren’t already using any of these, or are a hobbyist programmer, Streamlit Community Cloud is the best option.</p>
    <p class="normal">If you need more compute and are working in the machine learning or natural language processing space, you should use Hugging Face. If you need more compute and want a more general platform with a broad set of integrations, Heroku is a great option for you.</p>
    <p class="normal">Let’s get started with Hugging Face!</p>
    <h1 class="heading-1" id="_idParaDest-96">Deploying Streamlit with Hugging Face</h1>
    <p class="normal">Hugging Face offers an <a id="_idIndexMarker315"/>entire suite of products focused on <a id="_idIndexMarker316"/>machine learning and is especially used by machine learning engineers and folks in the natural language processing space. It gives developers the ability to easily use pre-trained models through its transformers library (which we already used!) but also create products that let developers host their own models, datasets, and even their own data apps through a product called Hugging Face Spaces. You can think of a Space as a place to deploy an app on the Hugging Face infrastructure, and it is quite easy to get started.</p>
    <p class="normal">For this chapter, we’ll deploy the same Hugging Face app that we created in <em class="chapterRef">Chapter 4</em>. We can deploy any of our Streamlit apps on Hugging Face, but I thought it would be more fitting to deploy that one!</p>
    <p class="normal">To start, we need to go to <a href="https://huggingface.co/spaces"><span class="url">https://huggingface.co/spaces</span></a> and click the button that says <strong class="screenText">Create new Space</strong>.</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B18444_08_01.png"/></figure>
    <p class="packt_figref">Figure 8.1: Hugging Face login</p>
    <p class="normal">After logging in, we will<a id="_idIndexMarker317"/> get a few options. We can name our Space, choose a license, select the type of Space that we want (Gradio is another popular option for data apps and is owned by Hugging Face), choose the Space hardware (note the paid and free options), and set our Space as public or private. The screenshot <a id="_idIndexMarker318"/>below shows the options I have chosen (you can name the Space anything you’d like, but the rest of these should be the same).</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B18444_08_02.png"/></figure>
    <p class="packt_figref">Figure 8.2: Hugging Face options</p>
    <p class="normal">Now, you should click the <strong class="screenText">Create Space</strong> button at the bottom of the page. Once you have created the Space, you need to clone that Space on your personal computer using the following Git command, which I cloned inside the main Streamlit for Data Science GitHub repository that this book is in:</p>
    <pre class="programlisting con"><code class="hljs-con">git clone https://huggingface.co/spaces/{your username}/{your_huggingface_space_name}
</code></pre>
    <p class="normal">Now that your <a id="_idIndexMarker319"/>repo is cloned, we need to create a file for our <a id="_idIndexMarker320"/>Streamlit app and another <code class="inlineCode">requirements.txt</code> file to use to tell Hugging Face Spaces which libraries we need for our app, using the following commands:</p>
    <pre class="programlisting con"><code class="hljs-con">cd {your_huggingface_space_name}
touch app.py 
touch requirements.txt
</code></pre>
    <p class="normal">Within the <code class="inlineCode">app.py</code> file, we can directly copy and paste the app we already created; the code is copied below: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
st.title(<span class="hljs-string">"Hugging Face Demo"</span>)
text = st.text_input(<span class="hljs-string">"Enter text to analyze"</span>)
st.cache_resource
<span class="hljs-keyword">def</span> <span class="hljs-title">get_model</span>():
    <span class="hljs-keyword">return</span> pipeline(<span class="hljs-string">"sentiment-analysis"</span>)
model = get_model()
<span class="hljs-keyword">if</span> text:
    result = model(text)
    st.write(<span class="hljs-string">"Sentiment:"</span>, result[<span class="hljs-number">0</span>][<span class="hljs-string">"label"</span>])
    st.write(<span class="hljs-string">"Confidence:"</span>, result[<span class="hljs-number">0</span>][<span class="hljs-string">"score"</span>])
</code></pre>
    <p class="normal">And for our <code class="inlineCode">requirements.txt</code> file, we just use three libraries, which we can add to the file like so: </p>
    <pre class="programlisting code"><code class="hljs-code">streamlit
transformers
torch
</code></pre>
    <p class="normal">Now that we have the files in the right state, we just use Git to add, commit, and push the changes: </p>
    <pre class="programlisting con"><code class="hljs-con">git add .
git commit –m 'added req, streamlit app'
git push
</code></pre>
    <p class="normal">When we push <a id="_idIndexMarker321"/>our changes from the command line, we<a id="_idIndexMarker322"/> will be asked to enter our Hugging Face username and password, and then if we go back to our <strong class="screenText">Hugging Face</strong> tab, our app is hosted! </p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B18444_08_03.png"/></figure>
    <p class="packt_figref">Figure 8.3: Hugging Face deployed app</p>
    <p class="normal">If we go back to our code and look at the <code class="inlineCode">README.md</code> file, we will notice that there are a bunch of useful configuration options, such as changing the emoji or the title. Hugging Face also allows us to specify other parameters like the Python version. The full documentation is in the link in your <code class="inlineCode">README.md</code>:</p>
    <figure class="mediaobject"><img alt="" role="presentation" src="../Images/B18444_08_04.png"/></figure>
    <p class="packt_figref">Figure 8.4: Hugging Face deployed app code</p>
    <p class="normal">And that is it for deploying Streamlit apps on Hugging Face!</p>
    <p class="normal">You can<a id="_idIndexMarker323"/> probably already notice some of the downsides<a id="_idIndexMarker324"/> of deploying on Hugging Face Spaces, which include a few more steps than Streamlit Community Cloud, and the large amount of real estate on apps that is taken by Hugging Face. Understandably, Hugging Face wants to make sure that anyone who sees your app knows that it is created using their product. They place a lot of their own branding and products at the top of your deployed app, which certainly negatively affects the app viewing experience. For other folks who are already using Hugging Face, this branding might be a big benefit as they can clone your Space and view popular Spaces and models, but for sending apps to non-ML colleagues or even friends, the branding is a downside of Spaces.</p>
    <p class="normal">The other main downside of Hugging Face Spaces is that they are often a bit behind in the versions of Streamlit that they support. As of this writing, they are running Streamlit version 1.10.0, and the latest version of Streamlit is 1.16.0. If you’re looking for the most recent Streamlit features, Hugging Face Spaces might not support them! This also is usually not a big deal for most Streamlit apps, but another factor to be aware of when choosing a platform.</p>
    <p class="normal">I hope it is clear to you the strong benefits and mild disadvantages of using Hugging Face Spaces. Now let’s move over to Heroku!</p>
    <h1 class="heading-1" id="_idParaDest-97">Deploying Streamlit with Heroku</h1>
    <p class="normal">Heroku is a<a id="_idIndexMarker325"/> Platform as a Service owned by Salesforce, optimized as a generic compute platform that you can use for everything from websites to APIs to Streamlit apps. Because of this, you have many more options with Heroku than with either Streamlit Community Cloud or Hugging Face Spaces, but getting started takes more effort.</p>
    <p class="normal">Please note that <a id="_idIndexMarker326"/>Heroku has no free tier, so if you do not want to<a id="_idIndexMarker327"/> follow along (or if you are already happy with Streamlit Community Cloud or Hugging Face Spaces), feel free to just skip to the next chapter! The reason Heroku is included in this book is that I wanted to provide an option that had more capacity, supported the most recent Streamlit versions without much branding, and was easy to use. Heroku is the best platform on those metrics, so I’ll cover it below!</p>
    <p class="normal">To<a id="_idIndexMarker328"/> deploy our Streamlit apps<a id="_idIndexMarker329"/> on Heroku, we need to do the following:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Set up and log in to Heroku.</li>
      <li class="numberedList">Clone and configure our local repository.</li>
      <li class="numberedList">Deploy to Heroku.</li>
    </ol>
    <p class="normal">Let’s look at each of these steps in detail!</p>
    <h2 class="heading-2" id="_idParaDest-98">Setting up and logging in to Heroku</h2>
    <p class="normal">In the <em class="italic">Technical requirements</em> section of this chapter, we covered how to download Heroku and create an account. Now, we need to log in to our Heroku from our command line by running the following command and logging in when prompted:</p>
    <pre class="programlisting con"><code class="hljs-con">heroku login
</code></pre>
    <p class="normal">This will take us to the Heroku page, and once we log in, we will be good to go. This command will keep you logged in on your machine indefinitely, unless your password changes or you purposely log out of Heroku.</p>
    <h2 class="heading-2" id="_idParaDest-99">Cloning and configuring our local repository</h2>
    <p class="normal">Next, we <a id="_idIndexMarker330"/>need to change our directory to where the<a id="_idIndexMarker331"/> penguin machine learning app is located. My app folder is inside my <code class="inlineCode">Documents</code> folder, so the following command takes me there, but your folder might be different:</p>
    <pre class="programlisting con"><code class="hljs-con">cd ~/Documents/penguin_ml
</code></pre>
    <p class="normal">If you do not already have the repository downloaded locally with a corresponding repository on GitHub, go ahead and stop by <em class="chapterRef">Chapter 5</em>, <em class="italic">Deploying Streamlit with Streamlit Community Cloud</em>, to see how to get started with GitHub. Instead, you can also run the following command to download the repository locally from my personal GitHub:</p>
    <pre class="programlisting con"><code class="hljs-con">git clone https://github.com/tylerjrichards/penguin_ml.git
</code></pre>
    <p class="normal">It is highly encouraged that you practice with your own GitHub repository, as this is much better practice than cloning an app from me to use to deploy to Heroku.</p>
    <p class="normal">Now we need to create a Heroku app with a unique name for our app with the next command (the app will be deployed as this name with <code class="inlineCode">.heroku.com</code> appended to the end of it). Mine will be <code class="inlineCode">penguin-machine-learning</code>, but go ahead and pick your own!</p>
    <pre class="programlisting con"><code class="hljs-con">heroku create penguin-machine-learning
</code></pre>
    <p class="normal">Once we have this, we need to explicitly make the connection between our Git repository and the Heroku app we have just created, which can be done with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">heroku git:remote -a penguin-machine-learning
</code></pre>
    <p class="normal">And finally, we<a id="_idIndexMarker332"/> are going to add two files to our repository that <a id="_idIndexMarker333"/>are needed to start up with Heroku, the <code class="inlineCode">Procfile</code> file and the <code class="inlineCode">streamlit_setup.sh</code> file. Heroku uses something called a <code class="inlineCode">Procfile</code> as a way to declare which commands the app should perform when starting up, and also to tell Heroku what type of application this is. For our Heroku apps, we also need this <code class="inlineCode">Procfile</code> to configure some setup for our app specific to Streamlit apps (such as the port configuration), and then also to run the <code class="inlineCode">streamlit run</code> command to launch our app. Let’s start by creating the <code class="inlineCode">streamlit_setup.sh</code> file using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">touch streamlit_setup.sh
</code></pre>
    <p class="normal">We can open this file with our text editor and put the following lines inside it, which creates our familiar <code class="inlineCode">config.toml</code> file in the base directory:</p>
    <pre class="programlisting con"><code class="hljs-con">mkdir -p ~/.streamlit
echo "[server]
headless = true
port = $PORT
enableCORS = false
" &gt; ~/.streamlit/config.toml
</code></pre>
    <p class="normal">Once we save this file, we need to create a <code class="inlineCode">Procfile</code> that runs this <code class="inlineCode">streamlit_setup.sh</code> file and then also runs our Streamlit app:</p>
    <pre class="programlisting con"><code class="hljs-con">touch Procfile
</code></pre>
    <p class="normal">Within <a id="_idIndexMarker334"/>the <code class="inlineCode">Procfile</code> file we just created, we will next <a id="_idIndexMarker335"/>add the following line:</p>
    <pre class="programlisting con"><code class="hljs-con">web: sh streamlit_setup.sh &amp;&amp; streamlit run penguins_streamlit.py
</code></pre>
    <p class="normal">Now that we have our Streamlit app all set up, our final step is to deploy it to Heroku!</p>
    <h2 class="heading-2" id="_idParaDest-100">Deploying to Heroku</h2>
    <p class="normal">Before we deploy, we have a couple of new files on our app, so we need to add those to our Git repository using the following commands:</p>
    <pre class="programlisting con"><code class="hljs-con">git add .
git commit -m 'added heroku files'
git push
</code></pre>
    <p class="normal">And now, our final step in this chapter is to push to Heroku, which we can do with this next command:</p>
    <pre class="programlisting con"><code class="hljs-con">git push heroku main
</code></pre>
    <p class="normal">This will kick off the Heroku build, and soon enough, we will see our Penguin app deployed to Heroku for anyone to go and view. The app we have been working on and just deployed can be found at the following link (with a screenshot attached!), <a href="https://penguin-machine-learning.herokuapp.com/"><span class="url">https://penguin-machine-learning.herokuapp.com/</span></a>, and the GitHub repository for this app can be found at <a href="https://github.com/tylerjrichards/penguin_ml"><span class="url">https://github.com/tylerjrichards/penguin_ml</span></a>. You can see the app in the following screenshot:</p>
    <figure class="mediaobject"><img alt="Figure 8.2 – Heroku App deployment " src="../Images/B18444_08_05.jpg"/></figure>
    <p class="packt_figref">Figure 8.5: Heroku app deployment</p>
    <p class="normal">As you can see, Heroku <a id="_idIndexMarker336"/>deployment is more difficult than Hugging Face <a id="_idIndexMarker337"/>Spaces or Streamlit Community Cloud but gives you the option to put more compute behind your app without adding Heroku branding. Heroku will also always support the most recent Streamlit features, which Hugging Face Spaces does not always do.</p>
    <p class="normal">The big downside for Heroku (other than the increased difficulty) is that as of November 28<sup class="superscript">th</sup>, 2022, Heroku no longer has a free tier, whereas Streamlit Community Cloud and Hugging Face Spaces both do. If you want the features, you have to pay for them!</p>
    <p class="normal">And that covers deploying Streamlit with Heroku! As you can see, Streamlit Community Cloud handles the majority of these difficulties out of the box, so I would make an effort to make Streamlit Community Cloud work whenever possible. However, this section should have given you an appreciation for the true breadth of options and configuration controls in front of us when we use Hugging Face Spaces and Heroku, which may come in handy in the future.</p>
    <h1 class="heading-1" id="_idParaDest-101">Summary</h1>
    <p class="normal">This has been by far the most technical of our chapters so far, so congratulations on making it through! Deploying applications is notoriously difficult and time-consuming, and requires skills from software engineering and DevOps, along with often requiring experience with version control software (such as Git) and UNIX-style commands and systems. This is part of the reason why Streamlit Community Cloud is such a crucial innovation, but in this chapter, we have learned how to push the edge of Streamlit deployment by renting our own virtual machines and deploying apps on Hugging Face Spaces and Heroku. We have also learned how to figure out what the right deployment strategy is before starting out, which will save hours or days of work (nothing is worse than finishing the deployment of an app and finding out you need to use another platform!).</p>
    <p class="normal">Next, we’ll move on to learning how to query from databases inside our Streamlit apps.</p>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask questions to the author, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="https://packt.link/sl"><span class="url">https://packt.link/sl</span></a></p>
    <p class="normal"><img alt="" role="presentation" src="../Images/QR_Code13440134443835796.png"/></p>
  </div>
</body></html>