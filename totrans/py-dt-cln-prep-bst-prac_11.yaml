- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consuming Time Series Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter about time series analysis, we will explore the fundamental
    concepts, methodologies, and practical applications of time series across various
    industries. Time series analysis involves studying data points collected over
    time to identify patterns and trends and make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will deep dive into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the components of time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying missing values in time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing values in time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering with time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying time series techniques in different industries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The complete code for this chapter can be found in this book’s GitHub repository
    at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter11](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter11).
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following code to install all the necessary libraries we will use in
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the components of time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series data refers to a sequence of observations or measurements that are
    collected and recorded *over time*. Unlike non-sequential data, where observations
    are taken at a single point in time, time series data captures information at
    multiple points in a sequential order. Each data point in a time series is associated
    with a specific timestamp, creating a temporal structure that allows trends, patterns,
    and dependencies to be analyzed over time. Let’s discuss the different components
    of the time series data, starting with the trend.
  prefs: []
  type: TYPE_NORMAL
- en: Trend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **trend** component represents the long-term movement or direction in the
    data. It reflects the overall pattern that persists over an extended period, indicating
    whether the values are generally increasing, decreasing, or remaining relatively
    constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trends have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Upward trend**: Values systematically increase over time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Downward trend**: Values systematically decrease over time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flat trend**: Values remain relatively constant over time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the trend is essential for making informed decisions about the long-term
    behavior of the phenomenon being observed. It provides insights into the overall
    direction and can be valuable for forecasting future trends. In the following
    section, we will present a use case inspired by the data world that focuses on
    the trend component.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing long-term sales trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this use case, we aim to analyze a decade-long sales trend to understand
    the growth pattern of a business from 2010 to 2020\. You can find the code for
    this example in this book’s GitHub repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/trend.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/trend.py).
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by generating a date range and corresponding sales data for each
    month over 10 years:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must plot the data to visualize the upward trend:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in the following graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Monthly sales data with upward trend](img/B19801_11_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Monthly sales data with upward trend
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.1* shows a consistent upward trend in sales over the decade. This
    indicates that the business has been growing steadily.'
  prefs: []
  type: TYPE_NORMAL
- en: In our initial analysis, we focused on understanding the overall upward trend
    in sales data over a decade. This provided us with valuable insights into the
    long-term growth of the business.
  prefs: []
  type: TYPE_NORMAL
- en: Often, businesses experience fluctuations that recur regularly within specific
    periods, such as months or quarters. This is known as seasonality. Recognizing
    these seasonal patterns can be just as crucial as understanding the overall trend
    as it helps businesses anticipate and prepare for periods of high or low demand.
    To illustrate this, let’s extend our analysis so that it includes seasonality
    in the sales data.
  prefs: []
  type: TYPE_NORMAL
- en: Seasonality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Seasonality** refers to the repetitive and predictable patterns that occur
    at regular intervals within the time series. These patterns often correspond to
    specific time frames, such as days, months, or seasons, and can be influenced
    by external factors such as weather, holidays, or cultural events.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike long-term trends, seasonality spans shorter time frames, exerting a short-term
    influence on the data. This recurring nature of seasonality allows businesses
    to anticipate and plan for fluctuations in demand, thereby optimizing their operations
    and strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Understanding seasonality helps in identifying recurring patterns and predicting
    when certain behaviors or events are likely to occur. This information is crucial
    for accurate forecasting and planning.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will extend the sales use case presented previously
    while focusing on the seasonal component.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing long-term sales trends with seasonality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this part of the use case, we aim to analyze a decade-long sales trend that
    includes seasonal variations. You can find the full code example here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/seasonality.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/seasonality.py).
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by generating a date range and corresponding sales data for each
    month over 10 years:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must plot the data to visualize the seasonal component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Monthly sales data with seasonality](img/B19801_11_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Monthly sales data with seasonality
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.2* reveals a repeating pattern every 12 months, indicating a clear
    seasonality in sales. Sales peak around mid-year and drop toward the end and beginning
    of the year, suggesting higher sales in summer and lower sales in winter. This
    pattern’s consistency over the years can aid in predicting future sales cycles.
    Understanding these seasonal trends is valuable for inventory management, marketing
    campaigns, and resource allocation during peak sales periods, allowing businesses
    to optimize their strategies accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While identifying trends and seasonality provides valuable insights into sales
    patterns, real-world data often contains another critical component: noise. In
    the following section, we will deep dive into noise and extend the sales use case
    so that it explores how noise affects sales.'
  prefs: []
  type: TYPE_NORMAL
- en: Noise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Noise**, also known as residuals or errors, represents the random fluctuations
    or irregularities in the time series data that cannot be attributed to the trend
    or seasonality. It reflects the variability in the data that is not explained
    by the underlying patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: While noise is often considered unwanted, it is a natural part of any real-world
    data. Recognizing and isolating noise is essential for building accurate models
    and understanding the inherent uncertainty in the time series.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will extend the sales use case presented previously
    while focusing on noise.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing sales data with noise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this use case, we aim to analyze sales data that includes noise, in addition
    to trends and seasonality. This will help us understand how random fluctuations
    impact our ability to identify underlying patterns. To follow along with this
    example, take a look at the following code: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/noise.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/1.decomposing_time_series/noise.py).
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will start by generating a date range for each month over 10 years:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must create sales data with noise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must plot the data to visualize the noise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Monthly sales data with noise](img/B19801_11_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Monthly sales data with noise
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.3* shows random, unpredictable variations that do not follow any
    specific pattern. These fluctuations occur over short timeframes, creating instability
    in the data and making it harder to see any patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we can identify the different time series components, let’s have a
    look at the different types of time series.
  prefs: []
  type: TYPE_NORMAL
- en: Types of time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will quickly revise the types of time series data – univariate
    and multivariate – while clarifying their distinctions and showcasing their applications.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate time series data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Univariate time series data** consists of a single variable or observation
    recorded over time. It is a one-dimensional time-ordered sequence, making it simpler
    to analyze compared to multivariate time series data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a univariate time series representing the monthly average temperature
    in a city over several years. You can find the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/2.types/univariate.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/2.types/univariate.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate our univariate time series data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create the data range we want, in this case from `2010-01-01`
    to `2020-12-31`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must create the corresponding values for the temperatures by adding
    noise using a **normal distribution** (also known as a Gaussian distribution):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s understand the value parameters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`20`: This is the `5`: This is the **standard deviation** of the normal distribution.
    The noise values will typically vary by about ±5 units from the mean. A larger
    standard deviation means the noise will be more spread out, while a smaller standard
    deviation means the noise values are closer to the mean.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The date range we created previously is passed as an index to the DataFrame.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s plot the univariate time series data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will output the following plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Univariate temperature data](img/B19801_11_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Univariate temperature data
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the univariate time series represents the monthly average temperature.
    Since the data is randomly generated with a mean of 20°C and some variation (a
    standard deviation of 5°C), the plot will exhibit random fluctuations around this
    average temperature.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the complexities of univariate time series data lays a solid foundation
    for delving into multivariate time series analysis. Unlike univariate data, which
    involves observing a single variable over time, multivariate time series data
    involves monitoring multiple interrelated variables simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate time series data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Multivariate time series data** involves multiple variables or observations
    recorded over time. Each variable is a time-ordered sequence, and the variables
    may be interdependent, capturing more complex relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a multivariate time series representing both the monthly average temperature
    and monthly rainfall in a city over several years. You can find the code for this
    example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/2.types/multivariate.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/2.types/multivariate.py).
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add the required libraries for this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s generate an example of multivariate time series data by using the
    temperature data we created previously and adding a new time series in the same
    DataFrame representing the rainfall data with different mean and standard deviation
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Combine all the time series into the same DataFrame, making sure to include
    both temperature and rainfall data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The combined time series DataFrame is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s plot the multivariate time series data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Multivariate data](img/B19801_11_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Multivariate data
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the multivariate time series includes both temperature and
    rainfall data, providing a more comprehensive view of environmental conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, univariate data is simpler to work with, while multivariate data allows
    us to capture more complex relationships and dependencies between variables over
    time. Multivariate analysis is essential for tackling real-world challenges in
    diverse fields such as economics, finance, environmental science, and healthcare,
    where understanding multifaceted relationships among variables is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a strong understanding of time series data, we can explore
    methods for cleaning and managing this type of data effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying missing values in time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identifying missing values in time series data is somewhat like identifying
    missing values in other types of data, but there are specific considerations due
    to the temporal nature of time series. Since we covered some of these techniques
    in [*Chapter 8*](B19801_08.xhtml#_idTextAnchor195), *Detecting and Handling Missing
    Values and Outliers*, let’s summarize them here and highlight their specific adaptations
    for analyzing time series data using a stock market analysis use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider a use case where we have daily stock prices (open, high, low,
    and close) for a particular company over several years. Our goal is to identify
    missing data in this time series to ensure the integrity of the dataset. You can
    find the code for this example here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/1.identify_missing_values.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/1.identify_missing_values.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by generating the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will generate a date range for business days from January 1, 2020,
    to December 31, 2023\. Here, `freq=''B''` is used to generate a range of dates
    that includes only *business days* (weekdays, excluding weekends):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must generate random stock prices for the date range with a length
    of *n*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must create a DataFrame by passing all the separate data points that
    were created in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s introduce random NaN values to simulate some missing values in the
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, drop random dates to simulate missing timestamps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, display the first few rows of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The key thing to notice here is that we have two kinds of missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: Complete rows missing, so one full date index is not available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some observations in some of the columns are missing for the current date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will mainly address the first case here as we covered the second case in
    [*Chapter 8*](B19801_08.xhtml#_idTextAnchor195)*, Detecting and Handling Missing
    Values and Outliers*. Let’s start with the simple but effective `isnull()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for NaNs or null values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike regular datasets, time series data points are ordered in time. Missing
    values can break the continuity and affect the analysis of trends and seasonal
    patterns. Let’s use the `isnull()` method to identify missing timestamps. Here,
    we are looking to find complete rows that are missing from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To check which dates are missing from a time series DataFrame, we need to create
    a full date range (with no missing values) in the frequency of our current DataFrame
    index and compare it against the date range we have in our current DataFrame.
    Here, we are creating a complete date range for business days:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To quickly see the missing index points, the DataFrame must be reindexed to
    this complete date range so that we can identify any missing timestamps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can use the `isnull()` method to identify any missing timestamps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we can see that there are some missing timestamps in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The analysis so far tells us that we have complete dates missing from the dataset.
    Now, let’s add some visual plots to help us better see the gaps in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As presented in [*Chapter 8*](B19801_08.xhtml#_idTextAnchor195)*, Detecting
    and Handling Missing Values and Outliers*, you can use the `isnull()` method to
    see how many nulls we have in each column – for example, `missing_values =` `df.isnull().sum()`.
  prefs: []
  type: TYPE_NORMAL
- en: Visual inspection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizing the data can help us identify missing values and patterns of missingness.
    Plots can reveal gaps in data that are not immediately obvious from a tabular
    inspection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with the example from the previous section, let’s plot our time
    series data and mark any missing values on our graph:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the closing prices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Mark missing timestamps with vertical lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in the following plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Daily closing prices with missing timestamps highlighted](img/B19801_11_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Daily closing prices with missing timestamps highlighted
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 11**.6*, the closing prices are plotted in blue with markers, while
    missing timestamps are highlighted with dotted lines, making it easy to see gaps
    in the data. Now, let’s explore the final method, known as lagged analysis. In
    this method, we create a lagged version of the series and compare it with the
    original to detect inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B19801_03.xhtml#_idTextAnchor064), *Data Profiling – Understanding
    Data Structure, Quality, and Distribution* we demonstrated various data profiling
    methods. You can apply similar techniques to time series data by using the built-in
    gap analysis feature. Simply pass `tsmode=True` when creating the profile report
    – for example, `profile =` `ProfileReport(df, tsmode=True)`.
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, it’s essential to explore effective strategies for handling
    missing data in time series.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values in time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Missing values are a common challenge in time series data and can arise due
    to various reasons, such as sensor failures, data transmission issues, or simply
    the absence of recorded observations. As we’ve discussed, two main scenarios often
    arise:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Some null values in features**: Imagine a stock market analysis where daily
    trading data is collected. While all trading days are accounted for, the volume
    of shares traded on certain days may be missing due to reporting errors. This
    scenario presents a challenge: how do you maintain the integrity of the dataset
    while ensuring that analyses remain robust?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete rows are missing**: Conversely, consider a weather monitoring system
    that records daily temperatures. If entire days of data are missing – perhaps
    due to sensor failures – this poses a significant issue. Missing timestamps means
    you cannot simply fill in values; the absence of data for those days disrupts
    the entire time series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will focus on solving the first scenario and consider
    the existence of null values in some features. Once we have done this, we can
    adjust it for the second one.
  prefs: []
  type: TYPE_NORMAL
- en: Removing missing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Removing missing data is a straightforward approach, but it should be done
    with caution while considering the impact on the overall dataset. Here are some
    scenarios where removal might be appropriate:'
  prefs: []
  type: TYPE_NORMAL
- en: If the missing values constitute a small percentage of the dataset (for example,
    less than 5%), removing them might be feasible. This approach works well if the
    data loss does not significantly impact the analysis or the conclusions drawn
    from the dataset. For example, in a dataset with 10,000 time points, if 50 points
    are missing, removing these 50 points (0.5% of the data) might not significantly
    affect the overall analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If imputing missing values would introduce too much uncertainty, especially
    if the values are critical and cannot be accurately estimated. This scenario is
    common when the missing values are highly unpredictable data, making imputation
    unreliable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If missing values occur completely at random and do not follow any systematic
    pattern. An example of this is sensor data where occasional random failures cause
    missing readings, but there is no underlying pattern to these failures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s revisit the stock market use case and see how we can drop the null values
    to see what effect this has on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Removing missing data in the stock market use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our stock prices data scenario, we will add some NaN values and evaluate
    the impact of removing them. You can find the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/2.remove_missing_values.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/2.remove_missing_values.py).
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with the example from the previous section, we will create the stock
    data with the different features. Then, we will randomly select some indexes from
    specific features (for example, `close` and `open`) so that we can map the values
    for each feature of that index to a NaN value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will map the indices that were randomly selected before to NaN values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s check how many NaNs or null values are available in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As expected, some null values were introduced on the `open` and `close` features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s check how many rows we have before removing any based on the nulls we
    have in the dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this stage, we will drop any rows that have NaN values in either the `close`
    or `low` columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot the time series data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Daily closing prices with missing data to be dropped/flagged](img/B19801_11_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Daily closing prices with missing data to be dropped/flagged
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 11**.7*, the original closing prices are plotted, and points
    that were dropped due to missing values are highlighted with red “x” markers.
    Remember that even with selective dropping, removing rows can lead to a loss of
    useful information as it reduces the sample size, which can decrease the statistical
    power of the analysis and affect the generalizability of the results.
  prefs: []
  type: TYPE_NORMAL
- en: In scenarios where retaining every timestamp is crucial but missing values within
    features need to be addressed, forward and backward filling offer practical solutions.
    These methods allow us to maintain the chronological integrity of time series
    data while efficiently filling in missing values based on adjacent observations.
    Let’s explore how forward and backward filling can effectively handle missing
    data in time series analyses.
  prefs: []
  type: TYPE_NORMAL
- en: Forward and backward fill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Forward fill** (**ffill**) and **backward fill** (**bfill**) are methods
    of imputing missing values by propagating the last known value forward or the
    next known value backward in the time series, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When dealing with time series backfilling, the choice between ffill and bfill
    depends on several factors and use cases. Here’s an overview of when to use each
    approach and the thought process behind these decisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ffill**: Forward filling, also known as **last observation carried forward**
    (**LOCF**), propagates the last known value forward to fill in missing data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s when you should use it:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When you believe the most recent known value is the best predictor of missing
    future values
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In financial time series, where carrying forward the last known price is often
    a reasonable assumption
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When dealing with slowly changing variables where persistence is a good assumption
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In scenarios where you want to maintain the most recent state until new information
    becomes available
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you’re still uncertain or find yourself pondering which method to use, answering
    “yes” to at least two of the following three questions can guide you in the right
    direction:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Is the variable likely to remain relatively stable over short periods?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Would using the last known value be a reasonable assumption for the missing
    data?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it more important to reflect the most recent known state rather than potential
    future changes?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bfill**, on the other hand, propagates the next known value backward to fill
    in missing data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s when you should use it:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When you have more confidence in future values than past values
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In scenarios where you want to retroactively apply known outcomes to previous
    missing periods
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When you’re dealing with lagged effects where future events influence past missing
    data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In cases where you want to align data with the next known state rather than
    the previous one
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you’re still uncertain or find yourself pondering which method to use, answering
    “yes” to the following questions will guide you in the right direction:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Is the next known value likely to be more representative of the missing data
    than the previous known value?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Are you dealing with a scenario where future information should inform past
    missing values?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Would aligning with the next known state provide more meaningful insights for
    your analysis?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, choosing between ffill and bfill often requires a combination of
    domain expertise, understanding of the data generation process, and consideration
    of the specific analytical goals. It’s also worth experimenting with both methods
    and comparing the results to see which provides more meaningful and accurate insights
    for your particular use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, there are some important considerations when using ffill and bfill
    in time series data. Let’s expand on those:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequential nature**: The sequential nature of time series data is indeed
    crucial for both ffill and bfill methods. Both methods rely on the assumption
    that adjacent data points are related, which is fundamental to time series analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ffill and increasing trends**: Ffill can be appropriate for increasing trends
    as it carries forward the last known value, potentially underestimating the true
    value in an upward trend. However, it may lead to a “staircase” effect in strongly
    increasing trends, potentially understating the rate of increase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bfill and decreasing trends**: Bfill can be suitable for decreasing trends
    as it pulls back future lower values, potentially overestimating the true value
    in a downward trend. It may create a similar “staircase” effect in strongly decreasing
    trends, potentially overstating the rate of decrease.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice between ffill and bfill should consider not just the direction of
    the trend, but also its *strength* and the *length* of missing data periods. For
    subtle trends, either method might be appropriate, and the choice may depend more
    on other factors, such as the nature of the data or the specific analysis goals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both methods can indeed propagate errors if the missing values are inconsistent
    with surrounding data points. This is particularly problematic for long stretches
    of missing data, where the filled values may significantly deviate from the true
    underlying pattern.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling outliers**: If an outlier precedes or follows a stretch of missing
    data, ffill or bfill can propagate this anomalous value, distorting the series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assumption of data continuity**: Both methods assume that the missing data
    can be **reasonably approximated** by adjacent known values, which may not always
    be true. For variables that can change abruptly or have discontinuities, these
    methods may be inappropriate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s revisit the stock prices example and see how we can fill the missing values
    on the columns with nulls.
  prefs: []
  type: TYPE_NORMAL
- en: Filling nulls in the stock market use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we will not be focusing on missing indexes, just on the missing
    data in some of the features available. Let’s deep dive into the code – as always,
    you can find the full end-to-end code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/3.back_forward_fill.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/3.back_forward_fill.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code introduces random missing values into the `close` and `open` columns
    of a DataFrame (`df`). It begins by randomly selecting 50 indices from the DataFrame’s
    index using `np.random.choice`. The selected indices are stored in two variables,
    `nan_indices_close` and `nan_indices_open`, which correspond to the rows where
    missing values will be inserted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code uses the `.loc` accessor to assign NaN to the `close` column
    at the indices specified by `nan_indices_close`, and similarly to the `open` column
    at the indices specified by `nan_indices_open`. Effectively, this creates 50 random
    missing values in each of these columns, which can be useful for simulating real-world
    data scenarios or testing data handling techniques:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fill NaN values using ffill and bfill:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s see the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, on `2020-01-07` and `2020-01-15`, there are missing values (NaN)
    in the `close` column. This indicates that the closing prices for these dates
    were not recorded or are otherwise unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we’ve learned, the ffill method (`close_ffill`) fills missing values with
    the last valid observation:'
  prefs: []
  type: TYPE_NORMAL
- en: For `2020-01-07`, the closing price is filled with the last known value from
    `2020-01-06` (`193.27`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `2020-01-15`, the missing value is filled with the last valid price from
    `2020-01-14` (`152.14`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, the bfill method (`close_bfill`) fills missing values with
    the next valid observation:'
  prefs: []
  type: TYPE_NORMAL
- en: For `2020-01-07`, since no subsequent valid price is recorded immediately, it
    takes the closing price from `2020-01-08` (`120.03`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `2020-01-15`, the value is filled with the next known price from `2020-01-16`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at what happened in the data after we performed the
    different filling methods:'
  prefs: []
  type: TYPE_NORMAL
- en: On `2020-01-07`, ffill overestimates the missing value compared to bfill, which
    aligns more closely with the next known value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On `2020-01-15`, ffill and bfill provide different estimates, with ffill potentially
    overestimating the value compared to bfill
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a general recommendation, we need to investigate the pattern of missing values.
    If the missing values are *random and sparse*, either method might be appropriate.
    However, if there is a systematic pattern, more sophisticated imputation methods
    might be needed, such as *interpolation*. Interpolation allows us to estimate
    missing data points by leveraging the existing values in the dataset, providing
    a more nuanced approach that can capture trends and patterns over time. We’ll
    discuss this in more detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Interpolation** is a method for estimating missing values by filling in the
    gaps based on the surrounding data points. Unlike ffill and bfill, which copy
    existing values, interpolation uses mathematical techniques to estimate missing
    values. There are different interpolation techniques and applications. So, let’s
    have a look at the available options, along with their considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear interpolation**: Linear interpolation connects two adjacent known
    data points with a straight line and estimates the missing values along this line.
    It is the simplest form of interpolation and assumes a linear relationship between
    the data points. It is suitable for datasets where changes between data points
    are expected to be linear or nearly linear. It is commonly used in financial data,
    temperature readings, and other environmental data where gradual changes are expected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial interpolation**: Polynomial interpolation fits a polynomial function
    to the known data points and uses this function to estimate missing values. Higher-order
    polynomials can capture more complex relationships between data points. It is
    suitable for datasets with non-linear trends, and it is usually used in scientific
    and engineering applications, where data follows a polynomial trend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spline interpolation**: Spline interpolation uses piecewise polynomials,
    typically cubic splines, to fit the data points, ensuring smoothness at the data
    points and providing a smooth curve through the data. It is suitable for datasets
    requiring smooth transitions between data points and is commonly used in computer
    graphics, signal processing, and environmental data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s use interpolation in our use case.
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation in the stock market use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider the same time series dataset presented previously with missing values.
    In this case, we want to impute these missing values using different interpolation
    methods. You can find the full code examples in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/4.interpolation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/3.missing_values/4.interpolation.py).
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code introduces random missing values into the `close` and `open`
    columns of our DataFrame (`df`), as we did in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following line of code is used to fill in missing values in the `close`
    column of our DataFrame (`df`) using linear interpolation. The code specifically
    employs **linear interpolation**, where the missing values are estimated by drawing
    a straight line between the nearest known data points before and after the missing
    value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can interpolate missing values using polynomial interpolation by changing
    the method argument to `method=''polynomial''`. This specifies that the interpolation
    should be done using a polynomial function of `order=3`. The `order` argument
    indicates the degree of the polynomial to be used. In this case, a cubic polynomial
    (third degree) is used, which means the function that estimates the missing values
    will be a curve, potentially providing a better fit for more complex data trends
    compared to a simple straight line (as in linear interpolation):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can interpolate missing values using spline interpolation by changing the
    method to `method=''spline''`. This specifies that the interpolation should be
    done using spline interpolation, which is a piecewise polynomial function that
    ensures smoothness at the data points. The `order=3` argument indicates the degree
    of the polynomial used in each piece of the spline. In this case, a cubic spline
    (third-degree polynomial) is used, meaning that the interpolation will involve
    fitting cubic polynomials to segments of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s plot the interpolated data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Daily closing prices interpolated](img/B19801_11_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Daily closing prices interpolated
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 11**.8*, we can see how the data changes with the different interpolation
    methods. To better grasp the differences, let’s have a look at the actual data
    after interpolation, as shown in *Figure 11**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Table of the daily closing prices interpolated](img/B19801_11_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Table of the daily closing prices interpolated
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compare the different interpolation methods and come to some conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On 2020-01-07, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear** **interpolation**: 156.649626'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial** **interpolation**: 142.704592'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spline** **interpolation**: 143.173016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On 2020-01-15, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear** **interpolation**: 144.628098'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial** **interpolation**: 127.403857'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spline** **interpolation**: 128.666028'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on this data, linear interpolation seems to provide higher estimates compared
    to polynomial and spline interpolation. It assumes a linear trend between data
    points, which might not be accurate for non-linear data. Polynomial interpolation
    seems to provide lower estimates and capture more complex relationships but can
    be prone to overfitting. Finally, spline interpolation provides smooth estimates
    that are intermediate between linear and polynomial interpolation, offering a
    balance between simplicity and accuracy. In this specific case, we would go with
    spline interpolation as it provides a smooth curve that avoids abrupt changes,
    and the results are more realistic and closely aligned with the expected trends
    in the data. While spline interpolation is recommended based on the provided data,
    it is essential to validate the interpolated values against known data points
    or domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation methods such as linear, polynomial, and spline interpolation can
    also be used to deal with *outliers* in time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing and tuning interpolation arguments for filling missing values involves
    understanding the characteristics of your data and the specific needs of your
    analysis. For straightforward data with a linear trend, linear interpolation is
    efficient and effective. However, if your data exhibits non-linear patterns, polynomial
    interpolation can provide a better fit, with the degree of the polynomial (`order`)
    influencing the complexity of the curve; lower orders work well for simpler trends,
    while higher orders may capture more detail but risk overfitting. Spline interpolation
    offers a smooth and flexible approach, with cubic splines (`order=3`) being commonly
    used for their balance of smoothness and flexibility. To tune these methods, start
    with simpler approaches and test more complex ones progressively while monitoring
    for overfitting and ensuring the fit aligns with the data’s underlying trends.
    Employ cross-validation, visual inspection, and statistical metrics to evaluate
    and refine your interpolation choices.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored the various techniques for handling missing data in
    time series, it’s essential to summarize the different filling methods to understand
    their unique applications and effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the different methods for missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Handling missing values in time series data is an involved process that requires
    thoughtful consideration of the specific context and characteristics of the dataset.
    The decision to drop values, use bfill, or apply interpolation should be guided
    by a careful assessment of the impact on subsequent analyses and the preservation
    of critical information within the time series. The following table summarizes
    the different techniques and can be used as a guide:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **When** **to Use** | **Pros** | **Cons** |'
  prefs: []
  type: TYPE_TB
- en: '| Dropping missing values | A small percentage of missing values | - Simplicity-
    Avoids imputation uncertainty | - Information loss- Potential bias |'
  prefs: []
  type: TYPE_TB
- en: '| Bfill | Missing values are expected to precede consistent values | - Preserves
    the general trend- Suitable for increasing trends | - Propagates errors if missing
    values are not similar to the following values |'
  prefs: []
  type: TYPE_TB
- en: '| Ffill | Missing values are expected to follow consistent values | - Simple
    to implement- Maintains recent state until new data is available | - Can misrepresent
    data if trends change- Propagates errors if missing values differ from previous
    values |'
  prefs: []
  type: TYPE_TB
- en: '| Linear interpolation | Missing values need to be estimated based on neighboring
    data points | - Simple and easy to implement- Preserves overall trend | - May
    not capture non-linear trends- Sensitive to outliers |'
  prefs: []
  type: TYPE_TB
- en: '| Polynomial interpolation | Missing values need to be estimated with more
    complex relationships | - Captures complex relationships- Flexible with polynomial
    order | - Can lead to overfitting and oscillations- Computationally intensive
    |'
  prefs: []
  type: TYPE_TB
- en: '| Spline interpolation | Missing values need to be estimated with smooth transitions
    | - Provides a smooth curve- Avoids oscillations of high-order polynomials | -
    More complex to implement- Computationally intensive |'
  prefs: []
  type: TYPE_TB
- en: Table 11.1 – Comparison of the different methods to handle missing data in time
    series
  prefs: []
  type: TYPE_NORMAL
- en: 'Having examined the various methods for filling missing values in time series
    data, it is equally important to address another critical aspect: the correlation
    of a time series with its own lagged values.'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Autocorrelation** and **partial autocorrelation** are crucial tools in time
    series analysis that provide insights into data patterns and guide model selection.
    For outlier detection, they help distinguish between genuine anomalies and expected
    variations, leading to more accurate and context-aware outlier identification.'
  prefs: []
  type: TYPE_NORMAL
- en: Autocorrelation and partial autocorrelation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Autocorrelation** refers to correlating a time series with its own lagged
    values. Simply put, it measures how each observation in a time series is related
    to its past observations. Autocorrelation is a crucial concept in understanding
    the temporal dependencies and patterns present in time series data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Partial autocorrelation function** (**PACF**), on the other hand, is a statistical
    tool that’s used in time series analysis to measure the correlation *between a
    time series and its lagged values after removing the effects of intermediate lags*.
    It provides a more direct measure of the relationship between observations at
    different time points, excluding the indirect effects of shorter lags.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both autocorrelation and partial autocorrelation help in the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal patterns**: They help identify patterns that repeat over time. This
    is crucial for understanding the inherent structure of the time series data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stationarity assessment**: They help in assessing the stationarity of a time
    series. A lack of stationarity can impact the reliability of statistical analyses
    and model predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lag selection for models**: They guide the selection of appropriate lags
    for time series models, such as **autoregressive** (**AR**) components in **autoregressive
    moving average** (**ARIMA**) models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonality detection**: Significant peaks in the **autocorrelation function**
    (**ACF**) plot at specific lags indicate the presence of seasonality, providing
    insights for further analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: Unusual patterns in the autocorrelation function may
    suggest anomalies or outliers in the data, prompting investigation and cleaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s perform an ACF and PACF analysis on the `close_filled` series from
    our stock price dataset. This analysis will help us determine the appropriate
    parameters (`p` and `q`) for the ARIMA modeling we will perform in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: ACT and PACF in the stock market use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will continue with the example we’ve used so far and add the ACT and PACF
    charts. As always, you can have a look at the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/4.analisis/autocorrelation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/4.analisis/autocorrelation.py).
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an autocorrelation plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a partial autocorrelation plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resultant plots are shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.10 – ACF and PACF plots](img/B19801_11_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – ACF and PACF plots
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explain what we can see in the preceding charts. Here’s what we can see
    for ACF:'
  prefs: []
  type: TYPE_NORMAL
- en: The ACF plot shows the correlation between the series and its lagged values
    at various lags (`lags=40`, in this example). The *X*-axis of the ACF plot represents
    the number of lags, indicating how many points back in time the correlation is
    being calculated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Y*-axis of the ACF plot represents the correlation coefficients between
    the original time series and its lagged values. The correlation values range from
    -1 to 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The blue shaded area represents the confidence interval. Bars that extend beyond
    the shaded area are considered statistically significant and indicate strong autocorrelation
    at those lags, which suggest potential values for the *q parameter in the ARIMA
    model* (MA order), as we will see in the following section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Significant peaks at regular intervals indicate the presence of *seasonality*
    in the time series data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is a significant autocorrelation at lag 1 in the ACF plot (a spike
    that goes beyond the blue-shaded region, as in our case), it suggests that the
    series has a strong correlation *with its immediate previous value*. This might
    indicate that the series is *not stationary* and may need differencing (d > 0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s what we can see for PACF:'
  prefs: []
  type: TYPE_NORMAL
- en: The PACF plot shows the correlation between the series and its lagged values
    after removing the effects explained by shorter lags.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Significant spikes in the PACF plot indicate that lag 1 and potentially lag
    2 could be good candidates for the *p parameter in the ARIMA model* (AR order).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When we specify `lags=40` in the context of ACF and PACF plots, we are examining
    the autocorrelation and partial autocorrelation of the time series at 40 different
    lag intervals. This means we will see how the series is correlated with itself
    from *lag 1 up to* *lag 40*.
  prefs: []
  type: TYPE_NORMAL
- en: ACF and PACF plots are crucial for identifying the underlying structure in a
    time series. In the next section, we will link the ACF and PACF analysis to outlier
    detection and handling to ensure our time series model captures the underlying
    patterns accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series data often exhibit seasonal patterns (for example, sales spikes
    during holidays) and trends (for example, gradual growth over the years). An outlier
    in this context might not be an anomaly; rather, it could reflect a normal seasonal
    effect or a change in the underlying trend. For example, a sudden spike in retail
    sales during Black Friday is expected and should not be treated as an outlier.
    Techniques such as **seasonal decomposition of time series** (**STL**), autocorrelation,
    and seasonal indices can aid in understanding the expected behavior of the data,
    thus providing a clearer basis for identifying outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying outliers with seasonal decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way to identify outliers in time series is to decompose the series into
    trend, seasonality, and residual components, as outliers are often identified
    in the residual component. To decompose the series into trend, seasonality, and
    residual components, we can use the STL method. This method helps in identifying
    and handling outliers by analyzing the residual component, which ideally should
    be white noise. Let’s see how we can do this using the stock market data. You
    can find the full code example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/5.outliers/1.seasonal_decomposition.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/5.outliers/1.seasonal_decomposition.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code snippet, we decompose the time series while assuming there are
    252 business days in a year. We will also calculate the Z-scores of residuals
    to identify outliers using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s plot the decomposed series:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 – Decomposed time series](img/B19801_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Decomposed time series
  prefs: []
  type: TYPE_NORMAL
- en: 'Outliers can be detected by analyzing the residual component. Significant deviations
    from zero or sudden spikes in the residual component indicate potential outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Table of decomposed values](img/B19801_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Table of decomposed values
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the decomposed time series in *Figure 11**.11*, we can analyze the
    outliers by examining the `residual` and `resid_z` columns. Typically, Z-scores
    with an absolute value greater than 2 or 3 are considered potential outliers.
    In this dataset, the largest positive residuals are observed on `2020-01-06` (Z-score:
    1.468043), `2020-01-17` (Z-score: 1.300488), and `2020-01-27` (Z-score: 1.172529),
    while the largest negative residuals are on `2020-01-15` (Z-score: -1.721474)
    and `2020-01-22` (Z-score: -1.082559). Although these values indicate some deviations
    from the trend and seasonal components, none of the Z-scores exceed the typical
    threshold of ±2 or ±3, suggesting that there are no extreme outliers in this dataset.
    The residuals appear to be relatively well-distributed around zero, indicating
    a good fit for the decomposition model. However, the dates with the largest deviations
    (`2020-01-06`, `2020-01-15`, and `2020-01-17`) might be worth investigating further
    for any unusual events or factors that could explain their deviation from the
    expected values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On digging deeper into this data to understand the reasons behind the fluctuations
    and upon closer inspection, we can see that the deviations on these dates were
    due to specific events and system issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Disclaimer!
  prefs: []
  type: TYPE_NORMAL
- en: The following events correspond to made-up events!
  prefs: []
  type: TYPE_NORMAL
- en: '`2020-01-06`: A technical glitch in the stock exchange’s trading system caused
    a temporary spike in prices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2020-01-15`: An erroneous trade input led to a sudden drop in prices, which
    was later corrected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2020-01-17`: A major economic announcement led to increased volatility and
    a brief surge in stock prices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2020-01-22`: A miscommunication about quarterly earnings results caused temporary
    panic selling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2020-01-27`: Rumors of a merger and acquisition led to speculative buying,
    temporarily inflating the prices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These findings helped us understand that the residuals’ deviations were not
    random but were due to specific, identifiable events. While these events did not
    qualify as significant outliers statistically, they highlighted the inherent volatility
    and noise in stock price data. Given the noisy nature of stock prices, even without
    significant outliers, smoothing techniques become crucial!
  prefs: []
  type: TYPE_NORMAL
- en: Handling outliers – model-based approaches – ARIMA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ARIMA models are widely used for forecasting time series data. They predict
    future values based on past observations, making them effective tools for identifying
    outliers by comparing actual values against predicted values. The ARIMA model
    consists of three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Autoregressive** (**AR**): Uses the dependency between an observation and
    several lagged observations (p)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrated** (**I**): Uses differencing of observations to make the time
    series stationary (d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Moving average** (**MA**): Uses dependency between an observation and a residual
    error from a moving average model applied to lagged observations (q)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ARIMA models are effective in handling the following outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Additive outliers** (**AO**): Sudden spikes or drops in the time series'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Innovative outliers** (**IO**): Changes that affect the entire series from
    the point of occurrence onwards'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s discuss how the ARIMA model can be used for outlier detection and smoothing
    in the context of the stock price data example we’ve been working with. You can
    find the full example at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/5.outliers/3.arima.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter11/5.outliers/3.arima.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the ARIMA model to the `close_filled` series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the residuals and Z-scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Identify any outliers based on the Z-score threshold (for example, ±3):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the original `close_filled` series and the smoothed series that was
    obtained from the ARIMA model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.13 – ARIMA smoothing and outlier detection](img/B19801_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – ARIMA smoothing and outlier detection
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate diagnostic plots to evaluate the model fit, including residual analysis,
    a **Quantile-Quantile** (**Q-Q**) plot, and standardized residuals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting plots are shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.14 – Residual analysis, Q-Q plot, and standardized residuals](img/B19801_11_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Residual analysis, Q-Q plot, and standardized residuals
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive a bit deeper into the diagnostic plots shown in *Figure 11**.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardized residuals**: Standardized residuals are the residuals from the
    ARIMA model scaled by their standard deviation. For the ARIMA model to be considered
    a good fit, the standardized residuals *should resemble white noise, meaning they
    should have no discernible pattern*. This implies that the residuals are randomly
    distributed with a mean of zero and constant variance. If a pattern is visible
    in the residuals, it suggests that the model has not captured some underlying
    structure in the data, and further refinement may be necessary. In our case, the
    *residuals* look like white noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Histogram plus KDE**: The histogram, combined with the **kernel density estimate**
    (**KDE**) plot of the residuals, provides a visual assessment of their distribution.
    For a well-fitted ARIMA model, the residuals should follow a normal distribution.
    The histogram should resemble the familiar bell curve, and the KDE plot should
    overlay a smooth curve that matches this shape. Deviations from the normal distribution,
    such as skewness or heavy tails, indicate that the residuals are not normally
    distributed, suggesting potential issues with the model. In our case, we don’t
    see any significant skewness or tails in the residuals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normal Q-Q plot**: The Q-Q plot compares the quantiles of the residuals to
    the quantiles of a normal distribution. If the residuals are normally distributed,
    the points on the Q-Q plot will lie along the 45-degree line. Significant deviations
    from this line indicate departures from normality. In our case, we don’t see any
    significant deviations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlogram** (**ACF of residuals**): The correlogram displays the ACF of
    the residuals. For a properly specified ARIMA model, the residuals should show
    no significant autocorrelation. This means that none of the lags should have statistically
    significant correlation coefficients. Significant spikes in the ACF plot indicate
    that the residuals are still correlated with their past values, suggesting that
    the model has not fully captured the time series’ structure. This can guide further
    model refinement, such as increasing the order of the AR or MA components. In
    our case, everything looks good.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is lag 0?
  prefs: []
  type: TYPE_NORMAL
- en: In the correlogram (ACF plot), the term **lag 0** refers to the autocorrelation
    of the time series with itself at lag 0, which is essentially the correlation
    of the time series with itself at the same time point. By definition, this correlation
    is always 1, because any time series is perfectly correlated with itself at lag
    0\. This means the autocorrelation value at lag 0 is always 1, which is why you
    see a spike at lag 0 in the ACF plot.
  prefs: []
  type: TYPE_NORMAL
- en: It is a good idea to go and play with the different settings and see their effect
    on the ARIMA model and the residuals.
  prefs: []
  type: TYPE_NORMAL
- en: Having explored the ARIMA method to detect and handle outliers in our stock
    price dataset, we have seen that outliers can significantly affect the accuracy
    and reliability of our time series model. While the ARIMA method helps in identifying
    and adjusting for these sudden changes, it’s also valuable to consider other approaches
    to robust outlier detection and handling. One such approach involves using moving
    window techniques, as we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Moving window techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Moving window techniques, also known as rolling or sliding window methods, involve
    analyzing a fixed-size subset or “window” of data that moves sequentially across
    a larger dataset. At each position of the window, a specific calculation or function
    is applied, such as computing the mean, median, sum, or more complex statistical
    measures. As the window shifts by one or more data points, the calculation is
    updated with the new subset of data. This method is particularly robust in time
    series analysis, where it is often used for smoothing data, identifying trends,
    or detecting anomalies over time.
  prefs: []
  type: TYPE_NORMAL
- en: The robustness of moving window techniques lies in their ability to provide
    localized analysis while maintaining a connection to the broader dataset. For
    example, when smoothing a time series, a moving average can reduce noise and highlight
    underlying trends without distorting the overall signal. Similarly, in financial
    data, moving windows can be used to compute rolling averages or volatilities,
    offering a real-time view of market conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will focus on two primary methods: **simple moving average**
    (**SMA**) and **exponential moving average** (**EMA**). Both can act as a basis
    that you can adjust with other statistics such as the median later.'
  prefs: []
  type: TYPE_NORMAL
- en: SMA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **SMA** is a commonly used statistical calculation that represents the
    average of a set of data points over a specified time. It is a type of moving
    average that smoothens out fluctuations in data to identify trends more easily.
    The SMA is calculated by summing up a set of values and dividing the sum by the
    number of data points. More advanced methods such as Kalman smoothing can estimate
    missing values by modeling the underlying process:'
  prefs: []
  type: TYPE_NORMAL
- en: '*SMA*t = (*X*t + *X*t–1 + *X*t–2 + ...+ *X*t–n+1)/*n*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*SMA*t is the SMA at time *t*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X*t + *X*t–1 + *X*t–2 + ...+ *X*t–n+1 are the values for the time period'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n* is the number of periods included in the calculations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s introduce the exponential moving average so that we can compare the
    two.
  prefs: []
  type: TYPE_NORMAL
- en: EMA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **EMA** gives more weight to recent data points and less weight to older
    data points. It uses an exponential decay formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*EM**A*t = *α* • *X*t + (1 – *α*) • *EM**A*t–1'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *α* is the smoothing factor.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss how the SMA and EMA can be used for outlier detection and
    smoothing in the context of the stock price data example we’ve been working with.
  prefs: []
  type: TYPE_NORMAL
- en: Smoothing with SMA and EMA on the stock price use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Continuing with the stock price data example we presented previously, let’s
    see the effect that SMA and EMA have on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s calculate the SMA with a window of 12 months:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `window` size for SMA and the `span` size for EMA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the SMA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the EMA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the residuals for the SMA and EMA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the original time series and the SMA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.15 – SMA and EMA](img/B19801_11_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – SMA and EMA
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we calculated the SMA and EMA using a window size of 20 and
    a span of 20, respectively. The window size for SMA determines how many previous
    data points are included in calculating the average at each point in time. Just
    like SMA, the frequency of your data points influences the choice of span. If
    your data is daily, a span zone of 20 might represent roughly 20 days of historical
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss the generated plot a little more:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SMA**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smoothing effect**: The SMA smooths the time series data by averaging the
    values within the window, reducing noise, and highlighting the underlying trend'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier impact**: While SMA reduces the impact of outliers, it can still
    be influenced by them since it considers all values within the window equally'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EMA**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smoothing effect**: The EMA also smooths the data but gives more weight to
    recent observations, making it more responsive to recent changes'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier impact**: EMA is less influenced by older outliers but can be more
    affected by recent ones due to its weighting scheme'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding a balance between smoothness and responsiveness
  prefs: []
  type: TYPE_NORMAL
- en: Larger window sizes result in smoother moving averages but may lag behind changes
    in the data. Smaller window sizes make the moving average more responsive to short-term
    fluctuations but might introduce more noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember the autocorrelation plot we created in *Figure 11**.10*? We can use
    the analysis to adjust the span or window size based on the observed autocorrelation
    patterns. The following points will help you guide the selection of the window
    size and span:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the frequency of your data points (daily, weekly, monthly).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the autocorrelation plot shows significant autocorrelation at shorter lags,
    a smaller span in EMA or a smaller window size in SMA can help maintain responsiveness
    to recent changes while mitigating the influence of short-term noise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your data exhibits seasonal patterns, you might choose a window size or span
    that aligns with the seasonal cycle. For example, if there’s a weekly seasonality,
    you might consider a window size of 5 or 7\. Use the autocorrelation chart to
    figure this out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To evaluate how well the window models perform, we can use the **mean absolute
    error** (**MAE**), as well as the **mean squared error** (**MSE**) and **root
    mean squared error** (**RMSE**). We can compare the errors between the original
    data and the smoothed values generated by these models, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16 – Performance metrics for SMA and EMA](img/B19801_11_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – Performance metrics for SMA and EMA
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure we have a clear understanding of the different metrics presented
    in *Figure 11**.16*, let’s look at this in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MAE**: This represents the average magnitude of the errors in a set of predictions,
    providing a simple average of the absolute differences between predicted and actual
    values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MSE**: This measures the average squared differences between predicted and
    actual values, penalizing larger errors more heavily than MAE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSE**: RMSE is the square root of MSE, offering an interpretable measure
    of the average magnitude of error, aligning with the scale of the original data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know what these terms represent, let’s unpick what they show for
    our stock prices use case. Lower MAE, MSE, and RMSE values indicate better performance
    of the smoothing method. While MAE and RMSE are very close for SMA and EMA, the
    MSE is lower for the exponential method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table compares and summarizes when to use the SMA and EMA:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Criteria** | **SMA** | **EMA** |'
  prefs: []
  type: TYPE_TB
- en: '| Type of smoothing | Simple and uniform smoothing of data points over a window
    | More responsive and adaptable, giving more weight to recent data points |'
  prefs: []
  type: TYPE_TB
- en: '| Weighing data points | Equal weight to all data points in the window | More
    weight to recent observations; older observations receive exponentially decreasing
    weights |'
  prefs: []
  type: TYPE_TB
- en: '| Responsiveness to changes | Lagging indicator; slower to respond to recent
    changes | More responsive to recent changes; adapts quickly to shifts in the data
    |'
  prefs: []
  type: TYPE_TB
- en: '| Suitability for stability | Suitable for stable and less volatile time series
    | Suitable for volatile or rapidly changing time series |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptability to trends | Smooths out long-term trends, suitable for identifying
    overall patterns | Adapts quickly to shifting trends, suitable for capturing recent
    changes |'
  prefs: []
  type: TYPE_TB
- en: '| Use case example | Analyzing long-term trends and identifying seasonality
    patterns | Capturing short-term fluctuations and reacting to market volatility
    |'
  prefs: []
  type: TYPE_TB
- en: '| Calculation complexity | Simpler calculation and easier to understand and
    implement | More complex calculations involve a smoothing factor |'
  prefs: []
  type: TYPE_TB
- en: Table 11.2 – Comparison between SMA and EMA
  prefs: []
  type: TYPE_NORMAL
- en: Beyond moving average techniques, exploring advanced feature engineering steps
    such as lags and differencing can significantly enrich our understanding and predictive
    capabilities. We’ll explore those in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering for time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective feature engineering is essential in time series analysis to uncover
    meaningful patterns and enhance predictive accuracy. It involves transforming
    raw data into informative features that capture temporal dependencies, seasonal
    variations, and other relevant aspects of the time series. The first technique
    we are going to explore is creating lags of features.
  prefs: []
  type: TYPE_NORMAL
- en: Lag features and their importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lag features are a crucial aspect of time series feature engineering as they
    allow us to transform time series data into a format suitable for supervised learning
    models. Lag features involve creating new variables that represent past observations
    of the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lag 1**: The value from the previous time step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lag 2**: The value from two time steps ago'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lag k**: The value from k time steps ago'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By shifting the time series data by a specified number of time steps (referred
    to as the lag), these past values are included as features in the model at the
    current timestamp. As we know, time series data often exhibits temporal dependencies,
    where the current value is related to past observations. Lag features help capture
    these dependencies, allowing the model to learn from historical patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss how the lag features can be used in the context of the stock
    price data example we’ve been working with.
  prefs: []
  type: TYPE_NORMAL
- en: Creating lag features in the stock price use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Continuing with the stock price data example we presented previously, let’s
    see the effect lag features have on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, introduce more aggressive outliers in the `close` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following function to create lagged features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create lagged features for the `close` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the original time series and lagged datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.17 – Original versus lagged features](img/B19801_11_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – Original versus lagged features
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 11**.17*, lag 1 (`close_lag_1`) represents the closing
    price from the previous day, lag 5 (`close_lag_5`) represents the closing price
    from 5 days ago, and so on. You can observe how each lag captures the historical
    values of the target variable. When adding lagged features to a time series, the
    start date of the data shifts forward because the first few data points cannot
    be used until the specified lag period is complete. This shift means that if you
    add more lags, the number of initial data points that lack complete lagged data
    increases, effectively pushing the start date forward.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to experiment with different lag values and see the effect on the
    dataset. Adjusting the lag values allows you to capture different degrees of temporal
    dependencies and trends in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Differencing time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 4*](B19801_04.xhtml#_idTextAnchor116), *Cleaning Messy Data and
    Data Manipulation*, we discussed how calculating the time difference between two
    datetime objects using the `diff()` function can help us measure the time elapsed
    between consecutive events. This technique is useful for understanding the temporal
    gaps in a sequence of timestamps. Similarly, in time series analysis, differencing
    is a powerful technique that’s used to stabilize the mean of a time series by
    removing changes in the level of a time series, thus eliminating trend and seasonality.
    Just as we calculated the time elapsed in the previous chapter, we can apply differencing
    to our stock market data to highlight changes over time. However, we will also
    introduce a new term – seasonal differencing.
  prefs: []
  type: TYPE_NORMAL
- en: Seasonal differencing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Seasonal differencing** is a technique that’s used to remove seasonal patterns
    from time series data, making it more stationary and suitable for analysis and
    forecasting. Seasonal differencing involves subtracting the value of an observation
    from a previous observation at a lag equal to the **seasonal** period. So, we
    need to identify the seasonal period with all the tools we provided previously
    and then take the seasonal period and difference the data on that.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For monthly data with an annual seasonal pattern, we can use the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y*''t =*y*t – *y*t–12'
  prefs: []
  type: TYPE_NORMAL
- en: 'For quarterly data, we can use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y*''t =*y*t – *y*t–4'
  prefs: []
  type: TYPE_NORMAL
- en: Here, is the seasonally differenced series and is the original series.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss how the difference can be used in the context of the stock
    price data example we’ve been working with.
  prefs: []
  type: TYPE_NORMAL
- en: Differencing the stock price data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To showcase the seasonal differencing, we will introduce some seasonality in
    the stock market data. Based on the analysis we’ve done so far, there is not a
    big seasonal component in the data. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a seasonal component (weekly seasonality with higher amplitude):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate random stock prices with added seasonality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the first difference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the second difference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, calculate the seasonal difference (weekly seasonality):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s demonstrate differencing by plotting the first, second, and seasonal
    differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.18 – Original versus differenced series](img/B19801_11_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – Original versus differenced series
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 11**.18*, we can observe the first, second, and seasonal differencing.
    We can see that in the original plot, there is some seasonality, but after the
    first difference, we can see that the seasonal component is minimized. But how
    can we evaluate this more statistically? Let’s perform some statistical tests
    to check for stationarity in the time series.
  prefs: []
  type: TYPE_NORMAL
- en: The Augmented Dickey-Fuller (ADF) test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **ADF** test is a statistical test that’s used to determine whether a time
    series is stationary or not. The ADF test examines the null hypothesis that a
    unit root is present in a time series sample. The presence of a unit root indicates
    that the time series is non-stationary. The alternative hypothesis is that the
    time series is stationary. For the ADF test, a more negative value indicates stronger
    evidence against the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: The p-value represents the probability of obtaining test results at least as
    extreme as the observed results, assuming that the null hypothesis is true. In
    the case of the ADF test, we want to see *a small p-value to reject the null hypothesis*
    *of non-stationarity*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude that a time series is stationary, we typically want to see the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**p-value < 0.05**: This is the most common threshold that’s used in statistical
    testing. If p < 0.05, we reject the null hypothesis at the 5% significance level.
    This means we have strong evidence to conclude the series is stationary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Even smaller p-values**: p < 0.01 (1% significance level) and p < 0.001 (0.1%
    significance level) provide even stronger evidence of stationarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s code up this test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it’s time for the results! We will run the test for the original time
    series (to check if it is stationary or not) and then for each of the differenced
    time series. Let’s explain the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The ADF statistic of -3.5899 is less than the 5% critical value of -2.8644,
    and the p-value is below 0.05\. This indicates that we can reject the null hypothesis
    of the presence of a unit root, suggesting that *the original series is likely
    stationary*. However, the result is relatively close to the critical value, indicating
    *borderline stationarity*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The ADF statistic of -11.7864 is well below the 5% critical value of -2.8644,
    and the p-value is extremely small. This strongly suggests that the first-differenced
    series is stationary. The significant drop in the ADF statistic compared to the
    original series indicates that first differencing has effectively removed any
    remaining trends or unit roots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The ADF statistic of -14.9569 is much lower than the 5% critical value, and
    the p-value is extremely small. This result suggests that the second-differenced
    series is also stationary. However, *over-differencing can lead to loss of meaningful
    patterns and increase noise*, so it’s essential to balance between achieving stationarity
    and maintaining the integrity of the series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the ADF statistic of -11.4833 is well below the 5% critical value,
    and the p-value is very small. This indicates that seasonal differencing has successfully
    made the series stationary. Seasonal differencing is particularly useful if the
    series exhibits periodic patterns at specific intervals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given these results, the first difference appears to be the most appropriate
    choice for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The original series is already stationary at the 1% level, but first differencing
    significantly improves stationarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first difference yields a highly significant result (p-value: 1.006e-21)
    without risking over-differencing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the second difference shows an even more significant result, it may lead
    to over-differencing, which can introduce unnecessary complexity and potentially
    remove important information from the series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seasonal differencing also shows strong results, but unless there’s a clear
    seasonal pattern in your data, the simpler first difference method is generally
    preferred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, first difference strikes a good balance between achieving stationarity
    and avoiding over-differencing. Now, it’s time to discuss some of the most common
    use cases in the time series space.
  prefs: []
  type: TYPE_NORMAL
- en: Applying time series techniques in different industries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ability to analyze temporal patterns provides a competitive advantage in
    today’s data-driven world across various industries. Here are some popular use
    cases across various industries:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sector** | **Use Case** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| Finance | Stock market analysis | Analyzing historical stock prices and trading
    volumes to make informed investment decisions |'
  prefs: []
  type: TYPE_TB
- en: '| Portfolio management | Assessing the performance of investment portfolios
    over time to optimize asset allocation |'
  prefs: []
  type: TYPE_TB
- en: '| Risk assessment | Modeling and forecasting financial risks such as market
    volatility and credit defaults |'
  prefs: []
  type: TYPE_TB
- en: '| Healthcare | Patient monitoring | Continuously tracking vital signs and health
    metrics for early detection of abnormalities |'
  prefs: []
  type: TYPE_TB
- en: '| Epidemiology | Analyzing temporal patterns of disease spread and predicting
    outbreaks |'
  prefs: []
  type: TYPE_TB
- en: '| Treatment effectiveness | Assessing the impact of medical interventions over
    time |'
  prefs: []
  type: TYPE_TB
- en: '| Meteorology | Weather forecasting | Analyzing historical weather patterns
    to predict future conditions |'
  prefs: []
  type: TYPE_TB
- en: '| Climate change studies | Monitoring long-term trends and variations in climate
    data |'
  prefs: []
  type: TYPE_TB
- en: '| Natural disaster prediction | Early detection of potential disasters such
    as hurricanes, floods, and droughts |'
  prefs: []
  type: TYPE_TB
- en: '| Manufacturing | Production planning | Forecasting demand and optimizing production
    schedules |'
  prefs: []
  type: TYPE_TB
- en: '| Quality control | Monitoring and ensuring product quality over time |'
  prefs: []
  type: TYPE_TB
- en: '| Equipment maintenance | Predictive maintenance based on the performance history
    of machinery |'
  prefs: []
  type: TYPE_TB
- en: '| Marketing | Sales forecasting | Predicting future sales based on historical
    data |'
  prefs: []
  type: TYPE_TB
- en: '| Customer engagement | Analyzing patterns of customer interaction with products
    and services |'
  prefs: []
  type: TYPE_TB
- en: '| Campaign optimization | Evaluating the impact of marketing initiatives over
    time |'
  prefs: []
  type: TYPE_TB
- en: '| **Sector** | **Use Case** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| Transportation | Traffic flow analysis | Monitoring and optimizing traffic
    patterns in urban areas |'
  prefs: []
  type: TYPE_TB
- en: '| Vehicle tracking | Tracking the movement and efficiency of transportation
    fleets |'
  prefs: []
  type: TYPE_TB
- en: '| Supply chain optimization | Forecasting demand and optimizing the movement
    of goods over time |'
  prefs: []
  type: TYPE_TB
- en: Table 11.3 – Time series techniques use cases
  prefs: []
  type: TYPE_NORMAL
- en: With that, we can summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series analysis plays a pivotal role in extracting meaningful insights
    and making informed decisions in a wide range of industries. As technology advances,
    sophisticated time series techniques will become increasingly integral to understanding
    complex temporal patterns and trends. Whether in finance, healthcare, or transportation,
    the ability to analyze and forecast time-dependent data empowers organizations
    to adapt, optimize, and make strategic decisions in an ever-evolving landscape.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered techniques for handling missing values and outliers,
    differencing methods, and feature engineering in time series analysis. We learned
    how to use ffill and bfill for missing values and compared their effects on stock
    price data. Differencing techniques, including first, second, and seasonal differencing,
    were applied to achieve stationarity and were evaluated using ADF tests. We also
    explored lagged features for capturing temporal dependencies and assessed model
    performance using metrics such as MAE, MSE, and RMSE. These skills will prepare
    you so that you can manage and analyze time series data effectively.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will pivot to a different type of data – text. Analyzing
    text data involves unique challenges and methodologies distinct from those used
    for numerical time series. We will deep dive into text preprocessing and cover
    text cleaning techniques, tokenization strategies, and spelling correction approaches,
    all of which are essential for any **natural language processing** (**NLP**) task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Downstream Data Cleaning – Consuming Unstructured Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part focuses on the challenges and techniques involved in processing unstructured
    data, such as text, images, and audio, in the context of modern machine learning,
    particularly **large language models** (**LLMs**). It provides a comprehensive
    overview of how to prepare unstructured data types for machine learning applications,
    ensuring that the data is properly preprocessed for analysis and model training.
    The chapters cover essential preprocessing methods for text, as well as image
    and audio data, offering readers the tools to work with more complex and varied
    datasets in today’s AI-driven landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B19801_12.xhtml#_idTextAnchor277)*, Text Preprocessing in the
    Era of LLMs*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B19801_13.xhtml#_idTextAnchor302)*, Image and Audio Preprocessing
    with LLMs*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
