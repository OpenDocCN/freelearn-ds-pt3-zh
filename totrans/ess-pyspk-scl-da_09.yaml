- en: 'Chapter 7: Supervised Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two chapters, you were introduced to the machine learning process,
    the various stages involved, and the first step of the process, namely **feature
    engineering**. Equipped with the fundamental knowledge of the machine learning
    process and with a usable set of machine learning features, you are ready to move
    on to the core part of the machine learning process, namely **model training**.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to the **supervised learning** category
    of machine learning algorithms, where you will learn about **parametric** and
    **non-parametric** algorithms, as well as gain the knowledge required to solve
    **regression** and **classification** problems using machine learning. Finally,
    you will implement a few regression algorithms using the Spark machine learning
    library, such as **linear regression** and **decision trees**, and a few classification
    algorithms such as **logistic regression**, **naïve Bayes**, and **support vector
    machines**. **Tree ensemble** methods will also be presented, which can improve
    the performance and accuracy of decision trees. A few real-world applications
    of both regression and classification will also be presented to help you gain
    an appreciation of how machine learning can be leveraged in some day-to-day scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following main topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to supervised machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world supervised learning applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toward the end of this chapter, you should have gained sufficient knowledge
    and the skills required for building your own regression and classification models
    at scale using Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be using Databricks Community Edition to run our code
    ([https://community.cloud.databricks.com](https://community.cloud.databricks.com)).
  prefs: []
  type: TYPE_NORMAL
- en: Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter07](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter07).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The datasets for this chapter can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to supervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A machine learning problem can be considered as a process where an unknown variable
    is derived from a set of known variables using a mathematical or statistical function.
    The difference here is that a machine learning algorithm learns the mapping function
    from a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning is a class of machine learning algorithms where a model
    is trained on a dataset and the outcome for each set of inputs is already known.
    This is known as supervised learning as the algorithm here behaves like a teacher,
    guiding the training process until the desired level of model performance is achieved.
    Supervised learning requires data that is already labeled. Supervised learning
    algorithms can be further classified as parametric and non-parametric algorithms.
    We will look at these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Parametric machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A machine learning algorithm that simplifies the learning process by summarizing
    the data with a fixed set of parameters is called a parametric learning algorithm.
    It achieves this by assuming a known form for the learning function and learning
    the coefficients of the linear function from the given dataset. The assumed form
    of the learning function is usually a linear function or an algebraic equation
    describing a straight line. Thus, parametric learning functions are also known
    as linear machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: One important property of parametric learning algorithms is that the number
    of parameters needed for the linear learning function is independent of the input
    training dataset. This greatly simplifies the learning process and makes it relatively
    faster. One disadvantage here is that the underlying learning function for the
    given dataset might not necessarily be a straight line, hence oversimplifying
    the learned model. However, most practical machine learning algorithms are parametric
    learning algorithms, such as linear regression, logistic regression, and naïve
    Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: Non-parametric machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Non-parametric learning algorithms do not make any assumptions regarding the
    form of the learning function. These algorithms make the best use of the training
    dataset by learning a mapping function, while still maintaining the ability to
    conform to unseen data. This means that non-parametric learning algorithms can
    learn from a wider variety of learning functions. The advantage of these algorithms
    is that that they are flexible and yield better performing models, while the disadvantages
    are that they usually require more data to learn, have relatively slow training
    times, and may sometimes lead to model overfitting. Some examples of non-parametric
    learning algorithms include K-nearest neighbors, decision trees, and support vector
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms have two major applications, namely regression
    and classification. We will explore these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regression is a supervised learning technique that helps us learn the correlation
    between a continuous output parameter called **Label** and a set of input parameters
    called **Features**. Regression produces machine learning models that predict
    a continuous label, given a feature vector. The concept of regression can be best
    explained using the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Linear regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_07_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – Linear regression
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the scatterplot represents data points spread across
    a two-dimensional space. The linear regression algorithm, being a parametric learning
    algorithm, assumes that the learning function will have a linear form. Thus, it
    learns the coefficients that are required to represent a straight line that approximately
    fits the data points on the scatterplot.
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib has distributed and scalable implementations of a few prominent
    regression algorithms, such as linear regression, decision trees, random forests,
    and gradient boosted trees. In the following sections, we will implement a few
    of these regression algorithms using Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapters, we cleaned, integrated, and curated a dataset containing
    online retail sales transactions by customers and captured their demographic information
    in the same integrated dataset. In [*Chapter 6*](B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107),
    *Feature Engineering – Extraction, Transformation, and Selection*, we also converted
    the pre-processed data into a feature vector that''s ready for machine learning
    training and stored it in **Delta Lake** as our offline **feature store**. Let''s
    make use of this feature-engineered dataset to train a regression algorithm that
    can find out the age of a customer by providing other features as parameters,
    as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding block of code, we have done the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we imported the `LinearRegression` algorithm from Spark MLlib.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The retail features were loaded from a Delta table and loaded into a Spark DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We only needed the feature vector and the label column for training a `LinearRegression`
    model, so we only selected these two columns in the training DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initialized a `LinearRegression` transformer by specifying the hyperparameters
    required by this algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we called the `fit` method on the training dataset to start the training
    process, which starts a Spark job behind the scenes that carries out the training
    task in a distributed manner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model has been successfully trained, we printed the model training
    summary, including the learned coefficients of the linear learning function and
    the intercept.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also displayed the model accuracy metrics, such as the RMSE and R-squared
    metrics. It is generally desirable to get a model with as lower an RMSE as possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, utilizing Spark MLlib's distributed implementation of linear regression,
    you can train a regression model in a distributed manner on a large dataset, without
    having to deal with any of the underlying complexities of distributed computing.
    The model can then be applied to a new set of data to generate predictions. Spark
    MLlib models can also be persisted to disk or a data lake using built-in methods
    and then reused later.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have trained a simple linear regression model using a parametric
    learning algorithm, let's look at using a non-parametric learning algorithm to
    solve the same regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: Regression using decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision trees are a popular form of non-parametric learning algorithm for solving
    both regression and classification machine learning problems. Decision trees are
    popular because they are easy to use, they can handle a wide variety of categorical
    as well as continuous features, and are also easy to interpret and explain.
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib's implementation of decision trees allows for distributed training
    by partitioning data by rows. Since non-parametric learning algorithms typically
    require large amounts of data, Spark's implementation of decision trees can scale
    to a very large number of rows, even millions or billions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train a decision tree to predict the age of a customer while using other
    online retail transactional features as input, as shown in the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we have done the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we imported the `DecisionTreeRegressor` Spark ML library, along with
    a utility method to help evaluate the accuracy of the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We loaded our feature vector dataset from Delta Lake into a Spark DataFrame
    and only selected the feature and label columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To be able to evaluate our model accuracy after the training process, we needed
    a dataset that wouldn't be used for training. Thus, we split our dataset into
    two sets for training and testing, respectively. We used 80% of the data for model
    training while preserving 20% for model evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initialized the `DecisionTreeRegressor` class with the required hyperparameters,
    resulting in a `Transformer` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We fit the `DecisionTreeRegressor` transformer to our training dataset, which
    resulted in a decision tree model estimator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We applied the model's `Estimator` object to the test dataset to produce actual
    predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This prediction DataFrame was then used along with the `RegressionEvaluator`
    utility method to derive the RMSE of the model, which can be used to evaluate
    the accuracy of the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By using Spark MLlib's built-in decision tree regression algorithms, we can
    train regressions models in a distributed fashion on very large amounts of data,
    in a very fast and efficient manner. One thing to note is that the RMSE value
    of both regression models is about the same. These models can be tuned further
    using model tuning techniques, which help improve their accuracy. You will learn
    more about model tuning in [*Chapter 9*](B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164),
    *Machine Learning Life Cycle Management*.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Classification is another type of supervised learning technique, where the
    task is to categorize a given dataset into different classes. Machine learning
    classifiers learn a mapping function from input parameters called **Features**
    that go to a discreet output parameter called **Label**. Here, the learning function
    tries to predict whether the label belongs to one of several known classes. The
    following diagram depicts the concept of classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Logistic regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_07_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – Logistic regression
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, a logistic regression algorithm is learning a mapping
    function that divides the data points in a two-dimensional space into two distinct
    classes. The learning algorithm learns the coefficients of a **Sigmoid function**,
    which classifies a set of input parameters into one of two possible classes. This
    type of classification can be split into two distinct classes. This is known as
    **binary classification** or **binomial classification**.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is a popular classification algorithm that can learn a model
    from labeled data to predict the class of an output variable. Spark MLlib's implementation
    of logistic regression supports both binomial and multinomial classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Binomial classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Binomial classification or binary classification is where the learning algorithm
    needs to classify whether the output variable belongs to one of two possible outcomes.
    Building on the example from previous sections, let''s train a model using logistic
    regression that tries to predict the gender of a customer, given other features
    from an online retail transaction. Let''s see how we can implement this using
    Spark MLlib, as shown in the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding block of code, we have done the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The gender in our training dataset is a string data type, so it needs to be
    converted into numeric format first. For this, we made use of `StringIndexer`
    to convert it into a numeric label column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initialized a `LogisticRegression` class by specifying the hyperparameters
    required by this algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we stitched the `StringIndexer` and `LogisticRegression` stages together
    into a pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we called the `fit` method on the training dataset to start the training
    process using the pipeline's `Transformer` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model had been successfully trained, we printed the model's coefficients
    and intercepts, along with the receiver-operating characteristic and the area
    under the ROC curve metric, to measure the accuracy of the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With that, we have seen how, using the logistic regression algorithm from the
    Spark machine learning library, to implement binary classification in a scalable
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In **multinomial classification**, the learning algorithm needs to predict
    more than two possible outcomes. Let''s extend the example from the previous section
    to build a model that, using logistic regression, tries to predict the country
    of origin of a customer, given other features from an online retail transaction,
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The previous code snippet is almost the same as the binary classification example,
    except the label column has more than two possible values and we specified the
    family parameter for the `LogisticRegression` class as `multinomial`. Once the
    model has been trained, the receiver-operating characteristics of the model and
    the area under the ROC curve metric can be displayed to measure the model's accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Classification using decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark MLlib comes with a `DecsionTreeClassifier` class to solve classification
    problems as well. In the following code, we will implement binary classification
    using decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous block of code, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we split our dataset into two sets for training and testing. This allows
    us to evaluate the model's accuracy once it has been trained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we made use of `StringIndexer` to convert the gender string column into
    a numeric label column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we initialized a `DecsionTreeClassifier` class with the required hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we combined the `StringIndexer` and `DecisionTreeClassifier` stages into
    a pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we called the `fit` method on the training dataset to start the model
    training process and applied the model's `Estimator` object on the test dataset
    to calculate predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we used this DataFrame, along with the `MulticlassClassificationEvaluator`
    utility method, to derive the accuracy of the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This way, we have seen how the Spark machine learning library's decision trees
    can be used to solve classification problems at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Naïve Bayes is a family of probabilistic classification algorithms based on
    the Bayes'' theorem, which assumes independence among features that are used as
    input to the learning algorithm. Bayes'' theorem can be used to predict the probability
    of an event happening, given that another event has already taken place. The naïve
    Bayes algorithm calculates the probability of the given set of input features
    for all possible values of the category of the output label, and then it picks
    the output with the maximum probability. Naïve Bayes can used for binomial as
    well as multinomial classification problems. Let''s see how naïve Bayes can be
    implemented using Spark MLlib, as shown in the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding block of code, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we split our dataset into two sets for training and testing, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we made use of `StringIndexer` to convert the gender string column into
    a numeric label column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we initialized a `NaiveBayes` class with the required hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we combined the `StringIndexer` and `NaiveBayes` stages into a pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we called the `fit` method on the training dataset to start the model
    training process, which applied the model's `Estimator` object to the test dataset
    to calculate predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This DataFrame was then used with the `MulticlassClassificationEvaluator` utility
    method to derive the accuracy of the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Multinomial and Bernoulli naïve Bayes models require non-negative features.
    Thus, it is recommended to select only features with positive values or to use
    another classification algorithm that can handle features with non-negative values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Support vector machines** (**SVM**) is a class of classification algorithms
    that takes data points as input and outputs the hyperplane that best separates
    the given data points into two separate classes, represented on a two-dimensional
    plane. Thus, SVM supports binomial classification problems only. Let''s implement
    binary classification using Spark MLlib''s implementation of SVM, as shown in
    the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we made use of `StringIndexer` to convert the gender column into a numeric
    label column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initialized a `LinearSVC` class by specifying the hyperparameters required
    by this algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we combined the `StringIndexer` and `LinearSVC` stages into a pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we called the `fit` method on the training dataset to start the training
    process using the pipeline's `Transformer` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model had been successfully trained, we printed the model coefficients
    and intercepts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So far, you have learned about the most popular supervised learning algorithms
    for solving regression and classification problems and saw their implementation
    details in Spark MLlib using working code examples. In the following section,
    you will be introduced to the concept of tree ensembles and how they can be used
    to combine multiple decision tree models to arrive at the best possible model.
  prefs: []
  type: TYPE_NORMAL
- en: Tree ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-parametric learning algorithms such as decision trees do not make any assumptions
    on the form of the learning function being learned and try to fit a model to the
    data at hand. However, decision trees run the risk of overfitting training data.
    Tree ensemble methods are a great way to leverage the benefits of decision trees
    while minimizing the risk of overfitting. Tree ensemble methods combine several
    decision trees to produce better-performing predictive models. Some popular tree
    ensemble methods include random forests and gradient boosted trees. We will explore
    how these ensemble methods can be used to build regression and classification
    models using Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: Regression using random forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Random forests build multiple decision trees and merge them to produce a more
    accurate model and reduce the risk of overfitting. Random forests can be used
    to train regression models, as shown in the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we split our dataset into two sets for training and testing, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initialized the `RandomForestRegressor` class with several trees to
    be trained. We set this to `5`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we fit the `RandomForestRegressor` transformer to our training dataset
    to get a random forest model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we applied the model's `Estimator` object to the test dataset to produce
    actual predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This DataFrame was then used with the `RegressionEvaluator` utility method to
    derive the `RMSE` value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we printed the trained random forest using the `toDebugString` attribute
    of the model object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classification using random forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like decision trees, random forests also support training multi-class
    classification models, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code snippet, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we split our dataset into two sets for training and testing, respectively.
    This will allow us to evaluate the model's accuracy once it has been trained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We made use of `StringIndexer` to convert the gender string column into a numeric
    label column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initialized a `RandomForestClassifier` class with the required hyperparameters
    and specified the number of decision trees to be trained as `5`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we joined the `StringIndexer` and `RandomForestClassifier` stages into
    a pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we called the `fit` method on the training dataset to start the model
    training process and applied the model's `Estimator` object to the test dataset
    to calculate predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This DataFrame was then used with the `MulticlassClassificationEvaluator` utility
    method to derive the accuracy of the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The random forest model can also be printed using the `toDebugString` attribute,
    which is available on the model object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This way, machine learning classification can be implemented at scale using
    the Spark machine learning library.
  prefs: []
  type: TYPE_NORMAL
- en: Regression using gradient boosted trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Gradient boosted trees** (**GBTs**) is another type of ensemble method based
    on decision trees that also improve the stability and accuracy of the trained
    model while minimizing the risk of overfitting. GBTs iteratively train several
    decision trees while minimizing a loss function through a process called gradient
    boosting. Let''s explore an example of training regression models using GBTs in
    Spark, as shown in the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we split our dataset into two sets for training and testing, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initialized the `GBTRegressor` class with the max iterations set to
    `5`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we fit the `RandomForestRegressor` transformer on our training dataset.
    This resulted in a random forest model estimator. After, we set the number of
    trees to be trained to `5`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we applied the model's `Estimator` object to the test dataset to produce
    actual predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This DataFrame was then with the `RegressionEvaluator` utility method to derive
    the `RMSE` value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The trained random forest can also be printed using the `toDebugString` attribute
    of the model object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This way, the GBTs algorithm from the Spark MLlib can be used to implement regression
    at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Classification using GBTs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GBTs can also be used to train classification models, as shown in the following
    code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code snippet, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we made use of `StringIndexer` to convert the gender string column into
    a numeric label column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initialized the `GBTClassifier` class and set the number of decision
    trees to be trained to `5`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we joined the `StringIndexer` and `RandomForestClassifier` stages into
    a pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we called the `fit` method on the training dataset to start the model
    training process and applied the model's `Estimator` object to the test dataset
    to calculate predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This DataFrame was then used with the `MulticlassClassificationEvaluator` utility
    method to derive the accuracy of the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So far, you have explored how to use tree ensemble methods to combine multiple
    decision trees to produce better and more accurate machine learning models for
    solving both regression and classification problems. In the following section,
    you will be introduced to some real-world applications of machine learning classification
    and regression models that can be applied to day-to-day scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world supervised learning applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past, data science and machine learning were used exclusively for academic
    research purposes. However, over the past decade, this field has found its use
    in actual business applications to help businesses find their competitive edge,
    improve overall business performance, and become profitable. In this section,
    we will look at some real-world applications of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Regression applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some of the applications of machine learning regression models and how they
    help improve business performance will be presented in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Customer lifetime value estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In any retail or CPG kind of business where customer churn is a huge factor,
    it is necessary to direct marketing spend at those customers who are profitable.
    In non-subscription kinds of businesses, typically 20% of the customer base generates
    up to 80% of revenue. Machine learning models can be leveraged to model and predict
    each customer's **lifetime value**. **Customer lifetime value** (**CLV**) models
    help predict an individual customer's **estimated lifetime**, which is an indicator
    of how much longer we can expect the customer to be profitable. CLV models can
    also be used to predict the amount of revenue that individual customers can be
    expected to generate over their expected lifetime. Thus, regression models can
    be used to estimate CLV and help direct marketing dollars toward promoting to
    and retaining profitable customers over their expected lifetimes.
  prefs: []
  type: TYPE_NORMAL
- en: Shipment lead time estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retailers, logistics firms, food service aggregators, or any businesses that
    are in the business of delivering products to customers need to be able to predict
    the amount of time it will take them to ship a product to a customer. Regression
    models can be used to take factors such as origin and destination ZIP codes, the
    past performance of shipments between these locations, inventory availability,
    and also seasonality, weather conditions, and even local traffic into consideration
    to build models that can estimate the amount of time it would take for the product
    to reach the customer. This helps the business with inventory optimization, supply
    chain planning, and even improving overall customer satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic price optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Dynamic price optimization**, also known as **dynamic pricing**, is where
    you set a price for a product or service based on current product demand or market
    conditions. It is a common practice in many industries, ranging from transportation
    to travel and hospitality, e-commerce, entertainment, and digital aggregators
    to perform dynamic price optimization. Businesses can take advantage of the massive
    amounts of data that is generated in the digital economy by adjusting prices in
    real time. Although dynamic pricing is an **optimization** problem, regression
    models can be used to predict the price at a given point in time, current demand,
    market conditions, and competitor pricing.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A few examples of how classification models can be used to solve business scenarios
    will be discussed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Financial fraud detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Financial fraud and identity theft are some of the biggest challenges facing
    the financial industry. Financial organizations have historically used statistical
    models and rules-based engines to detect financial fraud; however, fraudsters
    have managed to circumvent legacy fraud detection mechanisms using novel types
    of fraud. Classification models can be built using something rudimentary, such
    as naïve Bayes, or something much more robust, such as decision tree ensemble
    methods. These can be leveraged to keep up with emerging fraud patterns and flag
    financial transactions as fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: Email spam detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a common scenario that anyone using email must have witnessed; that
    is, getting unwanted and soliciting or sometimes outright offensive content via
    email. Classification models are being used by email providers to classify and
    flag emails as spam and exclude them from user inboxes.
  prefs: []
  type: TYPE_NORMAL
- en: Industrial machinery and equipment and failure prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Heavy industries such as oil and construction companies have already installed
    or started installing IoT devices on their heavy industrial equipment, which sends
    out a constant stream of telemetry and diagnostic data to backend servers. Classification
    models that have been trained on the swath of telemetry and diagnostic data can
    help predict machine failures, help industries prevent downtime, flag problematic
    ancillary part vendors, and even save huge costs by preventing massive machinery
    recalls.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classification models have always been part of high-end cameras that have built-in
    object tracking and autofocus functions. Modern-day mobile phone applications
    also leverage classification models to isolate the subject of the photograph from
    the background, as well as to identify and tag individuals in photographs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you were introduced to a class of machine learning algorithms
    called supervised learning algorithms, which can learn from well-labeled existing
    data. You explored the concepts of parametric and non-parametric learning algorithms
    and their pros and cons. Two major use cases of supervised learning algorithms
    called regression and classification were presented. Model training examples,
    along with code from Spark MLlib, were explored so that we could look at a few
    prominent types of regression and classification models. Tree ensemble methods,
    which improve the stability, accuracy, and performance of decision tree models
    by combining several models and preventing overfitting, were also presented.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you explored some real-world business applications of the various machine
    learning models presented in this chapter. We explained how supervised learning
    can be leveraged for business use cases, and working code samples were presented
    to help you train your models at scale using Spark MLlib and solve business problems
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore unsupervised machine learning algorithms,
    how they are different from supervised learning models, and their application
    in solving real-world business problems. We will also provide working code examples
    to showcase this.
  prefs: []
  type: TYPE_NORMAL
