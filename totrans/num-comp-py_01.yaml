- en: Journey from Statistics to Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent times, **machine learning** (**ML**) and data science have gained
    popularity like never before. This field is expected to grow exponentially in
    the coming years. First of all, what is machine learning? And why does someone
    need to take pains to understand the principles? Well, we have the answers for
    you. One simple example could be book recommendations in e-commerce websites when
    someone went to search for a particular book or any other product recommendations
    which were bought together to provide an idea to users which they might like.
    Sounds magic, right? In fact, utilizing machine learning can achieve much more
    than this.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is a branch of study in which a model can learn automatically
    from the experiences based on data without exclusively being modeled like in statistical
    models. Over a period and with more data, model predictions will become better.
  prefs: []
  type: TYPE_NORMAL
- en: In this first chapter, we will introduce the basic concepts which are necessary
    to understand statistical learning and create a foundation for full-time statisticians
    or software engineers who would like to understand the statistical workings behind
    the ML methods.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical terminology for model building and validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statistics is the branch of mathematics dealing with the collection, analysis,
    interpretation, presentation, and organization of numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Statistics are mainly classified into two subbranches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Descriptive statistics**: These are used to summarize data, such as the mean,
    standard deviation for continuous data types (such as age), whereas frequency
    and percentage are useful for categorical data (such as gender).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inferential statistics**: Many times, a collection of the entire data (also
    known as population in statistical methodology) is impossible, hence a subset
    of the data points is collected, also called a sample, and conclusions about the
    entire population will be drawn, which is known as inferential statistics. Inferences
    are drawn using hypothesis testing, the estimation of numerical characteristics,
    the correlation of relationships within data, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical modeling is applying statistics on data to find underlying hidden
    relationships by analyzing the significance of the variables.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning is the branch of computer science that utilizes past experience
    to learn from and use its knowledge to make future decisions. Machine learning
    is at the intersection of computer science, engineering, and statistics. The goal
    of machine learning is to generalize a detectable pattern or to create an unknown
    rule from given examples. An overview of machine learning landscape is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd3cfb9c-7c4e-41f8-90df-92e537e1d0b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Machine learning is broadly classified into three categories but nonetheless,
    based on the situation, these categories can be combined to achieve the desired
    results for particular applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: This is teaching machines to learn the relationship between
    other variables and a target variable, similar to the way in which a teacher provides
    feedback to students on their performance. The major segments within supervised
    learning are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification problem
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression problem
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: In unsupervised learning, algorithms learn by themselves
    without any supervision or without any target variable provided. It is a question
    of finding hidden patterns and relations in the given data. The categories in
    unsupervised learning are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: This allows the machine or agent to learn its behavior
    based on feedback from the environment. In reinforcement learning, the agent takes
    a series of decisive actions without supervision and, in the end, a reward will
    be given, either +1 or -1\. Based on the final payoff/reward, the agent reevaluates
    its paths. Reinforcement learning problems are closer to the artificial intelligence
    methodology rather than frequently used machine learning algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some cases, we initially perform unsupervised learning to reduce the dimensions
    followed by supervised learning when the number of variables is very high. Similarly,
    in some artificial intelligence applications, supervised learning combined with
    reinforcement learning could be utilized for solving a problem; an example is
    self-driving cars in which, initially, images are converted to some numeric format
    using supervised learning and combined with driving actions (left, forward, right,
    and backward).
  prefs: []
  type: TYPE_NORMAL
- en: Statistical fundamentals and terminology for model building and validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statistics itself is a vast subject on which a complete book could be written;
    however, here the attempt is to focus on key concepts that are very much necessary
    with respect to the machine learning perspective. In this section, a few fundamentals
    are covered and the remaining concepts will be covered in later chapters wherever
    it is necessary to understand the statistical equivalents of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Predictive analytics depends on one major assumption: that history repeats
    itself!'
  prefs: []
  type: TYPE_NORMAL
- en: By fitting a predictive model on historical data after validating key measures,
    the same model will be utilized for predicting future events based on the same
    explanatory variables that were significant on past data.
  prefs: []
  type: TYPE_NORMAL
- en: The first movers of statistical model implementers were the banking and pharmaceutical
    industries; over a period, analytics expanded to other industries as well.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical models are a class of mathematical models that are usually specified
    by mathematical equations that relate one or more variables to approximate reality.
    Assumptions embodied by statistical models describe a set of probability distributions,
    which distinguishes it from non-statistical, mathematical, or machine learning
    models
  prefs: []
  type: TYPE_NORMAL
- en: Statistical models always start with some underlying assumptions for which all
    the variables should hold, then the performance provided by the model is statistically
    significant. Hence, knowing the various bits and pieces involved in all building
    blocks provides a strong foundation for being a successful statistician.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section, we have described various fundamentals with relevant
    codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Population**: This is the totality, the complete list of observations, or
    all the data points about the subject under study.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample**: A sample is a subset of a population, usually a small portion of
    the population that is being analyzed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4f5a4037-baaf-46dc-abbc-7ca06fa0275e.png)'
  prefs: []
  type: TYPE_IMG
- en: Usually, it is expensive to perform an analysis of an entire population; hence,
    most statistical methods are about drawing conclusions about a population by analyzing
    a sample.
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameter versus statistic**: Any measure that is calculated on the population
    is a parameter, whereas on a sample it is called a **statistic**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean**: This is a simple arithmetic average, which is computed by taking
    the aggregated sum of values divided by a count of those values. The mean is sensitive
    to outliers in the data. An outlier is the value of a set or column that is highly
    deviant from the many other values in the same data; it usually has very high
    or low values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Median**: This is the midpoint of the data, and is calculated by either arranging
    it in ascending or descending order. If there are *N* observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mode**: This is the most repetitive data point in the data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2315b4ff-1753-4cb4-86da-5ad2f981ed0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Python code for the calculation of mean, median, and mode using a `numpy`
    array and the `stats` package is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff428ca6-bb72-432a-9a86-8bc6aa661437.png)'
  prefs: []
  type: TYPE_IMG
- en: We have used a NumPy array instead of a basic list as the data structure; the
    reason behind using this is the `scikit-learn` package built on top of NumPy array
    in which all statistical models and machine learning algorithms have been built
    on NumPy array itself. The `mode` function is not implemented in the `numpy` package,
    hence we have used SciPy's `stats` package. SciPy is also built on top of NumPy
    arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R code for descriptive statistics (mean, median, and mode) is given as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We have used the default `stats` package for R; however, the `mode` function
    was not built-in, hence we have written custom code for calculating the mode.
  prefs: []
  type: TYPE_NORMAL
- en: '**Measure of variation**: Dispersion is the variation in the data, and measures
    the inconsistencies in the value of variables in the data. Dispersion actually
    provides an idea about the spread rather than central values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Range**: This is the difference between the maximum and minimum of the value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variance**: This is the mean of squared deviations from the mean (*xi* =
    data points, *µ* = mean of the data, *N* = number of data points). The dimension
    of variance is the square of the actual values. The reason to use denominator
    *N-1* for a sample instead of *N* in the population is due the degree of freedom.
    *1* degree of freedom lost in a sample by the time of calculating variance is
    due to extraction of substitution of sample:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/665ab16b-6862-42cf-808b-a87f2ca40793.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Standard deviation**: This is the square root of variance. By applying the
    square root on variance, we measure the dispersion with respect to the original
    variable rather than square of the dimension:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7dde7512-4ce1-4567-b2d9-482b6808f36d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Quantiles**: These are simply identical fragments of the data. Quantiles
    cover percentiles, deciles, quartiles, and so on. These measures are calculated
    after arranging the data in ascending order:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Percentile**: This is nothing but the percentage of data points below the
    value of the original whole data. The median is the 50^(th) percentile, as the
    number of data points below the median is about 50 percent of the data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decile**: This is 10th percentile, which means the number of data points
    below the decile is 10 percent of the whole data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quartile**: This is one-fourth of the data, and also is the 25^(th) percentile.
    The first quartile is 25 percent of the data, the second quartile is 50 percent
    of the data, the third quartile is 75 percent of the data. The second quartile
    is also known as the median or 50^(th) percentile or 5^(th) decile.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interquartile range**: This is the difference between the third quartile
    and first quartile. It is effective in identifying outliers in data. The interquartile
    range describes the middle 50 percent of the data points.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/fb60b3ef-c652-4d4d-ae6a-d0af8bd62c50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Python code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2848dd9a-30f9-4873-80ea-e5fa6e2cd316.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The R code for dispersion (variance, standard deviation, range, quantiles,
    and IQR) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Hypothesis testing**: This is the process of making inferences about the
    overall population by conducting some statistical tests on a sample. Null and
    alternate hypotheses are ways to validate whether an assumption is statistically
    significant or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P-value**: The probability of obtaining a test statistic result is at least
    as extreme as the one that was actually observed, assuming that the null hypothesis
    is true (usually in modeling, against each independent variable, a p-value less
    than 0.05 is considered significant and greater than 0.05 is considered insignificant;
    nonetheless, these values and definitions may change with respect to context).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps involved in hypothesis testing are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Assume a null hypothesis (usually no difference, no significance, and so on;
    a null hypothesis always tries to assume that there is no anomaly pattern and
    is always homogeneous, and so on).
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect the sample.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate test statistics from the sample in order to verify whether the hypothesis
    is statistically significant or not.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide either to accept or reject the null hypothesis based on the test statistic.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Example of hypothesis testing**: A chocolate manufacturer who is also your
    friend claims that all chocolates produced from his factory weigh at least 1,000
    g and you have got a funny feeling that it might not be true; you both collected
    a sample of 30 chocolates and found that the average chocolate weight as 990 g
    with sample standard deviation as 12.5 g. Given the 0.05 significance level, can
    we reject the claim made by your friend?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The null hypothesis is that *µ0 ≥ 1000* (all chocolates weigh more than 1,000
    g).
  prefs: []
  type: TYPE_NORMAL
- en: 'Collected sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40b90cd1-fdcf-48bb-80e3-ccda20c33970.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate test statistic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e89db8ab-0abd-46df-aa50-039728e8e5b5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*t = (990 - 1000) / (12.5/sqrt(30)) = - 4.3818*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Critical t value from t tables = t0.05, 30 = 1.699 => - t0.05, 30 = -1.699*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P-value = 7.03 e-05*'
  prefs: []
  type: TYPE_NORMAL
- en: Test statistic is *-4.3818*, which is less than the critical value of *-1.699*.
    Hence, we can reject the null hypothesis (your friend's claim) that the mean weight
    of a chocolate is above 1,000 g.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, another way of deciding the claim is by using the p-value. A p-value
    less than *0.05* means both claimed values and distribution mean values are significantly
    different, hence we can reject the null hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebd0304a-1571-449d-b6fc-8545dedea197.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Python code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/aebd860b-f4d1-4fa4-bafe-b75f259358be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The R code for T-distribution is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Type I and II error**: Hypothesis testing is usually done on the samples
    rather than the entire population, due to the practical constraints of available
    resources to collect all the available data. However, performing inferences about
    the population from samples comes with its own costs, such as rejecting good results
    or accepting false results, not to mention separately, when increases in sample
    size lead to minimizing type I and II errors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type I error**: Rejecting a null hypothesis when it is true'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type II error**: Accepting a null hypothesis when it is false'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normal distribution**: This is very important in statistics because of the
    central limit theorem, which states that the population of all possible samples
    of size *n* from a population with mean *μ* and variance *σ2* approaches a normal
    distribution:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/55e41132-9bf5-4b7f-8ffe-e063b04aca3a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Example: Assume that the test scores of an entrance exam fit a normal distribution.
    Furthermore, the mean test score is *52* and the standard deviation is *16.3*.
    What is the percentage of students scoring *67* or more in the exam?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a880eec5-7560-4eba-8ecb-5db9e49fad60.jpg)![](img/181df08e-7eb1-452f-8863-a376b6d609f0.jpg)![](img/35c16a2d-06da-4cec-976f-32821be8de32.jpg)![](img/63b78d64-5416-47bc-aa66-dd469c152037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Python code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/134ae03f-464d-4b0c-8fe1-ffad35402bbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The R code for normal distribution is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Chi-square**: This test of independence is one of the most basic and common
    hypothesis tests in the statistical analysis of categorical data. Given two categorical
    random variables *X* and *Y*, the chi-square test of independence determines whether
    or not there exists a statistical dependence between them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The test is usually performed by calculating *χ2* from the data and *χ2* with
    (*m-1*, *n-1*) degrees from the table. A decision is made as to whether both variables
    are independent based on the actual value and table value, whichever is higher:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61bdd5ec-a242-4aaa-826b-b92bb925bb75.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Example: In the following table, calculate whether the smoking habit has an
    impact on exercise behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/758dda66-0370-42d3-8dc7-77d5b4cf377b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Python code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'While creating a table using the `crosstab` function, we will obtain both row
    and column totals field extra. However, in order to create the observed table,
    we need to extract the variables part and ignore the totals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `chi2_contingency` function in the stats package uses the observed table
    and subsequently calculates its expected table, followed by calculating the p-value
    in order to check whether two variables are dependent or not. If *p-value < 0.05*,
    there is a strong dependency between two variables, whereas if *p-value > 0.05*,
    there is no dependency between the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/81c1bb9d-6ac4-4958-a2ce-a0c1c14db08d.png)'
  prefs: []
  type: TYPE_IMG
- en: The p-value is `0.483`, which means there is no dependency between the smoking
    habit and exercise behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R code for chi-square is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**ANOVA**: Analyzing variance tests the hypothesis that the means of two or
    more populations are equal. ANOVAs assess the importance of one or more factors
    by comparing the response variable means at the different factor levels. The null
    hypothesis states that all population means are equal while the alternative hypothesis
    states that at least one is different.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: A fertilizer company developed three new types of universal fertilizers
    after research that can be utilized to grow any type of crop. In order to find
    out whether all three have a similar crop yield, they randomly chose six crop
    types in the study. In accordance with the randomized block design, each crop
    type will be tested with all three types of fertilizer separately. The following
    table represents the yield in g/m². At the 0.05 level of significance, test whether
    the mean yields for the three new types of fertilizers are all equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Fertilizer 1** | **Fertilizer 2** | **Fertilizer 3** |'
  prefs: []
  type: TYPE_TB
- en: '| 62 | 54 | 48 |'
  prefs: []
  type: TYPE_TB
- en: '| 62 | 56 | 62 |'
  prefs: []
  type: TYPE_TB
- en: '| 90 | 58 | 92 |'
  prefs: []
  type: TYPE_TB
- en: '| 42 | 36 | 96 |'
  prefs: []
  type: TYPE_TB
- en: '| 84 | 72 | 92 |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | 34 | 80 |'
  prefs: []
  type: TYPE_TB
- en: 'The Python code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculating one-way ANOVA using the `stats` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e1fd826b-4729-42e4-8f49-9586be23466a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Result: The p-value did come as less than 0.05, hence we can reject the null
    hypothesis that the mean crop yields of the fertilizers are equal. Fertilizers
    make a significant difference to crops.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The R code for ANOVA is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Confusion matrix**: This is the matrix of the actual versus the predicted.
    This concept is better explained with the example of cancer prediction using the
    model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/be777195-6394-4f75-9cc7-02fb8e685c51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some terms used in a confusion matrix are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives (TPs)**: True positives are cases when we predict the disease
    as yes when the patient actually does have the disease.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negatives (TNs)**: Cases when we predict the disease as no when the
    patient actually does not have the disease.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positives (FPs)**: When we predict the disease as yes when the patient
    actually does not have the disease. FPs are also considered to be type I errors.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negatives (FNs)**: When we predict the disease as no when the patient
    actually does have the disease. FNs are also considered to be type II errors.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision (P)**: When yes is predicted, how often is it correct?'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(TP/TP+FP)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall (R)/sensitivity/true positive rate**: Among the actual yeses, what
    fraction was predicted as yes?'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(TP/TP+FN)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**F1 score (F1)**: This is the harmonic mean of the precision and recall. Multiplying
    the constant of *2* scales the score to *1* when both precision and recall are
    *1*:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/052349b8-7696-4423-8d90-ba8360b45a48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Specificity**: Among the actual nos, what fraction was predicted as no? Also
    equivalent to *1- false positive rate*:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(TN/TN+FP)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Area under curve (ROC)**: Receiver operating characteristic curve is used
    to plot between **true positive rate** (**TPR**) and **false positive rate** (**FPR**),
    also known as a sensitivity and *1- specificity* graph:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a535dde5-2180-46dc-a48e-f8071ba4fa37.png)'
  prefs: []
  type: TYPE_IMG
- en: Area under curve is utilized for setting the threshold of cut-off probability
    to classify the predicted probability into various classes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation and performance window**: In statistical modeling, the model
    tries to predict the event in advance rather than at the moment, so that some
    buffer time will exist to work on corrective actions. For example, a question
    from a credit card company would be, for example, what is the probability that
    a particular customer will default in the coming 12-month period? So that I can
    call him and offer any discounts or develop my collection strategies accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to answer this question, a probability of default model (or behavioral
    scorecard in technical terms) needs to be developed by using independent variables
    from the past 24 months and a dependent variable from the next 12 months. After
    preparing data with *X* and *Y* variables, it will be split into 70 percent -
    30 percent as train and test data randomly; this method is called **in-time validation**
    as both train and test samples are from the same time period:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07d2d04a-8f13-4c95-b486-36baed358cfa.png)'
  prefs: []
  type: TYPE_IMG
- en: '**In-time and out-of-time validation**: In-time validation implies obtaining
    both a training and testing dataset from the same period of time, whereas out-of-time
    validation implies training and testing datasets drawn from different time periods.
    Usually, the model performs worse in out-of-time validation rather than in-time
    due to the obvious reason that the characteristics of the train and test datasets
    might differ.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**R-squared (coefficient of determination)**: This is the measure of the percentage
    of the response variable variation that is explained by a model. It also a measure
    of how well the model minimizes error compared with just utilizing the mean as
    an estimate. In some extreme cases, R-squared can have a value less than zero
    also, which means the predicted values from the model perform worse than just
    taking the simple mean as a prediction for all the observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b2793a7e-a64a-402b-8e71-7c3e214c21fe.jpg)![](img/0a9b0de7-29d5-4f21-9372-829ddfac3f3b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Adjusted R-squared**: The explanation of the adjusted R-squared statistic
    is almost the same as R-squared but it penalizes the R-squared value if extra
    variables without a strong correlation are included in the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/12581e1d-5f7c-4914-9bc0-590e0828ba2d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *R2* = sample R-squared value, *n* = sample size, *k* = number of predictors
    (or) variables.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusted R-squared value is the key metric in evaluating the quality of linear
    regressions. Any linear regression model having the value of *R2 adjusted >= 0.7*
    is considered as a good enough model to implement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: The R-squared value of a sample is *0.5,* with a sample size of *50*
    and the independent variables are *10* in number. Calculated adjusted R-squared:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/375aa180-1432-49e9-99b9-50601db2292c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Maximum likelihood estimate (MLE)**: This is estimating the parameter values
    of a statistical model (logistic regression, to be precise) by finding the parameter
    values that maximize the likelihood of making the observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Akaike information criteria (AIC)**: This is used in logistic regression,
    which is similar to the principle of adjusted R-square for linear regression.
    It measures the relative quality of a model for a given set of data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/69ac9b5d-5caa-45f1-9dcf-f7897ad171b1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *k* = number of predictors or variables
  prefs: []
  type: TYPE_NORMAL
- en: The idea of AIC is to penalize the objective function if extra variables without
    strong predictive abilities are included in the model. This is a kind of regularization
    in logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: '**Entropy**: This comes from information theory and is the measure of impurity
    in the data. If the sample is completely homogeneous, the entropy is zero and
    if the sample is equally divided, it has an entropy of *1*. In decision trees,
    the predictor with the most heterogeneousness will be considered nearest to the
    root node to classify given data into classes in a greedy mode. We will cover
    this topic in more depth in [Chapter 2](2fe96fbc-0c1d-437c-bad5-03dc13df3281.xhtml),
    *Tree-Based Machine Learning Models*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/909ab57f-71a9-4e0a-9f88-2d936cc536e3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *n* = number of classes. Entropy is maximal at the middle, with the value
    of *1* and minimal at the extremes as *0*. A low value of entropy is desirable
    as it will segregate classes better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2461a422-4f06-48e4-8075-7223267a0864.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Example: Given two types of coin in which the first one is a fair one (*1/2*
    head and *1/2* tail probabilities) and the other is a biased one (*1/3* head and
    *2/3* tail probabilities), calculate the entropy for both and justify which one
    is better with respect to modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54d91887-6f9b-4b17-9a61-55e9f37fe3ed.jpg)![](img/7b191a9d-a8da-47e8-98f8-1a5131f375ff.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From both values, the decision tree algorithm chooses the biased coin rather
    than the fair coin as an observation splitter due to the fact the value of entropy
    is less.
  prefs: []
  type: TYPE_NORMAL
- en: '**Information gain**: This is the expected reduction in entropy caused by partitioning
    the examples according to a given attribute. The idea is to start with mixed classes
    and to keep partitioning until each node reaches its observations of the purest
    class. At every stage, the variable with maximum information gain is chosen in
    greedy fashion:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Information gain = Entropy of parent - sum (weighted % * Entropy of child)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Weighted % = Number of observations in particular child / sum (observations
    in all child nodes)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gini**: Gini impurity is a measure of misclassification, which applies in
    a multiclass classifier context. Gini works almost the same as entropy, except
    Gini is faster to calculate:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f7e865a6-52ab-4fe2-84ac-7bc69f4affad.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *i* = number of classes. The similarity between Gini and entropy is shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be9e2a95-e9e1-4ea4-bcd2-d8bbf2e4c50e.png)'
  prefs: []
  type: TYPE_IMG
- en: Bias versus variance trade-off
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every model has both bias and variance error components in addition to white
    noise. Bias and variance are inversely related to each other; while trying to
    reduce one component, the other component of the model will increase. The true
    art lies in creating a good fit by balancing both. The ideal model will have both
    low bias and low variance.
  prefs: []
  type: TYPE_NORMAL
- en: Errors from the bias component come from erroneous assumptions in the underlying
    learning algorithm. High bias can cause an algorithm to miss the relevant relations
    between features and target outputs; this phenomenon causes an underfitting problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, errors from the variance component come from sensitivity
    to change in the fit of the model, even a small change in training data; high
    variance can cause an overfitting problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c7502ec-b60d-4891-abb8-d9f530bc61f2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An example of a high bias model is logistic or linear regression, in which the
    fit of the model is merely a straight line and may have a high error component
    due to the fact that a linear model could not approximate underlying data well.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a high variance model is a decision tree, in which the model may
    create a wiggly curve as a fit, in which even a small change in training data
    will cause a drastic change in the fit of the curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the moment, state-of-the-art models are utilizing high variance models such
    as decision trees and performing ensemble on top of them to reduce the errors
    caused by high variance and at the same time not compromising on increases in
    errors due to the bias component. The best example of this category is random
    forest, in which many decision trees will be grown independently and ensemble
    in order to come up with the best fit; we will cover this in upcoming chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1656ae24-5704-4e83-94dd-f0c7d83d6443.png)'
  prefs: []
  type: TYPE_IMG
- en: Train and test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In practice, data usually will be split randomly 70-30 or 80-20 into train
    and test datasets respectively in statistical modeling, in which training data
    utilized for building the model and its effectiveness will be checked on test
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/376bde10-df3d-45dc-afa8-3f27cd56fd5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following code, we split the original data into train and test data
    by 70 percent - 30 percent. An important point to consider here is that we set
    the seed values for random numbers in order to repeat the random sampling every
    time we create the same observations in training and testing data. Repeatability
    is very much needed in order to reproduce the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, `train size` is `0.7`, which means 70 percent of the
    data should be split into the training dataset and the remaining 30% should be
    in the testing dataset. Random state is seed in this process of generating pseudo-random
    numbers, which makes the results reproducible by splitting the exact same observations
    while running every time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The R code for the train and test split for statistical modeling is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have gained a high-level view of various basic building
    blocks and subcomponents involved in statistical modeling and machine learning,
    such as mean, variance, interquartile range, p-value, bias versus variance trade-off,
    AIC, Gini, the area under the curve, and so on with respect to the statistics
    context.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be covering complete tree-based models such as
    decision trees, random forest, boosted trees, ensemble of models, and so on to
    improve accuracy!
  prefs: []
  type: TYPE_NORMAL
