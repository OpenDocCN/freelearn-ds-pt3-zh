- en: '*Chapter 9*: Letting Your Data Speak for Itself with Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While making histograms we got a glimpse of a technique that visualizes aggregates,
    and not data points directly. In other words, we visualized data about our data.
    We will take this concept several steps further in this chapter, by using a machine
    learning technique to demonstrate some options that can be used to categorize
    or cluster our data. As you will see in this chapter, and even while using a single
    technique, there are numerous options and combinations of options that can be
    explored. This is where the value of interactive dashboards comes into play. It
    would be very tedious if users were to explore every single option by manually
    creating a chart for it.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is not an introduction to machine learning, nor does it assume
    any prior knowledge of it. We will explore a clustering technique called `sklearn`
    machine learning package. This will help us in grouping our data into clusters
    of observations that are similar to one another, and yet distinct from observations
    in other clusters. We will build a very simple model with a single-dimensional
    dataset, and then see how this can be applied to clustering countries in our `poverty`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with machine learning, then this chapter will hopefully
    give you some ideas of how to empower your users and allow them to tune and explore
    several aspects of the models used. If not, you should be able to complete the
    chapter, and it will hopefully inspire you to explore more machine learning concepts
    and techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will go through the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the optimal number of clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering countries by population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data with `scikit-learn`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an interactive K-Means clustering app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be exploring a few options from `sklearn`, as well as `NumPy`. Otherwise,
    we will be using the same tools we have been using. For visualization and building
    interactivity, Dash, JupyterDash, the Dash Core Component library, Dash HTML Components,
    Dash Bootstrap Components, Plotly, and Plotly Express will be used. For data manipulation
    and preparation, we will use `pandas` and `NumPy`. JupyterLab will be used for
    exploring and building independent functionality. Finally, `sklearn` will be used
    for building our machine learning models, as well as for preparing our data.
  prefs: []
  type: TYPE_NORMAL
- en: The code files of this chapter can be found on GitHub at [https://github.com/PacktPublishing/Interactive-Dashboards-and-Data-Apps-with-Plotly-and-Dash/tree/master/chapter_09](https://github.com/PacktPublishing/Interactive-Dashboards-and-Data-Apps-with-Plotly-and-Dash/tree/master/chapter_09).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action at [https://bit.ly/3x8PAmt](https://bit.ly/3x8PAmt).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, what exactly is clustering and when might it be helpful? Let''s start with
    a very simple example. Imagine you have a group of people for whom we want to
    make T-shirts. We can make a T-shirt for each one of them, in whatever size required.
    The main restriction is that we can only make one size. The sizes are as follows:
    [1, 2, 3, 4, 5, 7, 9, 11]. Think how you might tackle this problem. We will use
    the `KMeans` algorithm for that, so let''s start right away, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages and models. `NumPy` will be imported as a package,
    but from `sklearn` we will import the only model that we will be using for now,
    as illustrated in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a dataset of sizes in the required format. Note that each observation
    (person''s size) should be represented as a list, so we use the `reshape` method
    of `NumPy` arrays to get the data in the required format, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an instance of the `KMeans` model with the required number of clusters.
    An important feature of this model is that we provide the desired number of clusters
    for it. In this case, we were given a constraint, which is that we can only make
    T-shirts in one size, so we want to discover a single point that would be the
    center of our discovered cluster. We will explore the effect of the chosen number
    of clusters after that. Run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the data using the `fit` method. This means that we want the
    model we just created to "learn" the dataset based on this particular algorithm,
    and with the provided parameters/options. This is the code you will need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have a model that has been trained on this dataset, and we can go on
    to check some of its attributes. As a convention, the resulting attributes of
    the fitted models have a trailing underscore to them, as we will see now. We can
    now ask for the clusters that we asked for. The `cluster_centers_` attribute provides
    the answer to this. The centers (in this case, one center) are basically the means
    of the clusters of our data points. Let''s check the result, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We received the attribute in the form of a list of lists. The center of our
    cluster is `5.25`, apparently. You might be thinking that this is a convoluted
    way of calculating the mean of our dataset, and you would be right. Have a look
    at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Indeed, our cluster center happens to be the mean of our dataset, which is
    exactly what we asked for. To visualize this, the following screenshot shows the
    relative position of the cluster center, related to our data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – The Sizes data points, with the cluster center provided by KMeans](img/B16780_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – The Sizes data points, with the cluster center provided by KMeans
  prefs: []
  type: TYPE_NORMAL
- en: The chart depicted in the previous screenshot is very simple—we simply plot
    the sizes on the *x* axis, and an arbitrary constant value for the *y* axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to evaluate the performance of our model and how well it fits the
    data, there are several ways to do this—one way is by checking the `inertia_`
    attribute. This is an attribute of the instance we created and can be accessed
    after fitting it to the data, using dot notation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `inertia_` metric is the sum of squared distances of samples, to their closest
    cluster center. If the model performs well, the distances of the samples to the
    provided cluster centers should be as short as possible (data points are close
    to the cluster centers). A perfect model would have an inertia rate of `0`. Also,
    looking at it from the other side, we also know that asking for one cluster would
    give us the worst possible outcome because it is just one cluster, and to be the
    average point it has to be very far from the extreme data points.
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, we can improve the performance of the model simply by adding more
    clusters, because their distances to the centers would be reduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, imagine that I called you and shared some good news. We have an additional
    budget for one more size, and now we want to make T-shirts in two sizes. Translating
    to machine learning language, this means we need to create a new model with two
    clusters. We repeat the same steps and modify the `n_clusters` argument, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We now have two new centers, as specified.
  prefs: []
  type: TYPE_NORMAL
- en: It's not enough to know the centers of our clusters. For each point, we need
    to know which cluster it belongs to, or we want to know the size of T-shirt that
    we will give to each person in our group. We can also count them and check the
    number of points in each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `labels_` attribute contains this information, and can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that the labels are given using integers starting at `0`. Also note that
    the numbers don't mean anything quantitative. Points that have a zero label are
    not from the first cluster; nor are points with the label 1 "more" than the others
    in any way. These are just labels, such as calling them `group A`, `group B`,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can map the labels to their respective values by using the `zip` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This will be very important later when use those labels in our charts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also visualize the two centers to get a better idea of how this works.
    The following screenshot shows where the cluster centers are located relative
    to other data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 9.2 – The Sizes data points, with two cluster centers provided by
    KMeans](img/B16780_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – The Sizes data points, with two cluster centers provided by KMeans
  prefs: []
  type: TYPE_NORMAL
- en: 'The centers make sense visually. We can see that the first five points are
    close to each other and that the last three are distinct and far from them, with
    larger gaps. Having 3 and 9 as cluster centers makes sense, because each of them
    is the mean of the values of its own cluster. Let''s now numerically validate
    that we have improved the performance of our model by checking the inertia rate,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, the performance was tremendously improved and reduced from 85.5 to 18.0\.
    Nothing surprising here. As you can expect, every additional cluster would improve
    the performance until we reach the perfect result with an inertia rate of `0`.
    So, how do we evaluate the best options available for the number of clusters?
  prefs: []
  type: TYPE_NORMAL
- en: Finding the optimal number of clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now see the options we have in choosing the optimal number of clusters
    and what that entails, but let''s first take a look at the following screenshot
    to visualize how things progress from having one cluster to eight clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Data points and cluster centers for all possible cluster numbers](img/B16780_09_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Data points and cluster centers for all possible cluster numbers
  prefs: []
  type: TYPE_NORMAL
- en: We can see the full spectrum of possible clusters and how they relate to data
    points. At the end, when we specified 8, we got the perfect solution, where every
    data point is a cluster center.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, you might not want to go for the full solution, for two main reasons.
    Firstly, it is probably going to be prohibitive from a cost perspective. Imagine
    making 1,000 T-shirts with a few hundred sizes. Secondly, in practical situations,
    it usually wouldn't add much value to add more clusters after a certain fit has
    been achieved. Using our T-shirt example, imagine if we have two people with sizes
    5.3 and 5.27\. They would most likely wear the same size anyway.
  prefs: []
  type: TYPE_NORMAL
- en: So, we know that the optimal number of clusters is between one and the number
    of unique data points we have. We now want to explore the trade-offs and options
    of figuring out that optimal number. One of the strategies we can use is to check
    the value of additional—or incremental—clusters. When adding a new cluster, does
    it result in a meaningful reduction (improvement) in inertia? One such technique
    is called the "elbow technique." We plot the inertia values against the number
    of clusters and see where there is a sharp change in the direction of the curve.
    Let's do this now.
  prefs: []
  type: TYPE_NORMAL
- en: 'We loop through the numbers from 1 to 8, and for each number we go through
    the same process of instantiating a `KMeans` object and getting the inertia for
    that number of clusters. We then add that value to our `inertia` list, as illustrated
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As expected, our inertia improved from 85.5 to a perfect zero at the end.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now plot those to see where the elbow lies, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code produces the chart shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – The "elbow" method, showing inertia values for all possible
    cluster numbers](img/B16780_09_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – The "elbow" method, showing inertia values for all possible cluster
    numbers
  prefs: []
  type: TYPE_NORMAL
- en: 'You can clearly see a sudden drop when moving from 1 to 2 and that inertia
    keeps decreasing, but at a lower rate as we move toward the final value. So, three
    or maybe four clusters might be the point where we start to get diminishing returns,
    and that could be our optimal number of clusters. We also cheated a little by
    including one cluster, because we already knew that it would be the worst-performing
    number of clusters. You can see the same plot without the first value in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – The "elbow" method, showing inertia values for all possible
    cluster numbers excluding 1](img/B16780_09_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – The "elbow" method, showing inertia values for all possible cluster
    numbers excluding 1
  prefs: []
  type: TYPE_NORMAL
- en: This looks quite different and also shows that we cannot mechanically make a
    decision without knowing more about the data, the use case, and whatever constraints
    we might have.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example we explored was extremely simple in terms of the number of observations,
    as well as the number of dimensions, which was one dimension. `KMeans` clustering
    (and machine learning in general) usually handles multiple dimensions, and the
    concept is basically the same: we try to find centers of clusters that minimize
    the distance between them and the data points. For example, the following screenshot
    shows what a similar problem might look like in two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Clustered points in two dimensions](img/B16780_09_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Clustered points in two dimensions
  prefs: []
  type: TYPE_NORMAL
- en: This could correspond to additional measurements relating to our group of people.
    So, we might have their height on the *x* axis and their weight on the *y* axis,
    for example. You can imagine what `KMeans` would give us in this case. Of course,
    in real life, data is rarely so neatly clustered. You can also see how much accuracy
    we might lose by selecting the wrong number of clusters. If we specify three clusters,
    for example, the three blobs in the middle would probably be considered a single
    cluster, even though we can see that they are quite distinct from one another
    and that their points are quite close to each other. Also, if we specify seven
    or eight clusters, we could get unnecessary divisions between clusters, or we
    would have passed the elbow on the elbow chart in this case.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to use this understanding of clustering in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering countries by population
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will first understand this with one indicator that we are familiar with (population),
    and then make it interactive. We will cluster groups of countries based on their
    population.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with a possible practical situation. Imagine you were asked to group
    countries by population. You are supposed to have two groups of countries, of
    high and low populations. How do you do that? Where do you draw the line(s), and
    what does the total of the population have to be in order for it to qualify as
    "high"? Imagine that you were then asked to group countries into three or four
    groups based on their population. How would you update your clusters?
  prefs: []
  type: TYPE_NORMAL
- en: We can easily see how `KMeans` clustering is ideal for that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now do the same exercise with `KMeans` using one dimension, and then
    combine that with our knowledge of mapping, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and open the `poverty` dataset, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create variables for the year and desired indicators, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `KMeans` object with the desired number of clusters, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `df` object, which is the `poverty` DataFrame containing only countries
    and data from the selected year only. Run the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `data` object, which is a list of columns that we choose (in this
    case, we only chose one). Note in the following code snippet that we get its `values`
    attribute, which returns the underlying `NumPy` array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have now trained the model on our data and are ready to visualize the results.
    Remember our discussion in [*Chapter 7*](B16780_07_Final_NM_ePub.xhtml#_idTextAnchor106),
    *Exploring Map Plots and Enriching Your Dashboards with Markdown*, that in order
    to create a map, we simply need a DataFrame with a column containing country names
    (or codes)? This is enough to produce a map. If we want to color our countries,
    we need another column (or any list-like object) containing corresponding values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kmeans` object we just trained contains the labels of the countries and
    would tell us which country belongs to which cluster. We will use that to color
    countries, so we do this with one function call. Note that we can convert the
    labels to strings, which would cause Plotly Express to treat them as categorical
    and not continuous variables. The code is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This code produces the chart shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Countries clustered by population](img/B16780_09_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Countries clustered by population
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have already developed a template of options for maps, we can copy
    this and use it to enhance this map and make it consistent with the theme of our
    app. Let''s use that and see the effect of having **1**, **2**, **3**, and **4**
    clusters on the same map, and discuss the details. The following screenshot shows
    four maps, each with a different number of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Countries clustered by population using different numbers of
    clusters](img/B16780_09_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Countries clustered by population using different numbers of clusters
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The colors may not be easy to distinguish on these maps if you are reading the
    grayscale version, and I encourage you to check out the online version and repository.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, coloring the map with one cluster (one label for all countries)
    produces a map with a single color. Things start to get interesting when there
    are two clusters involved, and this also makes intuitive sense. The two countries
    forming the cluster with a higher population (namely, China and India) have very
    large populations that are also close to each other—1.39 billion and 1.35 billion,
    in this case. The third country, the **United States** (**US**), has a population
    of 327 million. This is exactly what KMeans is supposed to do. It gave us two
    groups of countries where countries in each cluster are very close to each other
    and far from countries in the other cluster. Of course, we introduced an important
    bias by selecting two as the number of clusters, and we saw how that might not
    be the optimal case.
  prefs: []
  type: TYPE_NORMAL
- en: When we chose three clusters, we can see that we have a medium-population cluster,
    with the US being one of them. Then, when we chose four, you can see that Russia
    and Japan were moved to the third category, even though they were in the second
    category when we had three clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We now have enough code and knowledge to take this to the next level. We want
    to give our users the option to select the number of clusters and indicator(s)
    that they want. We need to address some issues in our data first, so let's explore
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`scikit-learn` is one of the most widely used and comprehensive machine learning
    libraries in Python. It plays very well with the rest of the data-science ecosystem
    libraries, such as `NumPy`, `pandas`, and `matplotlib`. We will be using it for
    modeling our data and for some preprocessing as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have two issues that we need to tackle first: missing values and scaling
    data. Let''s see two simple examples for each, and then tackle them in our dataset.
    Let''s start with missing values.'
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Models need data, and they can't know what to do with a set of numbers containing
    missing values. In such cases (and there are many in our dataset), we need to
    make a decision on what to do with those missing values.
  prefs: []
  type: TYPE_NORMAL
- en: There are several options, and the right choice depends on the application as
    well as the nature of the data, but we won't get into those details. For simplicity,
    we will make a generic choice of replacing missing data with suitable values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore how we can impute missing values with a simple example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a simple dataset containing a missing value, in a suitable format, as
    illustrated in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `SimpleImputer` for `scikit-learn`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an instance of this class with a `mean` strategy, which is the default.
    As you might have guessed, there are other strategies for imputing missing values.
    The code is shown in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the data. This is where the model learns the data, given the
    conditions and options we set while instantiating it. The code is shown in the
    following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the data. Now that the model has learned the data, it is able to
    transform it according to the rules that we set. The `transform` method exists
    in many models and has a different meaning, depending on the context. In this
    case, transforming means imputing the missing data, using the `mean` strategy.
    The code can be seen in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the model has transformed the data by replacing the missing
    value with 1.5\. If you look at the other non-missing values [`1`, `2`, `1`, `2`],
    you can easily see that their mean is 1.5, and this is the result that we got.
    We could have specified a different strategy for imputing missing values, such
    as the median or the most frequent strategy. Each has its own advantages and disadvantages;
    we are simply exploring what can be done with machine learning in Dash.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we move on to scaling our data.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling data with scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Figure 9.6*, we saw how clustering might look in two dimensions. If we want
    to cluster our poverty data by two indicators, one of them would be on the *x*
    axis and the other would be on the *y* axis. Now, imagine if we had a population
    on one of the axes and a percentage indicator on the other axis. The data on the
    population axis would range from 0 to 1.4 billion, and the data on the other axis
    would range from 0 to 1 (or from 0 to 100). Any differences in the percentage
    indicator would have negligible influence on the distances, and the means would
    mainly be calculated using the disproportionate size of the population numbers.
    One solution to this problem is to scale values.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different strategies for scaling data and we will explore one of
    them—namely, standard scaling. The `StandardScaler` class assigns `z`-scores (or
    standard scores) to the data points and normalizes them. The z-score is simply
    calculated by subtracting each value from the mean and dividing by the standard
    deviation. There are other ways of calculating this but we will focus on a simple
    example to better illustrate this concept, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a simple dataset, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `StandardScaler` and create an instance of it, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit `scaler` to the data and transform it. For convenience, many models that
    have a `fit` and a `transform` method and also have a `fit_transform` method,
    which we will use now, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have now transformed our numbers to their equivalent z-scores. Note that
    the mean value 3 has now become 0\. Anything higher than 3 is positive, and anything
    lower is negative. The numbers also indicate how far (high or low) the corresponding
    value is from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: This way, when we have multiple features in our dataset, we can normalize them,
    compare them, and use them together. At the end of the day, what we care about
    is how extreme a certain value is and how close it is to the mean. A country with
    a Gini index of 90 is a very extreme case. It's as extreme as a country with a
    population of 1 billion. If we use those two together, the 1 billion will dominate
    and distort the calculation. Standardization helps us achieve a better way of
    dealing with data at different scales. It's still not perfect but is a very big
    improvement over using data in different scales. Now, we are able to use more
    than one feature while clustering our data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an interactive KMeans clustering app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now put everything together and make an interactive clustering application
    using our dataset. We will give users the option to choose the year, as well as
    the indicator(s) that they want. They can also select the number of clusters and
    get a visual representation of those clusters, in the form of a colored choropleth
    map, based on the discovered clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that it is challenging to interpret such results with multiple indicators
    because we will be handling more than one dimension. It can also be difficult
    if you are not an economist and don't know which indicators make sense to be checked
    with which other indicators, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows what we will be working toward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – An interactive KMeans clustering application](img/B16780_09_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – An interactive KMeans clustering application
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this is a fairly rich application in terms of the combinations
    of options that it provides. As I also mentioned, it's not straightforward to
    interpret, but as mentioned several times in the chapter, we are simply exploring
    what can be done with only one technique and using only some of its options.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have created many sliders and dropdowns so far in the book, so we won''t
    go into how to make them. We will simply make sure we have descriptive IDs for
    them, and I''ll leave it to you to fill in the blanks. As shown in the preceding
    screenshot, we have two sliders, one dropdown, and one graph component, so let''s
    set ID names for them. As usual, the following components should go wherever you
    want them to be in `app.layout`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now go through creating our callback function step by step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Associate the inputs and outputs in the callback, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the signature of the function with suitable parameter names, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a missing values imputer, a standard scaler, and a `KMeans` object.
    Note that with `SimpleImputer`, we also specify how missing values are encoded.
    In this case, they are encoded as `np.nan`, but in other cases they might be encoded
    differently, such as `N/A`, `0`, `-1`, or others. The code is shown in the following
    snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create `df`, a subset of `poverty` that has only country''s data and data from
    the selected year, and then select the `year` and `Country Name` columns, and
    the selected indicators. The code is shown in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create `data`, a subset of `df` that only contains the selected indicators''
    columns. The reason we have two distinct objects is that `df` is going to be used
    for plotting the map and will also use the year and country name. At the same
    time, `data` will only contain numeric values in order for our models to work
    with it. The code can be seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In some cases, as we saw several times in the book, we might come across a
    situation where we have a column that is completely empty. In this case, we can''t
    impute any missing values because we don''t have a mean, and we have absolutely
    no clue what to do with it. In this case, I think it''s best to not produce a
    chart and inform the user that for the selected combination of options, there
    is not enough data to run the models. We first check if we have such a situation.
    DataFrame objects have an `isna` method. When we run it, it returns the same DataFrame
    filled with `True` and `False` values, indicating whether or not the respective
    value is missing. We then run the `all` method on the resulting DataFrame. This
    will tell us if all values are missing for each column. Now, we have a pandas
    Series with `True` and `False` values. We check if any of them is `True` by using
    the `any` method. In this case, we create an empty chart with an informative title,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If everything went fine, and we don''t have a column with all its values missing,
    we proceed by creating a variable that has no missing values (imputed if some
    are missing), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we scale `data_no_na` using our instance of `StandardScaler`, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we fit the model to our scaled data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have everything we need to produce our chart—most importantly, the `labels_`
    attribute—and we can do so with a single call to `px.choropleth`. There is nothing
    new in the options we use in this function, as you can observe in the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After that, we copy the geographic attributes we already used to customize the
    map and make it consistent with the app as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the full function, including the geographic options for your reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We have made a big jump in this chapter in what can be visualized and interactively
    explored. We also got a minimal introduction to a single machine learning technique
    to cluster our data. Ideally, the options provided to your users will depend on
    the discipline you are working in. You might be an expert yourself in the domain
    you are dealing with, or you might work closely with such an expert. It is not
    only a matter of visualization and statistics, but domain knowledge is also a
    crucial aspect of analyzing data, and with machine learning this is critical.
  prefs: []
  type: TYPE_NORMAL
- en: I encourage you to learn more and see what you can achieve. Having the skills
    to create interactive dashboards is a big advantage for running machine learning
    models, as we saw, and allows you to discover trends and make decisions at a much
    faster rate. Eventually, you will be able to create automated solutions that provide
    recommendations, or make certain decisions based on certain inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now recap on what we learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We first got an idea of how clustering works. We built the simplest possible
    model for a tiny dataset. We ran the model a few times and evaluated the performance
    and outcomes for each of the numbers of clusters that we chose.
  prefs: []
  type: TYPE_NORMAL
- en: We then explored the elbow technique to evaluate different clusters and saw
    how we might discover the point of diminishing returns, where not much improvement
    is achieved by adding new clusters. With that knowledge, we used the same technique
    for clustering countries by a metric with which most of us are familiar and got
    firsthand experience in how it might work on real data.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we planned an interactive KMeans app and explored two techniques
    for preparing data before running our model. We mainly explored imputing missing
    values and scaling data.
  prefs: []
  type: TYPE_NORMAL
- en: This gave us enough knowledge to get our data in a suitable format for us to
    create our interactive app, which we did at the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We next explored advanced features of Dash callbacks—most notably, pattern-matching
    callbacks. The callbacks we have run so far have been straightforward and fixed.
    Many times, we want to create more dynamic interfaces for our users. For example,
    based on the selection of a certain value in a dropdown, we might want to display
    a special type of chart or create another dropdown. We will explore how this works
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
