<html><head></head><body>
		<div id="_idContainer117">
			<h1 id="_idParaDest-240" class="chapter-number"><a id="_idTextAnchor277"/><span class="koboSpan" id="kobo.1.1">12</span></h1>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor278"/><span class="koboSpan" id="kobo.2.1">Text Preprocessing in the Era of LLMs</span></h1>
			<p><span class="koboSpan" id="kobo.3.1">In the era of </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">Large Language Models</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">LLMs</span></strong><span class="koboSpan" id="kobo.7.1">), mastering </span><a id="_idIndexMarker942"/><span class="koboSpan" id="kobo.8.1">text preprocessing is more crucial than ever. </span><span class="koboSpan" id="kobo.8.2">As LLMs grow in complexity and capability, the foundation of successful </span><strong class="bold"><span class="koboSpan" id="kobo.9.1">Natural Language Processing</span></strong><span class="koboSpan" id="kobo.10.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.11.1">NLP</span></strong><span class="koboSpan" id="kobo.12.1">) tasks still lies in how well the text data is prepared. </span><span class="koboSpan" id="kobo.12.2">In this chapter, we will discuss text preprocessing, the foundation for any NLP Task. </span><span class="koboSpan" id="kobo.12.3">We will also explore essential preprocessing techniques, focusing on adapting them to maximize the potential </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">of LLMs.</span></span></p>
			<p><span class="koboSpan" id="kobo.14.1">In this chapter, we’ll cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">following topics:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.16.1">Relearning text preprocessing in the era </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">of LLMs</span></span></li>
				<li><span class="koboSpan" id="kobo.18.1">Text </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">cleaning techniques</span></span></li>
				<li><span class="koboSpan" id="kobo.20.1">Handling rare words and </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">spelling variations</span></span></li>
				<li><span class="No-Break"><span class="koboSpan" id="kobo.22.1">Chunking</span></span></li>
				<li><span class="No-Break"><span class="koboSpan" id="kobo.23.1">Tokenization strategies</span></span></li>
				<li><span class="koboSpan" id="kobo.24.1">Turning tokens </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">into embeddings</span></span></li>
			</ul>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor279"/><span class="koboSpan" id="kobo.26.1">Technical requirements</span></h1>
			<p><span class="koboSpan" id="kobo.27.1">The complete code for this chapter can be found in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">GitHub repository:</span></span></p>
			<p><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter12"><span class="No-Break"><span class="koboSpan" id="kobo.29.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter12</span></span></a><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-Best-Practices/tree/main/chapter12 "/></p>
			<p><span class="koboSpan" id="kobo.30.1">Let's install the necessary libraries we will use in </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">this chapter:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.32.1">
pip install transformers==4.42.4
pip install beautifulsoup4==4.12.3
pip install langchain-text-splitters==0.2.2
pip install tiktoken==0.7.0
pip install langchain==0.2.10
pip install langchain-experimental==0.0.62
pip install langchain-huggingface==0.0.3
pip install presidio_analyzer==2.2.355
pip install presidio_anonymizer==2.2.355
pip install rapidfuzz-3.9.4 thefuzz-0.22.1
pip install stanza==1.8.2
pip install tf-keras-2.17.0</span></pre>			<h1 id="_idParaDest-243"><a id="_idTextAnchor280"/><span class="koboSpan" id="kobo.33.1">Relearning text preprocessing in the era of LLMs</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.34.1">Text preprocessing</span></strong><span class="koboSpan" id="kobo.35.1"> involves </span><a id="_idIndexMarker943"/><span class="koboSpan" id="kobo.36.1">the application of various techniques to raw textual data with the aim of cleaning, organizing, and transforming it into a format </span><a id="_idIndexMarker944"/><span class="koboSpan" id="kobo.37.1">suitable for analysis or modeling. </span><span class="koboSpan" id="kobo.37.2">The primary goal is to enhance the quality of the data by addressing common challenges associated with unstructured text. </span><span class="koboSpan" id="kobo.37.3">This entails tasks such as cleaning irrelevant characters, handling variations, and preparing the data for downstream </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">NLP tasks.</span></span></p>
			<p><span class="koboSpan" id="kobo.39.1">With the rapid advancements in LLMs, the landscape of NLP has evolved significantly. </span><span class="koboSpan" id="kobo.39.2">However, fundamental preprocessing techniques such as text cleaning and tokenization remain crucial, albeit with some shifts in approach </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">and importance.</span></span></p>
			<p><span class="koboSpan" id="kobo.41.1">Staring with text cleaning, while LLMs have shown remarkable robustness to noise in input text, clean data still yields better results and is especially important for fine-tuning tasks. </span><span class="koboSpan" id="kobo.41.2">Basic cleaning techniques such as removing HTML tags, handling special characters, and normalizing text are still relevant. </span><span class="koboSpan" id="kobo.41.3">However, more advanced techniques such as spelling correction may be less critical for LLMs, as they can often handle minor spelling errors. </span><span class="koboSpan" id="kobo.41.4">Domain-specific cleaning remains important, especially when dealing with specialized vocabulary </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">or jargon.</span></span></p>
			<p><span class="koboSpan" id="kobo.43.1">Tokenization has evolved with the advent of subword tokenization methods used by most modern LLMs such </span><a id="_idIndexMarker945"/><span class="koboSpan" id="kobo.44.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.45.1">Byte-Pair Encoding</span></strong><span class="koboSpan" id="kobo.46.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.47.1">BPE</span></strong><span class="koboSpan" id="kobo.48.1">) or WordPiece. </span><span class="koboSpan" id="kobo.48.2">Traditional word-level tokenization is less common in LLM contexts. </span><span class="koboSpan" id="kobo.48.3">Some traditional NLP preprocessing steps such as stopword removal, stemming, and lemmatization have become less critical. </span><span class="koboSpan" id="kobo.48.4">Stopword removal, which involves eliminating common words such as “and” or “the,” is less necessary because LLMs can </span><a id="_idIndexMarker946"/><span class="koboSpan" id="kobo.49.1">understand their contextual importance and how they contribute to the meaning of a sentence. </span><span class="koboSpan" id="kobo.49.2">Similarly, stemming and lemmatization, which reduce words to their base forms (e.g., “running” to “run”), are less frequently used because LLMs can interpret different word forms accurately and understand their relationships within the text. </span><span class="koboSpan" id="kobo.49.3">This shift allows for a more nuanced understanding of language, capturing subtleties that rigid preprocessing </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">might miss.</span></span></p>
			<p><span class="koboSpan" id="kobo.51.1">The key message is that while LLMs can handle raw text impressively, preprocessing remains crucial in certain scenarios as it can improve model performance on specific tasks. </span><span class="koboSpan" id="kobo.51.2">Remember: </span><strong class="bold"><span class="koboSpan" id="kobo.52.1">garbage in, garbage out</span></strong><span class="koboSpan" id="kobo.53.1">. </span><span class="koboSpan" id="kobo.53.2">Cleaning and standardizing text can also reduce the number of tokens processed by an LLM, potentially lowering computational costs. </span><span class="koboSpan" id="kobo.53.3">New approaches are emerging that blend traditional preprocessing with LLM capabilities, using LLMs themselves for data cleaning and </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">preprocessing tasks.</span></span></p>
			<p><span class="koboSpan" id="kobo.55.1">In conclusion, while LLMs have reduced the need for extensive preprocessing in many NLP tasks, understanding and judiciously applying these fundamental techniques remains valuable. </span><span class="koboSpan" id="kobo.55.2">In the following sections, we will focus on the text preprocessing techniques that </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">remain relevant.</span></span></p>
			<h1 id="_idParaDest-244"><a id="_idTextAnchor281"/><span class="koboSpan" id="kobo.57.1">Text cleaning</span></h1>
			<p><span class="koboSpan" id="kobo.58.1">The</span><a id="_idIndexMarker947"/><span class="koboSpan" id="kobo.59.1"> primary goal of text cleaning is to transform unstructured textual information into a standardized and more manageable form. </span><span class="koboSpan" id="kobo.59.2">While cleaning text, several operations are commonly performed, such as the removal of HTML tags, special characters, and numerical values, as well as the standardization of letter cases and the handling of whitespaces and formatting issues. </span><span class="koboSpan" id="kobo.59.3">These operations collectively contribute to refining the quality of textual data and reducing its ambiguity. </span><span class="koboSpan" id="kobo.59.4">Let’s deep dive into </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">these techniques.</span></span></p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor282"/><span class="koboSpan" id="kobo.61.1">Removing HTML tags and special characters</span></h2>
			<p><span class="koboSpan" id="kobo.62.1">HTML tags are</span><a id="_idIndexMarker948"/><span class="koboSpan" id="kobo.63.1"> often present due to the extraction of content </span><a id="_idIndexMarker949"/><span class="koboSpan" id="kobo.64.1">from web pages. </span><span class="koboSpan" id="kobo.64.2">These tags, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.65.1">&lt;p&gt;</span></strong><span class="koboSpan" id="kobo.66.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.67.1">&lt;a&gt;</span></strong><span class="koboSpan" id="kobo.68.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.69.1">&lt;div&gt;</span></strong><span class="koboSpan" id="kobo.70.1">, carry </span><em class="italic"><span class="koboSpan" id="kobo.71.1">no semantic meaning</span></em><span class="koboSpan" id="kobo.72.1"> in the context of NLP and must be removed. </span><span class="koboSpan" id="kobo.72.2">The cleaning process involves the identification and stripping of HTML tags, leaving behind only the </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">actual words.</span></span></p>
			<p><span class="koboSpan" id="kobo.74.1">For </span><a id="_idIndexMarker950"/><span class="koboSpan" id="kobo.75.1">this </span><a id="_idIndexMarker951"/><span class="koboSpan" id="kobo.76.1">example, let’s consider a scenario where we have a dataset of user reviews for a product and want to prepare the text data for sentiment analysis. </span><span class="koboSpan" id="kobo.76.2">You can find the code for this section in the GitHub repository at </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/1.text_cleaning.py"><span class="koboSpan" id="kobo.77.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/1.text_cleaning.py</span></a><span class="koboSpan" id="kobo.78.1">. </span><span class="koboSpan" id="kobo.78.2">In this script, the data generation is also available for you, and you can follow the example step </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">by step.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.80.1">Important note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.81.1">Throughout this chapter, we’ve included key code snippets to illustrate the most important concepts. </span><span class="koboSpan" id="kobo.81.2">However, to see the complete code, including the libraries used, and to run the full end-to-end examples, please visit </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">the repository.</span></span></p>
			<p><span class="koboSpan" id="kobo.83.1">The first text preprocessing step that we will execute is the removal of HTML tags. </span><span class="koboSpan" id="kobo.83.2">Let’s have a look at the code step </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">by step:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.85.1">Let’s import the libraries for </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">this example:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.87.1">
from bs4 import BeautifulSoup
from transformers import BertTokenizer</span></pre></li>				<li><span class="koboSpan" id="kobo.88.1">The sample user reviews are </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">shown here:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.90.1">
reviews = [
  "&lt;html&gt;This product is &lt;b&gt;amazing!&lt;/b&gt;&lt;/html&gt;",
  "The product is good, but it could be better!!!",
  "I've never seen such a terrible product. </span><span class="koboSpan" id="kobo.90.2">0/10",
  "The product is AWESOME!!! </span><span class="koboSpan" id="kobo.90.3">Highly recommended!",
]</span></pre></li>				<li><span class="koboSpan" id="kobo.91.1">Next, we create a function that uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.92.1">BeautifulSoup</span></strong><span class="koboSpan" id="kobo.93.1"> to parse the HTML content and extract only the text, removing any </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">HTML tags:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.95.1">
def clean_html_tags(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()</span></pre></li>				<li><span class="koboSpan" id="kobo.96.1">Then </span><a id="_idIndexMarker952"/><span class="koboSpan" id="kobo.97.1">we preprocess all </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">the reviews:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.99.1">
def preprocess_text(text):
    text = clean_html_tags(text)
    return text
preprocessed_reviews = [preprocess_text(review) for review in reviews]</span></pre></li>				<li><span class="koboSpan" id="kobo.100.1">Finally, we</span><a id="_idIndexMarker953"/><span class="koboSpan" id="kobo.101.1"> get the preprocessed reviews </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.103.1">
- This product is amazing!
</span><span class="koboSpan" id="kobo.103.2">- The product is good, but it could be better!!!
</span><span class="koboSpan" id="kobo.103.3">- I've never seen such a terrible product. </span><span class="koboSpan" id="kobo.103.4">0/10
- The product is AWESOME!!! </span><span class="koboSpan" id="kobo.103.5">Highly recommended!</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.104.1">As we can see, all the HTML tags have been removed and the text is clean. </span><span class="koboSpan" id="kobo.104.2">We will continue enhancing this example by adding another common preprocessing step: handling the capitalization </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">of text.</span></span></p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor283"/><span class="koboSpan" id="kobo.106.1">Handling capitalization and letter case</span></h2>
			<p><span class="koboSpan" id="kobo.107.1">Text data</span><a id="_idIndexMarker954"/><span class="koboSpan" id="kobo.108.1"> often comes in various cases—uppercase, lowercase, or a mix of both. </span><span class="koboSpan" id="kobo.108.2">Inconsistent capitalization can lead to ambiguity in language processing tasks. </span><span class="koboSpan" id="kobo.108.3">Therefore, one common text-cleaning practice is to standardize the letter case throughout the corpus. </span><span class="koboSpan" id="kobo.108.4">This not only aids in maintaining consistency but also ensures that the model generalizes well across </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">different cases.</span></span></p>
			<p><span class="koboSpan" id="kobo.110.1">Building on the previous example, we are going to expand the preprocessing function to add one extra step: that of </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">letter standardization:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.112.1">Let’s first </span><a id="_idIndexMarker955"/><span class="koboSpan" id="kobo.113.1">remind ourselves what the reviews looked like after the removal of HTML tags from the previous </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">preprocessing step:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.115.1">
- This product is amazing!
</span><span class="koboSpan" id="kobo.115.2">- The product is good, but it could be better!!!
</span><span class="koboSpan" id="kobo.115.3">- I've never seen such a terrible product. </span><span class="koboSpan" id="kobo.115.4">0/10
- The product is AWESOME!!! </span><span class="koboSpan" id="kobo.115.5">Highly recommended!</span></pre></li>				<li><span class="koboSpan" id="kobo.116.1">The following function will convert all characters </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">into lowercase:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.118.1">
def standardize_case(text):
    return text.lower()</span></pre></li>				<li><span class="koboSpan" id="kobo.119.1">We will expand the </span><strong class="source-inline"><span class="koboSpan" id="kobo.120.1">preprocess_text</span></strong><span class="koboSpan" id="kobo.121.1"> function we presented in the previous example to convert all characters in the text to lowercase, making the text </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">case insensitive:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.123.1">
def preprocess_text(text):
    text = clean_html_tags(text)
    text = standardize_case(text)
    return text</span></pre></li>				<li><span class="koboSpan" id="kobo.124.1">Let’s print the </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">preprocessed reviews:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.126.1">
for preprocessed_review in preprocessed_reviews:
    print(f"- {preprocessed_review}")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.127.1">The lower-cased reviews are </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">presented here:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.129.1">- this product is amazing!
</span><span class="koboSpan" id="kobo.129.2">- the product is good, but it could be better!!!
</span><span class="koboSpan" id="kobo.129.3">- i've never seen such a terrible product. </span><span class="koboSpan" id="kobo.129.4">0/10
- the product is awesome!!! </span><span class="koboSpan" id="kobo.129.5">highly recommended!</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.130.1">Notice how </span><a id="_idIndexMarker956"/><span class="koboSpan" id="kobo.131.1">all the letters have turned to lower case! </span><span class="koboSpan" id="kobo.131.2">Go ahead and update the capitalization function as follows to turn everything to </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">upper case:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.133.1">
def standardize_case(text):
    return </span><strong class="bold"><span class="koboSpan" id="kobo.134.1">text.upper()</span></strong></pre>			<p><span class="koboSpan" id="kobo.135.1">The upper case reviews are </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">presented here:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.137.1">
- THIS PRODUCT IS AMAZING!
</span><span class="koboSpan" id="kobo.137.2">- THE PRODUCT IS GOOD, BUT IT COULD BE BETTER!!!
</span><span class="koboSpan" id="kobo.137.3">- I'VE NEVER SEEN SUCH A TERRIBLE PRODUCT. </span><span class="koboSpan" id="kobo.137.4">0/10
- THE PRODUCT IS AWESOME!!! </span><span class="koboSpan" id="kobo.137.5">HIGHLY RECOMMENDED!</span></pre>			<p><span class="koboSpan" id="kobo.138.1">In case you are wondering whether you should use lower or upper case, we’ve got </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">you covered.</span></span></p>
			<h3><span class="koboSpan" id="kobo.140.1">Lower or upper case?</span></h3>
			<p><span class="koboSpan" id="kobo.141.1">The </span><a id="_idIndexMarker957"/><span class="koboSpan" id="kobo.142.1">choice between using lowercase or uppercase text depends on the specific requirements of the NLP task. </span><span class="koboSpan" id="kobo.142.2">For instance, tasks such as sentiment analysis typically benefit from lowercasing, as it simplifies the text and reduces variability. </span><span class="koboSpan" id="kobo.142.3">Conversely, tasks</span><a id="_idIndexMarker958"/><span class="koboSpan" id="kobo.143.1"> such as </span><strong class="bold"><span class="koboSpan" id="kobo.144.1">Named Entity Recognition</span></strong><span class="koboSpan" id="kobo.145.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.146.1">NER</span></strong><span class="koboSpan" id="kobo.147.1">) may require preserving case information to accurately identify and </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">differentiate entities.</span></span></p>
			<p><span class="koboSpan" id="kobo.149.1">For example, in German, all nouns are capitalized, so maintaining the case is crucial for correct language representation. </span><span class="koboSpan" id="kobo.149.2">In contrast, English typically does not use capitalization to convey meaning, so lowercasing might be more appropriate for general </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">text analysis.</span></span></p>
			<p><span class="koboSpan" id="kobo.151.1">When dealing with text data from user inputs, such as social media posts or reviews, it’s important to consider the role of case variations. </span><span class="koboSpan" id="kobo.151.2">For instance, a tweet may use mixed case for emphasis or tone, which could be relevant for </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">sentiment analysis.</span></span></p>
			<p><span class="koboSpan" id="kobo.153.1">Modern LLMs </span><a id="_idIndexMarker959"/><span class="koboSpan" id="kobo.154.1">such as </span><strong class="bold"><span class="koboSpan" id="kobo.155.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="koboSpan" id="kobo.156.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.157.1">BERT</span></strong><span class="koboSpan" id="kobo.158.1">) and GPT-3 are trained on mixed-case text and handle both uppercase and lowercase effectively. </span><span class="koboSpan" id="kobo.158.2">These models utilize case information to enhance context and understanding. </span><span class="koboSpan" id="kobo.158.3">Their tokenizers are designed to manage case sensitivity inherently, processing text without needing </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">explicit conversion.</span></span></p>
			<p><span class="koboSpan" id="kobo.160.1">If your task</span><a id="_idIndexMarker960"/><span class="koboSpan" id="kobo.161.1"> requires distinguishing between different cases (e.g., recognizing proper nouns or acronyms), it is better to preserve the original casing. </span><span class="koboSpan" id="kobo.161.2">However, always consult the documentation and best practices for the specific model you are using. </span><span class="koboSpan" id="kobo.161.3">Some models might be optimized for lowercased input and could perform better if the text is converted </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">to lowercase.</span></span></p>
			<p><span class="koboSpan" id="kobo.163.1">The next step is to learn how we can deal with numerical values and symbols in </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">the text.</span></span></p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor284"/><span class="koboSpan" id="kobo.165.1">Dealing with numerical values and symbols</span></h2>
			<p><span class="koboSpan" id="kobo.166.1">Numerical </span><a id="_idIndexMarker961"/><span class="koboSpan" id="kobo.167.1">values, symbols, and mathematical expressions may be present in text data but may not always contribute meaningfully to the context. </span><span class="koboSpan" id="kobo.167.2">Cleaning them involves deciding whether to retain, replace, or remove these elements based on the specific requirements of </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">the task.</span></span></p>
			<p><span class="koboSpan" id="kobo.169.1">For instance, in sentiment analysis, numerical values might be less relevant, and their presence could be distracting. </span><span class="koboSpan" id="kobo.169.2">In contrast, for tasks related to quantitative analysis or financial sentiment, preserving numerical information </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">becomes crucial.</span></span></p>
			<p><span class="koboSpan" id="kobo.171.1">Building on the previous example, we are going to remove all the numbers and symbols in </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">the text:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.173.1">Let’s review how the data looked like after the previous </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">preprocessing step:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.175.1">
- This product is amazing!
</span><span class="koboSpan" id="kobo.175.2">- The product is good, but it could be better!!!
</span><span class="koboSpan" id="kobo.175.3">- I've never seen such a terrible product. </span><span class="koboSpan" id="kobo.175.4">0/10
- The product is AWESOME!!! </span><span class="koboSpan" id="kobo.175.5">Highly recommended!</span></pre></li>				<li><span class="koboSpan" id="kobo.176.1">Now let’s add a function that removes all characters from the text </span><em class="italic"><span class="koboSpan" id="kobo.177.1">except alphabetic characters </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.178.1">and spaces</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.180.1">
def remove_numbers_and_symbols(text):
    return ''.join(e for e in text if e.isalpha() or e.isspace())</span></pre></li>				<li><span class="koboSpan" id="kobo.181.1">Apply the text </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">preprocessing pipeline:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.183.1">
def preprocess_text(text):
    text = clean_html_tags(text)
    text = standardize_case(text)
    text = remove_numbers_and_symbols(text)
    return text</span></pre></li>				<li><span class="koboSpan" id="kobo.184.1">Let’s</span><a id="_idIndexMarker962"/><span class="koboSpan" id="kobo.185.1"> have a look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">preprocessed reviews:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.187.1">
- this product is amazing
- the product is good but it could be better
- ive never seen such a terrible product
- the product is awesome highly recommended</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.188.1">As you can see, after this preprocessing step, all the punctuations and symbols have been removed from the text. </span><span class="koboSpan" id="kobo.188.2">The decision to retain, replace, or remove symbols and punctuation during text preprocessing depends on the specific goals of your NLP task and the characteristics of </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">your dataset.</span></span></p>
			<h3><span class="koboSpan" id="kobo.190.1">Retaining symbols and punctuation</span></h3>
			<p><span class="koboSpan" id="kobo.191.1">With the</span><a id="_idIndexMarker963"/><span class="koboSpan" id="kobo.192.1"> advancements in LLMs, the approach to handling punctuation and symbols during text preprocessing has evolved significantly. </span><span class="koboSpan" id="kobo.192.2">Modern LLMs benefit from retaining punctuation and symbols due to their extensive training on diverse datasets. </span><span class="koboSpan" id="kobo.192.3">This retention helps these models understand context more accurately by capturing nuances such as emotions, emphasis, and sentence boundaries. </span><span class="koboSpan" id="kobo.192.4">For instance, punctuation marks such as exclamation points and question marks play a crucial role in sentiment analysis by conveying strong emotions, which improves the model’s performance. </span><span class="koboSpan" id="kobo.192.5">Similarly, in tasks such as text generation, punctuation maintains readability and structure, while in NER and translation, it aids in identifying proper nouns and </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">sentence boundaries.</span></span></p>
			<p><span class="koboSpan" id="kobo.194.1">On the other hand, there are scenarios where removing punctuation and symbols can be advantageous. </span><span class="koboSpan" id="kobo.194.2">Modern LLMs are robust enough to handle noisy data, but in certain applications, simplifying text by removing punctuation can streamline preprocessing and </span><em class="italic"><span class="koboSpan" id="kobo.195.1">reduce the number of unique tokens</span></em><span class="koboSpan" id="kobo.196.1">. </span><span class="koboSpan" id="kobo.196.2">This approach is beneficial for tasks such as topic modeling and clustering, where the focus is on content rather than structural elements. </span><span class="koboSpan" id="kobo.196.3">For example, removing punctuation can help identify core topics by eliminating distractions from sentence structure, and in text classification, it can standardize input data when punctuation does not add </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">significant value.</span></span></p>
			<p><span class="koboSpan" id="kobo.198.1">Another </span><a id="_idIndexMarker964"/><span class="koboSpan" id="kobo.199.1">approach is replacing punctuation and symbols with spaces or specific tokens, which helps in normalizing text while preserving some level of separation between tokens. </span><span class="koboSpan" id="kobo.199.2">This method can be particularly useful for custom tokenization strategies. </span><span class="koboSpan" id="kobo.199.3">In specialized NLP pipelines, replacing punctuation with specific tokens can retain important distinctions without adding unnecessary clutter to the text, facilitating more effective tokenization and preprocessing for </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">downstream tasks.</span></span></p>
			<p><span class="koboSpan" id="kobo.201.1">Let’s see a quick example on how to remove or replace symbols and punctuation. </span><span class="koboSpan" id="kobo.201.2">You can find the code for this section </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/2.punctuation.py"><span class="No-Break"><span class="koboSpan" id="kobo.203.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/2.punctuation.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.204.1">:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.205.1">Create the </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">sample text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.207.1">
text = "I love this product!!! </span><span class="koboSpan" id="kobo.207.2">It's amazing!!!"</span></pre></li>				<li><span class="koboSpan" id="kobo.208.1">Option 1: replace symbols and punctuation </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">with spaces:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.210.1">
replaced_text = text.translate(str.maketrans(string.punctuation, " " * len(string.punctuation)))
print("Replaced Text:", replaced_text)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.211.1">This will print the </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">following output:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.213.1">I love this product    It s amazing</span></pre></li>				<li><span class="koboSpan" id="kobo.214.1">Option 2: remove symbols </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">and punctuation:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.216.1">
removed_text = "".join(char for char in text if char.isalnum() or char.isspace())
print("Removed Text:", removed_text)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.217.1">This will print the </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">following output:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.219.1">I love this product Its amazing</span></pre></li>			</ul>
			<p><span class="koboSpan" id="kobo.220.1">Removing symbols and numbers is a crucial preprocessing step in text analysis that simplifies text</span><a id="_idIndexMarker965"/><span class="koboSpan" id="kobo.221.1"> by eliminating non-alphanumeric characters. </span><span class="koboSpan" id="kobo.221.2">The last thing we will discuss in this section is addressing whitespace issues to enhance text readability and ensure </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">consistent formatting.</span></span></p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor285"/><span class="koboSpan" id="kobo.223.1">Addressing whitespace and formatting issues</span></h2>
			<p><span class="koboSpan" id="kobo.224.1">Whitespaces </span><a id="_idIndexMarker966"/><span class="koboSpan" id="kobo.225.1">and formatting inconsistencies can be prevalent in text data, especially when it originates from diverse sources. </span><span class="koboSpan" id="kobo.225.2">Cleaning involves addressing issues such as multiple consecutive spaces, leading or trailing whitespaces, and variations in formatting styles. </span><span class="koboSpan" id="kobo.225.3">Regularization of whitespace ensures a standardized text representation, reducing the risk of misinterpretation by </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">downstream models.</span></span></p>
			<p><span class="koboSpan" id="kobo.227.1">Addressing whitespace and formatting issues remains crucial in the world of LLMs. </span><span class="koboSpan" id="kobo.227.2">Although modern LLMs exhibit robustness to various formatting inconsistencies, managing whitespace and formatting effectively can still enhance model performance and ensure </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">data consistency.</span></span></p>
			<p><span class="koboSpan" id="kobo.229.1">Standardizing whitespace and formatting creates a uniform dataset, which facilitates model training and analysis by minimizing noise and focusing attention on the content rather than formatting discrepancies. </span><span class="koboSpan" id="kobo.229.2">Enhanced readability, achieved through proper whitespace management, aids both human and machine learning interpretation by clearly delineating text elements. </span><span class="koboSpan" id="kobo.229.3">Furthermore, consistent whitespace handling is essential for accurate tokenization—a fundamental process in many NLP tasks—as it ensures precise identification and processing of words </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">and phrases.</span></span></p>
			<p><span class="koboSpan" id="kobo.231.1">So, let’s go back to the review example and add another step in the pipeline to </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">remove whitespaces:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.233.1">Let’s start by addressing whitespace and formatting issues. </span><span class="koboSpan" id="kobo.233.2">This function removes extra spaces and ensures that there is only one space </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">between words:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.235.1">
def remove_extra_whitespace(text):
    return ' '.join(text.split())</span></pre></li>				<li><span class="koboSpan" id="kobo.236.1">Next, we’ll add this to our text </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">preprocessing pipeline:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.238.1">
def preprocess_text(text):
    text = clean_html_tags(text)
    text = standardize_case(text)
    text = remove_numbers_and_symbols(text)
    text = remove_extra_whitespace(text)
    return text</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.239.1">Let’s have </span><a id="_idIndexMarker967"/><span class="koboSpan" id="kobo.240.1">a look at the reviews before applying the new step and focus on the whitespaces </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">marked here:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.242.1">
- this product</span><strong class="bold"><span class="koboSpan" id="kobo.243.1">    </span></strong><span class="koboSpan" id="kobo.244.1">is amazing
- the product is good but it could be better
- ive never seen such a terrible</span><strong class="bold"><span class="koboSpan" id="kobo.245.1">      </span></strong><span class="koboSpan" id="kobo.246.1">product
- the product is awesome highly recommended</span></pre>			<p><span class="koboSpan" id="kobo.247.1">Finally, let’s review the clean dataset, after having applied the </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">whitespace removal:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.249.1">
- this product is amazing
- the product is good but it could be better
- ive never seen such a terrible product
- the product is awesome highly recommended</span></pre>			<p><span class="koboSpan" id="kobo.250.1">Let’s move from pure text cleaning to focusing on safeguarding </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">the data.</span></span></p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor286"/><span class="koboSpan" id="kobo.252.1">Removing personally identifiable information</span></h2>
			<p><span class="koboSpan" id="kobo.253.1">When</span><a id="_idIndexMarker968"/><span class="koboSpan" id="kobo.254.1"> preprocessing text data, removing </span><strong class="bold"><span class="koboSpan" id="kobo.255.1">Personally Identifiable Information</span></strong><span class="koboSpan" id="kobo.256.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.257.1">PII</span></strong><span class="koboSpan" id="kobo.258.1">) is crucial for maintaining</span><a id="_idIndexMarker969"/><span class="koboSpan" id="kobo.259.1"> privacy, ensuring compliance with regulations, and improving data quality. </span><span class="koboSpan" id="kobo.259.2">For instance, consider a dataset of user reviews that includes names, email addresses, and phone numbers. </span><span class="koboSpan" id="kobo.259.3">If this sensitive information is not anonymized or removed, it poses significant risks such as privacy violations and potential misuse. </span><span class="koboSpan" id="kobo.259.4">Regulations </span><a id="_idIndexMarker970"/><span class="koboSpan" id="kobo.260.1">such as the </span><strong class="bold"><span class="koboSpan" id="kobo.261.1">General Data Protection Regulation</span></strong><span class="koboSpan" id="kobo.262.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.263.1">GDPR</span></strong><span class="koboSpan" id="kobo.264.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.265.1">California Consumer Privacy Act</span></strong><span class="koboSpan" id="kobo.266.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.267.1">CCPA</span></strong><span class="koboSpan" id="kobo.268.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.269.1">Health Insurance Portability and Accountability Act</span></strong><span class="koboSpan" id="kobo.270.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.271.1">HIPAA</span></strong><span class="koboSpan" id="kobo.272.1">) mandate</span><a id="_idIndexMarker971"/><span class="koboSpan" id="kobo.273.1"> that personal data must be handled</span><a id="_idIndexMarker972"/><span class="koboSpan" id="kobo.274.1"> carefully. </span><span class="koboSpan" id="kobo.274.2">Failing to remove PII can lead to legal penalties and loss of trust. </span><span class="koboSpan" id="kobo.274.3">Moreover, including identifiable details can introduce bias into machine learning models and compromise their generalization. </span><span class="koboSpan" id="kobo.274.4">Removing PII is essential for responsible AI development, as it allows for the creation and use of datasets that maintain individual privacy while still providing valuable insights for research </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">and analysis</span></span></p>
			<p><span class="koboSpan" id="kobo.276.1">The </span><a id="_idIndexMarker973"/><span class="koboSpan" id="kobo.277.1">following code snippet demonstrates how to use the presidio-analyzer and presidio-anonymizer libraries to detect and anonymize PII. </span><span class="koboSpan" id="kobo.277.2">Let’s have a look at the code step by step. </span><span class="koboSpan" id="kobo.277.3">The full code can be accessed </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/3.pii_detection.py"><span class="No-Break"><span class="koboSpan" id="kobo.279.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/3.pii_detection.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.280.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.281.1">Let’s start by importing the required libraries for </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">this example:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.283.1">
import pandas as pd
from presidio_analyzer import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine
from presidio_anonymizer.entities import OperatorConfig</span></pre></li>				<li><span class="koboSpan" id="kobo.284.1">We create a sample DataFrame with one column named </span><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">text</span></strong><span class="koboSpan" id="kobo.286.1"> containing sentences with different types of PII (e.g., names, email addresses, and </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">phone numbers):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.288.1">
data = {
    'text': [
        "Hello, my name is John Doe. </span><span class="koboSpan" id="kobo.288.2">My email is john.doe@example.com",
        "Contact Jane Smith at jane.smith@work.com",
        "Call her at 987-654-3210.",
        "This is a test message without PII."
</span><span class="koboSpan" id="kobo.288.3">    ]
}
df = pd.DataFrame(data)</span></pre></li>				<li><span class="koboSpan" id="kobo.289.1">We initialize </span><strong class="source-inline"><span class="koboSpan" id="kobo.290.1">AnalyzerEngine</span></strong><span class="koboSpan" id="kobo.291.1">  for </span><em class="italic"><span class="koboSpan" id="kobo.292.1">detecting</span></em><span class="koboSpan" id="kobo.293.1"> PII entities and </span><strong class="source-inline"><span class="koboSpan" id="kobo.294.1">AnonymizerEngine</span></strong><span class="koboSpan" id="kobo.295.1"> for </span><em class="italic"><span class="koboSpan" id="kobo.296.1">anonymizing</span></em><span class="koboSpan" id="kobo.297.1"> the detected </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">PII entities:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.299.1">
analyzer = AnalyzerEngine()
anonymizer = AnonymizerEngine()</span></pre></li>				<li><span class="koboSpan" id="kobo.300.1">Next, we’ll</span><a id="_idIndexMarker974"/><span class="koboSpan" id="kobo.301.1"> define an anonymization function that detects PII in the text and applies masking rules based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">entity type:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.303.1">
def anonymize_text(text):
    analyzer_results = analyzer.analyze(text=text, entities=</span><strong class="bold"><span class="koboSpan" id="kobo.304.1">["PERSON", "EMAIL_ADDRESS", "PHONE_NUMBER"]</span></strong><span class="koboSpan" id="kobo.305.1">, language="en")
    operators = {
        "PERSON": OperatorConfig("mask", {"masking_char": "*", "chars_to_mask": 4, "from_end": True}),
        "EMAIL_ADDRESS": OperatorConfig("mask", {"masking_char": "*", "chars_to_mask": 5, "from_end": True}),
        "PHONE_NUMBER": OperatorConfig("mask", {"masking_char": "*", "chars_to_mask": 6, "from_end": True})
    }
    anonymized_result = anonymizer.anonymize(
        text=text, analyzer_results=analyzer_results,
        operators=operators)
    return anonymized_result.text</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.306.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.307.1">anonymize_text</span></strong><span class="koboSpan" id="kobo.308.1"> function is designed to protect sensitive information within a given text by anonymizing specific types of entities. </span><span class="koboSpan" id="kobo.308.2">It first analyzes the text to identify entities such as names (</span><strong class="source-inline"><span class="koboSpan" id="kobo.309.1">PERSON</span></strong><span class="koboSpan" id="kobo.310.1">), email addresses (</span><strong class="source-inline"><span class="koboSpan" id="kobo.311.1">EMAIL_ADDRESS</span></strong><span class="koboSpan" id="kobo.312.1">), and phone numbers (</span><strong class="source-inline"><span class="koboSpan" id="kobo.313.1">PHONE_NUMBER</span></strong><span class="koboSpan" id="kobo.314.1">) using an analyzer. </span><span class="koboSpan" id="kobo.314.2">For each entity type, it then applies a masking operation to conceal part of the information. </span><span class="koboSpan" id="kobo.314.3">Specifically, it masks the last four characters of a person’s name, the last five</span><a id="_idIndexMarker975"/><span class="koboSpan" id="kobo.315.1"> characters of an email address, and the last six characters of a phone number. </span><span class="koboSpan" id="kobo.315.2">The function returns the text with these sensitive entities anonymized, ensuring that personal information is obscured while retaining the overall structure of </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">the text.</span></span></p></li>				<li><span class="koboSpan" id="kobo.317.1">Apply the anonymization function to </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">the DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.319.1">
df['anonymized_text'] = df['text'].apply(anonymize_text)</span></pre></li>				<li><span class="koboSpan" id="kobo.320.1">Display </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">the DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.322.1">
0    Hello, my name is John. </span><span class="koboSpan" id="kobo.322.2">My email is john.d...
</span><span class="koboSpan" id="kobo.322.3">1            Contact Jane S at jane.smith@wor*
2                            Call her at 987-65.
</span><span class="koboSpan" id="kobo.322.4">3                  This is a test message without PII.</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.323.1">By using these configurations, you can tailor the anonymization process to meet specific requirements and ensure that sensitive information is properly protected. </span><span class="koboSpan" id="kobo.323.2">This approach helps you comply with privacy regulations and protect sensitive information in </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">your datasets.</span></span></p>
			<p><span class="koboSpan" id="kobo.325.1">While removing PII is essential for protecting privacy and ensuring data compliance, another critical aspect of text preprocessing is handling rare words and </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">spelling variations.</span></span></p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor287"/><span class="koboSpan" id="kobo.327.1">Handling rare words and spelling variations</span></h1>
			<p><span class="koboSpan" id="kobo.328.1">The rise of LLMs has revolutionized how we interact with technology and process information, particularly in the world of handling spelling variations and rare words. </span><span class="koboSpan" id="kobo.328.2">Before the emergence of LLMs, managing these linguistic challenges required extensive manual effort, often involving specialized knowledge and painstakingly crafted algorithms. </span><span class="koboSpan" id="kobo.328.3">Traditional spell-checkers and language processors struggled with rare words and </span><a id="_idIndexMarker976"/><span class="koboSpan" id="kobo.329.1">variations, leading to frequent errors and inefficiencies. </span><span class="koboSpan" id="kobo.329.2">Today, LLMs such as GPT-4, Lllama3, and</span><a id="_idIndexMarker977"/><span class="koboSpan" id="kobo.330.1"> others have transformed this landscape by leveraging vast datasets and sophisticated machine-learning techniques to understand and generate text that accommodates a wide range of spelling variations and uncommon terminology. </span><span class="koboSpan" id="kobo.330.2">These models can recognize and correct misspellings, provide contextually appropriate suggestions, and accurately interpret rare words, enhancing the precision and reliability of </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">text processing.</span></span></p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor288"/><span class="koboSpan" id="kobo.332.1">Dealing with rare words</span></h2>
			<p><span class="koboSpan" id="kobo.333.1">In the era</span><a id="_idIndexMarker978"/><span class="koboSpan" id="kobo.334.1"> of LLMs such as GPT-3 and GPT-4, handling rare words has become less of a challenge compared to traditional NLP methods. </span><span class="koboSpan" id="kobo.334.2">These models have been trained on vast and diverse datasets, enabling them to understand and generate text with rare or even unseen words. </span><span class="koboSpan" id="kobo.334.3">However, there are still some considerations for text preprocessing and handling rare </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">words effectively.</span></span></p>
			<p><span class="koboSpan" id="kobo.336.1">So, how can we handle rare words with LLMs? </span><span class="koboSpan" id="kobo.336.2">There are some key concepts we need to understand, starting with tokenization. </span><span class="koboSpan" id="kobo.336.3">We won’t explore tokenization in detail here as we have a dedicated section later on; for now, let’s say </span><a id="_idIndexMarker979"/><span class="koboSpan" id="kobo.337.1">that LLMs use </span><strong class="bold"><span class="koboSpan" id="kobo.338.1">subword tokenization</span></strong><span class="koboSpan" id="kobo.339.1"> methods that break down rare words into more common subword units. </span><span class="koboSpan" id="kobo.339.2">This helps in managing </span><strong class="bold"><span class="koboSpan" id="kobo.340.1">Out-of-Vocabulary</span></strong><span class="koboSpan" id="kobo.341.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.342.1">OOV</span></strong><span class="koboSpan" id="kobo.343.1">) words</span><a id="_idIndexMarker980"/><span class="koboSpan" id="kobo.344.1"> by decomposing them into familiar components. </span><span class="koboSpan" id="kobo.344.2">The other interesting thing about LLMs is that even if they don’t know the word per se, they have contextual understanding capabilities, meaning that LLMs leverage context to infer the meaning of </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">rare words.</span></span></p>
			<p><span class="koboSpan" id="kobo.346.1">In the following code example, we will test GPT-2 to see if it can handle rare words. </span><span class="koboSpan" id="kobo.346.2">You can find the code in the repository </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/4.rare_words.py"><span class="No-Break"><span class="koboSpan" id="kobo.348.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/4.rare_words.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.349.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.350.1">Let’s import the </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">required libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.352.1">
from transformers import GPT2LMHeadModel, GPT2Tokenizer</span></pre></li>				<li><span class="koboSpan" id="kobo.353.1">Initialize</span><a id="_idIndexMarker981"/><span class="koboSpan" id="kobo.354.1"> the GPT-2 tokenizer </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">and model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.356.1">
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")</span></pre></li>				<li><span class="koboSpan" id="kobo.357.1">Define a text prompt with a </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">rare word:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.359.1">
text = "The </span><strong class="source-inline"><span class="koboSpan" id="kobo.360.1">quokka</span></strong><span class="koboSpan" id="kobo.361.1">, a rare marsupial,"</span></pre></li>				<li><span class="koboSpan" id="kobo.362.1">Encode the input text </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">to tokens:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.364.1">
indexed_tokens = tokenizer.encode(text, return_tensors='pt')</span></pre></li>				<li><span class="koboSpan" id="kobo.365.1">Generate text until the output length reaches 50 tokens. </span><span class="koboSpan" id="kobo.365.2">The model generates text based on the input prompt, leveraging its understanding of the context to handle the </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">rare word:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.367.1">
output_text = model.generate(indexed_tokens, max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.368.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.369.1">generate</span></strong><span class="koboSpan" id="kobo.370.1"> function in the given code snippet is used to produce text output from a model based on the input tokens provided. </span><span class="koboSpan" id="kobo.370.2">The parameters used in this function call control various aspects of the text </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">generation process:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">indexed_tokens</span></strong><span class="koboSpan" id="kobo.373.1">: This represents the input sequence that the model will use to start generating text. </span><span class="koboSpan" id="kobo.373.2">It consists of tokenized text that serves as the starting point </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">for generation.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.375.1">max_length=50</span></strong><span class="koboSpan" id="kobo.376.1">: This parameter sets the maximum length of the generated text. </span><span class="koboSpan" id="kobo.376.2">The model will generate up to 50 tokens, including the input tokens, ensuring that the output doesn’t exceed </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">this length.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">num_beams=5</span></strong><span class="koboSpan" id="kobo.379.1">: This controls the beam search process, where the model keeps track of the top five most likely sequences during generation. </span><span class="koboSpan" id="kobo.379.2">Beam search helps improve the quality of the generated text by exploring multiple possible outcomes simultaneously and selecting the most </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">likely one.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">no_repeat_ngram_size=2</span></strong><span class="koboSpan" id="kobo.382.1">: This prevents the model from repeating any sequence of two tokens (bigrams) within the generated text. </span><span class="koboSpan" id="kobo.382.2">It helps produce</span><a id="_idIndexMarker982"/><span class="koboSpan" id="kobo.383.1"> more coherent and less repetitive output by ensuring that the same phrases don’t appear </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">multiple times.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.385.1">early_stopping=True</span></strong><span class="koboSpan" id="kobo.386.1">: This parameter allows the generation process to stop early if all beams have reached the end of the text sequence (e.g., a sentence-ending token). </span><span class="koboSpan" id="kobo.386.2">This can make the generation process more efficient by avoiding unnecessary continuation when a complete and sensible output has already </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">been produced.</span></span></li></ul></li>			</ol>
			<p><span class="koboSpan" id="kobo.388.1">These parameters can be adjusted depending on the desired output. </span><span class="koboSpan" id="kobo.388.2">For instance, increasing </span><strong class="source-inline"><span class="koboSpan" id="kobo.389.1">max_length</span></strong><span class="koboSpan" id="kobo.390.1"> generates longer text, while modifying </span><strong class="source-inline"><span class="koboSpan" id="kobo.391.1">num_beams</span></strong><span class="koboSpan" id="kobo.392.1"> can balance quality and computational cost. </span><span class="koboSpan" id="kobo.392.2">Adjusting </span><strong class="source-inline"><span class="koboSpan" id="kobo.393.1">no_repeat_ngram_size</span></strong><span class="koboSpan" id="kobo.394.1"> changes the strictness of repetition prevention, and toggling </span><strong class="source-inline"><span class="koboSpan" id="kobo.395.1">early_stopping</span></strong><span class="koboSpan" id="kobo.396.1"> can affect the efficiency and length of the generated text. </span><em class="italic"><span class="koboSpan" id="kobo.397.1">I would advise that you go and play with these configurations to see how their output </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.398.1">is affected</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.400.1">The generated tokens are decoded back into </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">human-readable text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.402.1">
output_text_decoded = tokenizer.decode(output_text[0], skip_special_tokens=True)</span></pre></li>				<li><span class="koboSpan" id="kobo.403.1">Print the </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">decoded text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.405.1">
The quokka, a rare marsupial, is one of the world's most endangered species.</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.406.1">As we can see, the model understood the meaning of </span><em class="italic"><span class="koboSpan" id="kobo.407.1">quokka</span></em><span class="koboSpan" id="kobo.408.1"> and created a sequence of words, additional text that continues from the prompt, showcasing the language generation capabilities of LLMs. </span><span class="koboSpan" id="kobo.408.2">This is possible because LLMs turn the tokens into a numerical representation</span><a id="_idIndexMarker983"/><span class="koboSpan" id="kobo.409.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.410.1">embeddings</span></strong><span class="koboSpan" id="kobo.411.1">, as we will see later on, that capture the meaning </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">of words.</span></span></p>
			<p><span class="koboSpan" id="kobo.413.1">We discussed the use of rare words in text preprocessing. </span><span class="koboSpan" id="kobo.413.2">Let’s now move to another challenge—spelling errors </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">and typos.</span></span></p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor289"/><span class="koboSpan" id="kobo.415.1">Addressing spelling variations and typos</span></h2>
			<p><span class="koboSpan" id="kobo.416.1">The challenge with spelling variations </span><a id="_idIndexMarker984"/><span class="koboSpan" id="kobo.417.1">and typos is that it can lead to </span><em class="italic"><span class="koboSpan" id="kobo.418.1">different tokenizations for similar words</span></em><span class="koboSpan" id="kobo.419.1">. </span><span class="koboSpan" id="kobo.419.2">In the era of LLMs, handling spelling and typos has become more sophisticated. </span><span class="koboSpan" id="kobo.419.3">LLMs can understand contexts and generate text that often corrects such errors implicitly. </span><span class="koboSpan" id="kobo.419.4">However, explicit preprocessing to correct spelling mistakes can still enhance the performance of these models, especially in applications where accuracy is critical. </span><span class="koboSpan" id="kobo.419.5">There are different ways to address spelling variations and mistakes, as we will see in the following section, starting with </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">spelling correction.</span></span></p>
			<h3><span class="koboSpan" id="kobo.421.1">Spelling correction</span></h3>
			<p><span class="koboSpan" id="kobo.422.1">Let’s create an</span><a id="_idIndexMarker985"/><span class="koboSpan" id="kobo.423.1"> example of fixing spelling mistakes using an LLM with Hugging Face Transformers. </span><span class="koboSpan" id="kobo.423.2">We’ll use the experimental </span><strong class="source-inline"><span class="koboSpan" id="kobo.424.1">oliverguhr/spelling-correction-english-base</span></strong><span class="koboSpan" id="kobo.425.1"> spelling correction model for this demonstration. </span><span class="koboSpan" id="kobo.425.2">You can find the full code </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/5.spelling_checker.py"><span class="No-Break"><span class="koboSpan" id="kobo.427.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/5.spelling_checker.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.428.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.429.1">Define the spelling function pipeline. </span><span class="koboSpan" id="kobo.429.2">Inside this function, we initialize the spelling correction pipeline using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.430.1">oliverguhr/spelling-correction-english-base</span></strong><span class="koboSpan" id="kobo.431.1"> model. </span><span class="koboSpan" id="kobo.431.2">This model is specifically trained for spelling </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">correction tasks:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.433.1">
def fix_spelling(text):
    spell_check = pipeline("text2text-generation", model="oliverguhr/spelling-correction-english-base")</span></pre></li>				<li><span class="koboSpan" id="kobo.434.1">We use the pipeline to generate the corrected text. </span><span class="koboSpan" id="kobo.434.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.435.1">max_length</span></strong><span class="koboSpan" id="kobo.436.1"> parameter is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.437.1">2048</span></strong><span class="koboSpan" id="kobo.438.1"> to allow for longer </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">input texts:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.440.1">
    corrected = spell_check(text, max_length=2048)[0]['generated_text']
    return corrected</span></pre></li>				<li><span class="koboSpan" id="kobo.441.1">Test the function with some sample text containing </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">spelling mistakes:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.443.1">
sample_text = "My name si from Grece."
</span><span class="koboSpan" id="kobo.443.2">corrected_text = fix_spelling(sample_text)
Corrected text: </span><strong class="bold"><span class="koboSpan" id="kobo.444.1">My name is from Greece.</span></strong></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.445.1">It’s important </span><a id="_idIndexMarker986"/><span class="koboSpan" id="kobo.446.1">to note that this is an experimental model, and its performance may vary depending on the complexity and context of the input text. </span><span class="koboSpan" id="kobo.446.2">For more robust spelling and grammar correction, you might consider using more advanced models; however, some of them need authentication to download or sign agreements. </span><span class="koboSpan" id="kobo.446.3">So, for simplicity, we used an experimental model here. </span><span class="koboSpan" id="kobo.446.4">You can replace it with any model you have access to, from Llama3 to GPT4 </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">and others.</span></span></p>
			<p><span class="koboSpan" id="kobo.448.1">The significance of spelling correction in text preprocessing tasks takes us nicely to the concept of fuzzy matching, a technique that further enhances the accuracy and relevance of generated content by accommodating minor errors and variations in </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">input text.</span></span></p>
			<h3><span class="koboSpan" id="kobo.450.1">Fuzzy matching</span></h3>
			<p><span class="koboSpan" id="kobo.451.1">Fuzzy matching</span><a id="_idIndexMarker987"/><span class="koboSpan" id="kobo.452.1"> is a technique used to compare strings for similarity, even when they are not exactly the same. </span><span class="koboSpan" id="kobo.452.2">It’s like finding words that are “kind of similar” or “close enough.” </span><span class="koboSpan" id="kobo.452.3">So, we can use fuzzy matching algorithms to identify and map similar words, as well as to solve for variations and minor misspellings. </span><span class="koboSpan" id="kobo.452.4">We can enhance the spelling correction function by adding fuzzy matching using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.453.1">TheFuzz</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.454.1"> library.</span></span></p>
			<p><span class="koboSpan" id="kobo.455.1">Let’s go through the code that you can find </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/6.fuzzy_matching.py"><span class="No-Break"><span class="koboSpan" id="kobo.457.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/6.fuzzy_matching.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.458.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.459.1">We’ll start by installing </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">the library:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.461.1">
pip install thefuzz==0.22.1</span></pre></li>				<li><span class="koboSpan" id="kobo.462.1">Let’s import the </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">required libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.464.1">
from transformers import pipeline
from thefuzz import process, fuzz</span></pre></li>				<li><span class="koboSpan" id="kobo.465.1">Initialize the spelling </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">correction pipeline:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.467.1">
def fix_spelling(text, threshold=80):
    spell_check = pipeline("text2text-generation", model="oliverguhr/spelling-correction-english-base")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.468.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.469.1">oliverguhr/spelling-correction-english-base</span></strong><span class="koboSpan" id="kobo.470.1"> model is specifically fine-tuned for the task of spelling correction, making it a highly effective and efficient tool for spelling correction. </span><span class="koboSpan" id="kobo.470.2">This model has been trained to recognize </span><a id="_idIndexMarker988"/><span class="koboSpan" id="kobo.471.1">and correct common spelling errors in English text, leading to greater accuracy. </span><span class="koboSpan" id="kobo.471.2">It is optimized for text-to-text generation, which allows it to efficiently generate corrected versions of input text with minimal computational overhead. </span><span class="koboSpan" id="kobo.471.3">Additionally, its training likely involved exposure to datasets containing spelling errors and their corrections, enabling it to make informed and contextually </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">appropriate corrections.</span></span></p></li>				<li><span class="koboSpan" id="kobo.473.1">Generate the corrected text as in the </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">previous section:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.475.1">
    corrected = spell_check(text, max_length=2048)[0]['generated_text']</span></pre></li>				<li><span class="koboSpan" id="kobo.476.1">Split the original and corrected texts </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">into words:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.478.1">
    original_words = text.split()
    corrected_words = corrected.split()</span></pre></li>				<li><span class="koboSpan" id="kobo.479.1">Create a dictionary of common English words (you can expand </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">this list):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.481.1">
    common_words = set(['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at'])</span></pre></li>				<li><span class="koboSpan" id="kobo.482.1">Fuzzy match </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">each word:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.484.1">
    final_words = []
    for orig, corr in zip(original_words, corrected_words):
        if orig.lower() in common_words:
            final_words.append(orig)
        else:
            matches = process.extractOne(orig, [corr], scorer=fuzz.ratio)
            if matches[1] &gt;= threshold:
                final_words.append(matches[0])
            else:
                final_words.append(orig)
    return ' '.join(final_words)</span></pre></li>				<li><span class="koboSpan" id="kobo.485.1">Test the</span><a id="_idIndexMarker989"/><span class="koboSpan" id="kobo.486.1"> function with some sample text containing </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">spelling mistakes:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.488.1">
sample_text = "Lets do a copmarsion of speling mistaks in this sentense."
</span><span class="koboSpan" id="kobo.488.2">corrected_text = fix_spelling(sample_text)</span></pre></li>				<li><span class="koboSpan" id="kobo.489.1">Print </span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">the results:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.491.1">
Original text: Lets do a copmarsion of speling mistaks in this sentense.
</span><span class="koboSpan" id="kobo.491.2">Corrected text: Let's do a comparison of speling mistaks in this sentence.</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.492.1">Now, as you can see, not all the spelling mistakes are corrected. </span><span class="koboSpan" id="kobo.492.2">We could get some better performance by fine-tuning the model on the examples it usually misses. </span><span class="koboSpan" id="kobo.492.3">However, there is good news! </span><span class="koboSpan" id="kobo.492.4">The rise of LLMs has made it less critical to correct spelling mistakes because these models are designed to understand and process text contextually. </span><span class="koboSpan" id="kobo.492.5">Even when words are misspelled, LLMs can infer the intended meaning by analyzing the surrounding words and overall sentence structure. </span><span class="koboSpan" id="kobo.492.6">This ability reduces the need for perfect spelling, as the primary focus shifts to conveying the message rather than ensuring every word is </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">spelled correctly.</span></span></p>
			<p><span class="koboSpan" id="kobo.494.1">After completing the initial text preprocessing steps, the next critical phase is </span><strong class="bold"><span class="koboSpan" id="kobo.495.1">chunking</span></strong><span class="koboSpan" id="kobo.496.1">. </span><span class="koboSpan" id="kobo.496.2">This </span><a id="_idIndexMarker990"/><span class="koboSpan" id="kobo.497.1">process involves breaking the cleaned text into smaller, meaningful units. </span><span class="koboSpan" id="kobo.497.2">Let’s discuss that in the </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">following section.</span></span></p>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor290"/><span class="koboSpan" id="kobo.499.1">Chunking</span></h1>
			<p><span class="koboSpan" id="kobo.500.1">Chunking is an </span><a id="_idIndexMarker991"/><span class="koboSpan" id="kobo.501.1">essential preprocessing step in NLP that involves breaking down text into smaller, manageable units, or “chunks.” </span><span class="koboSpan" id="kobo.501.2">This process is crucial for various applications, including text summarization, sentiment analysis, information extraction, </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">and more.</span></span></p>
			<p><span class="koboSpan" id="kobo.503.1">Why is chunking becoming more and more important? </span><span class="koboSpan" id="kobo.503.2">By breaking down large documents, chunking enhances manageability and efficiency, particularly for models with </span><em class="italic"><span class="koboSpan" id="kobo.504.1">token limits</span></em><span class="koboSpan" id="kobo.505.1">, preventing overload and enabling smoother processing. </span><span class="koboSpan" id="kobo.505.2">It also improves accuracy by allowing models to </span><em class="italic"><span class="koboSpan" id="kobo.506.1">focus on smaller, coherent segments of text</span></em><span class="koboSpan" id="kobo.507.1">, which reduces noise and complexity compared to analyzing entire documents. </span><span class="koboSpan" id="kobo.507.2">Additionally, chunking helps maintain context within each segment, which is essential for tasks such as machine translation and text generation, ensuring that the model comprehends and processes the </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">text effectively.</span></span></p>
			<p><span class="koboSpan" id="kobo.509.1">Chunking can be implemented in many different ways; for instance, summarization may benefit from paragraph-level chunks, whereas sentiment analysis might use sentence-level chunks to capture nuanced emotional tones. </span><span class="koboSpan" id="kobo.509.2">In the following sections, we will focus on fixed-length, recursive, and semantic chunking as we see them more often in the </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">data world.</span></span></p>
			<h3><span class="koboSpan" id="kobo.511.1">Implementing fixed-length chunking</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.512.1">Fixed-length chunking</span></strong><span class="koboSpan" id="kobo.513.1"> involves </span><a id="_idIndexMarker992"/><span class="koboSpan" id="kobo.514.1">breaking text into chunks of a </span><em class="italic"><span class="koboSpan" id="kobo.515.1">predefined length</span></em><span class="koboSpan" id="kobo.516.1">, either by character count or token count. </span><span class="koboSpan" id="kobo.516.2">It is usually </span><a id="_idIndexMarker993"/><span class="koboSpan" id="kobo.517.1">preferred because it is very simple to implement and ensures uniform chunk sizes. </span><span class="koboSpan" id="kobo.517.2">However, as the split is random, it may split sentences or semantic units, leading to a loss of context. </span><span class="koboSpan" id="kobo.517.3">It is suitable for tasks where uniform chunk sizes are needed, such as certain types of </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">text classification.</span></span></p>
			<p><span class="koboSpan" id="kobo.519.1">To showcase </span><a id="_idIndexMarker994"/><span class="koboSpan" id="kobo.520.1">fixed-length chunking, we are going to work with review data again, but we will include a few lengthier reviews. </span><span class="koboSpan" id="kobo.520.2">You can see the full example </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/7.fixed_chunking.py"><span class="No-Break"><span class="koboSpan" id="kobo.522.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/7.fixed_chunking.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.523.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.524.1">Let’s start by loading the </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">example data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.526.1">
reviews = [
    "This smartphone has an excellent camera. </span><span class="koboSpan" id="kobo.526.2">The photos are sharp and the colors are vibrant. </span><span class="koboSpan" id="kobo.526.3">Overall, very satisfied with my purchase.",
    "I was disappointed with the laptop's performance. </span><span class="koboSpan" id="kobo.526.4">It frequently lags and the battery life is shorter than expected.",
    "The blender works great for making smoothies. </span><span class="koboSpan" id="kobo.526.5">It's powerful and easy to clean. </span><span class="koboSpan" id="kobo.526.6">Definitely worth the price.",
    "Customer support was unresponsive. </span><span class="koboSpan" id="kobo.526.7">I had to wait a long time for a reply, and my issue was not resolved satisfactorily.",
    "The book is a fascinating read. </span><span class="koboSpan" id="kobo.526.8">The storyline is engaging and the characters are well-developed. </span><span class="koboSpan" id="kobo.526.9">Highly recommend to all readers."
</span><span class="koboSpan" id="kobo.526.10">]</span></pre></li>				<li><span class="koboSpan" id="kobo.527.1">Import</span><a id="_idIndexMarker995"/><span class="koboSpan" id="kobo.528.1"> the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.529.1">TokenTextSplitter</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.530.1"> class:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.531.1">
from langchain_text_splitters import TokenTextSplitter</span></pre></li>				<li><span class="koboSpan" id="kobo.532.1">Initialize </span><a id="_idIndexMarker996"/><span class="koboSpan" id="kobo.533.1">the </span><strong class="source-inline"><span class="koboSpan" id="kobo.534.1">TokenTextSplitter</span></strong><span class="koboSpan" id="kobo.535.1"> class with a chunk size of </span><strong class="source-inline"><span class="koboSpan" id="kobo.536.1">50</span></strong><span class="koboSpan" id="kobo.537.1"> tokens and </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">no overlap:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.539.1">
text_splitter = TokenTextSplitter(chunk_size=50, chunk_overlap=0)</span></pre></li>				<li><span class="koboSpan" id="kobo.540.1">Combine the reviews into a single text block </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">for chunking:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.542.1">
text_block = " ".join(reviews)</span></pre></li>				<li><span class="koboSpan" id="kobo.543.1">Split the text into </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">token-based chunks:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.545.1">
chunks = text_splitter.split_text(text_block)</span></pre></li>				<li><span class="koboSpan" id="kobo.546.1">Print </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">the chunks:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.548.1">
Chunk 1:
This smartphone has an excellent camera. </span><span class="koboSpan" id="kobo.548.2">The photos are sharp and the colors are vibrant. </span><span class="koboSpan" id="kobo.548.3">Overall, very satisfied with my purchase. </span><span class="koboSpan" id="kobo.548.4">I was disappointed with the laptop's performance. </span><span class="koboSpan" id="kobo.548.5">It frequently lags and the battery life is shorter than expected. </span><span class="koboSpan" id="kobo.548.6">The blender works
Chunk 2:
great for making smoothies. </span><span class="koboSpan" id="kobo.548.7">It's powerful and easy to clean. </span><span class="koboSpan" id="kobo.548.8">Definitely worth the price. </span><span class="koboSpan" id="kobo.548.9">Customer support was unresponsive. </span><span class="koboSpan" id="kobo.548.10">I had to wait a long time for a reply, and my issue was not resolved satisfactorily. </span><span class="koboSpan" id="kobo.548.11">The book is a
Chunk 3:
fascinating read. </span><span class="koboSpan" id="kobo.548.12">The storyline is engaging and the characters are well-developed. </span><span class="koboSpan" id="kobo.548.13">Highly recommend to all readers.</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.549.1">To</span><a id="_idIndexMarker997"/><span class="koboSpan" id="kobo.550.1"> understand </span><a id="_idIndexMarker998"/><span class="koboSpan" id="kobo.551.1">how varying chunk sizes affect the output, you can modify the </span><strong class="source-inline"><span class="koboSpan" id="kobo.552.1">chunk_size</span></strong><span class="koboSpan" id="kobo.553.1"> parameter. </span><span class="koboSpan" id="kobo.553.2">For instance, you might try chunk sizes of </span><strong class="source-inline"><span class="koboSpan" id="kobo.554.1">20</span></strong><span class="koboSpan" id="kobo.555.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.556.1">70</span></strong><span class="koboSpan" id="kobo.557.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.558.1">150</span></strong><span class="koboSpan" id="kobo.559.1"> tokens. </span><span class="koboSpan" id="kobo.559.2">Here, you can see how you can adapt the code to test different </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">chunk sizes:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.561.1">
chunk_sizes = [20, 70, 150]
for size in chunk_sizes:
    print(f"Chunk Size: {size}")
    text_splitter = TokenTextSplitter(chunk_size=size, chunk_overlap=0)
    chunks = text_splitter.split_text(text_block)
    for i, chunk in enumerate(chunks):
        print(f"Chunk {i + 1}:")
        print(chunk)
        print("\n")</span></pre>			<p><span class="koboSpan" id="kobo.562.1">We successfully divided our review into the required chunks, but before moving forward, it’s crucial to understand the significance of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">chunk_overlap=0</span></strong></span><span class="No-Break"><strong class="bold"> </strong></span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">parameter.</span></span></p>
			<h4><span class="koboSpan" id="kobo.565.1">Chunk overlap</span></h4>
			<p><strong class="bold"><span class="koboSpan" id="kobo.566.1">Chunk overlap</span></strong><span class="koboSpan" id="kobo.567.1"> refers </span><a id="_idIndexMarker999"/><span class="koboSpan" id="kobo.568.1">to the number of characters or tokens that are </span><em class="italic"><span class="koboSpan" id="kobo.569.1">shared</span></em><span class="koboSpan" id="kobo.570.1"> between adjacent chunks when splitting a text. </span><span class="koboSpan" id="kobo.570.2">It’s the amount of text that “overlaps” between one chunk and </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">the next.</span></span></p>
			<p><span class="koboSpan" id="kobo.572.1">Chunk overlap is crucial as it helps preserve context and enhance the coherence of the text. </span><span class="koboSpan" id="kobo.572.2">By ensuring that adjacent chunks share some common content, overlap </span><em class="italic"><span class="koboSpan" id="kobo.573.1">maintains continuity</span></em><span class="koboSpan" id="kobo.574.1"> and prevents important information from being lost at the boundaries. </span><span class="koboSpan" id="kobo.574.2">For instance, if a document is divided into chunks without overlap, a critical piece of information could be split between two chunks, potentially rendering it inaccessible or causing a loss of meaning. </span><span class="koboSpan" id="kobo.574.3">In retrieval tasks, such as searching or question-answering, overlap ensures that relevant details are captured even if they fall across chunk boundaries, thereby improving the effectiveness of the retrieval process. </span><span class="koboSpan" id="kobo.574.4">For example, if a chunk ends mid-sentence, the overlap ensures that the entire sentence is considered, which is essential for accurate comprehension and </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">response generation.</span></span></p>
			<p><span class="koboSpan" id="kobo.576.1">Let’s consider a simple example to illustrate </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">chunk overlap:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.578.1">
Original text:
One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.</span></pre>			<p><span class="koboSpan" id="kobo.579.1">With a chunk size of five words and an overlap of one word, we’ll get the </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">following results:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.581.1">
Chunk 1: "One of the most </span><strong class="bold"><span class="koboSpan" id="kobo.582.1">important</span></strong><span class="koboSpan" id="kobo.583.1">"
Chunk 2: "</span><strong class="bold"><span class="koboSpan" id="kobo.584.1">important</span></strong><span class="koboSpan" id="kobo.585.1"> things I didn't </span><strong class="bold"><span class="koboSpan" id="kobo.586.1">understand</span></strong><span class="koboSpan" id="kobo.587.1">"
Chunk 3: "</span><strong class="bold"><span class="koboSpan" id="kobo.588.1">understand</span></strong><span class="koboSpan" id="kobo.589.1"> about the world </span><strong class="bold"><span class="koboSpan" id="kobo.590.1">when</span></strong><span class="koboSpan" id="kobo.591.1">"
Chunk 4: "</span><strong class="bold"><span class="koboSpan" id="kobo.592.1">when</span></strong><span class="koboSpan" id="kobo.593.1"> I was a </span><strong class="bold"><span class="koboSpan" id="kobo.594.1">child</span></strong><span class="koboSpan" id="kobo.595.1">"
Chunk 5: "</span><strong class="bold"><span class="koboSpan" id="kobo.596.1">child</span></strong><span class="koboSpan" id="kobo.597.1"> is the degree </span><strong class="bold"><span class="koboSpan" id="kobo.598.1">to</span></strong><span class="koboSpan" id="kobo.599.1">"
Chunk 6: "</span><strong class="bold"><span class="koboSpan" id="kobo.600.1">to</span></strong><span class="koboSpan" id="kobo.601.1"> which the returns </span><strong class="bold"><span class="koboSpan" id="kobo.602.1">for</span></strong><span class="koboSpan" id="kobo.603.1">"
Chunk 7: "</span><strong class="bold"><span class="koboSpan" id="kobo.604.1">for</span></strong><span class="koboSpan" id="kobo.605.1"> performance are </span><strong class="bold"><span class="koboSpan" id="kobo.606.1">superlinear</span></strong><span class="koboSpan" id="kobo.607.1">."</span></pre>			<p><span class="koboSpan" id="kobo.608.1">As you can see, each chunk overlaps with the next by </span><em class="italic"><span class="koboSpan" id="kobo.609.1">two words</span></em><span class="koboSpan" id="kobo.610.1">, helping to maintain context and </span><a id="_idIndexMarker1000"/><span class="koboSpan" id="kobo.611.1">prevent loss of meaning at chunk boundaries. </span><span class="koboSpan" id="kobo.611.2">Fixed-length chunking divides text into segments of a uniform size, but this method can sometimes fail to capture meaningful units of text, especially when dealing with natural language’s inherent variability. </span><span class="koboSpan" id="kobo.611.3">Transitioning to paragraph chunking, on the other hand, allows for a more contextually coherent approach by segmenting text based on its </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">natural structure.</span></span></p>
			<h3><span class="koboSpan" id="kobo.613.1">Implementing RecursiveCharacter chunking</span></h3>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.614.1">RecursiveCharacterTextSplitter</span></strong><span class="koboSpan" id="kobo.615.1"> is a </span><a id="_idIndexMarker1001"/><span class="koboSpan" id="kobo.616.1">sophisticated text-splitting tool designed</span><a id="_idIndexMarker1002"/><span class="koboSpan" id="kobo.617.1"> to handle more complex text segmentation tasks, especially when dealing with lengthy documents that need to be broken down into smaller, meaningful chunks. </span><span class="koboSpan" id="kobo.617.2">Unlike basic text splitters that simply cut text into fixed or variable-sized chunks, </span><strong class="source-inline"><span class="koboSpan" id="kobo.618.1">RecursiveCharacterTextSplitter</span></strong><span class="koboSpan" id="kobo.619.1"> uses a </span><a id="_idIndexMarker1003"/><span class="koboSpan" id="kobo.620.1">recursive approach to divide text, ensuring that each chunk is both contextually coherent and appropriately sized for processing by natural language models. </span><span class="koboSpan" id="kobo.620.2">Continuing from the review example, we will now demonstrate how to split a document into paragraphs using </span><strong class="source-inline"><span class="koboSpan" id="kobo.621.1">RecursiveCharacterTextSplitter</span></strong><span class="koboSpan" id="kobo.622.1">. </span><span class="koboSpan" id="kobo.622.2">You can find the full code </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/8.paragraph_chunking.py"><span class="No-Break"><span class="koboSpan" id="kobo.624.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/8.paragraph_chunking.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.625.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.626.1">We create a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.627.1">RecursiveCharacterTextSplitter</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.628.1"> instance:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.629.1">
text_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", " ", ""],
    chunk_size=200,
    chunk_overlap=0,
    length_function=len
    )</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.630.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.631.1">RecursiveCharacterTextSplitter</span></strong><span class="koboSpan" id="kobo.632.1"> instance is instantiated with </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">specific parameters:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.634.1">separators</span></strong><span class="koboSpan" id="kobo.635.1">: This is a list of separators used to split the text. </span><span class="koboSpan" id="kobo.635.2">Here, it includes double newlines (</span><strong class="source-inline"><span class="koboSpan" id="kobo.636.1">\n\n</span></strong><span class="koboSpan" id="kobo.637.1">), single newlines (</span><strong class="source-inline"><span class="koboSpan" id="kobo.638.1">\n</span></strong><span class="koboSpan" id="kobo.639.1">), spaces (</span><strong class="source-inline"> </strong><span class="koboSpan" id="kobo.640.1">), and empty strings (</span><strong class="source-inline"><span class="koboSpan" id="kobo.641.1">""</span></strong><span class="koboSpan" id="kobo.642.1">). </span><span class="koboSpan" id="kobo.642.2">This helps the splitter to use natural text boundaries and whitespace </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">for chunking.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">chunk_size</span></strong><span class="koboSpan" id="kobo.645.1">: This is the maximum size of each chunk, set to 200 characters. </span><span class="koboSpan" id="kobo.645.2">This means each chunk will be </span><em class="italic"><span class="koboSpan" id="kobo.646.1">up to</span></em><span class="koboSpan" id="kobo.647.1"> 200 </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">characters long.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.649.1">chunk_overlap</span></strong><span class="koboSpan" id="kobo.650.1">: This is the number of characters overlapping between adjacent chunks, set to 0. </span><span class="koboSpan" id="kobo.650.2">This means there is no overlap </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">between chunks.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.652.1">length_function</span></strong><span class="koboSpan" id="kobo.653.1">: This is a function used to measure the length of the text, set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.654.1">len</span></strong><span class="koboSpan" id="kobo.655.1">, which calculates the number of characters in </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">a string.</span></span></li></ul></li>				<li><span class="koboSpan" id="kobo.657.1">Split </span><a id="_idIndexMarker1004"/><span class="koboSpan" id="kobo.658.1">the</span><a id="_idIndexMarker1005"/><span class="koboSpan" id="kobo.659.1"> text </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">into chunks:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.661.1">
chunks = text_splitter.split_text(text_block)</span></pre></li>				<li><span class="koboSpan" id="kobo.662.1">Print the chunks. </span><span class="koboSpan" id="kobo.662.2">In this first one, the user is very satisfied with the smartphone camera, praising the sharpness and vibrant colors of the photos. </span><span class="koboSpan" id="kobo.662.3">However, the user is disappointed with the laptop’s performance, citing </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">frequent lags:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.664.1">
Chunk 1:
This smartphone has an excellent camera. </span><span class="koboSpan" id="kobo.664.2">The photos are sharp and the colors are vibrant. </span><span class="koboSpan" id="kobo.664.3">Overall, very satisfied with my purchase. </span><span class="koboSpan" id="kobo.664.4">I was disappointed with the laptop's performance. </span><span class="koboSpan" id="kobo.664.5">It frequently lags</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.665.1">The user is pleased with the blender, noting its effectiveness in making smoothies, its power, and its ease of cleaning. </span><span class="koboSpan" id="kobo.665.2">They consider it a good value for </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">the price:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.667.1">Chunk 2:
and the battery life is shorter than expected. </span><span class="koboSpan" id="kobo.667.2">The blender works great for making smoothies. </span><span class="koboSpan" id="kobo.667.3">It's powerful and easy to clean. </span><span class="koboSpan" id="kobo.667.4">Definitely worth the price. </span><span class="koboSpan" id="kobo.667.5">Customer support was unresponsive. </span><span class="koboSpan" id="kobo.667.6">I had to</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.668.1">The user had a negative experience with customer support, mentioning long wait times and unresolved issues. </span><span class="koboSpan" id="kobo.668.2">The user finds the book to be a fascinating read with </span><a id="_idIndexMarker1006"/><span class="koboSpan" id="kobo.669.1">an engaging storyline and </span><a id="_idIndexMarker1007"/><span class="koboSpan" id="kobo.670.1">well-developed characters, and they highly recommend it </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">to readers:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.672.1">Chunk 3:
wait a long time for a reply, and my issue was not resolved satisfactorily. </span><span class="koboSpan" id="kobo.672.2">The book is a fascinating read. </span><span class="koboSpan" id="kobo.672.3">The storyline is engaging and the characters are well-developed. </span><span class="koboSpan" id="kobo.672.4">Highly recommend to all</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.673.1">We are left with one </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">remaining word:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.675.1">Chunk 4:
Readers.</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.676.1">Now, these chunks are not perfect, but let’s understand how </span><strong class="source-inline"><span class="koboSpan" id="kobo.677.1">RecursiveCharacterTextSplitter</span></strong><span class="koboSpan" id="kobo.678.1"> works </span><a id="_idIndexMarker1008"/><span class="koboSpan" id="kobo.679.1">so that you can adjust it to your </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">use case:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.681.1">Chunk size target</span></strong><span class="koboSpan" id="kobo.682.1">: The splitter aims for chunks of about 200 characters, but this is a maximum rather than a strict requirement. </span><span class="koboSpan" id="kobo.682.2">It will try to create chunks as close to 200 characters as possible without exceeding </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">this limit.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.684.1">Recursive approach</span></strong><span class="koboSpan" id="kobo.685.1">: The recursive nature means it will apply these rules repeatedly, working its way through the separator list until it finds an appropriate </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">split point.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.687.1">Preserving semantic meaning</span></strong><span class="koboSpan" id="kobo.688.1">: By using this approach, the splitter attempts to keep semantically related content together. </span><span class="koboSpan" id="kobo.688.2">For example, it will try to avoid splitting in the middle of a paragraph or sentence </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">if possible.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.690.1">No overlap</span></strong><span class="koboSpan" id="kobo.691.1">: With </span><strong class="source-inline"><span class="koboSpan" id="kobo.692.1">chunk_overlap</span></strong><span class="koboSpan" id="kobo.693.1"> set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.694.1">0</span></strong><span class="koboSpan" id="kobo.695.1">, there’s no repetition of content between chunks. </span><span class="koboSpan" id="kobo.695.2">Each chunk </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">is distinct.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.697.1">Length function</span></strong><span class="koboSpan" id="kobo.698.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.699.1">len</span></strong><span class="koboSpan" id="kobo.700.1"> function is used to measure chunk size, meaning it’s counting characters rather </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">than tokens.</span></span></li>
			</ul>
			<p class="callout-heading"><span class="koboSpan" id="kobo.702.1">The length_function parameter</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.703.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.704.1">length_function</span></strong><span class="koboSpan" id="kobo.705.1"> parameter in </span><strong class="source-inline"><span class="koboSpan" id="kobo.706.1">RecursiveCharacterTextSplitter</span></strong><span class="koboSpan" id="kobo.707.1"> is a flexible option that allows you to define </span><em class="italic"><span class="koboSpan" id="kobo.708.1">how the length of text chunks is measured</span></em><span class="koboSpan" id="kobo.709.1">. </span><span class="koboSpan" id="kobo.709.2">While </span><strong class="source-inline"><span class="koboSpan" id="kobo.710.1">len</span></strong><span class="koboSpan" id="kobo.711.1"> is the default and most common choice, there are many other options, from token-based to word-based to </span><span class="No-Break"><span class="koboSpan" id="kobo.712.1">custom implementations.</span></span></p>
			<p><span class="koboSpan" id="kobo.713.1">While </span><a id="_idIndexMarker1009"/><span class="koboSpan" id="kobo.714.1">recursive</span><a id="_idIndexMarker1010"/><span class="koboSpan" id="kobo.715.1"> chunking focuses on creating chunks based on fixed sizes and natural separators, semantic chunking takes this a step further by grouping text based on its meaning and context. </span><span class="koboSpan" id="kobo.715.2">This method ensures that chunks are not only coherent in length but also semantically meaningful, improving the relevance and accuracy of downstream </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">NLP tasks.</span></span></p>
			<h3><span class="koboSpan" id="kobo.717.1">Implementing semantic chunking</span></h3>
			<p><span class="koboSpan" id="kobo.718.1">Semantic chunking</span><a id="_idIndexMarker1011"/><span class="koboSpan" id="kobo.719.1"> involves </span><a id="_idIndexMarker1012"/><span class="koboSpan" id="kobo.720.1">breaking text into chunks based on semantic meaning rather than just syntactic rules or fixed lengths. </span><span class="koboSpan" id="kobo.720.2">Behind the scenes, they use </span><em class="italic"><span class="koboSpan" id="kobo.721.1">embeddings</span></em><span class="koboSpan" id="kobo.722.1"> to group related sentences together (we will deep dive into embeddings in </span><a href="B19801_13.xhtml#_idTextAnchor302"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.723.1">Chapter 13</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.724.1">, Image and Audio Preprocessing with LLMs</span></em><span class="koboSpan" id="kobo.725.1">). </span><span class="koboSpan" id="kobo.725.2">We usually use semantic chunking for tasks requiring a deep understanding of context, such as question answering and </span><a id="_idIndexMarker1013"/><span class="koboSpan" id="kobo.726.1">thematic analysis. </span><span class="koboSpan" id="kobo.726.2">Let’s deep dive into the process behind </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">semantic chunking:</span></span></p>
			<ol>
				<li><strong class="bold"><span class="koboSpan" id="kobo.728.1">Text input</span></strong><span class="koboSpan" id="kobo.729.1">: The</span><a id="_idIndexMarker1014"/><span class="koboSpan" id="kobo.730.1"> process begins with a text input, which could be a document, a collection of sentences, or any textual data that needs to </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">be processed.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.732.1">Embedding generation</span></strong><span class="koboSpan" id="kobo.733.1">: Each</span><a id="_idIndexMarker1015"/><span class="koboSpan" id="kobo.734.1"> segment of the text (typically sentences or small groups of sentences (chunks)) is converted into a high-dimensional vector representation using embeddings. </span><span class="koboSpan" id="kobo.734.2">These embeddings are generated by pre-trained language models and the key to understand here is that these embeddings capture the semantic meaning of the text. </span><span class="koboSpan" id="kobo.734.3">In other words, we convert text into a numerical representation </span><em class="italic"><span class="koboSpan" id="kobo.735.1">that encodes </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.736.1">its meaning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">!</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.738.1">Similarity measurement</span></strong><span class="koboSpan" id="kobo.739.1">: The </span><a id="_idIndexMarker1016"/><span class="koboSpan" id="kobo.740.1">embeddings are then compared to measure the semantic similarity between different parts of the text. </span><span class="koboSpan" id="kobo.740.2">Techniques such as cosine similarity are often used to quantify how closely related different </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">segments are.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.742.1">Clustering</span></strong><span class="koboSpan" id="kobo.743.1">: Based</span><a id="_idIndexMarker1017"/><span class="koboSpan" id="kobo.744.1"> on the similarity scores, sentences </span><a id="_idIndexMarker1018"/><span class="koboSpan" id="kobo.745.1">or text segments are clustered together. </span><span class="koboSpan" id="kobo.745.2">The clustering algorithm groups sentences that are semantically similar into the same chunk. </span><span class="koboSpan" id="kobo.745.3">This ensures that each chunk maintains semantic coherence </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">and context.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.747.1">Chunk creation</span></strong><span class="koboSpan" id="kobo.748.1">: The</span><a id="_idIndexMarker1019"/><span class="koboSpan" id="kobo.749.1"> clustered sentences are then combined to form chunks. </span><span class="koboSpan" id="kobo.749.2">These chunks are designed to be semantically meaningful units of text, which can be more effectively processed by </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">NLP models.</span></span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.751.1">Let’s go back </span><a id="_idIndexMarker1020"/><span class="koboSpan" id="kobo.752.1">to the product review example and see what chunk we generate with semantic chunking. </span><span class="koboSpan" id="kobo.752.2">You can find the code </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/9.semantic_chunking.py"><span class="No-Break"><span class="koboSpan" id="kobo.754.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/9.semantic_chunking.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.755.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.756.1">Initialize </span><strong class="source-inline"><span class="koboSpan" id="kobo.757.1">SemanticChunker</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.758.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.759.1">HuggingFaceEmbeddings</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.760.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.761.1">
text_splitter = SemanticChunker(HuggingFaceEmbeddings())</span></pre></li>				<li><span class="koboSpan" id="kobo.762.1">Split the text </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">into chunks:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.764.1">
docs = text_splitter.create_documents([text_block])</span></pre></li>				<li><span class="koboSpan" id="kobo.765.1">Print </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">the chunks:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.767.1">
Chunk 1:
This smartphone has an excellent camera. </span><span class="koboSpan" id="kobo.767.2">The photos are sharp and the colors are vibrant. </span><span class="koboSpan" id="kobo.767.3">Overall, very satisfied with my purchase. </span><span class="koboSpan" id="kobo.767.4">I was disappointed with the laptop's performance. </span><span class="koboSpan" id="kobo.767.5">It frequently lags and the battery life is shorter than expected. </span><span class="koboSpan" id="kobo.767.6">The blender works great for making smoothies. </span><span class="koboSpan" id="kobo.767.7">It's powerful and easy to clean.
</span><span class="koboSpan" id="kobo.767.8">Chunk 2:
Definitely worth the price. </span><span class="koboSpan" id="kobo.767.9">Customer support was unresponsive. </span><span class="koboSpan" id="kobo.767.10">I had to wait a long time for a reply, and my issue was not resolved satisfactorily. </span><span class="koboSpan" id="kobo.767.11">The book is a fascinating read. </span><span class="koboSpan" id="kobo.767.12">The storyline is engaging and the characters are well-developed. </span><span class="koboSpan" id="kobo.767.13">Highly recommend to all readers.</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.768.1">Each chunk </span><a id="_idIndexMarker1021"/><span class="koboSpan" id="kobo.769.1">contains</span><a id="_idIndexMarker1022"/><span class="koboSpan" id="kobo.770.1"> related sentences that form a coherent segment. </span><span class="koboSpan" id="kobo.770.2">For example, chunk 1 discusses various product performances, while chunk 2 includes customer support experience and a book review. </span><span class="koboSpan" id="kobo.770.3">The chunks also maintain context within each segment, ensuring that related information is grouped together. </span><span class="koboSpan" id="kobo.770.4">A point for improvement is that chunk 1 includes reviews of different products (smartphone, laptop, and blender), and chunk 2 mixes a customer support experience with a book review, which could be seen as semantically unrelated. </span><span class="koboSpan" id="kobo.770.5">In this case, we could further split the text into smaller, more focused chunks to make it more coherent or/and tweak the parameters of the </span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">sematic chunker:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.772.1">
text_splitter = SemanticChunker(
    embeddings=embedding_model,
    buffer_size=200,
    add_start_index=True,
    breakpoint_threshold_type='percentile',
    breakpoint_threshold_amount=0.9,
    number_of_chunks=4,
    sentence_split_regex=r'\.|\n|\s'
)</span></pre>			<p><span class="koboSpan" id="kobo.773.1">You can find more details about these parameters in </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">the documentation:</span></span></p>
			<p><a href="https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html"><span class="No-Break"><span class="koboSpan" id="kobo.775.1">https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html</span></span></a></p>
			<p><span class="koboSpan" id="kobo.776.1">However, the steps to improve chunking in our case could look something </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">like this:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.778.1">Use different embedding models to see which provides the best embeddings for </span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">your text</span></span></li>
				<li><span class="koboSpan" id="kobo.780.1">Tweak the buffer size to find the right balance between chunk size </span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">and coherence</span></span></li>
				<li><span class="koboSpan" id="kobo.782.1">Adjust the threshold type and amount to optimize where chunks are split based on </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">semantic breaks</span></span></li>
				<li><span class="koboSpan" id="kobo.784.1">Customize the regular expression for sentence splitting to better fit the structure of </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">your text</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.786.1">Transitioning</span><a id="_idIndexMarker1023"/><span class="koboSpan" id="kobo.787.1"> from chunking to tokenization involves moving </span><a id="_idIndexMarker1024"/><span class="koboSpan" id="kobo.788.1">from a process where text is divided into larger, often syntactically significant segments (chunks) to a process where text is divided into smaller, more granular units (tokens). </span><span class="koboSpan" id="kobo.788.2">Let’s take a look at how </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.789.1">tokenization</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.790.1"> works.</span></span></p>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor291"/><span class="koboSpan" id="kobo.791.1">Tokenization</span></h1>
			<p><span class="koboSpan" id="kobo.792.1">Tokenization is the</span><a id="_idIndexMarker1025"/><span class="koboSpan" id="kobo.793.1"> process of breaking down a sequence of text into smaller units, or tokens, which can be words, subwords, or characters. </span><span class="koboSpan" id="kobo.793.2">This process is essential for converting text into a format suitable for </span><em class="italic"><span class="koboSpan" id="kobo.794.1">computational processing</span></em><span class="koboSpan" id="kobo.795.1">, enabling models to learn patterns at a </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">finer granularity.</span></span></p>
			<p><span class="koboSpan" id="kobo.797.1">Some key terms in the</span><a id="_idIndexMarker1026"/><span class="koboSpan" id="kobo.798.1"> Tokenization phase are </span><strong class="bold"><span class="koboSpan" id="kobo.799.1">vocabulary</span></strong><span class="koboSpan" id="kobo.800.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.801.1">Unique Identifiers</span></strong><span class="koboSpan" id="kobo.802.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.803.1">IDs</span></strong><span class="koboSpan" id="kobo.804.1">). </span><span class="koboSpan" id="kobo.804.2">The </span><a id="_idIndexMarker1027"/><span class="koboSpan" id="kobo.805.1">vocabulary is a fixed set of tokens </span><em class="italic"><span class="koboSpan" id="kobo.806.1">that a model knows</span></em><span class="koboSpan" id="kobo.807.1">. </span><span class="koboSpan" id="kobo.807.2">It can include words, subwords, punctuation, and special tokens (such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.808.1">[CLS]</span></strong><span class="koboSpan" id="kobo.809.1"> for classification, </span><strong class="source-inline"><span class="koboSpan" id="kobo.810.1">[SEP]</span></strong><span class="koboSpan" id="kobo.811.1"> for separation, etc.). </span><span class="koboSpan" id="kobo.811.2">Each token in the vocabulary is assigned an ID, which the model uses to represent the token internally. </span><span class="koboSpan" id="kobo.811.3">These IDs are integers and typically range from 0 to the size of the vocabulary </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">minus one.</span></span></p>
			<p><span class="koboSpan" id="kobo.813.1">Can all the words in the world fit into a vocabulary? </span><span class="koboSpan" id="kobo.813.2">The answer is </span><em class="italic"><span class="koboSpan" id="kobo.814.1">no</span></em><span class="koboSpan" id="kobo.815.1">! </span><span class="koboSpan" id="kobo.815.2">OOV words are words that are not present in the </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">model’s vocabulary.</span></span></p>
			<p><span class="koboSpan" id="kobo.817.1">Now that we know the main terms that are used, let’s explore the different types of tokenization and the challenges associated </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">with them.</span></span></p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor292"/><span class="koboSpan" id="kobo.819.1">Word tokenization</span></h2>
			<p><span class="koboSpan" id="kobo.820.1">Word tokenization</span><a id="_idIndexMarker1028"/><span class="koboSpan" id="kobo.821.1"> involves splitting text into </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">individual words.</span></span></p>
			<p><span class="koboSpan" id="kobo.823.1">For example, the sentence “Tokenization is crucial in NLP!” </span><span class="koboSpan" id="kobo.823.2">would be tokenized into </span><strong class="source-inline"><span class="koboSpan" id="kobo.824.1">["Tokenization", "is", "crucial", "in", "</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.825.1">NLP", "!"]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.827.1">Word tokenization</span><a id="_idIndexMarker1029"/><span class="koboSpan" id="kobo.828.1"> preserves whole words, which can be beneficial for tasks requiring word-level understanding. </span><span class="koboSpan" id="kobo.828.2">It works well for languages with clear word boundaries. </span><span class="koboSpan" id="kobo.828.3">It is a simple solution but can lead to problems with OOV words, especially in specialized domains such as medical texts and texts with a lot of misspellings. </span><span class="koboSpan" id="kobo.828.4">You can find the full code </span><span class="No-Break"><span class="koboSpan" id="kobo.829.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/10.word_tokenisation.py"><span class="No-Break"><span class="koboSpan" id="kobo.830.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/10.word_tokenisation.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.831.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.832.1">Let’s see a </span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">code example:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.834.1">Download the necessary NLTK data (run </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">this once):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.836.1">
nltk.download('punkt')</span></pre></li>				<li><span class="koboSpan" id="kobo.837.1">Take the following as </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">sample text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.839.1">
text = "The quick brown fox jumps over the lazy dog. </span><span class="koboSpan" id="kobo.839.2">It's unaffordable!"</span></pre></li>				<li><span class="koboSpan" id="kobo.840.1">Perform </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">word tokenization:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.842.1">
word_tokens = word_tokenize(text)</span></pre></li>				<li><span class="koboSpan" id="kobo.843.1">Print </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">the output:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.845.1">
Tokens:
['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', "'s", 'unaffordable', '!']</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.846.1">This type of word tokenization is useful when dealing with simple, well-formed text where each word is clearly separated by spaces and punctuation. </span><span class="koboSpan" id="kobo.846.2">It’s an easy method that aligns well with how humans perceive words. </span><span class="koboSpan" id="kobo.846.3">However, different forms of the same word (e.g., “run”, “running”, “ran”) are treated as separate tokens, which can dilute the model’s understanding. </span><span class="koboSpan" id="kobo.846.4">It can also result in a large vocabulary, especially for languages with rich </span><a id="_idIndexMarker1030"/><span class="koboSpan" id="kobo.847.1">morphology or many unique words. </span><span class="koboSpan" id="kobo.847.2">Finally, these models</span><a id="_idIndexMarker1031"/><span class="koboSpan" id="kobo.848.1"> struggle with words not seen during training, leading to </span><span class="No-Break"><span class="koboSpan" id="kobo.849.1">OOV tokens.</span></span></p>
			<p><span class="koboSpan" id="kobo.850.1">Given the limitations of word tokenization, subword tokenization methods have become popular. </span><span class="koboSpan" id="kobo.850.2">Subword tokenization strikes a balance between word-level and character-level tokenization, addressing many of the shortcomings </span><span class="No-Break"><span class="koboSpan" id="kobo.851.1">of both.</span></span></p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor293"/><span class="koboSpan" id="kobo.852.1">Subword tokenization</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.853.1">Subword tokenization</span></strong><span class="koboSpan" id="kobo.854.1"> splits</span><a id="_idIndexMarker1032"/><span class="koboSpan" id="kobo.855.1"> text into smaller units than </span><a id="_idIndexMarker1033"/><span class="koboSpan" id="kobo.856.1">words, typically subwords. </span><span class="koboSpan" id="kobo.856.2">By breaking the words into known subwords, it can handle OOV words. </span><span class="koboSpan" id="kobo.856.3">It reduces the vocabulary size and parameter count significantly. </span><span class="koboSpan" id="kobo.856.4">Let’s see the different options in the subword tokenization in the </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">following sections.</span></span></p>
			<h3><span class="koboSpan" id="kobo.858.1">Byte Pair Encoding (BPE)</span></h3>
			<p><span class="koboSpan" id="kobo.859.1">BPE starts</span><a id="_idIndexMarker1034"/><span class="koboSpan" id="kobo.860.1"> with individual </span><a id="_idIndexMarker1035"/><span class="koboSpan" id="kobo.861.1">characters and iteratively merges the most frequent pairs of tokens to create subwords. </span><span class="koboSpan" id="kobo.861.2">It was originally developed as a data compression algorithm but has been adapted for tokenization in NLP tasks. </span><span class="koboSpan" id="kobo.861.3">The process looks </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">like this:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.863.1">Start with a vocabulary of </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">individual characters.</span></span></li>
				<li><span class="koboSpan" id="kobo.865.1">Calculate the frequency of all character pairs in </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">the text.</span></span></li>
				<li><span class="koboSpan" id="kobo.867.1">Merge the most frequent pair of characters to form a </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">new token.</span></span></li>
				<li><span class="koboSpan" id="kobo.869.1">Repeat the process until the desired vocabulary size </span><span class="No-Break"><span class="koboSpan" id="kobo.870.1">is reached.</span></span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.871.1">This frequency-based merging strategy can be useful for languages with simpler morphological structures (e.g., English) or when a straightforward yet robust tokenization is needed. </span><span class="koboSpan" id="kobo.871.2">It is simple and computationally efficient due to the frequency-based merging. </span><span class="koboSpan" id="kobo.871.3">Let’s demonstrate an example of how to implement BPE for tokenization. </span><span class="koboSpan" id="kobo.871.4">You can find the full example </span><span class="No-Break"><span class="koboSpan" id="kobo.872.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/11.bpe_tokeniser.py"><span class="No-Break"><span class="koboSpan" id="kobo.873.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/11.bpe_tokeniser.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.874.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.875.1">Load</span><a id="_idIndexMarker1036"/><span class="koboSpan" id="kobo.876.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">pre-trained tokenizer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.878.1">
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')</span></pre></li>				<li><span class="koboSpan" id="kobo.879.1">Load the </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">sample text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.881.1">
text = "Tokenization in medical texts can include words like hyperlipidemia."</span></pre></li>				<li><span class="koboSpan" id="kobo.882.1">Tokenize</span><a id="_idIndexMarker1037"/> <span class="No-Break"><span class="koboSpan" id="kobo.883.1">the text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.884.1">
tokens = tokenizer.tokenize(text)</span></pre></li>				<li><span class="koboSpan" id="kobo.885.1">Convert tokens to </span><span class="No-Break"><span class="koboSpan" id="kobo.886.1">input IDs:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.887.1">
input_ids = tokenizer.convert_tokens_to_ids(tokens)</span></pre></li>				<li><span class="koboSpan" id="kobo.888.1">Print the tokens and the IDs, as </span><span class="No-Break"><span class="koboSpan" id="kobo.889.1">shown here:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.890.1">
Tokens: ['Token', 'ization', '</span><strong class="bold"><span class="koboSpan" id="kobo.891.1">Ġ</span></strong><span class="koboSpan" id="kobo.892.1">in', '</span><strong class="bold"><span class="koboSpan" id="kobo.893.1">Ġ</span></strong><span class="koboSpan" id="kobo.894.1">medical', '</span><strong class="bold"><span class="koboSpan" id="kobo.895.1">Ġ</span></strong><span class="koboSpan" id="kobo.896.1">texts', '</span><strong class="bold"><span class="koboSpan" id="kobo.897.1">Ġ</span></strong><span class="koboSpan" id="kobo.898.1">can', '</span><strong class="bold"><span class="koboSpan" id="kobo.899.1">Ġ</span></strong><span class="koboSpan" id="kobo.900.1">include', '</span><strong class="bold"><span class="koboSpan" id="kobo.901.1">Ġ</span></strong><span class="koboSpan" id="kobo.902.1">words', '</span><strong class="bold"><span class="koboSpan" id="kobo.903.1">Ġ</span></strong><span class="koboSpan" id="kobo.904.1">like', '</span><strong class="bold"><span class="koboSpan" id="kobo.905.1">Ġ</span></strong><span class="koboSpan" id="kobo.906.1">hyper', 'lip', 'idem', 'ia', '.']
Input IDs: [21920, 3666, 287, 1400, 1562, 649, 4551, 3545, 588, 20424, 3182, 1069, 257, 13]</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.907.1">The special character “Ġ” in the tokenization output has a specific meaning in the context of BPE tokenization. </span><span class="koboSpan" id="kobo.907.2">It indicates that the token following it originally had a preceding space or was at the beginning of the text, so it allows for preserving information about word boundaries and spacing in the </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">original text.</span></span></p>
			<p><span class="koboSpan" id="kobo.909.1">Let’s explain the output </span><span class="No-Break"><span class="koboSpan" id="kobo.910.1">we see:</span></span></p>
			<ul>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.911.1">Token</span></strong><span class="koboSpan" id="kobo.912.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.913.1">ization</span></strong><span class="koboSpan" id="kobo.914.1">: These are subwords of “Tokenization”, split without a “Ġ” because </span><em class="italic"><span class="koboSpan" id="kobo.915.1">they’re part of the </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.916.1">same word</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.917.1">.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.918.1">Ġ</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.919.1">in</span></strong><span class="koboSpan" id="kobo.920.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.921.1">Ġ</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.922.1">medical</span></strong><span class="koboSpan" id="kobo.923.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.924.1">Ġ</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.925.1">texts</span></strong><span class="koboSpan" id="kobo.926.1">, and so on: These tokens start with </span><strong class="bold"><span class="koboSpan" id="kobo.927.1">Ġ</span></strong><span class="koboSpan" id="kobo.928.1">, indicating they were </span><em class="italic"><span class="koboSpan" id="kobo.929.1">separate</span></em><span class="koboSpan" id="kobo.930.1"> words in the </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">original text.</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.932.1">hyper</span></strong><span class="koboSpan" id="kobo.933.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.934.1">lip</span></strong><span class="koboSpan" id="kobo.935.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.936.1">id</span></strong><span class="koboSpan" id="kobo.937.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.938.1">emia</span></strong><span class="koboSpan" id="kobo.939.1">: These are subwords of “hyperlipidemia”, a medical term. </span><span class="koboSpan" id="kobo.939.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.940.1">Ġ</span></strong><span class="koboSpan" id="kobo.941.1"> before </span><strong class="source-inline"><span class="koboSpan" id="kobo.942.1">hyper</span></strong><span class="koboSpan" id="kobo.943.1"> shows it’s a new word, while the subsequent subwords don’t have </span><strong class="bold"><span class="koboSpan" id="kobo.944.1">Ġ</span></strong><span class="koboSpan" id="kobo.945.1"> as they’re part of the same word. </span><span class="koboSpan" id="kobo.945.2">In medical terminology, many </span><a id="_idIndexMarker1038"/><span class="koboSpan" id="kobo.946.1">words are compounds or have specific prefixes or suffixes. </span><span class="koboSpan" id="kobo.946.2">The BPE tokenization helps break these down into meaningful subunits. </span><span class="koboSpan" id="kobo.946.3">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.947.1">hyperlipidemia</span></strong><span class="koboSpan" id="kobo.948.1"> is broken into </span><strong class="source-inline"><span class="koboSpan" id="kobo.949.1">hyper</span></strong><span class="koboSpan" id="kobo.950.1"> (prefix meaning </span><em class="italic"><span class="koboSpan" id="kobo.951.1">excessive</span></em><span class="koboSpan" id="kobo.952.1">), </span><strong class="source-inline"><span class="koboSpan" id="kobo.953.1">lip</span></strong><span class="koboSpan" id="kobo.954.1"> (related to fats), </span><strong class="source-inline"><span class="koboSpan" id="kobo.955.1">id</span></strong><span class="koboSpan" id="kobo.956.1"> (connecting element), and </span><strong class="source-inline"><span class="koboSpan" id="kobo.957.1">emia</span></strong><span class="koboSpan" id="kobo.958.1"> (suffix meaning </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.959.1">blood condition</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">).</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.961.1">Having explored </span><a id="_idIndexMarker1039"/><span class="koboSpan" id="kobo.962.1">BPE tokenization and its impact on text processing, we now turn our attention to WordPiece tokenization, another powerful method that further refines the handling of subword units in </span><span class="No-Break"><span class="koboSpan" id="kobo.963.1">NLP tasks.</span></span></p>
			<h3><span class="koboSpan" id="kobo.964.1">WordPiece tokenization</span></h3>
			<p><span class="koboSpan" id="kobo.965.1">WordPiece, used </span><a id="_idIndexMarker1040"/><span class="koboSpan" id="kobo.966.1">by BERT, starts with a base</span><a id="_idIndexMarker1041"/><span class="koboSpan" id="kobo.967.1"> vocabulary of characters, and iteratively adds the most frequent subword units. </span><span class="koboSpan" id="kobo.967.2">The process looks </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">like this:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.969.1">Start with a base vocabulary of individual characters and a special token for </span><span class="No-Break"><span class="koboSpan" id="kobo.970.1">unknown words.</span></span></li>
				<li><span class="koboSpan" id="kobo.971.1">Iteratively merge the most frequent pairs of tokens (starting with characters) to form new tokens until a predefined vocabulary size </span><span class="No-Break"><span class="koboSpan" id="kobo.972.1">is reached.</span></span></li>
				<li><span class="koboSpan" id="kobo.973.1">For any given word, the longest matching subword units from the vocabulary are used. </span><span class="koboSpan" id="kobo.973.2">This process is known</span><a id="_idIndexMarker1042"/><span class="koboSpan" id="kobo.974.1"> as </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.975.1">maximum matching</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.976.1">.</span></span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.977.1">WordPiece tokenization is effective for languages with complex word structures (e.g., Korean and Japanese) and when handling a diverse vocabulary efficiently is crucial. </span><span class="koboSpan" id="kobo.977.2">Its effectiveness comes from the fact that the merges are chosen based on maximizing the likelihood, leading to potentially more meaningful subwords. </span><span class="koboSpan" id="kobo.977.3">However, everything comes with a cost, and in this case, it is more computationally intensive due to the likelihood maximization step. </span><span class="koboSpan" id="kobo.977.4">Let’s have a look at a code example of how to perform WordPiece tokenization using the BERT tokenizer. </span><span class="koboSpan" id="kobo.977.5">You can find the full code </span><span class="No-Break"><span class="koboSpan" id="kobo.978.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/12.tokenisation_wordpiece.py"><span class="No-Break"><span class="koboSpan" id="kobo.979.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/12.tokenisation_wordpiece.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.980.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.981.1">Load the </span><a id="_idIndexMarker1043"/><span class="No-Break"><span class="koboSpan" id="kobo.982.1">pre-trained tokenizer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.983.1">
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')</span></pre></li>				<li><span class="koboSpan" id="kobo.984.1">Take some </span><span class="No-Break"><span class="koboSpan" id="kobo.985.1">sample text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.986.1">
text = "Tokenization in medical texts can include words like hyperlipidemia."</span></pre></li>				<li><span class="koboSpan" id="kobo.987.1">Tokenize </span><a id="_idIndexMarker1044"/><span class="koboSpan" id="kobo.988.1">the text. </span><span class="koboSpan" id="kobo.988.2">This method splits the input text into WordPiece tokens. </span><span class="koboSpan" id="kobo.988.3">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.989.1">unaffordable</span></strong><span class="koboSpan" id="kobo.990.1"> is broken down into </span><strong class="source-inline"><span class="koboSpan" id="kobo.991.1">un</span></strong><span class="koboSpan" id="kobo.992.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.993.1">##</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.994.1">afford</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.995.1">, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.996.1">##able</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.997.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.998.1">
tokens = tokenizer.tokenize(text)</span></pre></li>				<li><span class="koboSpan" id="kobo.999.1">Convert tokens to </span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">input IDs:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1001.1">
input_ids = tokenizer.convert_tokens_to_ids(tokens)
Tokens:
['token', '##ization', 'in', 'medical', 'texts', 'can', 'include', 'words', 'like', 'hyper', '##lip', '##idem', '##ia']
Input IDs:
[19204, 10859, 1999, 2966, 4524, 2064, 2421, 2540, 2066, 15088, 17750, 28285, 3676]</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1002.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1003.1">##</span></strong><span class="koboSpan" id="kobo.1004.1"> prefix is used to denote that the token is a continuation of the previous token. </span><span class="koboSpan" id="kobo.1004.2">Thus, it helps in reconstructing the original word by indicating that the token should be appended to the preceding token without </span><span class="No-Break"><span class="koboSpan" id="kobo.1005.1">a space.</span></span></p>
			<p><span class="koboSpan" id="kobo.1006.1">After examining tokenization methods such as BPE and WordPiece, it is crucial to consider how tokenizers can be tailored to handle specialized data, such as medical texts, to ensure precise and contextually relevant processing in these </span><span class="No-Break"><span class="koboSpan" id="kobo.1007.1">specific domains.</span></span></p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor294"/><span class="koboSpan" id="kobo.1008.1">Domain-specific data</span></h2>
			<p><span class="koboSpan" id="kobo.1009.1">When </span><a id="_idIndexMarker1045"/><span class="koboSpan" id="kobo.1010.1">working with domain-specific data such as medical texts, it’s crucial to ensure that the tokenizer can handle specialized vocabulary effectively. </span><span class="koboSpan" id="kobo.1010.2">When a domain has a high frequency of unique terms or specialized vocabulary, standard tokenizers may not perform optimally. </span><span class="koboSpan" id="kobo.1010.3">In this case, domain-specific tokenizers can better capture the nuances and terminology of the field, leading to improved model performance. </span><span class="koboSpan" id="kobo.1010.4">When you are faced with this challenge, there are some </span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">options available:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.1012.1">Train a tokenizer on a corpus of domain-specific texts to create a vocabulary that includes </span><span class="No-Break"><span class="koboSpan" id="kobo.1013.1">specialized terms</span></span></li>
				<li><span class="koboSpan" id="kobo.1014.1">Consider extending existing tokenizers with domain-specific tokens instead of training </span><span class="No-Break"><span class="koboSpan" id="kobo.1015.1">from scratch</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1016.1">However, how can you know that you need to go the extra mile and tune the tokenizer on the dataset? </span><span class="koboSpan" id="kobo.1016.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">find out.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1018.1">Evaluating the need for specialized tokenizers</span></h3>
			<p><span class="koboSpan" id="kobo.1019.1">As we explained, when </span><a id="_idIndexMarker1046"/><span class="koboSpan" id="kobo.1020.1">working with domain-specific data, such as medical texts, it’s essential to evaluate whether a specialized tokenizer is needed to ensure accurate and contextually relevant processing. </span><span class="koboSpan" id="kobo.1020.2">Let’s have a look at several key factors </span><span class="No-Break"><span class="koboSpan" id="kobo.1021.1">to consider:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1022.1">Analyze OOV rate</span></strong><span class="koboSpan" id="kobo.1023.1">: Determine the percentage of words in your domain-specific corpus that are not included in the standard tokenizer’s vocabulary. </span><span class="koboSpan" id="kobo.1023.2">A high OOV rate suggests that many important terms in your domain are not being recognized, highlighting the need for a specialized tokenizer to better handle the </span><span class="No-Break"><span class="koboSpan" id="kobo.1024.1">unique vocabulary.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1025.1">Examine tokenization quality</span></strong><span class="koboSpan" id="kobo.1026.1">: Check how standard tokenizers split domain-specific terms by manually reviewing sample tokenizations. </span><span class="koboSpan" id="kobo.1026.2">If crucial terms, such as medical terminology, are frequently broken into meaningless subwords, this indicates that the tokenizer is not well-suited for the domain and may </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">require customization.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1028.1">Compression ratio</span></strong><span class="koboSpan" id="kobo.1029.1">: Measure the average number of tokens per sentence using both standard and domain-specific tokenizers. </span><span class="koboSpan" id="kobo.1029.2">A significantly lower ratio with a domain-specific tokenizer suggests it is more efficient at compressing and representing domain knowledge, reducing redundancy, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1030.1">improving performance.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1031.1">For instance, in a </span><a id="_idIndexMarker1047"/><span class="koboSpan" id="kobo.1032.1">medical corpus, terms such as </span><em class="italic"><span class="koboSpan" id="kobo.1033.1">myocardial infarction</span></em><span class="koboSpan" id="kobo.1034.1"> might be tokenized as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1035.1">myo</span></strong><span class="koboSpan" id="kobo.1036.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1037.1">cardial</span></strong><span class="koboSpan" id="kobo.1038.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1039.1">infarction</span></strong><span class="koboSpan" id="kobo.1040.1"> by a standard tokenizer, leading to a loss of meaningful context. </span><span class="koboSpan" id="kobo.1040.2">A specialized medical tokenizer, however, might recognize </span><em class="italic"><span class="koboSpan" id="kobo.1041.1">myocardial infarction</span></em><span class="koboSpan" id="kobo.1042.1"> as a single term, preserving its meaning and improving the quality of downstream tasks such as entity recognition and text generation. </span><span class="koboSpan" id="kobo.1042.2">Similarly, if a standard tokenizer results in an OOV rate of 15% compared to just 3% with a specialized tokenizer, it clearly indicates the need for a tailored approach. </span><span class="koboSpan" id="kobo.1042.3">Lastly, if the compression ratio using a standard tokenizer is 1.8 tokens per sentence versus 1.2 tokens with a specialized tokenizer, it shows that the specialized tokenizer is more efficient and effective in capturing the </span><span class="No-Break"><span class="koboSpan" id="kobo.1043.1">domain-specific nuances.</span></span></p>
			<p><span class="koboSpan" id="kobo.1044.1">Let’s implement a small application to evaluate the tokenizers for different medical data. </span><span class="koboSpan" id="kobo.1044.2">The code for the example is available </span><span class="No-Break"><span class="koboSpan" id="kobo.1045.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/13.specialised_tokenisers.py"><span class="No-Break"><span class="koboSpan" id="kobo.1046.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/13.specialised_tokenisers.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1047.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1048.1">Initialize Stanza for </span><span class="No-Break"><span class="koboSpan" id="kobo.1049.1">biomedical text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1050.1">
stanza.download('en', package='mimic', processors='tokenize')
nlp = stanza.Pipeline('en', package='mimic', processors='tokenize')</span></pre></li>				<li><span class="koboSpan" id="kobo.1051.1">Initialize the standard </span><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">GPT-2 tokenizer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1053.1">
standard_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")</span></pre></li>				<li><span class="koboSpan" id="kobo.1054.1">Set </span><strong class="source-inline"><span class="koboSpan" id="kobo.1055.1">pad_token</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1056.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1057.1">eos_token</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1058.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1059.1">
standard_tokenizer.pad_token = standard_tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained("gpt2")</span></pre></li>				<li><span class="koboSpan" id="kobo.1060.1">Set </span><strong class="source-inline"><span class="koboSpan" id="kobo.1061.1">pad_token_id</span></strong><span class="koboSpan" id="kobo.1062.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.1063.1">the model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1064.1">
model.config.pad_token_id = model.config.eos_token_id</span></pre></li>				<li><span class="koboSpan" id="kobo.1065.1">A sample </span><a id="_idIndexMarker1048"/><span class="koboSpan" id="kobo.1066.1">medical corpus consisting of sentences related to myocardial infarction and heart conditions </span><span class="No-Break"><span class="koboSpan" id="kobo.1067.1">is defined:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1068.1">
corpus = [
  "The patient suffered a myocardial infarction.",
  "Early detection of heart attack is crucial.",
  "Treatment for myocardial infarction includes medication.",
  "Patients with heart conditions require regular check-ups.",
  "Myocardial infarction can lead to severe complications."
</span><span class="koboSpan" id="kobo.1068.2">]</span></pre></li>				<li><span class="koboSpan" id="kobo.1069.1">The following </span><strong class="source-inline"><span class="koboSpan" id="kobo.1070.1">stanza_tokenize</span></strong><span class="koboSpan" id="kobo.1071.1"> function uses the Stanza pipeline to tokenize the text and return a list </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">of tokens:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1073.1">
def stanza_tokenize(text):
    doc = nlp(text)
    tokens = [word.text for sent in doc.sentences for word in sent.words]
    return tokens</span></pre></li>				<li><span class="koboSpan" id="kobo.1074.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1075.1">calculate_oov_and_compression</span></strong><span class="koboSpan" id="kobo.1076.1"> function tokenizes each sentence in the corpus and calculates the OOV rate, as well as the average tokens per sentence, and returns all tokens. </span><span class="koboSpan" id="kobo.1076.2">For standard tokenizers, it checks whether tokens are in the vocabulary, while for Stanza, it does not check OOV </span><span class="No-Break"><span class="koboSpan" id="kobo.1077.1">tokens explicitly:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1078.1">
def calculate_oov_and_compression(corpus, tokenizer):
    oov_count = 0
    total_tokens = 0
    all_tokens = []
    for sentence in corpus:
        tokens = tokenizer.tokenize(sentence) if hasattr(tokenizer, 'tokenize') else stanza_tokenize(sentence)
        all_tokens.extend(tokens)
        total_tokens += len(tokens)
        oov_count += tokens.count(tokenizer.oov_token) if hasattr(tokenizer, 'oov_token') else 0
    oov_rate = (oov_count / total_tokens) * 100 if total_tokens &gt; 0 else 0
    avg_tokens_per_sentence = total_tokens / len(corpus)
return oov_rate, avg_tokens_per_sentence, all_tokens</span></pre></li>				<li><span class="koboSpan" id="kobo.1079.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1080.1">analyze_token_utilization</span></strong><span class="koboSpan" id="kobo.1081.1"> function calculates the frequency of each token</span><a id="_idIndexMarker1049"/><span class="koboSpan" id="kobo.1082.1"> in the corpus and returns a dictionary of token </span><span class="No-Break"><span class="koboSpan" id="kobo.1083.1">utilization percentages:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1084.1">
def analyze_token_utilization(tokens):
    token_counts = Counter(tokens)
    total_tokens = len(tokens)
    utilization = {token: count / total_tokens for token, count in token_counts.items()}
    return utilization</span></pre></li>				<li><span class="koboSpan" id="kobo.1085.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1086.1">calculate_perplexity</span></strong><span class="koboSpan" id="kobo.1087.1"> function calculates the perplexity of the model on the given text, which is a measure of how well the model predicts </span><span class="No-Break"><span class="koboSpan" id="kobo.1088.1">the sample:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1089.1">
def calculate_perplexity(tokenizer, model, text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(inputs, labels=inputs["input_ids"])
    return torch.exp(outputs.loss).item()</span></pre></li>				<li><span class="koboSpan" id="kobo.1090.1">The following</span><a id="_idIndexMarker1050"/><span class="koboSpan" id="kobo.1091.1"> script evaluates both the standard GPT-2 tokenizer and the Stanza medical tokenizer by calculating the OOV rate, average tokens per sentence, token utilization, and perplexity. </span><span class="koboSpan" id="kobo.1091.2">Finally, it prints the results for each tokenizer and compares their performance on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1092.1">myocardial </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1093.1">infarction</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1094.1"> term:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1095.1">
for tokenizer_name, tokenizer in [("Standard GPT-2", standard_tokenizer), ("Stanza Medical", stanza_tokenize)]:
    oov_rate, avg_tokens, all_tokens = calculate_oov_and_compression(corpus, tokenizer)
    utilization = analyze_token_utilization(all_tokens)
    print(f"\n{tokenizer_name} Tokenizer:")
    print(f"OOV Rate: {oov_rate:.2f}%")
    print(f"Average Tokens per Sentence: {avg_tokens:.2f}")
    print("Top 5 Most Used Tokens:")
    for token, freq in sorted(utilization.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f" {token}: {freq:.2%}")</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1096.1">Let’s see </span><a id="_idIndexMarker1051"/><span class="koboSpan" id="kobo.1097.1">the results</span><a id="_idIndexMarker1052"/><span class="koboSpan" id="kobo.1098.1"> of the two tokenizers </span><a id="_idIndexMarker1053"/><span class="koboSpan" id="kobo.1099.1">presented in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">following table:</span></span></p>
			<table id="table001-9" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1101.1">Metric</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1102.1">Standard </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1103.1">GPT-2 tokenizer</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1104.1">Stanza </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1105.1">Medical tokenizer</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1106.1">OOV rate</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1107.1">0.00%</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1108.1">0.00%</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1109.1">Average tokens </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1110.1">per sentence</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1111.1">10.80</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1112.1">7.60</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="5">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1113.1">Top five most </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1114.1">used tokens</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1115.1">. </span><span class="koboSpan" id="kobo.1115.2">: </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1116.1">9.26%</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1117.1">. </span><span class="koboSpan" id="kobo.1117.2">: </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1118.1">13.16%</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1119.1">ocard : </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1120.1">5.56%</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1121.1">infarction : </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1122.1">7.89%</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1123.1">ial : </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1124.1">5.56%</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1125.1">myocardial : </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1126.1">5.26%</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1127.1">Ġinf : </span><span class="No-Break"><span class="koboSpan" id="kobo.1128.1">5.56%</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1129.1">heart : </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1130.1">5.26%</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1131.1">ar : </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1132.1">5.56%</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1133.1">The : </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1134.1">2.63%</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1135.1">Table 12.1 – Comparison of GPT-2 tokenizer and specialized medical tokenizer</span></p>
			<p><span class="koboSpan" id="kobo.1136.1">As we see in the table, both tokenizers show an OOV rate of 0.00%, indicating that all tokens in the corpus are recognized by both tokenizers. </span><span class="koboSpan" id="kobo.1136.2">The Stanza Medical tokenizer has a lower average rate of tokens per sentence (7.60) compared to the Standard GPT-2 tokenizer (10.80). </span><span class="koboSpan" id="kobo.1136.3">This suggests that the Stanza Medical tokenizer is more efficient at compressing domain-specific terms into fewer tokens. </span><span class="koboSpan" id="kobo.1136.4">The Standard GPT-2 tokenizer splits meaningful medical terms into smaller subwords, leading to less meaningful token utilization (e.g., </span><strong class="source-inline"><span class="koboSpan" id="kobo.1137.1">ocard</span></strong><span class="koboSpan" id="kobo.1138.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1139.1">ial</span></strong><span class="koboSpan" id="kobo.1140.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1141.1">Ġ</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1142.1">inf</span></strong><span class="koboSpan" id="kobo.1143.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1144.1">ar</span></strong><span class="koboSpan" id="kobo.1145.1">). </span><span class="koboSpan" id="kobo.1145.2">However, the Stanza Medical tokenizer maintains the integrity of medical terms (e.g., </span><strong class="source-inline"><span class="koboSpan" id="kobo.1146.1">infarction</span></strong><span class="koboSpan" id="kobo.1147.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1148.1">myocardial</span></strong><span class="koboSpan" id="kobo.1149.1">), making the tokens more meaningful and contextually relevant. </span><span class="koboSpan" id="kobo.1149.2">Based on the analysis, the Stanza Medical tokenizer should be preferred for medical text processing because of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1">following reasons:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.1151.1">It efficiently tokenizes domain-specific terms into </span><span class="No-Break"><span class="koboSpan" id="kobo.1152.1">fewer tokens</span></span></li>
				<li><span class="koboSpan" id="kobo.1153.1">It preserves the integrity and meaning of </span><span class="No-Break"><span class="koboSpan" id="kobo.1154.1">medical terms</span></span></li>
				<li><span class="koboSpan" id="kobo.1155.1">It provides more meaningful and contextually relevant tokens, which is crucial for tasks such as entity recognition and text generation in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1156.1">medical domain</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1157.1">The </span><a id="_idIndexMarker1054"/><span class="koboSpan" id="kobo.1158.1">Standard GPT-2 tokenizer, while</span><a id="_idIndexMarker1055"/><span class="koboSpan" id="kobo.1159.1"> useful for general text, splits medical terms into </span><a id="_idIndexMarker1056"/><span class="koboSpan" id="kobo.1160.1">subwords, leading to potential loss of context and meaning, making it less suitable for specialized </span><span class="No-Break"><span class="koboSpan" id="kobo.1161.1">medical text.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1162.1">Vocabulary size trade-offs</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1163.1">Larger vocabularies can capture more domain-specific terms but increase the model size and computational requirements. </span><span class="koboSpan" id="kobo.1163.2">Find a balance that adequately covers domain terminology without </span><span class="No-Break"><span class="koboSpan" id="kobo.1164.1">excessive growth.</span></span></p>
			<p><span class="koboSpan" id="kobo.1165.1">Having evaluated the performance of different tokenization methods, including their handling of OOV terms and their efficiency in compressing domain-specific knowledge, the next logical step is to explore how these tokenized outputs are transformed into meaningful numerical representations through embedding techniques. </span><span class="koboSpan" id="kobo.1165.2">This transition is crucial, as embeddings form the foundation of how models understand and process the </span><span class="No-Break"><span class="koboSpan" id="kobo.1166.1">tokenized text.</span></span></p>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor295"/><span class="koboSpan" id="kobo.1167.1">Turning tokens into embeddings</span></h1>
			<p><span class="koboSpan" id="kobo.1168.1">Embeddings</span><a id="_idIndexMarker1057"/><span class="koboSpan" id="kobo.1169.1"> are numerical representations of words, phrases, or entire </span><a id="_idIndexMarker1058"/><span class="koboSpan" id="kobo.1170.1">documents in a high-dimensional vector space. </span><span class="koboSpan" id="kobo.1170.2">Essentially, we represent words as arrays of numbers to capture their semantic meaning. </span><span class="koboSpan" id="kobo.1170.3">These numerical arrays aim to encode the underlying significance of words and sentences, allowing models to understand and process text in a meaningful way. </span><span class="koboSpan" id="kobo.1170.4">Let’s explore the process from tokenization </span><span class="No-Break"><span class="koboSpan" id="kobo.1171.1">to embedding.</span></span></p>
			<p><span class="koboSpan" id="kobo.1172.1">The process starts with tokenization, whereby text is split into manageable units called tokens. </span><span class="koboSpan" id="kobo.1172.2">For instance, the sentence “The cat sat on the mat” might be tokenized into individual words or subword units such as [“The”, “cat”, “sat”, “on”, “the”, “mat”]. </span><span class="koboSpan" id="kobo.1172.3">Once the text is tokenized, each token is mapped to an embedding vector using an embedding layer or lookup table. </span><span class="koboSpan" id="kobo.1172.4">This table is often initialized with random values and then trained to capture meaningful relationships between words. </span><span class="koboSpan" id="kobo.1172.5">For instance, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1173.1">cat</span></strong><span class="koboSpan" id="kobo.1174.1"> might be represented as a </span><span class="No-Break"><span class="koboSpan" id="kobo.1175.1">300-dimensional vector.</span></span></p>
			<p><span class="koboSpan" id="kobo.1176.1">Advanced models such </span><a id="_idIndexMarker1059"/><span class="koboSpan" id="kobo.1177.1">as transformers (e.g., BERT or GPT) generate contextual</span><a id="_idIndexMarker1060"/><span class="koboSpan" id="kobo.1178.1"> embeddings, where the vector representation of a word is influenced by its surrounding words. </span><span class="koboSpan" id="kobo.1178.2">This allows the model to understand nuances and context, such as distinguishing between “bank” in “river bank” versus “</span><span class="No-Break"><span class="koboSpan" id="kobo.1179.1">financial bank.”</span></span></p>
			<p><span class="koboSpan" id="kobo.1180.1">Let’s go and have a look at these models in </span><span class="No-Break"><span class="koboSpan" id="kobo.1181.1">more detail.</span></span></p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor296"/><span class="koboSpan" id="kobo.1182.1">BERT – Contextualized Embedding Models</span></h2>
			<p><span class="koboSpan" id="kobo.1183.1">BERT is a</span><a id="_idIndexMarker1061"/><span class="koboSpan" id="kobo.1184.1"> powerful NLP model developed by Google. </span><span class="koboSpan" id="kobo.1184.2">It belongs to the family of transformer-based models and is pre-trained on massive amounts of text data to learn contextualized representations </span><span class="No-Break"><span class="koboSpan" id="kobo.1185.1">of words.</span></span></p>
			<p><span class="koboSpan" id="kobo.1186.1">The </span><a id="_idIndexMarker1062"/><span class="koboSpan" id="kobo.1187.1">BERT embedding model is a </span><em class="italic"><span class="koboSpan" id="kobo.1188.1">component</span></em><span class="koboSpan" id="kobo.1189.1"> of the BERT architecture that generates contextualized word embeddings. </span><span class="koboSpan" id="kobo.1189.2">Unlike traditional word embeddings that assign a fixed vector representation to each word, BERT embeddings are context-dependent, capturing the meaning of words in the context of the entire sentence. </span><span class="koboSpan" id="kobo.1189.3">Here’s an explanation of how to use the BERT embedding </span><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">model </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/14.embedding_bert.py"><span class="No-Break"><span class="koboSpan" id="kobo.1191.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/14.embedding_bert.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1192.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1193.1">Load a pre-trained BERT model </span><span class="No-Break"><span class="koboSpan" id="kobo.1194.1">and tokenizer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1195.1">
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")</span></pre></li>				<li><span class="koboSpan" id="kobo.1196.1">Encode the </span><span class="No-Break"><span class="koboSpan" id="kobo.1197.1">input text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1198.1">
input_text = "BERT embeddings capture contextual information."
</span><span class="koboSpan" id="kobo.1198.2">inputs= tokenizer(input_text, return_tensors="pt")</span></pre></li>				<li><span class="koboSpan" id="kobo.1199.1">Obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.1200.1">BERT embeddings:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1201.1">
with torch.no_grad():
    outputs = model(inputs)</span></pre></li>				<li><span class="koboSpan" id="kobo.1202.1">Print </span><span class="No-Break"><span class="koboSpan" id="kobo.1203.1">the</span></span><span class="No-Break"><a id="_idIndexMarker1063"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1204.1"> embeddings:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1205.1">
print("Shape of the embeddings tensor:", last_hidden_states.shape)
</span><strong class="bold"><span class="koboSpan" id="kobo.1206.1">Shape of the embeddings tensor: torch.Size([1, 14, 768])</span></strong></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1207.1">The shape of the</span><a id="_idIndexMarker1064"/><span class="koboSpan" id="kobo.1208.1"> embeddings tensor will be (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1209.1">1</span></strong><span class="koboSpan" id="kobo.1210.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1211.1">sequence_length</span></strong><span class="koboSpan" id="kobo.1212.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1213.1">hidden_size</span></strong><span class="koboSpan" id="kobo.1214.1">). </span><span class="koboSpan" id="kobo.1214.2">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1215.1">sequence_length</span></strong><span class="koboSpan" id="kobo.1216.1"> is the number of tokens in the input sentence, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1217.1">hidden_size</span></strong><span class="koboSpan" id="kobo.1218.1"> is the size of the hidden states in the BERT model (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1219.1">768</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1220.1">for </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1221.1">bert-base-uncased</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1222.1">).</span></span></p>
			<p><span class="koboSpan" id="kobo.1223.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1224.1">[CLS]</span></strong><span class="koboSpan" id="kobo.1225.1"> token embedding represents the entire input sentence and is often used for classification tasks. </span><span class="koboSpan" id="kobo.1225.2">It’s the first token in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1226.1">output tensor:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1227.1">
CLS token embedding: [ 0.23148441 -0.32737488 ...  0.02315655]</span></pre>			<p><span class="koboSpan" id="kobo.1228.1">The embedding for the first actual word in the sentence represents the contextualized embedding for that specific word. </span><span class="koboSpan" id="kobo.1228.2">The embedding associated with the first actual word in a sentence is not just a static or isolated representation of that word. </span><span class="koboSpan" id="kobo.1228.3">Instead, it’s a “context-aware” or “contextualized” embedding, meaning it reflects how the word’s meaning is influenced by the surrounding words in the sentence. </span><span class="koboSpan" id="kobo.1228.4">In simpler terms, this embedding captures not only the word’s intrinsic meaning but also how that meaning changes based on the context provided by the other words around it. </span><span class="koboSpan" id="kobo.1228.5">This is a key feature of models such as BERT, which generate different embeddings for the same word depending on its usage in </span><span class="No-Break"><span class="koboSpan" id="kobo.1229.1">different contexts:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1230.1">
First word embedding: [ 0.00773875  0.24699381 ... </span><span class="koboSpan" id="kobo.1230.2">-0.09120814]</span></pre>			<p><span class="koboSpan" id="kobo.1231.1">The key thing to understand here is that we started with text and we have vectors or embeddings as outputs. </span><span class="koboSpan" id="kobo.1231.2">The tokenization step is happening behind the scenes when we use the tokenizer provided by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1232.1">transformers</span></strong><span class="koboSpan" id="kobo.1233.1"> library. </span><span class="koboSpan" id="kobo.1233.2">The tokenizer converts the input sentence into tokens and their corresponding token IDs, which are then passed to the BERT model. </span><span class="koboSpan" id="kobo.1233.3">Remember that each word in the sentence has its own embedding that reflects its meaning within the context of </span><span class="No-Break"><span class="koboSpan" id="kobo.1234.1">the sentence.</span></span></p>
			<p><span class="koboSpan" id="kobo.1235.1">BERT’s versatility has enabled it to perform exceptionally well across a wide array of NLP tasks. </span><span class="koboSpan" id="kobo.1235.2">However, the need for more efficient and task-specific embeddings has led to the development of </span><a id="_idIndexMarker1065"/><span class="koboSpan" id="kobo.1236.1">models such as </span><strong class="bold"><span class="koboSpan" id="kobo.1237.1">BAAI General Embedding</span></strong><span class="koboSpan" id="kobo.1238.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1239.1">BGE</span></strong><span class="koboSpan" id="kobo.1240.1">). </span><span class="koboSpan" id="kobo.1240.2">BGE is designed to</span><a id="_idIndexMarker1066"/><span class="koboSpan" id="kobo.1241.1"> be smaller and faster, providing high-quality embeddings optimized for tasks such as semantic similarity and </span><span class="No-Break"><span class="koboSpan" id="kobo.1242.1">information retrieval.</span></span></p>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor297"/><span class="koboSpan" id="kobo.1243.1">BGE</span></h2>
			<p><span class="koboSpan" id="kobo.1244.1">The </span><a id="_idIndexMarker1067"/><span class="koboSpan" id="kobo.1245.1">BAAI/bge-small-en model is part of a series of BGE models </span><a id="_idIndexMarker1068"/><span class="koboSpan" id="kobo.1246.1">developed by the </span><strong class="bold"><span class="koboSpan" id="kobo.1247.1">Beijing Academy of Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.1248.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1249.1">BAAI</span></strong><span class="koboSpan" id="kobo.1250.1">). </span><span class="koboSpan" id="kobo.1250.2">These models are designed for generating high-quality embeddings for text, typically used in various NLP tasks such as text classification, semantic search, </span><span class="No-Break"><span class="koboSpan" id="kobo.1251.1">and more.</span></span></p>
			<p><span class="koboSpan" id="kobo.1252.1">These models generate embeddings (vector representations) for text. </span><span class="koboSpan" id="kobo.1252.2">The embeddings capture the semantic meaning of the text, making them useful for tasks such as similarity search, clustering, and classification. </span><span class="koboSpan" id="kobo.1252.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1253.1">bge-small-en</span></strong><span class="koboSpan" id="kobo.1254.1"> model is a smaller, English-specific model in this series. </span><span class="koboSpan" id="kobo.1254.2">Let’s see an example. </span><span class="koboSpan" id="kobo.1254.3">The full code for this example is available </span><span class="No-Break"><span class="koboSpan" id="kobo.1255.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/15.embedding_bge.py"><span class="No-Break"><span class="koboSpan" id="kobo.1256.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/15.embedding_bge.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1257.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1258.1">Define the model name </span><span class="No-Break"><span class="koboSpan" id="kobo.1259.1">and parameters:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1260.1">
model_name = "BAAI/bge-small-en"
model_kwargs = {"device": "cpu"}
encode_kwargs = {"normalize_embeddings": True}</span></pre></li>				<li><span class="koboSpan" id="kobo.1261.1">Initialize the </span><span class="No-Break"><span class="koboSpan" id="kobo.1262.1">embeddings model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1263.1">
bge_embeddings = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)</span></pre></li>				<li><span class="koboSpan" id="kobo.1264.1">We sample a few sentences </span><span class="No-Break"><span class="koboSpan" id="kobo.1265.1">to embed:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1266.1">
sentences = [
    "The quick brown fox jumps over the lazy dog.",
    "I love machine learning and natural language processing."
</span><span class="koboSpan" id="kobo.1266.2">]</span></pre></li>				<li><span class="koboSpan" id="kobo.1267.1">Generate </span><a id="_idIndexMarker1069"/><span class="koboSpan" id="kobo.1268.1">embeddings for </span><span class="No-Break"><span class="koboSpan" id="kobo.1269.1">each sentence:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1270.1">
embeddings = [bge_embeddings.embed_query(sentence) for sentence in sentences]</span></pre></li>				<li><span class="koboSpan" id="kobo.1271.1">Print </span><span class="No-Break"><span class="koboSpan" id="kobo.1272.1">the </span></span><span class="No-Break"><a id="_idIndexMarker1070"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1273.1">embeddings:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1274.1">
[-0.07455343008041382, -0.004580824635922909, 0.021685084328055382, 0.06458176672458649, 0.020278634503483772]...
</span><span class="koboSpan" id="kobo.1274.2">Length of embedding: 384
Embedding for sentence 2: [-0.025911744683980942, 0.0050039878115057945, -0.011821565218269825, -0.020849423483014107, 0.06114110350608826]...</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1275.1">BGE models, such as bge-small-en, are designed to be smaller and more efficient for embedding generation tasks compared to larger, more general models such as BERT. </span><span class="koboSpan" id="kobo.1275.2">This efficiency translates to reduced memory usage and faster inference times, making BGE models particularly suitable for applications where computational resources are limited or where real-time processing is crucial. </span><span class="koboSpan" id="kobo.1275.3">While BERT is a versatile, general-purpose model capable of handling a wide range of NLP tasks, BGE models are specifically optimized for generating high-quality embeddings. </span><span class="koboSpan" id="kobo.1275.4">This optimization allows BGE models to provide comparable or even superior performance for specific tasks, such as semantic search and information retrieval, where the quality of embeddings is paramount. </span><span class="koboSpan" id="kobo.1275.5">By focusing on the precision and semantic richness of embeddings, BGE models leverage advanced techniques such as learned sparse embeddings, which combine the benefits of both dense and sparse representations. </span><span class="koboSpan" id="kobo.1275.6">This targeted optimization enables BGE models to excel in scenarios that demand nuanced text representation and efficient processing, making them a better choice for embedding-centric applications compared to the more generalized </span><span class="No-Break"><span class="koboSpan" id="kobo.1276.1">BERT model.</span></span></p>
			<p><span class="koboSpan" id="kobo.1277.1">Building on the success of both BERT and BGE, the introduction of </span><strong class="bold"><span class="koboSpan" id="kobo.1278.1">General Text Embeddings</span></strong><span class="koboSpan" id="kobo.1279.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1280.1">GTEs</span></strong><span class="koboSpan" id="kobo.1281.1">) marks another significant step forward. </span><span class="koboSpan" id="kobo.1281.2">GTE models are specifically fine-tuned to deliver robust and efficient embeddings tailored for various </span><span class="No-Break"><span class="koboSpan" id="kobo.1282.1">text-related applications.</span></span></p>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor298"/><span class="koboSpan" id="kobo.1283.1">GTE</span></h2>
			<p><span class="koboSpan" id="kobo.1284.1">GTEs</span><a id="_idIndexMarker1071"/><span class="koboSpan" id="kobo.1285.1"> represent</span><a id="_idIndexMarker1072"/><span class="koboSpan" id="kobo.1286.1"> the next generation of embedding models, designed to address the growing demand for specialized and efficient text representations. </span><span class="koboSpan" id="kobo.1286.2">GTE models excel in providing high-quality embeddings for specific tasks such as semantic similarity, clustering, and information retrieval. </span><span class="koboSpan" id="kobo.1286.3">Let’s see them in action. </span><span class="koboSpan" id="kobo.1286.4">The full code is available </span><span class="No-Break"><span class="koboSpan" id="kobo.1287.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/16.embedding_gte.py"><span class="No-Break"><span class="koboSpan" id="kobo.1288.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter12/16.embedding_gte.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1289.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1290.1">Load the </span><span class="No-Break"><span class="koboSpan" id="kobo.1291.1">GTE-base model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1292.1">
model = SentenceTransformer('thenlper/gte-base')</span></pre></li>				<li><span class="koboSpan" id="kobo.1293.1">We sample a few random texts </span><span class="No-Break"><span class="koboSpan" id="kobo.1294.1">to embed:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1295.1">
texts = [
    "The quick brown fox jumps over the lazy dog.",
    "I love machine learning and natural language processing.",
    "Embeddings are useful for many NLP tasks."
</span><span class="koboSpan" id="kobo.1295.2">]</span></pre></li>				<li><span class="koboSpan" id="kobo.1296.1">Generate </span><span class="No-Break"><span class="koboSpan" id="kobo.1297.1">the embeddings:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1298.1">
embeddings = model.encode(texts</span></pre></li>				<li><span class="koboSpan" id="kobo.1299.1">Print the shape of </span><span class="No-Break"><span class="koboSpan" id="kobo.1300.1">the embeddings:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1301.1">
print(f"Shape of embeddings: {embeddings.shape}")
Shape of embeddings: (3, 768)</span></pre></li>				<li><span class="koboSpan" id="kobo.1302.1">Print the first few values of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1303.1">first embedding:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1304.1">
[-0.02376037 -0.04635307  0.02570779  0.01606994  0.05594607]</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1305.1">One of </span><a id="_idIndexMarker1073"/><span class="koboSpan" id="kobo.1306.1">the standout features of GTE is its efficiency. </span><span class="koboSpan" id="kobo.1306.2">By maintaining a smaller model size and faster inference times, GTE is well-suited for real-time applications and environments </span><a id="_idIndexMarker1074"/><span class="koboSpan" id="kobo.1307.1">with constrained computational resources. </span><span class="koboSpan" id="kobo.1307.2">This efficiency does not come at the cost of performance; GTE models continue to deliver exceptional results across various text-processing tasks. </span><span class="koboSpan" id="kobo.1307.3">However, their reduced complexity handling can be a limitation, as the smaller model size may impede their ability to process highly intricate or nuanced texts effectively. </span><span class="koboSpan" id="kobo.1307.4">This could result in the less accurate capture of subtle contextual details, affecting performance in more complex scenarios. </span><span class="koboSpan" id="kobo.1307.5">Additionally, a GTE’s focus on efficiency might lead to diminished generalization capabilities; although it excels in specific tasks, it may struggle to adapt to a wide variety of diverse or less common language inputs. </span><span class="koboSpan" id="kobo.1307.6">Furthermore, the model’s smaller size may constrain its fine-tuning flexibility, potentially limiting its ability to adapt to specialized tasks or domains due to a reduced capacity for learning and storing intricate patterns specific to </span><span class="No-Break"><span class="koboSpan" id="kobo.1308.1">niche applications.</span></span></p>
			<h2 id="_idParaDest-262"><a id="_idTextAnchor299"/><span class="koboSpan" id="kobo.1309.1">Selecting the right embedding model</span></h2>
			<p><span class="koboSpan" id="kobo.1310.1">When </span><a id="_idIndexMarker1075"/><span class="koboSpan" id="kobo.1311.1">selecting a model for your application, start by identifying your specific use case and domain. </span><span class="koboSpan" id="kobo.1311.2">Whether you need a model for classification, clustering, retrieval, or summarization, and whether your domain is legal, medical, or general text, will significantly influence </span><span class="No-Break"><span class="koboSpan" id="kobo.1312.1">your choice.</span></span></p>
			<p><span class="koboSpan" id="kobo.1313.1">Next, evaluate the model’s </span><em class="italic"><span class="koboSpan" id="kobo.1314.1">size and memory usage</span></em><span class="koboSpan" id="kobo.1315.1">. </span><span class="koboSpan" id="kobo.1315.2">Larger models generally provide better performance but come with increased computational requirements and higher latency. </span><span class="koboSpan" id="kobo.1315.3">Begin with a smaller model for initial prototyping and consider transitioning to a larger one if your needs evolve. </span><span class="koboSpan" id="kobo.1315.4">Pay attention to the embedding dimensions, as larger dimensions offer a richer representation of the data but are also more computationally intensive. </span><span class="koboSpan" id="kobo.1315.5">Striking a balance between capturing detailed information and maintaining operational efficiency </span><span class="No-Break"><span class="koboSpan" id="kobo.1316.1">is key.</span></span></p>
			<p><span class="koboSpan" id="kobo.1317.1">Assess </span><em class="italic"><span class="koboSpan" id="kobo.1318.1">inference time carefully</span></em><span class="koboSpan" id="kobo.1319.1">, particularly if you have real-time application requirements; models with higher latency might necessitate GPU acceleration to meet performance standards. </span><span class="koboSpan" id="kobo.1319.2">Finally, evaluate the model’s performance using benchmarks such as </span><a id="_idIndexMarker1076"/><span class="koboSpan" id="kobo.1320.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.1321.1">Massive Text Embedding Benchmark</span></strong><span class="koboSpan" id="kobo.1322.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1323.1">MTEB</span></strong><span class="koboSpan" id="kobo.1324.1">) to compare across various metrics. </span><span class="koboSpan" id="kobo.1324.2">Consider both intrinsic evaluations, which examine the model’s understanding of semantic and syntactic relationships, and extrinsic evaluations, which assess performance on specific </span><span class="No-Break"><span class="koboSpan" id="kobo.1325.1">downstream tasks.</span></span></p>
			<h2 id="_idParaDest-263"><a id="_idTextAnchor300"/><span class="koboSpan" id="kobo.1326.1">Solving real problems with embeddings</span></h2>
			<p><span class="koboSpan" id="kobo.1327.1">With the </span><a id="_idIndexMarker1077"/><span class="koboSpan" id="kobo.1328.1">advancements in embedding models such as BERT, BGE, and GTE, we can tackle a wide range of challenges across various domains. </span><span class="koboSpan" id="kobo.1328.2">These models enable us to solve different problems, as </span><span class="No-Break"><span class="koboSpan" id="kobo.1329.1">presented here:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1330.1">Semantic search</span></strong><span class="koboSpan" id="kobo.1331.1">: Embeddings improve search relevance by capturing the contextual meaning of queries and documents, enhancing the accuracy of </span><span class="No-Break"><span class="koboSpan" id="kobo.1332.1">search results.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1333.1">Recommendation systems</span></strong><span class="koboSpan" id="kobo.1334.1">: They facilitate personalized content suggestions based on user preferences and behaviors, tailoring recommendations to </span><span class="No-Break"><span class="koboSpan" id="kobo.1335.1">individual needs.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1336.1">Text classification</span></strong><span class="koboSpan" id="kobo.1337.1">: Embeddings enable accurate categorization of documents into predefined classes, such as for sentiment analysis or </span><span class="No-Break"><span class="koboSpan" id="kobo.1338.1">topic identification.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1339.1">Information retrieval</span></strong><span class="koboSpan" id="kobo.1340.1">: They enhance the accuracy of retrieving relevant documents from extensive datasets, improving the efficiency of information </span><span class="No-Break"><span class="koboSpan" id="kobo.1341.1">retrieval systems.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1342.1">Natural language understanding</span></strong><span class="koboSpan" id="kobo.1343.1">: Embeddings support tasks such as NER, helping systems identify and classify key entities </span><span class="No-Break"><span class="koboSpan" id="kobo.1344.1">within text.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1345.1">Clustering techniques</span></strong><span class="koboSpan" id="kobo.1346.1">: They improve the organization of similar documents or topics in large datasets, aiding in better clustering and </span><span class="No-Break"><span class="koboSpan" id="kobo.1347.1">data management.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1348.1">Multimodal data processing</span></strong><span class="koboSpan" id="kobo.1349.1">: Embeddings are essential for integrating and analyzing text, image, and audio data, leading to more comprehensive insights and enhanced </span><span class="No-Break"><span class="koboSpan" id="kobo.1350.1">decision-making capabilities.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1351.1">Let’s summarize the learnings from </span><span class="No-Break"><span class="koboSpan" id="kobo.1352.1">this chapter.</span></span></p>
			<h1 id="_idParaDest-264"><a id="_idTextAnchor301"/><span class="koboSpan" id="kobo.1353.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.1354.1">In this chapter, we had a look at text preprocessing, which is an essential step in NLP. </span><span class="koboSpan" id="kobo.1354.2">We saw different text cleaning techniques, from handling HTML tags and capitalization to addressing numerical values and whitespace challenges. </span><span class="koboSpan" id="kobo.1354.3">We deep-dived into tokenization, examining word and subword tokenization, with practical Python examples. </span><span class="koboSpan" id="kobo.1354.4">Finally, we explored various methods for embedding documents and introduced some of the most popular embedding models </span><span class="No-Break"><span class="koboSpan" id="kobo.1355.1">available today.</span></span></p>
			<p><span class="koboSpan" id="kobo.1356.1">In the next chapter, we will continue our journey with unstructured data, delving into image and </span><span class="No-Break"><span class="koboSpan" id="kobo.1357.1">audio preprocessing.</span></span></p>
		</div>
	</body></html>