<html><head></head><body>
  <div id="_idContainer135" class="Basic-Text-Frame">
    <h1 class="chapterNumber">11</h1>
    <h1 id="_idParaDest-278" class="chapterTitle">The pandas Ecosystem</h1>
    <p class="normal">While the pandas library offers an impressive array of features, its popularity owes much to the vast amount of third-party libraries that work with it in a complementary fashion. We cannot hope to cover all of those libraries in this chapter, nor can we even dive too deep into how any individual library works. However, just knowing these tools exist and understanding what they offer can serve as a great inspiration for future learning.</p>
    <p class="normal">While pandas is an amazing tool, it has its flaws, which we have tried to highlight throughout this book; pandas cannot hope to solve every analytical problem there is. I strongly encourage you to get familiar with the tools outlined in this chapter and to also refer to the pandas ecosystem documentation (<a href="https://pandas.pydata.org/about/"><span class="url">https://pandas.pydata.org/about/</span></a>) when looking for new and specialized tools.</p>
    <p class="normal">As a technical note on this chapter, it is possible that these code blocks may break or change behavior as new releases of the libraries are released. While we went to great lengths throughout this book to try and write pandas code that is “future-proof”, it becomes more difficult to guarantee that as we write about third party dependencies (and their dependencies). If you encounter any issues running the code in this chapter, be sure to reference the <code class="inlineCode">requirements.txt</code> file provided alongside the code samples with this book. That file will contain a list of dependencies and versions that are known to work with this chapter.</p>
    <p class="normal">We will cover the following recipes in this chapter:</p>
    <ul>
      <li class="bulletList">Foundational libraries</li>
      <li class="bulletList">Exploratory data analysis</li>
      <li class="bulletList">Data validation</li>
      <li class="bulletList">Visualization </li>
      <li class="bulletList">Data science</li>
      <li class="bulletList">Databases </li>
      <li class="bulletList">Other DataFrame libraries</li>
    </ul>
    <h1 id="_idParaDest-279" class="heading-1">Foundational libraries</h1>
    <p class="normal">Like many open<a id="_idIndexMarker552"/> source libraries, pandas builds functionality on top of other foundational libraries, letting them manage lower-level details while pandas offers more user-friendly functionality. If you find yourself wanting to dive deeper into technical details beyond what you learn with pandas, these are the libraries you’ll want to focus on.</p>
    <h2 id="_idParaDest-280" class="heading-2">NumPy</h2>
    <p class="normal">NumPy labels itself<a id="_idIndexMarker553"/> as the <em class="italic">fundamental package for scientific computing with Python</em>, and it<a id="_idIndexMarker554"/> is the library on top of which pandas was originally built. NumPy is actually an <em class="italic">n</em>-dimensional library, so you are not limited to two-dimensional data like we get with a <code class="inlineCode">pd.DataFrame</code> (pandas actually used to offer 3-d and 4-d panel structures, but they are now long gone).</p>
    <p class="normal">Throughout this book, we have shown you how to construct pandas objects from NumPy objects, as you can see in the following <code class="inlineCode">pd.DataFrame</code> constructor:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">arr = np.arange(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>).reshape(<span class="hljs-number">3</span>, -<span class="hljs-number">1</span>)
df = pd.DataFrame(arr)
df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">     0    1    2
0    1    2    3
1    4    5    6
2    7    8    9
</code></code></pre>
    <p class="normal">However, you can also create NumPy arrays from <code class="inlineCode">pd.DataFrame</code> objects by using the <code class="inlineCode">pd.DataFrame.to_numpy</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.to_numpy()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">array([[1, 2, 3],
      [4, 5, 6],
      [7, 8, 9]])
</code></code></pre>
    <p class="normal">Many NumPy functions accept a <code class="inlineCode">pd.DataFrame</code> as an argument and will still even return a <code class="inlineCode">pd.DataFrame</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">np.log(df)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">     0           1           2
0    0.000000    0.693147    1.098612
1    1.386294    1.609438    1.791759
2    1.945910    2.079442    2.197225
</code></code></pre>
    <p class="normal">The main thing to keep in mind with NumPy is that its interoperability with pandas will degrade the moment you need missing values in non-floating point types, or more generally when you try to use data types that are neither integral nor floating point. </p>
    <p class="normal">The exact rules for this are too complicated to list in this book, but generally, I would advise against ever <a id="_idIndexMarker555"/>calling <code class="inlineCode">pd.Series.to_numpy</code> or <code class="inlineCode">pd.DataFrame.to_numpy</code> for <a id="_idIndexMarker556"/>anything other than floating point and integral data.</p>
    <h2 id="_idParaDest-281" class="heading-2">PyArrow</h2>
    <p class="normal">The other main library <a id="_idIndexMarker557"/>that pandas is built on top of is Apache <a id="_idIndexMarker558"/>Arrow, which labels itself as a <em class="italic">cross-language development platform for in-memory analytics</em>. Started by Wes McKinney (the creator of pandas) and announced in his influential Apache Arrow and the <em class="italic">10 Things I Hate About pandas</em> post (<a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals/"><span class="url">https://wesmckinney.com/blog/apache-arrow-pandas-internals/</span></a>), the Apache Arrow project defines the memory layout for one-dimensional data structures in a way that allows different languages, programs, and libraries to work with the same data. In addition to defining these structures, the Apache Arrow project offers a vast suite of tooling for libraries to implement the Apache Arrow specifications.</p>
    <p class="normal">An implementation of Apache Arrow in Python, PyArrow, has been used in particular instances throughout this book. While pandas does not expose a method to convert a <code class="inlineCode">pd.DataFrame</code> into PyArrow, the PyArrow library offers a <code class="inlineCode">pa.Table.from_pandas</code> method for that exact purpose:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">tbl = pa.Table.from_pandas(df)
tbl
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">pyarrow.Table
0: int64
1: int64
2: int64
----
0: [[1,4,7]]
1: [[2,5,8]]
2: [[3,6,9]]
</code></code></pre>
    <p class="normal">PyArrow similarly offers a <code class="inlineCode">pa.Table.to_pandas</code> method to get you from a <code class="inlineCode">pa.Table</code> into a <code class="inlineCode">pd.DataFrame</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">tbl.to_pandas()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">     0    1    2
0    1    2    3
1    4    5    6
2    7    8    9
</code></code></pre>
    <p class="normal">Generally, PyArrow is considered a lower-level library than pandas. It mostly aims to serve other library authors more than it does general users looking for a DataFrame library, so, unless you are authoring a library, you may not often need to convert to PyArrow from a <code class="inlineCode">pd.DataFrame</code>. However, as the Apache Arrow ecosystem grows, the fact that pandas and<a id="_idIndexMarker559"/> PyArrow <a id="_idIndexMarker560"/>can interoperate opens up a world of integration opportunities for pandas with many other analytical libraries and databases.</p>
    <h1 id="_idParaDest-282" class="heading-1">Exploratory data analysis</h1>
    <p class="normal">Oftentimes, you will<a id="_idIndexMarker561"/> find yourself provided with a dataset that you know very little about. Throughout this book, we’ve shown ways to manually sift through data, but there are also tools out there that can help automate potentially tedious tasks and help you grasp the data in a shorter amount of time.</p>
    <h2 id="_idParaDest-283" class="heading-2">YData Profiling</h2>
    <p class="normal">YData Profiling <a id="_idIndexMarker562"/>bills<a id="_idIndexMarker563"/> itself as the “<em class="italic">leading package for data profiling, that automates and standardizes the generation of detailed reports, complete with statistics and visualizations</em>.” While we discovered how to manually explore data back in the chapter on visualization, this package can be used as a quick-start to automatically generate many useful reports and features.</p>
    <p class="normal">To compare this to some of the work we did in those chapters, let’s take another look at the vehicles dataset. For now, we are just going to pick a small subset of columns to keep our YData Profiling minimal; for large datasets, the performance can often degrade:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_csv(
    "data/vehicles.csv.<span class="hljs-built_in">zip</span>",
    dtype_backend="numpy_nullable",
    usecols=[
        "<span class="hljs-built_in">id</span>",
        "engId",
        "make",
        "model",
        "cylinders",
        "city08",
        "highway08",
        "year",
        "trany",
    ]
)
df.head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    city08   cylinders   engId   …   model               trany           year
0   19       4           9011    …   Spider Veloce 2000  Manual 5-spd    1985
1   9        12          22020   …   Testarossa          Manual 5-spd    1985
2   23       4           2100    …   Charger             Manual 5-spd    1985
3   10       8           2850    …   B150/B250 Wagon 2WD Automatic 3-spd 1985
4   17       4           66031   …   Legacy AWD Turbo    Manual 5-spd    1993
5 rows × 9 columns
</code></code></pre>
    <p class="normal">YData Profiling <a id="_idIndexMarker564"/>allows<a id="_idIndexMarker565"/> you to easily create a profile report, which contains many common visualizations and helps describe the columns you are working with in your <code class="inlineCode">pd.DataFrame</code>.</p>
    <p class="normal">This book was written using <code class="inlineCode">ydata_profiling</code> version 4.9.0. To create the profile report, simply run:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">from</span> ydata_profiling <span class="hljs-keyword">import</span> ProfileReport
profile = ProfileReport(df, title="Vehicles Profile Report")
</code></code></pre>
    <p class="normal">If running code within a Jupyter notebook, you can see the output of this directly within the notebook with a call to:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">profile.to_widgets()
</code></code></pre>
    <p class="normal">If you are not using Jupyter, you can alternatively export that profile to a local HTML file and open it from there:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">profile.to_file("vehicles_profile.html")
</code></code></pre>
    <p class="normal">When looking at the profile, the first thing you will see is a high-level <strong class="screenText">Overview</strong> section that lists the number <a id="_idIndexMarker566"/>of cells with missing data, number of<a id="_idIndexMarker567"/> duplicate rows, etc.:</p>
    <figure class="mediaobject"><img src="../Images/B31091_11_01.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.1: Overview provided by YData Profiling</p>
    <p class="normal">Each column from your <code class="inlineCode">pd.DataFrame</code> will be detailed. In the case of a column with continuous values, YData Profiling will create a histogram for you:</p>
    <figure class="mediaobject"><img src="../Images/B31091_11_02.png" alt="A screenshot of a graph  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.2: Histogram generated by YData Profiling</p>
    <p class="normal">For categorical <a id="_idIndexMarker568"/>variables, the tool will generate a word <a id="_idIndexMarker569"/>cloud visualization:</p>
    <figure class="mediaobject"><img src="../Images/B31091_11_03.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.3: Word cloud generated by YData Profiling</p>
    <p class="normal">To understand how your continuous variables may or may not be correlated, the profile contains a very concise heat map that colors each pair accordingly:</p>
    <figure class="mediaobject"><img src="../Images/B31091_11_04.png" alt="A screenshot of a color chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.4: Heat map generated by YData Profiling</p>
    <p class="normal">While you still will likely <a id="_idIndexMarker570"/>need to dive further into your datasets <a id="_idIndexMarker571"/>than what this library provides, it can be a great starting point and can help automate the generation of otherwise tedious plots.</p>
    <h1 id="_idParaDest-284" class="heading-1">Data validation</h1>
    <p class="normal">The “garbage in, garbage out” principle<a id="_idIndexMarker572"/> in computing says that no matter how great your code may be, if you start with poor-quality data, your analysis will yield poor-quality results. All too often, data practitioners struggle with issues like unexpected missing data, duplicate values, and broken relationships between modeling entities.</p>
    <p class="normal">Fortunately, there are tools to help you automate both the data that is input to and output from your models, which ensures trust in the work that you are performing. In this recipe, we are going to look at Great Expectations.</p>
    <h2 id="_idParaDest-285" class="heading-2">Great Expectations</h2>
    <p class="normal">This book was <a id="_idIndexMarker573"/>written <a id="_idIndexMarker574"/>using Great Expectations version 1.0.2. To get started, let’s once again look at our vehicles dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_csv(
    "data/vehicles.csv.<span class="hljs-built_in">zip</span>",
    dtype_backend="numpy_nullable",
    dtype={
        "rangeA": pd.StringDtype(),
        "mfrCode": pd.StringDtype(),
        "c240Dscr": pd.StringDtype(),
        "c240bDscr": pd.StringDtype()
    }
)
df.head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">   barrels08  barrelsA08  charge120  …  phevCity  phevHwy  phevComb
0  14.167143  0.0         0.0        …  0         0        0
1  27.046364  0.0         0.0        …  0         0        0
2  11.018889  0.0         0.0        …  0         0        0
3  27.046364  0.0         0.0        …  0         0        0
4  15.658421  0.0         0.0        …  0         0        0
5 rows × 84 columns
</code></code></pre>
    <p class="normal">There are a few different ways to use Great Expectations, not all of which can be documented in this cookbook. For the sake of having a self-contained example, we are going to set up and process all of our expectations in memory.</p>
    <p class="normal">To do this, we are going to import the <code class="inlineCode">great_expectations</code> library and create a <code class="inlineCode">context</code> for our tests:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">import</span> great_expectations <span class="hljs-keyword">as</span> gx
context = gx.get_context()
</code></code></pre>
    <p class="normal">Within the context, you can create a data source and a data asset. For non-DataFrame sources like SQL, the data source would typically contain connection credentials, but with the <code class="inlineCode">pd.DataFrame</code> residing in memory there is less work to do. The data asset is a grouping mechanism for results. Here we are just creating one data asset, but in real-life use cases you may decide that you want multiple assets to store and organize the validation results that Great Expectations outputs:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">datasource = context.data_sources.add_pandas(name="pandas_datasource")
data_asset = datasource.add_dataframe_asset(name="vehicles")
</code></code></pre>
    <p class="normal">From there, you can create a batch definition within Great Expectations. For non-DataFrame sources, the batch definition would tell the library how to retrieve data from the source. In the case of pandas, the batch definition will simply retrieve all of the data from the associated <code class="inlineCode">pd.DataFrame</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">batch_definition_name = "dataframe_definition"
batch_definition = data_asset.add_batch_definition_whole_dataframe(
    batch_definition_name
)
batch = batch_definition.get_batch(batch_parameters={
    "dataframe": df
})
</code></code></pre>
    <p class="normal">At this point, you can start to make assertions about the data. For instance, you can use Great Expectations<a id="_idIndexMarker575"/> to <a id="_idIndexMarker576"/>ensure that a column does not contain any null values:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">city_exp = gx.expectations.ExpectColumnValuesToNotBeNull(
    column="city08"
)
result = batch.validate(city_exp)
<span class="hljs-built_in">print</span>(result)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">{
  "success": true,
  "expectation_config": {
    "type": "expect_column_values_to_not_be_null",
    "kwargs": {
      "batch_id": "pandas_datasource-vehicles",
      "column": "city08"
    },
    "meta": {}
  },
  "result": {
    "element_count": 48130,
    "unexpected_count": 0,
    "unexpected_percent": 0.0,
    "partial_unexpected_list": [],
    "partial_unexpected_counts": [],
    "partial_unexpected_index_list": []
  },
  "meta": {},
  "exception_info": {
    "raised_exception": false,
    "exception_traceback": null,
    "exception_message": null
  }
}
</code></code></pre>
    <p class="normal">That same<a id="_idIndexMarker577"/> expectation<a id="_idIndexMarker578"/> applied to the <code class="inlineCode">cylinders</code> column will not be successful:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">cylinders_exp = gx.expectations.ExpectColumnValuesToNotBeNull(
    column="cylinders"
)
result = batch.validate(cylinders_exp)
<span class="hljs-built_in">print</span>(result)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">{
  "success": false,
  "expectation_config": {
    "type": "expect_column_values_to_not_be_null",
    "kwargs": {
      "batch_id": "pandas_datasource-vehicles",
      "column": "cylinders"
    },
    "meta": {}
  },
  "result": {
    "element_count": 48130,
    "unexpected_count": 965,
    "unexpected_percent": 2.0049864949096197,
    "partial_unexpected_list": [
      null,
      null,
      ...
      null,
      null
    ],
    "partial_unexpected_counts": [
      {
        "value": null,
        "count": 20
      }
    ],
    "partial_unexpected_index_list": [
      7138,
      7139,
      8143,
      ...
      23022,
      23023,
      23024
    ]
  },
  "meta": {},
  "exception_info": {
    "raised_exception": false,
    "exception_traceback": null,
    "exception_message": null
  }
}
</code></code></pre>
    <p class="normal">For brevity, we have only shown you how to set expectations around nullability, but there is an entire Expectations Gallery at <a href="https://greatexpectations.io/expectations/"><span class="url">https://greatexpectations.io/expectations/</span></a> you can use for other assertions. Great Expectations also works with other tools like Spark, PostgreSQL, etc., so you<a id="_idIndexMarker579"/> can apply your expectations at many<a id="_idIndexMarker580"/> different points in your data transformation pipeline.</p>
    <h1 id="_idParaDest-286" class="heading-1">Visualization</h1>
    <p class="normal">Back in <em class="chapterRef">Chapter 6</em>, <em class="italic">Visualization</em>, we<a id="_idIndexMarker581"/> discussed at length visualization using matplotlib, and we even discussed using Seaborn for advanced plots. These tools are great for generating static charts, but when you want to add some level of interactivity, you will need to opt for other libraries.</p>
    <p class="normal">For this recipe, we are going to load the same data from the vehicles dataset we used back in our <em class="italic">Scatter plots</em> recipe from <em class="italic">Chapter 6</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_csv(
    "data/vehicles.csv.<span class="hljs-built_in">zip</span>",
    dtype_backend="numpy_nullable",
    dtype={
        "rangeA": pd.StringDtype(),
        "mfrCode": pd.StringDtype(),
        "c240Dscr": pd.StringDtype(),
        "c240bDscr": pd.StringDtype()
    }
)
df.head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">   barrels08  barrelsA08  charge120  …  phevCity  phevHwy  phevComb
0  14.167143  0.0         0.0        …  0         0        0
1  27.046364  0.0         0.0        …  0         0        0
2  11.018889  0.0         0.0        …  0         0        0
3  27.046364  0.0         0.0        …  0         0        0
4  15.658421  0.0         0.0        …  0         0        0
5 rows × 84 columns
</code></code></pre>
    <h2 id="_idParaDest-287" class="heading-2">Plotly</h2>
    <p class="normal">Let’s start by looking at Plotly, which<a id="_idIndexMarker582"/> can be used to create visualizations with a<a id="_idIndexMarker583"/> high degree of interactivity, making it a popular choice within Jupyter notebooks. To use it, simply pass <code class="inlineCode">plotly</code> as the <code class="inlineCode">backend=</code> argument to <code class="inlineCode">pd.DataFrame.plot</code>. We are also going to add a <code class="inlineCode">hover_data=</code> argument, which Plotly can use to add labels to each data point:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.plot(
    kind="scatter",
    x="city08",
    y="highway08",
    backend="plotly",
    hover_data={"make": <span class="hljs-literal">True</span>, "model": <span class="hljs-literal">True</span>, "year": <span class="hljs-literal">True</span>},
)
</code></code></pre>
    <p class="normal">If you inspect this in a Jupyter notebook or HTML page, you will see that you can hover over any data point to reveal more details:</p>
    <figure class="mediaobject"><img src="../Images/B31091_11_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.5: Hovering over a data point with Plotly</p>
    <p class="normal">You can even select an area of the chart to zoom into the data points:</p>
    <figure class="mediaobject"><img src="../Images/B31091_11_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.6: Zooming in with Plotly</p>
    <p class="normal">As you can see, Plotly is very easy to use with the same pandas API you have seen throughout this book. If<a id="_idIndexMarker584"/> you desire interactivity with your plots, it is a great<a id="_idIndexMarker585"/> tool to make use of.</p>
    <h2 id="_idParaDest-288" class="heading-2">PyGWalker</h2>
    <p class="normal">All of the plotting code <a id="_idIndexMarker586"/>you have<a id="_idIndexMarker587"/> seen so far is declarative in nature; i.e., you tell pandas that you want a bar, line, scatter plot, etc., and pandas generates that for you. However, many users may prefer having a more “free-form” tool for exploration, where they can just drag and drop elements to make charts on the fly.</p>
    <p class="normal">If that is what you are after, then you will want to take a look at the PyGWalker library. With a very succinct API, you can generate an interactive tool within a Jupyter notebook, with which you can drag and drop different elements to generate various charts:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">import</span> pygwalker <span class="hljs-keyword">as</span> pyg
pyg.walk(df)
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_11_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.7: PyGWalker within a Jupyter notebook</p>
    <h1 id="_idParaDest-289" class="heading-1">Data science</h1>
    <p class="normal">While pandas offers <a id="_idIndexMarker588"/>some built-in statistical algorithms, it cannot hope to cover all of the statistical and machine learning algorithms that are used in the domain of data science. Fortunately, however, many of the libraries that do specialize further in data science offer very tight integrations with pandas, letting you move data from one library to the next rather seamlessly.</p>
    <h2 id="_idParaDest-290" class="heading-2">scikit-learn</h2>
    <p class="normal">scikit-learn is a popular<a id="_idIndexMarker589"/> machine learning library that can help with both<a id="_idIndexMarker590"/> supervised and unsupervised learning. The scikit-learn library offers an impressive array of algorithms for classification, prediction, and clustering tasks, while also providing tools to pre-process and cleanse your data.</p>
    <p class="normal">We cannot hope to cover all of these features, but for the sake of showcasing something, let’s once again load the vehicles dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_csv(
    "data/vehicles.csv.<span class="hljs-built_in">zip</span>",
    dtype_backend="numpy_nullable",
    dtype={
        "rangeA": pd.StringDtype(),
        "mfrCode": pd.StringDtype(),
        "c240Dscr": pd.StringDtype(),
        "c240bDscr": pd.StringDtype()
    }
)
df.head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">   barrels08  barrelsA08  charge120  …  phevCity  phevHwy  phevComb
0  14.167143  0.0         0.0        …  0         0        0
1  27.046364  0.0         0.0        …  0         0        0
2  11.018889  0.0         0.0        …  0         0        0
3  27.046364  0.0         0.0        …  0         0        0
4  15.658421  0.0         0.0        …  0         0        0
5 rows × 84 columns
</code></code></pre>
    <p class="normal">Now let’s assume that we want to create an algorithm to predict the combined mileage a vehicle will achieve, inferring it from other attributes in the data. Since mileage is a continuous variable, we can opt for a linear regression model to make our predictions.</p>
    <p class="normal">The linear regression model we are going to work with will want to use features that are also numeric. While there are ways we could artificially convert some of our non-numeric data into numeric (e.g., using the technique from the <em class="italic">One-hot encoding with pd.get_dummies</em> recipe back in <em class="chapterRef">Chapter 5</em>, <em class="italic">Algorithms and How to Apply Them</em>), we are just going to ignore any non-numeric columns for now. The linear regression model is also unable to handle<a id="_idIndexMarker591"/> missing data. We know from the <em class="italic">Exploring continuous</em><em class="italic"><a id="_idIndexMarker592"/></em><em class="italic"> data</em> recipe from <em class="chapterRef">Chapter 6</em> that this dataset has two continuous variables with missing data. While we could try to interpolate those values, we are again going to take the simple route in this example and just drop them:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">num_df = df.select_dtypes(include=["number"])
num_df = num_df.drop(columns=["cylinders", "displ"])
</code></code></pre>
    <p class="normal">The scikit-learn model will need to know the <em class="italic">features</em> we want to use for prediction (commonly notated as <code class="inlineCode">X</code>) and the target variable we are trying to predict (commonly notated as <code class="inlineCode">y</code>). It is also a good practice to split the data into training and testing datasets, which we can do with the <code class="inlineCode">train_test_split</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
target_col = "comb08"
X = num_df.drop(columns=[target_col])
y = num_df[target_col]
X_train, X_test, y_train, y_test = train_test_split(X, y)
</code></code></pre>
    <p class="normal">With our data in this form, we can go ahead and train the linear regression model and then apply it to our test data to generate predictions:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> linear_model
 
regr = linear_model.LinearRegression()
regr.fit(X_train, y_train)
y_pred = regr.predict(X_test)
</code></code></pre>
    <p class="normal">Now that we have predictions from our test dataset, we can compare them back to the actual values we withheld as part of testing. This is a good way to measure how accurate the model is that we fit.</p>
    <p class="normal">There are many different ways to manage model accuracy, but for now, we can opt for the commonly used and relatively simple <code class="inlineCode">mean_squared_error</code>, which scikit-learn also provides as a convenience function:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error
mean_squared_error(y_test, y_pred)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0.11414180317382835
</code></code></pre>
    <p class="normal">If you are interested in knowing more, I highly recommend you read through the documentation and <a id="_idIndexMarker593"/>examples<a id="_idIndexMarker594"/> on the scikit-learn website, or check out books like <em class="italic">Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python</em> (<a href="https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312"><span class="url">https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312</span></a>).</p>
    <h2 id="_idParaDest-291" class="heading-2">XGBoost</h2>
    <p class="normal">For another great <a id="_idIndexMarker595"/>machine<a id="_idIndexMarker596"/> learning library, let’s now turn our attention to XGBoost, which implements algorithms using Gradient boosting. XGBoost is extremely performant, scales well, scores well in machine learning competitions, and pairs well with data that is stored in a <code class="inlineCode">pd.DataFrame</code>. If you are already familiar with scikit-learn, the API it uses will feel familiar.</p>
    <p class="normal">XGBoost can be used for both classification and regression. Since we just performed a regression analysis with scikit-learn, let’s now work through a classification example where we try to predict the make of a vehicle from the numeric features in the dataset.</p>
    <p class="normal">The vehicles dataset we are working with has 144 different makes. For our analysis, we are going to just pick a small subset of consumer brands:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">brands = {"Dodge", "Toyota", "Volvo", "BMW", "Buick", "Audi", "Volkswagen", "Subaru"}
df2 = df[df["make"].isin(brands)]
df2 = df2.drop(columns=["cylinders", "displ"])
</code></code></pre>
    <p class="normal">From there, we are going to split our data into features (<code class="inlineCode">X</code>) and a target variable (<code class="inlineCode">y</code>). For the purposes of the machine learning algorithm, we also need to convert our target variable into categorical <a id="_idIndexMarker597"/>data type, so that the algorithm can predict values like <code class="inlineCode">0</code>, <code class="inlineCode">1</code>, <code class="inlineCode">2</code>, etc instead of <code class="inlineCode">"Dodge," "Toyota," "Volvo,"</code> etc.:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">X = df2.select_dtypes(include=["number"])
y = df2["make"].astype(pd.CategoricalDtype())
</code></code></pre>
    <p class="normal">With that out of the way, we can once again use the <code class="inlineCode">train_test_split</code> function from scikit-learn to create training and testing data. Note that we are using <code class="inlineCode">pd.Series.cat.codes</code> to use the numeric value assigned to our categorical data type, rather than the string:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">X_train, X_test, y_train, y_test = train_test_split(X, y.cat.codes)
</code></code></pre>
    <p class="normal">Finally, we can import the <code class="inlineCode">XGBClassifier</code> from XGBoost, train it on our data, and apply it to our test features to generate predictions:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">from</span> xgboost <span class="hljs-keyword">import</span> XGBClassifier
bst = XGBClassifier()
bst.fit(X_train, y_train)
preds = bst.predict(X_test)
</code></code></pre>
    <p class="normal">Now that we have the predictions, we can validate how many of them matched the target variables included as part of our testing data:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">accuracy = (preds == y_test).<span class="hljs-built_in">sum</span>() / <span class="hljs-built_in">len</span>(y_test)
<span class="hljs-built_in">print</span>(f"Model prediction accuracy <span class="hljs-keyword">is</span>: {accuracy:<span class="hljs-number">.2</span>%}")
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">Model prediction accuracy is: 97.07%
</code></code></pre>
    <p class="normal">Once again, we are only scratching the surface of what you can do with a library like XGBoost. There are many different ways to tweak your model to improve accuracy, prevent over-/underfitting, optimize for a different outcome, etc. For users wanting to learn more about this great library, I advise checking out the XGBoost documentation or books like <em class="italic">Hands-On</em><em class="italic"><a id="_idIndexMarker598"/></em><em class="italic"> Gradient Boosting with XGBoost and scikit-learn</em>.</p>
    <h1 id="_idParaDest-292" class="heading-1">Databases</h1>
    <p class="normal">Database knowledge is an<a id="_idIndexMarker599"/> important tool in the toolkit of any data practitioner. While pandas is a great tool for single-machine, in-memory computations, databases offer a very complementary set of analytical tools that can help with the storage and distribution of analytical processes.</p>
    <p class="normal">Back in <em class="chapterRef">Chapter 4</em>, <em class="italic">The pandas I/O System</em>, we walked through how to transfer data between pandas and theoretically any database. However, a relatively more recent database called DuckDB is worth some extra consideration, as it allows you to even more seamlessly bridge the worlds of dataframes and databases together.</p>
    <h2 id="_idParaDest-293" class="heading-2">DuckDB</h2>
    <p class="normal">DuckDB is a lightweight<a id="_idIndexMarker600"/> database system that offers a zero-copy integration with <a id="_idIndexMarker601"/>Apache Arrow, a technology that also underpins efficient data sharing and usage with pandas. It is extremely lightweight and, unlike most database systems, can be easily embedded into other tools or processes. Most importantly, DuckDB is optimized for analytical workloads.</p>
    <p class="normal">DuckDB makes it easy to query data in your <code class="inlineCode">pd.DataFrame</code> using SQL. Let’s see this in action by loading the vehicles dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_csv(
    "data/vehicles.csv.<span class="hljs-built_in">zip</span>",
    dtype_backend="numpy_nullable",
    dtype={
        "rangeA": pd.StringDtype(),
        "mfrCode": pd.StringDtype(),
        "c240Dscr": pd.StringDtype(),
        "c240bDscr": pd.StringDtype()
    }
)
df.head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">   barrels08  barrelsA08  charge120  …  phevCity  phevHwy  phevComb
0  14.167143  0.0         0.0        …  0         0        0
1  27.046364  0.0         0.0        …  0         0        0
2  11.018889  0.0         0.0        …  0         0        0
3  27.046364  0.0         0.0        …  0         0        0
4  15.658421  0.0         0.0        …  0         0        0
5 rows × 84 columns
</code></code></pre>
    <p class="normal">By passing a <code class="inlineCode">CREATE TABLE</code> statement <a id="_idIndexMarker602"/>to <code class="inlineCode">duckdb.sql</code>, you can load the data<a id="_idIndexMarker603"/> from the <code class="inlineCode">pd.DataFrame</code> into a table:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">import</span> duckdb
duckdb.sql("CREATE TABLE vehicles AS SELECT * FROM df")
</code></code></pre>
    <p class="normal">Once the table is created, you can query from it with SQL:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">duckdb.sql("SELECT COUNT(*) FROM vehicles WHERE make = 'Honda'")
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">┌──────────────┐
│ count_star() │
│    int64     │
├──────────────┤
│         1197 │
└──────────────┘
</code></code></pre>
    <p class="normal">If you want to convert your results back to a <code class="inlineCode">pd.DataFrame</code>, you use the <code class="inlineCode">.df</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">duckdb.sql(
    "SELECT make, model, year, <span class="hljs-built_in">id</span>, city08 FROM vehicles where make = 'Honda' LIMIT <span class="hljs-number">5</span>"
).df()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    make    model          year   id      city08
0   Honda   Accord Wagon   1993   10145   18
1   Honda   Accord Wagon   1993   10146   20
2   Honda   Civic Del Sol  1994   10631   25
3   Honda   Civic Del Sol  1994   10632   30
4   Honda   Civic Del Sol  1994   10633   23
</code></code></pre>
    <p class="normal">For a deeper dive into DuckDB, I strongly advise checking out the DuckDB documentation and, for a greater understanding of where it fits in the grand scheme of databases, the <em class="italic">Why DuckDB</em> article (<a href="https://duckdb.org/why_duckdb"><span class="url">https://duckdb.org/why_duckdb</span></a>). Generally, DuckDB’s focus is on single-user <a id="_idIndexMarker604"/>analytics, but<a id="_idIndexMarker605"/> if you are interested in a shared, cloud-based data warehouse, you may also want to look at MotherDuck (<a href="https://motherduck.com/"><span class="url">https://motherduck.com/</span></a>).</p>
    <h1 id="_idParaDest-294" class="heading-1">Other DataFrame libraries</h1>
    <p class="normal">Soon after pandas was<a id="_idIndexMarker606"/> developed, it became the de facto DataFrame library in the Python space. Since then, many new DataFrame libraries have been developed in the space, which all aim to address some of the shortcomings of pandas while introducing their own novel design decisions.</p>
    <h2 id="_idParaDest-295" class="heading-2">Ibis</h2>
    <p class="normal">Ibis is yet <a id="_idIndexMarker607"/>another <a id="_idIndexMarker608"/>amazing analytics tool created by Wes McKinney, the creator of pandas. At a high level, Ibis is a DataFrame “frontend” that gives you one generic API through which you can query multiple “backends.”</p>
    <p class="normal">To help understand what that means, it is worth contrasting that with the design approach of pandas. In pandas, the API or “frontend” for a group by and a sum looks like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("column").agg(result="<span class="hljs-built_in">sum</span>")
</code></code></pre>
    <p class="normal">From this code snippet, the frontend of pandas defines how the query looks (i.e., for a group-by the operation you must call <code class="inlineCode">pd.DataFrame.groupby</code>). Behind the scenes, pandas dictates how the <code class="inlineCode">pd.DataFrame</code> is stored (in memory using pandas’ own representation) and even dictates how the summation should be performed against that in-memory representation.</p>
    <p class="normal">In Ibis, a similar expression would look like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.group_by("column").agg(result=df.<span class="hljs-built_in">sum</span>())
</code></code></pre>
    <p class="normal">While the API exposed to the user may not be all that different, the similarities between Ibis and pandas stop there. Ibis does not dictate how you store the data you are querying; it can be stored in BigQuery, DuckDB, MySQL, PostgreSQL, etc., and it can be even stored in another DataFrame library like pandas. Beyond the storage, Ibis does not dictate how summation should be performed; instead, it leaves it to an execution engine. Many SQL databases have their own execution engine, but others may defer to third-party libraries like Apache DataFusion (<a href="https://datafusion.apache.org/"><span class="url">https://datafusion.apache.org/</span></a>).</p>
    <p class="normal">To use a <code class="inlineCode">pd.DataFrame</code> through Ibis, you will need to wrap it with the <code class="inlineCode">ibis.memtable</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">import</span> ibis
df = pd.read_csv(
    "data/vehicles.csv.<span class="hljs-built_in">zip</span>",
    dtype_backend="numpy_nullable",
    usecols=["<span class="hljs-built_in">id</span>", "year", "make", "model", "city08"],
)
t = ibis.memtable(df)
</code></code></pre>
    <p class="normal">With that out of the way, you can then start to query the data just as you would with pandas but using the Ibis API:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">t.<span class="hljs-built_in">filter</span>(t.make == "Honda").select("make", "model", "year", "city08")
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">r0 := InMemoryTable
  data:
    PandasDataFrameProxy:
             city08     id        make                model  year
      0          19      1  Alfa Romeo   Spider Veloce 2000  1985
      1           9     10     Ferrari           Testarossa  1985
      2          23    100       Dodge              Charger  1985
      3          10   1000       Dodge  B150/B250 Wagon 2WD  1985
      4          17  10000      Subaru     Legacy AWD Turbo  1993
      ...       ...    ...         ...                  ...   ...
      48125      19   9995      Subaru               Legacy  1993
      48126      20   9996      Subaru               Legacy  1993
      48127      18   9997      Subaru           Legacy AWD  1993
      48128      18   9998      Subaru           Legacy AWD  1993
      48129      16   9999      Subaru     Legacy AWD Turbo  1993
 
      [48130 rows x 5 columns]
 r1 := Filter[r0]
  r0.make == 'Honda'
 Project[r1]
  make:   r1.make
  model:  r1.model
  year:   r1.year
  city08: r1.city08
</code></code></pre>
    <p class="normal">It is worth noting that the preceding code does not actually return a result. Unlike pandas, which executes all of the operations you give it <em class="italic">eagerly</em>, Ibis collects all of the expressions you want and waits to perform execution until explicitly required. This practice is commonly called <em class="italic">deferred</em> or <em class="italic">lazy</em> execution.</p>
    <p class="normal">The advantage of deferring is that Ibis can find ways to optimize the query that you are telling it to perform. Our query is asking Ibis to find all rows where the make is Honda and then select a few columns, but it might be faster for the underlying database to select the columns first and then perform the filter. How that works is abstracted from the end user; users are just required to tell Ibis what they want and Ibis takes care of how to retrieve that data.</p>
    <p class="normal">To materialize this back into a <code class="inlineCode">pd.DataFrame</code>, you can chain in a call to <code class="inlineCode">.to_pandas</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">t.<span class="hljs-built_in">filter</span>(t.make == "Honda").select("make", "model", "year", "city08").to_pandas().head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">     make    model          year   city08
0    Honda   Accord Wagon   1993   18
1    Honda   Accord Wagon   1993   20
2    Honda   Civic Del Sol  1994   25
3    Honda   Civic Del Sol  1994   30
4    Honda   Civic Del Sol  1994   23
</code></code></pre>
    <p class="normal">However, you are not required to return a <code class="inlineCode">pd.DataFrame</code>. If you wanted a PyArrow table instead, you could opt for <code class="inlineCode">.to_pyarrow</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">t.<span class="hljs-built_in">filter</span>(t.make == "Honda").select("make", "model", "year", "city08").to_pyarrow()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">pyarrow.Table
make: string
model: string
year: int64
city08: int64
----
make: [["Honda","Honda","Honda","Honda","Honda",...,"Honda","Honda","Honda","Honda","Honda"]]
model: [["Accord Wagon","Accord Wagon","Civic Del Sol","Civic Del Sol","Civic Del Sol",...,"Prelude","Prelude","Prelude","Accord","Accord"]]
year: [[1993,1993,1994,1994,1994,...,1993,1993,1993,1993,1993]]
city08: [[18,20,25,30,23,...,21,19,19,19,21]]
</code></code></pre>
    <p class="normal">For more information<a id="_idIndexMarker609"/> on Ibis, be sure<a id="_idIndexMarker610"/> to check out the Ibis documentation. There is even an Ibis tutorial aimed specifically at users coming from pandas.</p>
    <h2 id="_idParaDest-296" class="heading-2">Dask</h2>
    <p class="normal">Another popular<a id="_idIndexMarker611"/> library that has a history closely tied to pandas is <a id="_idIndexMarker612"/>Dask. Dask is a framework that provides a similar API to the <code class="inlineCode">pd.DataFrame</code> but scales its usage to parallel computations and datasets that exceed the amount of memory available on your system.</p>
    <p class="normal">If we wanted to convert our vehicles dataset to a Dask DataFrame, we can use the <code class="inlineCode">dask.dataframe.from_pandas</code> function with a <code class="inlineCode">npartitions=</code> argument that controls how to divide up the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">import</span> dask.dataframe <span class="hljs-keyword">as</span> dd
ddf = dd.from_pandas(df, npartitions=<span class="hljs-number">10</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">/home/willayd/clones/Pandas-Cookbook-Third-Edition/lib/python3.9/site-packages/dask/dataframe/__init__.py:42: FutureWarning:
Dask dataframe query planning is disabled because dask-expr is not installed.
You can install it with `pip install dask[dataframe]` or `conda install dask`.
This will raise in a future version.
 warnings.warn(msg, FutureWarning)
</code></code></pre>
    <p class="normal">By splitting your DataFrame into different partitions, Dask allows you to perform computations against each partition in parallel, which can help immensely with performance and scalability.</p>
    <p class="normal">Much like Ibis, Dask performs calculations lazily. If you want to force a calculation, you will want to call the <code class="inlineCode">.compute</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ddf.size.compute()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">3991932
</code></code></pre>
    <p class="normal">To go from a Dask<a id="_idIndexMarker613"/> DataFrame <a id="_idIndexMarker614"/>back to pandas, simply call <code class="inlineCode">ddf.compute</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ddf.compute().head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">     city08    id      make          model                 year
0    19        1       Alfa Romeo    Spider Veloce 2000    1985
1    9         10      Ferrari       Testarossa            1985
2    23        100     Dodge         Charger               1985
3    10        1000    Dodge         B150/B250 Wagon 2WD   1985
4    17        10000   Subaru        Legacy AWD Turbo      1993
</code></code></pre>
    <h2 id="_idParaDest-297" class="heading-2">Polars</h2>
    <p class="normal">Polars is a newcomer <a id="_idIndexMarker615"/>to the DataFrame space and has developed<a id="_idIndexMarker616"/> impressive features and a dedicated following in a very short amount of time. The Polars library is Apache Arrow native, so it has a much cleaner type system and consistent missing value handling than what pandas offers today (for the history of the pandas type system and all of its flaws, be sure to give <em class="chapterRef">Chapter 3</em>, <em class="italic">Data Types</em>, a good read).</p>
    <p class="normal">In addition to a simpler and cleaner type system, Polars can scale to datasets that are larger than memory, and it even offers a lazy execution engine coupled with a query optimizer that can make it easier to write performant, scalable code.</p>
    <p class="normal">For a naive conversion from pandas to Polars, you can use <code class="inlineCode">polars.from_pandas</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">import</span> polars <span class="hljs-keyword">as</span> pl
pl_df = pl.from_pandas(df)
pl_df.head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">shape: (5, 84)
barrels08  barrelsA08  charge120  charge240  ...  phevCity  phevHwy  phevComb
f64        f64         f64        f64        ...  i64       i64      i64
14.167143  0.0         0.0        0.0        ...    0         0        0
27.046364  0.0         0.0        0.0        ...    0         0        0
11.018889  0.0         0.0        0.0        ...    0         0        0
27.046364  0.0         0.0        0.0        ...    0         0        0
15.658421  0.0         0.0        0.0        ...    0         0        0
</code></code></pre>
    <p class="normal">For lazy execution, you will want to try out the <code class="inlineCode">pl.LazyFrame</code>, which can take the <code class="inlineCode">pd.DataFrame</code> directly as an argument:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">lz_df = pl.LazyFrame(df)
</code></code></pre>
    <p class="normal">Much like we saw <a id="_idIndexMarker617"/>with <a id="_idIndexMarker618"/>Ibis, the lazy execution engine of Polars can take care of optimizing the best path for doing a filter and select. To execute the plan, you will need to chain in a call to <code class="inlineCode">pl.LazyFrame.collect</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">lz_df.<span class="hljs-built_in">filter</span>(
    pl.col("make") == "Honda"
).select(["make", "model", "year", "city08"]).collect().head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">shape: (5, 4)
make    model   year    city08
str     str     i64     i64
"Honda" "Accord Wagon"  1993    18
"Honda" "Accord Wagon"  1993    20
"Honda" "Civic Del Sol" 1994    25
"Honda" "Civic Del Sol" 1994    30
"Honda" "Civic Del Sol" 1994    23
</code></code></pre>
    <p class="normal">If you would like to convert back to pandas from Polars, both the <code class="inlineCode">pl.DataFrame</code> and <code class="inlineCode">pl.LazyFrame</code> offer a <code class="inlineCode">.to_pandas</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">lz_df.<span class="hljs-built_in">filter</span>(
    pl.col("make") == "Honda"
).select(["make", "model", "year", "city08"]).collect().to_pandas().head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">      make   model          year  city08
0     Honda  Accord Wagon   1993  18
1     Honda  Accord Wagon   1993  20
2     Honda  Civic Del Sol  1994  25
3     Honda  Civic Del Sol  1994  30
4     Honda  Civic Del Sol  1994  23
</code></code></pre>
    <p class="normal">For a more detailed<a id="_idIndexMarker619"/> look <a id="_idIndexMarker620"/>at Polars and all of the great things it has to offer, I suggest checking out the <em class="italic">Polars Cookbook</em> (<a href="https://www.packtpub.com/en-us/product/polars-cookbook-9781805121152"><span class="url">https://www.packtpub.com/en-us/product/polars-cookbook-9781805121152</span></a>).</p>
    <h2 id="_idParaDest-298" class="heading-2">cuDF</h2>
    <p class="normal">If you have a Nvidia <a id="_idIndexMarker621"/>device and the CUDA toolkit available to you, you may <a id="_idIndexMarker622"/>also be interested in cuDF. In theory, cuDF is a “drop-in” replacement for pandas; as long as you have the right hardware and tooling, it will take your pandas expressions and run them on your GPU, simply by importing cuDF before pandas:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">import</span> cudf.pandas
cudf.pandas.install()
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
</code></code></pre>
    <p class="normal">Given the power of modern GPUs compared to CPUs, this library can offer users a significant performance boost<a id="_idIndexMarker623"/> without having to change the way code is written. For the right users with the right <a id="_idIndexMarker624"/>hardware, that type of out-of-the-box performance boost can be invaluable.</p>
    <h1 id="_idParaDest-299" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/pandas"><span class="url">https://packt.link/pandas</span></a></p>
    <p class="normal"><img src="../Images/QR_Code5040900042138312.png" alt=""/></p>
  </div>
  <div id="_idContainer137">
    <h1 id="_idParaDest-300" class="heading-1">Leave a Review! </h1>
    <p class="normal">Thank you for purchasing this book from Packt Publishing—we hope you enjoyed it! Your feedback is invaluable and helps us improve and grow. Please take a moment to leave an <a href="Chapter_11.xhtml"><span class="url">Amazon review</span></a>; it will only take a minute, but it makes a big difference for readers like you.</p>
    <p class="normal">Scan the QR code below to receive a free ebook of your choice.</p>
    <figure class="mediaobject"><img src="../Images/QR_Code1474021820358918656.png" alt=""/></figure>
    <p class="packt_figref"><a href="https://packt.link/NzOWQ"><span class="url">https://packt.link/NzOWQ</span></a></p>
  </div>


  <div id="_idContainer143">
    <p class="BM-packtLogo"><img src="../Images/New_Packt_Logo1.png" alt=""/></p>
    <p class="normal"><a href="https://www.packt.com"><span class="url">packt.com</span></a></p>
    <p class="normal">Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.</p>
    <h1 id="_idParaDest-301" class="heading-1">Why subscribe?</h1>
    <ul>
      <li class="bulletList">Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</li>
      <li class="bulletList">Improve your learning with Skill Plans built especially for you</li>
      <li class="bulletList">Get a free eBook or video every month</li>
      <li class="bulletList">Fully searchable for easy access to vital information</li>
      <li class="bulletList">Copy and paste, print, and bookmark content</li>
    </ul>
    <p class="normal">At <a href="https://www.packt.com"><span class="url">www.packt.com</span></a>, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.</p>
    <p class="eop"/>
    <p class="eop"/>
    <h1 id="_idParaDest-302" class="mainHeading">Other Books You May Enjoy</h1>
    <p class="normal">If you enjoyed this book, you may be interested in these other books by Packt:</p>
    <p class="BM-bookCover"><a href="https://www.packtpub.com/en-us/product/machine-learning-with-pytorch-and-scikit-learn-9781801819312"><img src="../Images/978-1-80181-931-2.png" alt=""/></a></p>
    <p class="normal"><strong class="keyWord">Machine Learning with PyTorch and Scikit-Learn</strong></p>
    <p class="normal">Sebastian Raschka</p>
    <p class="normal">Yuxi (Hayden) Liu</p>
    <p class="normal">Vahid Mirjalili</p>
    <p class="normal">ISBN: 978-1-80181-931-2</p>
    <ul>
      <li class="bulletList">Explore frameworks, models, and techniques for machines to learn from data</li>
      <li class="bulletList">Use scikit-learn for machine learning and PyTorch for deep learning</li>
      <li class="bulletList">Train machine learning classifiers on images, text, and more</li>
      <li class="bulletList">Build and train neural networks, transformers, and boosting algorithms</li>
      <li class="bulletList">Discover best practices for evaluating and tuning models</li>
      <li class="bulletList">Predict continuous target outcomes using regression analysis</li>
      <li class="bulletList">Dig deeper into textual and social media data using sentiment analysis</li>
    </ul>
    <p class="eop"/>
    <p class="eop"/>
    <p class="BM-bookCover"><a href="https://www.packtpub.com/en-us/product/deep-learning-with-tensorflow-and-keras-3rd-edition-9781803232911"><img src="../Images/978-1-80323-291-1.png" alt=""/></a></p>
    <p class="normal"><strong class="keyWord">Deep Learning with TensorFlow and Keras, Third Edition</strong></p>
    <p class="normal">Amita Kapoor</p>
    <p class="normal">Antonio Gulli</p>
    <p class="normal">Sujit Pal</p>
    <p class="normal">ISBN: 978-1-80323-291-1</p>
    <ul>
      <li class="bulletList">Learn how to use the popular GNNs with TensorFlow to carry out graph mining tasks</li>
      <li class="bulletList">Discover the world of transformers, from pretraining to fine-tuning to evaluating them</li>
      <li class="bulletList">Apply self-supervised learning to natural language processing, computer vision, and audio signal processing</li>
      <li class="bulletList">Combine probabilistic and deep learning models using TensorFlow Probability</li>
      <li class="bulletList">Train your models on the cloud and put TF to work in real environments</li>
      <li class="bulletList">Build machine learning and deep learning systems with TensorFlow 2.x and the Keras API</li>
    </ul>
    <p class="eop"/>
    <p class="eop"/>
    <p class="BM-bookCover"><a href="https://www.packtpub.com/en-us/product/machine-learning-for-algorithmic-trading-9781839217715"><img src="../Images/978-1-78934-641-1.png" alt=""/></a></p>
    <p class="normal"><strong class="keyWord">Machine Learning for Algorithmic Trading, Second Edition</strong></p>
    <p class="normal">Stefan Jansen</p>
    <p class="normal">ISBN: 978-1-83921-771-5</p>
    <ul>
      <li class="bulletList">Leverage market, fundamental, and alternative text and image data</li>
      <li class="bulletList">Research and evaluate alpha factors using statistics, Alphalens, and SHAP values</li>
      <li class="bulletList">Implement machine learning techniques to solve investment and trading problems</li>
      <li class="bulletList">Backtest and evaluate trading strategies based on machine learning using Zipline and Backtrader</li>
      <li class="bulletList">Optimize portfolio risk and performance analysis using pandas, NumPy, and pyfolio</li>
      <li class="bulletList">Create a pairs trading strategy based on cointegration for US equities and ETFs</li>
      <li class="bulletList">Train a gradient boosting model to predict intraday returns using AlgoSeek s high-quality trades and quotes data</li>
    </ul>
    <p class="eop"/>
    <p class="eop"/>
    <h1 id="_idParaDest-303" class="heading-1">Packt is searching for authors like you</h1>
    <p class="normal">If you’re interested in becoming an author for Packt, please visit <a href="https://authors.packtpub.com"><span class="url">authors.packtpub.com</span></a> and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</p>
    <h1 id="_idParaDest-304" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="Other_Books_You_May_Enjoy.xhtml"><span class="url">https://packt.link/pandas</span></a></p>
    <p class="normal"><img src="../Images/QR_Code5040900042138312.png" alt=""/></p>
  </div>
</body></html>