<html><head></head><body>
<div epub:type="chapter" id="_idContainer137">
<h1 class="chapter-number" id="_idParaDest-150"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-151"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.2.1">Going at Scale</span></h1>
<p><span class="koboSpan" id="kobo.3.1">After building and testing models in the previous chapter, we will now address the requirements and considerations for scaling time-series analysis in large and distributed computing environments. </span><span class="koboSpan" id="kobo.3.2">We will cover the different ways that Apache Spark can be used to scale the previous examples in </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.4.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.5.1">, starting with feature engineering and moving on to hyperparameter tuning and single- and multi-model training. </span><span class="koboSpan" id="kobo.5.2">This information is crucial as we face the requirement to analyze large volumes of time-series data in a </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">timely manner.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Why do we need to scale </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">time-series analysis?</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Scaling out </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">feature engineering</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Scaling out </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">model training</span></span></li>
</ul>
<h1 id="_idParaDest-152"><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.15.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.16.1">Before getting into the main topics, we will cover here the technical requirements for this chapter, which are </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.18.1">GitHub repository</span></strong><span class="koboSpan" id="kobo.19.1">: The code for this chapter can be found in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.20.1">ch8</span></strong><span class="koboSpan" id="kobo.21.1"> folder of the book’s GitHub repository at </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">this URL:</span></span><p class="list-inset"><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch8"><span class="No-Break"><span class="koboSpan" id="kobo.23.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch8</span></span></a></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.24.1">Synthetic data</span></strong><span class="koboSpan" id="kobo.25.1">: We will use the Synthetic Data Vault tool, a Python library for creating synthetic tabular data. </span><span class="koboSpan" id="kobo.25.2">You can find more information on Synthetic Data Vault </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">here: </span></span><a href="https://docs.sdv.dev/sdv"><span class="No-Break"><span class="koboSpan" id="kobo.27.1">https://docs.sdv.dev/sdv</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.28.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.29.1">Databricks platform</span></strong><span class="koboSpan" id="kobo.30.1">: The Databricks Community Edition, while free to use, is limited in resources. </span><span class="koboSpan" id="kobo.30.2">Similarly, the resources are likely to be limited when using a personal computer or laptop. </span><span class="koboSpan" id="kobo.30.3">With the requirement to demonstrate the scaling of computing power in this chapter, we will be using the non-Community version of the Databricks platform. </span><span class="koboSpan" id="kobo.30.4">As discussed in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.31.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.32.1">, you can sign up for a 14-day free trial of Databricks, which will require you to first have an account with a cloud provider. </span><span class="koboSpan" id="kobo.32.2">Some cloud providers offer free credits at the start. </span><span class="koboSpan" id="kobo.32.3">This will provide you with more resources than on the Community Edition, for a limited time. </span><span class="koboSpan" id="kobo.32.4">Note that at the end of the trial period, the billing will switch to the credit card you provided </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">at registration.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.34.1">The Databricks compute configuration used is as per </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.35.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.36.1">.1</span></em><span class="koboSpan" id="kobo.37.1">. </span><span class="koboSpan" id="kobo.37.2">The worker and driver types shown here are based on AWS, which is different from what is available on Azure and GCP. </span><span class="koboSpan" id="kobo.37.3">Note that the UI can be subject to change, in which case refer to the latest Databricks </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">documentation here:</span></span></p><p class="list-inset"><a href="https://docs.databricks.com/en/compute/configure.html"><span class="No-Break"><span class="koboSpan" id="kobo.39.1">https://docs.databricks.com/en/compute/configure.html</span></span></a></p></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer130">
<span class="koboSpan" id="kobo.40.1"><img alt="" src="image/B18568_08_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.41.1">Figure 8.1: Databricks compute configuration</span></p>
<h1 id="_idParaDest-153"><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.42.1">Why do we need to scale time-series analysis?</span></h1>
<p><span class="koboSpan" id="kobo.43.1">The need to </span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.44.1">scale time-series analysis usually results from a requirement to perform the analysis faster or on a bigger dataset. </span><span class="koboSpan" id="kobo.44.2">In this chapter, we will look at decreasing the processing time achieved in </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.45.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.46.1"> while increasing the dataset size fivefold. </span><span class="koboSpan" id="kobo.46.2">This will be possible thanks to the scale of processing offered by </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">Apache Spark.</span></span></p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.48.1">Scaled-up dataset</span></h2>
<p><span class="koboSpan" id="kobo.49.1">To exercise</span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.50.1"> Spark’s scalability, we will need a more extensive dataset than we have used. </span><span class="koboSpan" id="kobo.50.2">While you may already have such a dataset, for the sake of this chapter, we will scale the household energy consumption dataset we used in </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.51.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.52.1"> and earlier chapters. </span><span class="koboSpan" id="kobo.52.2">The scaled dataset will be generated using the Synthetic Data Vault tool, mentioned in the </span><em class="italic"><span class="koboSpan" id="kobo.53.1">Technical </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.54.1">requirements</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.55.1"> section.</span></span></p>
<p><span class="koboSpan" id="kobo.56.1">The code for this section is in </span><strong class="source-inline"><span class="koboSpan" id="kobo.57.1">ts-spark_ch8_1.dbc</span></strong><span class="koboSpan" id="kobo.58.1">. </span><span class="koboSpan" id="kobo.58.2">We import the code into Databricks, similar to the approach explained for the Community Edition in the </span><em class="italic"><span class="koboSpan" id="kobo.59.1">Step-by-step: Loading and visualizing time series</span></em><span class="koboSpan" id="kobo.60.1"> section of </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.61.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.62.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.63.1">In this code, we want to generate energy consumption data for four other families using one household’s data to scale </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">it fivefold.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">We begin by capturing the metadata of </span><strong class="source-inline"><span class="koboSpan" id="kobo.66.1">pdf_main</span></strong><span class="koboSpan" id="kobo.67.1">, which is the smaller reference dataset. </span><span class="koboSpan" id="kobo.67.2">The metadata is used as input to create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.68.1">GaussianCopulaSynthesizer</span></strong><span class="koboSpan" id="kobo.69.1"> object named </span><strong class="source-inline"><span class="koboSpan" id="kobo.70.1">synthesizer</span></strong><span class="koboSpan" id="kobo.71.1">, which represents a statistical model of the data. </span><span class="koboSpan" id="kobo.71.2">The synthesizer is, in turn, trained on the reference dataset (</span><strong class="source-inline"><span class="koboSpan" id="kobo.72.1">pdf_main</span></strong><span class="koboSpan" id="kobo.73.1">) with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.74.1">fit</span></strong><span class="koboSpan" id="kobo.75.1"> method. </span><span class="koboSpan" id="kobo.75.2">This model is finally used to generate synthetic data with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.76.1">sample</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.77.1"> method.</span></span></p>
<p><span class="koboSpan" id="kobo.78.1">The smaller reference dataset (</span><strong class="source-inline"><span class="koboSpan" id="kobo.79.1">pdf_main</span></strong><span class="koboSpan" id="kobo.80.1">) is associated with customer identifier (</span><strong class="source-inline"><span class="koboSpan" id="kobo.81.1">cust_id</span></strong><span class="koboSpan" id="kobo.82.1">) </span><strong class="source-inline"><span class="koboSpan" id="kobo.83.1">1</span></strong><span class="koboSpan" id="kobo.84.1">, and the synthetic datasets are associated with identifiers </span><strong class="source-inline"><span class="koboSpan" id="kobo.85.1">2</span></strong><span class="koboSpan" id="kobo.86.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.87.1">3</span></strong><span class="koboSpan" id="kobo.88.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.89.1">4</span></strong><span class="koboSpan" id="kobo.90.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.92.1">5</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.94.1">
# Initialize metadata object for the dataset
</span><strong class="bold"><span class="koboSpan" id="kobo.95.1">metadata</span></strong><span class="koboSpan" id="kobo.96.1"> = SingleTableMetadata()
# Automatically detect and set the metadata from the Pandas DataFrame
metadata.detect_from_dataframe(</span><strong class="bold"><span class="koboSpan" id="kobo.97.1">pdf_main</span></strong><span class="koboSpan" id="kobo.98.1">)
# Initialize the Gaussian Copula Synthesizer with the dataset metadata
</span><strong class="bold"><span class="koboSpan" id="kobo.99.1">synthesizer</span></strong><span class="koboSpan" id="kobo.100.1"> = </span><strong class="bold"><span class="koboSpan" id="kobo.101.1">GaussianCopulaSynthesizer</span></strong><span class="koboSpan" id="kobo.102.1">(metadata)
# Fit the synthesizer model to the Pandas DataFrame
synthesizer.</span><strong class="bold"><span class="koboSpan" id="kobo.103.1">fit</span></strong><span class="koboSpan" id="kobo.104.1">(pdf_main)
…
# Define the number of customer datasets to generate:
num_customers = 5
# Count the number of rows in the original dataset:
sample_size = df_main.count()
i = 1
df_all = df_main.withColumn(
    '</span><strong class="bold"><span class="koboSpan" id="kobo.105.1">cust_id'</span></strong><span class="koboSpan" id="kobo.106.1">, F.lit(i)
) # Add a 'cust_id' column to the original dataset with a constant 
# value of 1
…
    synthetic_data = spark.createDataFrame(
        synthesizer.</span><strong class="bold"><span class="koboSpan" id="kobo.107.1">sample</span></strong><span class="koboSpan" id="kobo.108.1">(num_rows=sample_size)
) # Generate synthetic data matching the original dataset's size
…</span></pre> <p><span class="koboSpan" id="kobo.109.1">Running the</span><a id="_idIndexMarker713"/><span class="koboSpan" id="kobo.110.1"> code in </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.111.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.112.1"> on this new, larger dataset is not going to be performant. </span><span class="koboSpan" id="kobo.112.2">We can scale up or out time-series analysis on larger datasets. </span><span class="koboSpan" id="kobo.112.3">We will explain both types of </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">scaling next.</span></span></p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.114.1">Scaling up</span></h2>
<p><span class="koboSpan" id="kobo.115.1">Scaling up is the</span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.116.1"> simpler way to scale and does not require us to change the code we wrote in </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.117.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.118.1">. </span><span class="koboSpan" id="kobo.118.2">We improve performance in this way by adding more memory (RAM) and using a more powerful CPU or even GPU. </span><span class="koboSpan" id="kobo.118.3">This works to a certain point before we reach scaling limits, prohibitive costs, or diminishing returns. </span><span class="koboSpan" id="kobo.118.4">In fact, due to system bottlenecks and overheads, scaling up does not result in linear </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">performance improvement.</span></span></p>
<p><span class="koboSpan" id="kobo.120.1">To scale further, we need to </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">scale out.</span></span></p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.122.1">Scaling out</span></h2>
<p><span class="koboSpan" id="kobo.123.1">Instead of making</span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.124.1"> our one machine more powerful, scaling out involves adding more machines and parallelizing the processing. </span><span class="koboSpan" id="kobo.124.2">This requires a mechanism for the code to be distributed and executed in parallel, which is what Apache </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">Spark provides.</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">In the upcoming sections, we will cover the following different ways that Apache Spark can be used to scale out </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">time-series analysis:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.128.1">Feature engineering</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.129.1">Model training</span></span></li>
</ul>
<h1 id="_idParaDest-157"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.130.1">Feature engineering</span></h1>
<p><span class="koboSpan" id="kobo.131.1">Apache Spark can </span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.132.1">be used to scale out feature engineering with its distributed computing framework. </span><span class="koboSpan" id="kobo.132.2">This enables parallel processing of feature engineering tasks, which we will demonstrate in </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">this section.</span></span></p>
<p><span class="koboSpan" id="kobo.134.1">We will continue our discussion on data preparation in </span><a href="B18568_05.xhtml#_idTextAnchor103"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.135.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.136.1"> and improve the feature engineering done in </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.137.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.138.1">. </span><span class="koboSpan" id="kobo.138.2">We will be using the pandas-based code examples from the </span><em class="italic"><span class="koboSpan" id="kobo.139.1">Development and testing</span></em><span class="koboSpan" id="kobo.140.1"> section of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.141.1">Chapter 7</span></em></span><span class="koboSpan" id="kobo.142.1"> as a base for our discussion in this section. </span><span class="koboSpan" id="kobo.142.2">We will see in the following examples how non-Spark code is rewritten to be Spark compatible to avail the benefits of its </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">scalability feature.</span></span></p>
<p><span class="koboSpan" id="kobo.144.1">While there are many ways in which Spark can be used for feature engineering, we will focus on the following three related to improving </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.145.1">Chapter </span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.146.1">7</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.147.1">’s code:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.148.1">Column transformations</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.149.1">Resampling</span></span></li>
<li><span class="koboSpan" id="kobo.150.1">Lag </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">values calculation</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.152.1">Let’s begin with </span><a id="_idIndexMarker717"/><span class="koboSpan" id="kobo.153.1">column transformations in the </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">next section.</span></span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.155.1">Column transformations</span></h2>
<p><span class="koboSpan" id="kobo.156.1">In the first code example, we</span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.157.1"> will rewrite the column transformations code present in </span><strong class="source-inline"><span class="koboSpan" id="kobo.158.1">ts-spark_ch7_1e_lgbm_comm.dbc</span></strong><span class="koboSpan" id="kobo.159.1">, which is used in the </span><em class="italic"><span class="koboSpan" id="kobo.160.1">Development and testing</span></em><span class="koboSpan" id="kobo.161.1"> section of </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.162.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.163.1">. </span><span class="koboSpan" id="kobo.163.2">We will change the code to a Spark-enabled version by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.164.1">pyspark.sql.functions</span></strong><span class="koboSpan" id="kobo.165.1"> library. </span><span class="koboSpan" id="kobo.165.2">For this, we need to do </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.167.1">Replace the </span><strong class="source-inline"><span class="koboSpan" id="kobo.168.1">Date</span></strong><span class="koboSpan" id="kobo.169.1"> column with the concatenation (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.170.1">concat_ws</span></strong><span class="koboSpan" id="kobo.171.1"> function) of the existing </span><strong class="source-inline"><span class="koboSpan" id="kobo.172.1">Date</span></strong><span class="koboSpan" id="kobo.173.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">Time</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.175.1"> columns.</span></span></li>
<li><span class="koboSpan" id="kobo.176.1">Convert (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.177.1">to_timestamp</span></strong><span class="koboSpan" id="kobo.178.1"> function) the </span><strong class="source-inline"><span class="koboSpan" id="kobo.179.1">Date</span></strong><span class="koboSpan" id="kobo.180.1"> column into </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">timestamp format.</span></span></li>
<li><span class="koboSpan" id="kobo.182.1">Replace selectively (with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.183.1">when</span></strong><span class="koboSpan" id="kobo.184.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">otherwise</span></strong><span class="koboSpan" id="kobo.186.1"> condition) the incorrect values, </span><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">?</span></strong><span class="koboSpan" id="kobo.188.1">, in </span><strong class="source-inline"><span class="koboSpan" id="kobo.189.1">Global_active_power</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.190.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.191.1">None</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.193.1">Replace (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">regexp_replace</span></strong><span class="koboSpan" id="kobo.195.1"> function) </span><strong class="source-inline"><span class="koboSpan" id="kobo.196.1">,</span></strong><span class="koboSpan" id="kobo.197.1"> with </span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">.</span></strong><span class="koboSpan" id="kobo.199.1"> to be in the proper format for a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.200.1">float</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.201.1"> value.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.202.1">The following code extract demonstrates the </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">preceding steps:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.204.1">
from </span><strong class="bold"><span class="koboSpan" id="kobo.205.1">pyspark.sql</span></strong><span class="koboSpan" id="kobo.206.1"> import </span><strong class="bold"><span class="koboSpan" id="kobo.207.1">functions</span></strong><span class="koboSpan" id="kobo.208.1"> as F
# Combine 'Date' and 'Time' into a single 'Date' column of timestamp 
# type
df_all = df_all.withColumn(
    'Date',
    F.</span><strong class="bold"><span class="koboSpan" id="kobo.209.1">to_timestamp</span></strong><span class="koboSpan" id="kobo.210.1">(
        F.</span><strong class="bold"><span class="koboSpan" id="kobo.211.1">concat_ws</span></strong><span class="koboSpan" id="kobo.212.1">(' ', F.col('Date'), F.col('Time')),
        'd/M/yyyy HH:mm:ss')
)...
</span><span class="koboSpan" id="kobo.212.2"># Select only the 'cust_id', 'Date' and 'Global_active_power' columns
df_all = df_all.select(
    'cust_id', 'Date', 'Global_active_power'
)
# Replace '?' </span><span class="koboSpan" id="kobo.212.3">with None and convert 'Global_active_power' to float
df_all = df_all.withColumn(
    'Global_active_power',
    F.</span><strong class="bold"><span class="koboSpan" id="kobo.213.1">when</span></strong><span class="koboSpan" id="kobo.214.1">(F.col('Global_active_power') == '?', None)
    .</span><strong class="bold"><span class="koboSpan" id="kobo.215.1">otherwise</span></strong><span class="koboSpan" id="kobo.216.1">(F.</span><strong class="bold"><span class="koboSpan" id="kobo.217.1">regexp_replace</span></strong><span class="koboSpan" id="kobo.218.1">(
        'Global_active_power', ',', '.').cast('float')
    )
)
# Sort the DataFrame based on 'cust_id' and 'Date'
df_all = df_all.orderBy('cust_id', 'Date')</span></pre> <p><span class="koboSpan" id="kobo.219.1">After</span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.220.1"> leveraging Spark to parallelize the column transformations, the next code improvement we will be covering is for resampling the </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">time-series data.</span></span></p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.222.1">Resampling</span></h2>
<p><span class="koboSpan" id="kobo.223.1">In the second</span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.224.1"> code conversion example, we will rewrite the hourly resampling code present in </span><strong class="source-inline"><span class="koboSpan" id="kobo.225.1">ts-spark_ch7_1e_lgbm_comm.dbc</span></strong><span class="koboSpan" id="kobo.226.1">, which is used in the </span><em class="italic"><span class="koboSpan" id="kobo.227.1">Development and testing</span></em><span class="koboSpan" id="kobo.228.1"> section of </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.229.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.230.1">.  </span><span class="koboSpan" id="kobo.230.2">We want to calculate the hourly mean of </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.232.1"> for each customer. </span><span class="koboSpan" id="kobo.232.2">For this, we need to do </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.234.1">Convert the </span><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">Date</span></strong><span class="koboSpan" id="kobo.236.1"> column to its date and hour components using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">date_format</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.238.1"> function.</span></span></li>
<li><span class="koboSpan" id="kobo.239.1">Resample to the hourly mean of </span><strong class="source-inline"><span class="koboSpan" id="kobo.240.1">Global_active_power</span></strong><span class="koboSpan" id="kobo.241.1"> (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">agg</span></strong><span class="koboSpan" id="kobo.243.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">mean</span></strong><span class="koboSpan" id="kobo.245.1"> functions) for each customer (the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">groupBy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.247.1"> function).</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.248.1">The following code </span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.249.1">demonstrates the </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">preceding steps:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.251.1">
from pyspark.sql import functions as F
# Convert the 'Date' column to a string representing the
# start of the hour for each timestamp
data_hr = df_all.withColumn(
    'Date',
    F.</span><strong class="bold"><span class="koboSpan" id="kobo.252.1">date_format</span></strong><span class="koboSpan" id="kobo.253.1">('Date', 'yyyy-MM-dd HH:00:00'))
# Group the data by 'cust_id' and the hourly 'Date',
# then calculate the mean 'Global_active_power' for each group
data_hr = data_hr.</span><strong class="bold"><span class="koboSpan" id="kobo.254.1">groupBy</span></strong><span class="koboSpan" id="kobo.255.1">(
    'cust_id', 'Date').</span><strong class="bold"><span class="koboSpan" id="kobo.256.1">agg</span></strong><span class="koboSpan" id="kobo.257.1">(
    F.</span><strong class="bold"><span class="koboSpan" id="kobo.258.1">mean</span></strong><span class="koboSpan" id="kobo.259.1">('Global_active_power').alias('Global_active_power')
)</span></pre> <p><span class="koboSpan" id="kobo.260.1">Now that we have </span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.261.1">used Spark to parallelize the resampling, the next code improvement we will be covering is for calculating the lag values of the </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">time-series data.</span></span></p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.263.1">Calculating lag values</span></h2>
<p><span class="koboSpan" id="kobo.264.1">In the third</span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.265.1"> example of scaling feature engineering with Apache Spark, we will rewrite the lag calculation code present in </span><strong class="source-inline"><span class="koboSpan" id="kobo.266.1">ts-spark_ch7_1e_lgbm_comm.dbc</span></strong><span class="koboSpan" id="kobo.267.1">, which is used in the </span><em class="italic"><span class="koboSpan" id="kobo.268.1">Development and testing</span></em><span class="koboSpan" id="kobo.269.1"> section of </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.270.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.271.1">. </span><span class="koboSpan" id="kobo.271.2">We want to calculate different lag values for each customer. </span><span class="koboSpan" id="kobo.271.3">For this, we need to do </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.273.1">Define a sliding date window over which to calculate the lags for each of the customers (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.274.1">partitionBy</span></strong><span class="koboSpan" id="kobo.275.1"> function). </span><span class="koboSpan" id="kobo.275.2">We have the dates ordered (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.276.1">orderBy</span></strong><span class="koboSpan" id="kobo.277.1"> function) for </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">each customer.</span></span></li>
<li><span class="koboSpan" id="kobo.279.1">Calculate the different lags over the sliding window (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">lag</span></strong><span class="koboSpan" id="kobo.281.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.282.1">over</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.283.1"> functions).</span></span></li>
<li><span class="koboSpan" id="kobo.284.1">Note that as the lag calculation is based on previous values, some of the lag values at the beginning of the dataset will not have enough prior values for calculation and will be empty. </span><span class="koboSpan" id="kobo.284.2">We remove the rows with these empty lag values using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">dropna</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.286.1"> function.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.287.1">The following code demonstrates the </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">preceding steps:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.289.1">
from pyspark.sql.window import Window
from pyspark.sql import functions as F
# Define a window specification partitioned by -
# 'cust_id' and ordered by the 'Date' column
windowSpec = </span><strong class="bold"><span class="koboSpan" id="kobo.290.1">Window</span></strong><span class="koboSpan" id="kobo.291.1">.</span><strong class="bold"><span class="koboSpan" id="kobo.292.1">partitionBy</span></strong><span class="koboSpan" id="kobo.293.1">("cust_id").</span><strong class="bold"><span class="koboSpan" id="kobo.294.1">orderBy</span></strong><span class="koboSpan" id="kobo.295.1">("Date")
# Add lagged features to the DataFrame to incorporate
#  past values as features for forecasting
# Apply the lag function to create the lagged column,
#  separately for each 'cust_id'
# Lag by 1, 2, 3, 4, 5, 12, 24, 168 hours (24 hours * 7 days)
lags = [1, 2, 3, 4, 5, 12, 24, 24*7]
for l in lags:
    data_hr = data_hr.withColumn(
        'Global_active_power_lag' + str(l),
        F.</span><strong class="bold"><span class="koboSpan" id="kobo.296.1">lag</span></strong><span class="koboSpan" id="kobo.297.1">(F.col('Global_active_power'), l).</span><strong class="bold"><span class="koboSpan" id="kobo.298.1">over</span></strong><span class="koboSpan" id="kobo.299.1">(windowSpec))
# Remove rows with NaN values that were introduced by
#  shifting (lagging) operations
data_hr = data_hr.</span><strong class="bold"><span class="koboSpan" id="kobo.300.1">dropna</span></strong><span class="koboSpan" id="kobo.301.1">()</span></pre> <p><span class="koboSpan" id="kobo.302.1">By using Spark functions instead of pandas, we will enable Spark to parallelize the lag calculations for </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">large datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">Now that we have </span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.305.1">covered the different ways of leveraging Apache Spark to improve the feature engineering part of </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.306.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.307.1">’s code, we will dive deep into the scaling out of </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">model training.</span></span></p>
<h1 id="_idParaDest-161"><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.309.1">Model training</span></h1>
<p><span class="koboSpan" id="kobo.310.1">In this section, we will cover</span><a id="_idIndexMarker725"/><span class="koboSpan" id="kobo.311.1"> the following different ways that Apache Spark can be used for model training </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">at scale:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.313.1">Hyperparameter tuning</span></span></li>
<li><span class="koboSpan" id="kobo.314.1">Single model training </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">in parallel</span></span></li>
<li><span class="koboSpan" id="kobo.316.1">Multiple models training </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">in parallel</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.318.1">These approaches enable efficient model training when we have large datasets or many models </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">to train.</span></span></p>
<p><span class="koboSpan" id="kobo.320.1">Hyperparameter tuning can be an expensive computation when the same model is trained repeatedly with many different hyperparameters. </span><span class="koboSpan" id="kobo.320.2">We want to be able to leverage Spark to find the best </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">hyperparameters efficiently.</span></span></p>
<p><span class="koboSpan" id="kobo.322.1">Similarly, training a single model on a large dataset can take a long time. </span><span class="koboSpan" id="kobo.322.2">In other cases, we may have many models to train for distinct time-series datasets. </span><span class="koboSpan" id="kobo.322.3">We want to speed these up by parallelizing the training on </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">Spark clusters.</span></span></p>
<p><span class="koboSpan" id="kobo.324.1">We will go into the</span><a id="_idIndexMarker726"/><span class="koboSpan" id="kobo.325.1"> details of these approaches next, starting with hyperparameter tuning in the </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">next section.</span></span></p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.327.1">Hyperparameter tuning</span></h2>
<p><span class="koboSpan" id="kobo.328.1">As discussed </span><a id="_idIndexMarker727"/><span class="koboSpan" id="kobo.329.1">in </span><a href="B18568_04.xhtml#_idTextAnchor087"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.330.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.331.1">, hyperparameter </span><a id="_idIndexMarker728"/><span class="koboSpan" id="kobo.332.1">tuning in machine learning is the process of finding the best set of configurations for a machine learning algorithm. </span><span class="koboSpan" id="kobo.332.2">This search for optimal hyperparameters can be parallelized using libraries such as GridSearchCV, Hyperopt, and Optuna, which provide the framework, in conjunction with Apache Spark, for the backend </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">processing parallelism.</span></span></p>
<p><span class="koboSpan" id="kobo.334.1">We discussed Spark’s processing parallelism in </span><a href="B18568_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.335.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.336.1">. </span><span class="koboSpan" id="kobo.336.2">Here we will focus more specifically on the use of Optuna in conjunction with Apache Spark for </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">hyperparameter tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.338.1">If you recall, in </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.339.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.340.1">, we used GridSearchCV on a single node to tune the hyperparameters of the LightGBM model. </span><span class="koboSpan" id="kobo.340.2">We will improve on this in the code example in this section by parallelizing the process. </span><span class="koboSpan" id="kobo.340.3">We will use Optuna with Spark to find the best hyperparameters for the LightGBM model we explored in </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.341.1">Chapter 7</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.342.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.343.1">Optuna is an open source hyperparameter optimization framework that is used to automate hyperparameter searches. </span><span class="koboSpan" id="kobo.343.2">You can find more information on Optuna </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">here:</span></span><span class="No-Break"> </span><a href="https://optuna.org/"><span class="No-Break"><span class="koboSpan" id="kobo.345.1">https://optuna.org/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.346.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.347.1">We will begin the tuning process by defining an </span><strong class="source-inline"><span class="koboSpan" id="kobo.348.1">objective</span></strong><span class="koboSpan" id="kobo.349.1"> function (which we will later optimize using Optuna). </span><span class="koboSpan" id="kobo.349.2">This </span><strong class="source-inline"><span class="koboSpan" id="kobo.350.1">objective</span></strong><span class="koboSpan" id="kobo.351.1"> function does </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.353.1">Define the search space in </span><strong class="source-inline"><span class="koboSpan" id="kobo.354.1">params</span></strong><span class="koboSpan" id="kobo.355.1"> with the range of </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">hyperparameter values.</span></span></li>
<li><span class="koboSpan" id="kobo.357.1">Initialize the LightGBM </span><strong class="source-inline"><span class="koboSpan" id="kobo.358.1">LGBMRegressor</span></strong><span class="koboSpan" id="kobo.359.1"> model with the parameters specific to </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">the trial.</span></span></li>
<li><span class="koboSpan" id="kobo.361.1">Train (</span><strong class="source-inline"><span class="koboSpan" id="kobo.362.1">fit</span></strong><span class="koboSpan" id="kobo.363.1">) the model on the </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">training dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.365.1">Use the model to predict the </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">validation dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.367.1">Calculate the model evaluation </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">metric (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.369.1">mean_absolute_percentage_error</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">).</span></span></li>
<li><span class="koboSpan" id="kobo.371.1">Return the </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">evaluation metric.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.373.1">The following </span><a id="_idIndexMarker729"/><span class="koboSpan" id="kobo.374.1">code demonstrates</span><a id="_idIndexMarker730"/><span class="koboSpan" id="kobo.375.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">preceding steps:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.377.1">
import lightgbm as lgb
from sklearn.metrics import mean_absolute_percentage_error
import </span><strong class="bold"><span class="koboSpan" id="kobo.378.1">optuna</span></strong><span class="koboSpan" id="kobo.379.1">
def </span><strong class="bold"><span class="koboSpan" id="kobo.380.1">objective</span></strong><span class="koboSpan" id="kobo.381.1">(trial):
    # Define the hyperparameter configuration space
    </span><strong class="bold"><span class="koboSpan" id="kobo.382.1">params</span></strong><span class="koboSpan" id="kobo.383.1"> = {
        # Specify the learning task and
        #  the corresponding learning objective:
        "objective": "regression",
        # Evaluation metric for the model performance:
        "metric": "rmse",
        # Number of boosted trees to fit:
        "n_estimators": trial.suggest_int("n_estimators", 50, 200),
        # Learning rate for gradient descent:
        "learning_rate": trial.suggest_float(
            "learning_rate", 0.001, 0.1, log=True),
        # Maximum tree leaves for base learners:
        "num_leaves": trial.suggest_int("num_leaves", 30, 100),
    }
    # Initialize the LightGBM model with the trial's parameters:
    model = lgb.</span><strong class="bold"><span class="koboSpan" id="kobo.384.1">LGBMRegressor</span></strong><span class="koboSpan" id="kobo.385.1">(**params)
    # Train the model with the training dataset:
    model.</span><strong class="bold"><span class="koboSpan" id="kobo.386.1">fit</span></strong><span class="koboSpan" id="kobo.387.1">(X_train, y_train)
    # Generate predictions for the validation dataset:
    y_pred = model.</span><strong class="bold"><span class="koboSpan" id="kobo.388.1">predict</span></strong><span class="koboSpan" id="kobo.389.1">(X_test)
    # Calculate the Mean Absolute Percentage Error (MAPE)
    #  for model evaluation:
    mape = </span><strong class="bold"><span class="koboSpan" id="kobo.390.1">mean_absolute_percentage_error</span></strong><span class="koboSpan" id="kobo.391.1">(y_test, y_pred)
    # Return the MAPE as the objective to minimize
    return </span><strong class="bold"><span class="koboSpan" id="kobo.392.1">mape</span></strong></pre> <p><span class="koboSpan" id="kobo.393.1">Once the</span><a id="_idIndexMarker731"/><span class="koboSpan" id="kobo.394.1"> objective function is</span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.395.1"> defined, the next steps are </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.397.1">Register Spark (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.398.1">register_spark</span></strong><span class="koboSpan" id="kobo.399.1"> function) as </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">the backend.</span></span></li>
<li><span class="koboSpan" id="kobo.401.1">Create a study (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.402.1">create_study</span></strong><span class="koboSpan" id="kobo.403.1"> function), which is a collection of trials, to minimize the </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">evaluation metric.</span></span></li>
<li><span class="koboSpan" id="kobo.405.1">Run the study on the Spark </span><strong class="source-inline"><span class="koboSpan" id="kobo.406.1">parallel_backend</span></strong><span class="koboSpan" id="kobo.407.1"> to optimize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">objective</span></strong><span class="koboSpan" id="kobo.409.1"> function </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">over </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.411.1">n_trials</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.413.1">The following code demonstrates the </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">preceding steps:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.415.1">
from joblibspark import register_spark
# This line registers Apache Spark as the backend for
# parallel computing with Joblib, enabling distributed
# computing capabilities for Joblib-based parallel tasks.
</span><strong class="bold"><span class="koboSpan" id="kobo.416.1">register_spark</span></strong><span class="koboSpan" id="kobo.417.1">()
…
# Create a new study object with the goal of minimizing the objective # function
study2 = optuna.</span><strong class="bold"><span class="koboSpan" id="kobo.418.1">create_study</span></strong><span class="koboSpan" id="kobo.419.1">(direction='minimize')
# Set Apache Spark as the backend for parallel execution of –
# trials with unlimited jobs
with joblib.</span><strong class="bold"><span class="koboSpan" id="kobo.420.1">parallel_backend</span></strong><span class="koboSpan" id="kobo.421.1">("spark", n_jobs=-1):
    # Optimize the study by evaluating the –
    #  objective function over 10 trials:
    study2.</span><strong class="bold"><span class="koboSpan" id="kobo.422.1">optimize</span></strong><span class="koboSpan" id="kobo.423.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.424.1">objective</span></strong><span class="koboSpan" id="kobo.425.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.426.1">n_trials</span></strong><span class="koboSpan" id="kobo.427.1">=10)</span></pre> <p><span class="koboSpan" id="kobo.428.1">We have at this</span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.429.1"> point the result of the </span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.430.1">optimization. </span><span class="koboSpan" id="kobo.430.2">We can now display the evaluation metric (</span><strong class="source-inline"><span class="koboSpan" id="kobo.431.1">trial.value</span></strong><span class="koboSpan" id="kobo.432.1">) and parameters (</span><strong class="source-inline"><span class="koboSpan" id="kobo.433.1">trial.params</span></strong><span class="koboSpan" id="kobo.434.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">for </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.436.1">best_trial</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.438.1">
# Retrieve the best trial from the optimization study
trial = study2.</span><strong class="bold"><span class="koboSpan" id="kobo.439.1">best_trial</span></strong><span class="koboSpan" id="kobo.440.1">
# Print the best trial's objective function value,
#  typically accuracy or loss
print(f"Best trial accuracy: {</span><strong class="bold"><span class="koboSpan" id="kobo.441.1">trial.value</span></strong><span class="koboSpan" id="kobo.442.1">}")
print("Best trial params: ")
# Iterate through the best trial's hyperparameters and print them
for key, value in </span><strong class="bold"><span class="koboSpan" id="kobo.443.1">trial.params</span></strong><span class="koboSpan" id="kobo.444.1">.items():
    print(f"    {key}: {value}")</span></pre> <p><span class="koboSpan" id="kobo.445.1">The outcome of the hyperparameter tuning, shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.446.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.447.1">.2</span></em><span class="koboSpan" id="kobo.448.1">, is the best hyperparameters found within the search space specified, as well as the related </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">model accuracy.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer131">
<span class="koboSpan" id="kobo.450.1"><img alt="" src="image/B18568_08_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.451.1">Figure 8.2: Hyperparameter tuning – best trials</span></p>
<p><span class="koboSpan" id="kobo.452.1">In addition to the scaling of the hyperparameter tuning stage, which we have seen in this section, Spark </span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.453.1">clusters can </span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.454.1">also be used to parallelize the next step, that is, fitting the model to the training data. </span><span class="koboSpan" id="kobo.454.2">We will cover </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">this next.</span></span></p>
<h2 id="_idParaDest-163"><a id="_idTextAnchor164"/><span class="koboSpan" id="kobo.456.1">Single model in parallel</span></h2>
<p><span class="koboSpan" id="kobo.457.1">Ensemble methods </span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.458.1">such as Random Forest and gradient boosting machines can benefit from task parallelism during the model training stage. </span><span class="koboSpan" id="kobo.458.2">Each tree in a Random Forest can be trained independently, making it possible to parallelize across multiple processors. </span><span class="koboSpan" id="kobo.458.3">Similarly in the case of Gradient Boosting models such as LightGBM and XGBoost, the tree’s construction can be parallelized, even though the boosting itself </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">is sequential,</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">In </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.461.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.462.1">’s example in the </span><em class="italic"><span class="koboSpan" id="kobo.463.1">Classical machine learning model</span></em><span class="koboSpan" id="kobo.464.1"> section, we used LightGBM. </span><span class="koboSpan" id="kobo.464.2">This model was not Spark enabled. </span><span class="koboSpan" id="kobo.464.3">Here, as we want to demonstrate training parallelism with a Spark-enabled Gradient Boosting model, we will use </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">SparkXGBRegressor</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.466.1"> instead.</span></span></p>
<p><span class="koboSpan" id="kobo.467.1">As a first step, we will build a vector of the features using </span><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">VectorAssember</span></strong><span class="koboSpan" id="kobo.469.1">, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.471.1">
from pyspark.ml.feature import </span><strong class="bold"><span class="koboSpan" id="kobo.472.1">VectorAssembler</span></strong><span class="koboSpan" id="kobo.473.1">
# Define a list to hold the names of the lag feature columns
inputCols = []
# Loop through the list of lag intervals to create feature column 
# names
for l in lags:
    inputCols.append('Global_active_power_lag' + str(l))
# Initialize VectorAssembler with the
# created feature column names and specify the output column name
assembler = </span><strong class="bold"><span class="koboSpan" id="kobo.474.1">VectorAssembler</span></strong><span class="koboSpan" id="kobo.475.1">(
    inputCols=inputCols, outputCol="features")</span></pre> <p><span class="koboSpan" id="kobo.476.1">We then </span><a id="_idIndexMarker738"/><span class="koboSpan" id="kobo.477.1">create the </span><strong class="source-inline"><span class="koboSpan" id="kobo.478.1">SparkXGBRegressor</span></strong><span class="koboSpan" id="kobo.479.1"> model object, setting </span><strong class="source-inline"><span class="koboSpan" id="kobo.480.1">num_workers</span></strong><span class="koboSpan" id="kobo.481.1"> to all available workers, and specifying the target column </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.483.1">label_col</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.484.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.485.1">
from xgboost.spark import </span><strong class="bold"><span class="koboSpan" id="kobo.486.1">SparkXGBRegressor</span></strong><span class="koboSpan" id="kobo.487.1">
# Initialize the SparkXGBRegressor for the regression task.
</span><span class="koboSpan" id="kobo.487.2"># `num_workers` is set to the default parallelism level of -
#   the Spark context to utilize all available cores.
</span><span class="koboSpan" id="kobo.487.3"># `label_col` specifies the target variable column name for 
# prediction.
</span><span class="koboSpan" id="kobo.487.4"># `missing` is set to 0.0 to handle missing values in the dataset.
</span><span class="koboSpan" id="kobo.487.5">xgb_model = </span><strong class="bold"><span class="koboSpan" id="kobo.488.1">SparkXGBRegressor</span></strong><span class="koboSpan" id="kobo.489.1">(
    </span><strong class="bold"><span class="koboSpan" id="kobo.490.1">num_workers</span></strong><span class="koboSpan" id="kobo.491.1">=sc.defaultParallelism,
    </span><strong class="bold"><span class="koboSpan" id="kobo.492.1">label_col</span></strong><span class="koboSpan" id="kobo.493.1">="Global_active_power", missing=0.0
)</span></pre> <p><span class="koboSpan" id="kobo.494.1">As we have seen so far, hyperparameter tuning is an important step in finding the best model. </span><span class="koboSpan" id="kobo.494.2">In the following code example, we will use </span><strong class="source-inline"><span class="koboSpan" id="kobo.495.1">ParamGridBuilder</span></strong><span class="koboSpan" id="kobo.496.1"> to specify the range of parameters that are specific to the model and that we want </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">to evaluate.</span></span></p>
<p><span class="koboSpan" id="kobo.498.1">We then pass the parameters to </span><strong class="source-inline"><span class="koboSpan" id="kobo.499.1">CrossValidator</span></strong><span class="koboSpan" id="kobo.500.1"> together with </span><strong class="source-inline"><span class="koboSpan" id="kobo.501.1">RegressionEvaluator</span></strong><span class="koboSpan" id="kobo.502.1">. </span><span class="koboSpan" id="kobo.502.2">We will use the root mean square error (</span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">rmse</span></strong><span class="koboSpan" id="kobo.504.1">) as the evaluation metric. </span><span class="koboSpan" id="kobo.504.2">This is the default metric for </span><strong class="source-inline"><span class="koboSpan" id="kobo.505.1">RegressionEvaluator</span></strong><span class="koboSpan" id="kobo.506.1">, making it suitable for our </span><a id="_idIndexMarker739"/><span class="No-Break"><span class="koboSpan" id="kobo.507.1">example here:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.508.1">
from pyspark.ml.tuning import </span><strong class="bold"><span class="koboSpan" id="kobo.509.1">CrossValidator</span></strong><span class="koboSpan" id="kobo.510.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.511.1">ParamGridBuilder</span></strong><span class="koboSpan" id="kobo.512.1">
from pyspark.ml.evaluation import </span><strong class="bold"><span class="koboSpan" id="kobo.513.1">RegressionEvaluator</span></strong><span class="koboSpan" id="kobo.514.1">
# Initialize the parameter grid for hyperparameter tuning
# - max_depth: specifies the maximum depth of the trees in the model
# - n_estimators: defines the number of trees in the ensemble
paramGrid = </span><strong class="bold"><span class="koboSpan" id="kobo.515.1">ParamGridBuilder</span></strong><span class="koboSpan" id="kobo.516.1">()\
    .addGrid(xgb_model.max_depth, [5, 10])\
    .addGrid(xgb_model.n_estimators, [30, 100])\
    .build()
# Initialize the regression evaluator for model evaluation
# - metricName: specifies the metric to use for evaluation,
#    here RMSE (Root Mean Squared Error)
# - labelCol: the name of the label column
# - predictionCol: the name of the prediction column
evaluator = </span><strong class="bold"><span class="koboSpan" id="kobo.517.1">RegressionEvaluator</span></strong><span class="koboSpan" id="kobo.518.1">(
    metricName="rmse",
    LabelCol = xgb_model.getLabelCol(),
    PredictionCol = xgb_model.getPredictionCol()
)
# Initialize the CrossValidator for hyperparameter tuning
# - estimator: the model to be tuned
# - evaluator: the evaluator to be used for model evaluation
# - estimatorParamMaps: the grid of parameters to be used for tuning
cv = </span><strong class="bold"><span class="koboSpan" id="kobo.519.1">CrossValidator</span></strong><span class="koboSpan" id="kobo.520.1">(
    estimator = xgb_model, evaluator = evaluator,
    estimatorParamMaps = paramGrid)</span></pre> <p><span class="koboSpan" id="kobo.521.1">At this point, we are </span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.522.1">ready to build a pipeline (</span><strong class="source-inline"><span class="koboSpan" id="kobo.523.1">Pipeline</span></strong><span class="koboSpan" id="kobo.524.1">) to train (</span><strong class="source-inline"><span class="koboSpan" id="kobo.525.1">fit</span></strong><span class="koboSpan" id="kobo.526.1">) the model.  </span><span class="koboSpan" id="kobo.526.2">We will do this by combining in sequence the </span><strong class="source-inline"><span class="koboSpan" id="kobo.527.1">VectorAssembler</span></strong><span class="koboSpan" id="kobo.528.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.529.1">assembler</span></strong><span class="koboSpan" id="kobo.530.1">) and </span><strong class="source-inline"><span class="koboSpan" id="kobo.531.1">CrossValidator</span></strong><span class="koboSpan" id="kobo.532.1"> (</span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.533.1">cv</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">) stages:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.535.1">
from pyspark.ml import </span><strong class="bold"><span class="koboSpan" id="kobo.536.1">Pipeline</span></strong><span class="koboSpan" id="kobo.537.1">
# Initialize a Pipeline object with two stages:
# a feature assembler and a cross-validator for model tuning
pipeline = </span><strong class="bold"><span class="koboSpan" id="kobo.538.1">Pipeline</span></strong><span class="koboSpan" id="kobo.539.1">(stages = [</span><strong class="bold"><span class="koboSpan" id="kobo.540.1">assembler</span></strong><span class="koboSpan" id="kobo.541.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.542.1">cv</span></strong><span class="koboSpan" id="kobo.543.1">])</span></pre> <p><span class="koboSpan" id="kobo.544.1">In this example, we want to demonstrate parallelism for a single model corresponding to a single customer, so we will limit (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.545.1">filter</span></strong><span class="koboSpan" id="kobo.546.1"> function) the training data to </span><strong class="source-inline"><span class="koboSpan" id="kobo.547.1">cust_id</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.548.1">1</span></strong><span class="koboSpan" id="kobo.549.1">. </span><span class="koboSpan" id="kobo.549.2">We then take all the records (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.550.1">head</span></strong><span class="koboSpan" id="kobo.551.1"> function) for training except the last 48 hours as these will be used for testing. </span><span class="koboSpan" id="kobo.551.2">This results in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.552.1">train_hr</span></strong><span class="koboSpan" id="kobo.553.1"> DataFrame with the hourly </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">training data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.555.1">
# Filter the dataset for customer with cust_id equal to 1
train_hr = data_hr.</span><strong class="bold"><span class="koboSpan" id="kobo.556.1">filter</span></strong><span class="koboSpan" id="kobo.557.1">('</span><strong class="bold"><span class="koboSpan" id="kobo.558.1">cust_id</span></strong><span class="koboSpan" id="kobo.559.1"> == 1')
# Create a Spark DataFrame excluding the last 48 records for training
train_hr = spark.createDataFrame(
    train_hr.</span><strong class="bold"><span class="koboSpan" id="kobo.560.1">head</span></strong><span class="koboSpan" id="kobo.561.1">(train_hr.count() - 48)
)
# Fit the pipeline model to the training data
pipelineModel = pipeline.fit(train_hr)</span></pre> <p><span class="koboSpan" id="kobo.562.1">Similarly, for testing, we will filter in </span><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">cust_id</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.564.1">1</span></strong><span class="koboSpan" id="kobo.565.1"> and, in this case, use the last 48 hours. </span><span class="koboSpan" id="kobo.565.2">We can then</span><a id="_idIndexMarker741"/><span class="koboSpan" id="kobo.566.1"> apply (</span><strong class="source-inline"><span class="koboSpan" id="kobo.567.1">transform</span></strong><span class="koboSpan" id="kobo.568.1">) the model (</span><strong class="source-inline"><span class="koboSpan" id="kobo.569.1">pipelineModel</span></strong><span class="koboSpan" id="kobo.570.1">) to the test data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.571.1">test_hr</span></strong><span class="koboSpan" id="kobo.572.1">) to get the prediction of energy consumption for these </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">48 hours:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.574.1">
# Filter the dataset for customer with cust_id equal to 1 for testing
test_hr = data_hr.</span><strong class="bold"><span class="koboSpan" id="kobo.575.1">filter</span></strong><span class="koboSpan" id="kobo.576.1">('</span><strong class="bold"><span class="koboSpan" id="kobo.577.1">cust_id</span></strong><span class="koboSpan" id="kobo.578.1"> == 1')
# Create a Spark DataFrame including the last 48 records for testing
test_hr = spark.createDataFrame(train_hr.tail(48))
…
# Apply the trained pipeline model to the test data to generate 
# predictions
predictions = </span><strong class="bold"><span class="koboSpan" id="kobo.579.1">pipelineModel</span></strong><span class="koboSpan" id="kobo.580.1">.</span><strong class="bold"><span class="koboSpan" id="kobo.581.1">transform</span></strong><span class="koboSpan" id="kobo.582.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.583.1">test_hr</span></strong><span class="koboSpan" id="kobo.584.1">)</span></pre> <p><span class="koboSpan" id="kobo.585.1">Once we have the model’s predictions on the test data, we can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.586.1">RegressionEvaluator</span></strong><span class="koboSpan" id="kobo.587.1"> (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.588.1">evaluator</span></strong><span class="koboSpan" id="kobo.589.1"> object) to calculate (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.590.1">evaluate</span></strong><span class="koboSpan" id="kobo.591.1"> function) </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">the RMSE:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.593.1">
# Evaluate the model's performance using
# Root Mean Squared Error (RMSE) metric
rmse = </span><strong class="bold"><span class="koboSpan" id="kobo.594.1">evaluator</span></strong><span class="koboSpan" id="kobo.595.1">.</span><strong class="bold"><span class="koboSpan" id="kobo.596.1">evaluate</span></strong><span class="koboSpan" id="kobo.597.1">(predictions)</span></pre> <p><span class="koboSpan" id="kobo.598.1">For comparison, we also </span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.599.1">calculate the </span><strong class="bold"><span class="koboSpan" id="kobo.600.1">Symmetric Mean Absolute Percentage Error</span></strong><span class="koboSpan" id="kobo.601.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.602.1">SMAPE</span></strong><span class="koboSpan" id="kobo.603.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.604.1">Weighted Average Percentage Error</span></strong><span class="koboSpan" id="kobo.605.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.606.1">WAPE</span></strong><span class="koboSpan" id="kobo.607.1">) similarly </span><a id="_idIndexMarker743"/><span class="koboSpan" id="kobo.608.1">to how we have done in the </span><em class="italic"><span class="koboSpan" id="kobo.609.1">Classical machine learning model</span></em><span class="koboSpan" id="kobo.610.1"> section of </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.611.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.612.1">. </span><span class="koboSpan" id="kobo.612.2">The results</span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.613.1"> are shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.614.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.615.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer132">
<span class="koboSpan" id="kobo.617.1"><img alt="" src="image/B18568_08_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.618.1">Figure 8.3: XGBoost evaluation metrics</span></p>
<p><span class="koboSpan" id="kobo.619.1">We plot the forecast against the actual values in </span><em class="italic"><span class="koboSpan" id="kobo.620.1">Figures 8.4</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.621.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.622.1">8.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer133">
<span class="koboSpan" id="kobo.624.1"><img alt="" src="image/B18568_08_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.625.1">Figure 8.4: XGBoost forecast versus actuals (training and testing)</span></p>
<p><span class="koboSpan" id="kobo.626.1">We zoom in on</span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.627.1"> the testing period in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.628.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.629.1">.5</span></em><span class="koboSpan" id="kobo.630.1"> for a visual comparison of the forecast </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">and actuals.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<span class="koboSpan" id="kobo.632.1"><img alt="" src="image/B18568_08_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.633.1">Figure 8.5: XGBoost forecast versus actuals (zoom on test data)</span></p>
<p><span class="koboSpan" id="kobo.634.1">In this section, we have seen parallelism in single-model training. </span><span class="koboSpan" id="kobo.634.2">This requires the use of a library, such as XGBoost used here, which supports a multi-node processing backend such as Apache Spark. </span><span class="koboSpan" id="kobo.634.3">In addition to ensemble methods, other models, such as deep learning, can benefit from </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">training parallelism.</span></span></p>
<p><span class="koboSpan" id="kobo.636.1">Multiple models</span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.637.1"> can also be trained in parallel, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">explore next.</span></span></p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.639.1">Multiple models in parallel</span></h2>
<p><span class="koboSpan" id="kobo.640.1">Earlier in this</span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.641.1"> chapter, we scaled the dataset to represent the household energy consumption of multiple customers. </span><span class="koboSpan" id="kobo.641.2">In this section, we will train a different machine learning model for each customer in parallel. </span><span class="koboSpan" id="kobo.641.3">This is required if we want to predict the energy consumption of individual customers based on their own historical consumption. </span><span class="koboSpan" id="kobo.641.4">There are several other use cases where such multi-model training is required, for example, in the retail industry when doing sales forecasting for individual products </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">or stores.</span></span></p>
<p><span class="koboSpan" id="kobo.643.1">Coming back to our energy consumption example, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">train_model</span></strong><span class="koboSpan" id="kobo.645.1"> function does the following for </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">each customer:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.647.1">Get the customer ID (</span><strong class="source-inline"><span class="koboSpan" id="kobo.648.1">cust_id</span></strong><span class="koboSpan" id="kobo.649.1">) from the pandas DataFrame passed </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">as input.</span></span></li>
<li><span class="koboSpan" id="kobo.651.1">Choose the features (</span><strong class="source-inline"><span class="koboSpan" id="kobo.652.1">X</span></strong><span class="koboSpan" id="kobo.653.1">) and target (</span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.654.1">y</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">) variables.</span></span></li>
<li><span class="koboSpan" id="kobo.656.1">Split (</span><strong class="source-inline"><span class="koboSpan" id="kobo.657.1">train_test_split</span></strong><span class="koboSpan" id="kobo.658.1">) the dataset into training and testing, specifying </span><strong class="source-inline"><span class="koboSpan" id="kobo.659.1">shuffle</span></strong><span class="koboSpan" id="kobo.660.1"> as </span><strong class="source-inline"><span class="koboSpan" id="kobo.661.1">False</span></strong><span class="koboSpan" id="kobo.662.1"> to preserve the time order. </span><span class="koboSpan" id="kobo.662.2">As discussed in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.663.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.664.1">, this is an important consideration for </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">time-series datasets.</span></span></li>
<li><span class="koboSpan" id="kobo.666.1">Perform hyperparameter tuning with </span><strong class="source-inline"><span class="koboSpan" id="kobo.667.1">GridSearchCV</span></strong><span class="koboSpan" id="kobo.668.1"> using </span><strong class="source-inline"><span class="koboSpan" id="kobo.669.1">LGBMRegressor</span></strong><span class="koboSpan" id="kobo.670.1"> as the model and </span><strong class="source-inline"><span class="koboSpan" id="kobo.671.1">TimeSeriesSplit</span></strong><span class="koboSpan" id="kobo.672.1"> for the </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">dataset splits.</span></span></li>
<li><span class="koboSpan" id="kobo.674.1">Train (</span><strong class="source-inline"><span class="koboSpan" id="kobo.675.1">fit</span></strong><span class="koboSpan" id="kobo.676.1">) the final model with the best hyperparameters (</span><strong class="source-inline"><span class="koboSpan" id="kobo.677.1">best_params</span></strong><span class="koboSpan" id="kobo.678.1">) on the full </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">training dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.680.1">Test the final model on the test dataset and calculate the evaluation metrics (</span><strong class="source-inline"><span class="koboSpan" id="kobo.681.1">rmse</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.682.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.683.1">mape</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">).</span></span></li>
<li><span class="koboSpan" id="kobo.685.1">Return the result of </span><strong class="source-inline"><span class="koboSpan" id="kobo.686.1">train_model</span></strong><span class="koboSpan" id="kobo.687.1"> in a DataFrame with </span><strong class="source-inline"><span class="koboSpan" id="kobo.688.1">cust_id</span></strong><span class="koboSpan" id="kobo.689.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.690.1">best_params</span></strong><span class="koboSpan" id="kobo.691.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.692.1">rmse</span></strong><span class="koboSpan" id="kobo.693.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.695.1">mape</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.697.1">The following </span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.698.1">code shows the function definition with the </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">preceding steps:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.700.1">
def train_model(df_pandas: pd.DataFrame) -&gt; pd.DataFrame:
    # Extract the customer ID for which the model is being trained
    cust_id = df_pandas["</span><strong class="bold"><span class="koboSpan" id="kobo.701.1">cust_id</span></strong><span class="koboSpan" id="kobo.702.1">"].iloc[0]
    # Select features and target variables from the DataFrame
    </span><strong class="bold"><span class="koboSpan" id="kobo.703.1">X</span></strong><span class="koboSpan" id="kobo.704.1"> = df_pandas[[
        'Global_active_power_lag1', 'Global_active_power_lag2',
        'Global_active_power_lag3', 'Global_active_power_lag4',
        'Global_active_power_lag5', 'Global_active_power_lag12',
        'Global_active_power_lag24', 'Global_active_power_lag168'
    ]]
    </span><strong class="bold"><span class="koboSpan" id="kobo.705.1">y</span></strong><span class="koboSpan" id="kobo.706.1"> = df_pandas['Global_active_power']
    # Split the dataset into training and testing sets, preserving 
    # time order
    X_train, X_test, y_train, y_test = </span><strong class="bold"><span class="koboSpan" id="kobo.707.1">train_test_split</span></strong><span class="koboSpan" id="kobo.708.1">(
        X, y, test_size=0.2, </span><strong class="bold"><span class="koboSpan" id="kobo.709.1">shuffle</span></strong><span class="koboSpan" id="kobo.710.1">=</span><strong class="bold"><span class="koboSpan" id="kobo.711.1">False</span></strong><span class="koboSpan" id="kobo.712.1">, random_state=12
    )
    # Define the hyperparameter space for LightGBM model tuning
    param_grid = {
        'num_leaves': [30, 50, 100],
        'learning_rate': [0.1, 0.01, 0.001],
        'n_estimators': [50, 100, 200]
    }
    # Initialize the LightGBM regressor model
    lgbm = lgb.</span><strong class="bold"><span class="koboSpan" id="kobo.713.1">LGBMRegressor</span></strong><span class="koboSpan" id="kobo.714.1">()
    # Initialize TimeSeriesSplit for cross-validation to
    #  respect time series data structure
    tscv = </span><strong class="bold"><span class="koboSpan" id="kobo.715.1">TimeSeriesSplit</span></strong><span class="koboSpan" id="kobo.716.1">(n_splits=10)
    # Perform grid search with cross-validation
    gsearch = </span><strong class="bold"><span class="koboSpan" id="kobo.717.1">GridSearchCV</span></strong><span class="koboSpan" id="kobo.718.1">(
        estimator=lgbm, param_grid=param_grid, cv=tscv)
    gsearch.fit(X_train, y_train)
    # Extract the best hyperparameters
    best_params = gsearch.best_params_
    # Train the final model using the best parameters
    final_model = lgb.LGBMRegressor(**</span><strong class="bold"><span class="koboSpan" id="kobo.719.1">best_params</span></strong><span class="koboSpan" id="kobo.720.1">)
    final_model.</span><strong class="bold"><span class="koboSpan" id="kobo.721.1">fit</span></strong><span class="koboSpan" id="kobo.722.1">(X_train, y_train)
    # Make predictions on the test set
    y_pred = final_model.predict(X_test)
    # Calculate RMSE and MAPE metrics
    </span><strong class="bold"><span class="koboSpan" id="kobo.723.1">rmse</span></strong><span class="koboSpan" id="kobo.724.1"> = np.sqrt(mean_squared_error(y_test, y_pred))
    </span><strong class="bold"><span class="koboSpan" id="kobo.725.1">mape</span></strong><span class="koboSpan" id="kobo.726.1"> = mean_absolute_percentage_error(y_test, y_pred)
    # Prepare the results DataFrame to return
    return_df = pd.DataFrame(
        [[cust_id, str(best_params), rmse, mape]],
        columns=["cust_id", "best_params", "rmse", "mape"]
    )
    return return_df</span></pre> <p><span class="koboSpan" id="kobo.727.1">Now that the model</span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.728.1"> training function is defined, we can launch it in parallel for each customer (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.729.1">groupBy</span></strong><span class="koboSpan" id="kobo.730.1"> function), passing a pandas DataFrame of all the rows for this specific customer to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.731.1">applyInPandas</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.732.1"> function.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.733.1">pandas UDFs, mapInPandas, and applyInPandas</span></p>
<p class="callout"><span class="koboSpan" id="kobo.734.1">Using Spark-enabled libraries, as we did in the previous section with single-model parallel training, is usually faster for large datasets than single-machine libraries. </span><span class="koboSpan" id="kobo.734.2">There are, however, cases when we have to use a library that isn’t implemented natively for Spark’s parallel processing. </span><span class="koboSpan" id="kobo.734.3">In these situations, we can use pandas </span><strong class="bold"><span class="koboSpan" id="kobo.735.1">User-Defined Functions</span></strong><span class="koboSpan" id="kobo.736.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.737.1">UDFs</span></strong><span class="koboSpan" id="kobo.738.1">), </span><strong class="source-inline"><span class="koboSpan" id="kobo.739.1">mapInPandas</span></strong><span class="koboSpan" id="kobo.740.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.741.1">applyInPandas</span></strong><span class="koboSpan" id="kobo.742.1">. </span><span class="koboSpan" id="kobo.742.2">These methods allow you to call pandas</span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.743.1"> operations in a distributed way from Spark. </span><span class="koboSpan" id="kobo.743.2">The common use cases are </span><span class="No-Break"><span class="koboSpan" id="kobo.744.1">as follows:</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.745.1">- </span><strong class="bold"><span class="koboSpan" id="kobo.746.1">pandas UDF</span></strong><span class="koboSpan" id="kobo.747.1">: One input</span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.748.1"> row for one </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">output row</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.750.1">- </span><strong class="bold"><span class="koboSpan" id="kobo.751.1">mapInPandas</span></strong><span class="koboSpan" id="kobo.752.1">: One </span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.753.1">input row for multiple </span><span class="No-Break"><span class="koboSpan" id="kobo.754.1">output rows</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.755.1">- </span><strong class="bold"><span class="koboSpan" id="kobo.756.1">applyInPandas</span></strong><span class="koboSpan" id="kobo.757.1">: Multiple input </span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.758.1">rows for one </span><span class="No-Break"><span class="koboSpan" id="kobo.759.1">output row</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.760.1">Note that these are general guidance and that there is great flexibility in how these methods can </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">be used.</span></span></p>
<p><span class="koboSpan" id="kobo.762.1">In the example in this section, we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.763.1">applyInPandas</span></strong><span class="koboSpan" id="kobo.764.1"> as we want to execute a pandas-enabled function for all the rows in the dataset corresponding to a specific customer for model training. </span><span class="koboSpan" id="kobo.764.2">We want the function to output one row with the result of model training for the </span><span class="No-Break"><span class="koboSpan" id="kobo.765.1">specific customer.</span></span></p>
<p><span class="koboSpan" id="kobo.766.1">Note how, in the following code extract, we specified the </span><strong class="source-inline"><span class="koboSpan" id="kobo.767.1">train_model_result_schema</span></strong><span class="koboSpan" id="kobo.768.1"> schema of the function’s return value. </span><span class="koboSpan" id="kobo.768.2">This is a requirement for </span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.769.1">serializing the result that is added to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.770.1">train</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.771.1">_model_result_df</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.772.1">pandas DataFrame:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.773.1">
from pyspark.sql.functions import lit
# Group the data by customer ID and apply the
#  train_model function to each group using Pandas UDF
# The schema for the resulting DataFrame is defined by
#  train_model_result_schema
# Cache the resulting DataFrame to optimize performance for
#  subsequent actions
</span><strong class="bold"><span class="koboSpan" id="kobo.774.1">train_model_result_df</span></strong><span class="koboSpan" id="kobo.775.1"> = (
    data_hr
    .groupby("cust_id")
    .applyInPandas(train_model, schema=</span><strong class="bold"><span class="koboSpan" id="kobo.776.1">train_model_result_schema</span></strong><span class="koboSpan" id="kobo.777.1">)
    .cache()
)</span></pre> <p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.778.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.779.1">.6</span></em><span class="koboSpan" id="kobo.780.1"> shows the outcome of the multi-model training. </span><span class="koboSpan" id="kobo.780.2">It shows the best hyperparameters (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.781.1">best_params</span></strong><span class="koboSpan" id="kobo.782.1"> column) and evaluation metrics (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.783.1">rmse</span></strong><span class="koboSpan" id="kobo.784.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.785.1">mape</span></strong><span class="koboSpan" id="kobo.786.1"> columns) for </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">each customer.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<span class="koboSpan" id="kobo.788.1"><img alt="" src="image/B18568_08_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.789.1">Figure 8.6: Multi-model training – best hyperparameters and evaluation metrics</span></p>
<p><span class="koboSpan" id="kobo.790.1">With this example, we have trained five different models representing different customers. </span><span class="koboSpan" id="kobo.790.2">We have found the best hyperparameters to use for each model, which we are then able to </span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.791.1">use to do individual energy </span><span class="No-Break"><span class="koboSpan" id="kobo.792.1">consumption forecasting.</span></span></p>
<p><span class="koboSpan" id="kobo.793.1">With this, we conclude the different ways in which we can leverage Apache Spark to scale time-series analysis. </span><span class="koboSpan" id="kobo.793.2">Next, we will discuss some of the ways that the training process can </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">be optimized.</span></span></p>
<h1 id="_idParaDest-165"><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.795.1">Training optimization</span></h1>
<p><span class="koboSpan" id="kobo.796.1">When </span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.797.1">training machine learning models at a large scale, several inefficiencies and overheads can impact resource utilization and performance. </span><span class="koboSpan" id="kobo.797.2">These include </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.799.1">Idle time waiting for resources such as GPU, network, and storage accesses, which can delay the </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">training process.</span></span></li>
<li><span class="koboSpan" id="kobo.801.1">Frequent checkpointing, which saves the model during training to avoid restarting in case of failure. </span><span class="koboSpan" id="kobo.801.2">This results in additional storage and time during </span><span class="No-Break"><span class="koboSpan" id="kobo.802.1">model training.</span></span></li>
<li><span class="koboSpan" id="kobo.803.1">Hardware or software failures during the training result in restarts, which waste resources and delay </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">the training.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.805.1">The following mitigation techniques can be used, depending on the model being trained and the library </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">in use:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.807.1">Eliminate the cause of idle wait times by provisioning sufficient compute, network, and </span><span class="No-Break"><span class="koboSpan" id="kobo.808.1">storage resources</span></span></li>
<li><span class="koboSpan" id="kobo.809.1">Avoid too </span><span class="No-Break"><span class="koboSpan" id="kobo.810.1">frequent checkpointing</span></span></li>
<li><span class="koboSpan" id="kobo.811.1">Rearrange features based on correlation with the target variable or their importance to facilitate convergence during </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">model training</span></span></li>
<li><span class="koboSpan" id="kobo.813.1">Reduce the dimensionality of the dataset, choosing the most </span><span class="No-Break"><span class="koboSpan" id="kobo.814.1">informative features</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.815.1">While the implementation details of these techniques are beyond our scope here, we recommend researching and addressing these points when operating at a large scale due to the potentially high impact on cost, efficiency, </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">and scalability.</span></span></p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor167"/><span class="koboSpan" id="kobo.817.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.818.1">In this chapter, we saw the need to scale the processing capacity for bigger datasets. </span><span class="koboSpan" id="kobo.818.2">We examined different ways of using Apache Spark to this end. </span><span class="koboSpan" id="kobo.818.3">Building on and extending the code examples from </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.819.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.820.1">, we focused on scaling the feature engineering and model training stages. </span><span class="koboSpan" id="kobo.820.2">We looked at leveraging Spark to scale transformations, aggregations, lag values calculation, hyperparameter tuning, and single- and multi-model training </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">in parallel.</span></span></p>
<p><span class="koboSpan" id="kobo.822.1">In the next chapter, we will cover the considerations for going to production with time-series analysis, using and extending what we have learned </span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">so far.</span></span></p>
<h1 id="_idParaDest-167"><a id="_idTextAnchor168"/><span class="koboSpan" id="kobo.824.1">Join our community on Discord</span></h1>
<p><span class="koboSpan" id="kobo.825.1">Join our community’s Discord space for discussions with the authors and </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">other readers:</span></span></p>
<p><a href="https://packt.link/ds"><span class="No-Break"><span class="koboSpan" id="kobo.827.1">https://packt.link/ds</span></span></a></p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<span class="koboSpan" id="kobo.828.1"><img alt="" src="image/ds_(1).jpg"/></span>
</div>
</div>
</div>
</body></html>