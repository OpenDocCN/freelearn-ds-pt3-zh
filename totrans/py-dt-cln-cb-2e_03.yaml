- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Taking the Measure of Your Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Within a week of receiving a new dataset, at least one person is likely to ask
    us a familiar question – “so, how does it look?” This is not always asked relaxedly,
    and others are not usually excited to hear about all of the red flags we have
    already found. There might be a sense of urgency to declare the data ready for
    analysis. Of course, if we sign off on it too soon, this can create much larger
    problems; the presentation of invalid results, the misinterpretation of variable
    relationships, and having to redo major chunks of our analysis. The key is sorting
    out what we need to know about the data before we explore anything else in the
    data. The recipes in this chapter offer techniques for determining if the data
    is in good enough shape to begin the analysis, so that even if we cannot say,
    “it looks fine,” we can at least say, “I’m pretty sure I have identified the main
    issues, and here they are.”
  prefs: []
  type: TYPE_NORMAL
- en: Often our domain knowledge is quite limited, or at least not nearly as good
    as those who created the data. We have to quickly get a sense of what we are looking
    at even when we have little substantive understanding of the individuals or events
    reflected in the data. Many times (for some of us, most of the time) there is
    not anything like a data dictionary or codebook accompanying the receipt of the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quick. Ask yourself what the first few things you try to find out in this situation
    are; that is, when you first get data about which you know little. It is probably
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: How are the rows of the dataset uniquely identified? (What is the unit of analysis?)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many rows and columns are in the dataset?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the key categorical variables and the frequencies of each value?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are important continuous variables distributed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How might variables be related to each other – for example, how might the distribution
    of continuous variables vary according to categories in the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What variable values are out of expected ranges, and how are missing values
    distributed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We go over essential tools and strategies for answering the first four questions
    in this chapter. We look into the last two questions in the following chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'I should point out that this first take on our data is important even when
    the structure of the data is familiar; when, for example, we receive data for
    a new month or year with the same column names and data types as in previous periods.
    It is hard to guard against the sense that we can just rerun our old programs;
    it’s difficult to be as vigilant as we were the first few times we prepared the
    data for analysis. Most of us have probably been in situations where we receive
    new data with a familiar structure, but the answers to the preceding questions
    are meaningfully different: new valid values for key categorical variables; rare
    values that have always been permissible but that have not been seen for several
    periods; and unexpected changes in the status of clients/students/customers. It
    is important to build routines for understanding our data and follow them, regardless
    of our familiarity with it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting a first look at your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting and organizing columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating frequencies for categorical variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating summary statistics for continuous variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using generative AI to display descriptive statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need pandas, Numpy, and Matplotlib to complete the recipes in this
    chapter. I used pandas 2.1.4, but the code will run on pandas 1.5.3 or later.
  prefs: []
  type: TYPE_NORMAL
- en: The code in this chapter can be downloaded from the book’s github repository,
    [https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition).
  prefs: []
  type: TYPE_NORMAL
- en: Getting a first look at your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will work with two datasets in this chapter: *The National Longitudinal
    Survey of Youth for 1997*, a survey conducted by the United States government
    that surveyed the same group of individuals from 1997 through 2023; and the counts
    of COVID-19 cases and deaths by country from *Our World in Data*.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will mainly be using the pandas library for this recipe. We will use pandas
    tools to take a closer look at the **National Longitudinal Survey** (**NLS**)
    and COVID-19 case data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: The NLS of Youth was conducted by the United States Bureau of Labor Statistics.
    This survey started with a cohort of individuals in 1997 who were born between
    1980 and 1985, with annual follow-ups each year through to 2023\. For this recipe,
    I pulled 89 variables on grades, employment, income, and attitudes toward government
    from the hundreds of data items in the survey. Separate files for SPSS, Stata,
    and SAS can be downloaded from the repository. The NLS data can be downloaded
    from [https://www.nlsinfo.org](https://www.nlsinfo.org). You must create an investigator
    account to download the data, but there is no charge.
  prefs: []
  type: TYPE_NORMAL
- en: Our World in Data provides COVID-19 public use data at [https://ourworldindata.org/covid-cases](https://ourworldindata.org/covid-cases).
    The dataset includes total cases and deaths, tests administered, hospital beds,
    and demographic data such as median age, gross domestic product, and a human development
    index, which is a composite measure of standard of living, educational levels,
    and life expectancy. The dataset used in this recipe was downloaded on March 3,
    2024
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will get an initial look at the NLS and COVID-19 data, including the number
    of rows and columns, and the data types:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries and load the DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set and show the index and the size of the `nls97` data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Also, check to see whether the index values are unique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Show the data types and `non-null` value counts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Show the first 2 rows of the `nls97` data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use transpose to show a little more of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Set and show the index and size for the COVID-19 data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Also, check to see whether the index values are unique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Show the data types and `non-null` value counts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Show a sample of 2 rows of the COVID-19 data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This has given us a good foundation for understanding our DataFrames, including
    their sizes and column data types.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We set and display the index of the `nls97` DataFrame, which is called `personid`,
    in *Step 2*. It is a more meaningful index than the default pandas `RangeIndex`,
    which is essentially the row numbers with zero base. Often there is a unique identifier
    when working with individuals as the unit of analysis. This is a good candidate
    for an index. It makes selecting a row by that identifier easier. Rather than
    using the `nls97.loc[personid==1000061]` statement to get the row for that person,
    we can use `nls97.loc[1000061]`. We will try this out in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: pandas makes it easy to view the number of rows and columns, the data type and
    numbers of non-missing values for each column, and the values of the columns for
    a few rows of your data. This can be accomplished by using the `shape` attribute
    and calling the `info` method, followed by either the `head` or `sample` methods.
    Using the `head(2)` method shows the first two rows, but sometimes it is helpful
    to grab a row from anywhere in the DataFrame, in which case we would use `sample`.
    (We set the seed when we call `sample` (`random_state=1`) to get the same results
    whenever we run the code.) We can chain our call to `head` or `sample` with a
    `T` to transpose it. This reverses the display of rows and columns. That is helpful
    when there are more columns than can be shown horizontally and you want to be
    able to see all of them. By transposing the rows and columns we are able to see
    all of the columns.
  prefs: []
  type: TYPE_NORMAL
- en: The `shape` attribute of the `nls97` DataFrame tells us that there are 8,984
    rows and 88 non-index columns. Since `personid` is the index, it is not included
    in the column count. The `info` method shows us that many of the columns have
    object data types and that some have a large number of missing values. `satverbal`
    and `satmath` have only about 1,400 valid values.
  prefs: []
  type: TYPE_NORMAL
- en: The `shape` attribute of the `covidtotals` DataFrame tells us that there are
    231 rows and 16 columns, which does not include the country `iso_code` column
    used for the index (`iso_code` is a unique three-digit identifier for each country).
    The key variables for most analyses we would do are `total_cases`, `total_deaths`,
    `total_cases_pm,` and `total_deaths_pm`. `total_cases` and `total_deaths` are
    present for each country, but `total_cases_pm` and `total_deaths_pm` are missing
    for one country.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I find that thinking through the index when working with a data file can remind
    me of the unit of analysis. That is not obvious with the NLS data, as it is actually
    panel data disguised as person-level data. Panel, or longitudinal, datasets have
    data for the same individuals over some regular duration. In this case, data was
    collected for each person over a 26-year span, from 1997 till 2023\. The administrators
    of the survey have flattened it for analysis purposes by creating columns for
    certain responses over the years, such as college enrollment (`colenroct15` through
    `colenroct17`). This is a fairly standard practice, but it is likely that we will
    need to do some reshaping for some analyses.
  prefs: []
  type: TYPE_NORMAL
- en: One thing I pay careful attention to when receiving any panel data is any drop-off
    in responses to key variables over time. Notice the drop off in valid values from
    `colenroct15` to `colenroct17`. By October of 2017, only 75% of respondents provided
    a valid response (6,734/8,984). That is definitely worth keeping in mind during
    subsequent analysis, since the 6,734 remaining respondents may be different in
    important ways from the overall sample of 8,984.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A recipe in *Chapter 1*, *Anticipating Data Cleaning Issues When Importing Tabular
    Data with pandas*, shows how to persist pandas DataFrames as feather or pickle
    files. In later recipes in this chapter, we will look at descriptives and frequencies
    for these two DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: We reshape the NLS data in *Chapter 11*, *Tidying and Reshaping Data*, recovering
    some of its actual structure as panel data. This is necessary for statistical
    methods such as survival analysis, and is closer to tidy data ideals.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting and organizing columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explore several ways to select one or more columns from your DataFrame in
    this recipe. We can select columns by passing a list of column names to the `[]`
    bracket operator, or by using the pandas-specific `loc` and `iloc` data accessors.
  prefs: []
  type: TYPE_NORMAL
- en: When cleaning data or doing exploratory or statistical analyses, it is helpful
    to focus on the variables that are relevant to the issue or analysis at hand.
    This makes it important to group columns according to their substantive or statistical
    relationships with each other, or to limit the columns we are investigating at
    any one time. How many times have we said to ourselves something like, *“Why does
    variable A have a value of x when variable B has a value of y?”* We can only do
    that when the amount of data we are viewing at a given moment does not exceed
    our perceptive abilities at that moment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue working with the **National Longitudinal Survey** (**NLS**)
    data in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will explore several ways to select columns:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `pandas` library and load the NLS data into pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Also, convert all columns in the NLS data of the object data type to the category
    data type. Do this by selecting object data type columns with `select_dtypes`
    and using `transform` plus a `lambda` function to change the data type to `category`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Select a column using the pandas `[]` bracket operator and the `loc` and `iloc`
    accessors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We pass a string matching a column name to the bracket operator to return a
    pandas series. If we pass a list of one element with that column name (`nls97[[''gender'']]`),
    a DataFrame is returned. We can also use the `loc` and `iloc` accessors to select
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Select multiple columns from a pandas DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the bracket operator and `loc` to select a few columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Select multiple columns based on a list of columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you are selecting more than a few columns, it is helpful to create a list
    of column names separately. Here, we create a `keyvars` list of key variables
    for analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Select one or more columns by filtering by column name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select all of the `weeksworked##` columns using the `filter` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Select all columns of the category data type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `select_dtypes` method to select columns by data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Organize columns using lists of column names.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use lists to organize the columns in your DataFrame. You can easily change
    the order of columns or exclude some columns in this way. Here, we move the columns
    in the `demoadult` list to the front:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the new reorganized DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding steps showed how to select columns and change the order of columns
    in a `pandas` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both the `[]` bracket operator and the `loc` data accessor are very handy for
    selecting and organizing columns. Each returns a DataFrame when passed a list
    of names of columns. The columns will be ordered according to the passed list
    of column names.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 1*, we use `nls97.select_dtypes([''object''])` to select columns with
    object data type and chain that with `transform` and a `lambda` function (`transform(lambda
    x: x.astype(''category''))`) to change those columns to category. We use the `loc`
    accessor to only update columns with the object data type (`nls97.loc[:, nls97.dtypes
    == ''object'']`). We go into much more detail on the use of `transform`, `apply`
    (which works similarly to `transform`), and `lambda` functions in *Chapter 6*,
    *Cleaning and Exploring Data with Series Operations*.'
  prefs: []
  type: TYPE_NORMAL
- en: We select columns by data type in *Step 6*. `select_dtypes` becomes quite useful
    when passing columns to methods such as `describe` or `value_counts`, when you
    want to limit the analysis to continuous or categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 8*, we concatenate six different lists when using the bracket operator.
    This moves the column names in `demoadult` to the front and organizes all of the
    columns by those six groups. There are now clear *high school record* and *weeks
    worked* sections in our DataFrame’s columns.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also use `select_dtypes` to exclude data types. Also, if we are just
    interested in the `info` results, we can chain the `select_dtypes` call with the
    `info` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The `filter` operator can also take a regular expression. For example, you
    can return the columns that have `income` in their names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many of these techniques can be used to create `pandas` Series as well as DataFrames.
    We demonstrate this in *Chapter 6*, *Cleaning and Exploring Data with Series Operations*.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting rows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we are taking the measure of our data and otherwise answering the question,
    *“How does it look?”*, we constantly zoom in and out and look at aggregated numbers
    and particular rows. But there are also important data issues that are only obvious
    at an intermediate-zoom level, issues that we only notice when looking at some
    subset of rows. This recipe demonstrates how to use pandas tools to detect data
    issues in subsets of our data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue working with the NLS data in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will go over several techniques for selecting rows in a `pandas` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and `numpy`, and load the `nls97` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use slicing to start at the 1001^(st) row and go to the 1004^(th) row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`nls97[1000:1004]` selects every row starting from the row indicated by the
    integer to the left of the colon (`1000`, in this case) to, but not including,
    the row indicated by the integer to the right of the colon (`1004`). The row at
    `1000` is actually the 1001^(st) row because of zero-based indexing. Each row
    appears as a column in the output since we have transposed the resulting DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Use slicing to start at the 1001^(st) row and go to the 1004^(th) row, skipping
    every other row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The integer after the second colon (`2` in this case) indicates the size of
    the step. When the step is excluded it is assumed to be 1\. Notice that by setting
    the value of the step to `2`, we are skipping every other row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Select the first three rows using `[]` operator slicing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By not providing a value to the left of the colon in `[:3]`, we are telling
    the operator to get rows from the start of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Note that `nls97[:3]` returns the same DataFrame as `nls97.head(3)` would have
    returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the last three rows using `[]` operator slicing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that `nls97[-3:]` returns the same DataFrame as `nls97.tail(3)` would have
    returned.
  prefs: []
  type: TYPE_NORMAL
- en: Select a few rows using the `loc` data accessor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `loc` accessor to select by `index` label. We can pass a list of index
    labels or we can specify a range of labels. (Recall that we have set `personid`
    as the index.) Note that `nls97.loc[[195884,195891,195970]]` and `nls97.loc[195884:195970]`
    return the same DataFrame, since those rows are contiguous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Select a row from the beginning of the DataFrame with the `iloc` data accessor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`iloc` differs from `loc` in that it takes a list of row position integers,
    rather than index labels. For that reason, it works similarly to bracket operator
    slicing. In this step, we first pass a one-item list with the value of `0`. That
    returns a DataFrame with the first row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Select a few rows from the beginning of the DataFrame with the `iloc` data accessor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We pass a three-item list, `[0,1,2]`, to return a DataFrame of the first three
    rows of `nls97`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: We would get the same result if we passed `[0:3]` to the accessor.
  prefs: []
  type: TYPE_NORMAL
- en: Select a few rows from the end of the DataFrame with the `iloc` data accessor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use `nls97.iloc[[-3,-2,-1]]` to retrieve the last three rows of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: We would have gotten the same result with `nls97.iloc[-3:]`. By not providing
    a value to the right of the colon in `[-3:]`, we are telling the accessor to get
    all rows from the third-to-last row to the end of the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Select multiple rows conditionally using boolean indexing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a DataFrame of just individuals receiving very little sleep. About 5%
    of survey respondents got 4 or fewer hours of sleep per night, of the 6,706 individuals
    who responded to that question. Test who is getting 4 or fewer hours of sleep
    with `nls97.nightlyhrssleep<=4`, which generates a pandas Series of `True` and
    `False` values that we assign to `sleepcheckbool`. Pass that Series to the `loc`
    accessor to create a `lowsleep` DataFrame. `lowsleep` has approximately the number
    of rows we are expecting. We do not need to do the extra step of assigning the
    boolean Series to a variable. This is done here only for explanatory purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Select rows based on multiple conditions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It may be that folks who are not getting a lot of sleep also have a fair number
    of children who live with them. Use `describe` to get a sense of the distribution
    of the number of children for those who have `lowsleep`. About a quarter have
    three or more children. Create a new DataFrame with individuals who have `nightlyhrssleep`
    of 4 or less and number of children at home of 3 or more. The `&` is the logical
    *and* operator in pandas and indicates that both conditions have to be true for
    the row to be selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: We would have gotten the same result if we worked from the `lowsleep` DataFrame
    – `lowsleep3pluschildren = lowsleep.loc[lowsleep.childathome>=3]` – but then we
    would not have been able to demonstrate testing multiple conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Select rows and columns based on multiple conditions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass the condition to the `loc` accessor to select rows. Also, pass a list
    of column names to select:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: The preceding steps demonstrated the key techniques for selecting rows in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We used the `[]` bracket operator in *Steps 2* through *5* to do standard Python-like
    slicing to select rows. That operator allows us to easily select rows based on
    a list or a range of values indicated with slice notation. This notation takes
    the form of `[start:end:step]`, where a value of `1` for `step` is assumed if
    no value is provided. When a negative number is used for `start`, it represents
    the number of rows from the end of the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: The `loc` accessor, used in *Step 6*, selects rows based on row index labels.
    Since `personid` is the index for the DataFrame, we can pass a list of one or
    more `personid` values to the `loc` accessor to get a DataFrame with rows for
    those index labels. We can also pass a range of index labels to the accessor,
    which will return a DataFrame with all rows having index labels between the label
    to the left of the colon and the label to the right (inclusive); so, `nls97.loc[195884:195970]`
    returns a DataFrame for rows with `personid` between `195884` and `195970`, including
    those two values.
  prefs: []
  type: TYPE_NORMAL
- en: The `iloc` accessor works very much like the bracket operator. We see this in
    *Steps 7* through *9*. We can pass either a list of integers or a range using
    slicing notation.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most valuable pandas capabilities is boolean indexing. It makes it
    easy to select rows conditionally. We see this in *Step 10*. A test returns a
    boolean series. The `loc` accessor selects all rows for which the test is `True`.
    We actually didn’t need to assign the boolean data Series to the variable that
    we then passed to the `loc` operator. We could have just passed the test to the
    `loc` accessor with `nls97.loc[nls97.nightlyhrssleep<=4]`.
  prefs: []
  type: TYPE_NORMAL
- en: We should take a closer look at how we used the `loc` accessor to select rows
    in *Step 11*. Each condition in `nls97.loc[(nls97.nightlyhrssleep<=4) & (nls97.childathome>=3)]`
    is placed in parentheses. An error will be generated if the parentheses are excluded.
    The `&` operator is the equivalent of `and` in standard Python, meaning that *both*
    conditions have to be `True` for the given row to be selected. We would have used
    `|` for `or` if we had wanted to select the rows where *either* condition was
    `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, *Step 12* demonstrates how to select both rows and columns in one
    call to the `loc` accessor. The criteria for rows appear before the comma and
    the columns to select appear after the comma, as in the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: This returns the `nightlyhrssleep` and `childathome` columns for all rows where
    the individual has `nightlyhrssleep` of less than or equal to 4, and `childathome`
    greater than or equal to 3.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We used three different tools to select rows from a pandas DataFrame in this
    recipe: the `[]` bracket operator, and two pandas-specific accessors, `loc` and
    `iloc`. This is a little confusing if you are new to pandas, but it becomes clear
    which tool to use in which situation after just a few months. If you came to pandas
    with a fair bit of Python and NumPy experience, you likely find the `[]` operator
    most familiar. However, the pandas documentation recommends against using the
    `[]` operator for production code. I have settled on a routine of using that operator
    only for selecting columns from a DataFrame. I use the `loc` accessor when selecting
    rows by boolean indexing or by index label, and the `iloc` accessor for selecting
    rows by row number. Since my workflow has me using a fair bit of boolean indexing,
    I use `loc` much more than the other methods.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recipe immediately preceding this one, *Selecting and organizing columns*,
    has a more detailed discussion on selecting columns.
  prefs: []
  type: TYPE_NORMAL
- en: Generating frequencies for categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many years ago, a very seasoned researcher said to me, *“90% of what we’re going
    to find, we’ll see in the frequency distributions*.*”* That message has stayed
    with me. The more one-way and two-way frequency distributions (crosstabs) I do
    on a DataFrame, the better I understand it. We will do one-way distributions in
    this recipe, and crosstabs in subsequent recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We continue our work with the NLS. We will also be doing a fair bit of column
    selection using filter methods. It is not necessary to review the recipe in this
    chapter on column selection, but it might be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use pandas tools to generate frequencies, particularly the very handy `value_counts`:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the `pandas` library and the `nls97` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Also, convert the columns of object data type to the category data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Show the names of the columns of the category data type and check for the number
    of missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that there are no missing values for `gender` and only a few for `highestdegree`,
    but many for `maritalstatus` and other columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Show the frequencies for marital status:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Turn off sorting by frequency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Show percentages instead of counts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Show the percentages for all government responsibility columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Filter the DataFrame for just the government responsibility columns, then use
    `apply` to run `value_counts` on all columns in that DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Find the percentages, for all government responsibility columns, of people who
    are married.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Do what we did in *Step 6*, but first select only rows with marital status
    equal to `Married`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Find the frequencies and percentages for all category columns in the DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, open a file to write out the frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates a file, the beginning of which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: As these steps demonstrate, `value_counts` is quite useful when we need to generate
    frequencies for one or more columns of a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the columns in the `nls97` DataFrame (57 out of 88) have the object
    data type. If we are working with data that is logically categorical, but does
    not have a category data type in pandas, there are good reasons to convert it
    to the category type. Not only does this save memory; it also makes data cleaning
    a little easier, as we saw in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The star of the show for this recipe is the `value_counts` method. It can generate
    frequencies for a Series, as we did with `nls97.maritalstatus.value_counts`. It
    can also be run on a whole DataFrame as we did with `nls97.filter(like="gov").apply(pd.Series.value_counts,
    normalize=True)`. We first create a DataFrame with just the government responsibility
    columns and then pass the resulting DataFrame to `value_counts` with `apply`.
  prefs: []
  type: TYPE_NORMAL
- en: You probably noticed that in *Step 7*, I split the chaining over several lines
    to make it easier to read. There is no rule about when it makes sense to do that.
    I generally try to do that whenever the chaining involves three or more operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 8*, we iterated over all of the columns with the category data type:
    `for col in nls97.select_dtypes(include=["category"])`. For each of those columns,
    we ran `value_counts` to get frequencies and `value_counts` again to get percentages.
    We used a `print` function to generate the carriage returns necessary to make
    the output readable. All of this is saved to the `frequencies.txt` file in the
    `views` subfolder. I find it handy to have a bunch of one-way frequencies around
    just to check before doing any work with categorical variables. *Step 8* accomplishes
    that.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Frequency distributions may be the most important statistical tool for discovering
    potential data issues with categorical data. The one-way frequencies we generated
    in this recipe are a good foundation for further insights.
  prefs: []
  type: TYPE_NORMAL
- en: However, we often only detect problems once we examine the relationships between
    categorical variables and other variables, categorical or continuous. Although
    we stopped short of doing two-way frequencies in this recipe, we did start the
    process of splitting up the data for investigation in *Step 7*. In that step,
    we looked at government responsibility responses for married individuals and saw
    that those responses differed from those for the sample overall.
  prefs: []
  type: TYPE_NORMAL
- en: This raises several questions about our data that we need to explore. Are there
    important differences in response rates by marital status, and might this matter
    for the distribution of the government responsibility variables? We also want
    to be careful about drawing conclusions before considering potential confounding
    variables. Are married respondents likely to be older or to have more children,
    and are those more important factors in their government responsibility answers?
  prefs: []
  type: TYPE_NORMAL
- en: I am using the marital status variable as an example of the kind of queries
    that producing one-way frequencies, like the ones in this recipe, are likely to
    generate. It is always good to have some bivariate analyses (a correlation matrix,
    some crosstabs, or a few scatter plots) at the ready should questions like these
    come up. We will generate those in the next two chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Generating summary statistics for continuous variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: pandas has a good number of tools we can use to get a sense of the distribution
    of continuous variables. We will focus on the splendid functionality of `describe`
    in this recipe and demonstrate the usefulness of histograms for visualizing variable
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Before doing any analysis with a continuous variable, it is important to have
    a good understanding of how it is distributed – its central tendency, its spread,
    and its skewness. This understanding greatly informs our efforts to identify outliers
    and unexpected values. But it is also crucial information in and of itself. I
    do not think it overstates the case to say that we understand a particular variable
    well if we have a good understanding of how it is distributed, and any interpretation
    without that understanding will be incomplete or flawed in some way.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will work with the COVID-19 totals data in this recipe. You will need **Matplotlib**
    to run this. If it is not installed on your machine already, you can install it
    in the terminal by entering `pip install matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take a look at the distribution of a few key continuous variables:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas`, `numpy`, and `matplotlib`, and load the COVID-19 case totals
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s remind ourselves of the structure of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the descriptive statistics on the COVID-19 totals columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Take a closer look at the distribution of values for the cases and deaths columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use NumPy’s `arange` method to pass a list of floats from 0 to 1.0 to the `quantile`
    method of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'View the distribution of total cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![fig](img/B18596_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: A plot of total COVID-19 cases'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding steps demonstrated the use of `describe` and Matplotlib’s `hist`
    method, which are essential tools when working with continuous variables.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We used the `describe` method in *Step 3* to examine some summary statistics
    and the distribution of the key variables. It is often a red flag when the mean
    and median (the value at the 50^(th) percentile) have dramatically different values.
    Cases and deaths are heavily skewed to the right (reflected in the mean being
    much higher than the median). This alerts us to the presence of outliers at the
    upper end. This is true even with the adjustment for population size, as both
    `total_cases_pm` and `total_deaths_pm` show this same skew. We do more analysis
    of outliers in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The more detailed percentile data in *Step 4* further supports this sense of
    skewness. For instance, the gap between the 90^(th)-percentile and 100^(th)-percentile
    values for cases and deaths is substantial. These are good first indicators that
    we are not dealing with normally distributed data here. Even if this is not due
    to errors, this matters for the statistical testing we will do down the road.
    On the list of things we want to note when asked, *“How does the data look?”*,
    this is one of the first things we want to say.
  prefs: []
  type: TYPE_NORMAL
- en: The histogram of total cases confirms that much of the distribution is between
    0 and 100,000, with a few outliers and 1 extreme outlier. Visually, the distribution
    looks much more log-normal than normal. Log-normal distributions have fatter tails
    and do not have negative values.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We take a closer look at outliers and unexpected values in the next chapter.
    We do much more with visualizations in *Chapter 5*, *Using Visualizations for
    the Identification of Unexpected Values*.
  prefs: []
  type: TYPE_NORMAL
- en: Using generative AI to display descriptive statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI tools provide data scientists with a great opportunity to streamline
    the data cleaning and exploration parts of our workflow. Large language models,
    in particular, have the potential to make this work much easier and more intuitive.
    Using these tools, we can select rows and columns by criteria, generate summary
    statistics, and plot variables.
  prefs: []
  type: TYPE_NORMAL
- en: A simple way to introduce generative AI tools into your data exploration is
    with PandasAI. PandasAI uses the OpenAI API to translate natural language queries
    into data selection and operations that pandas can understand. As of July 2023,
    OpenAI is the only large language model API that can be used with PandasAI, though
    the developers of the library anticipate adding other APIs.
  prefs: []
  type: TYPE_NORMAL
- en: We can use PandasAI to substantially reduce the lines of code we need to write
    to produce some of the tabulations and visualizations we have created so far in
    this chapter. The steps in this recipe show how you can do that.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to install PandasAI to run the code in this recipe. You can do that
    with `pip install pandasai`. We will work with the COVID-19 data again, which
    is available in the GitHub repository, as is the code.
  prefs: []
  type: TYPE_NORMAL
- en: You will also need an API key from OpenAI. You can get one at [platform.openai.com](https://platform.openai.com).
    You will need to set up an account and then click on your profile in the upper-right
    corner, followed by **View API keys**.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create a PandasAI instance in the following steps and use it to take a look
    at the COVID-19 data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing `pandas` and the `PandasAI` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load the COVID-19 data and instantiate a `PandasAI SmartDataframe`
    object. The `SmartDataframe` object will allow us to work with our data using
    natural language:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at the structure of the COVID-19 data. We can do this by passing
    natural language instructions to the SmartDataframe’s `chat` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is also straightforward to see the first few rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see which locations (countries) have the highest total cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can show the highest total cases per million as well, and also show other
    columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Notice that we do not need to add the underscores in `total_cases_pm` or `total_deaths_pm`.
    The chat method figures out that that is what we meant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also create a `SmartDataframe` with selected columns. When we use the
    `chat` method in this step, it figures out that a `SmartDataframe` should be returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We do not need to be very precise in the language we pass to PandasAI. Instead
    of writing `Select`, we could have written `Get` or `Grab`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can select rows by summary statistic. For example, we can choose those rows
    where the total cases per million is greater than the 95^(th) percentile. Note
    that this might take a little while to run on your machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see how continuous variables are distributed by asking for their distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also get group totals. Let’s get the total cases and deaths by region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can easily generate plots on the COVID-19 data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code generates the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![temp_chart](img/B18596_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Distribution of total cases per million'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also generate a scatterplot. Let’s look at total cases per million against
    total deaths per million:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![temp_chart](img/B18596_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Scatterplot of total cases per million against total deaths per
    million'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can indicate which plotting tool we want to use. Using `regplot` here might
    be helpful to give us a better sense of the relationship between cases and deaths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![temp_chart](img/B18596_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Regression plot'
  prefs: []
  type: TYPE_NORMAL
- en: 'The extreme values for cases or deaths make it harder to visualize the relationship
    between the two for much of the range. Let’s also ask PandasAI to remove the extreme
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![temp_chart](img/B18596_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Regression plot without the extreme values'
  prefs: []
  type: TYPE_NORMAL
- en: This removed deaths per million above 350 and cases per million above 20,000\.
    It is easier to see the slope of the relationship over much of the data. We will
    work more with `regplot` and many other plotting tools in *Chapter 5*, *Using
    Visualizations for the Identification of Unexpected Values*.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These examples demonstrate how intuitive it is to use PandasAI. Generative AI
    tools like PandasAI have the potential to improve our exploratory work by making
    it possible to interact with the data nearly as quickly as we can imagine new
    analyses. We only need to pass natural language queries to the PandasAI object
    to get the results we want.
  prefs: []
  type: TYPE_NORMAL
- en: The queries we pass are not commands. We can use any language we want that conveys
    our intent. Recall, for example, that we were able to write `select`, `get`, or
    even `grab` to choose columns. OpenAI’s large language model is generally very
    good at understanding what we mean.
  prefs: []
  type: TYPE_NORMAL
- en: It is a good idea to check the PandasAI log file to see the code that is generated
    when you pass instructions to the SmartDataframe `chat` method. The pandasai.log
    file will be in the same folder as your Python code.
  prefs: []
  type: TYPE_NORMAL
- en: A tool that helps us move more swiftly from question to answer can improve our
    thinking and analysis. It is definitely worth experimenting with if you have not
    done so already, even if you have well-established routines for looking at your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The PandasAI GitHub repository is a great place to go for more information
    and to keep apprised of updates in the library. You can get to it here: [https://github.com/gventuri/pandas-ai](https://github.com/gventuri/pandas-ai).
    We will return to the PandasAI library in recipes throughout this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered key steps we need to take the day after we convert our
    raw data into a pandas DataFrame. We explored techniques for examining the structure
    of our data, including the number of rows and columns, and data types. We also
    learned how to generate frequencies for categorical variables, and began to look
    at how values for one variable change with the values of another variable. Finally,
    we saw how to examine the distribution of continuous variables, including with
    sample statistics such as the mean, minimum, and max, and by plotting. This sets
    us up for the topics in the next chapter, where we will use techniques to identify
    outliers in our data.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://discord.gg/p8uSgEAETX](https://discord.gg/p8uSgEAETX )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code10336218961138498953.png)'
  prefs: []
  type: TYPE_IMG
