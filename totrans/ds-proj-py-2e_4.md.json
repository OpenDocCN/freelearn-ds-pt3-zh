["```py\nX_poly = np.linspace(-3,5,81)\nprint(X_poly[:5], '...', X_poly[-5:])\n```", "```py\n[-3\\. -2.9 -2.8 -2.7 -2.6] ... [4.6 4.7 4.8 4.9 5\\. ]\n```", "```py\ndef cost_function(X):\n    return X * (X-2)\ny_poly = cost_function(X_poly)\nplt.plot(X_poly, y_poly)\nplt.xlabel('Parameter value')\nplt.ylabel('Cost function')\nplt.title('Error surface')\n```", "```py\n    X_poly = np.linspace(-3,5,81)\n    print(X_poly[:5], '...', X_poly[-5:])\n    def cost_function(X):\n        return X * (X-2)\n    y_poly = cost_function(X_poly)\n    plt.plot(X_poly, y_poly)\n    plt.xlabel('Parameter value')\n    plt.ylabel('Cost function')\n    plt.title('Error surface')\n    ```", "```py\n    def gradient(X):\n        return (2*X) - 2\n    x_start = 4.5\n    learning_rate = 0.75\n    x_next = x_start - gradient(x_start)*learning_rate\n    x_next\n    -0.75\n    ```", "```py\n    plt.plot(X_poly, y_poly)\n    plt.plot([x_start, x_next],\n             [cost_function(x_start), cost_function(x_next)],\n             '-o')\n    plt.xlabel('Parameter value')\n    plt.ylabel('Cost function')\n    plt.legend(['Error surface', 'Gradient descent path'])\n    ```", "```py\n    iterations = 15\n    x_path = np.empty(iterations,)\n    x_path[0] = x_start\n    for iteration_count in range(1,iterations):\n        derivative = gradient(x_path[iteration_count-1])\n        x_path[iteration_count] = x_path[iteration_count-1] \\\n                                  - (derivative*learning_rate)\n    x_path\n    ```", "```py\n    array([ 4.5       , -0.75      ,  1.875     ,  0.5625    ,  1.21875   ,\n            0.890625  ,  1.0546875 ,  0.97265625,  1.01367188,  0.99316406,\n            1.00341797,  0.99829102,  1.00085449,  0.99957275,  1.00021362])\n    ```", "```py\n    plt.plot(X_poly, y_poly)\n    plt.plot(x_path, cost_function(x_path), '-o')\n    plt.xlabel('Parameter value')\n    plt.ylabel('Cost function')\n    plt.legend(['Error surface', 'Gradient descent path'])\n    ```", "```py\n    from sklearn.datasets import make_classification\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import roc_auc_score\n    ```", "```py\n    X_synthetic, y_synthetic = make_classification(\n        n_samples=1000, n_features=200,\n        n_informative=3, n_redundant=10,\n        n_repeated=0, n_classes=2,\n        n_clusters_per_class=2,\n        weights=None, flip_y=0.01,\n        class_sep=0.8, hypercube=True,\n        shift=0.0, scale=1.0,\n        shuffle=True, random_state=24)\n    ```", "```py\n    print(X_synthetic.shape, y_synthetic.shape)\n    print(np.mean(y_synthetic))\n    ```", "```py\n    (1000, 200) (1000,)\n    0.501\n    ```", "```py\n    for plot_index in range(4):\n        plt.subplot(2, 2, plot_index+1)\n        plt.hist(X_synthetic[:, plot_index])\n        plt.title('Histogram for feature {}'.format(plot_index+1))\n    plt.tight_layout()\n    ```", "```py\n    X_syn_train, X_syn_test, y_syn_train, y_syn_test = \\\n    train_test_split(X_synthetic, y_synthetic,\\\n                     test_size=0.2, random_state=24)\n    lr_syn = LogisticRegression(solver='liblinear', penalty='l1',\n                                C=1000, random_state=1)\n    lr_syn.fit(X_syn_train, y_syn_train)\n    ```", "```py\n    lr_syn.fit(X_syn_train, y_syn_train)\n    ```", "```py\n    LogisticRegression(C=1000, penalty='l1', random_state=1, \\\n                       solver='liblinear')\n    ```", "```py\n    y_syn_train_predict_proba = lr_syn.predict_proba(X_syn_train)\n    roc_auc_score(y_syn_train, y_syn_train_predict_proba[:,1])\n    ```", "```py\n    0.9420000000000001\n    ```", "```py\n    y_syn_test_predict_proba = lr_syn.predict_proba(X_syn_test)\n    roc_auc_score(y_syn_test, y_syn_test_predict_proba[:,1])\n    ```", "```py\n    0.8075807580758075\n    ```", "```py\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\n```", "```py\nn_folds = 4\nk_folds = KFold(n_splits=n_folds, shuffle=False)\n```", "```py\nfor train_index, test_index in k_folds_iterator.split(X_syn_train,\n                                                      y_syn_train):\n```", "```py\nk_folds = StratifiedKFold(n_splits=n_folds, shuffle=False)\n```", "```py\nk_folds = StratifiedKFold(n_splits=n_folds, shuffle=True,\n                          random_state=1)\n```", "```py\n    C_val_exponents = np.linspace(3,-3,13)\n    C_val_exponents\n    ```", "```py\n    array([ 3\\. ,  2.5,  2\\. ,  1.5,  1\\. ,  0.5,  0\\. , -0.5, -1\\. , -1.5, -2\\. , -2.5, -3\\. ])\n    ```", "```py\n    C_vals = np.float(10)**C_val_exponents\n    C_vals\n    ```", "```py\n    array([1.00000000e+03, 3.16227766e+02, 1.00000000e+02, 3.16227766e+01,\n           1.00000000e+01, 3.16227766e+00, 1.00000000e+00, 3.16227766e-01,\n           1.00000000e-01, 3.16227766e-02, 1.00000000e-02, 3.16227766e-03,\n           1.00000000e-03])\n    ```", "```py\n    from sklearn.metrics import roc_curve\n    ```", "```py\n    def cross_val_C_search(k_folds, C_vals, model, X, Y):\n    ```", "```py\n    n_folds = k_folds.n_splits\n    cv_train_roc_auc = np.empty((n_folds, len(C_vals)))\n    cv_test_roc_auc = np.empty((n_folds, len(C_vals)))\n    ```", "```py\n    cv_test_roc = [[]]*len(C_vals)\n    ```", "```py\n    for c_val_counter in range(len(C_vals)):\n        #Set the C value for the model object\n        model.C = C_vals[c_val_counter]\n        #Count folds for each value of C\n        fold_counter = 0\n    ```", "```py\n    for train_index, test_index in k_folds.split(X, Y):\n    ```", "```py\n    X_cv_train, X_cv_test = X[train_index], X[test_index]\n    y_cv_train, y_cv_test = Y[train_index], Y[test_index]\n    ```", "```py\n    model.fit(X_cv_train, y_cv_train)\n    ```", "```py\n    y_cv_train_predict_proba = model.predict_proba(X_cv_train)\n    cv_train_roc_auc[fold_counter, c_val_counter] = \\\n    roc_auc_score(y_cv_train, y_cv_train_predict_proba[:,1])\n    ```", "```py\n    y_cv_test_predict_proba = model.predict_proba(X_cv_test)\n    cv_test_roc_auc[fold_counter, c_val_counter] = \\\n    roc_auc_score(y_cv_test, y_cv_test_predict_proba[:,1])\n    ```", "```py\n    this_fold_roc = roc_curve(y_cv_test, y_cv_test_predict_proba[:,1])\n    cv_test_roc[c_val_counter].append(this_fold_roc)\n    ```", "```py\n    fold_counter += 1\n    ```", "```py\n    print('Done with C = {}'.format(lr_syn.C))\n    ```", "```py\n    return cv_train_roc_auc, cv_test_roc_auc, cv_test_roc\n    ```", "```py\n    cv_train_roc_auc, cv_test_roc_auc, cv_test_roc = \\\n    cross_val_C_search(k_folds, C_vals, lr_syn, X_syn_train, y_syn_train)\n    ```", "```py\n    Done with C = 1000.0\n    Done with C = 316.22776601683796\n    Done with C = 100.0\n    Done with C = 31.622776601683793\n    Done with C = 10.0\n    Done with C = 3.1622776601683795\n    Done with C = 1.0\n    Done with C = 0.31622776601683794\n    Done with C = 0.1\n    Done with C = 0.03162277660168379\n    Done with C = 0.01\n    Done with C = 0.0031622776601683794\n    Done with C = 0.001\n    ```", "```py\n    for this_fold in range(k_folds.n_splits):\n        plt.plot(C_val_exponents, cv_train_roc_auc[this_fold], '-o',\\\n                 color=cmap(this_fold),\\\n                 label='Training fold {}'.format(this_fold+1))\n        plt.plot(C_val_exponents, cv_test_roc_auc[this_fold], '-x',\\\n                 color=cmap(this_fold),\\\n                 label='Testing fold {}'.format(this_fold+1))\n    plt.ylabel('ROC AUC')\n    plt.xlabel('log$_{10}$(C)')\n    plt.legend(loc = [1.1, 0.2])\n    plt.title('Cross validation scores for each fold')\n    ```", "```py\n    plt.plot(C_val_exponents, np.mean(cv_train_roc_auc, axis=0), \\\n             '-o', label='Average training score')\n    plt.plot(C_val_exponents, np.mean(cv_test_roc_auc, axis=0), \\\n             '-x', label='Average testing score')\n    plt.ylabel('ROC AUC')\n    plt.xlabel('log$_{10}$(C)')\n    plt.legend()\n    plt.title('Cross validation scores averaged over all folds')\n    ```", "```py\n    best_C_val_bool = C_val_exponents == -1.5\n    best_C_val_bool.astype(int)\n    ```", "```py\n    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n    ```", "```py\n    best_C_val_ix = np.nonzero(best_C_val_bool.astype(int)) best_C_val_ix[0][0]\n    ```", "```py\n    9\n    ```", "```py\n    for this_fold in range(k_folds_n_splits):\n        fpr = cv_test_roc[best_C_val_ix[0][0]][this_fold][0]\n        tpr = cv_test_roc[best_C_val_ix[0][0]][this_fold][1]\n        plt.plot(fpr, tpr, label='Fold {}'.format(this_fold+1))\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curves for each fold at C = $10^{-1.5}$')\n    plt.legend()\n    ```", "```py\n    lr_syn.C = 10**(-1.5)\n    lr_syn.fit(X_syn_train, y_syn_train)\n    ```", "```py\n    LogisticRegression(C=0.03162277660168379, penalty='l1', \\\n                       random_state=1, solver='liblinear'))\n    ```", "```py\n    y_syn_train_predict_proba = lr_syn.predict_proba(X_syn_train)\n    roc_auc_score(y_syn_train, y_syn_train_predict_proba[:,1])\n    ```", "```py\n    0.8802812499999999\n    ```", "```py\n    y_syn_test_predict_proba = lr_syn.predict_proba(X_syn_test)\n    roc_auc_score(y_syn_test, y_syn_test_predict_proba[:,1])\n    ```", "```py\n    0.8847884788478848\n    ```", "```py\n    sum((lr_syn.coef_ != 0)[0])\n    ```", "```py\n    2\n    ```", "```py\n    lr_syn.intercept_\n    ```", "```py\n    array([0.])\n    ```", "```py\nfrom sklearn.preprocessing import MinMaxScaler\nmin_max_sc = MinMaxScaler()\n```", "```py\nfrom sklearn.pipeline import Pipeline\nscale_lr_pipeline = Pipeline(steps=[('scaler', min_max_sc), \\\n                                    ('model', lr)])\n```", "```py\nmake_interactions = PolynomialFeatures(degree=2, \\\n                                       interaction_only=True, \\\n                                       include_bias=False)\n```"]