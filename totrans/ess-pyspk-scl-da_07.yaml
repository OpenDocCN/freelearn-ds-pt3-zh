- en: 'Chapter 5: Scalable Machine Learning with PySpark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we have established that modern-day data is growing
    at a rapid rate, with a volume, velocity, and veracity not possible for traditional
    systems to keep pace with. Thus, we learned about distributed computing to keep
    up with the ever-increasing data processing needs and saw practical examples of
    ingesting, cleansing, and integrating data to bring it to a level that is conducive
    to business analytics using the power and ease of use of Apache Spark's unified
    data analytics platform. This chapter, and the chapters that follow, will explore
    the data science and **machine learning** (**ML**) aspects of data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Today, the computer science disciplines of AI and ML have made a massive comeback
    and are pervasive. Businesses everywhere need to leverage these techniques to
    remain competitive, expand their customer base, introduce novel product lines,
    and stay profitable. However, traditional ML and data science techniques were
    designed to deal with limited samples of data and are not inherently scalable.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides you with an overview of traditional ML algorithms, including
    supervised and unsupervised ML techniques and explores real-world use cases of
    ML in business applications. Then you will learn about the need for scalable ML.
    A few techniques for scaling out ML algorithms in a distributed fashion to handle
    very large data samples will be presented. Then, we will dive into the ML library
    of Apache Spark, called MLlib, along with code examples, to perform data wrangling
    using Apache Spark's MLlib to explore, clean, and manipulate data in order to
    get it ready for ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: ML overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling out machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data wrangling with Apache Spark and MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained an appreciation for scalable
    ML and its business applications and acquired a basic understanding of Apache
    Spark's scalable ML library, named MLlib. You will have acquired the skills necessary
    to utilize MLlib to clean and transform data and get it ready for ML applications
    at scale, helping you to reduce the time taken for data cleansing tasks, and making
    your overall ML life cycle much more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using the Databricks Community Edition to run our
    code: [https://community.cloud.databricks.com](https://community.cloud.databricks.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code and data used in this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter05](https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter05).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine Learning** is a field of AI and computer science that leverages statistical
    models and computer science algorithms for learning patterns inherent in data,
    without being explicitly programmed. ML consists of algorithms that automatically
    convert patterns within data into models. Where pure mathematical or rule-based
    models perform the same task over and over again, an ML model learns from data
    and its performance can be greatly improved by exposing it to vast amounts of
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: A typical ML process involves applying an ML algorithm to a known dataset called
    the training dataset, to generate a new ML model. This process is generally termed
    *model training* or *model fitting*. Some ML models are trained on datasets containing
    a known correct answer that we intend to predict in an unknown dataset. This known,
    correct value in the training dataset is termed the *label*.
  prefs: []
  type: TYPE_NORMAL
- en: "Once the model is trained, the resultant model is applied to new data in order\
    \ to \Lpredict the required values. This process is generally referred to as **model\
    \ inference** or **model scoring**."
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Instead of training a single model, it is a best practice to train multiple
    models using various different model parameters called **hyperparameters** and
    select the best model among all the trained models based on well-defined **accuracy
    metrics**. This process of training multiple models based on different parameters
    is generally referred to as hyperparameter **tuning** or **cross-validation**.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of ML algorithms include classification, regression, clustering, collaborative
    filtering, and dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Types of ML algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML algorithms can be classified into three broad categories, namely, supervised
    learning, unsupervised learning, and reinforcement learning, as discussed in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Supervised learning** is a type of ML where a model is trained on a dataset
    with a known value called the **label**. This label is tagged in the training
    dataset and represents the correct answer to the problem we are trying to solve.
    Our intention with supervised learning is to predict the label in an unknown dataset
    once the model is trained on a known dataset with the tagged label.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of supervised learning algorithms include Linear Regression, Logistic
    Regression, Naive Bayes Classifiers, K-Nearest Neighbor, Decision Trees, Random
    Forest, Gradient Boosted Trees, and Support Vector Machine.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning can be divided into two main classes, namely, regression
    and classification problems. While regression deals with predicting an unknown
    label, classification tries to classify the training dataset into known categories.
    Detailed implementation of supervised learning using **Apache Spark** **MLlib**
    will be introduced in [*Chapter 6*](B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107),
    *Supervised Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Unsupervised learning** is a type of ML where the training data is unknown
    to the algorithm and is not already labeled with a correct answer. Unsupervised
    learning involves learning the structure of an unknown, unlabeled dataset, without
    any guidance from the user. Here, the task of the machine is to group data into
    cohorts or groups according to certain similarities or differences without any
    prior training.'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning can be further divided into **clustering** and **association**
    problems. Clustering deals with discovering cohorts within the training dataset,
    while association deals with discovering rules within the data that describe the
    relationship between entities. Examples of unsupervised learning include K-means
    clustering and collaborative filtering. Unsupervised learning will be explored
    at length with coding examples using Apache Spark MLlib in [*Chapter 7*](B16736_07_Final_JM_ePub.xhtml#_idTextAnchor128),
    *Unsupervised Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Reinforcement learning** is employed by software systems and machines to
    find the best possible behavior or path that should be taken in a given situation.
    Unlike supervised learning, which already holds the correct answer within the
    training dataset, in reinforcement learning, there is no correct answer, but the
    reinforcement agent employs trial and error to decide the outcome and is designed
    to learn from experience. The reinforcement agent is either rewarded or penalized
    depending on the path chosen and the goal here is to maximize the reward.'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is used in applications such as self-driving cars, robotics,
    industrial automation, and natural language processing for chatbot agents. There
    aren't any out-of-the-box implementations of reinforcement learning within Apache
    Spark MLlib, so further exploration of this concept is beyond the scope of this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Another branch of data science and ML is **deep learning**, which leverages
    advanced techniques for ML, such as neural networks, which are also becoming very
    prominent these days. Although Apache Spark does support certain deep learning
    algorithms, these concepts are too advanced to be included within the scope of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Business use cases of ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have discussed various categories of ML and briefly introduced you
    to the tasks the ML models can perform. In this section, you will learn about
    some real-life applications of ML algorithms that help solve actual business problems
    across various industry verticals.
  prefs: []
  type: TYPE_NORMAL
- en: Customer churn prevention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building customer churn models using ML can be very useful in identifying all
    those customers who are likely to stop engaging with your business and also help
    you gain insight into the factors that might lead them to churn. A churn model
    can simply be a regression model that estimates the risk score of each individual.
    Customer churn models can be very useful in identifying customers at risk of churning,
    thereby allowing businesses to implement strategies for customer retention.
  prefs: []
  type: TYPE_NORMAL
- en: Customer lifetime value modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retail businesses generate a huge share of their revenue from a small cohort
    of high-value customers who provide repeat business. Customer lifetime value models
    can estimate a customer's lifetime, a period after which they might churn. They
    can also predict the total revenue that a customer would probably generate over
    their lifetime. Thus, estimating the revenue that a potential high-value customer
    might bring over their lifetime could be essential in redirecting marketing dollars
    to attracting and retaining such customers.
  prefs: []
  type: TYPE_NORMAL
- en: Demand forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Brick and mortar businesses, as well as online businesses, have limited storage
    space within their actual stores as well as at the warehouse. Hence, it is important
    to properly stock the available storage space with products that will actually
    be in demand. You could develop a simple model based on seasonality and the month
    of the year. However, building a sophisticated ML model that includes not just
    seasonality and historical data, but external data such as current trends on social
    media, weather forecast data, and customer sentiment on social media, could lead
    to better forecasting of demand and help maximize revenues as a result.
  prefs: []
  type: TYPE_NORMAL
- en: Shipment lead-time prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any business that involves delivery and logistics operations, whether it be
    an online retailer or a food delivery aggregator, needs to be able to estimate
    the amount of time it would take for the order to reach the customer. Often, this
    shipment lead time is an essential decision-making factor by the customer in terms
    of doing business with you versus moving on to a competitor. Regression models
    can be used to accurately estimate the amount of time required to deliver the
    product to the customer's zip code, based on factors such as product origin and
    destination locations, weather, and other seasonal data.
  prefs: []
  type: TYPE_NORMAL
- en: Market basket analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Market basket analysis is a technique for making product recommendations to
    customers based on items already in their basket. ML can be used to discover association
    rules within product categories by leveraging the collaborative filtering algorithm
    in order to make product recommendations to online customers based on the items
    already in their cart and their past purchases. This is a prominent use case used
    by pretty much every e-tailer.
  prefs: []
  type: TYPE_NORMAL
- en: Financial fraud detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ML has the inherent capability to detect patterns within data. Thus, ML can
    be leveraged to build models that can detect anomalies across financial transactions
    to flag certain transactions as being fraudulent. Traditionally, financial institutions
    have already been leveraging rule-based models for fraud detection, but incorporating
    ML models into the mix makes the fraud models even more potent, thereby helping
    to detect novel fraud patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Information extraction using natural language processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pharma companies and businesses that generate a large corpus of knowledge are
    faced with a unique challenge specific to their industry. Trying to identify whether
    a certain piece of knowledge has already been created by another group within
    the vast organization is not a straightforward problem at organizations with tens
    of thousands of employees. ML's natural language processing techniques can be
    applied to sort, group, classify, and label a large corpus of documents so that
    users can easily search if a similar piece of knowledge already exists.
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have explored the basics of ML, the different types of ML algorithms,
    and their applications in real-world business use cases. In the following section,
    we will discuss the need for scalable ML and a few techniques for scaling our
    ML algorithms, and get an introduction to Apache Spark's native, scalable ML library
    called **MLlib** and its application for performing data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling out machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we learned that ML is a set of algorithms that, instead
    of being explicitly programmed, automatically learn patterns hidden within data.
    Thus, an ML algorithm exposed to a larger dataset can potentially result in a
    better-performing model. However, traditional ML algorithms were designed to be
    trained on a limited data sample and on a single machine at a time. This means
    that the existing ML libraries are not inherently scalable. One solution to this
    problem is to down-sample a larger dataset to fit in the memory of a single machine,
    but this also potentially means that the resulting models aren't as accurate as
    they could be.
  prefs: []
  type: TYPE_NORMAL
- en: Also, typically, several ML models are built on the same dataset, simply varying
    the parameters supplied to the algorithm. Out of these several models, the best
    model is chosen for production purposes, using a technique called **hyperparameter
    tuning**. Building several models using a single machine, one model after another,
    in a linear manner takes a very long time to arrive at the best possible model,
    leading to a longer time to production and, hence, a longer time to market.
  prefs: []
  type: TYPE_NORMAL
- en: Given these scalability challenges with traditional ML algorithms, there is
    a need to either scale out existing ML algorithms or new scalable ML algorithms.
    We will explore some techniques for scaling out ML algorithms in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for scaling ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Two of the prominent techniques for scaling out ML algorithms are described
    in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Embarrassingly parallel processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Embarrassingly parallel processing** is a type of parallel computing technique
    where little to no effort is required to split a given computational problem into
    smaller parallel tasks. This is possible when the parallelized tasks do not have
    any interdependencies, and all tasks can execute completely independently of one
    another.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's try to apply this to the problem of scaling out single machine ML
    algorithms on very large datasets, and at the outset, this doesn't seem like a
    simple task at all. However, consider the problem of hyperparameter tuning or
    cross-validation. Here, we can run multiple parallel models, each with different
    model parameters, but on the same smaller dataset that can fit into a single machine's
    memory. Here, we can easily train multiple models on the same dataset by varying
    the model parameters. Thus, by leveraging the embarrassingly parallel processing
    technique, we can accelerate our model-building process by several orders of magnitude,
    helping us get to the best possible model within hours instead of several weeks
    or even months, and thereby accelerating your business time to value. You will
    learn more about applying this technique to scale out single-node Python ML libraries
    using Apache Spark in [*Chapter 10*](B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176)*,
    Scaling Out Single-Node Machine Learning Using PySpark*.
  prefs: []
  type: TYPE_NORMAL
- en: Scalable ML algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the embarrassingly parallel computing technique helps us get better models
    with greater accuracy in a faster time, it is still limited by smaller dataset
    sizes. This means that we might still be missing out on potential patterns of
    data because of down-sampling. To overcome this, we need ML algorithms that can
    inherently scale out across multiple machines and can train on a very large dataset
    in a distributed manner. Apache Spark's native ML library, called MLlib, consists
    of such inherently scalable ML algorithms, and we will explore MLlib further in
    the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Apache Spark's ML library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLlib is Apache Spark's native ML library. Being a native library, MLlib has
    tight integration with the rest of Spark's APIs and libraries, including Spark
    SQL Engine, DataFrame APIs, Spark SQL API, and even Structured Streaming. This
    gives Apache Spark the unique advantage of being a truly unified data analytics
    platform that can perform all tasks pertaining to data analytics, starting from
    data ingestion to data transformation, the ad hoc analysis of data, building sophisticated
    ML models, and even leveraging those models for production use cases. In the following
    section, you will explore more of Spark MLlib and its core components.
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the early versions of Apache Spark, MLlib was based on Spark's RDD API. Starting
    with Spark version 2.0, a new ML library based on DataFrame APIs was introduced.
    Now, in Spark 3.0 and versions above this, the DataFrame API-based MLlib is standard,
    while the older RDD-based MLlib is in maintenance mode with no future enhancements
    planned.
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame-based MLlib closely mimics traditional single-machine Python-based
    ML libraries such as scikit-learn and consists of three major components, called
    transformers, estimators, and pipelines, as described in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A **transformer** is an algorithm that takes a DataFrame as input, performs
    processing on the DataFrame columns, and returns another DataFrame. An ML model
    trained using Spark MLlib is a transformer that takes a raw DataFrame and returns
    another DataFrame with the original raw data along with the new prediction columns.
    A typical transformer pipeline is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – A transformer pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_05_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – A transformer pipeline
  prefs: []
  type: TYPE_NORMAL
- en: In the previous diagram, a typical transformer pipeline is depicted, where a
    series of transformer stages, including a **VectorIndexer** and an already trained
    **Linear Regression Model**, are applied to the raw DataFrame. The result is a
    new DataFrame with all the original columns, along with some new columns containing
    predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The transformation DataFrame operation is a different concept compared to transformers
    within Spark's MLlib. While both transform one DataFrame into another DataFrame
    and are lazily evaluated, the former is an operation performed on a DataFrame,
    while the latter is an actual ML algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Estimators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An **estimator** is another algorithm that accepts a DataFrame as input and
    results in a transformer. Any ML algorithm is an estimator in that it transforms
    a DataFrame with raw data into a DataFrame with actual predictions. An estimator
    pipeline is depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – An estimator pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_05_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – An estimator pipeline
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, a **Transformer** is first applied to a DataFrame
    with raw data to result in a **Feature Vector** DataFrame. An **Estimator** in
    the form of a **Linear Regression** **Algorithm** is then applied to the DataFrame
    containing **Feature Vectors** to result in a **Transformer** in the form of a
    newly trained **Linear Regression Model**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A feature vector is a special data structure within the Spark MLlib library.
    It is a DataFrame column consisting of actual vector objects of the floating-point
    type. Since ML is based on mathematics and statistics, all ML algorithms exclusively
    operate on vectors of floating-point values. Raw data is converted into feature
    vectors using feature extraction and feature engineering techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An ML pipeline within Spark MLlib chains together several stages of transformers
    and estimators into a DAG that performs an end-to-end ML operation ranging from
    data cleansing, to feature engineering, to actual model training. A pipeline could
    be a transformer-only pipeline or an estimator-only pipeline or a mix of the two.
  prefs: []
  type: TYPE_NORMAL
- en: Using the available transformers and estimators within Spark MLlib, an entire
    end-to-end ML pipeline can be constructed. A typical ML pipeline consists of several
    stages, starting with data wrangling, feature engineering, model training, and
    model inferencing. You will learn more about data wrangling techniques in the
    following section.
  prefs: []
  type: TYPE_NORMAL
- en: Data wrangling with Apache Spark and MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data wrangling**, also referred to within the data science community as **data
    munging**, or simply **data preparation**, is the first step in a typical data
    science process. Data wrangling involves sampling, exploring, selecting, manipulating,
    and cleansing data to make it ready for ML applications. Data wrangling takes
    up to 60 to 80 percent of the whole data science process and is the most crucial
    step in guaranteeing the accuracy of the ML model being built. The following sections
    explore the data wrangling process using Apache Spark and MLlib.'
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data preprocessing is the first step in the data wrangling process and involves
    gathering, exploring, and selecting the data elements useful for solving the problem
    at hand. The data science process typically succeeds the data engineering process
    and the assumption here is that clean and integrated data is already available
    in the data lake. However, data that is clean enough for BI may not be good enough
    for data science. Also, data science applications require additional datasets,
    which may not be useful for other analytics use cases, and hence, may not yet
    be clean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start manipulating and wrangling our data, we need to load it into
    a Spark DataFrame and explore the data to gain some understanding of its structure.
    The following code example will make use of the integrated dataset produced toward
    the end of [*Chapter 3*](B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056), *Data
    Cleansing and Integration*, named `retail_silver.delta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we perform the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: We load data from the data lake into a Spark DataFrame using the `spark.read()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We print its schema to check the data types of the columns using the `printSchema()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We display a few columns from the DataFrame to check their values using the
    `select()` operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We generate basic statistics on the DataFrame using the `describe()` operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data cleansing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the code example in the previous section, you must have noticed that most
    data types are just strings types. The dataset might also contain duplicates,
    and there are also `NULL` values in the data. Let''s fix these inconsistencies
    in the dataset, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we perform the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: We de-duplicate data using the `dropduplicates()` operation using the key columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then cast datetime columns into the proper timestamp type, using the `to_timestamp()`
    function, by supplying the correct format of the timestamp.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We change the data types of the DataFrame using the `CAST()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We replace missing values and `NULL` values with `0` using the `na.fill()` operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This section showed you how to perform basic data cleansing at scale using PySpark.
    The following section will show you how to manipulate data steps such as filtering
    and renaming.
  prefs: []
  type: TYPE_NORMAL
- en: Data manipulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have a cleaner dataset, you can perform operations to filter out any
    data not required by your use case, rename columns to follow your naming conventions,
    and drop any unwanted data columns, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we perform the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: We filter, slice, and dice data using the `where()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We rename columns using the `withColumnsRenamed()` function and drop unwanted
    columns using the `drop()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We convert the Spark DataFrame to a PySpark DataFrame using the `toPandas()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sometimes, there is an ML algorithm that's not available in Spark MLlib, or
    there's a custom algorithm built using single-node Python libraries. For these
    use cases, you can convert your Spark DataFrame into a pandas DataFrame, as shown
    in *step 3* previously.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Converting a Spark DataFrame to a pandas Dataframe involves collecting all the
    data from the Executors onto the Spark driver. Thus, care needs to be taken that
    this conversion is only applied to smaller datasets, otherwise this could lead
    to an `OutOfMemory` error on the Driver node.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the concept of ML and the different types
    of ML algorithms. You also learned about some of the real-world applications of
    ML to help businesses minimize losses and maximize revenues and accelerate their
    time to market. You were introduced to the necessity of scalable ML and two different
    techniques for scaling out ML algorithms. Apache Spark's native ML Library, MLlib,
    was introduced, along with its major components.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned a few techniques to perform data wrangling to clean, manipulate,
    and transform data to make it more suitable for the data science process. In the
    following chapter, you will learn about the send phase of the ML process, called
    feature extraction and feature engineering, where you will learn to apply various
    scalable algorithms to transform individual data fields to make them even more
    suitable for data science applications.
  prefs: []
  type: TYPE_NORMAL
