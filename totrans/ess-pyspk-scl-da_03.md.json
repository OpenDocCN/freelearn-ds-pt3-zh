["```py\ndataframe_mysql = spark.read.format(\"jdbc\").options(\n    url=\"jdbc:mysql://localhost:3306/pysparkdb\",\n    driver = \"org.mariadb.jdbc.Driver\",\n    dbtable = \"authors\",\n    user=\"#####\",\n    password=\"@@@@@\").load()\ndataframe_mysql.show()\n```", "```py\nretail_df = (spark\n         .read\n         .format(\"csv\")\n         .option(\"inferSchema\", \"true\")\n         .option(\"header\",\"true\")\n         .load(\"dbfs:/FileStore/shared_uploads/snudurupati@outlook.com/\")\n      )\nretail_df.show()\n```", "```py\nkafka_df = (spark.read\n        .format(\"kafka\")\n        .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n        .option(\"subscribe\", \"wordcount\")\n        .option(\"startingOffsets\", \"earliest\")\n        .load()\n        )\nkafka_df.show()\n```", "```py\nwordcount_df = spark.createDataFrame(\n    [(\"data\", 10), (\"parallel\", 2), (\"Processing\", 30),       \t     (\"Spark\", 50), (\"Apache\", 10)], (\"word\", \"count\"))\n```", "```py\nwordcount_df.write.format(\"jdbc\").options(\n    url=\"jdbc:mysql://localhost:3306/pysparkdb\",\n    driver = \"org.mariadb.jdbc.Driver\",\n    dbtable = \"word_counts\",\n    user=\"######\",\n    password=\"@@@@@@\").save()\n```", "```py\n(wordcount_df\n        .write\n        .option(\"header\", \"true\")\n        .mode(\"overwrite\")\n        .save(\"/tmp/data-lake/wordcount.csv\")\n)\n```", "```py\nRaw_df = spark.read.format(\"image\").load(\"/FileStore/FileStore/shared_uploads/images/\")\nraw_df.printSchema()\nimage_df = raw_df.select(\"image.origin\", \"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\", \"image.data\")\nimage_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"/tmp/data-lake/images.csv\") \n```", "```py\nfrom pyspark.sql.functions import max, lit\ntemp_df = final_df.withColumn(\"max_width\", lit(final_df.agg(max(\"width\")).first()[0]))\ntemp_df.where(\"width == max_width\").show()\n```", "```py\ncsv_df = spark.read.options(header=\"true\", inferSchema=\"true\").csv(\"/tmp/data-lake/images.csv\")\ncsv_df.printSchema()\ncsv_df.show() \n```", "```py\nfrom pyspark.sql.functions import max, lit\ntemp_df = csv_df.withColumn(\"max_width\", lit(csv_df.agg(max(\"width\")).first()[0]))\ntemp_df.where(\"width == max_width\").show()\n```", "```py\nfinal_df.write.parquet(\"/tmp/data-lake/images.parquet\")\nparquet_df = spark.read.parquet(\"/tmp/data-lake/images.parquet\")\nparquet_df.printSchema()\nparquet_df.show()\n```", "```py\ntemp_df = parquet_df.withColumn(\"max_width\", lit(parquet_df.agg(max(\"width\")).first()[0]))\ntemp_df.where(\"width == max_width\").show()\n```", "```py\nretail_df = (spark\n                 .read\n                 .option(\"header\", \"true\")\n                 .option(\"inferSchema\", \"true\")\n                 .csv(\"/FileStore/shared_uploads/online_retail/online_retail.csv\")\n            )\n```", "```py\n(retail_df\n       .write\n       .mode(\"overwrite\")\n       .parquet(\"/tmp/data-lake/online_retail.parquet\")\n)\n```", "```py\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\neventSchema = ( StructType()\n  .add('InvoiceNo', StringType()) \n  .add('StockCode', StringType())\n  .add('Description', StringType()) \n  .add('Quantity', IntegerType()) \n  .add('InvoiceDate', StringType()) \n  .add('UnitPrice', DoubleType()) \n  .add('CustomerID', IntegerType()) \n  .add('Country', StringType())     \n)\n```", "```py\nkafka_df = (spark\n                 .readStream\n                   .format(\"kafka\")\n                   .option(\"kafka.bootstrap.servers\", \n                           \"localhost:9092\")\n                   .option(\"subscribe\", \"retail_events\")\n                   .option(\"startingOffsets\", \"earliest\")\n                 .load()\n            )\n```", "```py\nfrom pyspark.sql.functions import col, from_json, to_date\nretail_df = (kafka_df\n              .select(from_json(col(\"value\").cast(StringType()), eventSchema).alias(\"message\"), col(\"timestamp\").alias(\"EventTime\"))\n              .select(\"message.*\", \"EventTime\")\n)\n```", "```py\nbase_path = \"/tmp/data-lake/retail_events.parquet\"\n(retail_df\n  .withColumn(\"EventDate\", to_date(retail_df.EventTime))\n    .writeStream\n      .format('parquet')\n      .outputMode(\"append\")\n      .trigger(once=True)\n      .option('checkpointLocation', base_path + '/_checkpoint')\n  .start(base_path)\n)\n```", "```py\nbatch_df = spark.read.parquet(\"/tmp/data-lake/online_retail.parquet\")\nspeed_df = spark.read.parquet(\"/tmp/data-lake/retail_events.parquet\").drop(\"EventDate\").drop(\"EventTime\")\nserving_df = batch_df.union(speed_df)\nserving_df.createOrReplaceGlobalTempView(\"serving_layer\")\n```", "```py\n%sql\nSELECT count(*) FROM global_temp.serving_layer;\n```"]