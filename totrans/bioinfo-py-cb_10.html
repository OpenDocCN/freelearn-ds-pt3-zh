<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer075">
<h1 class="chapter-number" id="_idParaDest-238"><a id="_idTextAnchor237"/>9</h1>
<h1 id="_idParaDest-239"><a id="_idTextAnchor238"/>Bioinformatics Pipelines</h1>
<p>Pipelines <a id="_idIndexMarker644"/>are fundamental within any data science environment. Data processing is never a single task. Many pipelines are implemented via ad hoc scripts. This can be done in a useful way, but in many cases, they fail several fundamental viewpoints, chiefly reproducibility, maintainability, and extensibility.</p>
<p>In bioinformatics, you can find three <a id="_idIndexMarker645"/>main types of pipeline system:</p>
<ul>
<li>Frameworks such as<a id="_idIndexMarker646"/> Galaxy (<a href="https://usegalaxy.org">https://usegalaxy.org</a>), which are geared toward users, that is, they expose easy-to-use user interfaces and hide most of the underlying machinery.</li>
<li>Programmatic workflows – geared toward code interfaces that, while generic, originate from the<a id="_idIndexMarker647"/> bioinformatics space. Two examples are <a id="_idIndexMarker648"/>Snakemake (<a href="https://snakemake.readthedocs.io/">https://snakemake.readthedocs.io/</a>) and Nextflow (<a href="https://www.nextflow.io/">https://www.nextflow.io/</a>).</li>
<li>Totally generic workflow systems such as <a id="_idIndexMarker649"/>Apache Airflow (<a href="https://airflow.incubator.apache.org/">https://airflow.incubator.apache.org/</a>), which take a less data-centric approach to workflow management.</li>
</ul>
<p>In this chapter, we will discuss Galaxy, which is especially important for bioinformaticians supporting users that are less inclined to code their own solutions. While you may not be a typical user of these pipeline systems, you might still have to support them. Fortunately, Galaxy provides APIs, which will be our main focus.</p>
<p>We will also be discussing Snakemake and Nextflow as generic workflow tools with programmatic interfaces that originated in the bioinformatics space. We will cover both tools, as they are the most common in the field. We will solve a similar bioinformatics problem using both Snakemake and Nextflow. We will get a taste of both frameworks and hopefully be able to decide on a favorite.</p>
<p>The code for these recipes is presented not as notebooks, but as Python scripts available in the <strong class="source-inline">Chapter09</strong> directory of the book’s repository.</p>
<p>In this chapter, you will find recipes for the following:</p>
<ul>
<li>Introducing Galaxy servers</li>
<li>Accessing Galaxy using the API</li>
<li>Developing a variant analysis pipeline with Snakemake</li>
<li>Developing a variant analysis pipeline with Nextflow</li>
</ul>
<h1 id="_idParaDest-240"><a id="_idTextAnchor239"/>Introducing Galaxy servers</h1>
<p>Galaxy (<a href="https://galaxyproject.org/tutorials/g101/">https://galaxyproject.org/tutorials/g101/</a>) is an<a id="_idIndexMarker650"/> open source system that empowers<a id="_idIndexMarker651"/> non-computational users to do computational biology. It is the most widely used, user-friendly pipeline system available. Galaxy can be installed on a server by any user, but there are also plenty of other servers on the web with public access, the flagship being <a href="http://usegalaxy.org">http://usegalaxy.org</a>.</p>
<p>Our focus in the following recipes will be the programming side of Galaxy: interfacing using the Galaxy API and developing a Galaxy tool to extend its functionality. Before you start, you are strongly advised to approach Galaxy as a user. You can do this by creating a free account at <a href="http://usegalaxy.org">http://usegalaxy.org</a>, and playing around with it a bit. Reaching a level of understanding that includes knowledge of the workflows is recommended.</p>
<h2 id="_idParaDest-241"><a id="_idTextAnchor240"/>Getting ready</h2>
<p>In this recipe, we will carry out a local installation of a<a id="_idIndexMarker652"/> Galaxy server using Docker. As such, a local Docker installation is required. The level of complexity will vary across operating systems: easy on Linux, medium on macOS, and medium to hard on Windows.</p>
<p>This installation is recommended for the next two recipes but you may also be able to use existing public servers. Note that the interfaces of public servers can change over time, so what works today may not work tomorrow. Instructions on how to use public servers for the next two recipes are available in the <em class="italic">There’s more...</em> section.</p>
<h2 id="_idParaDest-242"><a id="_idTextAnchor241"/>How to do it…</h2>
<p>Take a look at the following steps. These assume that you have a Docker-enabled command line:</p>
<ol>
<li>First, we pull the <a id="_idIndexMarker653"/>Galaxy Docker image with the following command:<p class="source-code"><strong class="bold">docker pull bgruening/galaxy-stable:20.09</strong></p></li>
</ol>
<p>This will pull Björn Grüning’s amazing Docker Galaxy image. Use the <strong class="source-inline">20.09</strong> label, as shown in the preceding command; anything more recent could break this recipe and the next recipe.</p>
<ol>
<li value="2">Create a directory on your system. This directory will hold the persistent output of the Docker container across runs.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Docker containers are transient with regard to disk space. This means that when you stop the container, all disk changes will be lost. This can be solved by mounting volumes from the host on Docker, as in the next step. All content in the mounted volumes will persist.</p>
<ol>
<li value="3">We can now run the image with the following command:<p class="source-code"><strong class="bold">docker run -d -v YOUR_DIRECTORY:/export -p 8080:80 -p 8021:21 bgruening/galaxy-stable:20.09</strong></p></li>
</ol>
<p>Replace <strong class="source-inline">YOUR_DIRECTORY</strong> with the full path to the directory that you created in <em class="italic">step 2</em>. If the preceding command fails, make sure you have permission to run Docker. This will vary across operating systems.</p>
<ol>
<li value="4">Check the content of <strong class="source-inline">YOUR_DIRECTORY</strong>. The first time the image runs, it will create all of the files needed for persistent execution across Docker runs. That means maintaining user databases, datasets, and workflows.</li>
</ol>
<p>Point your browser to <strong class="source-inline">http://localhost:8080</strong>. If you get any errors, wait a few seconds. You should see the following screen:</p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<img alt="Figure 9.1 - The Galaxy Docker home page " height="679" src="image/B17942_09_01.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 - The Galaxy Docker home page</p>
<ol>
<li value="5">Now log in (see the top bar) with the default username and password combination: <strong class="source-inline">admin</strong> and <strong class="source-inline">password</strong>.</li>
<li>From the top menu, choose <strong class="bold">User</strong>, and inside, choose <strong class="bold">Preferences</strong>.</li>
<li>Now, choose <strong class="bold">Manage API Key</strong>.</li>
</ol>
<p>Do not <a id="_idIndexMarker654"/>change the API key. The purpose of the preceding exercise is for you to know where the API key is. In real scenarios, you will have to go to this screen to get your key. Just note the API key: <strong class="source-inline">fakekey</strong>. In normal situations, this will be an MD5 hash, by the way.</p>
<p>So, at this stage, we have our server installed with the following (default) credentials: the user as <strong class="source-inline">admin</strong>, password as <strong class="source-inline">password</strong>, and API key as <strong class="source-inline">fakekey</strong>. The access point is <strong class="source-inline">localhost:8080</strong>.</p>
<h2 id="_idParaDest-243"><a id="_idTextAnchor242"/>There’s more</h2>
<p>The way Björn Grüning’s image is going to be used throughout this chapter is quite simple; after all, this is not a book on system administration or DevOps, but a programming one. If you visit <a href="https://github.com/bgruening/docker-galaxy-stable">https://github.com/bgruening/docker-galaxy-stable</a>, you will see that there is an infinite number of ways to configure the image, and all are well documented. Our simple approach here works for our development purposes.</p>
<p>If you don’t want to install Galaxy<a id="_idIndexMarker655"/> on your local computer, you can use a public server such as <a href="https://usegalaxy.org">https://usegalaxy.org</a> to do the next recipe. It is not 100% foolproof, as services change over time, but it will probably be very close. Take the following steps:</p>
<ol>
<li value="1">Create an account on a public server (<a href="https://usegalaxy.org">https://usegalaxy.org</a> or other).</li>
<li>Follow the previous instructions for accessing your API key.</li>
<li>In the next recipe, you will have to replace the host, user, password, and API key.</li>
</ol>
<h1 id="_idParaDest-244"><a id="_idTextAnchor243"/>Accessing Galaxy using the API</h1>
<p>While <a id="_idIndexMarker656"/>Galaxy’s main use case is via an easy-to-use web<a id="_idIndexMarker657"/> interface, it also provides a REST API for programmatic access. There are interfaces provided in several <a id="_idIndexMarker658"/>languages, for example, Python support is available from BioBlend (<a href="https://bioblend.readthedocs.io">https://bioblend.readthedocs.io</a>).</p>
<p>Here, we are going to develop a script that will load a BED file into Galaxy and call a tool to convert it to GFF format. We will load the file using Galaxy’s FTP server.</p>
<h2 id="_idParaDest-245"><a id="_idTextAnchor244"/>Getting ready</h2>
<p>If you did not go through the previous recipe, please read the corresponding <em class="italic">There’s more...</em> section. The code was tested in a local server, as prepared in the preceding recipe, so it might require some adaptations if you run it against a public server.</p>
<p>Our code will need to authenticate itself against the Galaxy server in order to perform the necessary operations. Because security is an important issue, this recipe will not be totally naive with regard to it. Our script will be configured via a YAML file, for example:</p>
<pre class="source-code">
rest_protocol: http
server: localhost
rest_port: 8080
sftp_port: 8022
user: admin
password: password
api_key: fakekey</pre>
<p>Our script <a id="_idIndexMarker659"/>will not accept this file as plain text, but it will require it to<a id="_idIndexMarker660"/> be encrypted. That being said, there is a big hole in our security plan: we will be using HTTP (instead of HTTPS), which means that passwords will pass in the clear over the network. Obviously, this is a bad solution, but space considerations put a limit on what we can do (especially in the preceding recipe). Really secure solutions will require HTTPS.</p>
<p>We will need a script that takes a YAML file and generates an encrypted version:</p>
<pre class="source-code">
import base64
import getpass
from io import StringIO
import os
from ruamel.yaml import YAML
from cryptography.fernet import Fernet
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
password = getpass.getpass('Please enter the password:').encode()
salt = os.urandom(16)
kdf = PBKDF2HMAC(algorithm=hashes.SHA256(), length=32, salt=salt,
                 iterations=100000, backend=default_backend())
key = base64.urlsafe_b64encode(kdf.derive(password))
fernet = Fernet(key)
with open('salt', 'wb') as w:
    w.write(salt)
yaml = YAML()
content = yaml.load(open('galaxy.yaml', 'rt', encoding='utf-8'))
print(type(content), content)
output = StringIO()
yaml.dump(content, output)
print ('Encrypting:\n%s' % output.getvalue())
enc_output = fernet.encrypt(output.getvalue().encode())
with open('galaxy.yaml.enc', 'wb') as w:
    w.write(enc_output) </pre>
<p>The<a id="_idIndexMarker661"/> preceding file can be found at <strong class="source-inline">Chapter09/pipelines/galaxy/encrypt.py</strong> in the GitHub repository.</p>
<p>You will need to <a id="_idIndexMarker662"/>input a password for encryption.</p>
<p>The preceding code is not Galaxy-related: it reads a <strong class="source-inline">YAML</strong> file and encrypts it with a password supplied by the user. It uses the <strong class="source-inline">cryptography</strong> module encryption and <strong class="source-inline">ruaml.yaml</strong> for <strong class="source-inline">YAML</strong> processing. Two files are output: the encrypted <strong class="source-inline">YAML</strong> file and the <strong class="source-inline">salt</strong> file for encryption. For security reasons, the <strong class="source-inline">salt</strong> file should not be public.</p>
<p>This approach to securing credentials is far from sophisticated; it is mostly illustrative that you have to be careful with your code when dealing with authentication tokens. There are far more instances on the web of hardcoded security credentials.</p>
<h2 id="_idParaDest-246"><a id="_idTextAnchor245"/>How to do it…</h2>
<p>Take a<a id="_idIndexMarker663"/> look at the following steps, which can be found in <strong class="source-inline">Chapter09/pipelines/galaxy/api.py</strong>:</p>
<ol>
<li value="1">We start by decrypting our configuration file. We need to supply a password:<p class="source-code">import base64</p><p class="source-code">from collections import defaultdict</p><p class="source-code">import getpass</p><p class="source-code">import pprint</p><p class="source-code">import warnings</p><p class="source-code">from ruamel.yaml import YAML</p><p class="source-code">from cryptography.fernet import Fernet</p><p class="source-code">from cryptography.hazmat.backends import default_backend</p><p class="source-code">from cryptography.hazmat.primitives import hashes</p><p class="source-code">from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC</p><p class="source-code">import pandas as pd</p><p class="source-code">Import paramiko</p><p class="source-code">from bioblend.galaxy import GalaxyInstance</p><p class="source-code">pp = pprint.PrettyPrinter()</p><p class="source-code">warnings.filterwarnings('ignore')</p><p class="source-code"># explain above, and warn</p><p class="source-code">with open('galaxy.yaml.enc', 'rb') as f:</p><p class="source-code">    enc_conf = f.read()</p><p class="source-code">password = getpass.getpass('Please enter the password:').encode()</p><p class="source-code">with open('salt', 'rb') as f:</p><p class="source-code">    salt = f.read()</p><p class="source-code">kdf = PBKDF2HMAC(algorithm=hashes.SHA256(), length=32, salt=salt,</p><p class="source-code">                 iterations=100000, backend=default_backend())</p><p class="source-code">key = base64.urlsafe_b64encode(kdf.derive(password))</p><p class="source-code">fernet = Fernet(key)</p><p class="source-code">yaml = YAML()</p><p class="source-code">conf = yaml.load(fernet.decrypt(enc_conf).decode())</p></li>
</ol>
<p>The last<a id="_idIndexMarker664"/> line summarizes it all: the <strong class="source-inline">YAML</strong> module will load the configuration from a decrypted file. Note that we also read <strong class="source-inline">salt</strong> in order to be able to decrypt the file.</p>
<ol>
<li value="2">We’ll <a id="_idIndexMarker665"/>now get all configuration variables, prepare the <a id="_idIndexMarker666"/>server URL, and specify the name of the Galaxy history that we will be creating (<strong class="source-inline">bioinf_example</strong>):<p class="source-code">server = conf['server']</p><p class="source-code">rest_protocol = conf['rest_protocol']</p><p class="source-code">rest_port = conf['rest_port']</p><p class="source-code">user = conf['user']</p><p class="source-code">password = conf['password']</p><p class="source-code">ftp_port = int(conf['ftp_port'])</p><p class="source-code">api_key = conf['api_key']</p><p class="source-code">rest_url = '%s://%s:%d' % (rest_protocol, server, rest_port)</p><p class="source-code">history_name = 'bioinf_example'</p></li>
<li>Finally, we are able to connect to the Galaxy server:<p class="source-code">gi = GalaxyInstance(url=rest_url, key=api_key)</p><p class="source-code">gi.verify = False</p></li>
<li>We will <a id="_idIndexMarker667"/>now list all <strong class="source-inline">histories</strong> available:<p class="source-code">histories = gi.histories</p><p class="source-code">print('Existing histories:')</p><p class="source-code">for history in histories.get_histories():</p><p class="source-code">    if history['name'] == history_name:</p><p class="source-code">        histories.delete_history(history['id'])</p><p class="source-code">    print('  - ' + history['name'])</p><p class="source-code">print()</p></li>
</ol>
<p>On the first execution, you will get an unnamed history, but on the other executions, you will also get <strong class="source-inline">bioinf_example</strong>, which we will delete at this stage so that we start with a clean slate.</p>
<ol>
<li value="5">Afterward, we <a id="_idIndexMarker668"/>create the <strong class="source-inline">bioinf_example</strong> history :<p class="source-code">ds_history = histories.create_history(history_name)</p></li>
</ol>
<p>If you want, you <a id="_idIndexMarker669"/>can check on the web interface, and you will find the new history there.</p>
<ol>
<li value="6">We are going to upload the file now; this requires an SFTP connection. The file is supplied with this code:<p class="source-code">print('Uploading file')</p><p class="source-code">transport = paramiko.Transport((server, sftp_port))</p><p class="source-code">transport.connect(None, user, password)</p><p class="source-code">sftp = paramiko.SFTPClient.from_transport(transport)</p><p class="source-code">sftp.put('LCT.bed', 'LCT.bed')</p><p class="source-code">sftp.close()</p><p class="source-code">transport.close()</p></li>
<li>We will now tell Galaxy to load the file on the FTP server into its internal database:<p class="source-code">gi.tools.upload_from_ftp('LCT.bed', ds_history['id'])</p></li>
<li>Let’s summarize the contents of our history:<p class="source-code">def summarize_contents(contents):</p><p class="source-code"> summary = defaultdict(list)</p><p class="source-code"> for item in contents:</p><p class="source-code"> summary['íd'].append(item['id'])</p><p class="source-code"> summary['híd'].append(item['hid'])</p><p class="source-code"> summary['name'].append(item['name'])</p><p class="source-code"> summary['type'].append(item['type'])</p><p class="source-code"> summary['extension'].append(item['extension'])</p><p class="source-code"> return pd.DataFrame.from_dict(summary)</p><p class="source-code">print('History contents:')</p><p class="source-code">pd_contents = summarize_contents(contents)</p><p class="source-code">print(pd_contents)</p><p class="source-code">print()</p></li>
</ol>
<p>We only have one entry:</p>
<p class="source-code">                 íd  híd     name  type extension</p>
<p class="source-code">0  f2db41e1fa331b3e    1  LCT.bed  file      auto</p>
<ol>
<li value="9">Let’s inspect<a id="_idIndexMarker670"/> the metadata <a id="_idIndexMarker671"/>for our <strong class="source-inline">BED</strong> file:<p class="source-code">print('Metadata for LCT.bed')</p><p class="source-code">bed_ds = contents[0]</p><p class="source-code">pp.pprint(bed_ds)</p><p class="source-code">print()</p></li>
</ol>
<p>The result consists of the following:</p>
<p class="source-code">{'create_time': '2018-11-28T21:27:28.952118',</p>
<p class="source-code"> 'dataset_id': 'f2db41e1fa331b3e',</p>
<p class="source-code"> 'deleted': False,</p>
<p class="source-code"> 'extension': 'auto',</p>
<p class="source-code"> 'hid': 1,</p>
<p class="source-code"> 'history_content_type': 'dataset',</p>
<p class="source-code"> 'history_id': 'f2db41e1fa331b3e',</p>
<p class="source-code"> 'id': 'f2db41e1fa331b3e',</p>
<p class="source-code"> 'name': 'LCT.bed',</p>
<p class="source-code"> 'purged': False,</p>
<p class="source-code"> 'state': 'queued',</p>
<p class="source-code"> 'tags': [],</p>
<p class="source-code"> 'type': 'file',</p>
<p class="source-code"> 'type_id': 'dataset-f2db41e1fa331b3e',</p>
<p class="source-code"> 'update_time': '2018-11-28T21:27:29.149933',</p>
<p class="source-code"> 'url': '/api/histories/f2db41e1fa331b3e/contents/f2db41e1fa331b3e',</p>
<p class="source-code"> 'visible': True}</p>
<ol>
<li value="10">Let’s turn our<a id="_idIndexMarker672"/> attention to the existing tools on the server and <a id="_idIndexMarker673"/>get metadata about them:<p class="source-code">print('Metadata about all tools')</p><p class="source-code">all_tools = gi.tools.get_tools()</p><p class="source-code">pp.pprint(all_tools)</p><p class="source-code">print()</p></li>
</ol>
<p>This will print a long list of tools.</p>
<ol>
<li value="11">Now let’s get some information about our tool:<p class="source-code">bed2gff = gi.tools.get_tools(name='Convert BED to GFF')[0]</p><p class="source-code">print("Converter metadata:")</p><p class="source-code">pp.pprint(gi.tools.show_tool(bed2gff['id'], io_details=True, link_details=True))</p><p class="source-code">print()</p></li>
</ol>
<p>The tool’s <a id="_idIndexMarker674"/>name was available in the preceding step. Note <a id="_idIndexMarker675"/>that we get the first element of a list as, in theory, there could be more than one version of the tool installed. The abridged output is as follows:</p>
<p class="source-code">{'config_file': '/galaxy-central/lib/galaxy/datatypes/converters/bed_to_gff_converter.xml',</p>
<p class="source-code"> 'id': 'CONVERTER_bed_to_gff_0',</p>
<p class="source-code"> 'inputs': [{'argument': None,</p>
<p class="source-code">             'edam': {'edam_data': ['data_3002'],</p>
<p class="source-code">                      'edam_formats': ['format_3003']},</p>
<p class="source-code">             'extensions': ['bed'],</p>
<p class="source-code">             'label': 'Choose BED file',</p>
<p class="source-code">             'multiple': False,</p>
<p class="source-code">             'name': 'input1',</p>
<p class="source-code">             'optional': False,</p>
<p class="source-code">             'type': 'data',</p>
<p class="source-code">             'value': None}],</p>
<p class="source-code"> 'labels': [],</p>
<p class="source-code"> 'link': '/tool_runner?tool_id=CONVERTER_bed_to_gff_0',</p>
<p class="source-code"> 'min_width': -1,</p>
<p class="source-code"> 'model_class': 'Tool',</p>
<p class="source-code"> 'name': 'Convert BED to GFF',</p>
<p class="source-code"> 'outputs': [{'edam_data': 'data_1255',</p>
<p class="source-code">              'edam_format': 'format_2305',</p>
<p class="source-code">              'format': 'gff',</p>
<p class="source-code">              'hidden': False,</p>
<p class="source-code">              'model_class': 'ToolOutput',</p>
<p class="source-code">              'name': 'output1'}],</p>
<p class="source-code"> 'panel_section_id': None,</p>
<p class="source-code"> 'panel_section_name': None,</p>
<p class="source-code"> 'target': 'galaxy_main',</p>
<p class="source-code"> 'version': '2.0.0'}</p>
<ol>
<li value="12">Finally, let’s<a id="_idIndexMarker676"/> run a tool <a id="_idIndexMarker677"/>to convert our <strong class="source-inline">BED</strong> file into <strong class="source-inline">GFF</strong>:<p class="source-code">def dataset_to_param(dataset):</p><p class="source-code">    return dict(src='hda', id=dataset['id'])</p><p class="source-code">tool_inputs = {</p><p class="source-code">    'input1': dataset_to_param(bed_ds)</p><p class="source-code">    }</p><p class="source-code">gi.tools.run_tool(ds_history['id'], bed2gff['id'], tool_inputs=tool_inputs)</p></li>
</ol>
<p>The parameters of the tool can be inspected in the preceding step. If you go to the web interface, you will see something similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<img alt="Figure 9.2 - Checking the results of our script via Galaxy’s web interface " height="590" src="image/B17942_09_02.jpg" width="1371"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 - Checking the results of our script via Galaxy’s web interface</p>
<p>Thus, we have<a id="_idIndexMarker678"/> accessed <a id="_idIndexMarker679"/>Galaxy using its REST API.</p>
<h1 id="_idParaDest-247"><a id="_idTextAnchor246"/>Deploying a variant analysis pipeline with Snakemake</h1>
<p>Galaxy is mostly<a id="_idIndexMarker680"/> geared toward users who are<a id="_idIndexMarker681"/> less inclined to program. Knowing how to deal with it, even if you prefer a more programmer-friendly environment, is important because of its pervasiveness. It is reassuring that an API exists to interact with Galaxy. But if you want a more programmer-friendly pipeline, there are many alternatives available. In this chapter, we explore two widely used programmer-friendly pipelines: <strong class="source-inline">snakemake</strong> and Nextflow. In this recipe, we consider <strong class="source-inline">snakemake</strong>.</p>
<p>Snakemake is implemented in Python and shares many traits with it. That being said, its fundamental inspiration is a Makefile, the framework used by the venerable <strong class="source-inline">make</strong>-building system.</p>
<p>Here, we will develop a mini variant analysis pipeline with <strong class="source-inline">snakemake</strong>. The objective here is n<a id="_idTextAnchor247"/>ot to get the scientific part right – we cover that in other chapters – but to see how to create pipelines with <strong class="source-inline">snakemake</strong>. Our mini pipeline will download HapMap data, subsample it at 1%, do a simple PCA, and draw it.</p>
<h2 id="_idParaDest-248"><a id="_idTextAnchor248"/>Getting ready</h2>
<p>You will need Plink 2 installed alongside <strong class="source-inline">snakemake</strong>. To display execution strategies, you will also need Graphviz to draw the execution. We will define the following tasks:</p>
<ol>
<li value="1">Downloading data</li>
<li>Uncompressing it</li>
<li>Sub-sampling it at 1%</li>
<li>Computing the PCA on the 1% sub-sample</li>
<li>Charting the PCA</li>
</ol>
<p>Our<a id="_idIndexMarker682"/> pipeline recipe will have two parts: the actual <a id="_idIndexMarker683"/>coding of the pipeline in <strong class="source-inline">snakemake</strong> and a support script in Python.</p>
<p>The <strong class="source-inline">snakemake</strong> code for this can be found in <strong class="source-inline">Chapter09/snakemake/Snakefile</strong>, whereas the Python support script is in <strong class="source-inline">Chapter09/snakemake/plot_pca.py</strong>.</p>
<h2 id="_idParaDest-249"><a id="_idTextAnchor249"/>How to do it…</h2>
<ol>
<li value="1">The first task is downloading the data:<p class="source-code">from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider </p><p class="source-code">HTTP = HTTPRemoteProvider()</p><p class="source-code">download_root = "https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3"</p><p class="source-code">remote_hapmap_map = f"{download_root}/plink_format/hapmap3_r3_b36_fwd.consensus.qc.poly.map.gz"</p><p class="source-code">remote_hapmap_ped = f"{download_root}/plink_format/hapmap3_r3_b36_fwd.consensus.qc.poly.ped.gz"</p><p class="source-code">remote_hapmap_rel = f"{download_root}/relationships_w_pops_041510.txt"</p><p class="source-code"> </p><p class="source-code">rule plink_download:</p><p class="source-code">    input:</p><p class="source-code">        map=HTTP.remote(remote_hapmap_map, keep_local=True),</p><p class="source-code">        ped=HTTP.remote(remote_hapmap_ped, keep_local=True),</p><p class="source-code">        rel=HTTP.remote(remote_hapmap_rel, keep_local=True)</p><p class="source-code"> </p><p class="source-code">    output:</p><p class="source-code">        map="scratch/hapmap.map.gz",</p><p class="source-code">        ped="scratch/hapmap.ped.gz",</p><p class="source-code">        rel="data/relationships.txt"</p><p class="source-code"> </p><p class="source-code">    shell:</p><p class="source-code">        "mv {input.map} {output.map};"</p><p class="source-code">        "mv {input.ped} {output.ped};"</p><p class="source-code">        "mv {input.rel} {output.rel}"</p></li>
</ol>
<p>Snakemake’s language is Python-dependent, as you can see from the very first lines, which should be easy to understand from a Python perspective. The fundamental part is the rule. It has a set of input streams, which are rendered via <strong class="source-inline">HTTP.remote</strong> in our case, as we are dealing with remote files, followed by the output. We put two files in a <strong class="source-inline">scratch</strong> directory (the ones that are still uncompressed) and one in the <strong class="source-inline">data</strong> directory. Finally, our pipeline code is a simple shell script that moves the downloaded HTTP files to their final location. Note how the shell script refers to inputs and outputs.</p>
<ol>
<li value="2">With <a id="_idIndexMarker684"/>this script, downloading the <a id="_idIndexMarker685"/>files is easy. Run the following on the command line:<p class="source-code">snakemake -c1 data/relationships.txt</p></li>
</ol>
<p>This tells <strong class="source-inline">snakemake</strong> that you want to materialize <strong class="source-inline">data/relationships.txt</strong> . We will be using a single core, <strong class="source-inline">-c1</strong> . As this is an output of the <strong class="source-inline">plink_download</strong> rule, the rule will then be run (unless the file is already available – in that case, <strong class="source-inline">snakemake</strong> will do nothing). Here is an abridged version of the output:</p>
<p class="source-code"><strong class="bold">Building DAG of jobs...</strong></p>
<p class="source-code"><strong class="bold">Using shell: /usr/bin/bash</strong></p>
<p class="source-code"><strong class="bold">Provided cores: 1 (use --cores to define parallelism)</strong></p>
<p class="source-code"><strong class="bold">Rules claiming more threads will be scaled down.</strong></p>
<p class="source-code"><strong class="bold">Job stats:</strong></p>
<p class="source-code"><strong class="bold">job               count    min threads    max threads</strong></p>
<p class="source-code"><strong class="bold">--------------  -------  -------------  -------------</strong></p>
<p class="source-code"><strong class="bold">plink_download        1              1              1</strong></p>
<p class="source-code"><strong class="bold">total                 1              1              1</strong></p>
<p class="source-code"><strong class="bold"> </strong></p>
<p class="source-code"><strong class="bold">Select jobs to execute...</strong></p>
<p class="source-code"><strong class="bold"> </strong></p>
<p class="source-code"><strong class="bold">[Mon Jun 13 18:54:26 2022]</strong></p>
<p class="source-code"><strong class="bold">rule plink_download:</strong></p>
<p class="source-code"><strong class="bold">    input: ftp.ncbi.nlm.nih.gov/hapmap/ge [...]</strong></p>
<p class="source-code"><strong class="bold">    output: [..], data/relationships.txt</strong></p>
<p class="source-code"><strong class="bold">    jobid: 0</strong></p>
<p class="source-code"><strong class="bold">    reason: Missing output files: data/relationships.txt</strong></p>
<p class="source-code"><strong class="bold">    resources: tmpdir=/tmp</strong></p>
<p class="source-code"><strong class="bold"> </strong></p>
<p class="source-code"><strong class="bold">Downloading from remote: [...]relationships_w_pops_041510.txt</strong></p>
<p class="source-code"><strong class="bold">Finished download.</strong></p>
<p class="source-code"><strong class="bold">[...]</strong></p>
<p class="source-code"><strong class="bold">Finished job 0.</strong></p>
<p class="source-code"><strong class="bold">1 of 1 steps (100%) done</strong></p>
<p>Snakemake gives you some information about which jobs will be executed and starts running those.</p>
<ol>
<li value="3">Now<a id="_idIndexMarker686"/> that we have the data, let’s see<a id="_idIndexMarker687"/> the rule for uncompressing it:<p class="source-code">PLINKEXTS = ['ped', 'map']</p><p class="source-code">rule uncompress_plink:</p><p class="source-code">    input:</p><p class="source-code">        "scratch/hapmap.{plinkext}.gz"</p><p class="source-code">    output:</p><p class="source-code">        "data/hapmap.{plinkext}"</p><p class="source-code">    shell:</p><p class="source-code">        "gzip -dc {input} &gt; {output}"</p></li>
</ol>
<p>The most interesting feature here is the way that we can specify multiple files to download. Note how the <strong class="source-inline">PLINKEXTS</strong> list is converted into discrete <strong class="source-inline">plinkext</strong> elements in the code. You can execute by requesting an output from the rule.</p>
<ol>
<li value="4">Now, let’s<a id="_idIndexMarker688"/> subsample our<a id="_idIndexMarker689"/> data to 1%:<p class="source-code">rule subsample_1p:</p><p class="source-code">    input:</p><p class="source-code">        "data/hapmap.ped",</p><p class="source-code">        "data/hapmap.map"</p><p class="source-code"> </p><p class="source-code">    output:</p><p class="source-code">        "data/hapmap1.ped",</p><p class="source-code">        "data/hapmap1.map"</p><p class="source-code"> </p><p class="source-code">    run:</p><p class="source-code">        shell(f"plink2 --pedmap {input[0][:-4]} --out {output[0][:-4]} --thin 0.01 --geno 0.1 --export ped")</p></li>
</ol>
<p>The new content is in the last two lines: we are not using <strong class="source-inline">script</strong>, but <strong class="source-inline">run</strong>. This tells <strong class="source-inline">snakemake</strong> that the execution is Python-based with a few extra functions available. Here we see the shell function, which executes a shell script. The string is a Python <strong class="source-inline">f</strong>-string – note the reference to the <strong class="source-inline">snakemake</strong> <strong class="source-inline">input</strong> and <strong class="source-inline">output</strong> variables in the string. You could put more complex Python code here – for example, you could iterate over the inputs.</p>
<p class="callout-heading">TIP</p>
<p class="callout">Here, we are assuming that Plink is available, as we pre-installed it, but <strong class="source-inline">snakemake</strong> does provide some functionality to deal with dependencies. More specifically, <strong class="source-inline">snakemake</strong> rules can be annotated with a <strong class="source-inline">YAML</strong> file pointing to <strong class="source-inline">conda</strong> dependencies. </p>
<ol>
<li value="5">Now<a id="_idIndexMarker690"/> that we have our data<a id="_idIndexMarker691"/> sub-sampled, let’s compute the PCA. In this case, we will use Plink’s internal PCA framework to do the computation:<p class="source-code">rule plink_pca:</p><p class="source-code">    input:</p><p class="source-code">        "data/hapmap1.ped",</p><p class="source-code">        "data/hapmap1.map"</p><p class="source-code">    output:</p><p class="source-code">        "data/hapmap1.eigenvec",</p><p class="source-code">        "data/hapmap1.eigenval"</p><p class="source-code">    shell:</p><p class="source-code">        "plink2 --pca --file data/hapmap1 -out data/hapmap1"</p></li>
<li>As with most pipeline systems, <strong class="source-inline">snakemake</strong> constructs a <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) of <a id="_idIndexMarker692"/>operations to execute. At any moment, you can ask <strong class="source-inline">snakemake</strong> to present you a DAG of what you will execute to generate your request. For example, to generate the PCA, use the following:<p class="source-code">snakemake --dag data/hapmap1.eigenvec | dot -Tsvg &gt; bio.svg</p></li>
</ol>
<p>This would <a id="_idIndexMarker693"/>generate the following <a id="_idIndexMarker694"/>figure:</p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<img alt="Figure 9.3 - The DAG to compute the PCA " height="869" src="image/B17942_09_03.jpg" width="771"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 - The DAG to compute the PCA</p>
<ol>
<li value="7">Finally, let’s generate <a id="_idIndexMarker695"/>the <strong class="source-inline">plot</strong> rule for the PCA:<p class="source-code">rule plot_pca:</p><p class="source-code">    input:</p><p class="source-code">        "data/hapmap1.eigenvec",</p><p class="source-code">        "data/hapmap1.eigenval"</p><p class="source-code"> </p><p class="source-code">    output:</p><p class="source-code">        "pca.png"</p><p class="source-code"> </p><p class="source-code">    script:</p><p class="source-code">        "./plot_pca.py"</p></li>
</ol>
<p>The <strong class="source-inline">plot</strong> rule introduces a new type of execution, <strong class="source-inline">script</strong>. In this case, an external Python script is called to process the rule. </p>
<ol>
<li value="8">Our <a id="_idIndexMarker696"/>Python script to generate the chart is the following:<p class="source-code">import pandas as pd</p><p class="source-code"> </p><p class="source-code">eigen_fname = snakemake.input[0] if snakemake.input[0].endswith('eigenvec') else snakemake.input[1]</p><p class="source-code">pca_df = pd.read_csv(eigen_fname, sep='\t') </p><p class="source-code">ax = pca_df.plot.scatter(x=2, y=3, figsize=(16, 9))</p><p class="source-code">ax.figure.savefig(snakemake.output[0])</p></li>
</ol>
<p>The Python script has access to the <strong class="source-inline">snakemake</strong> object. This object exposes the content of the rule: note how we make use of <strong class="source-inline">input</strong> to get the PCA data and <strong class="source-inline">output</strong> to generate the image.</p>
<ol>
<li value="9">Finally, the <a id="_idIndexMarker697"/>code to produce a <a id="_idIndexMarker698"/>rough chart is as follows:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer074">
<img alt="Figure 9.4 - A very rough PCA produced by the Snakemake pipeline " height="429" src="image/B17942_09_04.jpg" width="586"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 - A very rough PCA produced by the Snakemake pipeline</p>
<h2 id="_idParaDest-250"><a id="_idTextAnchor250"/>There’s more</h2>
<p>The preceding recipe was made to run on a simple configuration of <strong class="source-inline">snakemake</strong>. There are many more ways to construct rules in <strong class="source-inline">snakemake</strong>.</p>
<p>The most important issue that we didn’t discuss is the fact that <strong class="source-inline">snakemake</strong> can execute code in many different environments, from the local computer (as in our case), on-premises clusters, to the cloud. It would be unreasonable to ask for anything more than using a local computer to try <strong class="source-inline">snakemake</strong>, but don’t forget that <strong class="source-inline">snakemake</strong> can manage complex computing environments.</p>
<p>Remember that <strong class="source-inline">snakemake</strong>, while implemented in Python, is conceptually based on <strong class="source-inline">make</strong>. It’s a subjective analysis to decide whether you like the (snake)make design. For an alternative design approach, check the next recipe, which uses Nextflow.</p>
<h1 id="_idParaDest-251"><a id="_idTextAnchor251"/>Deploying a variant analysis pipeline with Nextflow</h1>
<p>There are two<a id="_idIndexMarker699"/> main players in the pipeline <a id="_idIndexMarker700"/>framework space in bioinformatics: <strong class="source-inline">snakemake</strong> and Nextflow. They provide pipeline functionality whilst having different design approaches. Snakemake is based on Python, but its language and philosophy come from the <strong class="source-inline">make</strong> tool used to compile complex programs with dependencies. Nextflow is Java-based (more precisely, it’s implemented in Groovy – a language that works on top of the Java Virtual Machine) and has its own <strong class="bold">Domain Specific Language</strong> (<strong class="bold">DSL</strong>) for<a id="_idIndexMarker701"/> implementing pipelines. The main purpose of this recipe (and the previous recipe) is to give you a flavor of Nextflow so that you can compare it with <strong class="source-inline">snakemake</strong> and choose the one that better suits your needs.</p>
<p class="callout-heading">TIP</p>
<p class="callout">There are many perspectives on how to evaluate a pipeline system. Here, we present a perspective based on the language used to specify the pipeline. However, there are others that you should consider when choosing a pipeline system. For example, how well does it support your execution environment (such as an on-premises cluster or a cloud), does it support your tools (or allow easy development of extensions to deal with new tools), and does it provide good recovery and monitoring functionalities?</p>
<p>Here, we<a id="_idIndexMarker702"/> will develop a pipeline with<a id="_idIndexMarker703"/> Nextflow that provides the same functionality as we implemented with <strong class="source-inline">snakemake</strong>, thus allowing for a fair comparison from the pipeline design point of view. The objective here is not to get the scientific part right – we cover that in other chapters – but to see how to create pipelines with <strong class="source-inline">snakemake</strong>. Our mini pipeline will download HapMap data, sub-sample it at 1%, do a simple PCA, and draw it.</p>
<h2 id="_idParaDest-252"><a id="_idTextAnchor252"/>Getting ready</h2>
<p>You will need Plink 2 installed along with Nextflow. Nextflow itself requires some software from the Java space: notably the Java Runtime Environment and Groovy.</p>
<p>We will define the following tasks:</p>
<ol>
<li value="1">Downloading data</li>
<li>Uncompressing it</li>
<li>Sub-sampling it at 1%</li>
<li>Computing the PCA on the 1% sub-sample</li>
<li>Charting the PCA</li>
</ol>
<p>The Nextflow code for this can be found in <strong class="source-inline">Chapter09/nextflow/pipeline.nf</strong>.</p>
<h2 id="_idParaDest-253"><a id="_idTextAnchor253"/>How to do it…</h2>
<ol>
<li value="1">The<a id="_idIndexMarker704"/> first task is downloading<a id="_idIndexMarker705"/> the data:<p class="source-code">nextflow.enable.dsl=2</p><p class="source-code">download_root = "https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/hapmap3_r3"</p><p class="source-code"> process plink_download {</p><p class="source-code">  output:</p><p class="source-code">  path 'hapmap.map.gz'</p><p class="source-code">  path 'hapmap.ped.gz'</p><p class="source-code">  script:</p><p class="source-code">  """</p><p class="source-code">  wget $download_root/plink_format/hapmap3_r3_b36_fwd.consensus.qc.poly.map.gz -O hapmap.map.gz</p><p class="source-code">  wget $download_root/plink_format/hapmap3_r3_b36_fwd.consensus.qc.poly.ped.gz -O hapmap.ped.gz</p><p class="source-code">   """</p><p class="source-code">}</p></li>
</ol>
<p>Remember that the underlying language for the pipeline is not Python but Groovy, so the syntax will be a bit different, such as using braces for blocks or ignoring indentation.</p>
<p>We create a process (a pipeline building block in Nextflow) called <strong class="source-inline">plink_download</strong>, which downloads the Plink files. It only specifies outputs. The first output will be the <strong class="source-inline">hapmap.map.gz</strong> file and the second output will be <strong class="source-inline">hapmap.ped.gz</strong>. This process will have two output channels (another Nextflow concept, akin to a stream), which can be consumed by another process.</p>
<p>The code for the process is, by default, a bash script. It’s important to note how the script outputs files with names that are synchronized with the output section. Also, see how we refer to the variables defined in the pipeline (<strong class="source-inline">download_root</strong>, in our case).</p>
<ol>
<li value="2">Let’s now<a id="_idIndexMarker706"/> define a process to consume<a id="_idIndexMarker707"/> the channels with the HapMap files and decompress them:<p class="source-code">process uncompress_plink {</p><p class="source-code">  publishDir 'data', glob: '*', mode: 'copy'</p><p class="source-code">  </p><p class="source-code">  input:</p><p class="source-code">  path mapgz</p><p class="source-code">  path pedgz</p><p class="source-code"> </p><p class="source-code">  output:</p><p class="source-code">  path 'hapmap.map'</p><p class="source-code">  path 'hapmap.ped'</p><p class="source-code"> </p><p class="source-code">  script:</p><p class="source-code">  """</p><p class="source-code">  gzip -dc $mapgz &gt; hapmap.map</p><p class="source-code">  gzip -dc $pedgz &gt; hapmap.ped</p><p class="source-code">  """</p><p class="source-code">}</p></li>
</ol>
<p>There are three issues of note in this process: we now have a couple of inputs (remember that we have a couple of outputs from the previous process). Our script also now refers to input variables (<strong class="source-inline">$mapgz</strong> and <strong class="source-inline">$pedgz</strong>). Finally, we publish the output by using <strong class="source-inline">publishDir</strong>. Therefore, any files that are not published will only be stored temporarily.</p>
<ol>
<li value="3">Let’s <a id="_idIndexMarker708"/>specify a first version of the <a id="_idIndexMarker709"/>workflow that downloads and uncompresses the files:<p class="source-code">workflow {</p><p class="source-code">    plink_download | uncompress_plink</p><p class="source-code">}</p></li>
<li>We can execute the workflow by running the following on the shell:<p class="source-code">nextflow run pipeline.nf -resume</p></li>
</ol>
<p>The <strong class="source-inline">resume</strong> flag at the end will make sure that the pipeline will continue from whatever step was already completed. The steps are, on local execution, stored in the <strong class="source-inline">work</strong> directory.</p>
<ol>
<li value="5">If we remove the <strong class="source-inline">work</strong> directory, we don’t want to download the HapMap files if they were already published. As this is outside the <strong class="source-inline">work</strong> directory, hence not directly tracked, we need to change the workflow to track the data in the published directory:<p class="source-code">workflow {</p><p class="source-code">    ped_file = file('data/hapmap.ped')</p><p class="source-code">    map_file = file('data/hapmap.map')</p><p class="source-code">    if (!ped_file.exists() | !map_file.exists()) {</p><p class="source-code">        plink_download | uncompress_plink</p><p class="source-code">    }</p><p class="source-code">}</p></li>
</ol>
<p>There are <a id="_idIndexMarker710"/>alternative ways of doing<a id="_idIndexMarker711"/> this, but I wanted to introduce a bit of Groovy code, as you might sometimes have to write code in Groovy. There are ways to use Python code, as you will see soon.</p>
<ol>
<li value="6">Now, we need to subsample the data:<p class="source-code">process subsample_1p {</p><p class="source-code">  input:</p><p class="source-code">  path 'hapmap.map'</p><p class="source-code">  path 'hapmap.ped'</p><p class="source-code"> </p><p class="source-code">  output:</p><p class="source-code">  path 'hapmap1.map'</p><p class="source-code">  path 'hapmap1.ped'</p><p class="source-code"> </p><p class="source-code">  script:</p><p class="source-code">  """</p><p class="source-code">  plink2 --pedmap hapmap --out hapmap1 --thin 0.01 --geno 0.1 --export ped</p><p class="source-code">  """</p><p class="source-code">}</p></li>
<li>Let’s now compute the PCA using Plink:<p class="source-code">process plink_pca {</p><p class="source-code">  input:</p><p class="source-code">  path 'hapmap.map'</p><p class="source-code">  path 'hapmap.ped'</p><p class="source-code">  output:</p><p class="source-code">  path 'hapmap.eigenvec'</p><p class="source-code">  path 'hapmap.eigenval'</p><p class="source-code">   script:</p><p class="source-code">  """</p><p class="source-code">  plink2 --pca --pedmap hapmap -out hapmap</p><p class="source-code">  """</p><p class="source-code">}</p></li>
<li>Finally, let’s <a id="_idIndexMarker712"/>plot <a id="_idIndexMarker713"/>the PCA:<p class="source-code">process plot_pca {</p><p class="source-code">  publishDir '.', glob: '*', mode: 'copy'</p><p class="source-code"> </p><p class="source-code">  input:</p><p class="source-code">  path 'hapmap.eigenvec'</p><p class="source-code">  path 'hapmap.eigenval'</p><p class="source-code"> </p><p class="source-code">  output:</p><p class="source-code">  path 'pca.png'</p><p class="source-code"> </p><p class="source-code">  script:</p><p class="source-code">  """</p><p class="source-code">  #!/usr/bin/env python</p><p class="source-code">  import pandas as pd</p><p class="source-code"> </p><p class="source-code">  pca_df = pd.read_csv('hapmap.eigenvec', sep='\t') </p><p class="source-code">  ax = pca_df.plot.scatter(x=2, y=3, figsize=(16, 9))</p><p class="source-code">  ax.figure.savefig('pca.png')</p><p class="source-code">  """</p><p class="source-code">}</p></li>
</ol>
<p>The new<a id="_idIndexMarker714"/> feature of this code is that we <a id="_idIndexMarker715"/>specify the bash script using the shebang (<strong class="source-inline">#!</strong>) operator, which allows us to call an external scripting language to process the data.</p>
<p>Here is our final workflow:</p>
<p class="source-code">workflow {</p>
<p class="source-code">    ped_file = file('data/hapmap.ped')</p>
<p class="source-code">    map_file = file('data/hapmap.map')</p>
<p class="source-code">    if (!ped_file.exists() | !map_file.exists()) {</p>
<p class="source-code">        plink_download | uncompress_plink | subsample_1p | plink_pca | plot_pca</p>
<p class="source-code">    }</p>
<p class="source-code">    else {</p>
<p class="source-code">        subsample_1p(</p>
<p class="source-code">            Channel.fromPath('data/hapmap.map'),</p>
<p class="source-code">            Channel.fromPath('data/hapmap.ped')) | plink_pca | plot_pca</p>
<p class="source-code">    }</p>
<p class="source-code">}</p>
<p>We either download the data or use the already downloaded data.</p>
<p>While there are other dialects for designing the complete workflow, I would like you to note how we use <strong class="source-inline">subsample_1p</strong> when the files are available; we can explicitly pass two channels to a process.</p>
<ol>
<li value="9">We can run the pipeline and request an HTML report on execution:<p class="source-code">nextflow run pipeline.nf -with-report report/report.xhtml</p></li>
</ol>
<p>The report is <a id="_idIndexMarker716"/>quite exhaustive and will allow you to<a id="_idIndexMarker717"/> figure out the parts of the pipeline that are expensive from different perspectives, whether related to time, memory, CPU consumption, or I/O.</p>
<h2 id="_idParaDest-254"><a id="_idTextAnchor254"/>There’s more</h2>
<p>This was a simple introductory example of Nextflow, which hopefully will allow you to get a flavor of the framework, particularly so that you can compare it with <strong class="source-inline">snakemake</strong>. Nextflow has many more functionalities and you are encouraged to check out its website.</p>
<p>As with <strong class="source-inline">snakemake</strong>, the most important issue that we didn’t discuss is the fact that Nextflow can execute code in many different environments, from the local computer, on-premises clusters, to the cloud. Check Nextflow’s documentation to see which computing environments are currently supported.</p>
<p>As important as the underlying language is, Groovy with Nextflow and Python with <strong class="source-inline">snakemake</strong>, make sure to compare other factors. This includes not only where both pipelines can execute (locally, in a cluster, or in a cloud) but also the design of the framework, as they use quite different paradigms.</p>
</div>
<div>
<div id="_idContainer076">
</div>
</div>
</div></body></html>