- en: 'Chapter 8: Unsupervised Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two chapters, you were introduced to the supervised learning
    class of machine learning algorithms, their real-world applications, and how to
    implement them at scale using Spark MLlib. In this chapter, you will be introduced
    to the unsupervised learning category of machine learning, where you will learn
    about parametric and non-parametric unsupervised algorithms. A few real-world
    applications of **clustering** and **association** algorithms will be presented
    to help you understand the applications of **unsupervised learning** to solve
    real-life problems. You will gain basic knowledge and understanding of clustering
    and association problems when using unsupervised machine learning. We will also
    look at the implementation details of a few clustering algorithms in Spark ML,
    such as **K-means clustering**, **hierarchical clustering**, **latent Dirichlet
    allocation**, and an association algorithm called **alternating least squares**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering using machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building association using machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world applications of unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have gained sufficient knowledge and
    practical understanding of the clustering and association types of unsupervised
    machine learning algorithms, their practical applications, and skills to implement
    these types of algorithms at scale using Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be using Databricks Community Edition to run our code
    ([https://community.cloud.databricks.com](https://community.cloud.databricks.com)).
  prefs: []
  type: TYPE_NORMAL
- en: Sign-up instructions can be found at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter can be downloaded from [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter08](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter08).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The datasets for this chapter can be found at [https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data](https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to unsupervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning is a machine learning technique where no guidance is available
    to the learning algorithm in the form of known label values in the training data.
    Unsupervised learning is useful in categorizing unknown data points into groups
    based on patterns, similarities, or differences that are inherent within the data,
    without any prior knowledge of the data.
  prefs: []
  type: TYPE_NORMAL
- en: In supervised learning, a model is trained on known data, and then inferences
    are drawn from the model using new, unseen data. On the other hand, in unsupervised
    learning, the model training process in itself is the end goal, where patterns
    hidden within the training data are discovered during the model training process.
    Unsupervised learning is harder compared to supervised learning since it is difficult
    to ascertain if the results of an unsupervised learning algorithm are meaningful
    without any external evaluation, especially without access to any correctly labeled
    data.
  prefs: []
  type: TYPE_NORMAL
- en: One of the advantages of unsupervised learning is that it helps interpret very
    large datasets where labeling existing data would not be practical. Unsupervised
    learning is also useful for tasks such as predicting the number of classes within
    a dataset, or grouping and clustering data before applying a supervised learning
    algorithm. It is also very useful in solving classification problems as unsupervised
    learning can work well with unlabelled data, and there is no need for any manual
    intervention either. Unsupervised learning can be classified into two major learning
    techniques, known as clustering and association. These will be presented in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering using machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In machine learning, clustering deals with identifying patterns or structures
    within uncategorized data without needing any external guidance. Clustering algorithms
    parse given data to identify clusters or groups with matching patterns that exist
    in the dataset. The result of clustering algorithms are clusters of data that
    can be defined as a collection of objects that are similar in a certain way. The
    following diagram illustrates how clustering works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Clustering'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16736_08_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Clustering
  prefs: []
  type: TYPE_NORMAL
- en: In the previous diagram, an uncategorized dataset is being passed through a
    clustering algorithm, resulting in the data being categorized into smaller clusters
    or groups of data, based on a data point's proximity to another data point in
    a two-dimensional Euclidian space.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the clustering algorithm groups data based on the Euclidean distance between
    the data on a two-dimensional plane. Clustering algorithms consider the Euclidean
    distance between data points in the training dataset in that, within a cluster,
    the distance between the data points should be small, while outside the cluster,
    the distance between the data points should be large. A few types of clustering
    techniques that are available in Spark MLlib will be presented in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**K-means** is the most popular clustering algorithm and one of the simplest
    of the unsupervised learning algorithms as well. The K-means clustering algorithm
    works iteratively on the provided dataset to categorize it into *k* groups. The
    larger the value of *k*, the smaller the size of the clusters, and vice versa.
    Thus, with K-means, the user can control the number of clusters that are identified
    within the given dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: In K-means clustering, each cluster is defined by creating a center for each
    cluster. These centroids are placed as far away as possible from each other. Then,
    K-means associates each data point with the given dataset to its nearest centroid,
    thus forming the first group of clusters. K-means then iteratively recalculates
    the centroids' position within the dataset so that it's as close to the center
    of the identified clusters. This process stops when the centroids don't need to
    be moved anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block illustrates how to implement K-means clustering using
    Spark MLlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we used `import` to import the appropriate MLlib packages related to
    clustering and clustering evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we imported the already existing feature vector that we derived during
    the feature engineering process into a Spark DataFrame and stored it in the data
    lake in Delta format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, a new `KMeans` object was initialized by us passing in the number of desired
    clusters and the column name for the feature vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `fit()` method was called on the training DataFrame to kick off the learning
    process. A model object was generated as a result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predictions on the original training dataset were generated by calling the `transform()`
    method on the model object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we invoked Spark MLlib's `ClusteringEvaluator()` helper function, which
    is useful for evaluating clustering algorithms, and applied it to the predictions
    DataFrame we generated in the previous step. This resulted in a value referred
    to as `silhouette`, which is a measure of consistency within clusters and is calculated
    based on the Euclidean distance measure between data points. A `silhouette` value
    closer to `1` means that the points within a cluster are close together and that
    points outside the cluster are far apart. The closer the `silhouette` value is
    to `1`, the more performant the learned model is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we printed the centroids of each of the categorized clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This way, using just a few lines of code, uncategorized data can easily be clustered
    using Spark's implementation of the K-means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering using bisecting K-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Hierarchical clustering** is a type of clustering technique where all the
    data points start within a single cluster. They are then recursively split into
    smaller clusters by moving them down a hierarchy. Spark ML implements this kind
    of divisive hierarchical clustering via the bisecting K-means algorithm. The following
    example illustrates how to implement bisecting K-means clustering using Spark
    MLlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code snippet, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialized a new `BisectingKMeans` object by passing in the number
    of desired clusters and the column name for the feature column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `fit()` method was called on the training DataFrame to start the learning
    process. A model object was generated as a result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, predictions on the original training dataset were generated by calling
    the `transform()` method on the model object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we invoked Spark MLlib's `ClusteringEvaluator()` helper function, which
    is useful for evaluating clustering algorithms, and applied it to the predictions
    DataFrame we generated in the previous step. This results in the `silhouette`
    value, which is a measure of consistency within clusters and is calculated based
    on the Euclidean distance measure between data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we printed the centroids of each of the clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned a clustering technique, let's find out about a learning
    technique in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling using latent Dirichlet allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Topic modeling** is a learning technique where you categorize documents.
    Topic modeling is not the same as topic classification since topic classification
    is a supervised learning technique where the learning model tries to classify
    unseen documents based on some previously labeled data. On the other hand, topic
    modeling categorizes documents containing text or natural language in the same
    way as clustering groups categorize numeric data without any external guidance.
    Thus, topic modeling is an unsupervised learning problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent Dirichlet allocation** (**LDA**) is a popular topic modeling technique.
    The goal of LDA is to associate a given document with a particular topic based
    on the keywords found within the document. Here, the topics are unknown and hidden
    within the documents, thus the latent part of LDA. LDA works by assuming each
    word within a document belongs to a different topic and assigns a probability
    score to each word. Once the probability of each word belonging to a particular
    topic is estimated, LDA tries to pick all the words belonging to a topic by setting
    a threshold and choosing every word that meets or exceeds that threshold value.
    LDA also considers each document to be just a bag of words, without placing any
    importance on the grammatical role played by the individual words. Also, stop
    words in a language such as articles, conjunctions, and interjections need to
    be removed before LDA is applied as these words do not carry any topic information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example illustrates how LDA can be implemented using Spark
    MLlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code snippet, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we imported the appropriate MLlib packages related to LDA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we imported the already existing feature vector that had been derived
    during the feature engineering process into a Spark DataFrame and stored it in
    the data lake in Delta format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we initialized a new `LDA` object by passing in the number of clusters
    and the maximum number of iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the `fit()` method was called on the training DataFrame to start the learning
    process. A model object was generated as a result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The topics that were modeled by the LDA algorithm can be shown by using the
    `describeTopics()` method on the model object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we have seen, by using Apache Spark's implementation of the LDA algorithm,
    topic modeling can be implemented at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian mixture model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the disadvantages of K-means clustering is that it will associate every
    data point with exactly one cluster. This way, it is not possible to get the probability
    of a data point belonging to a particular cluster. The **Gaussian mixture model**
    (**GSM**) attempts to solve this hard clustering problem of K-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: GSM is a probabilistic model for representing a subset of a sample within an
    overall sample of data points. A GSM represents a mixture of several Gaussian
    distributions of data points, where a data point is drawn from one of the *K*
    Gaussian distributions and has a probability score of it belonging to one of those
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example describes the implementation details of a GSM using
    Spark ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we initialized a new `GaussianMixture` object after
    importing the appropriate libraries from the `pyspark.ml.clustering` package.
    Then, we passed in some hyperparameters, including the number of clusters and
    the name of the column containing the feature vector. Then, we trained the model
    using the `fit()` method and displayed the results of the trained model using
    the model's `gaussianDF` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have seen different kinds of clustering and topic modeling techniques
    and their implementations when using Spark MLlib. In the following section, you
    will learn about another type of unsupervised learning algorithm called **association
    rules**.
  prefs: []
  type: TYPE_NORMAL
- en: Building association rules using machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`if-then-else` statements that help show the probability of relationships between
    entities. The association rules technique is widely used in recommender systems,
    market basket analysis, and affinity analysis problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering using alternating least squares
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, **collaborative filtering** is more commonly used for **recommender
    systems**. A recommender system is a technique that's used to filter information
    by considering user preference. Based on user preference and taking into consideration
    their past behavior, recommender systems can make predictions on items that the
    user might like. Collaborative filtering performs information filtering by making
    use of historical user behavior data and their preferences to build a user-item
    association matrix. Spark ML uses the **alternating least squares** algorithm
    to implement the collaborative filtering technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example demonstrates Spark MLlib''s implementation of the
    alternating least squares algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code block, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we generated the ratings dataset as a Spark DataFrame using the feature
    dataset stored in the Delta Lake. A few of the columns that the ALS algorithm
    required, such as `user_id`, `item_id`, and `ratings` were not in the required
    integer format. Thus, we used the `CAST` Spark SQL method to convert them into
    the required data format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we initialized an ALS object with the desired parameters and split our
    training dataset into two random parts using the `randomSplit()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After, we started the learning process by calling the `fit()` method on the
    training dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we evaluated the accuracy metric's `RMSE` using the evaluator provided
    by Spark MLlib.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we gathered the predictions for the top 5 item recommendations for
    each user and the top 5 user recommendations per item using the built-in `recommendForAllUsers()`
    and `recommendForAllItems()` methods, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This way, you can leverage alternating least squares to build recommender systems
    for use cases such as movie recommendations for a **video on demand** platform,
    product recommendations, or **market basket analysis** for an e-tailer application.
    Spark MLlib helps you implement this scale with only a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to clustering and association rules, Spark MLlib also allows you
    to implement **dimensionality reduction** algorithms such as **singular value
    decomposition** (**SVD**) and **principal component analysis** (**PCA**). Dimensionality
    reduction is the process of reducing the number of random variables under consideration.
    Though an unsupervised learning method, dimensionality reduction is useful for
    feature extraction and selection. A detailed discussion of this topic is beyond
    the scope of this book, and Spark MLlib only has the dimensionality reduction
    algorithm's implementation available for the RDD API. More details on dimensionality
    reduction can be found in Apache Spark's public documentation at [https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html](https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will delve into a few more real-life applications of
    unsupervised learning algorithms that are in use today by various businesses.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world applications of unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms are being used today to solve some real-world
    business challenges. We will take a look at a few such challenges in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section presents some of the real-world business applications of clustering
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Customer segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retail marketing teams, as well as business-to-customer organizations, are always
    trying to optimize their marketing spends. Marketing teams in particular are concerned
    with one specific metric called **cost per acquisition** (**CPA**). CPA is indicative
    of the amount that an organization needs to spend to acquire a single customer,
    and an optimal CPA means a better return on marketing investments. The best way
    to optimize CPA is via customer segmentation as this improves the effectiveness
    of marketing campaigns. Traditional customer segmentation takes standard customer
    features such as demographic, geographic, and social information into consideration,
    along with historical transactional data, to define standard customer segments.
    This traditional way of customer segmentation is time-consuming and involves a
    lot of manual work and is prone to errors. However, machine learning algorithms
    can be leveraged to find hidden patterns and associations among data sources.
    Also, in recent years. the number of customer touchpoints has increased, and it
    is not practical and intuitive to identify patterns among all those customer touchpoints
    to identify patterns manually. However, machine learning algorithms can easily
    parse through millions of records and surface insights that can be leveraged promptly
    by marketing teams to meet their customers where they want, when they want. Thus,
    by leveraging clustering algorithms, marketers can improve the efficacy of their
    marketing campaigns via refined customer segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Retail assortment optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retailers with brick-and-mortar stores have limited store space. Thus, they
    need to ensure that their store space is utilized optimally by placing only those
    products that are highly likely to sell. A classic example of assortment optimization
    is that of a hardware retailer, stocking up on lawnmowers during the deep winter
    season in the midwestern parts of the United States, when it is highly likely
    to snow through the season. In this example, the store space is being sub-optimally
    utilized by having lawnmowers in there, which have a very small chance of selling
    during the snow season. A better choice would have been space heaters, snow shovels,
    or other winter season equipment. To overcome this problem, retailers usually
    employ analysts, who take historical transactional data, seasonality, and current
    trends into consideration to make recommendations on the optimal assortment of
    products that are appropriate for the season and location of the store. However,
    what if we increase the scale of this problem to a much larger retailer, with
    thousands of warehouses and tens of thousands of retail outlets? At such a scale,
    manually planning optimal assortments of products becomes impractical and very
    time-consuming, reducing the time to value drastically. Assortment optimization
    can be treated as a clustering problem, and clustering algorithms can be applied
    to help plan how these clusters will be sorted. Here, several more data points
    must be taken into consideration, including historical consumer buying patterns,
    seasonality, trends on social media, search patterns on search engines, and more.
    This not only helps with better assortment optimization but also in increased
    revenues, a decrease in product waste, and faster time to market for businesses.
  prefs: []
  type: TYPE_NORMAL
- en: Customer churn analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is becoming increasingly difficult for businesses to acquire customers because
    of ever-changing customer preferences and fierce competition in the marketplace.
    Thus,  businesses need to retain existing customers. **Customer churn rate** is
    one of the prominent metrics that business executives want to minimize. Machine
    learning classification algorithms can be used to predict if a particular customer
    will churn. However, having an understanding of the factors that affect churn
    would be useful, so that they can change or improve their operations to increase
    customer satisfaction. Clustering algorithms can be used not only to identify
    which group of customers are likely to churn, but also to further the analysis
    by identifying a set of factors that are affecting churn. Businesses can then
    act on these churn factors to either bring new products into the mix or improve
    churn to improve customer satisfaction and, in turn, decrease customer churn.
  prefs: []
  type: TYPE_NORMAL
- en: Insurance fraud detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Insurance companies traditionally use manual inspection, along with rules engines,
    to flag insurance claims as fraudulent. However, as the size of the data increases,
    the traditional methods might miss a sizeable portion of the claims since manual
    inspection is time-consuming and error-prone, and fraudsters are constantly innovating
    and devising new ways of committing fraud. Machine learning clustering algorithms
    can be used to group new claims with existing fraud clusters, and classification
    algorithms can be used to classify whether these claims are fraudulent. This way,
    by leveraging machine learning and clustering algorithms, insurance companies
    can constantly detect and prevent insurance fraud.
  prefs: []
  type: TYPE_NORMAL
- en: Association rules and collaborative filtering applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Association rules and collaborative filtering are techniques that are used for
    building recommender systems. This section will explore some practical use cases
    of recommendation systems for practical business applications.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recommendation systems are employed by e-retailers to perform market basket
    analysis, where the system makes product recommendations to users based on their
    preferences, as well as items already in their cart. Recommendation systems can
    also be used for location- or proximity-based recommendations, such as displaying
    ads or coupons when a customer is near a particular store. Recommendation systems
    are also used in marketing, where marketers can get recommendations regarding
    users who are likely to buy an item, which helps with the effectiveness of marketing
    campaigns.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation systems are also heavily employed by online music and video service
    providers for user content personalization. Here, recommendation systems are used
    to make new music or video recommendations to users based on their preferences
    and historical usage patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced you to unsupervised learning algorithms, as well as
    how to categorize unlabeled data and identify associations between data entities.
    Two main areas of unsupervised learning algorithms, namely clustering and association
    rules, were presented. You were introduced to the most popular clustering and
    collaborative filtering algorithms. You were also presented with working code
    examples of clustering algorithms such as K-means, bisecting K-means, LDA, and
    GSM using code in Spark MLlib. You also saw code examples for building a recommendation
    engine using the alternative least-squares algorithm in Spark MLlib. Finally,
    a few real-world business applications of unsupervised learning algorithms were
    presented. We looked at several concepts, techniques, and code examples surrounding
    unsupervised learning algorithms so that you can train your models at scale using
    Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: So far, in this and the previous chapter, you have only explored the data wrangling,
    feature engineering, and model training parts of the machine learning process.
    In the next chapter, you will be introduced to machine learning life cycle management,
    where you will explore concepts such as model performance tuning, tracking machine
    learning experiments, storing machine learning models in a central repository,
    and operationalizing ML models before putting them in production applications.
    Finally, an open end-to-end ML life cycle management tool called MLflow will also
    be introduced and explored.
  prefs: []
  type: TYPE_NORMAL
