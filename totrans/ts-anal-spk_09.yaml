- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going to Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our time series analysis model built and tested, and the ability to scale
    seen in the previous chapter, we will now explore the practical considerations
    and steps involved in deploying time series models into production with the Spark
    framework. This information is crucial to guide you through the transition from
    development to real-world implementation, ensuring the reliability and effectiveness
    of time series models in operational environments. With many machine learning
    projects stuck in development and proof of concept stages, grasping the nuances
    of deploying to production enhances your ability to integrate time series analyses
    seamlessly into decision-making processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'While in [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087) we covered the broader
    end-to-end view of a time series analysis project, in this chapter, we’re going
    to focus on going to production with the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and reporting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore with code examples the deployment of a scalable
    end-to-end workflow for time series analysis in our own container-based environment.
    A tremendous amount of work goes into building a production-ready environment,
    which goes way beyond what we can reasonably cover in this chapter. We will focus
    instead on providing an example as a starting point. We will see how what we have
    learned so far about time series analysis comes together to create an end-to-end
    workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at this URL: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by setting up our environment for the example.
  prefs: []
  type: TYPE_NORMAL
- en: Environment setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using Docker containers, like in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063)
    and [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087), for the platform infrastructure.
    Follow the instructions in the *Using a container for deployment* section of [*Chapter
    3*](B18568_03.xhtml#_idTextAnchor063) and the *Environment setup* section of [*Chapter
    4*](B18568_04.xhtml#_idTextAnchor087) on setting up the container environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the environment is set up, download the deployment script from the Git
    repository for this chapter, which is at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch9)'
  prefs: []
  type: TYPE_NORMAL
- en: You can then start the container environment as per the *Environment startup*
    section of [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087). Do a quick visual
    validation of the components as per the *Accessing the UIs* section of the same
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get into the nitty-gritty of the code, let’s step back with an overview
    of the workflow to see the big picture of what we will be building in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code example in this chapter includes two workflows. These are implemented
    as **Directed Acyclic Graphs** (**DAGs**) in Airflow, like in [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087).
    The best way to visualize the workflows is from the DAG views in Airflow, as per
    *Figures 9.1* and *9.2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two workflows are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ts-spark_ch9_data-ml-ops`: This is an example of the end-to-end process, shown
    in *Figure 9**.1*, which includes the following tasks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_config`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ingest_train_data`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transform_train_data`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_and_log_model`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forecast`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ingest_eval_data`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transform_eval_data`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_forecast`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18568_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Airflow DAG for the end-to-end workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '`ts-spark_ch9_data-ml-ops_runall`: This second workflow, shown in *Figure 9**.2*,
    calls the preceding one multiple times with different ranges of dates. It simulates
    what happens in the real world whereby the preceding end-to-end workflow is launched
    at a regular interval, say daily or weekly, with new data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18568_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Airflow DAG with multiple calls to the end-to-end workflow'
  prefs: []
  type: TYPE_NORMAL
- en: The code for these Airflow DAGs is in the `dags` folder. They are in Python
    (`.py` files) and can be visualized via a text, or preferably a code, editor.
  prefs: []
  type: TYPE_NORMAL
- en: Modularity
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that for the example here, we could have joined up all the
    code of these individual tasks into one big task. Instead, we have broken the
    workflow into multiple tasks to illustrate the best practice of modularization.
    In a real-world situation, this facilitates independent code change, scaling,
    and task reruns. Different teams may have ownership of the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Workflow separation
  prefs: []
  type: TYPE_NORMAL
- en: The workflows we are demonstrating in this example can be further separated
    in your own implementation. For instance, it is common to have the model-training-related
    tasks, the forecasting, and the model evaluation in their own individual workflows
    launched at different time intervals.
  prefs: []
  type: TYPE_NORMAL
- en: We will explain each of these DAGs and related tasks in detail in the upcoming
    sections, starting with `ts-spark_ch9_data-ml-ops_runall`.
  prefs: []
  type: TYPE_NORMAL
- en: Simulation and runs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in *Figure 9**.2*, `ts-spark_ch9_data-ml-ops_runall` has five tasks,
    which we will explain further here.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the `_runall` workflow is to simulate the real-world execution
    of the cycle of training, forecasting, and evaluation at regular intervals. In
    our example, each task of the `_runall` workflow corresponds to one cycle of training,
    forecasting, and evaluation. We will call each task a run and have five runs in
    all, corresponding to the five tasks of `_runall`. These tasks will be scheduled
    at regular intervals, daily, weekly, monthly, or so. In the example here, we are
    just running them sequentially, one after the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each task calls the `ts-spark_ch9_data-ml-ops` workflow with a different set
    of parameters. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`runid`: An integer to identify the run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`START_DATE`: The start date in the time series dataset to use for training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRAIN_END_DATE`: The end date in the time series dataset for training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EVAL_END_DATE`: The end date in the time series dataset for evaluation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The way the different runs are configured is with a sliding window of 5 years
    of training data and 1 year of evaluation data in our example. In a real-world
    scenario, the evaluation date range is likely to be shorter, corresponding to
    a shorter forecasting horizon.
  prefs: []
  type: TYPE_NORMAL
- en: 'The run configurations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The tasks are defined as follows to trigger the `ts-spark_ch9_data-ml-ops`
    workflow passing the run configuration as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The tasks are then launched sequentially as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can kick off this `ts-spark_ch9_data-ml-ops_runall` Airflow DAG from the
    Airflow DAG view as per *Figure 9**.3*, by clicking on the run (**>**) button
    highlighted in green.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Run the Airflow DAG'
  prefs: []
  type: TYPE_NORMAL
- en: The outcome of this DAG can be seen in *Figure 9**.2*, showing the status of
    the individual tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss the details of these tasks, which, as we have seen, call
    the `ts-spark_ch9_data-ml-ops` workflow with different parameters. Let’s start
    with the first step, `get_config`, tasked with handling these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first task in the `ts-spark_ch9_data-ml-ops` workflow is `t0` and it calls
    the `get_config` function to retrieve the configuration needed to run the workflow.
    These are passed as parameters when calling the workflow. They are, as mentioned
    earlier, the run identifier and date ranges of the time series data for which
    we want to run the workflow. We will see how they are used in the subsequent tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code that defines task `t0` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_config` function, which is called by task `t0`, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'def ingest_train_data(_vars, **kwargs):'
  prefs: []
  type: TYPE_NORMAL
- en: sdf = spark.read.csv(
  prefs: []
  type: TYPE_NORMAL
- en: DATASOURCE, header=True, inferSchema=True
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: sdf = sdf.filter(
  prefs: []
  type: TYPE_NORMAL
- en: (F.col('date') >= F.lit(_vars['START_DATE'])) &
  prefs: []
  type: TYPE_NORMAL
- en: (F.col('date') <= F.lit(_vars['TRAIN_END_DATE']))
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: data_ingest_count = sdf.count()
  prefs: []
  type: TYPE_NORMAL
- en: sdf.write.format("delta").mode("overwrite").save(
  prefs: []
  type: TYPE_NORMAL
- en: f"/data/delta/ts-spark_ch9_bronze_train_{_vars['runid']}"
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: _vars['train_ingest_count'] = data_ingest_count
  prefs: []
  type: TYPE_NORMAL
- en: return _vars
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'def transform_train_data(_vars, **kwargs):'
  prefs: []
  type: TYPE_NORMAL
- en: sdf = spark.read.format("delta").load(
  prefs: []
  type: TYPE_NORMAL
- en: f"/data/delta/ts-spark_ch9_bronze_train_{_vars['runid']}"
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: sdf = sdf.selectExpr(
  prefs: []
  type: TYPE_NORMAL
- en: '"date as ds",'
  prefs: []
  type: TYPE_NORMAL
- en: '"cast(daily_min_temperature as double) as y"'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: sdf = sdf.dropna()
  prefs: []
  type: TYPE_NORMAL
- en: data_transform_count = sdf.count()
  prefs: []
  type: TYPE_NORMAL
- en: sdf.write.format("delta").mode("overwrite").save(
  prefs: []
  type: TYPE_NORMAL
- en: f"/data/delta/ts-spark_ch9_silver_train_{_vars['runid']}"
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: _vars['train_transform_count'] = data_transform_count
  prefs: []
  type: TYPE_NORMAL
- en: return _vars
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'def train_and_log_model(_vars, **kwargs):'
  prefs: []
  type: TYPE_NORMAL
- en: sdf = spark.read.format("delta").load(
  prefs: []
  type: TYPE_NORMAL
- en: f"/data/delta/ts-spark_ch9_silver_train_{_vars['runid']}"
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: pdf = sdf.toPandas()
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.set_experiment(
  prefs: []
  type: TYPE_NORMAL
- en: '''ts-spark_ch9_data-ml-ops_time_series_prophet_train'''
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.start_run()
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.log_param("DAG_NAME", DAG_NAME)
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.log_param("TRAIN_START_DATE", _vars['START_DATE'])
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.log_metric(
  prefs: []
  type: TYPE_NORMAL
- en: '''train_ingest_count'', _vars[''train_ingest_count''])'
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: model = Prophet().fit(pdf)
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: cv_metrics_name = [
  prefs: []
  type: TYPE_NORMAL
- en: '"mse", "rmse", "mae", "mdape", "smape", "coverage"]'
  prefs: []
  type: TYPE_NORMAL
- en: cv_params = cross_validation(
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: _cv_metrics = performance_metrics(cv_params)
  prefs: []
  type: TYPE_NORMAL
- en: cv_metrics = {
  prefs: []
  type: TYPE_NORMAL
- en: 'n: _cv_metrics[n].mean() for n in cv_metrics_name}'
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: signature = infer_signature(train, predictions)
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.prophet.log_model(
  prefs: []
  type: TYPE_NORMAL
- en: model, artifact_path=ARTIFACT_DIR,
  prefs: []
  type: TYPE_NORMAL
- en: signature=signature, registered_model_name=model_name,)
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.log_params(param)
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.log_metrics(cv_metrics)
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.end_run()
  prefs: []
  type: TYPE_NORMAL
- en: return _vars
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'def forecast(_vars, **kwargs):'
  prefs: []
  type: TYPE_NORMAL
- en: '# Load the model from the Model Registry'
  prefs: []
  type: TYPE_NORMAL
- en: model_uri = f"models:/{model_name}/{model_version}"
  prefs: []
  type: TYPE_NORMAL
- en: _model = mlflow.prophet.load_model(model_uri)
  prefs: []
  type: TYPE_NORMAL
- en: forecast = _model.predict(
  prefs: []
  type: TYPE_NORMAL
- en: _model.make_future_dataframe(
  prefs: []
  type: TYPE_NORMAL
- en: periods=365, include_history = False))
  prefs: []
  type: TYPE_NORMAL
- en: sdf = spark.createDataFrame(forecast[
  prefs: []
  type: TYPE_NORMAL
- en: '[''ds'', ''yhat'', ''yhat_lower'', ''yhat_upper'']])'
  prefs: []
  type: TYPE_NORMAL
- en: sdf.write.format("delta").mode("overwrite").save(
  prefs: []
  type: TYPE_NORMAL
- en: f"/data/delta/ts-spark_ch9_gold_forecast_{_vars['runid']}")
  prefs: []
  type: TYPE_NORMAL
- en: print(f"forecast:\n${forecast.tail(30)}")
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.end_run()
  prefs: []
  type: TYPE_NORMAL
- en: return _vars
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'def eval_forecast(_vars, **kwargs):'
  prefs: []
  type: TYPE_NORMAL
- en: sdf = spark.read.format("delta").load(
  prefs: []
  type: TYPE_NORMAL
- en: f"/data/delta/ts-spark_ch9_silver_eval_{_vars['runid']}")
  prefs: []
  type: TYPE_NORMAL
- en: sdf_forecast = spark.read.format("delta").load(
  prefs: []
  type: TYPE_NORMAL
- en: f"/data/delta/ts-spark_ch9_gold_forecast_{_vars['runid']}")
  prefs: []
  type: TYPE_NORMAL
- en: sdf_eval = sdf.join(sdf_forecast, 'ds', "inner")
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: evaluator = RegressionEvaluator(
  prefs: []
  type: TYPE_NORMAL
- en: labelCol='y', predictionCol='yhat', metricName='rmse')
  prefs: []
  type: TYPE_NORMAL
- en: eval_rmse = evaluator.evaluate(sdf_eval)
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.set_experiment('ts-spark_ch9_data-ml-ops_time_series_prophet_eval')
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.start_run()
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.log_param("DAG_NAME", DAG_NAME)
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.log_param("EVAL_START_DATE", _vars['START_DATE'])
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.log_metric('eval_rmse', _vars['eval_rmse'])
  prefs: []
  type: TYPE_NORMAL
- en: mlflow.end_run()
  prefs: []
  type: TYPE_NORMAL
- en: return _vars
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: t1 >> t2 >> t3 >> t4 >> t5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: t1 >> t2 >> [t3, t4] >> t5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
