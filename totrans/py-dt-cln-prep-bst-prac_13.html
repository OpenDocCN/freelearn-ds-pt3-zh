<html><head></head><body>
		<div id="_idContainer135">
			<h1 id="_idParaDest-265" class="chapter-number"><a id="_idTextAnchor302"/><span class="koboSpan" id="kobo.1.1">13</span></h1>
			<h1 id="_idParaDest-266"><a id="_idTextAnchor303"/><span class="koboSpan" id="kobo.2.1">Image and Audio Preprocessing with LLMs</span></h1>
			<p><span class="koboSpan" id="kobo.3.1">In this chapter, we delve into the preprocessing of unstructured data, specifically focusing on images and audio. </span><span class="koboSpan" id="kobo.3.2">We explore various techniques and models designed to extract meaningful information from these types of media. </span><span class="koboSpan" id="kobo.3.3">The discussion includes a detailed examination of image preprocessing methods, the use of </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">optical character recognition</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">OCR</span></strong><span class="koboSpan" id="kobo.7.1">) for </span><a id="_idIndexMarker1078"/><span class="koboSpan" id="kobo.8.1">extracting text from images, the capabilities of the BLIP model for generating image captions, and the application of the Whisper model for converting audio </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">into text.</span></span></p>
			<p><span class="koboSpan" id="kobo.10.1">In this chapter, we’ll cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">following topics:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.12.1">The current era of </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">image preprocessing</span></span></li>
				<li><span class="koboSpan" id="kobo.14.1">Extracting text </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">from images</span></span></li>
				<li><span class="koboSpan" id="kobo.16.1">Handling </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">audio data</span></span></li>
			</ul>
			<h1 id="_idParaDest-267"><a id="_idTextAnchor304"/><span class="koboSpan" id="kobo.18.1">Technical requirements</span></h1>
			<p><span class="koboSpan" id="kobo.19.1">The complete code for this chapter can be found in the following GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13"><span class="No-Break"><span class="koboSpan" id="kobo.21.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.22.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.23.1">Let's install the necessary libraries we will use in </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">this chapter:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.25.1">
pip install torchvision
pip install keras==3.4.1
pip install tensorflow==2.17.0
pip install opencv-python==4.10.0.84
pip install opencv-python==4.10.0.84
pip install paddleocr==2.8.1
pip install paddlepaddle==2.6.1</span></pre>			<h1 id="_idParaDest-268"><a id="_idTextAnchor305"/><span class="koboSpan" id="kobo.26.1">The current era of image preprocessing</span></h1>
			<p><span class="koboSpan" id="kobo.27.1">In the</span><a id="_idIndexMarker1079"/><span class="koboSpan" id="kobo.28.1"> era of advanced visual models, such as diffusion models, and models such as OpenAI’s CLIP, preprocessing has become crucial to ensure the quality, consistency, and suitability of images for training and inference. </span><span class="koboSpan" id="kobo.28.2">These models require images to be in a format that maximizes their ability to learn intricate patterns and generate high-quality results. </span><span class="koboSpan" id="kobo.28.3">In this section, we will go through all the preprocessing steps to make your images ready for the </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">subsequent tasks.</span></span></p>
			<p><span class="koboSpan" id="kobo.30.1">Across this section, we will use a common use case, which is to prepare images for training a diffusion model. </span><span class="koboSpan" id="kobo.30.2">You can find the code for this exercise in the GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/1.image_prerpocessing.py"><span class="No-Break"><span class="koboSpan" id="kobo.32.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/1.image_prerpocessing.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.33.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.34.1">Let’s start by loading </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">some images.</span></span></p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor306"/><span class="koboSpan" id="kobo.36.1">Loading the images</span></h2>
			<p><span class="koboSpan" id="kobo.37.1">Perform </span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.38.1">the following steps to load </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">the images:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.40.1">First, we load the required packages for </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">this exercise:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.42.1">
from PIL import Image
import numpy as np
import cv2
import requests
from io import BytesIO
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.43.1">Then, we load the images into our environment. </span><span class="koboSpan" id="kobo.43.2">We’ll use the Python Pillow library to handle loading </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">the images.</span></span></p></li>				<li><span class="koboSpan" id="kobo.45.1">Next, we create</span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.46.1"> a function to load an image from a URL. </span><span class="koboSpan" id="kobo.46.2">This function fetches the image from the given URL and loads it into a </span><strong class="source-inline"><span class="koboSpan" id="kobo.47.1">PIL</span></strong><span class="koboSpan" id="kobo.48.1"> image object using </span><strong class="source-inline"><span class="koboSpan" id="kobo.49.1">BytesIO</span></strong><span class="koboSpan" id="kobo.50.1"> to handle the </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">byte data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.52.1">
def load_image_from_url(url):
    response = requests.get(url)
    img = Image.open(BytesIO(response.content))
    return img</span></pre></li>				<li><span class="koboSpan" id="kobo.53.1">Then, we’ll create a helper function to display our images. </span><span class="koboSpan" id="kobo.53.2">We will be using this function across </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">the chapter:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.55.1">
def show_image(image, title="Image"):
    plt.imshow(image)
    plt.title(title)
    plt.axis('off')
    plt.show()</span></pre></li>				<li><span class="koboSpan" id="kobo.56.1">Now, we’ll pass the image URL to our </span><strong class="source-inline"><span class="koboSpan" id="kobo.57.1">load_image_from_url</span></strong><span class="koboSpan" id="kobo.58.1"> function. </span><span class="koboSpan" id="kobo.58.2">Here, we are using a random image URL from Unsplash, but you can use any image you have </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">access to:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.60.1">
image_url = "https://images.unsplash.com/photo-1593642532871-8b12e02d091c"
image = load_image_from_url(image_url)</span></pre></li>				<li><span class="koboSpan" id="kobo.61.1">Let’s display the original image that we just loaded using the function </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">we created:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.63.1">
show_image(image, "Original Image")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.64.1">This will display the following </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">output image:</span></span></p></li>			</ol>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<span class="koboSpan" id="kobo.66.1"><img src="image/B19801_13_1.jpg" alt="Figure 13.1 – Original image before any preprocessing"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.67.1">Figure 13.1 – Original image before any preprocessing</span></p>
			<p><span class="koboSpan" id="kobo.68.1">Image preprocessing is </span><a id="_idIndexMarker1082"/><span class="koboSpan" id="kobo.69.1">crucial for preparing visual data for ingestion by </span><strong class="bold"><span class="koboSpan" id="kobo.70.1">machine learning</span></strong><span class="koboSpan" id="kobo.71.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.72.1">ML</span></strong><span class="koboSpan" id="kobo.73.1">) models. </span><span class="koboSpan" id="kobo.73.2">Let’s delve deeper into each technique, explaining the concepts and demonstrating their application with </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">Python code.</span></span></p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor307"/><span class="koboSpan" id="kobo.75.1">Resizing and cropping</span></h2>
			<p><span class="koboSpan" id="kobo.76.1">Effective</span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.77.1"> preprocessing can </span><a id="_idIndexMarker1084"/><span class="koboSpan" id="kobo.78.1">significantly enhance the performance of AI and ML models by ensuring that the most relevant features are highlighted and easily detectable in the images. </span><strong class="bold"><span class="koboSpan" id="kobo.79.1">Cropping</span></strong><span class="koboSpan" id="kobo.80.1"> is a </span><a id="_idIndexMarker1085"/><span class="koboSpan" id="kobo.81.1">technique that can help the model focus on relevant features. </span><span class="koboSpan" id="kobo.81.2">The main idea is to trim or cut away the outer edges of an image to improve framing, focus on the main subject, or eliminate unwanted elements. </span><span class="koboSpan" id="kobo.81.3">The size of the crop depends on the specific requirements of the task. </span><span class="koboSpan" id="kobo.81.4">For example, in object detection, the crop should focus on the object of interest, while in image classification, the crop should ensure that the main subject is centered and occupies most of </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">the frame.</span></span></p>
			<p><span class="koboSpan" id="kobo.83.1">There are many different techniques for cropping images from a simple fixed-size cropping to more involved object-aware cropping. </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">Fixed-size cropping</span></strong><span class="koboSpan" id="kobo.85.1"> involves adjusting</span><a id="_idIndexMarker1086"/><span class="koboSpan" id="kobo.86.1"> all images to a predetermined size, ensuring uniformity across the dataset, which is useful for </span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.87.1">applications that require standardized </span><a id="_idIndexMarker1088"/><span class="koboSpan" id="kobo.88.1">input sizes, such as training certain types of neural networks. </span><span class="koboSpan" id="kobo.88.2">However, it may result in the loss of important information if the main subject is not centered. </span><strong class="bold"><span class="koboSpan" id="kobo.89.1">Aspect ratio preservation</span></strong><span class="koboSpan" id="kobo.90.1"> avoids </span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.91.1">distortion by maintaining the original image’s aspect ratio while cropping, which is achieved through padding (adding borders to the image to reach the desired dimensions) or scaling (resizing the image while maintaining its aspect ratio, followed by cropping to the target size). </span><strong class="bold"><span class="koboSpan" id="kobo.92.1">Center cropping</span></strong><span class="koboSpan" id="kobo.93.1"> involves</span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.94.1"> cropping the image around its center, assuming the main subject is generally located in the middle, and is commonly used in image classification tasks where the main subject should occupy most of the frame. </span><strong class="bold"><span class="koboSpan" id="kobo.95.1">Object-aware cropping</span></strong><span class="koboSpan" id="kobo.96.1"> uses </span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.97.1">algorithms to detect the main subject within the image and crop around it, ensuring that the main subject is always emphasized, regardless of its position within the original image. </span><span class="koboSpan" id="kobo.97.2">This technique is particularly useful in object detection and </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">recognition tasks.</span></span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.99.1">Resizing</span></strong><span class="koboSpan" id="kobo.100.1"> is a </span><a id="_idIndexMarker1092"/><span class="koboSpan" id="kobo.101.1">fundamental step in image preprocessing for AI and ML tasks, focusing on adjusting the dimensions of an image to a standard size required by the model. </span><span class="koboSpan" id="kobo.101.2">This process is crucial for ensuring that the input data is consistent and suitable for the specific requirements of various AI and </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">ML algorithms.</span></span></p>
			<p><span class="koboSpan" id="kobo.103.1">Let’s add some steps to the image preprocessing pipeline we started in the previous section to see the effects of cropping and resizing. </span><span class="koboSpan" id="kobo.103.2">The following function resizes the image to a specified target size (256x256 pixels in this case). </span><span class="koboSpan" id="kobo.103.3">We expect the image to appear uniformly sized down to fit within the </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">target dimensions:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.105.1">
def resize_and_crop(image, target_size):
    image = image.resize((target_size, target_size),
    Image.LANCZOS)
    return image
target_size = 256
processed_image = resize_and_crop(image, target_size)</span></pre>			<p><span class="koboSpan" id="kobo.106.1">Let’s print the resulting image using the </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">following code:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.108.1">
show_image(processed_image, "Resized and Cropped Image")</span></pre>			<p><span class="koboSpan" id="kobo.109.1">This will display the </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">following output:</span></span></p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<span class="koboSpan" id="kobo.111.1"><img src="image/B19801_13_2.jpg" alt="Figure 13.2 – Image after resizing and cropping"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.112.1">Figure 13.2 – Image after resizing and cropping</span></p>
			<p><span class="koboSpan" id="kobo.113.1">As we</span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.114.1"> can see from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.115.1">Figure 13</span></em></span><em class="italic"><span class="koboSpan" id="kobo.116.1">.2</span></em><span class="koboSpan" id="kobo.117.1">, the image is resized to</span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.118.1"> a square of 256x256 pixels, altering the aspect ratio of the original image that was not square. </span><span class="koboSpan" id="kobo.118.2">Thus, resizing ensures a uniform input size for all data, which facilitates the batch processing and the passing of data to models </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">for training.</span></span></p>
			<p><span class="koboSpan" id="kobo.120.1">Next, we will discuss the normalization of images, which is not far from the normalization of features discussed in </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">previous chapters.</span></span></p>
			<h2 id="_idParaDest-271"><a id="_idTextAnchor308"/><span class="koboSpan" id="kobo.122.1">Normalizing and standardizing the dataset</span></h2>
			<p><span class="koboSpan" id="kobo.123.1">To ensure data</span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.124.1"> consistency and help the training of models</span><a id="_idIndexMarker1096"/><span class="koboSpan" id="kobo.125.1"> converge faster, we can force the input data to a common range of values. </span><span class="koboSpan" id="kobo.125.2">This adjustment involves scaling the input data between </span><strong class="source-inline"><span class="koboSpan" id="kobo.126.1">0</span></strong><span class="koboSpan" id="kobo.127.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.128.1">1</span></strong><span class="koboSpan" id="kobo.129.1">, also known as </span><strong class="bold"><span class="koboSpan" id="kobo.130.1">standardization</span></strong><span class="koboSpan" id="kobo.131.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.132.1">normalizing</span></strong><span class="koboSpan" id="kobo.133.1"> using the mean and</span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.134.1"> standard deviation of </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">the dataset.</span></span></p>
			<p><span class="koboSpan" id="kobo.136.1">For most deep</span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.137.1"> learning models, forcing pixel values to the range </span><strong class="source-inline"><span class="koboSpan" id="kobo.138.1">[0, 1]</span></strong><span class="koboSpan" id="kobo.139.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.140.1">[-1, 1]</span></strong><span class="koboSpan" id="kobo.141.1"> is standard practice. </span><span class="koboSpan" id="kobo.141.2">This can be achieved by dividing pixel values by 255 (for </span><strong class="source-inline"><span class="koboSpan" id="kobo.142.1">[0, 1]</span></strong><span class="koboSpan" id="kobo.143.1">) or by subtracting the mean and dividing by the standard deviation (for </span><strong class="source-inline"><span class="koboSpan" id="kobo.144.1">[-1, 1]</span></strong><span class="koboSpan" id="kobo.145.1">). </span><span class="koboSpan" id="kobo.145.2">In image classification tasks, this tactic ensures that the input images have consistent pixel values. </span><span class="koboSpan" id="kobo.145.3">For example, in a dataset of handwritten digits (such as MNIST), normalizing or standardizing the pixel values helps the </span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.146.1">model learn the patterns of the digits more effectively. </span><span class="koboSpan" id="kobo.146.2">In object detection tasks, it helps in accurately detecting and classifying objects within an image. </span><span class="koboSpan" id="kobo.146.3">However, normalization and standardization are not limited to image preprocessing; they are a fundamental step in preparing data for any </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">ML problem.</span></span></p>
			<p><span class="koboSpan" id="kobo.148.1">Let’s expand the</span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.149.1"> previous example by adding the normalization and the standardization step. </span><span class="koboSpan" id="kobo.149.2">The first function performs the normalization to ensure that the pixel values are in a common scale, in this case, between the range </span><strong class="source-inline"><span class="koboSpan" id="kobo.150.1">[0, 1]</span></strong><span class="koboSpan" id="kobo.151.1"> and we do that by dividing </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">by 255:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.153.1">
def normalize(image):
    image_array = np.array(image)
    normalized_array = image_array / 255.0
    return normalized_array
normalized_image = normalize(processed_image)</span></pre>			<p><span class="koboSpan" id="kobo.154.1">The normalized image can be seen in the </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">following figure:</span></span></p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<span class="koboSpan" id="kobo.156.1"><img src="image/B19801_13_3.jpg" alt="Figure 13.3 – Picture after normalization"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.157.1">Figure 13.3 – Picture after normalization</span></p>
			<p><span class="koboSpan" id="kobo.158.1">As we </span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.159.1">can see from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.160.1">Figure 13</span></em></span><em class="italic"><span class="koboSpan" id="kobo.161.1">.3</span></em><span class="koboSpan" id="kobo.162.1">, visually, the image remains the same, at least to the human eye. </span><span class="koboSpan" id="kobo.162.2">Normalization does not alter the relative intensity of the pixels; it only scales them to a different range so the content and details </span><a id="_idIndexMarker1102"/><span class="koboSpan" id="kobo.163.1">should </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">remain unchanged.</span></span></p>
			<p><span class="koboSpan" id="kobo.165.1">Let’s move on to the standardization exercise. </span><span class="koboSpan" id="kobo.165.2">Before standardization, pixel values are in the range </span><strong class="source-inline"><span class="koboSpan" id="kobo.166.1">[0, 255]</span></strong><span class="koboSpan" id="kobo.167.1"> and follow the natural distribution of image intensities. </span><span class="koboSpan" id="kobo.167.2">The idea with standardization is that all the pixel values will be transformed to have a mean of </span><strong class="source-inline"><span class="koboSpan" id="kobo.168.1">0</span></strong><span class="koboSpan" id="kobo.169.1"> and a standard deviation of </span><strong class="source-inline"><span class="koboSpan" id="kobo.170.1">1</span></strong><span class="koboSpan" id="kobo.171.1">. </span><span class="koboSpan" id="kobo.171.2">Let’s see how we can do that in the </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">following code:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.173.1">
def standardize(image):
    image_array = np.array(image)
    mean = np.mean(image_array, axis=(0, 1), keepdims=True)
    std = np.std(image_array, axis=(0, 1), keepdims=True)
    standardized_array = (image_array - mean) / std
    return standardized_array
standardized_image = standardize(processed_image)</span></pre>			<p><span class="koboSpan" id="kobo.174.1">In this case, the appearance of the image might change since standardization shifts the mean to </span><strong class="source-inline"><span class="koboSpan" id="kobo.175.1">0</span></strong><span class="koboSpan" id="kobo.176.1"> and scales the values. </span><span class="koboSpan" id="kobo.176.2">This can make the image look different, possibly more contrasted, or with</span><a id="_idIndexMarker1103"/><span class="koboSpan" id="kobo.177.1"> changed brightness. </span><span class="koboSpan" id="kobo.177.2">However, the</span><a id="_idIndexMarker1104"/><span class="koboSpan" id="kobo.178.1"> image content should still </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">be recognizable.</span></span></p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<span class="koboSpan" id="kobo.180.1"><img src="image/B19801_13_4.jpg" alt="Figure 13.4 – Picture after standardization"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.181.1">Figure 13.4 – Picture after standardization</span></p>
			<p><span class="koboSpan" id="kobo.182.1">Apart from the transformed image shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.183.1">Figure 13</span></em></span><em class="italic"><span class="koboSpan" id="kobo.184.1">.4</span></em><span class="koboSpan" id="kobo.185.1">, the mean and standard deviation for the values are </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">printed here:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.187.1">
Mean after standardization: 0.0
Standard deviation after standardization: 1.000000000000416</span></pre>			<p><span class="koboSpan" id="kobo.188.1">This confirms that the standardization has correctly scaled the pixel values. </span><span class="koboSpan" id="kobo.188.2">Let’s now move on to the data </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">augmentation part.</span></span></p>
			<h2 id="_idParaDest-272"><a id="_idTextAnchor309"/><span class="koboSpan" id="kobo.190.1">Data augmentation</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.191.1">Data augmentation</span></strong><span class="koboSpan" id="kobo.192.1"> aims</span><a id="_idIndexMarker1105"/><span class="koboSpan" id="kobo.193.1"> to create more variability in</span><a id="_idIndexMarker1106"/><span class="koboSpan" id="kobo.194.1"> the dataset by applying random transformations, such as rotation, flipping, translation, color jittering, and contrast adjustment. </span><span class="koboSpan" id="kobo.194.2">This artificially expands the dataset with modified versions of existing images, which helps with model generalization and performance, especially when working with </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">limited data.</span></span></p>
			<p><span class="koboSpan" id="kobo.196.1">Common</span><a id="_idIndexMarker1107"/><span class="koboSpan" id="kobo.197.1"> augmentation techniques include geometric transformations, such as rotation, flipping, and scaling, which change the spatial orientation and size of the images. </span><span class="koboSpan" id="kobo.197.2">For example, rotating an image by 15 degrees or flipping it horizontally can create new perspectives for the model to learn from. </span><span class="koboSpan" id="kobo.197.3">Color space alterations, such as adjusting brightness, contrast, or hue, can simulate different lighting conditions and improve the model’s ability to recognize objects in varying environments. </span><span class="koboSpan" id="kobo.197.4">Adding noise or blur can help the model become more resilient to imperfections and distortions in </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">real-world data.</span></span></p>
			<p><span class="koboSpan" id="kobo.199.1">Let’s go back to </span><a id="_idIndexMarker1108"/><span class="koboSpan" id="kobo.200.1">our example to see how we can create </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">image variations:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.202.1">First, we will define the transformations that we will apply to </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">the image:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.204.1">Rotation range</span></strong><span class="koboSpan" id="kobo.205.1">: Randomly rotate the image within a range of </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">40 degrees.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.207.1">Width shift range</span></strong><span class="koboSpan" id="kobo.208.1">: Randomly shift the image horizontally by 20% of </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">the width.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.210.1">Height shift range</span></strong><span class="koboSpan" id="kobo.211.1">: Randomly shift the image vertically by 20% of </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">the height.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.213.1">Shear range</span></strong><span class="koboSpan" id="kobo.214.1">: Randomly apply </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">shearing transformations.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.216.1">Zoom range</span></strong><span class="koboSpan" id="kobo.217.1">: Randomly zoom in or out </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">by 20%.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.219.1">Horizontal flip</span></strong><span class="koboSpan" id="kobo.220.1">: Randomly flip the </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">image horizontally.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.222.1">Fill mode</span></strong><span class="koboSpan" id="kobo.223.1">: Define how to fill in newly created pixels after a transformation. </span><span class="koboSpan" id="kobo.223.2">(Here, using “nearest” </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">pixel values.)</span></span></li></ul></li>
				<li><span class="koboSpan" id="kobo.225.1">Let’s create a function to apply </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">these transformations:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.227.1">
datagen = ImageDataGenerator(
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)</span></pre></li>				<li><span class="koboSpan" id="kobo.228.1">Then, we</span><a id="_idIndexMarker1109"/><span class="koboSpan" id="kobo.229.1"> will apply the transformations we just defined to </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">the image:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.231.1">
def augment_image(image):
    image = np.expand_dims(image, axis=0) # Add batch dimension
    augmented_iter = datagen.flow(image, batch_size=1)
    augmented_image = next(augmented_iter)[0]
    return augmented_image
augmented_image = augment_image(normalized_image)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.232.1">Let’s print the </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">augmented image:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.234.1">show_image(augmented_image, "Augmented Image")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.235.1">This will display the </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">following image:</span></span></p></li>			</ol>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<span class="koboSpan" id="kobo.237.1"><img src="image/B19801_13_5.jpg" alt="Figure 13.5 – Picture augmentation"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.238.1">Figure 13.5 – Picture augmentation</span></p>
			<p><span class="koboSpan" id="kobo.239.1">As we can see from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.240.1">Figure 13</span></em></span><em class="italic"><span class="koboSpan" id="kobo.241.1">.5</span></em><span class="koboSpan" id="kobo.242.1">, the image has some significant changes; however, the image still</span><a id="_idIndexMarker1110"/><span class="koboSpan" id="kobo.243.1"> remains recognizable and the concept in the picture remains </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">the same.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.245.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.246.1">As we are using some random parameters in the data augmentation phase, you may produce a slightly different image at </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">this stage.</span></span></p>
			<p><span class="koboSpan" id="kobo.248.1">Data augmentation’s importance lies in its ability to increase dataset diversity, which by extension helps prevent overfitting, as the model learns to recognize patterns and features from a wider range of examples rather than memorizing the training data. </span><span class="koboSpan" id="kobo.248.2">Let’s move on to the next part and dive deep into the noise </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">reduction options.</span></span></p>
			<h2 id="_idParaDest-273"><a id="_idTextAnchor310"/><span class="koboSpan" id="kobo.250.1">Noise reduction</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.251.1">Noise</span></strong><span class="koboSpan" id="kobo.252.1"> in </span><a id="_idIndexMarker1111"/><span class="koboSpan" id="kobo.253.1">images refers to the random variations in pixel </span><a id="_idIndexMarker1112"/><span class="koboSpan" id="kobo.254.1">values that can distort the visual quality of an image and by extension affect the performance of models during training. </span><span class="koboSpan" id="kobo.254.2">These variations often appear as tiny, irregular spots or textures, such as random dots, patches, or a gritty texture, disrupting the smoothness and clarity of the image. </span><span class="koboSpan" id="kobo.254.3">They often make the image look less sharp and can obscure important details, which can be problematic for both visual interpretation and for models that rely on clear, accurate data </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">for training.</span></span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.256.1">Noise reduction</span></strong><span class="koboSpan" id="kobo.257.1"> attempts </span><a id="_idIndexMarker1113"/><span class="koboSpan" id="kobo.258.1">to reduce the random variations and make the data simpler. </span><span class="koboSpan" id="kobo.258.2">The minimization of these random variations in pixel values helps improve image quality and model accuracy as they can mislead models during training. </span><span class="koboSpan" id="kobo.258.3">In the following subsections, we expand on some common denoising techniques used in the data field, including Gaussian smoothing, non-local means denoising, and </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">wavelet denoising.</span></span></p>
			<h3><span class="koboSpan" id="kobo.260.1">Gaussian smoothing</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.261.1">Gaussian blur</span></strong><span class="koboSpan" id="kobo.262.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.263.1">Gaussian smoothing</span></strong><span class="koboSpan" id="kobo.264.1">) applies a Gaussian filter to the image, which </span><a id="_idIndexMarker1114"/><span class="koboSpan" id="kobo.265.1">works by taking the pixel values within a </span><a id="_idIndexMarker1115"/><span class="koboSpan" id="kobo.266.1">specified neighborhood around each pixel and averaging them. </span><span class="koboSpan" id="kobo.266.2">The filter assigns higher weights to the pixels closer to the center of the neighborhood and lower weights to those farther away, following the Gaussian distribution. </span><span class="koboSpan" id="kobo.266.3">The denoised image will appear smoother but with slightly blurred edges, making it useful in applications where slight blurring is acceptable or desired, such as artistic effects or before edge detection algorithms to reduce noise. </span><span class="koboSpan" id="kobo.266.4">Let’s see the code for applying Gaussian smoothing to </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">the image:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.268.1">
def gaussian_blur(image):
    blurred_image = cv2.GaussianBlur(image, (5, 5), 0)
    return blurred_image</span></pre>			<p><span class="koboSpan" id="kobo.269.1">Let’s display the </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">denoised image:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.271.1">
blurred_image = gaussian_blur(noisy_image)
show_image(blurred_image, "Gaussian Blur")</span></pre>			<p><span class="koboSpan" id="kobo.272.1">The denoised image can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.273.1">Figure 13</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.274.1">.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">:</span></span></p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<span class="koboSpan" id="kobo.276.1"><img src="image/B19801_13_6.jpg" alt="Figure 13.6 – Denoised images – Gaussian blur on the median blur on the right"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.277.1">Figure 13.6 – Denoised images – Gaussian blur on the median blur on the right</span></p>
			<p><span class="koboSpan" id="kobo.278.1">In the next section, we will discuss the </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">bilateral filter.</span></span></p>
			<h3><span class="koboSpan" id="kobo.280.1">The bilateral filter</span></h3>
			<p><span class="koboSpan" id="kobo.281.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.282.1">bilateral filter</span></strong><span class="koboSpan" id="kobo.283.1"> smoothens</span><a id="_idIndexMarker1116"/><span class="koboSpan" id="kobo.284.1"> images by considering </span><a id="_idIndexMarker1117"/><span class="koboSpan" id="kobo.285.1">both spatial and intensity differences. </span><span class="koboSpan" id="kobo.285.2">It averages the pixel values based on their spatial closeness and color similarity. </span><span class="koboSpan" id="kobo.285.3">Let’s have a look at </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">the code:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.287.1">
def bilateral_filter(image):
    image_uint8 = (image * 255).astype(np.uint8)
    filtered_image = cv2.bilateralFilter(
        image_uint8, </span><strong class="bold"><span class="koboSpan" id="kobo.288.1">9, 75, 75</span></strong><span class="koboSpan" id="kobo.289.1">)
    filtered_image = filtered_image / 255.0
    return filtered_image</span></pre>			<p><span class="koboSpan" id="kobo.290.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.291.1">bilateralFilter</span></strong><span class="koboSpan" id="kobo.292.1"> function takes some arguments that we need </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">to explain:</span></span></p>
			<ul>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.294.1">9</span></strong><span class="koboSpan" id="kobo.295.1">: This is the diameter of each pixel neighborhood used during filtering. </span><span class="koboSpan" id="kobo.295.2">A larger value means that more pixels will be considered in the computation, resulting in a stronger </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">smoothing effect.</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">75</span></strong><span class="koboSpan" id="kobo.298.1">: This is the filter sigma in the color space. </span><span class="koboSpan" id="kobo.298.2">A larger value means that farther colors within the pixel neighborhood will be mixed, resulting in larger areas of </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">semi-equal color.</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">75</span></strong><span class="koboSpan" id="kobo.301.1">: This is the</span><a id="_idIndexMarker1118"/><span class="koboSpan" id="kobo.302.1"> filter sigma in the coordinate space. </span><span class="koboSpan" id="kobo.302.2">A larger value means farther pixels will influence each other if their colors are close enough. </span><span class="koboSpan" id="kobo.302.3">It controls the amount </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">of smoothing.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.304.1">Let’s use the </span><a id="_idIndexMarker1119"/><span class="koboSpan" id="kobo.305.1">function and see the </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">resulting output:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.307.1">
bilateral_filtered_image = bilateral_filter(noisy_image)
show_image(bilateral_filtered_image, "Bilateral Filter")</span></pre>			<p><span class="koboSpan" id="kobo.308.1">The denoised image can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.309.1">Figure 13</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.310.1">.7</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">.</span></span></p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<span class="koboSpan" id="kobo.312.1"><img src="image/B19801_13_7_Merged.jpg" alt="Figure 13.7 – Denoised images – left bilateral filter, right non-local mean denoising"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.313.1">Figure 13.7 – Denoised images – left bilateral filter, right non-local mean denoising</span></p>
			<p><span class="koboSpan" id="kobo.314.1">In the next section, we will discuss the non-local </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">means denoising.</span></span></p>
			<h3><span class="koboSpan" id="kobo.316.1">Non-local means denoising</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.317.1">Non-local means denoising</span></strong><span class="koboSpan" id="kobo.318.1"> reduces noise by comparing patches of the image and averaging </span><a id="_idIndexMarker1120"/><span class="koboSpan" id="kobo.319.1">similar patches, even if they </span><a id="_idIndexMarker1121"/><span class="koboSpan" id="kobo.320.1">are far apart. </span><span class="koboSpan" id="kobo.320.2">This method works by comparing small patches of pixels across the entire image, rather than just neighboring pixels. </span><span class="koboSpan" id="kobo.320.3">Unlike simpler methods that only consider nearby pixels, non-local means denoising searches the image for patches that are similar, even if they are located far apart. </span><span class="koboSpan" id="kobo.320.4">When a match is found, the method averages these similar patches together to determine the final </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">pixel value.</span></span></p>
			<p><span class="koboSpan" id="kobo.322.1">This approach</span><a id="_idIndexMarker1122"/><span class="koboSpan" id="kobo.323.1"> is particularly effective at preserving fine details and textures because it can recognize and retain patterns that are </span><a id="_idIndexMarker1123"/><span class="koboSpan" id="kobo.324.1">consistent throughout the image, rather than just smoothing over everything indiscriminately. </span><span class="koboSpan" id="kobo.324.2">By averaging only the patches that are truly similar, it reduces noise while maintaining the integrity of important image features, making it an excellent choice for applications where detail preservation </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">is crucial.</span></span></p>
			<p><span class="koboSpan" id="kobo.326.1">Let’s have a look at </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">the code:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.328.1">
def remove_noise(image):
    image_uint8 = (image * 255).astype(np.uint8)
    denoised_image = cv2.fastNlMeansDenoisingColored(
        image_uint8, None, </span><strong class="bold"><span class="koboSpan" id="kobo.329.1">h=10, templateWindowSize=7,</span></strong><span class="koboSpan" id="kobo.330.1">
        </span><strong class="bold"><span class="koboSpan" id="kobo.331.1">searchWindowSize=21)</span></strong><span class="koboSpan" id="kobo.332.1">
    denoised_image = denoised_image / 255.0
    return denoised_image
denoised_image = remove_noise(noisy_image)
show_image(denoised_image, "Non-Local Means Denoising")</span></pre>			<p><span class="koboSpan" id="kobo.333.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.334.1">fastNlMeansDenoisingColored</span></strong><span class="koboSpan" id="kobo.335.1"> function applies the non-local means denoising algorithm to the image. </span><span class="koboSpan" id="kobo.335.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.336.1">h=10</span></strong><span class="koboSpan" id="kobo.337.1"> argument reflects the filtering strength. </span><span class="koboSpan" id="kobo.337.2">A higher value removes more noise but may also remove some image details. </span><span class="koboSpan" id="kobo.337.3">The size in pixels of the template patch used to compute weights is reflected in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.338.1">templateWindowSize</span></strong><span class="koboSpan" id="kobo.339.1"> variable. </span><span class="koboSpan" id="kobo.339.2">This value should be an odd number. </span><span class="koboSpan" id="kobo.339.3">A greater value means more smoothing. </span><span class="koboSpan" id="kobo.339.4">Finally, </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">searchWindowSize</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.341.1">=21</span></strong><span class="koboSpan" id="kobo.342.1"> means the size of the window in pixels used to compute a weighted average for a given pixel should be odd. </span><span class="koboSpan" id="kobo.342.2">A greater value means </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">more smoothing.</span></span></p>
			<p><span class="koboSpan" id="kobo.344.1">Why use an odd number for window sizes, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.345.1">templateWindowSize</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.346.1">and</span></span><span class="No-Break"><em class="italic"> </em></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.347.1">searchWindowSize</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">?</span></span></p>
			<p><span class="koboSpan" id="kobo.349.1">The primary reason for using an odd number is to ensure that there is a clear center pixel within the window. </span><span class="koboSpan" id="kobo.349.2">For example, in a 3x3 window (where 3 is an odd number), the center pixel is the one at position “(2,2)”. </span><span class="koboSpan" id="kobo.349.3">This center pixel is crucial because the algorithm often </span><a id="_idIndexMarker1124"/><span class="koboSpan" id="kobo.350.1">calculates how similar the surrounding pixels are in comparison to this central pixel. </span><span class="koboSpan" id="kobo.350.2">If an even-sized window were used, there would be no single, central pixel, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.351.1">Figure 13</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.352.1">.8</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">.</span></span></p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<span class="koboSpan" id="kobo.354.1"><img src="image/B19801_13_8.jpg" alt="Figure 13.8 – Use an odd number for window sizes"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.355.1">Figure 13.8 – Use an odd number for window sizes</span></p>
			<p><span class="koboSpan" id="kobo.356.1">Using an </span><a id="_idIndexMarker1125"/><span class="koboSpan" id="kobo.357.1">odd number simplifies the computation of weights and distances between the central pixel and its neighbors. </span><span class="koboSpan" id="kobo.357.2">This simplification is essential in algorithms such as non-local means, where the distances between pixels influence the weight given to each pixel in the averaging process. </span><span class="koboSpan" id="kobo.357.3">An odd-sized window naturally allows for straightforward indexing and less </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">complex calculations.</span></span></p>
			<p><span class="koboSpan" id="kobo.359.1">Regarding the </span><strong class="source-inline"><span class="koboSpan" id="kobo.360.1">searchWindowSize</span></strong><span class="koboSpan" id="kobo.361.1"> parameter, this defines the area within which the algorithm looks for similar patches to the one currently being processed. </span><span class="koboSpan" id="kobo.361.2">Having an odd-sized window for this search area ensures that there is a central pixel around which the search is centered. </span><span class="koboSpan" id="kobo.361.3">This helps in accurately identifying similar patches and applying the denoising effect uniformly across </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">the image.</span></span></p>
			<p><span class="koboSpan" id="kobo.363.1">The denoised image can be seen in the previous section in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.364.1">Figure 13</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.365.1">.7</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.367.1">In the next section, we will discuss the last method, the </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">median blur.</span></span></p>
			<h3><span class="koboSpan" id="kobo.369.1">Median blur</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.370.1">Median blur</span></strong><span class="koboSpan" id="kobo.371.1"> replaces</span><a id="_idIndexMarker1126"/><span class="koboSpan" id="kobo.372.1"> each pixel’s value with the median</span><a id="_idIndexMarker1127"/><span class="koboSpan" id="kobo.373.1"> value of the neighboring pixels. </span><span class="koboSpan" id="kobo.373.2">This method is particularly effective for removing “salt-and-pepper” noise, where pixels are randomly set to black or white, as we will see later. </span><span class="koboSpan" id="kobo.373.3">Let’s first denoise the image with the median blur method and then we will see how this method solves the </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">salt-and-pepper effect.</span></span></p>
			<p><span class="koboSpan" id="kobo.375.1">The following function performs the </span><strong class="bold"><span class="koboSpan" id="kobo.376.1">median blur</span></strong><span class="koboSpan" id="kobo.377.1"> technique. </span><span class="koboSpan" id="kobo.377.2">It calls the </span><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">medianBlur</span></strong><span class="koboSpan" id="kobo.379.1"> function, which requires the input image to be in an 8-bit unsigned integer format (</span><strong class="source-inline"><span class="koboSpan" id="kobo.380.1">uint8</span></strong><span class="koboSpan" id="kobo.381.1">), where pixel values range from </span><strong class="source-inline"><span class="koboSpan" id="kobo.382.1">0</span></strong><span class="koboSpan" id="kobo.383.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.384.1">255</span></strong><span class="koboSpan" id="kobo.385.1">. </span><span class="koboSpan" id="kobo.385.2">By multiplying the image by 255, the pixel values are scaled to the range </span><strong class="source-inline"><span class="koboSpan" id="kobo.386.1">[</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.387.1">0, 255]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.389.1">
def perform_median_blur(image):
    image_uint8 = (image * 255).astype(np.uint8)
    #The parameter below specifies the size of the kernel (5x5).
</span><span class="koboSpan" id="kobo.389.2">    blurred_image = cv2.medianBlur(image_uint8, </span><strong class="bold"><span class="koboSpan" id="kobo.390.1">5</span></strong><span class="koboSpan" id="kobo.391.1">)
    blurred_image = blurred_image / 255.0
    return blurred_image</span></pre>			<p><span class="koboSpan" id="kobo.392.1">Let’s display the denoised image using the </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">following code:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.394.1">
median_blurred_image = median_blur(noisy_image)
show_image(median_blurred_image, "Median Blur")</span></pre>			<p><span class="koboSpan" id="kobo.395.1">The denoised image can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.396.1">Figure 13</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.397.1">.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">:</span></span></p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<span class="koboSpan" id="kobo.399.1"><img src="image/B19801_13_9.jpg" alt="Figure 13.9 – Denoised images – median blur"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.400.1">Figure 13.9 – Denoised images – median blur</span></p>
			<p><span class="koboSpan" id="kobo.401.1">As promised, let’s now discuss the salt-and-pepper </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">noise effect.</span></span></p>
			<h4><span class="koboSpan" id="kobo.403.1">Salt-and-pepper noise</span></h4>
			<p><strong class="bold"><span class="koboSpan" id="kobo.404.1">Salt-and-pepper noise</span></strong><span class="koboSpan" id="kobo.405.1"> is a type </span><a id="_idIndexMarker1128"/><span class="koboSpan" id="kobo.406.1">of impulse </span><a id="_idIndexMarker1129"/><span class="koboSpan" id="kobo.407.1">noise characterized by the presence of randomly distributed black-and-white pixels in an image. </span><span class="koboSpan" id="kobo.407.2">This noise can be caused by various factors, such as errors in data transmission, malfunctioning camera sensors, or environmental conditions during image acquisition. </span><span class="koboSpan" id="kobo.407.3">The black pixels are referred to as “pepper noise,” while the white pixels are known as “salt noise.” </span><span class="koboSpan" id="kobo.407.4">This noise type is particularly detrimental to image quality as it can obscure important details and make edge detection and image </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">restoration challenging.</span></span></p>
			<p><span class="koboSpan" id="kobo.409.1">To showcase this, we have created a function that adds this noise effect to the original image so that we can then </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">denoise it:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.411.1">
def add_salt_and_pepper_noise(image, salt_prob=0.02, pepper_prob=0.02):
    noisy_image = np.copy(image)
    num_salt = np.ceil(salt_prob * image.size)
    coords = [np.random.randint(0, i - 1, int(num_salt)) for i in image.shape]
     noisy_image[coords[0], coords[1], :] = 1
     num_pepper = np.ceil(pepper_prob * image.size)
    coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in image.shape]
     noisy_image[coords[0], coords[1], :] = 0
    return noisy_image</span></pre>			<p><span class="koboSpan" id="kobo.412.1">This function takes </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">three arguments:</span></span></p>
			<ul>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">image</span></strong><span class="koboSpan" id="kobo.415.1">: The input image to which noise will </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">be added</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">salt_prob</span></strong><span class="koboSpan" id="kobo.418.1">: The probability of a pixel being turned into salt </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">noise (white)</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.420.1">pepper_prob</span></strong><span class="koboSpan" id="kobo.421.1">: The probability of a pixel being turned into pepper </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">noise (black)</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.423.1">This function</span><a id="_idIndexMarker1130"/><span class="koboSpan" id="kobo.424.1"> adds salt-and-pepper noise to an image. </span><span class="koboSpan" id="kobo.424.2">It </span><a id="_idIndexMarker1131"/><span class="koboSpan" id="kobo.425.1">starts by creating a copy of the input image to avoid altering the original. </span><span class="koboSpan" id="kobo.425.2">To introduce salt noise (white pixels), it calculates the number of pixels to be affected based on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.426.1">salt_prob</span></strong><span class="koboSpan" id="kobo.427.1"> parameter, generates random coordinates for these pixels, and sets them to white. </span><span class="koboSpan" id="kobo.427.2">Similarly, for pepper noise (black pixels), it calculates the number of affected pixels using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.428.1">pepper_prob</span></strong><span class="koboSpan" id="kobo.429.1"> parameter, generates random coordinates, and sets these pixels to black. </span><span class="koboSpan" id="kobo.429.2">The noisy image is </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">then returned.</span></span></p>
			<p><span class="koboSpan" id="kobo.431.1">To apply this effect on the data you need to set the following flag to </span><strong class="source-inline"><span class="koboSpan" id="kobo.432.1">True</span></strong><span class="koboSpan" id="kobo.433.1">. </span><span class="koboSpan" id="kobo.433.2">The flag can be found in the code after the </span><strong class="source-inline"><span class="koboSpan" id="kobo.434.1">add_salt_and_pepper_noise</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.435.1">function definition:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.436.1">
use_salt_and_pepper_noise = </span><strong class="bold"><span class="koboSpan" id="kobo.437.1">True</span></strong><span class="koboSpan" id="kobo.438.1">
if use_salt_and_pepper_noise:
noisy_image = add_salt_and_pepper_noise(tensor_to_image(tensor_image))
show_image(noisy_image, "Salt-and-Pepper Noisy Image")</span></pre>			<p><span class="koboSpan" id="kobo.439.1">The image with the noise can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.440.1">Figure 13</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.441.1">.10</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">:</span></span></p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<span class="koboSpan" id="kobo.443.1"><img src="image/B19801_13_10.jpg" alt="Figure 13.10 – Salt-and-pepper noise"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.444.1">Figure 13.10 – Salt-and-pepper noise</span></p>
			<p><span class="koboSpan" id="kobo.445.1">Now, let’s apply the </span><a id="_idIndexMarker1132"/><span class="koboSpan" id="kobo.446.1">different </span><a id="_idIndexMarker1133"/><span class="koboSpan" id="kobo.447.1">denoising techniques we’ve learned so far to the preceding image. </span><span class="koboSpan" id="kobo.447.2">The different denoising effects can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.448.1">Figure 13</span></em></span><em class="italic"><span class="koboSpan" id="kobo.449.1">.11</span></em><span class="koboSpan" id="kobo.450.1"> and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.451.1">Figure 13</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.452.1">.12</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">.</span></span></p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<span class="koboSpan" id="kobo.454.1"><img src="image/B19801_13_11_Merged.jpg" alt="Figure 13.11 – Left: Gaussian blur, right: median blur"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.455.1">Figure 13.11 – Left: Gaussian blur, right: median blur</span></p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<span class="koboSpan" id="kobo.456.1"><img src="image/B19801_13_12_Merged.jpg" alt="Figure 13.12 – Left: bilateral filter, right: non-local means denoising"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.457.1">Figure 13.12 – Left: bilateral filter, right: non-local means denoising</span></p>
			<p><span class="koboSpan" id="kobo.458.1">As we can</span><a id="_idIndexMarker1134"/><span class="koboSpan" id="kobo.459.1"> see, the </span><a id="_idIndexMarker1135"/><span class="koboSpan" id="kobo.460.1">median blur method really excels at removing this kind of noise, whereas all the other methods really struggle to remove it. </span><span class="koboSpan" id="kobo.460.2">In the next part of the chapter, we will discuss some image use cases that are becoming more popular in the data world, such as creating image captions and extracting text </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">from images.</span></span></p>
			<h1 id="_idParaDest-274"><a id="_idTextAnchor311"/><span class="koboSpan" id="kobo.462.1">Extracting text from images</span></h1>
			<p><span class="koboSpan" id="kobo.463.1">When </span><a id="_idIndexMarker1136"/><span class="koboSpan" id="kobo.464.1">discussing ways to extract text from images, the OCR technology is the one that comes to mind. </span><span class="koboSpan" id="kobo.464.2">The OCR technology allows us to handle textual information embedded in images, allowing for the digitization of printed documents, automating data entry, and </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">enhancing accessibility.</span></span></p>
			<p><span class="koboSpan" id="kobo.466.1">One of the primary benefits of OCR technology today is its ability to significantly reduce the need for manual data entry. </span><span class="koboSpan" id="kobo.466.2">For example, businesses can convert paper documents into digital formats using OCR, which not only saves physical storage space but also enhances document management processes. </span><span class="koboSpan" id="kobo.466.3">This conversion makes it easier to search, retrieve, and share documents, streamlining operations and </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">improving productivity.</span></span></p>
			<p><span class="koboSpan" id="kobo.468.1">In transportation, particularly with self-driving cars, OCR technology is used to read road signs and number plates. </span><span class="koboSpan" id="kobo.468.2">This capability is vital for navigation and ensuring compliance with traffic regulations. </span><span class="koboSpan" id="kobo.468.3">By accurately interpreting signage and vehicle identification, OCR contributes to the safe and efficient functioning of </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">autonomous vehicles.</span></span></p>
			<p><span class="koboSpan" id="kobo.470.1">Moreover, OCR technology</span><a id="_idIndexMarker1137"/><span class="koboSpan" id="kobo.471.1"> is employed in social media monitoring to detect brand logos and text in images. </span><span class="koboSpan" id="kobo.471.2">This application is particularly beneficial for marketing and brand management, as it enables companies to track brand visibility and engagement across social platforms. </span><span class="koboSpan" id="kobo.471.3">For instance, brands can use OCR to identify unauthorized use of their logos or monitor the spread of promotional materials, thereby enhancing their marketing strategies and protecting their </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">brand identity.</span></span></p>
			<p><span class="koboSpan" id="kobo.473.1">Let’s see how we can apply OCR in the data world with an open </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">source solution.</span></span></p>
			<h2 id="_idParaDest-275"><a id="_idTextAnchor312"/><span class="koboSpan" id="kobo.475.1">PaddleOCR</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.476.1">PaddleOCR</span></strong><span class="koboSpan" id="kobo.477.1"> is an open</span><a id="_idIndexMarker1138"/><span class="koboSpan" id="kobo.478.1"> source OCR tool developed by PaddlePaddle, which</span><a id="_idIndexMarker1139"/><span class="koboSpan" id="kobo.479.1"> is Baidu’s deep learning platform. </span><span class="koboSpan" id="kobo.479.2">The </span><a id="_idIndexMarker1140"/><span class="koboSpan" id="kobo.480.1">repository provides end-to-end OCR capabilities, including text detection, text recognition, and multilingual </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">support (</span></span><a href="https://github.com/PaddlePaddle/PaddleOCR"><span class="No-Break"><span class="koboSpan" id="kobo.482.1">https://github.com/PaddlePaddle/PaddleOCR</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.483.1">).</span></span></p>
			<p><span class="koboSpan" id="kobo.484.1">The PaddleOCR process </span><a id="_idIndexMarker1141"/><span class="koboSpan" id="kobo.485.1">has a lot of steps in place that can be seen in the following </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.486.1">Figure 13</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.487.1">.13</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">:</span></span></p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<span class="koboSpan" id="kobo.489.1"><img src="image/B19801_13_13.jpg" alt="Figure 13.13 – OCR process step by step"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.490.1">Figure 13.13 – OCR process step by step</span></p>
			<p><span class="koboSpan" id="kobo.491.1">Let’s break down the process step </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">by step:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.493.1">The process begins with an input image that may </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">contain text.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.495.1">Image preprocessing</span></strong><span class="koboSpan" id="kobo.496.1">: The image</span><a id="_idIndexMarker1142"/><span class="koboSpan" id="kobo.497.1"> may undergo various preprocessing steps, such as resizing, converting to grayscale, and noise reduction, to enhance </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">text visibility.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.499.1">Text detection</span></strong><span class="koboSpan" id="kobo.500.1">: The model detects regions of the image that contain text. </span><span class="koboSpan" id="kobo.500.2">This may involve algorithms such as </span><strong class="bold"><span class="koboSpan" id="kobo.501.1">Efficient and Accurate Scene Text</span></strong><span class="koboSpan" id="kobo.502.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.503.1">EAST</span></strong><span class="koboSpan" id="kobo.504.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.505.1">Differentiable Binarization</span></strong><span class="koboSpan" id="kobo.506.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.507.1">DB</span></strong><span class="koboSpan" id="kobo.508.1">) to find bounding boxes around </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">the text.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.510.1">Text recognition</span></strong><span class="koboSpan" id="kobo.511.1">: The detected text regions are fed into a recognition model (often a </span><strong class="bold"><span class="koboSpan" id="kobo.512.1">convolutional neural network</span></strong><span class="koboSpan" id="kobo.513.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.514.1">CNN</span></strong><span class="koboSpan" id="kobo.515.1">) followed by a </span><strong class="bold"><span class="koboSpan" id="kobo.516.1">long short-term model</span></strong><span class="koboSpan" id="kobo.517.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.518.1">LSTM</span></strong><span class="koboSpan" id="kobo.519.1">) or a transformer) to convert the visual text into </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">digital text.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.521.1">Post-processing</span></strong><span class="koboSpan" id="kobo.522.1">: The</span><a id="_idIndexMarker1143"/><span class="koboSpan" id="kobo.523.1"> recognized text may be further refined through spell-checking, grammar correction, or contextual analysis to </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">improve accuracy.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.525.1">Extracted text</span></strong><span class="koboSpan" id="kobo.526.1">: The final output consists of extracted digital text ready for </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">further use.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.528.1">Annotated image</span></strong><span class="koboSpan" id="kobo.529.1">: Optionally, an annotated version of the original image can be generated, showing the detected text regions along with the </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">recognized text.</span></span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.531.1">It may seem complicated initially but luckily, most of these steps are abstracted away from the user and are handled by the PaddleOCR package automatically. </span><span class="koboSpan" id="kobo.531.2">Let’s introduce a use case for OCR to extract text from YouTube </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">video thumbnails.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.533.1">YouTube thumbnails</span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.534.1">YouTube thumbnails</span></strong><span class="koboSpan" id="kobo.535.1"> are </span><a id="_idIndexMarker1144"/><span class="koboSpan" id="kobo.536.1">small, clickable images that represent a video on the platform. </span><span class="koboSpan" id="kobo.536.2">They serve as the visual preview that users see before clicking to watch a video. </span><span class="koboSpan" id="kobo.536.3">Thumbnails are crucial for attracting viewers, as they often play a significant role in influencing whether someone decides to watch </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">the content.</span></span></p>
			<p><span class="koboSpan" id="kobo.538.1">By analyzing the text present in thumbnails, such as video titles and promotional phrases, stakeholders can gain insights into viewer engagement and content trends. </span><span class="koboSpan" id="kobo.538.2">For instance, a marketing team can collect thumbnails from a range of videos and employ OCR to extract keywords and phrases that frequently appear in high-performing content. </span><span class="koboSpan" id="kobo.538.3">This </span><a id="_idIndexMarker1145"/><span class="koboSpan" id="kobo.539.1">analysis can reveal which terms resonate most with audiences, enabling creators to optimize their future thumbnails and align their messaging with popular themes. </span><span class="koboSpan" id="kobo.539.2">Additionally, the extracted text can </span><a id="_idIndexMarker1146"/><span class="koboSpan" id="kobo.540.1">inform </span><strong class="bold"><span class="koboSpan" id="kobo.541.1">search engine optimization</span></strong><span class="koboSpan" id="kobo.542.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.543.1">SEO</span></strong><span class="koboSpan" id="kobo.544.1">) strategies by identifying trending keywords to incorporate into video titles, descriptions, and tags, ultimately enhancing video discoverability. </span><span class="koboSpan" id="kobo.544.2">In our case, we have provided a folder on the GitHub repository with YouTube thumbnails from the channel I am cohosting </span><a id="_idIndexMarker1147"/><span class="koboSpan" id="kobo.545.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.546.1">Vector Lab</span></strong><span class="koboSpan" id="kobo.547.1">, discussing Gen AI and ML concepts. </span><span class="koboSpan" id="kobo.547.2">Here’s a link to the images folder on </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13/images"><span class="No-Break"><span class="koboSpan" id="kobo.549.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter13/images</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.550.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.551.1">The images in the folder look like the following figure and the idea is to pass all these images and extract the text depicted on </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">the image.</span></span></p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<span class="koboSpan" id="kobo.553.1"><img src="image/B19801_13_14.jpg" alt="Figure 13.14 – Example YouTube thumbnail"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.554.1">Figure 13.14 – Example YouTube thumbnail</span></p>
			<p><span class="koboSpan" id="kobo.555.1">Let’s see how we can </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">achieve that:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.557.1">We’ll start by </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">initializing PaddleOCR:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.559.1">
ocr = PaddleOCR(use_angle_cls=True, lang='en')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.560.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.561.1">use_angle_cls=True</span></strong><span class="koboSpan" id="kobo.562.1"> flag enables the use of an angle classifier in the OCR process. </span><span class="koboSpan" id="kobo.562.2">The angle classifier helps improve the accuracy of text recognition by determining the orientation of the text in the image. </span><span class="koboSpan" id="kobo.562.3">This is particularly useful for images where text might not be horizontally aligned (e.g., rotated or </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">skewed text).</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.564.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.565.1">lang='en'</span></strong><span class="koboSpan" id="kobo.566.1"> parameter specifies the language for OCR. </span><span class="koboSpan" id="kobo.566.2">In this case, </span><strong class="source-inline"><span class="koboSpan" id="kobo.567.1">'en'</span></strong><span class="koboSpan" id="kobo.568.1"> indicates that the text to be recognized is in English. </span><span class="koboSpan" id="kobo.568.2">PaddleOCR supports multiple languages and sets the appropriate language in case you want to perform OCR in a language other </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">than English.</span></span></p></li>				<li><span class="koboSpan" id="kobo.570.1">Next, we </span><a id="_idIndexMarker1148"/><span class="koboSpan" id="kobo.571.1">define the path to the folder containing images to extract the </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">text from:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.573.1">
folder_path = 'chapter13/images'</span></pre></li>				<li><span class="koboSpan" id="kobo.574.1">Then, we specify the supported image extensions. </span><span class="koboSpan" id="kobo.574.2">In our case, we only have </span><strong class="source-inline"><span class="koboSpan" id="kobo.575.1">.png</span></strong><span class="koboSpan" id="kobo.576.1">, but you can add any image type in </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">the folder:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.578.1">
supported_extensions = ('.png', '.jpg', '.jpeg')</span></pre></li>				<li><span class="koboSpan" id="kobo.579.1">Next, we get all paths to the images in the folder that we will use to load </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">the images:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.581.1">
image_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.lower().endswith(supported_extensions)]</span></pre></li>				<li><span class="koboSpan" id="kobo.582.1">Then, we create an empty DataFrame to store </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">the results:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.584.1">
df = pd.DataFrame(columns=['Image Path', 'Extracted Text'])</span></pre></li>				<li><span class="koboSpan" id="kobo.585.1">We use the following code to check if there are no image paths returned, which would mean that either there are no images in the folder or the images that exist in the folder don’t have any of the </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">supported extensions:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.587.1">
if not image_paths:
    print("No images found in the specified folder.")
else:
    for image_path in image_paths:
        </span><strong class="bold"><span class="koboSpan" id="kobo.588.1">process_image(image_path)</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.589.1">Now, let’s expand on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.590.1">process_image</span></strong><span class="koboSpan" id="kobo.591.1"> function. </span><span class="koboSpan" id="kobo.591.2">This function processes images and extracts text. </span><span class="koboSpan" id="kobo.591.3">For each thumbnail image, the function will extract any visible text, such as titles, keywords, or </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">promotional phrases:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.593.1">
def process_image(image_path):
    result = ocr.ocr(image_path, cls=True)
    extracted_text = ""
    for line in result[0]:
        extracted_text += line[1][0] + " "
        print(f"Extracted Text from {os.path.basename(image_path)}:\n{extracted_text}\n")
    df.loc[len(df)] = [image_path, extracted_text]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.594.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">process_image</span></strong><span class="koboSpan" id="kobo.596.1"> function performs OCR on an image specified by </span><strong class="source-inline"><span class="koboSpan" id="kobo.597.1">image_path</span></strong><span class="koboSpan" id="kobo.598.1">. </span><span class="koboSpan" id="kobo.598.2">It starts </span><a id="_idIndexMarker1149"/><span class="koboSpan" id="kobo.599.1">by invoking the </span><strong class="source-inline"><span class="koboSpan" id="kobo.600.1">ocr</span></strong><span class="koboSpan" id="kobo.601.1"> method from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.602.1">PaddleOCR</span></strong><span class="koboSpan" id="kobo.603.1"> library, which processes the image and returns the recognized text along with other details. </span><span class="koboSpan" id="kobo.603.2">The function initializes an empty string, </span><strong class="source-inline"><span class="koboSpan" id="kobo.604.1">extracted_text</span></strong><span class="koboSpan" id="kobo.605.1">, to accumulate the recognized text. </span><span class="koboSpan" id="kobo.605.2">It then iterates through each line of text detected by the OCR process, appending each line to </span><strong class="source-inline"><span class="koboSpan" id="kobo.606.1">extracted_text</span></strong><span class="koboSpan" id="kobo.607.1"> along with a space for separation. </span><span class="koboSpan" id="kobo.607.2">After processing the entire image, it prints the accumulated text along with the filename of the image. </span><span class="koboSpan" id="kobo.607.3">Finally, the function adds a new entry to a DataFrame called </span><strong class="source-inline"><span class="koboSpan" id="kobo.608.1">df</span></strong><span class="koboSpan" id="kobo.609.1">, storing </span><strong class="source-inline"><span class="koboSpan" id="kobo.610.1">image_path</span></strong><span class="koboSpan" id="kobo.611.1"> and the corresponding </span><strong class="source-inline"><span class="koboSpan" id="kobo.612.1">extracted_text</span></strong><span class="koboSpan" id="kobo.613.1"> in a new row, thus updating the DataFrame with the latest </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">OCR results.</span></span></p></li>				<li><span class="koboSpan" id="kobo.615.1">Optionally, we can save the DataFrame to a CSV file using the </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.617.1">
df.to_csv('extracted_texts.csv', index=False)</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.618.1">The results can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.619.1">Figure 13</span></em></span><em class="italic"><span class="koboSpan" id="kobo.620.1">.15</span></em><span class="koboSpan" id="kobo.621.1">, where we have the path to the image on the left and the extracted text on </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">the right.</span></span></p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<span class="koboSpan" id="kobo.623.1"><img src="image/B19801_13_15.jpg" alt="Figure 13.15 – OCR output"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.624.1">Figure 13.15 – OCR output</span></p>
			<p><span class="koboSpan" id="kobo.625.1">The results are </span><a id="_idIndexMarker1150"/><span class="koboSpan" id="kobo.626.1">great; however, we can see that there are some misspellings in certain cases, probably due to the font of the text in the images. </span><span class="koboSpan" id="kobo.626.2">The key thing to understand here is that we don’t have to deal with the images anymore, but only with the text, thereby significantly simplifying our challenge. </span><span class="koboSpan" id="kobo.626.3">Based on what we learned in </span><a href="B19801_12.xhtml#_idTextAnchor277"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.627.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.628.1">, </span><em class="italic"><span class="koboSpan" id="kobo.629.1">Text Preprocessing in the Era of LLMs</span></em><span class="koboSpan" id="kobo.630.1"> we can now manipulate and clean the text in various ways, such as chunking it, embedding it, or passing it through </span><strong class="bold"><span class="koboSpan" id="kobo.631.1">large language models</span></strong><span class="koboSpan" id="kobo.632.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.633.1">LLMs</span></strong><span class="koboSpan" id="kobo.634.1">), as we will see in the </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">next part.</span></span></p>
			<h2 id="_idParaDest-276"><a id="_idTextAnchor313"/><span class="koboSpan" id="kobo.636.1">Using LLMs with OCR</span></h2>
			<p><span class="koboSpan" id="kobo.637.1">OCR technology, despite </span><a id="_idIndexMarker1151"/><span class="koboSpan" id="kobo.638.1">its advancements, often produces errors, especially with complex layouts, low-quality images, or unusual fonts. </span><span class="koboSpan" id="kobo.638.2">These</span><a id="_idIndexMarker1152"/><span class="koboSpan" id="kobo.639.1"> errors include misrecognized characters and incorrect word breaks. </span><span class="koboSpan" id="kobo.639.2">So, the idea is to pass the OCR-extracted text through an LLM to correct these errors as LLMs understand context and can improve grammar and readability. </span><span class="koboSpan" id="kobo.639.3">Moreover, raw OCR output may be inconsistently formatted and hard to read; LLMs can reformat and restructure text, ensuring coherent and well-structured content. </span><span class="koboSpan" id="kobo.639.4">This automated proofreading reduces the need for manual intervention, saving time and minimizing human error. </span><span class="koboSpan" id="kobo.639.5">LLMs also standardize the text, making it consistent and easier to integrate into other systems, such as databases and </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">analytical tools.</span></span></p>
			<p><span class="koboSpan" id="kobo.641.1">In this section, we will expand the thumbnail example to pass the extracted text through an LLM to clean it. </span><span class="koboSpan" id="kobo.641.2">To run this example, you need to do the following </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">setup first.</span></span></p>
			<h3><span class="koboSpan" id="kobo.643.1">Hugging Face setup</span></h3>
			<p><span class="koboSpan" id="kobo.644.1">In order to </span><a id="_idIndexMarker1153"/><span class="koboSpan" id="kobo.645.1">run this example, you will need to have an account with Hugging Face and a token to authenticate. </span><span class="koboSpan" id="kobo.645.2">To do that, follow </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">these steps:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.647.1">Go </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">to </span></span><a href="https://huggingface.co"><span class="No-Break"><span class="koboSpan" id="kobo.649.1">https://huggingface.co</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.650.1">.</span></span></li>
				<li><span class="koboSpan" id="kobo.651.1">Create an account if you don’t </span><span class="No-Break"><span class="koboSpan" id="kobo.652.1">have one.</span></span></li>
				<li><span class="koboSpan" id="kobo.653.1">Go </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">to </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.655.1">Settings</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">.</span></span></li>
				<li><span class="koboSpan" id="kobo.657.1">Then, click on </span><strong class="bold"><span class="koboSpan" id="kobo.658.1">Access Tokens</span></strong><span class="koboSpan" id="kobo.659.1">. </span><span class="koboSpan" id="kobo.659.2">You should see the </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">following page:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<span class="koboSpan" id="kobo.661.1"><img src="image/B19801_13_16.jpg" alt="Figure 13.16 – Creating a new access token in Hugging Face"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.662.1">Figure 13.16 – Creating a new access token in Hugging Face</span></p>
			<ol>
				<li value="5"><span class="koboSpan" id="kobo.663.1">Click on the </span><strong class="bold"><span class="koboSpan" id="kobo.664.1">Create new token</span></strong><span class="koboSpan" id="kobo.665.1"> button to generate a new </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">personal token.</span></span></li>
				<li><span class="koboSpan" id="kobo.667.1">Remember to copy and keep this token as we will need to paste it in the code file </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">to authenticate!</span></span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.669.1">Now, we are ready to dive into </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">the code.</span></span></p>
			<h3><span class="koboSpan" id="kobo.671.1">Cleaning text with LLMs</span></h3>
			<p><span class="koboSpan" id="kobo.672.1">Let’s have a</span><a id="_idIndexMarker1154"/><span class="koboSpan" id="kobo.673.1"> look at the code that you can find in the GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/3.ocr_with_llms.py"><span class="No-Break"><span class="koboSpan" id="kobo.675.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/3.ocr_with_llms.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.676.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.677.1">We start by reading in the OCR-extracted text that we wrote in the </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">previous step:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.679.1">
df = pd.read_csv('extracted_texts.csv')</span></pre></li>				<li><span class="koboSpan" id="kobo.680.1">We then initialize the Hugging Face model. </span><span class="koboSpan" id="kobo.680.2">In this case, we are using </span><strong class="source-inline"><span class="koboSpan" id="kobo.681.1">Mistral-Nemo-Instruct-2407</span></strong><span class="koboSpan" id="kobo.682.1">, but you can replace it with any LLM you have </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">access to:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.684.1">
model_name = "mistralai/Mistral-Nemo-Instruct-2407"</span></pre><p class="list-inset"><strong class="bold"><span class="koboSpan" id="kobo.685.1">Hugging Face</span></strong><span class="koboSpan" id="kobo.686.1"> is a </span><a id="_idIndexMarker1155"/><span class="koboSpan" id="kobo.687.1">platform that provides a diverse repository of pretrained models, which can be accessed and integrated easily using user-friendly APIs. </span><span class="koboSpan" id="kobo.687.2">Hugging Face models come with detailed documentation and benefit from continuous innovation driven by a collaborative community. </span><span class="koboSpan" id="kobo.687.3">I see it as being similar to how GitHub serves as a repository for code; Hugging Face functions as a repository for ML models. </span><span class="koboSpan" id="kobo.687.4">Importantly, many models on Hugging Face are available for free, making it a cost-effective option for individuals </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">and researchers.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.689.1">In contrast, there are many other paid models available, such as Azure OpenAI, which provides access to models such as GPT-3 and GPT-4. </span><span class="koboSpan" id="kobo.689.2">These models can be accessed on a paid model, and you have to manage the authentication process, which is different from authenticating with </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">Hugging Face.</span></span></p></li>				<li><span class="koboSpan" id="kobo.691.1">Add your Hugging Face API token, which was created in</span><a id="_idTextAnchor314"/><span class="koboSpan" id="kobo.692.1"> the previous </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">setup section:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.694.1">
api_token = </span><strong class="bold"><span class="koboSpan" id="kobo.695.1">"add_your_token"</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.696.1">Here is the LangChain setup with some few-shot examples. </span><span class="koboSpan" id="kobo.696.2">Here, we use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.697.1">PromptTemplate</span></strong><span class="koboSpan" id="kobo.698.1"> class, which helps create a prompt for </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">the model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.700.1">
prompt_template = PromptTemplate(
    input_variables=["text"],
    template='''
    Correct the following text for spelling errors and return only the corrected text in lowercase. </span><span class="koboSpan" id="kobo.700.2">Respond using JSON format, strictly according to the following schema:
    {{"corrected_text": "corrected text in lowercase"}}
     Examples: Three examples are provided to guide the model:
     Input: Shows the input text needing correction.
</span><span class="koboSpan" id="kobo.700.3">     Output: Provides the expected JSON format for the corrected text. </span><span class="koboSpan" id="kobo.700.4">This helps the model learn what is required and encourages it to follow the same format when generating its responses.
</span><span class="koboSpan" id="kobo.700.5">    Examples:
    Input: "Open vs Proprietary LLMs"
    Output: {{"corrected_text": "open vs proprietary llms"}}
    Input: "HOW TO MITIGATE SaCURITY RISKS IN AI AND ML SYSTEM VECTOR LAB"
     Output: {{"corrected_text": "how to mitigate security risks in ai and ml system vector lab"}}
    Input: "BUILDING DBRX-CLASS CUSTOM LLMS WITH MOSAIC A1 TRAINING VECTOR LAB"
     Output: {{"corrected_text": "building dbrx-class custom llms with mosaic a1 training vector lab"}}
    Text to Correct: Placeholder {text} that will be replaced with the actual input text when calling the model.
</span><span class="koboSpan" id="kobo.700.6">    Text to correct:
    {text}
    Final Instruction: Specifies that the output should be in JSON format only, which reinforces the expectation that the model should avoid unnecessary explanations or additional text.
</span><span class="koboSpan" id="kobo.700.7">    Output (JSON format only):
    '''
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.701.1">The code</span><a id="_idIndexMarker1156"/><span class="koboSpan" id="kobo.702.1"> snippet defines a </span><strong class="source-inline"><span class="koboSpan" id="kobo.703.1">PromptTemplate</span></strong><span class="koboSpan" id="kobo.704.1"> class for use with a language model to correct spelling errors in text. </span><span class="koboSpan" id="kobo.704.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.705.1">PromptTemplate</span></strong><span class="koboSpan" id="kobo.706.1"> class is initialized with two key parameters: </span><strong class="source-inline"><span class="koboSpan" id="kobo.707.1">input_variables</span></strong><span class="koboSpan" id="kobo.708.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.709.1">template</span></strong><span class="koboSpan" id="kobo.710.1">. </span><span class="koboSpan" id="kobo.710.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.711.1">input_variables</span></strong><span class="koboSpan" id="kobo.712.1"> parameter specifies the input variable as </span><strong class="source-inline"><span class="koboSpan" id="kobo.713.1">["text"]</span></strong><span class="koboSpan" id="kobo.714.1">, which represents the text that will be corrected. </span><span class="koboSpan" id="kobo.714.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.715.1">template</span></strong><span class="koboSpan" id="kobo.716.1"> parameter contains the prompt structure sent to the model. </span><span class="koboSpan" id="kobo.716.2">This structure includes clear instructions for the model to correct spelling errors and return the output in lowercase, formatted as JSON. </span><span class="koboSpan" id="kobo.716.3">The JSON schema specifies the expected output format, ensuring consistency in responses. </span><span class="koboSpan" id="kobo.716.4">The template also provides three examples of input text and their corresponding corrected output in JSON format, guiding the model on how to process similar requests. </span><span class="koboSpan" id="kobo.716.5">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.717.1">{text}</span></strong><span class="koboSpan" id="kobo.718.1"> placeholder in the template will be replaced with the actual input text when the model is invoked. </span><span class="koboSpan" id="kobo.718.2">The final instruction emphasizes that the output should be strictly in JSON format, avoiding any additional text </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">or explanations.</span></span></p></li>				<li><span class="koboSpan" id="kobo.720.1">We initialize</span><a id="_idIndexMarker1157"/><span class="koboSpan" id="kobo.721.1"> the model from Hugging Face using the model name and API token that we </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">specified earlier:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.723.1">
huggingface_llm = HuggingFaceHub(repo_id=model_name, huggingfacehub_api_token=api_token, model_kwargs={"task": "text-generation"})</span></pre></li>				<li><span class="koboSpan" id="kobo.724.1">We then combine the prompt template and the model, creating a chain that will take input text, apply the prompt, and </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">create output:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.726.1">
llm_chain = LLMChain(prompt=prompt_template, llm=huggingface_llm)</span></pre></li>				<li><span class="koboSpan" id="kobo.727.1">We use </span><strong class="source-inline"><span class="koboSpan" id="kobo.728.1">llm_chain</span></strong><span class="koboSpan" id="kobo.729.1"> to generate </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">a response:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.731.1">
response = llm_chain.invoke(text)</span></pre></li>				<li><span class="koboSpan" id="kobo.732.1">Finally, we apply text correction to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.733.1">Extracted </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.734.1">Text</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.735.1"> column:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.736.1">
df['Corrected Text'] = df['Extracted Text'].apply(correct_text)</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.737.1">Let’s present some of </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">the results:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.739.1">
Original: HOW TO MITIGATE SaCURITY RISKS IN AI AND ML SYSTEM VECTOR LAB
Corrected: how to mitigate security risks in ai and ml system vector lab
Original: BUILDING DBRX-CLASS CUSTOM LLMS WITH MOSAIC A1 TRAINING VECTOR LAB
Corrected: building dbrx-class custom llms with mosaic </span><strong class="bold"><span class="koboSpan" id="kobo.740.1">a1</span></strong><span class="koboSpan" id="kobo.741.1"> training vector lab
Original: MPROVING TeXT2SO L PeRFORMANCe WITH EASE ON DATABRICKS 7 VECTOR LAB
Corrected: improving </span><strong class="bold"><span class="koboSpan" id="kobo.742.1">text2so l</span></strong><span class="koboSpan" id="kobo.743.1"> performance with ease on databricks 7 vector lab</span></pre>			<p><span class="koboSpan" id="kobo.744.1">As you can see, the</span><a id="_idIndexMarker1158"/><span class="koboSpan" id="kobo.745.1"> results are quite good; we have managed to convert all the text to lowercase and fix most of the misspellings. </span><span class="koboSpan" id="kobo.745.2">The misspelled words that were not corrected and flagged in the preceding samples seem to be very technical content. </span><span class="koboSpan" id="kobo.745.3">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.746.1">text2so l</span></strong><span class="koboSpan" id="kobo.747.1"> means </span><strong class="source-inline"><span class="koboSpan" id="kobo.748.1">text2sql</span></strong><span class="koboSpan" id="kobo.749.1">, which is challenging for the model to fix unless it is fine-tuned on this type of correction data. </span><span class="koboSpan" id="kobo.749.2">Another approach you could try is to include these very technical cases that the model seems to miss in the few-shot examples in the prompt to “teach” the model how to interpret </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">these words.</span></span></p>
			<p><span class="koboSpan" id="kobo.751.1">In the code file on the GitHub repository, you’ll see that we have added error handling and parsing for the JSON output. </span><span class="koboSpan" id="kobo.751.2">This is necessary because we are asking the model to return the output in a specific format, but LLMs do not always follow these instructions precisely. </span><span class="koboSpan" id="kobo.751.3">There is currently ongoing work on enforcing the output of LLMs in a specific format, but at this point, it is experimental. </span><span class="koboSpan" id="kobo.751.4">You can find more information </span><span class="No-Break"><span class="koboSpan" id="kobo.752.1">here: </span></span><a href="https://python.langchain.com/v0.1/docs/integrations/llms/lmformatenforcer_experimental/"><span class="No-Break"><span class="koboSpan" id="kobo.753.1">https://python.langchain.com/v0.1/docs/integrations/llms/lmformatenforcer_experimental/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.754.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.755.1">As of now, we have seen how we can use OCR to extract text from images and then fix the extracted text by passing it to an LLM. </span><span class="koboSpan" id="kobo.755.2">In the case of a thumbnail image, this extracted text can also be used as an image caption, as the video’s title is usually depicted on the image. </span><span class="koboSpan" id="kobo.755.3">However, there are cases where the image doesn’t contain any text, and we need to ask the model to infer the caption based on what it has seen and understood from the image. </span><span class="koboSpan" id="kobo.755.4">This will be the point of discussion for the </span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">next part.</span></span></p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor315"/><span class="koboSpan" id="kobo.757.1">Creating image captions</span></h2>
			<p><span class="koboSpan" id="kobo.758.1">Creating accurate</span><a id="_idIndexMarker1159"/><span class="koboSpan" id="kobo.759.1"> and meaningful captions for images involves not only recognizing and interpreting visual content but also understanding context and generating descriptive text that accurately reflects the image’s content. </span><span class="koboSpan" id="kobo.759.2">The complexity arises from the need for models to process various elements in an image, such as objects, scenes, and activities, and then translate these elements into coherent and relevant language. </span><span class="koboSpan" id="kobo.759.3">This challenge is further compounded by the diverse and often subtle nature of visual information, which can include different lighting conditions, angles, </span><span class="No-Break"><span class="koboSpan" id="kobo.760.1">and contexts.</span></span></p>
			<p><span class="koboSpan" id="kobo.761.1">To showcase the difference between the technique we demonstrated earlier, and the captioning based on image understanding, we will use the same images from the thumbnails and</span><a id="_idIndexMarker1160"/><span class="koboSpan" id="kobo.762.1"> attempt to create captions for them based on the image understanding process instead of the text extraction process. </span><span class="koboSpan" id="kobo.762.2">You can find the code for this part </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/4.image_captioning.py"><span class="No-Break"><span class="koboSpan" id="kobo.764.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/4.image_captioning.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.765.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.766.1">Now, let’s dive into </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">the code:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.768.1">Let’s start by importing the libraries we’ll need for </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">this example:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.770.1">
import os
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
from transformers import BlipProcessor, BlipForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM
from langchain import PromptTemplate, LLMChain
from langchain.llms import HuggingFaceHub</span></pre></li>				<li><span class="koboSpan" id="kobo.771.1">We then define the folder containing </span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">the images:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.773.1">
folder_path = 'chapter13/images'</span></pre></li>				<li><span class="koboSpan" id="kobo.774.1">Next, we make a list of supported </span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">image extensions:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.776.1">
supported_extensions = ('.png', '.jpg', '.jpeg')</span></pre></li>				<li><span class="koboSpan" id="kobo.777.1">We then get all image paths for each image in </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">the folder:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.779.1">
image_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.lower().endswith(supported_extensions)]</span></pre></li>				<li><span class="koboSpan" id="kobo.780.1">We create an empty DataFrame to store </span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">the results:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.782.1">
df = pd.DataFrame(columns=['Image Path', 'Generated Caption', 'Refined Caption'])</span></pre></li>				<li><span class="koboSpan" id="kobo.783.1">Then, we initialize the BLIP model and processor for </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">image captioning:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.785.1">
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.786.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.787.1">BlipForConditionalGeneration</span></strong><span class="koboSpan" id="kobo.788.1"> model is a pretrained model designed for image captioning. </span><span class="koboSpan" id="kobo.788.2">It generates descriptive text (captions) for given images by</span><a id="_idIndexMarker1161"/><span class="koboSpan" id="kobo.789.1"> understanding the visual content and producing coherent and relevant descriptions. </span><span class="koboSpan" id="kobo.789.2">The model is based on the BLIP architecture, which is optimized for linking visual and textual information. </span><strong class="source-inline"><span class="koboSpan" id="kobo.790.1">BlipProcessor</span></strong><span class="koboSpan" id="kobo.791.1"> is responsible for preparing images and text inputs in a format suitable for the BLIP model. </span><span class="koboSpan" id="kobo.791.2">It handles the preprocessing of images (such as resizing and normalization) and any required text formatting to ensure that the data fed into the model is in the </span><span class="No-Break"><span class="koboSpan" id="kobo.792.1">correct format.</span></span></p></li>				<li><span class="koboSpan" id="kobo.793.1">Now, we initialize the LLM for text refinement. </span><span class="koboSpan" id="kobo.793.2">Once we create the caption with the BLIP model, we will then pass it to an LLM again to clean and optimize </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">the caption:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.795.1">
llm_model_name = "google/flan-t5-small" # You can play with any other model from hugging phase as well
tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.796.1">This piece of code specifies the name of the pretrained model to be used. </span><span class="koboSpan" id="kobo.796.2">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.797.1">"google/flan-t5-small"</span></strong><span class="koboSpan" id="kobo.798.1"> refers to a specific version of the T5 model, called </span><strong class="source-inline"><span class="koboSpan" id="kobo.799.1">FLAN-T5 Small</span></strong><span class="koboSpan" id="kobo.800.1">, developed by Google. </span><span class="koboSpan" id="kobo.800.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.801.1">AutoTokenizer</span></strong><span class="koboSpan" id="kobo.802.1"> class from Hugging Face’s Transformers library is used to load the tokenizer associated with the specified model. </span><span class="koboSpan" id="kobo.802.2">As we learned in </span><a href="B19801_12.xhtml#_idTextAnchor277"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.803.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.804.1">, </span><em class="italic"><span class="koboSpan" id="kobo.805.1">Text Preprocessing in the Era of LLMs</span></em><span class="koboSpan" id="kobo.806.1">, tokenizers are responsible for converting raw text into token IDs that the model can understand. </span><span class="koboSpan" id="kobo.806.2">They handle tasks such as tokenizing (splitting text into manageable units), adding</span><a id="_idIndexMarker1162"/><span class="koboSpan" id="kobo.807.1"> special tokens, and encoding the text in a format suitable for model input. </span><span class="koboSpan" id="kobo.807.2">Finally, it loads the </span><strong class="source-inline"><span class="koboSpan" id="kobo.808.1">google/flan-t5-small sequence-to-sequence</span></strong><span class="koboSpan" id="kobo.809.1"> language model, which is suitable for tasks such as translation, summarization, or any task where the model needs to generate text based on some input text. </span><span class="koboSpan" id="kobo.809.2">The model has been pretrained on a large dataset, enabling it to understand and generate human-like text, and it is perfect for our use case of </span><span class="No-Break"><span class="koboSpan" id="kobo.810.1">caption generation.</span></span></p></li>				<li><span class="koboSpan" id="kobo.811.1">Next, we need to chain all our steps together and we will use functionality from LangChain to </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">do so:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.813.1">
api_token = "</span><strong class="bold"><span class="koboSpan" id="kobo.814.1">add_your_hugging_face_token</span></strong><span class="koboSpan" id="kobo.815.1">"
prompt_template = PromptTemplate(input_variables=["text"], template="Refine and correct the following caption: {text}")
huggingface_llm = HuggingFaceHub(repo_id=llm_model_name, huggingfacehub_api_token=api_token)
llm_chain = LLMChain(prompt=prompt_template, llm=huggingface_llm)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.816.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.817.1">PromptTemplate</span></strong><span class="koboSpan" id="kobo.818.1"> object, which is used to define how prompts (input requests) are structured for the language model is created here. </span><span class="koboSpan" id="kobo.818.2">Here, we need a much simpler prompt than the one in the previous example as the task is simpler to explain to the model. </span><span class="koboSpan" id="kobo.818.3">This instructs the model to refine and correct the provided caption. </span><span class="koboSpan" id="kobo.818.4">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.819.1">{text}</span></strong><span class="koboSpan" id="kobo.820.1"> placeholder will be replaced with the actual text that needs refinement. </span><span class="koboSpan" id="kobo.820.2">Then, an instance of </span><strong class="source-inline"><span class="koboSpan" id="kobo.821.1">HuggingFaceHub</span></strong><span class="koboSpan" id="kobo.822.1"> is created and finally, we create the LLMChain to connect the prompt with the </span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">language model.</span></span></p></li>				<li><span class="koboSpan" id="kobo.824.1">We create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.825.1">refine_caption</span></strong><span class="koboSpan" id="kobo.826.1"> function that accepts a generated caption as input and creates a prompt by formatting </span><strong class="source-inline"><span class="koboSpan" id="kobo.827.1">prompt_template</span></strong><span class="koboSpan" id="kobo.828.1"> with the input caption. </span><span class="koboSpan" id="kobo.828.2">It then uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.829.1">llm_chain</span></strong><span class="koboSpan" id="kobo.830.1"> to run the prompt through the LLM, generating a refined caption, and it returns the </span><span class="No-Break"><span class="koboSpan" id="kobo.831.1">refined caption:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.832.1">
def refine_caption(caption):
    prompt = prompt_template.format(text=caption)
    refined_caption = llm_chain.run(prompt)
    return refined_caption</span></pre></li>				<li><span class="koboSpan" id="kobo.833.1">We then </span><a id="_idIndexMarker1163"/><span class="koboSpan" id="kobo.834.1">create the </span><strong class="source-inline"><span class="koboSpan" id="kobo.835.1">generate_caption</span></strong><span class="koboSpan" id="kobo.836.1"> function, which accepts an image path </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">as input:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.838.1">
def generate_caption(image_path):
    image = Image.open(image_path).convert("RGB")
    inputs = blip_processor(images=image, return_tensors="pt")
    outputs = blip_model.generate(inputs)
    caption = blip_processor.decode(outputs[0], skip_special_tokens=True)
    return caption</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.839.1">This function performs </span><span class="No-Break"><span class="koboSpan" id="kobo.840.1">the following:</span></span></p><ul><li><span class="koboSpan" id="kobo.841.1">The function opens the image file and converts it to </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">RGB format.</span></span></li><li><span class="koboSpan" id="kobo.843.1">It then processes the image using </span><strong class="source-inline"><span class="koboSpan" id="kobo.844.1">blip_processor</span></strong><span class="koboSpan" id="kobo.845.1">, returning a tensor suitable for the </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">BLIP model.</span></span></li><li><span class="koboSpan" id="kobo.847.1">The function generates a caption by passing the processed image to the BLIP model. </span><span class="koboSpan" id="kobo.847.2">It finally decodes the model’s output into a human-readable caption, skipping special tokens, and returns </span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">the caption.</span></span></li></ul></li>				<li><span class="koboSpan" id="kobo.849.1">Finally, we process each image in the folder, generate an image caption, refine it, and append the final result to </span><span class="No-Break"><span class="koboSpan" id="kobo.850.1">a DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.851.1">
if not image_paths:
    print("No images found in the specified folder.")
else:
    for image_path in image_paths:
        caption = generate_caption(image_path)
        print(f"Generated Caption for {os.path.basename(image_path)}:\n{caption}\n")
        refined_caption = refine_caption(caption)
        print(f"Refined Caption:\n{refined_caption}\n")
        df.loc[len(df)] = [image_path, caption, refined_caption]</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.852.1">Let’s have a </span><a id="_idIndexMarker1164"/><span class="koboSpan" id="kobo.853.1">look at the captions generated by </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">this process:</span></span></p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<span class="koboSpan" id="kobo.855.1"><img src="image/B19801_13_18.jpg" alt="Figure 13.17 – Image caption creation"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.856.1">Figure 13.17 – Image caption creation</span></p>
			<p><span class="koboSpan" id="kobo.857.1">As we can see, this caption is poor compared to the previous method we demonstrated. </span><span class="koboSpan" id="kobo.857.2">The model attempts to understand what is happening in the image and grasp the context, but since the context is derived from a thumbnail, it ends up being quite inadequate.  </span><span class="koboSpan" id="kobo.857.3">We need to understand that thumbnails often provide limited context about the video content; while they are designed to attract clicks, they may not convey enough information for the model to generate informative captions. </span><span class="koboSpan" id="kobo.857.4">The lack of context in combination with the fact that thumbnails are frequently visually cluttered with various images, graphics, and text elements makes it challenging for the model to discern the main subject or context. </span><span class="koboSpan" id="kobo.857.5">This complexity can lead to captions that are less coherent or relevant than we have experienced. </span><span class="koboSpan" id="kobo.857.6">So, in the case of dealing with thumbnails, the OCR process </span><span class="No-Break"><span class="koboSpan" id="kobo.858.1">is best.</span></span></p>
			<p><span class="koboSpan" id="kobo.859.1">However, in cases</span><a id="_idIndexMarker1165"/><span class="koboSpan" id="kobo.860.1"> where images do not contain text, unlike thumbnails that are often filled with written elements, the image understanding process becomes the primary method for generating captions. </span><span class="koboSpan" id="kobo.860.2">Since these images lack textual information, relying on the model’s visual understanding is essential for creating accurate and meaningful descriptions. </span><span class="koboSpan" id="kobo.860.3">Here is some homework for you: Pass through the BLIP process an image that has no text and see what </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">you get!</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.862.1">But what about videos?</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.863.1">To handle videos, the process involves reading the video file and capturing frames at specified intervals, allowing us to analyze each frame, so, </span><em class="italic"><span class="koboSpan" id="kobo.864.1">each image</span></em><span class="koboSpan" id="kobo.865.1">, independently. </span><span class="koboSpan" id="kobo.865.2">Once we have the frames, we can apply techniques like those used for images, such as OCR for text extraction, or image understanding models, such as BLIP, for </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">caption generation.</span></span></p>
			<p><span class="koboSpan" id="kobo.867.1">Next, we will move from image to audio data and discuss how we can simplify the </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">audio processing.</span></span></p>
			<h1 id="_idParaDest-278"><a id="_idTextAnchor316"/><span class="koboSpan" id="kobo.869.1">Handling audio data</span></h1>
			<p><span class="koboSpan" id="kobo.870.1">A lot of </span><a id="_idIndexMarker1166"/><span class="koboSpan" id="kobo.871.1">work is happening in the audio processing space with the most significant advancements</span><a id="_idIndexMarker1167"/><span class="koboSpan" id="kobo.872.1"> happening in </span><strong class="bold"><span class="koboSpan" id="kobo.873.1">automatic speech recognition</span></strong><span class="koboSpan" id="kobo.874.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.875.1">ASR</span></strong><span class="koboSpan" id="kobo.876.1">) models. </span><span class="koboSpan" id="kobo.876.2">These models transform spoken language into written text, allowing the seamless integration of voice inputs into text-based workflows, thereby making it easier to analyze, search, and interact with. </span><span class="koboSpan" id="kobo.876.3">For instance, voice assistants, such as Siri and Google Assistant, rely on ASR to understand and respond to user commands, while transcription services convert meeting recordings into searchable </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">text documents.</span></span></p>
			<p><span class="koboSpan" id="kobo.878.1">This conversion allows </span><a id="_idIndexMarker1168"/><span class="koboSpan" id="kobo.879.1">the passing of text input to LLMs to unlock powerful capabilities, such as sentiment analysis, topic modeling, automated summarization, and even supporting chat applications. </span><span class="koboSpan" id="kobo.879.2">For example, customer service call centers can use ASR to transcribe conversations, which can then be analyzed for customer sentiment or common issues, improving service quality </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">and efficiency.</span></span></p>
			<p><span class="koboSpan" id="kobo.881.1">Handling audio data as text not only enhances accessibility and usability but also facilitates more efficient data storage and retrieval. </span><span class="koboSpan" id="kobo.881.2">Text data takes up less space than audio files and is easier to index and search. </span><span class="koboSpan" id="kobo.881.3">Moreover, it bridges the gap between spoken and written communication, enabling more natural and intuitive user interactions across various platforms and devices. </span><span class="koboSpan" id="kobo.881.4">For instance, integrating ASR in educational apps can help students with disabilities access spoken content in a text format, making learning </span><span class="No-Break"><span class="koboSpan" id="kobo.882.1">more inclusive.</span></span></p>
			<p><span class="koboSpan" id="kobo.883.1">As ASR technologies continue to improve, the ability to accurately and efficiently convert audio to text will become increasingly important, driving innovation and expanding the potential of AI-driven solutions. </span><span class="koboSpan" id="kobo.883.2">Enhanced ASR models will further benefit areas such as real-time translation services, automated note-taking in professional settings, and accessibility tools for individuals with hearing impairments, showcasing the broad and transformative impact of </span><span class="No-Break"><span class="koboSpan" id="kobo.884.1">this technology.</span></span></p>
			<p><span class="koboSpan" id="kobo.885.1">In the next section, we will discuss the Whisper model, which is effective for transforming audio into text and performing a range of audio </span><span class="No-Break"><span class="koboSpan" id="kobo.886.1">processing tasks.</span></span></p>
			<h2 id="_idParaDest-279"><a id="_idTextAnchor317"/><span class="koboSpan" id="kobo.887.1">Using Whisper for audio-to-text conversion</span></h2>
			<p><span class="koboSpan" id="kobo.888.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.889.1">Whisper model</span></strong><span class="koboSpan" id="kobo.890.1"> from </span><a id="_idIndexMarker1169"/><span class="koboSpan" id="kobo.891.1">OpenAI is a powerful tool for</span><a id="_idIndexMarker1170"/><span class="koboSpan" id="kobo.892.1"> transforming audio to text and serves as a base for many modern AI and ML applications. </span><span class="koboSpan" id="kobo.892.2">The applications range from real-time transcription and customer service to healthcare and education, showcasing its versatility and importance in the evolving landscape </span><a id="_idIndexMarker1171"/><span class="koboSpan" id="kobo.893.1">of audio </span><span class="No-Break"><span class="koboSpan" id="kobo.894.1">processing technology:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.895.1">Whisper can be integrated into voice assistant systems, such as Siri, Google Assistant, and Alexa, to accurately transcribe user commands </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">and queries.</span></span></li>
				<li><span class="koboSpan" id="kobo.897.1">Call centers can use Whisper to transcribe customer interactions, allowing for sentiment analysis, quality assurance, and topic detection, thereby enhancing </span><span class="No-Break"><span class="koboSpan" id="kobo.898.1">service quality.</span></span></li>
				<li><span class="koboSpan" id="kobo.899.1">Platforms </span><a id="_idIndexMarker1172"/><span class="koboSpan" id="kobo.900.1">such as YouTube and podcast services can use Whisper to generate subtitles and transcriptions, improving accessibility and </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">content indexing.</span></span></li>
				<li><span class="koboSpan" id="kobo.902.1">Whisper</span><a id="_idIndexMarker1173"/><span class="koboSpan" id="kobo.903.1"> can be used in real-time transcription services for meetings, lectures, and live events. </span><span class="koboSpan" id="kobo.903.2">This helps create accurate text records that are easy to search and </span><span class="No-Break"><span class="koboSpan" id="kobo.904.1">analyze later.</span></span></li>
				<li><span class="koboSpan" id="kobo.905.1">In telemedicine, Whisper can transcribe doctor-patient conversations accurately, facilitating better record-keeping and analysis. </span><span class="koboSpan" id="kobo.905.2">Moreover, it can assist in creating automated medical notes from </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">audio recordings.</span></span></li>
				<li><span class="koboSpan" id="kobo.907.1">Educational platforms can use Whisper to transcribe lectures and tutorials, providing students with written records of spoken content, enhancing learning </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">and accessibility.</span></span></li>
				<li><span class="koboSpan" id="kobo.909.1">Security systems use direct audio processing to verify identity based on unique vocal characteristics, offering a more secure and non-intrusive method </span><span class="No-Break"><span class="koboSpan" id="kobo.910.1">of authentication.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.911.1">As a pretrained model, Whisper can be used out of the box for many tasks, reducing the need for extensive fine-tuning and allowing for quick integration into various applications. </span><span class="koboSpan" id="kobo.911.2">The model supports multiple languages, making it versatile for global applications and diverse user bases. </span><span class="koboSpan" id="kobo.911.3">While Whisper primarily focuses on transforming audio to text, it also benefits from advancements in handling audio signals, potentially capturing nuances, such as tone and emotion. </span><span class="koboSpan" id="kobo.911.4">Although direct audio processing (such as emotion detection or music analysis) might require additional specialized models, Whisper’s robust transcription capability is foundational for </span><span class="No-Break"><span class="koboSpan" id="kobo.912.1">many applications.</span></span></p>
			<p><span class="koboSpan" id="kobo.913.1">Using some audio</span><a id="_idIndexMarker1174"/><span class="koboSpan" id="kobo.914.1"> from the </span><strong class="bold"><span class="koboSpan" id="kobo.915.1">Vector Lab</span></strong><span class="koboSpan" id="kobo.916.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.917.1">@VectorLab</span></strong><span class="koboSpan" id="kobo.918.1">) videos, we will parse the audio through Whisper to get the </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">extracted text.</span></span></p>
			<h2 id="_idParaDest-280"><a id="_idTextAnchor318"/><span class="koboSpan" id="kobo.920.1">Extracting text from audio</span></h2>
			<p><span class="koboSpan" id="kobo.921.1">The </span><a id="_idIndexMarker1175"/><span class="koboSpan" id="kobo.922.1">following code demonstrates how to use the Whisper model from Hugging Face to transcribe audio files into text. </span><span class="koboSpan" id="kobo.922.2">It covers loading necessary libraries, processing an audio file, generating a transcription using the model, and finally decoding and printing the transcribed text. </span><span class="koboSpan" id="kobo.922.3">Let’s have a look at the code, which you can also find </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/5.whisper.py"><span class="No-Break"><span class="koboSpan" id="kobo.924.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/5.whisper.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.925.1">.</span></span></p>
			<p><span class="No-Break"><span class="koboSpan" id="kobo.926.1">Let’s begin:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.927.1">We’ll start by importing the </span><span class="No-Break"><span class="koboSpan" id="kobo.928.1">required libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.929.1">
import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import librosa</span></pre></li>				<li><span class="koboSpan" id="kobo.930.1">We start by loading the Whisper processor and model from </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">Hugging Face:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.932.1">
processor = WhisperProcessor.from_pretrained("openai/whisper-large-v2")
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2")</span></pre></li>				<li><span class="koboSpan" id="kobo.933.1">Next, we define the path to your </span><span class="No-Break"><span class="koboSpan" id="kobo.934.1">audio file:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.935.1">
audio_path = "chapter13/audio/3.chain orchestrator.mp3"</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.936.1">You can replace this file with any other audio </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">you want.</span></span></p></li>				<li><span class="koboSpan" id="kobo.938.1">Then, we load the </span><span class="No-Break"><span class="koboSpan" id="kobo.939.1">audio file:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.940.1">
audio, rate = librosa.load(</span><strong class="bold"><span class="koboSpan" id="kobo.941.1">audio_path, sr=16000</span></strong><span class="koboSpan" id="kobo.942.1">)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.943.1">Let’s expand </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">on this:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.945.1">audio</span></strong><span class="koboSpan" id="kobo.946.1"> will be a NumPy array containing the </span><span class="No-Break"><span class="koboSpan" id="kobo.947.1">audio samples.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.948.1">rate</span></strong><span class="koboSpan" id="kobo.949.1"> is the sampling rate of the audio file. </span><span class="koboSpan" id="kobo.949.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.950.1">sr=16000</span></strong><span class="koboSpan" id="kobo.951.1"> argument resamples the audio to a sampling rate of 16 kHz, which is the required input sampling rate for the </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">Whisper mode.</span></span></li></ul></li>				<li><span class="koboSpan" id="kobo.953.1">Now, we</span><a id="_idIndexMarker1176"/><span class="koboSpan" id="kobo.954.1"> preprocess the audio file for the </span><span class="No-Break"><span class="koboSpan" id="kobo.955.1">Whisper model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.956.1">
input_features = processor(audio, sampling_rate=rate, return_tensors="pt").input_features</span></pre></li>				<li><span class="koboSpan" id="kobo.957.1">We then generate </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">the transcription:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.959.1">
with torch.no_grad():
    predicted_ids = model.generate(input_features)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.960.1">This line passes the preprocessed audio features to the model to generate transcription IDs. </span><span class="koboSpan" id="kobo.960.2">The model produces token IDs that correspond to the </span><span class="No-Break"><span class="koboSpan" id="kobo.961.1">transcribed text.</span></span></p></li>				<li><span class="koboSpan" id="kobo.962.1">Now, we decode the </span><span class="No-Break"><span class="koboSpan" id="kobo.963.1">generated transcription:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.964.1">
transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.965.1">This line decodes the predicted token IDs back into readable text. </span><span class="koboSpan" id="kobo.965.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.966.1">[0]</span></strong><span class="koboSpan" id="kobo.967.1"> value at the end extracts the first (and only) transcription from the </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">resulting list.</span></span></p></li>				<li><span class="koboSpan" id="kobo.969.1">Finally, we print the </span><span class="No-Break"><span class="koboSpan" id="kobo.970.1">transcribed text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.971.1">
"As you can see, you need what we call a chain orchestrator to coordinate all the steps. </span><span class="koboSpan" id="kobo.971.2">So all the steps from raising the question all the way to the response. </span><span class="koboSpan" id="kobo.971.3">And the most popular open source packages are Lama Index and LangChain that we can recommend. </span><span class="koboSpan" id="kobo.971.4">Very nice. </span><span class="koboSpan" id="kobo.971.5">So these chains, these steps into the RAG application or any other LLM application, you can have many steps happening, right? </span><span class="koboSpan" id="kobo.971.6">So you need this chain to help them orchestrate"</span></pre></li>			</ol>
			<p class="callout-heading"><span class="koboSpan" id="kobo.972.1">Is the transcription slow?</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.973.1">Depending on the model size and your hardware capabilities, the transcription process might take </span><span class="No-Break"><span class="koboSpan" id="kobo.974.1">some time.</span></span></p>
			<p><span class="koboSpan" id="kobo.975.1">As we</span><a id="_idIndexMarker1177"/><span class="koboSpan" id="kobo.976.1"> can see, the transcription is excellent. </span><span class="koboSpan" id="kobo.976.2">Now, in the use case we are dealing with, after transcribing the YouTube video, there are several valuable actions you can take on this project. </span><span class="koboSpan" id="kobo.976.3">First, you can create captions or subtitles to improve accessibility for viewers who are deaf or hard of hearing. </span><span class="koboSpan" id="kobo.976.4">Additionally, writing a summary or extracting key points can help viewers grasp the main ideas without watching the entire video. </span><span class="koboSpan" id="kobo.976.5">The transcription can also be transformed into a blog post or article, providing more context on the topic discussed. </span><span class="koboSpan" id="kobo.976.6">Extracting quotes or highlights from the transcription allows you to create engaging social media posts that promote the video. </span><span class="koboSpan" id="kobo.976.7">Utilizing the transcription for SEO purposes can improve the video’s search engine ranking by including relevant keywords in the description. </span><span class="koboSpan" id="kobo.976.8">You can also develop FAQs or discussion questions based on the video to encourage viewer engagement. </span><span class="koboSpan" id="kobo.976.9">Additionally, the transcription can serve as a reference for research, and you might consider adapting it into a script for an audiobook or podcast. </span><span class="koboSpan" id="kobo.976.10">Incorporating the transcription into educational materials, such as lesson plans, is another effective way to utilize the content. </span><span class="koboSpan" id="kobo.976.11">Lastly, you can create visual summaries or infographics based on the key points to present the main ideas visually. </span><span class="koboSpan" id="kobo.976.12">How cool </span><span class="No-Break"><span class="koboSpan" id="kobo.977.1">is that?</span></span></p>
			<p><span class="koboSpan" id="kobo.978.1">In the following section, we will expand the use case and do some emotion detection from the </span><span class="No-Break"><span class="koboSpan" id="kobo.979.1">transcribed text.</span></span></p>
			<h3><span class="koboSpan" id="kobo.980.1">Detecting emotions</span></h3>
			<p><span class="koboSpan" id="kobo.981.1">Emotion </span><a id="_idIndexMarker1178"/><span class="koboSpan" id="kobo.982.1">detection from text, often referred to as sentiment analysis or emotion recognition, is a subfield of </span><strong class="bold"><span class="koboSpan" id="kobo.983.1">natural language processing</span></strong><span class="koboSpan" id="kobo.984.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.985.1">NLP</span></strong><span class="koboSpan" id="kobo.986.1">) that</span><a id="_idIndexMarker1179"/><span class="koboSpan" id="kobo.987.1"> focuses on identifying and classifying emotions conveyed in written content. </span><span class="koboSpan" id="kobo.987.2">This area of study has gained significant traction due to the growing amount of textual data generated across social media, customer feedback, and </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">other platforms.</span></span></p>
			<p><span class="koboSpan" id="kobo.989.1">In our case, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.990.1">j-hartmann/emotion-english-distilroberta-base</span></strong><span class="koboSpan" id="kobo.991.1"> model, built upon the DistilRoBERTa architecture. </span><span class="koboSpan" id="kobo.991.2">The DistilRoBERTa model is a smaller and faster variant of the RoBERTa model, which itself is based on the Transformer architecture. </span><span class="koboSpan" id="kobo.991.3">This model is specifically fine-tuned for emotion detection tasks. </span><span class="koboSpan" id="kobo.991.4">It has been</span><a id="_idIndexMarker1180"/><span class="koboSpan" id="kobo.992.1"> trained on a dataset designed to recognize various emotions expressed in text, making it adept at identifying and classifying emotions from written content. </span><span class="koboSpan" id="kobo.992.2">It is designed to detect the following emotions </span><span class="No-Break"><span class="koboSpan" id="kobo.993.1">from text:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.994.1">Joy</span></strong><span class="koboSpan" id="kobo.995.1">: This represents happiness </span><span class="No-Break"><span class="koboSpan" id="kobo.996.1">and positivity</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.997.1">Sadness</span></strong><span class="koboSpan" id="kobo.998.1">: This reflects feelings of sorrow </span><span class="No-Break"><span class="koboSpan" id="kobo.999.1">and unhappiness</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1000.1">Anger</span></strong><span class="koboSpan" id="kobo.1001.1">: This indicates feelings of frustration, annoyance, </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">or rage</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1003.1">Fear</span></strong><span class="koboSpan" id="kobo.1004.1">: This conveys feelings of anxiety </span><span class="No-Break"><span class="koboSpan" id="kobo.1005.1">or apprehension</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1006.1">Surprise</span></strong><span class="koboSpan" id="kobo.1007.1">: This represents astonishment </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">or unexpectedness</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1009.1">Disgust</span></strong><span class="koboSpan" id="kobo.1010.1">: This reflects feelings of aversion </span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">or distaste</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1012.1">Neutral</span></strong><span class="koboSpan" id="kobo.1013.1">: This indicates a lack of strong emotion </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">or feeling</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1015.1">These emotions are typically derived from various datasets that categorize text based on emotional expressions, allowing the model to classify input text into these </span><span class="No-Break"><span class="koboSpan" id="kobo.1016.1">predefined categories.</span></span></p>
			<p><span class="koboSpan" id="kobo.1017.1">Let’s have a look at the code, which is also available </span><span class="No-Break"><span class="koboSpan" id="kobo.1018.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/6.emotion_detection.py"><span class="No-Break"><span class="koboSpan" id="kobo.1019.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/6.emotion_detection.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1020.1">.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1021.1">Memory check</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1022.1">The following code is memory intensive, so you may need to allocate more memory if working on virtual machines or Google Collab. </span><span class="koboSpan" id="kobo.1022.2">The code was tested on Mac M1, 16 </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">GB memory.</span></span></p>
			<p><span class="koboSpan" id="kobo.1024.1">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1025.1">start coding:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1026.1">We first import the libraries required for </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">this example:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1028.1">
import torch
import pandas as pd
from transformers import WhisperProcessor, WhisperForConditionalGeneration, AutoModelForSequenceClassification, AutoTokenizer
import librosa
import numpy as np</span></pre></li>				<li><span class="koboSpan" id="kobo.1029.1">We then</span><a id="_idIndexMarker1181"/><span class="koboSpan" id="kobo.1030.1"> load the Whisper processor and model from </span><span class="No-Break"><span class="koboSpan" id="kobo.1031.1">Hugging Face:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1032.1">
whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-large-v2")
whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v2")</span></pre></li>				<li><span class="koboSpan" id="kobo.1033.1">Then, we load the emotion detection processor and model from </span><span class="No-Break"><span class="koboSpan" id="kobo.1034.1">Hugging Face:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1035.1">
emotion_model_name = "j-hartmann/emotion-english-distilroberta-base"
emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_name)
emotion_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_name)</span></pre></li>				<li><span class="koboSpan" id="kobo.1036.1">We define the path to your </span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">audio file:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1038.1">
audio_path = "chapter13/audio/3.chain orchestrator.mp3" # Replace with your actual audio file path</span></pre></li>				<li><span class="koboSpan" id="kobo.1039.1">Once the path is defined, we load the </span><span class="No-Break"><span class="koboSpan" id="kobo.1040.1">audio file:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1041.1">
audio, rate = librosa.load(audio_path, sr=16000)</span></pre></li>				<li><span class="koboSpan" id="kobo.1042.1">We create a function called </span><strong class="source-inline"><span class="koboSpan" id="kobo.1043.1">split_audio</span></strong><span class="koboSpan" id="kobo.1044.1"> to split audio </span><span class="No-Break"><span class="koboSpan" id="kobo.1045.1">into chunks:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1046.1">
def split_audio(audio, rate, chunk_duration=30):
    chunk_length = int(rate * chunk_duration)
    num_chunks = int(np.ceil(len(audio)/chunk_length))
    return [audio[i*chunk_length:(i+1)*chunk_length] for i in range(num_chunks)]</span></pre></li>				<li><span class="koboSpan" id="kobo.1047.1">We also </span><a id="_idIndexMarker1182"/><span class="koboSpan" id="kobo.1048.1">create a function to transcribe audio </span><span class="No-Break"><span class="koboSpan" id="kobo.1049.1">using Whisper:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1050.1">
def transcribe_audio(audio_chunk, rate):
    input_features = whisper_processor(audio_chunk, sampling_rate=rate, return_tensors="pt").input_features
    with torch.no_grad():
        predicted_ids = whisper_model.generate(input_features)
    transcription = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1051.1">The function preprocesses the audio file for the Whisper model and generates the transcription. </span><span class="koboSpan" id="kobo.1051.2">Once it’s generated, the function decodes the </span><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">generated transcription.</span></span></p></li>				<li><span class="koboSpan" id="kobo.1053.1">We then create a function to detect emotions from text using the emotion </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">detection model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1055.1">
def detect_emotion(text):
    inputs = emotion_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    outputs = emotion_model(inputs)
    predicted_class_id = torch.argmax(outputs.logits, dim=-1).item()
    emotions = emotion_model.config.id2label
    return emotions[predicted_class_id]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1056.1">This</span><a id="_idIndexMarker1183"/><span class="koboSpan" id="kobo.1057.1"> function begins by tokenizing the input text with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1058.1">emotion_tokenizer</span></strong><span class="koboSpan" id="kobo.1059.1">, converting it into PyTorch tensors while handling padding, truncation, and maximum length constraints. </span><span class="koboSpan" id="kobo.1059.2">The tokenized input is then fed into </span><strong class="source-inline"><span class="koboSpan" id="kobo.1060.1">emotion_model</span></strong><span class="koboSpan" id="kobo.1061.1">, which generates raw prediction scores (logits) for various emotion classes. </span><span class="koboSpan" id="kobo.1061.2">The function identifies the emotion with the highest score using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1062.1">torch.argmax</span></strong><span class="koboSpan" id="kobo.1063.1"> to determine the class ID. </span><span class="koboSpan" id="kobo.1063.2">This ID is then mapped to the corresponding emotion label through the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1064.1">id2label</span></strong><span class="koboSpan" id="kobo.1065.1"> dictionary provided by the model’s configuration. </span><span class="koboSpan" id="kobo.1065.2">Finally, the function returns the detected emotion as a </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">readable label!</span></span></p></li>				<li><span class="koboSpan" id="kobo.1067.1">Then, we split the audio </span><span class="No-Break"><span class="koboSpan" id="kobo.1068.1">into chunks:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1069.1">
audio_chunks = split_audio(audio, rate, chunk_duration=30) # 30-second chunks</span></pre></li>				<li><span class="koboSpan" id="kobo.1070.1">We also create a DataFrame to store </span><span class="No-Break"><span class="koboSpan" id="kobo.1071.1">the results:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1072.1">
df = pd.DataFrame(columns=['Chunk Index', 'Transcription', 'Emotion'])</span></pre></li>				<li><span class="koboSpan" id="kobo.1073.1">Finally, we process each </span><span class="No-Break"><span class="koboSpan" id="kobo.1074.1">audio chunk:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1075.1">
for i, audio_chunk in enumerate(audio_chunks):
    transcription = transcribe_audio(audio_chunk,rate)
    emotion = detect_emotion(transcription)
    # Append results to DataFrame
    df.loc[i] = [i, transcription, emotion]</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1076.1">The output emotions are shown for each chunk of transcribed text, and in our case, all are neutral, as the video is just a teaching </span><span class="No-Break"><span class="koboSpan" id="kobo.1077.1">concept video:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1078.1">
   Chunk Index  Emotion
0            0  neutral
1            1  neutral
2            2  neutral</span></pre>			<p><span class="koboSpan" id="kobo.1079.1">Now, we will expand our </span><a id="_idIndexMarker1184"/><span class="koboSpan" id="kobo.1080.1">use case a bit further to demonstrate how you can take the transcribed text and pass it through an LLM to create highlights for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1081.1">YouTube video.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1082.1">Automatically creating video highlights</span></h3>
			<p><span class="koboSpan" id="kobo.1083.1">In the era of </span><a id="_idIndexMarker1185"/><span class="koboSpan" id="kobo.1084.1">digital content consumption, viewers often seek concise and engaging summaries of longer videos. </span><span class="koboSpan" id="kobo.1084.2">Automatically creating video highlights involves analyzing video content and extracting key moments that capture the essence of the material. </span><span class="koboSpan" id="kobo.1084.3">This process saves time and improves content accessibility, making it a valuable tool for educators, marketers, and entertainment </span><span class="No-Break"><span class="koboSpan" id="kobo.1085.1">providers alike.</span></span></p>
			<p><span class="koboSpan" id="kobo.1086.1">Let’s have a look at the code. </span><span class="koboSpan" id="kobo.1086.2">You can find it at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">link: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/7.write_highlights.py"><span class="No-Break"><span class="koboSpan" id="kobo.1088.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/7.write_highlights.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1089.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.1090.1">In this code, we will expand the Whisper example. </span><span class="koboSpan" id="kobo.1090.2">We will transcribe the text, then join all the transcribed chunks together, and finally, we will pass all the transcriptions to the LLM to create the highlights for the entire video. </span><span class="koboSpan" id="kobo.1090.3">Let’s continue the </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">previous example:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1092.1">We start by initializing the Hugging </span><span class="No-Break"><span class="koboSpan" id="kobo.1093.1">Face model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1094.1">
model_name = "mistralai/Mistral-Nemo-Instruct-2407" # Using Mistral for instruction-following</span></pre></li>				<li><span class="koboSpan" id="kobo.1095.1">Then, we add your Hugging Face </span><span class="No-Break"><span class="koboSpan" id="kobo.1096.1">API token:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1097.1">
api_token = "" # Replace with your actual API token</span></pre></li>				<li><span class="koboSpan" id="kobo.1098.1">Here’s the LangChain setup that we’ll be using in this use case. </span><span class="koboSpan" id="kobo.1098.2">Notice the new prompt that </span><span class="No-Break"><span class="koboSpan" id="kobo.1099.1">we added:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1100.1">
prompt_template = PromptTemplate(
    input_variables=["text"],
    template='''</span><strong class="bold"><span class="koboSpan" id="kobo.1101.1">This is the transcribed text from a YouTube video. </span><span class="koboSpan" id="kobo.1101.2">Write the key highlights from this video in bullet format.</span></strong><span class="koboSpan" id="kobo.1102.1">
    {text}
    Output:
    '''
    )
huggingface_llm = HuggingFaceHub(repo_id=model_name, huggingfacehub_api_token=api_token, model_kwargs={"task": "text-generation"})
llm_chain = LLMChain(prompt=prompt_template, llm=huggingface_llm)</span></pre></li>				<li><span class="koboSpan" id="kobo.1103.1">Next, we</span><a id="_idIndexMarker1186"/><span class="koboSpan" id="kobo.1104.1"> generate </span><span class="No-Break"><span class="koboSpan" id="kobo.1105.1">the transcription:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1106.1">
def transcribe_audio(audio_chunk, rate):
    input_features = whisper_processor(audio_chunk,
        sampling_rate=rate,
        return_tensors="pt").input_features
    with torch.no_grad():
        predicted_ids = \
            whisper_model.generate(input_features)
    transcription = whisper_processor.batch_decode(
        predicted_ids, skip_special_tokens=True)[0]
    return transcription</span></pre></li>				<li><span class="koboSpan" id="kobo.1107.1">Then, we create a function to generate the key highlights from text using </span><span class="No-Break"><span class="koboSpan" id="kobo.1108.1">the LLM:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1109.1">
def generate_highlights(text):
    try:
        response = llm_chain.run(text)
        return response.strip() # Clean up any whitespace around the response
    except Exception as e:
        print(f"Error generating highlights: {e}")
        return "error" # Handle errors gracefully</span></pre></li>				<li><span class="koboSpan" id="kobo.1110.1">Next, we split the audio </span><span class="No-Break"><span class="koboSpan" id="kobo.1111.1">into chunks:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1112.1">
audio_chunks = split_audio(audio, rate, chunk_duration=30) # 30-second chunks</span></pre></li>				<li><span class="koboSpan" id="kobo.1113.1">We then </span><a id="_idIndexMarker1187"/><span class="koboSpan" id="kobo.1114.1">transcribe each </span><span class="No-Break"><span class="koboSpan" id="kobo.1115.1">audio chunk:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1116.1">
transcriptions = [transcribe_audio(chunk, rate) for chunk in audio_chunks]</span></pre></li>				<li><span class="koboSpan" id="kobo.1117.1">Then, we join all transcriptions into a </span><span class="No-Break"><span class="koboSpan" id="kobo.1118.1">single text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1119.1">
full_transcription = " ".join(transcriptions)</span></pre></li>				<li><span class="koboSpan" id="kobo.1120.1">Finally, we generate highlights from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1121.1">full transcription:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1122.1">
highlights = generate_highlights(full_transcription)</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1123.1">Let’s see the automatically </span><span class="No-Break"><span class="koboSpan" id="kobo.1124.1">created highlights:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1125.1">
Chain Orchestrator: Required to coordinate all steps in a LLM (Large Language Model) application, such as RAG (Retrieval-Augmented Generation).
</span><span class="koboSpan" id="kobo.1125.2">Popular Open Source Packages: Lama Index and LangChain are recommended for this purpose.
</span><span class="koboSpan" id="kobo.1125.3">Modularization: Chains allow for modularization of the process, making it easier to update or change components like LMs or vector stores without rebuilding the entire application.
</span><span class="koboSpan" id="kobo.1125.4">Rapid Advancements in </span><strong class="bold"><span class="koboSpan" id="kobo.1126.1">JNNIA</span></strong></pre>			<p><span class="koboSpan" id="kobo.1127.1">As we can see, there</span><a id="_idIndexMarker1188"/><span class="koboSpan" id="kobo.1128.1"> are some minor mistakes, mainly coming from the Whisper process, but other than that, it is actually </span><span class="No-Break"><span class="koboSpan" id="kobo.1129.1">pretty good.</span></span></p>
			<p><span class="koboSpan" id="kobo.1130.1">In the next part, we will quickly review the research happening in the audio space, as it is a rapidly </span><span class="No-Break"><span class="koboSpan" id="kobo.1131.1">evolving field.</span></span></p>
			<h2 id="_idParaDest-281"><a id="_idTextAnchor319"/><span class="koboSpan" id="kobo.1132.1">Future research in audio preprocessing</span></h2>
			<p><span class="koboSpan" id="kobo.1133.1">There is a </span><a id="_idIndexMarker1189"/><span class="koboSpan" id="kobo.1134.1">growing trend toward the development of multimodal LLMs capable of processing various types of data, including audio. </span><span class="koboSpan" id="kobo.1134.2">Currently, many language models are primarily text-based, but we anticipate the emergence of models that can handle text, images, and audio simultaneously. </span><span class="koboSpan" id="kobo.1134.3">These multimodal LLMs have diverse applications, such as generating image captions and providing medical diagnoses based on patient reports. </span><span class="koboSpan" id="kobo.1134.4">Research is underway to extend LLMs to support direct speech inputs. </span><span class="koboSpan" id="kobo.1134.5">As noted, “Several studies have attempted to extend LLMs to support direct speech inputs with a connection module” (</span><a href="https://arxiv.org/html/2406.07914v2"><span class="koboSpan" id="kobo.1135.1">https://arxiv.org/html/2406.07914v2</span></a><span class="koboSpan" id="kobo.1136.1">), indicating ongoing efforts to incorporate audio processing capabilities into LLMs. </span><span class="koboSpan" id="kobo.1136.2">Although not only relevant to audio, LLMs face several challenges with other data types, including </span><span class="No-Break"><span class="koboSpan" id="kobo.1137.1">the following:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.1138.1">High computational resources required </span><span class="No-Break"><span class="koboSpan" id="kobo.1139.1">for processing</span></span></li>
				<li><span class="koboSpan" id="kobo.1140.1">Data privacy and </span><span class="No-Break"><span class="koboSpan" id="kobo.1141.1">security concerns</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1142.1">Researchers are actively exploring various strategies to overcome these challenges. </span><span class="koboSpan" id="kobo.1142.2">To address the high computational demands, there is a focus on developing more efficient algorithms and architectures, such as transformer models with reduced parameter sizes and optimized training techniques. </span><span class="koboSpan" id="kobo.1142.3">Techniques such as model compression, quantization, and distillation are being employed to make these models more resource-efficient without sacrificing performance (</span><a href="https://arxiv.org/abs/2401.13601"><span class="koboSpan" id="kobo.1143.1">https://arxiv.org/abs/2401.13601</span></a><span class="koboSpan" id="kobo.1144.1">, </span><a href="https://arxiv.org/html/2408.04275v1"><span class="koboSpan" id="kobo.1145.1">https://arxiv.org/html/2408.04275v1</span></a><span class="koboSpan" id="kobo.1146.1">, </span><a href="https://arxiv.org/html/2408.01319v1"><span class="koboSpan" id="kobo.1147.1">https://arxiv.org/html/2408.01319v1</span></a><span class="koboSpan" id="kobo.1148.1">). </span><span class="koboSpan" id="kobo.1148.2">In terms of data privacy and security, researchers are investigating privacy-preserving ML techniques, including federated learning and differential privacy. </span><span class="koboSpan" id="kobo.1148.3">These approaches aim to protect sensitive data by allowing models to learn from decentralized data sources without exposing individual data points. </span><span class="koboSpan" id="kobo.1148.4">Additionally, advancements in encryption and secure multi-party computation are being integrated to ensure that data remains confidential</span><a id="_idIndexMarker1190"/><span class="koboSpan" id="kobo.1149.1"> throughout the processing pipeline. </span><span class="koboSpan" id="kobo.1149.2">These efforts are crucial for enabling the widespread adoption of multimodal LLMs across various domains while ensuring they remain efficient and secure (</span><a href="https://towardsdatascience.com/differential-privacy-and-federated-learning-for-medical-data-0f2437d6ece9"><span class="koboSpan" id="kobo.1150.1">https://towardsdatascience.com/differential-privacy-and-federated-learning-for-medical-data-0f2437d6ece9</span></a><span class="koboSpan" id="kobo.1151.1">, </span><a href="https://arxiv.org/pdf/2403.05156"><span class="No-Break"><span class="koboSpan" id="kobo.1152.1">https://arxiv.org/pdf/2403.05156</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1153.1">, </span></span><a href="https://pair.withgoogle.com/explorables/federated-learning/"><span class="No-Break"><span class="koboSpan" id="kobo.1154.1">https://pair.withgoogle.com/explorables/federated-learning/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">).</span></span></p>
			<p><span class="koboSpan" id="kobo.1156.1">Let’s now summarize the learnings from </span><span class="No-Break"><span class="koboSpan" id="kobo.1157.1">this chapter.</span></span></p>
			<h1 id="_idParaDest-282"><a id="_idTextAnchor320"/><span class="koboSpan" id="kobo.1158.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.1159.1">In this chapter, we covered various image processing techniques, such as loading, resizing, normalizing, and standardizing images to prepare them for ML applications. </span><span class="koboSpan" id="kobo.1159.2">We implemented augmentation to generate diverse variations for improved model generalization and applied noise removal to enhance image quality. </span><span class="koboSpan" id="kobo.1159.3">We also examined the use of OCR for text extraction from images, particularly addressing the challenges presented by thumbnails. </span><span class="koboSpan" id="kobo.1159.4">Additionally, we explored the BLIP model’s capability to generate captions based on visual content. </span><span class="koboSpan" id="kobo.1159.5">Furthermore, we discussed video processing techniques involving frame extraction and key </span><span class="No-Break"><span class="koboSpan" id="kobo.1160.1">moment analysis.</span></span></p>
			<p><span class="koboSpan" id="kobo.1161.1">Finally, we introduced the Whisper model, highlighting its effectiveness in converting audio to text and its automatic speech recognition capabilities across </span><span class="No-Break"><span class="koboSpan" id="kobo.1162.1">multiple languages.</span></span></p>
			<h1 id="_idParaDest-283"><a id="_idTextAnchor321"/><span class="koboSpan" id="kobo.1163.1">This concludes the book! </span><span class="koboSpan" id="kobo.1163.2">You did it!</span></h1>
			<p><span class="koboSpan" id="kobo.1164.1">I want to express my sincere gratitude for your dedication to finishing this book. </span><span class="koboSpan" id="kobo.1164.2">I’ve aimed to share the insights from my experience, with a focus on ML and AI, as these fields have been central to my career. </span><span class="koboSpan" id="kobo.1164.3">I find them incredibly fascinating and transformative, though I might be a </span><span class="No-Break"><span class="koboSpan" id="kobo.1165.1">bit biased.</span></span></p>
			<p><span class="koboSpan" id="kobo.1166.1">As you’ve seen in the later chapters, I believe LLMs are poised to revolutionize the field and the way we process data. </span><span class="koboSpan" id="kobo.1166.2">That’s why I dedicated the last chapters to building a foundation and showcasing how effortlessly different types of data can be transformed and manipulated </span><span class="No-Break"><span class="koboSpan" id="kobo.1167.1">using LLMs.</span></span></p>
			<p><span class="koboSpan" id="kobo.1168.1">Please take my advice and spend some time diving into the code examples provided. </span><span class="koboSpan" id="kobo.1168.2">Implement these techniques in your daily tasks and projects. </span><span class="koboSpan" id="kobo.1168.3">If there’s anything you find yourself doing manually or redoing frequently, </span><em class="italic"><span class="koboSpan" id="kobo.1169.1">code it up</span></em><span class="koboSpan" id="kobo.1170.1"> to streamline your process. </span><span class="koboSpan" id="kobo.1170.2">This hands-on practice will help reinforce your learning. </span><span class="koboSpan" id="kobo.1170.3">Experiment with the techniques and concepts from this book on your own projects, as real growth occurs when you adapt and innovate with these tools in </span><span class="No-Break"><span class="koboSpan" id="kobo.1171.1">practical scenarios.</span></span></p>
			<p><span class="koboSpan" id="kobo.1172.1">I’m traveling the world speaking at conferences and running workshops. </span><span class="koboSpan" id="kobo.1172.2">If you see me at one of these events, don’t hesitate to say hello and talk to me! </span><span class="koboSpan" id="kobo.1172.3">Who knows, our paths might cross at one of these events! </span><span class="koboSpan" id="kobo.1172.4">In any case, I’d also love to hear about your progress and see what you’ve learned and built. </span><span class="koboSpan" id="kobo.1172.5">Feel free to share your experiences with me—your insights and developments are always exciting to see. </span><span class="koboSpan" id="kobo.1172.6">So, let’s stay connected! </span><span class="koboSpan" id="kobo.1172.7">Connect with me on LinkedIn (</span><a href="https://www.linkedin.com/in/maria-zervou-533222107/"><span class="koboSpan" id="kobo.1173.1">https://www.linkedin.com/in/maria-zervou-533222107/</span></a><span class="koboSpan" id="kobo.1174.1">) and you can subscribe to my YouTube channel (</span><a href="https://www.youtube.com/channel/UCY2Z8Sc2L0wQnTOQPlLzUQw"><span class="koboSpan" id="kobo.1175.1">https://www.youtube.com/channel/UCY2Z8Sc2L0wQnTOQPlLzUQw</span></a><span class="koboSpan" id="kobo.1176.1">) for ongoing tutorials and content to keep you informed about new developments and techniques in ML </span><span class="No-Break"><span class="koboSpan" id="kobo.1177.1">and AI.</span></span></p>
			<p><span class="koboSpan" id="kobo.1178.1">Thank you once again for your time and effort. </span><span class="koboSpan" id="kobo.1178.2">Remember, learning is just the beginning; real growth comes from practicing and applying your knowledge. </span><span class="koboSpan" id="kobo.1178.3">I’m excited to see where your journey leads you and hope our paths cross again in </span><span class="No-Break"><span class="koboSpan" id="kobo.1179.1">the future.</span></span></p>
		</div>
	</body></html>