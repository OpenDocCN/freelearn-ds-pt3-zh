<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer030">
			<h1 id="_idParaDest-75"><a id="_idTextAnchor075"/>Chapter 4: Real-Time Data Analytics</h1>
			<p>In the modern big data world, data is being generated at a tremendous pace, that is, faster than any of the past decade's technologies can handle, such as batch processing ETL tools, data warehouses, or business analytics systems. It is essential to process data and draw insights in real time for businesses to make tactical decisions that help them to stay competitive. Therefore, there is a need for real-time analytics systems that can process data in real or near real-time and help end users get to the latest data as quickly as possible.</p>
			<p>In this chapter, you will explore the architecture and components of a real-time big data analytics processing system, including message queues as data sources, Delta as the data sink, and Spark's Structured Streaming as the stream processing engine. You will learn techniques to handle late-arriving data using stateful processing Structured Streaming. The techniques for maintaining an exact replica of source systems in a data lake using <strong class="bold">Change Data Capture</strong> (<strong class="bold">CDC</strong>) will also be presented. You will learn how to build multi-hop stream processing pipelines to progressively improve data quality from raw data to cleansed and enriched data that is ready for data analytics. You will gain the essential skills to implement a scalable, fault-tolerant, and near real-time analytics system using Apache Spark.</p>
			<p>In this chapter, we will cover the following main topics:</p>
			<ul>
				<li>Real-time analytics systems architecture </li>
				<li>Stream processing engines </li>
				<li>Real-time analytics industry use cases</li>
				<li>Simplifying the Lambda Architecture using Delta Lake</li>
				<li>CDC</li>
				<li>Multi-hop streaming pipelines</li>
			</ul>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor076"/>Technical requirements</h1>
			<p>In this chapter, you will be using the Databricks Community Edition to run your code. This can be found at <a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a>:</p>
			<ul>
				<li>Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. </li>
				<li>The code and data used in this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter04">https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter04</a>.</li>
			</ul>
			<p>Before we dive deeper into implementing real-time stream processing data pipelines with Apache Spark, first, we need to understand the general architecture of a real-time analytics pipeline and its various components, as described in the following section.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor077"/>Real-time analytics systems architecture</h1>
			<p>A real-time data analytics<a id="_idIndexMarker329"/> system, as the name suggests, processes data in real time. This is because it is generated at the source, making it available<a id="_idIndexMarker330"/> for business users with the minimal latency possible. It consists of several important components, namely, streaming data sources, a stream processing engine, streaming data sinks, and the actual real-time data consumers, as illustrated in the following diagram:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="Images/B16736_04_01.jpg" alt="Figure 4.1 – Real-time data analytics&#13;&#10;" width="1623" height="799"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Real-time data analytics</p>
			<p>The preceding diagram depicts a typical real-time data analytics systems architecture. In the following sections, we will explore each of the components in more detail.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor078"/>Streaming data sources</h2>
			<p>Similar to any of the other <strong class="bold">enterprise decision support Systems</strong>, a <strong class="bold">real-time data analytics system</strong> also starts<a id="_idIndexMarker331"/> with data sources. Businesses generate data continuously<a id="_idIndexMarker332"/> in real time; therefore, any data source<a id="_idIndexMarker333"/> used by a batch processing system is also a streaming<a id="_idIndexMarker334"/> data source. The only difference is in how often you ingest data from the data source. In batch processing mode, data is ingested periodically, whereas, in a real-time streaming system, data is continuously ingested from the same data source. However, there are a few considerations to bear in mind before continuously ingesting data from a data source. These can be described as follows:</p>
			<ul>
				<li>Can the data source keep up with the demands of a real-time streaming analytics engine? Or will the streaming engine end up taxing the data source?</li>
				<li>Can the data source communicate with the streaming engine asynchronously and replay events in any arbitrary order that the streaming engine requires?</li>
				<li>Can the data source replay events in the exact order that they occurred at the source?</li>
			</ul>
			<p>The preceding three points<a id="_idIndexMarker335"/> bring up some important requirements<a id="_idIndexMarker336"/> regarding streaming data sources. A streaming data source should be distributed and scalable in order to keep up with the demands of a real-time streaming analytics system. Note that it must be able to replay events in any arbitrary order. This is so that the streaming engine has the flexibility to process events in any order or restart the process in the case of any failures. For certain real-time use cases, such as CDC, it is important that you replay events in the exact same order they occurred at the source in order to maintain data integrity.</p>
			<p>Due to the previously mentioned reasons, no operating system is fit to be a streaming data source. In the cloud and big data works, it is recommended that you use a scalable, fault-tolerant, and asynchronous message queue such as Apache Kafka, AWS Kinesis, Google Pub/Sub, or Azure Event Hub. Cloud-based data lakes such as AWS S3, Azure Blob, and ADLS storage or Google Cloud Storage are also suitable as streaming data sources to a certain extent for certain use cases.</p>
			<p>Now that we have an understanding of streaming data sources, let's take a look at how to ingest data from a data source such as a data lake in a streaming fashion, as shown in the flowing code snippet:</p>
			<p class="source-code">stream_df = (spark.readStream</p>
			<p class="source-code">                    .format("csv")</p>
			<p class="source-code">                    .option("header", "true")</p>
			<p class="source-code">                    .schema(eventSchema)</p>
			<p class="source-code">                    .option("maxFilesPerTrigger", 1)</p>
			<p class="source-code">                    .load("/FileStore/shared_uploads/online_retail/"))</p>
			<p>In the previous code, we define a streaming DataFrame that reads one file at a time from a data lake location. The <strong class="source-inline">readStream()</strong> method of the <strong class="source-inline">DataStreamReader</strong> object is used to create the streaming DataFrame. The data format is specified as CSV, and the schema information is defined using the <strong class="source-inline">eventSchema</strong> object. Finally, the location of the CSV files within the data lake is specified using the <strong class="source-inline">load()</strong> function. The <strong class="source-inline">maxFilesPerTrigger</strong> option specifies that only one file must be read by the stream at a time. This is useful for throttling the rate of stream processing, if required, because of the compute resource constraints.</p>
			<p>Once we have the streaming<a id="_idIndexMarker337"/> DataFrame created, it can be further<a id="_idIndexMarker338"/> processed using any of the available functions in the DataFrame API and persisted to a streaming data sink, such as a data lake. We will cover this in the following section.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor079"/>Streaming data sinks</h2>
			<p>Once data streams<a id="_idIndexMarker339"/> are read from their respective streaming sources and processed, they need to be stored onto some form<a id="_idIndexMarker340"/> of persistent storage for further downstream consumption. Although any regular data sink could act as a streaming data sink, a number of considerations apply when choosing a streaming data sink. Some of these considerations include the following:</p>
			<ul>
				<li>What are the latency requirements for data consumption?</li>
				<li>What kind of data will consumers be consuming in the data stream?</li>
			</ul>
			<p>Latency is an important factor when choosing the streaming data source, the data sink, and the actual streaming engine. Depending on the latency requirements, you might need to choose an entirely different end-to-end streaming architecture. Streaming use cases can be classified into two broad categories, depending on the latency requirements:</p>
			<ul>
				<li>Real-time transactional systems</li>
				<li>Near real-time analytics systems</li>
			</ul>
			<h3>Real-time transactional systems</h3>
			<p>Real-time transactional systems are operational<a id="_idIndexMarker341"/> systems that are, typically, interested<a id="_idIndexMarker342"/> in processing events pertaining to a single entity or transaction at a time. Let's consider an example of an online retail business where a customer visits an e-tailer's and browses through a few product categories in a given session. An operational system would be focused on capturing all of the events of that particular session and maybe display a discount coupon or make a specific recommendation to that user in real time. In this kind of scenario, the latency requirement is ultra-low and, usually, ranges in sub-seconds. These kinds of use cases require an ultra-low latency streaming<a id="_idIndexMarker343"/> engine along with an ultra-low latency<a id="_idIndexMarker344"/> streaming sink such as an in-memory database, such as <strong class="bold">Redis</strong> or <strong class="bold">Memcached</strong>, for instance.</p>
			<p>Another example<a id="_idIndexMarker345"/> of a real-time transactional use case would be a CRM system where a customer service representative is trying to make an upsell or cross-sell recommendation to a live customer online. Here, the streaming engine needs to fetch certain precalculated metrics for a specific customer from a data store, which contains information about millions of customers. It also needs to fetch some real-time data points from the CRM system itself in order to generate a personalized recommendation for that specific customer. All of this needs to happen in a matter of seconds. A <strong class="bold">NoSQL</strong> database with constant<a id="_idIndexMarker346"/> seek times for arbitrary key records would be a good fit for the data source as well as a data sink to quickly fetch records for a particular key such as <strong class="source-inline">CustomerID</strong>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Spark's Structured Streaming, with its micro-batch style of stream processing model, isn't well suited for real-time stream processing use cases where there is an ultra-low latency requirement to process events as they happen at the source. Structured Streaming has been designed for maximum throughput and scalability rather than for ultra-low latency. Apache Flink or another streaming engine that was purpose-built would be a good fit for real-time transactional use cases.</p>
			<p>Now that you have gained<a id="_idIndexMarker347"/> an understanding of real-time analytics engines along with an example of a real-time analytics use case, in the following section, we will take a look at a more prominent and practical way of processing analytics in near real time.</p>
			<h3>Near real-time analytics systems</h3>
			<p>Near real-time analytics systems<a id="_idIndexMarker348"/> are analytics systems that process<a id="_idIndexMarker349"/> an aggregate of records in near real time and have a latency requirement ranging from a few seconds to a few minutes. These systems are not interested in processing events for a single entity or transaction but generate metrics or KPIs for an aggregate of transactions to depict the state of business in real time. Sometimes, these systems might also generate sessions of events for a single transaction or entity but for later offline consumption.</p>
			<p>Since this type of real-time analytics system<a id="_idIndexMarker350"/> processes a very large volume<a id="_idIndexMarker351"/> of data, throughput and scalability are of key importance here. Additionally, since the processed output is either being fed into a Business Intelligence system for real-time reporting or into persistent storage for consumption in an asynchronous manner, a data lake or a data warehouse is an ideal streaming data sink for this kind of use case. Examples of near real-time analytics use cases are presented, in detail, in the <em class="italic">Real-time analytics industry use cases</em> section. Apache Spark was designed to handle near real-time analytics use cases that require maximum throughput for large volumes of data with scalability.</p>
			<p>Now that you have an understanding of streaming data sources, data sinks, and the kind of real-time use cases that Spark's Structured Streaming is better suited to solve, let's take a deeper dive into the actual streaming engines.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor080"/>Stream processing engines</h1>
			<p>A stream processing engine<a id="_idIndexMarker352"/> is the most critical component of any real-time data analytics system. The role of the stream processing engine is to continuously process events from a streaming data source and ingest them into a streaming data sink. The stream processing engine can process events as they arrive in a real real-time fashion or group a subset of events into a small batch and process one micro-batch at a time in </p>
			<p>a near real-time manner. The choice of the engine greatly depends on the type of use case and the processing latency requirements. Some examples of modern streaming engines include Apache Storm, Apache Spark, Apache Flink, and Kafka Streams.</p>
			<p>Apache Spark comes with a stream processing engine called <strong class="bold">Structured Streaming</strong>, which is based on Spark's SQL engine<a id="_idIndexMarker353"/> and DataFrame APIs. Structured Streaming uses the micro-batch<a id="_idIndexMarker354"/> style of processing and treats each incoming micro-batch as a small Spark DataFrame. It applies DataFrame operations to each micro-batch just like any other Spark DataFrame. The programming model for Structured Streaming treats the output dataset as an unbounded table and processes incoming events as a stream of continuous micro-batches. Structured Streaming generates a query plan for each micro-batch, processes them, and then appends them to the output dataset, treating it just like an unbounded table, as illustrated in the following diagram:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="Images/B16736_04_02.jpg" alt="Figure 4.2 – The Structured Streaming programming model&#13;&#10;" width="430" height="223"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – The Structured Streaming programming model</p>
			<p>As shown in the preceding diagram, Structured Streaming treats each incoming micro-batch of data like a small Spark DataFrame and appends it to the end of an existing Streaming DataFrame. An elaboration of Structured Streaming's programming model, with examples, was presented in the <em class="italic">Ingesting data in real time using Structured Streaming</em> section of <a href="B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Ingestion</em>.</p>
			<p>Structured Streaming can simply process streaming events as they arrive in micro-batches and persist the output to a streaming data sink. However, in real-world scenarios, the simple model of stream processing might not be practical because of <strong class="bold">Late-Arriving Data</strong>. Structured Streaming<a id="_idIndexMarker355"/> also supports a stateful processing model to deal with data that is either arriving late or is out of order. You will learn more about handling late-arriving data in the <em class="italic">Handling late-arriving data</em> section.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor081"/>Real-time data consumers</h2>
			<p>The final component<a id="_idIndexMarker356"/> of a real-time data analytics system<a id="_idIndexMarker357"/> is the actual data consumer. Data consumers can be actual business users that consume real-time data by the means of ad hoc Spark SQL queries, via interactive operational dashboards or other systems that take the output of a streaming engine and further process it. Real-time business dashboards are consumed by business users, and typically, these have slightly higher latency requirements as a human mind can only comprehend data at a given rate. Structured Streaming is a good fit for these use cases and can write the streaming output to a database where it can be further fed into a Business Intelligence system.</p>
			<p>The output of a streaming engine can also be consumed by other business applications such as a mobile app or a web app. Here, the use case could be something such as hyper-personalized user recommendations, where the processed output of a streaming engine could be further passed on to something such as an online inference engine for generating personalized user recommendations. Structured Streaming can be used for these use cases as long as the latency requirements are in the range of a few seconds to a few minutes.</p>
			<p>In summary, real-time data analytics has a few important components such as the streaming data sources and sinks, the actual streaming engine, and the final real-time data consumers. The choice of data source, data sink, and the actual engine in your architecture depends on your actual real-time data consumers, the use case that is being solved, the processing latency, and the throughput requirements. Now, in the following section, we'll take a look at some examples of real-world industry use cases that leverage real-time data analytics.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor082"/>Real-time analytics industry use cases</h1>
			<p>There is an actual need for and an advantage<a id="_idIndexMarker358"/> to processing data in real time, so companies are quickly shifting from batch processing to real-time data processing. In this section, let's take a look at a few examples of real-time<a id="_idIndexMarker359"/> data analytics by industry verticals.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor083"/>Real-time predictive analytics in manufacturing</h2>
			<p>With the advent of the <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>), manufacturing and other industries are generating a high volume<a id="_idIndexMarker360"/> of IoT data from their machines and heavy equipment. This data<a id="_idIndexMarker361"/> can be leveraged in few different ways to improve the way industries work and help them to save costs. One such example is predictive maintenance, where IoT data is continuously ingested from industrial equipment and machinery, data science, and machine learning techniques that have been applied to the data to identify patterns that can predict equipment or part failures. When this process is performed in real time, it can help to predict equipment and part failures before they actually happen. In this way, maintenance can be performed proactively, preventing downtime and thus preventing any lost revenue or missed manufacturing targets.</p>
			<p>Another example is the construction industry where IoT data, such as equipment uptime, fuel consumption, and more, can be analyzed to identify any underutilized equipment and any equipment that can be redirected in real time for optimal utilization.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor084"/>Connected vehicles in the automotive sector</h2>
			<p>Modern vehicles come<a id="_idIndexMarker362"/> with a plethora of connected features that definitely make the life of a consumer much easier and more convenient. Vehicle telematics, as well as user data generated by such vehicles, can be used for a variety of use cases or to further provide convenience features for the end user, such as real-time personalized in-vehicle content and services, advanced navigation and route guidance, and remote monitoring. Telematics data can be used by the manufacturer to unlock use cases such as predicting a vehicle's maintenance window or part failure and proactively alerting ancillary vendors and dealerships. Predicting part failures and better managing vehicle recalls helps automotive manufacturers with huge costs.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor085"/>Financial fraud detection</h2>
			<p>Modern-day personal finance<a id="_idIndexMarker363"/> is rapidly moving from physical to digital, and with that comes the novel threat of digital financial threats such as fraud and identity theft. Therefore, there is a need for financial institutions to proactively assess millions of transactions in real time for fraud and to alert and protect the individual consumer of such fraud. Highly scalable, fault-tolerant real-time analytics systems are required to detect and prevent financial fraud at such a scale.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor086"/>IT security threat detection</h2>
			<p>Consumer electronics<a id="_idIndexMarker364"/> manufactures of online connected devices, as well as corporations, have to continuously monitor their end users' devices for any malicious activity to safeguard the identity and assets of their end users. Monitoring petabytes of data requires real-time analytics systems that can process millions of records per second in real time.</p>
			<p>Based on the previously described industry use cases, you might observe that real-time data analytics is becoming more and more prominent by the day. However, real-time data analytics systems don't necessarily negate the need for the batch processing of data. It is very much required for enriching real-time data streams with static data, generating lookups that add context to real-time data, and generating features that are required for real-time data science and machine learning use cases. In <a href="B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Ingestion</em>, you learned about an architecture<a id="_idIndexMarker365"/> that could efficiently unify batch and real-time processing, called the <strong class="bold">Lambda Architecture</strong>. In the following section, you will learn how to further simplify the Lambda Architecture using Structured Streaming in combination with Delta Lake.</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor087"/>Simplifying the Lambda Architecture using Delta Lake</h1>
			<p>A typical Lambda Architecture<a id="_idIndexMarker366"/> has three major components: a batch layer, a streaming layer, and a serving layer. In <a href="B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>,<em class="italic"> Data Ingestion</em>, you were able to view an implementation of the Lambda Architecture<a id="_idIndexMarker367"/> using Apache Spark's unified data processing framework. The Spark DataFrames API, Structured Streaming, and SQL engine help to make Lambda Architecture simpler. However, multiple data storage layers are still required to handle batch data and streaming data separately. These separate data storage layers could be easily consolidated by using the Spark SQL engine as the service layer. However, that might still lead to multiple copies of data and might require further consolidation of data using additional batch jobs in order to present the user with a single consistent and integrated view of data. This issue can be overcome by making use of Delta Lake as a persistent data storage layer for the Lambda Architecture.</p>
			<p>Since Delta Lake<a id="_idIndexMarker368"/> comes built with ACID transactional<a id="_idIndexMarker369"/> and isolation properties for write operations, it can provide the seamless unification of batch and streaming data, further simplifying the Lambda Architecture. This is illustrated in the following diagram:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="Images/B16736_04_03.jpg" alt="Figure 4.3 – A Lambda Architecture with Apache Spark and Delta Lake&#13;&#10;" width="1307" height="802"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – A Lambda Architecture with Apache Spark and Delta Lake</p>
			<p>In the preceding diagram, a simplified Lambda Architecture is presented. Here, batch data, as well as streaming data, is simultaneously ingested using batch processing with Apache Spark and Structured Streaming, respectively. Ingesting both batch and streaming data into a single Delta Lake table greatly simplifies the Lambda Architecture. Once the data has been ingested into Delta Lake, it is instantly available for further downstream use cases such as ad hoc data exploration via Spark SQL queries, near real-time Business Intelligence reports and dashboards, and data science and machine learning use cases. Since processed<a id="_idIndexMarker370"/> data is continuously streamed into Delta Lake, it can be consumed in both <a id="_idIndexMarker371"/>a streaming and batch manner:</p>
			<ol>
				<li>Let's take a look at how this simplified Lambda Architecture can be implemented using Apache Spark and Delta Lake, as illustrated in the following block of code:<p class="source-code">retail_batch_df = (spark</p><p class="source-code">                 .read</p><p class="source-code">                 .option("header", "true")</p><p class="source-code">                 .option("inferSchema", "true")</p><p class="source-code">                 .csv("/FileStore/shared_uploads/online_retail/online_retail.csv"))</p><p>In the preceding code snippet, we create a Spark DataFrame by reading a CSV file stored on the data lake using the <strong class="source-inline">read()</strong> function. We specify the options to infer the headers and schema from the semi-structured CSV file itself. The result of this is a Spark DataFrame, named <strong class="source-inline">retail_batch_df</strong>, that is a pointer to the data and structure of the retail data stored in the CSV files.</p></li>
				<li>Now, let's convert this CSV data into Delta Lake format and store it as a Delta table on the data lake, as shown in the following block of code:<p class="source-code">(retail_batch_df</p><p class="source-code">       .write</p><p class="source-code">       .mode("overwrite")</p><p class="source-code">       .format("delta")</p><p class="source-code">       .option("path", "/tmp/data-lake/online_retail.delta")</p><p class="source-code">       .saveAsTable("online_retail"))</p><p>In the preceding code snippet, we save the <strong class="source-inline">retail_batch_df</strong> Spark DataFrame to the data<a id="_idIndexMarker372"/> lake as a Delta table using the <strong class="source-inline">write()</strong> function<a id="_idIndexMarker373"/> along with the <strong class="source-inline">saveAsTable()</strong> function. The format is specified as <strong class="source-inline">delta</strong>, and a location for the table is specified using the <strong class="source-inline">path</strong> option. The result is a Delta table named <strong class="source-inline">online_retail</strong> with its data stored, in Delta Lake format, on the data lake.</p><p class="callout-heading">Tip</p><p class="callout">When a Spark DataFrame is saved<a id="_idIndexMarker374"/> as a table, with a location specified, the table is called an external table. As a best practice, it is recommended that you always create external tables because the data of an external table is preserved even if the table definition is deleted.</p></li>
			</ol>
			<p>In the preceding block of code, we performed an initial load of the data using Spark's batch processing:</p>
			<ul>
				<li>Now, let's load some incremental data into the same Delta table defined previously, named <strong class="source-inline">online_retail</strong>, using Spark's Structured Streaming. This is illustrated in the following block of code:<p class="source-code">retail_stream_df = (spark</p><p class="source-code">                 .readStream</p><p class="source-code">                 .schema(retailSchema)</p><p class="source-code">                 .csv("/FileStore/shared_uploads/online_retail/"))</p><p>In the preceding code snippet, we read a set of CSV files stored on the data lake in a streaming fashion using the <strong class="source-inline">readStream()</strong> function. Structured Streaming requires the schema of data being read to be specified upfront, which we supply using the <strong class="source-inline">schema</strong> option. The result is a Structured Streaming DataFrame named <strong class="source-inline">retail_stream_df</strong>.</p></li>
				<li>Now, let's ingest this stream of data into the same Delta table, named <strong class="source-inline">online_retail</strong>, which was<a id="_idIndexMarker375"/> created earlier during the initial load. This is shown<a id="_idIndexMarker376"/> in the following block of code:<p class="source-code">(retail_stream_df</p><p class="source-code">       .writeStream</p><p class="source-code">       .outputMode("append")</p><p class="source-code">       .format("delta")</p><p class="source-code">       .option("checkpointLocation", "/tmp/data-lake/online_retail.delta/")</p><p class="source-code">       .start("/tmp/data-lake/online_retail.delta"))</p><p>In the preceding code block, the streaming <strong class="source-inline">retail_stream_df</strong> DataFrame is being ingested into the existing Delta table named <strong class="source-inline">online_retail</strong> using Structured Streaming's <strong class="source-inline">writeStream()</strong> function. The <strong class="source-inline">outputMode</strong> option is specified as <strong class="source-inline">append</strong>. This is because we want to continuously append new data to the existing Delta table. Since Structured Streaming guarantees <strong class="bold">exactly-once semantics</strong>, a <strong class="source-inline">checkpointLocation</strong> needs to be specified. This is so that Structured Streaming can track the progress of the processed data and restart exactly from the point where it left in the case of failures or if the streaming process restarts.</p><p class="callout-heading">Note</p><p class="callout">A Delta table stores all the required schema information in the Delta Transaction Log. This makes registering Delta tables with a metastore completely optional, and it is only required while accessing Delta tables via external tools or via Spark SQL.</p></li>
			</ul>
			<p>You can now observe from the previous blocks of code that the combination of Spark's unified batch and stream processing already simplifies Lambda Architecture by using a single unified analytics engine. With the addition of Delta Lake's transactional and isolation properties and batch and streaming unification, your Lambda Architecture can be further simplified, giving you a powerful and scalable platform that allows you to get to your freshest data in just a few seconds to a few minutes. One prominent use case of streaming data ingestion is maintaining a replica of the source transactional system data in the data lake. This replica<a id="_idIndexMarker377"/> should include all the delete, update, and insert operations that take place in the source system. Generally, this use case is termed CDC and follows a pattern similar<a id="_idIndexMarker378"/> to the one described in this section. In the following section, we will dive deeper into implementing CDC using Apache Spark and Delta Lake.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor088"/>Change Data Capture</h1>
			<p>Generally, operational<a id="_idIndexMarker379"/> systems do not maintain historical data for extended periods of time. Therefore, it is essential that an exact replica of the transactional system data be maintained in the data lake along with its history. This has a few advantages, including providing you with a historical audit log of all your transactional data. Additionally, this huge wealth of data can help you to unlock novel business use cases and data patterns that could take your business to the next level.</p>
			<p>Maintaining an exact replica of a transactional system in the data lake means capturing all of the changes to every transaction that takes place in the source system and replicating it in the data lake. This process is generally called CDC. CDC requires you to not only capture all the new transactions and append them to the data lake but also capture any deletes or updates to the transactions that happen in the source system. This is not an ordinary feat to achieve on data lakes, as data lakes have meager to no support at all for updating or deleting arbitrary records. However, CDC on data lakes is made possible with Delta Lake's full support to insert, update, and delete any number of arbitrary records. Additionally, the combination of Apache Spark and Delta Lake makes the architecture simple.</p>
			<p>Let's implement a CDC process using Apache Spark and Delta Lake, as illustrated in the following block of code:</p>
			<p class="source-code">(spark</p>
			<p class="source-code">   .read</p>
			<p class="source-code">     .option("header", "true")</p>
			<p class="source-code">     .option("inferSchema", "true")</p>
			<p class="source-code">     .csv("/FileStore/shared_uploads/online_retail/online_retail.csv")</p>
			<p class="source-code">   .write</p>
			<p class="source-code">     .mode("overwrite")</p>
			<p class="source-code">     .format("delta")</p>
			<p class="source-code">     .option("path", "/tmp/data-lake/online_retail.delta")</p>
			<p class="source-code">     .saveAsTable("online_retail"))</p>
			<p>In the preceding code snippet, we perform an initial load of a static set of data into a Delta table using batch processing with Spark. We simply use Spark DataFrame's <strong class="source-inline">read()</strong> function to read a set of static CSV files and save them into a Delta table using the <strong class="source-inline">saveAsTable()</strong> function. Here, we use the <strong class="source-inline">path</strong> option to define the table as an external table. The result is a delta table with a static set of initial data from the source table.</p>
			<p>Here, the question<a id="_idIndexMarker380"/> is how did we end up with transactional data from an operational system, which typically happens to be RDBMS, in the form of a set of text files in the data lake? The answer is a specialist set of tools that are purpose-built for reading CDC data from operational systems and converting and staging them onto either a data lake or a message queue or another database of choice. Some examples of such CDC tools include Oracle's Golden Gate and AWS Database Migration Service.</p>
			<p class="callout-heading">Note </p>
			<p class="callout">Apache Spark can handle CDC data and ingest it seamlessly into Delta Lake; however, it is not suited for building end-to-end CDC pipelines, including ingesting from operational sources. There are open source and proprietary tools specifically built for this purpose, such as StreamSets, Fivetran, Apache Nifi, and more.</p>
			<p>Now that we have an initial set of static transactional data loaded into a Delta table,let's ingest some real-time data into the same Delta table, as shown in the following block of code:</p>
			<p class="source-code">retail_stream_df = (spark</p>
			<p class="source-code">                 .readStream</p>
			<p class="source-code">                 .schema(retailSchema)</p>
			<p class="source-code">                 .csv("/FileStore/shared_uploads/online_retail/"))</p>
			<p>In the preceding<a id="_idIndexMarker381"/> code snippet, we define a streaming DataFrame from a location on the data lake. Here, the assumption is that a third-party CDC tool is constantly adding new files to the location on the data lake with the latest transactional data.</p>
			<p>Now, we can merge the change data into the existing Delta table, as shown in the following block of code:</p>
			<p class="source-code">from delta.tables import *</p>
			<p class="source-code">deltaTable = DeltaTable.forPath(spark, "/tmp/data-lake/online_retail.delta")</p>
			<p class="source-code">def upsertToDelta(microBatchOutputDF, batchId):</p>
			<p class="source-code">  deltaTable.alias("a").merge(</p>
			<p class="source-code">      microBatchOutputDF.dropDuplicates(["InvoiceNo", "InvoiceDate"]).alias("b"),</p>
			<p class="source-code">      "a.InvoiceNo = b.InvoiceNo and a.InvoiceDate = b.InvoiceDate") \</p>
			<p class="source-code">    .whenMatchedUpdateAll() \</p>
			<p class="source-code">    .whenNotMatchedInsertAll() \</p>
			<p class="source-code">    .execute()</p>
			<p>In the preceding code block, the following happens:</p>
			<ol>
				<li value="1">We recreate a definition for the existing Delta table using the Delta Lake location and the <strong class="source-inline">DeltaTable.forPath()</strong> function. The result is a pointer to the Delta table in Spark's memory, named <strong class="source-inline">deltaTable</strong>.</li>
				<li>Then, we define a function named <strong class="source-inline">upsertToDelta()</strong> that performs the actual <strong class="source-inline">merge</strong> or <strong class="source-inline">upsert</strong> operation into the existing Delta table.</li>
				<li>The existing Delta table is aliased using the letter of <strong class="source-inline">a</strong>, and the Spark DataFrame containing new updates from each streaming micro-batch is aliased as letter <strong class="source-inline">b</strong>.</li>
				<li>The incoming<a id="_idIndexMarker382"/> updates from the streaming micro-batch might actually contain duplicates. The reason for the duplicates is that a given transaction might have undergone multiple updates by the time its data reaches Structured Streaming. Therefore, there is a need to deduplicate the streaming micro-batch prior to merging into the Delta table. This is achieved by applying the <strong class="source-inline">dropDuplicates()</strong> function on the streaming micro-batch DataFrame.</li>
				<li>The streaming updates are then merged into the Delta table by applying the <strong class="source-inline">merge()</strong> function on the existing Delta table. An equality condition is applied to the key columns of both the DataFrames, and all matching records from the streaming micro-batch updates are updated in the existing Delta table using the <strong class="source-inline">whenMatchedUpdateAll()</strong> function.</li>
				<li>Any records from the streaming micro-batch that don't already exist in the target Delta table are inserted using the <strong class="source-inline">whenNotMatchedInsertAll()</strong> function.<p class="callout-heading">Note</p><p class="callout">It is necessary to deduplicate the streaming updates coming in the form of micro-batches as a given transaction might have undergone multiple updates by the time our streaming job actually gets to process it. It is a common industry practice to select the latest update per transaction based on the key column and the latest timestamp. In the absence of such a timestamp column in the source table, most CDC tools have the functionality to scan records in the correct order that they were created or updated and insert their own timestamp column.</p></li>
			</ol>
			<p>In this way, using a simple <strong class="source-inline">merge()</strong> function, change<a id="_idIndexMarker383"/> data can be easily merged into an existing Delta table stored on any data lake. This functionality greatly simplifies the architectural complexity of CDC use cases that are implemented in real-time analytics systems.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">It is imperative that events arrive in the exact same order they were created at the source for CDC use cases. For instance, a delete operation cannot be applied prior to an insert operation. This would lead to incorrect data outright. Certain message queues do not preserve the order of events as they arrive in the queue, and care should be taken to preserve event ordering.</p>
			<p>Behind the scenes, Spark automatically scales the merge process, making it scalable to even petabytes of data. In this way, Delta Lake brings data warehouse-like functionality to cloud-based data lakes that weren't actually designed to handle analytics types of use cases.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">A Delta merge might progressively get slower as the data size in the target Delta table increases. A Delta merge's performance can be improved by using an appropriate data partitioning scheme and specifying data partition column(s) in the merge clause. In this way, a Delta merge will only select those partitions that actually need to be updated, thus greatly improving merge performance.</p>
			<p>Another phenomenon that is unique to a real-time streaming analytics use case is late-arriving data. When an event or an update<a id="_idIndexMarker384"/> to an event arrives at the streaming engine a little later than expected, it is called late-arriving data. A capable streaming engine needs to be able to handle late-arriving data or data arriving out of order. In the following section, we will explore handling late-arriving data in more detail.</p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor089"/>Handling late-arriving data</h1>
			<p>Late-arriving data is a situation<a id="_idIndexMarker385"/> that is unique to real-time streaming analytics, where events<a id="_idIndexMarker386"/> related to the same transaction do not arrive in time to be processed together, or they arrive out of order at the time of processing. Structured Streaming supports stateful stream processing to handle such scenarios. We will explore these concepts further next.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor090"/>Stateful stream processing using windowing and watermarking</h2>
			<p>Let's consider the example<a id="_idIndexMarker387"/> of an online retail transaction where a user<a id="_idIndexMarker388"/> is browsing through<a id="_idIndexMarker389"/> the e-tailer's website. We would<a id="_idIndexMarker390"/> like to calculate the user session based on one of the two following events taking place: either the users exit the e-tailer's portal or a timeout occurs. Another example<a id="_idIndexMarker391"/> is that a user places<a id="_idIndexMarker392"/> an order and then subsequently updates the order, and due to the network or some other delay, we receive the update first and then the original order creation event. Here, we would want to wait to receive any late or out-of-order data before we go ahead and save the data to the final storage location.</p>
			<p>In both of the previously mentioned scenarios, the streaming engine needs to be able to store and manage certain state information pertaining to each transaction in order to account for late-arriving data. Spark's Structured Streaming can automatically handle late-arriving data by implementing stateful processing using the concept of <strong class="bold">Windowing</strong>.</p>
			<p>Before we dive deeper into the concept of windowing in Structured Screaming, you need to understand<a id="_idIndexMarker393"/> the concept of event time. <strong class="bold">Event time</strong> is the timestamp at which an event of a transaction is generated at the source. For instance, the timestamp at which an order is placed becomes the event time for the order creation event. Similarly, if the same transaction is updated at the source, then the update timestamp becomes the event time for the update event of the transaction. Event time is an important parameter for any stateful processing engine in order to determine which event took place first.</p>
			<p>Using windowing, Structured Steaming maintains a state for each key and updates the state for a given key if a new event for the same key arrives, as illustrated in the following diagram:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="Images/B16736_04_04.jpg" alt="Figure 4.4 – Stateful stream processing&#13;&#10;" width="477" height="208"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Stateful stream processing</p>
			<p>In the preceding illustration, we have a stream of transactional events of orders being placed. Each of <strong class="bold">O1</strong>, <strong class="bold">O2</strong>, and <strong class="bold">O3</strong>, indicates the order numbers, and <strong class="bold">T</strong>, <strong class="bold">T+03</strong>, and so on, indicates timestamps<a id="_idIndexMarker394"/> at which orders were created. The input<a id="_idIndexMarker395"/> stream has a steady stream<a id="_idIndexMarker396"/> of order-related events<a id="_idIndexMarker397"/> being generated. We define a stateful window of <strong class="bold">10</strong> minutes with a sliding interval of every <strong class="bold">5</strong> minutes. What we are trying to achieve here in the window is to update the count of each unique order placed. As you can see, at each <strong class="bold">5</strong>-minute interval, any new events of the same order get an updated count. This simple illustration depicts how stateful processing works in a stream processing scenario.</p>
			<p>However, there is one problem with this type of stateful processing; that is, the state seems to be perpetually maintained, and over a period of time, the state data itself might grow to be too huge to fit into the cluster memory. It is also not practical to maintain the state perpetually. This is because real-world scenarios rarely ever require the state to be maintained for extended periods of time. Therefore, we need a mechanism to expire the state after a certain time interval. Structured Streaming has the ability to define a watermark that governs for how long the individual state is maintained per key, and it drops the state as soon as the watermark expires for a given key.</p>
			<p class="callout-heading">Note </p>
			<p class="callout">In spite of defining a watermark, the state could still grow to be quite large, and Structured Streaming has the ability to spill the state data onto the executor's local disk when needed. Structured Streaming can also be configured to use an external state store such as RocksDB in order to maintain state data for a very large number of keys ranging in the millions.</p>
			<p>The following code blocks<a id="_idIndexMarker398"/> show the implementation details<a id="_idIndexMarker399"/> of arbitrary stateful processing<a id="_idIndexMarker400"/> with Spark's Structured<a id="_idIndexMarker401"/> Streaming using the event time, windowing, and watermarking:</p>
			<ol>
				<li value="1">Let's implement the concepts of <strong class="bold">windowing</strong> and <strong class="bold">watermarking</strong> using Structured Streaming, as shown in the following code example:<p class="source-code">from pyspark.sql.functions import window, max, count, current_timestamp, to_timestamp</p><p class="source-code">raw_stream_df = (spark</p><p class="source-code">                 .readStream</p><p class="source-code">                 .schema(retailSchema)</p><p class="source-code">                 .option("header", True)</p><p class="source-code">                 .csv("/FileStore/shared_uploads/online_retail/")</p><p class="source-code">                 .withColumn("InvoiceTime", to_timestamp("InvoiceDate", 'dd/M/yy HH:mm')))</p><p>In the preceding code block, we define a streaming DataFrame using a location on the data lake. Additionally, we append a new column to the DataFrame named <strong class="source-inline">InvoiceTime</strong> by converting the <strong class="source-inline">InvoiceDate</strong> column from <strong class="source-inline">StringType</strong> into <strong class="source-inline">TimestampType</strong>.</p></li>
				<li>Next, we will perform some stateful processing on the <strong class="source-inline">raw_stream_df</strong> Streaming DataFrame by defining windowing and watermarking functions on it, as shown in the following block of code:<p class="source-code">aggregated_df = (</p><p class="source-code">    raw_stream_df.withWatermark("InvoiceTime", </p><p class="source-code">                                "1 minutes")</p><p class="source-code">    .groupBy("InvoiceNo", window("InvoiceDate", </p><p class="source-code">                                 "30 seconds", </p><p class="source-code">                                 "10 seconds", </p><p class="source-code">                                 "0 seconds"))</p><p class="source-code">    .agg(max("InvoiceDate").alias("event_time"),</p><p class="source-code">         count("InvoiceNo").alias("order_count"))</p><p class="source-code">)</p><p>The following observations can be drawn from the preceding code snippet:</p><ol><li>We define a watermark on the <strong class="source-inline">raw_stream_df</strong> streaming DataFrame for <strong class="source-inline">1</strong> minute. This specifies that Structured Streaming should accumulate a state for each key<a id="_idIndexMarker402"/> for only a duration of <strong class="source-inline">1</strong> minute. The watermark duration depends entirely<a id="_idIndexMarker403"/> on your use case and how<a id="_idIndexMarker404"/> late your data<a id="_idIndexMarker405"/> is expected to arrive.</li><li>We define a group by function on the key column, named <strong class="source-inline">InvoiceNo</strong>, and define the desired window for our stateful operation as <strong class="source-inline">30</strong> seconds with a sliding window of every <strong class="source-inline">10</strong> seconds. This means that our keys will be aggregated every <strong class="source-inline">10</strong> seconds after the initial <strong class="source-inline">30</strong>-second window.</li><li>We define the aggregation functions to be <strong class="source-inline">max</strong> on the timestamp column and <strong class="source-inline">count</strong> on the key column.</li><li>The streaming process will write data to the streaming sink as soon as the watermark expires for each key.</li></ol></li>
				<li>Once the stateful stream has been defined using windowing and watermarking functions, we can quickly verify whether the stream is working as expected, as shown in the following code snippet:<p class="source-code">(aggregated_df</p><p class="source-code">   .writeStream</p><p class="source-code">   .queryName("aggregated_df")</p><p class="source-code">   .format("memory")</p><p class="source-code">   .outputMode("complete")</p><p class="source-code">   .start())</p><p>The previous code block writes the output of the stateful processing streaming DataFrame to a memory sink and specifies a <strong class="source-inline">queryName</strong> property. The stream gets registered as an in-memory table with the specified query name, and it can be easily queried using Spark SQL to quickly verify the correctness of the code.</p></li>
			</ol>
			<p>In this way, making<a id="_idIndexMarker406"/> use of windowing and watermarking<a id="_idIndexMarker407"/> functionalities provided<a id="_idIndexMarker408"/> by Structured Streaming, stateful<a id="_idIndexMarker409"/> stream processing can be implemented using Structured Streaming and late-arriving data can be easily handled. Another aspect to pay attention to in all of the previous code examples presented in this chapter, so far, is how the streaming data progressively gets transformed from its raw state into a processed state and further into an aggregated state. This methodology<a id="_idIndexMarker410"/> of progressively transforming data using multiple streaming processes is generally called a <strong class="bold">multi-hop architecture</strong>. In the following section, we will explore this methodology further.</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor091"/>Multi-hop pipelines</h1>
			<p>A multi-hop pipeline<a id="_idIndexMarker411"/> is an architecture for building a series of streaming jobs chained together so that each job in the pipeline processes the data and improves the quality of the data progressively. A typical data analytics pipeline consists of multiple stages, including data ingestion, data cleansing and integration, and data aggregation. Later on, it consists of data science and machine learning-related steps, including feature engineering and machine learning training and scoring. This process progressively improves the quality of data until it is finally ready for end user consumption.</p>
			<p>With Structured Streaming, all these stages of the data analytics pipelines can be chained together into a <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) of streaming jobs. In this way, new raw data continuously enters one end of the pipeline and gets progressively processed by each stage of the pipeline. Finally, end user ready data exits from the tail end of the pipeline. A typical multi-hop architecture is presented in the following diagram:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="Images/B16736_04_05.jpg" alt="Figure 4.5 – The Multi-hop pipeline architecture&#13;&#10;" width="1173" height="590"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – The Multi-hop pipeline architecture</p>
			<p>The previous diagram represents a multi-hop pipeline architecture, where raw data is ingested into the data lake and is processed in order to improve its quality through each stage of the data analytics pipeline until it is finally ready for end user use cases. End user use cases could be Business Intelligence and reporting, or they can be for further processing into predictive analytics use cases using data science and machine learning techniques.</p>
			<p>Although it seems like<a id="_idIndexMarker412"/> a simple architecture to implement, a few key prerequisites have to be met for the seamless implementation of multi-hop pipelines, without frequent developer intervention. The prerequisites<a id="_idIndexMarker413"/> are as follows:</p>
			<ol>
				<li value="1">For the stages of the pipelines to be chained together seamlessly, the data processing engine needs to support exactly-once data processing guarantees resiliency to data loss upon failures.</li>
				<li>The data processing engine needs to have capabilities to maintain watermark data. This is so that it is aware of the progress of data processed at a given point in time and can seamlessly pick up new data arriving in a streaming manner and process it.</li>
				<li>The underlying data storage layer needs to support transactional and isolation guarantees so that there is no need for any developer intervention of any bad or incorrect data clean-up upon job failures.</li>
			</ol>
			<p>Apache Spark's Structured Streaming solves the previously mentioned points, <em class="italic">1</em> and <em class="italic">2</em>, as it guarantees exactly-once data processing semantics and has built-in support for checkpointing. This is to keep track of the data processing progress and also to help with restarting a failed job exactly at the point where it left off. Point <em class="italic">3</em> is supported by Delta Lake with its ACID transactional guarantees and support for simultaneous batch and streaming jobs.</p>
			<ol>
				<li value="1">Let's implement an example multi-hop pipeline using Structured Streaming and Delta Lake, as shown in the following blocks of code:<p class="source-code"> raw_stream_df = (spark</p><p class="source-code">                 .readStream</p><p class="source-code">                 .schema(retailSchema)</p><p class="source-code">                 .option("header", True)</p><p class="source-code">                 .csv("/FileStore/shared_uploads/online_retail/"))</p><p class="source-code">(raw_stream_df</p><p class="source-code">   .writeStream</p><p class="source-code">     .format("delta")</p><p class="source-code">     .option("checkpointLocation", </p><p class="source-code">             "/tmp/delta/raw_stream.delta/checkpoint")</p><p class="source-code">     .start("/tmp/delta/raw_stream.delta/"))</p><p>In the preceding code block, we create a raw streaming DataFrame by ingesting source data from its raw format into the data lake in Delta Lake format. The <strong class="source-inline">checkpointLocation</strong> provides the streaming job with resiliency to failures whereas Delta Lake for the target location provides transactional and isolation guarantees for <strong class="source-inline">write</strong> operations.</p></li>
				<li>Now we can further<a id="_idIndexMarker414"/> process the raw ingested data using another job to further improve the quality of data, as shown in the following code block:<p class="source-code">integrated_stream_df = (raw_stream_df</p><p class="source-code">                          .withColumn("InvoiceTime", to_timestamp("InvoiceDate", 'dd/M/yy HH:mm')))</p><p class="source-code">(integrated_stream_df</p><p class="source-code">   .writeStream</p><p class="source-code">     .format("delta")</p><p class="source-code">     .option("checkpointLocation", "/tmp/delta/int_stream.delta/checkpoint")</p><p class="source-code">     .start("/tmp/delta/int_stream.delta/"))</p><p>In the preceding block of code, we convert a string column into a timestamp column and persist the cleansed data into Delta Lake. This is the second stage of our multi-hop pipeline, and typically, this stage reads from the Delta table generated by the previous raw data ingestion stage. Again, the use of a checkpoint location here helps to perform the incremental processing of data, processing any new records added to the raw Delta table as they arrive.</p></li>
				<li>Now we can define<a id="_idIndexMarker415"/> the final stage of the pipeline where we aggregate the data to create highly summarized data that is ready for end user consumption, as shown in the following code snippet:<p class="source-code">aggregated_stream_df = (integrated_stream_df</p><p class="source-code">.withWatermark("InvoiceTime", "1 minutes")</p><p class="source-code">.groupBy("InvoiceNo", window("InvoiceTime", </p><p class="source-code">         "30 seconds", "10 seconds", "0 seconds"))</p><p class="source-code">.agg(max("InvoiceTime").alias("event_time"),</p><p class="source-code">         count("InvoiceNo").alias("order_count")))</p><p class="source-code">(aggregated_stream_df</p><p class="source-code">   .writeStream</p><p class="source-code">     .format("delta")</p><p class="source-code">     .option("checkpointLocation", </p><p class="source-code">             "/tmp/delta/agg_stream.delta/checkpoint")</p><p class="source-code">     .start("/tmp/delta/agg_stream.delta/"))</p><p>In the preceding code block, integrated and cleansed data is aggregated into the highest level of summary data. This can be further consumed by Business Intelligence or data science and machine learning use cases. This stage of the pipeline also makes use of the checkpoint location and Delta table for resiliency to job failures and keep the tracking of new data that needs to be processed when it arrives.</p></li>
			</ol>
			<p>Therefore, with the combination of Apache Spark's Structured Streaming and Delta Lake, implementing multi-hop architecture becomes seamless and efficient. The different stages of a multi-hop could be implemented as a single monolithic job containing multiple streaming processes. As a best practice, the individual streaming processes for each stage<a id="_idIndexMarker416"/> of the pipeline are broken down into multiple independent streaming jobs, which can be further chained together into a DAG using an external orchestrator such as Apache Airflow. The advantage of the latter is easier maintenance of the individual streaming jobs and the minimized downtime of the overall pipeline when an individual stage of the pipeline needs to be updated or upgraded.</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor092"/>Summary</h1>
			<p>In this chapter, you were introduced to the need for real-time data analytics systems and the advantages they have to offer in terms of getting the freshest data to business users, helping businesses improve their time to market, and minimizing any lost opportunity costs. The architecture of a typical real-time analytics system was presented, and the major components were described. A real-time analytics architecture using Apache Spark's Structured Streaming was also depicted. A few examples of prominent industry use cases of real-time data analytics were described. Also, you were introduced to a simplified Lambda Architecture using the combination of Structured Streaming and Delta Lake. The use case for CDC, including its requirements and benefits, was presented, and techniques for ingesting CDC data into Delta Lake were presented along with working examples leveraging Structured Streaming for implementing a CDC use case.</p>
			<p>Finally, you learned a technique for progressively improving data quality from data ingestion into highly aggregated and summarized data, in near real time, called multi-hop pipelines. You also examined a simple implementation of multi-hop pipelines using the powerful combination of Structured Streaming and Delta Lake.</p>
			<p>This concludes the data engineering section of this book. The skills you have learned so far will help you to embark on a data analytics journey starting with raw transactional data from operational source systems, ingesting it into data lakes, cleansing the data, and integrating the data. Also, you should be familiar with building end-to-end data analytics pipelines that progressively improve the quality of data in a real-time streaming fashion and result in pristine, highest-level aggregated data that can be readily consumed by Business Intelligence and reporting use cases.</p>
			<p>In the following chapters, you will build on the data engineering concepts learned thus far and delve into the realm of predictive analytics using Apache Spark's data science and machine learning capabilities. In the next chapter, we will begin with the concepts of exploratory data analysis and feature engineering.</p>
		</div>
	</div></body></html>