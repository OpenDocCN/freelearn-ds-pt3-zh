["```py\n>>> import os \n\"\"\" First change the following directory link to where all input files do exist \"\"\" \n>>> os.chdir(\"D:\\\\Book writing\\\\Codes\\\\Chapter 8\") \n\nK-means algorithm from scikit-learn has been utilized in the following example \n\n# K-means clustering \n>>> import numpy as np \n>>> import pandas as pd \n>>> import matplotlib.pyplot as plt \n>>> from scipy.spatial.distance import cdist, pdist \n\n>>> from sklearn.cluster import KMeans \n>>> from sklearn.metrics import silhouette_score\n```", "```py\n>>> iris = pd.read_csv(\"iris.csv\") \n>>> print (iris.head()) \n```", "```py\n>>> x_iris = iris.drop(['class'],axis=1) \n>>> y_iris = iris[\"class\"] \n```", "```py\n>>> k_means_fit = KMeans(n_clusters=3,max_iter=300) \n>>> k_means_fit.fit(x_iris) \n\n>>> print (\"\\nK-Means Clustering - Confusion Matrix\\n\\n\",pd.crosstab(y_iris, k_means_fit.labels_,rownames = [\"Actuall\"],colnames = [\"Predicted\"]) )      \n>>> print (\"\\nSilhouette-score: %0.3f\" % silhouette_score(x_iris, k_means_fit.labels_, metric='euclidean')) \n```", "```py\n>>> for k in range(2,10): \n...     k_means_fitk = KMeans(n_clusters=k,max_iter=300) \n...     k_means_fitk.fit(x_iris) \n...     print (\"For K value\",k,\",Silhouette-score: %0.3f\" % silhouette_score(x_iris, k_means_fitk.labels_, metric='euclidean')) \n```", "```py\n# Avg. within-cluster sum of squares \n>>> K = range(1,10) \n\n>>> KM = [KMeans(n_clusters=k).fit(x_iris) for k in K] \n>>> centroids = [k.cluster_centers_ for k in KM] \n\n>>> D_k = [cdist(x_iris, centrds, 'euclidean') for centrds in centroids] \n\n>>> cIdx = [np.argmin(D,axis=1) for D in D_k] \n>>> dist = [np.min(D,axis=1) for D in D_k] \n>>> avgWithinSS = [sum(d)/x_iris.shape[0] for d in dist] \n\n# Total with-in sum of square \n>>> wcss = [sum(d**2) for d in dist] \n>>> tss = sum(pdist(x_iris)**2)/x_iris.shape[0] \n>>> bss = tss-wcss \n\n# elbow curve - Avg. within-cluster sum of squares \n>>> fig = plt.figure() \n>>> ax = fig.add_subplot(111) \n>>> ax.plot(K, avgWithinSS, 'b*-') \n>>> plt.grid(True) \n>>> plt.xlabel('Number of clusters') \n>>> plt.ylabel('Average within-cluster sum of squares') \n```", "```py\n# elbow curve - percentage of variance explained \n>>> fig = plt.figure() \n>>> ax = fig.add_subplot(111) \n>>> ax.plot(K, bss/tss*100, 'b*-') \n>>> plt.grid(True) \n>>> plt.xlabel('Number of clusters') \n>>> plt.ylabel('Percentage of variance explained')\n>>> plt.show()\n```", "```py\nsetwd(\"D:\\\\Book writing\\\\Codes\\\\Chapter 8\")   \n\niris_data = read.csv(\"iris.csv\")   \nx_iris =   iris_data[,!names(iris_data) %in% c(\"class\")]   \ny_iris = iris_data$class   \n\nkm_fit = kmeans(x_iris,centers   = 3,iter.max = 300 )   \n\nprint(paste(\"K-Means   Clustering- Confusion matrix\"))   \ntable(y_iris,km_fit$cluster)   \n\nmat_avgss = matrix(nrow = 10,   ncol = 2)   \n\n# Average within the cluster   sum of square   \nprint(paste(\"Avg. Within   sum of squares\"))   \nfor (i in (1:10)){   \n  km_fit =   kmeans(x_iris,centers = i,iter.max = 300 )   \n  mean_km =   mean(km_fit$withinss)   \n  print(paste(\"K-Value\",i,\",Avg.within   sum of squares\",round(mean_km, 2)))   \n  mat_avgss[i,1] = i   \n  mat_avgss[i,2] = mean_km   \n}   \n plot(mat_avgss[,1],mat_avgss[,2],type   = 'o',xlab = \"K_Value\",ylab = \"Avg. within sum of square\")   \ntitle(\"Avg. within sum of   squares vs. K-value\")   \n\nmat_varexp = matrix(nrow = 10,   ncol = 2)   \n# Percentage of Variance   explained   \nprint(paste(\"Percent.   variance explained\"))   \nfor (i in (1:10)){   \n  km_fit =   kmeans(x_iris,centers = i,iter.max = 300 )   \n  var_exp =   km_fit$betweenss/km_fit$totss   \n  print(paste(\"K-Value\",i,\",Percent   var explained\",round(var_exp,4)))   \n  mat_varexp[i,1]=i   \n  mat_varexp[i,2]=var_exp   \n}   \n\nplot(mat_varexp[,1],mat_varexp[,2],type   = 'o',xlab = \"K_Value\",ylab = \"Percent Var explained\")   \ntitle(\"Avg. within sum of   squares vs. K-value\") \n```", "```py\n>>> import numpy as np\n>>> w, v = np.linalg.eig(np.array([[ 0.91335 ,0.75969 ],[ 0.75969,0.69702]]))\n\\>>> print (\"\\nEigen Values\\n\", w)\n>>> print (\"\\nEigen Vectors\\n\", v)\n```", "```py\n# PCA - Principal Component Analysis \n>>> import matplotlib.pyplot as plt \n>>> from sklearn.decomposition import PCA \n>>> from sklearn.datasets import load_digits \n\n>>> digits = load_digits() \n>>> X = digits.data \n>>> y = digits.target \n\n>>> print (digits.data[0].reshape(8,8)) \n```", "```py\n>>> plt.matshow(digits.images[0])  \n>>> plt.show()  \n```", "```py\n>>> from sklearn.preprocessing import scale \n>>> X_scale = scale(X,axis=0)\n```", "```py\n>>> pca = PCA(n_components=2) \n>>> reduced_X = pca.fit_transform(X_scale) \n\n>>> zero_x, zero_y = [],[] ; one_x, one_y = [],[] \n>>> two_x,two_y = [],[]; three_x, three_y = [],[] \n>>> four_x,four_y = [],[]; five_x,five_y = [],[] \n>>> six_x,six_y = [],[]; seven_x,seven_y = [],[] \n>>> eight_x,eight_y = [],[]; nine_x,nine_y = [],[] \n```", "```py\n>>> for i in range(len(reduced_X)): \n...     if y[i] == 0: \n...         zero_x.append(reduced_X[i][0]) \n...         zero_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 1: \n...         one_x.append(reduced_X[i][0]) \n...         one_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 2: \n...         two_x.append(reduced_X[i][0]) \n...         two_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 3: \n...         three_x.append(reduced_X[i][0]) \n...         three_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 4: \n...         four_x.append(reduced_X[i][0]) \n...         four_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 5: \n...         five_x.append(reduced_X[i][0]) \n...         five_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 6: \n...         six_x.append(reduced_X[i][0]) \n...         six_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 7: \n...         seven_x.append(reduced_X[i][0]) \n...         seven_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 8: \n...         eight_x.append(reduced_X[i][0]) \n...         eight_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 9: \n...         nine_x.append(reduced_X[i][0]) \n...         nine_y.append(reduced_X[i][1]) \n>>> zero = plt.scatter(zero_x, zero_y, c='r', marker='x',label='zero') \n>>> one = plt.scatter(one_x, one_y, c='g', marker='+') \n>>> two = plt.scatter(two_x, two_y, c='b', marker='s') \n\n>>> three = plt.scatter(three_x, three_y, c='m', marker='*') \n>>> four = plt.scatter(four_x, four_y, c='c', marker='h') \n>>> five = plt.scatter(five_x, five_y, c='r', marker='D') \n\n>>> six = plt.scatter(six_x, six_y, c='y', marker='8') \n>>> seven = plt.scatter(seven_x, seven_y, c='k', marker='*') \n>>> eight = plt.scatter(eight_x, eight_y, c='r', marker='x') \n\n>>> nine = plt.scatter(nine_x, nine_y, c='b', marker='D') \n\n>>> plt.legend((zero,one,two,three,four,five,six,seven,eight,nine), \n...            ('zero','one','two','three','four','five','six', 'seven','eight','nine'), \n...            scatterpoints=1, \n...            loc='lower left', \n...            ncol=3, \n...            fontsize=10) \n\n>>> plt.xlabel('PC 1') \n>>> plt.ylabel('PC 2') \n\n>>> plt.show() \n```", "```py\n# 3-Dimensional data \n>>> pca_3d = PCA(n_components=3) \n>>> reduced_X3D = pca_3d.fit_transform(X_scale) \n\n>>> zero_x, zero_y,zero_z = [],[],[] ; one_x, one_y,one_z = [],[],[] \n>>> two_x,two_y,two_z = [],[],[]; three_x, three_y,three_z = [],[],[] \n>>> four_x,four_y,four_z = [],[],[]; five_x,five_y,five_z = [],[],[] \n>>> six_x,six_y,six_z = [],[],[]; seven_x,seven_y,seven_z = [],[],[] \n>>> eight_x,eight_y,eight_z = [],[],[]; nine_x,nine_y,nine_z = [],[],[] \n\n>>> for i in range(len(reduced_X3D)): \n\n...     if y[i]==10: \n...         continue  \n\n...     elif y[i] == 0: \n...         zero_x.append(reduced_X3D[i][0]) \n...         zero_y.append(reduced_X3D[i][1]) \n...         zero_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 1: \n...         one_x.append(reduced_X3D[i][0]) \n...         one_y.append(reduced_X3D[i][1]) \n...         one_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 2: \n...         two_x.append(reduced_X3D[i][0]) \n...         two_y.append(reduced_X3D[i][1]) \n...         two_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 3: \n...         three_x.append(reduced_X3D[i][0]) \n...         three_y.append(reduced_X3D[i][1]) \n...         three_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 4: \n...         four_x.append(reduced_X3D[i][0]) \n...         four_y.append(reduced_X3D[i][1]) \n...         four_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 5: \n...         five_x.append(reduced_X3D[i][0]) \n...         five_y.append(reduced_X3D[i][1]) \n...         five_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 6: \n...         six_x.append(reduced_X3D[i][0]) \n...         six_y.append(reduced_X3D[i][1]) \n...         six_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 7: \n...         seven_x.append(reduced_X3D[i][0]) \n...         seven_y.append(reduced_X3D[i][1]) \n...         seven_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 8: \n...         eight_x.append(reduced_X3D[i][0]) \n...         eight_y.append(reduced_X3D[i][1]) \n...         eight_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 9: \n...         nine_x.append(reduced_X3D[i][0]) \n...         nine_y.append(reduced_X3D[i][1]) \n...         nine_z.append(reduced_X3D[i][2]) \n\n # 3- Dimensional plot \n>>> from mpl_toolkits.mplot3d import Axes3D \n>>> fig = plt.figure() \n>>> ax = fig.add_subplot(111, projection='3d') \n\n>>> ax.scatter(zero_x, zero_y,zero_z, c='r', marker='x',label='zero') \n>>> ax.scatter(one_x, one_y,one_z, c='g', marker='+',label='one') \n>>> ax.scatter(two_x, two_y,two_z, c='b', marker='s',label='two') \n\n>>> ax.scatter(three_x, three_y,three_z, c='m', marker='*',label='three') \n>>> ax.scatter(four_x, four_y,four_z, c='c', marker='h',label='four') \n>>> ax.scatter(five_x, five_y,five_z, c='r', marker='D',label='five') \n\n>>> ax.scatter(six_x, six_y,six_z, c='y', marker='8',label='six') \n>>> ax.scatter(seven_x, seven_y,seven_z, c='k', marker='*',label='seven') \n>>> ax.scatter(eight_x, eight_y,eight_z, c='r', marker='x',label='eight') \n\n>>> ax.scatter(nine_x, nine_y,nine_z, c='b', marker='D',label='nine') \n>>> ax.set_xlabel('PC 1') \n>>> ax.set_ylabel('PC 2') \n>>> ax.set_zlabel('PC 3')\n```", "```py\n>>> plt.legend(loc='upper left', numpoints=1, ncol=3, fontsize=10, bbox_to_anchor=(0, 0)) \n\n>>> plt.show()\n```", "```py\n# Choosing number of Principal Components \n>>> max_pc = 30 \n\n>>> pcs = [] \n>>> totexp_var = [] \n\n>>> for i in range(max_pc): \n...     pca = PCA(n_components=i+1) \n...     reduced_X = pca.fit_transform(X_scale) \n...     tot_var = pca.explained_variance_ratio_.sum() \n...     pcs.append(i+1) \n...     totexp_var.append(tot_var) \n\n>>> plt.plot(pcs,totexp_var,'r') \n>>> plt.plot(pcs,totexp_var,'bs') \n>>> plt.xlabel('No. of PCs',fontsize = 13) \n>>> plt.ylabel('Total variance explained',fontsize = 13) \n```", "```py\n>>> plt.xticks(pcs,fontsize=13) \n>>> plt.yticks(fontsize=13) \n>>> plt.show() \n\n```", "```py\n# PCA   \ndigits_data = read.csv(\"digitsdata.csv\")   \n\nremove_cols = c(\"target\")   \nx_data =   digits_data[,!(names(digits_data) %in% remove_cols)]   \ny_data = digits_data[,c(\"target\")]   \n\n# Normalizing the data   \nnormalize <- function(x)   {return((x - min(x)) / (max(x) - min(x)))}   \ndata_norm <-   as.data.frame(lapply(x_data, normalize))   \ndata_norm <-   replace(data_norm, is.na(data_norm), 0.0)   \n\n# Extracting Principal   Components   \npr_out =prcomp(data_norm)   \npr_components_all = pr_out$x   \n\n# 2- Dimensional PCA   \nK_prcomps = 2   \npr_components =   pr_components_all[,1:K_prcomps]   \n\npr_components_df =   data.frame(pr_components)   \npr_components_df =   cbind(pr_components_df,digits_data$target)   \nnames(pr_components_df)[K_prcomps+1]   = \"target\"   \n\nout <- split(   pr_components_df , f = pr_components_df$target )   \nzero_df = out$`0`;one_df =   out$`1`;two_df = out$`2`; three_df = out$`3`; four_df = out$`4`   \nfive_df = out$`5`;six_df =   out$`6`;seven_df = out$`7`;eight_df = out$`8`;nine_df = out$`9`   \n\nlibrary(ggplot2)   \n# Plotting 2-dimensional PCA   \nggplot(pr_components_df, aes(x   = PC1, y = PC2, color = factor(target,labels = c(\"zero\",\"one\",\"two\",   \"three\",\"four\", \"five\",\"six\",\"seven\",\"eight\",\"nine\"))))   +    \ngeom_point()+ggtitle(\"2-D   PCA on Digits Data\") +   \nlabs(color = \"Digtis\")   \n\n# 3- Dimensional PCA   \n# Plotting 3-dimensional PCA   \nK_prcomps = 3   \n\npr_components =   pr_components_all[,1:K_prcomps]   \npr_components_df =   data.frame(pr_components)   \npr_components_df =   cbind(pr_components_df,digits_data$target)   \nnames(pr_components_df)[K_prcomps+1]   = \"target\"   \n\npr_components_df$target =   as.factor(pr_components_df$target)   \n\nout <- split(   pr_components_df , f = pr_components_df$target )   \nzero_df = out$`0`;one_df =   out$`1`;two_df = out$`2`; three_df = out$`3`; four_df = out$`4`   \nfive_df = out$`5`;six_df =   out$`6`;seven_df = out$`7`;eight_df = out$`8`;nine_df = out$`9`   \n\nlibrary(scatterplot3d)   \ncolors <- c(\"darkred\",   \"darkseagreen4\", \"deeppink4\", \"greenyellow\", \"orange\",   \"navyblue\", \"red\", \"tan3\", \"steelblue1\",   \"slateblue\")   \ncolors <- colors[as.numeric(pr_components_df$target)]   \ns3d =   scatterplot3d(pr_components_df[,1:3], pch = 16, color=colors,   \nxlab = \"PC1\",ylab = \"PC2\",zlab   = \"PC3\",col.grid=\"lightblue\",main = \"3-D PCA on   Digits Data\")   \nlegend(s3d$xyz.convert(3.1,   0.1, -3.5), pch = 16, yjust=0,   \n       legend =   levels(pr_components_df$target),col =colors,cex = 1.1,xjust = 0)   \n\n# Choosing number of Principal   Components   \npr_var =pr_out$sdev ^2   \npr_totvar = pr_var/sum(pr_var)   \nplot(cumsum(pr_totvar), xlab=\"Principal   Component\", ylab =\"Cumilative Prop. of Var.\",   ylim=c(0,1),type=\"b\",main = \"PCAs vs. Cum prop of Var   Explained\") \n```", "```py\n# SVD \n>>> import matplotlib.pyplot as plt \n>>> from sklearn.datasets import load_digits \n\n>>> digits = load_digits() \n>>> X = digits.data \n>>> y = digits.target \n```", "```py\n>>> from sklearn.utils.extmath import randomized_svd \n>>> U,Sigma,VT = randomized_svd(X,n_components=15,n_iter=300,random_state=42) \n\n>>> import pandas as pd \n>>> VT_df = pd.DataFrame(VT) \n\n>>> print (\"\\nShape of Original Matrix:\",X.shape) \n>>> print (\"\\nShape of Left Singular vector:\",U.shape) \n>>> print (\"Shape of Singular value:\",Sigma.shape) \n>>> print (\"Shape of Right Singular vector\",VT.shape) \n```", "```py\n>>> n_comps = 15 \n>>> from sklearn.decomposition import TruncatedSVD \n>>> svd = TruncatedSVD(n_components=n_comps, n_iter=300, random_state=42) \n>>> reduced_X = svd.fit_transform(X) \n\n>>> print(\"\\nTotal Variance explained for %d singular features are %0.3f\"%(n_comps, svd.explained_variance_ratio_.sum())) \n```", "```py\n# Choosing number of Singular Values \n>>> max_singfeat = 30 \n>>> singfeats = [] \n>>> totexp_var = [] \n\n>>> for i in range(max_singfeat): \n...     svd = TruncatedSVD(n_components=i+1, n_iter=300, random_state=42) \n...     reduced_X = svd.fit_transform(X) \n...     tot_var = svd.explained_variance_ratio_.sum() \n...     singfeats.append(i+1) \n...     totexp_var.append(tot_var) \n\n>>> plt.plot(singfeats,totexp_var,'r') \n>>> plt.plot(singfeats,totexp_var,'bs') \n>>> plt.xlabel('No. of Features',fontsize = 13) \n>>> plt.ylabel('Total variance explained',fontsize = 13) \n\n>>> plt.xticks(pcs,fontsize=13) \n>>> plt.yticks(fontsize=13) \n>>> plt.show()\n```", "```py\n#SVD    \nlibrary(svd)   \n\ndigits_data = read.csv(\"digitsdata.csv\")   \n\nremove_cols = c(\"target\")   \nx_data =   digits_data[,!(names(digits_data) %in% remove_cols)]   \ny_data = digits_data[,c(\"target\")]   \n\nsv2 <- svd(x_data,nu=15)   \n\n# Computing the square of the   singular values, which can be thought of as the vector of matrix energy   \n# in order to pick top singular   values which preserve at least 80% of variance explained   \nenergy <- sv2$d ^ 2   \ntot_varexp = data.frame(cumsum(energy)   / sum(energy))   \n\nnames(tot_varexp) = \"cum_var_explained\"   \ntot_varexp$K_value =   1:nrow(tot_varexp)   \n\nplot(tot_varexp[,2],tot_varexp[,1],type   = 'o',xlab = \"K_Value\",ylab = \"Prop. of Var Explained\")   \ntitle(\"SVD - Prop. of Var   explained with K-value\")    \n```", "```py\n# Deep Auto Encoders \n>>> import matplotlib.pyplot as plt \n>>> from sklearn.preprocessing import StandardScaler \n>>> from sklearn.datasets import load_digits \n\n>>> digits = load_digits() \n>>> X = digits.data \n>>> y = digits.target \n\n>>> print (X.shape) \n>>> print (y.shape) \n>>> x_vars_stdscle = StandardScaler().fit_transform(X) \n>>> print (x_vars_stdscle.shape) \n```", "```py\n>>> from keras.layers import Input,Dense \n>>> from keras.models import Model\n```", "```py\n# 2-Dimensional Architecture \n\n>>> input_layer = Input(shape=(64,),name=\"input\") \n\n>>> encoded = Dense(32, activation='relu',name=\"h1encode\")(input_layer) \n>>> encoded = Dense(16, activation='relu',name=\"h2encode\")(encoded) \n>>> encoded = Dense(2, activation='relu',name=\"h3latent_layer\")(encoded) \n\n>>> decoded = Dense(16, activation='relu',name=\"h4decode\")(encoded) \n>>> decoded = Dense(32, activation='relu',name=\"h5decode\")(decoded) \n>>> decoded = Dense(64, activation='sigmoid',name=\"h6decode\")(decoded) \n```", "```py\n>>> autoencoder = Model(input_layer, decoded) \n```", "```py\n>>> autoencoder.compile(optimizer=\"adam\", loss=\"mse\") \n```", "```py\n# Fitting Encoder-Decoder model \n>>> autoencoder.fit(x_vars_stdscle, x_vars_stdscle, epochs=100,batch_size=256, shuffle=True,validation_split= 0.2 ) \n```", "```py\n# Extracting Encoder section of the Model for prediction of latent variables \n>>> encoder = Model(autoencoder.input,autoencoder.get_layer(\"h3latent_layer\").output) \n\nExtracted encoder section of the whole model used for prediction of input variables to generate sparse 2-dimensional representation, which is being performed with the following code \n\n# Predicting latent variables with extracted Encoder model \n>>> reduced_X = encoder.predict(x_vars_stdscle)  \n```", "```py\n >>> print (reduced_X.shape) \n```", "```py\n>>> zero_x, zero_y = [],[] ; one_x, one_y = [],[] \n>>> two_x,two_y = [],[]; three_x, three_y = [],[] \n>>> four_x,four_y = [],[]; five_x,five_y = [],[] \n>>> six_x,six_y = [],[]; seven_x,seven_y = [],[] \n>>> eight_x,eight_y = [],[]; nine_x,nine_y = [],[] \n\n# For 2-Dimensional data \n>>> for i in range(len(reduced_X)): \n...     if y[i] == 0: \n...         zero_x.append(reduced_X[i][0]) \n...         zero_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 1: \n...         one_x.append(reduced_X[i][0]) \n...         one_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 2: \n...         two_x.append(reduced_X[i][0]) \n...         two_y.append(reduced_X[i][1]) \n ...     elif y[i] == 3: \n...         three_x.append(reduced_X[i][0]) \n...         three_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 4: \n...         four_x.append(reduced_X[i][0]) \n...         four_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 5: \n...         five_x.append(reduced_X[i][0]) \n...         five_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 6: \n...         six_x.append(reduced_X[i][0]) \n...         six_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 7: \n...         seven_x.append(reduced_X[i][0]) \n...         seven_y.append(reduced_X[i][1]) \n\n...     elif y[i] == 8: \n...         eight_x.append(reduced_X[i][0]) \n ...        eight_y.append(reduced_X[i][1]) \n\n ...    elif y[i] == 9: \n ...        nine_x.append(reduced_X[i][0]) \n ...        nine_y.append(reduced_X[i][1]) \n\n>>> zero = plt.scatter(zero_x, zero_y, c='r', marker='x',label='zero') \n>>> one = plt.scatter(one_x, one_y, c='g', marker='+') \n>>> two = plt.scatter(two_x, two_y, c='b', marker='s') \n\n>>> three = plt.scatter(three_x, three_y, c='m', marker='*') \n>>> four = plt.scatter(four_x, four_y, c='c', marker='h') \n>>> five = plt.scatter(five_x, five_y, c='r', marker='D') \n\n>>> six = plt.scatter(six_x, six_y, c='y', marker='8') \n>>> seven = plt.scatter(seven_x, seven_y, c='k', marker='*') \n>>> eight = plt.scatter(eight_x, eight_y, c='r', marker='x') \n\n>>> nine = plt.scatter(nine_x, nine_y, c='b', marker='D') \n\n>>> plt.legend((zero,one,two,three,four,five,six,seven,eight,nine), \n...  ('zero','one','two','three','four','five','six','seven','eight','nine'), \n...            scatterpoints=1,loc='lower right',ncol=3,fontsize=10) \n\n>>> plt.xlabel('Latent Feature 1',fontsize = 13) \n>>> plt.ylabel('Latent Feature 2',fontsize = 13) \n\n>>> plt.show() \n```", "```py\n# 3-Dimensional architecture \n>>> input_layer = Input(shape=(64,),name=\"input\") \n\n>>> encoded = Dense(32, activation='relu',name=\"h1encode\")(input_layer) \n>>> encoded = Dense(16, activation='relu',name=\"h2encode\")(encoded) \n>>> encoded = Dense(3, activation='relu',name=\"h3latent_layer\")(encoded) \n\n>>> decoded = Dense(16, activation='relu',name=\"h4decode\")(encoded) \n>>> decoded = Dense(32, activation='relu',name=\"h5decode\")(decoded) \n>>> decoded = Dense(64, activation='sigmoid',name=\"h6decode\")(decoded) \n\n>>> autoencoder = Model(input_layer, decoded) \nautoencoder.compile(optimizer=\"adam\", loss=\"mse\") \n\n# Fitting Encoder-Decoder model \n>>> autoencoder.fit(x_vars_stdscle, x_vars_stdscle, epochs=100,batch_size=256, shuffle=True,validation_split= 0.2) \n```", "```py\n# Extracting Encoder section of the Model for prediction of latent variables \n>>> encoder = Model(autoencoder.input,autoencoder.get_layer(\"h3latent_layer\").output) \n\n# Predicting latent variables with extracted Encoder model \n>>> reduced_X3D = encoder.predict(x_vars_stdscle) \n\n>>> zero_x, zero_y,zero_z = [],[],[] ; one_x, one_y,one_z = [],[],[] \n>>> two_x,two_y,two_z = [],[],[]; three_x, three_y,three_z = [],[],[] \n>>> four_x,four_y,four_z = [],[],[]; five_x,five_y,five_z = [],[],[] \n>>> six_x,six_y,six_z = [],[],[]; seven_x,seven_y,seven_z = [],[],[] \n>>> eight_x,eight_y,eight_z = [],[],[]; nine_x,nine_y,nine_z = [],[],[] \n\n>>> for i in range(len(reduced_X3D)): \n\n...     if y[i]==10: \n...         continue \n\n...     elif y[i] == 0: \n...         zero_x.append(reduced_X3D[i][0]) \n...         zero_y.append(reduced_X3D[i][1]) \n...         zero_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 1: \n...         one_x.append(reduced_X3D[i][0]) \n...         one_y.append(reduced_X3D[i][1]) \n...         one_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 2: \n...         two_x.append(reduced_X3D[i][0]) \n...         two_y.append(reduced_X3D[i][1]) \n...         two_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 3: \n...         three_x.append(reduced_X3D[i][0]) \n...         three_y.append(reduced_X3D[i][1]) \n...         three_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 4: \n...         four_x.append(reduced_X3D[i][0]) \n...         four_y.append(reduced_X3D[i][1]) \n...         four_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 5: \n...         five_x.append(reduced_X3D[i][0]) \n...         five_y.append(reduced_X3D[i][1]) \n...         five_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 6: \n...         six_x.append(reduced_X3D[i][0]) \n...         six_y.append(reduced_X3D[i][1]) \n...         six_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 7: \n...         seven_x.append(reduced_X3D[i][0]) \n...         seven_y.append(reduced_X3D[i][1]) \n...         seven_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 8: \n...         eight_x.append(reduced_X3D[i][0]) \n...         eight_y.append(reduced_X3D[i][1]) \n...         eight_z.append(reduced_X3D[i][2]) \n\n...     elif y[i] == 9: \n...         nine_x.append(reduced_X3D[i][0]) \n...         nine_y.append(reduced_X3D[i][1]) \n...         nine_z.append(reduced_X3D[i][2]) \n\n # 3- Dimensional plot \n>>> from mpl_toolkits.mplot3d import Axes3D \n>>> fig = plt.figure() \n>>> ax = fig.add_subplot(111, projection='3d') \n\n>>> ax.scatter(zero_x, zero_y,zero_z, c='r', marker='x',label='zero') \n>>> ax.scatter(one_x, one_y,one_z, c='g', marker='+',label='one') \n>>> ax.scatter(two_x, two_y,two_z, c='b', marker='s',label='two') \n\n>>> ax.scatter(three_x, three_y,three_z, c='m', marker='*',label='three') \n>>> ax.scatter(four_x, four_y,four_z, c='c', marker='h',label='four') \n>>> ax.scatter(five_x, five_y,five_z, c='r', marker='D',label='five') \n\n>>> ax.scatter(six_x, six_y,six_z, c='y', marker='8',label='six') \n>>> ax.scatter(seven_x, seven_y,seven_z, c='k', marker='*',label='seven') \n>>> ax.scatter(eight_x, eight_y,eight_z, c='r', marker='x',label='eight') \n >>> ax.scatter(nine_x, nine_y,nine_z, c='b', marker='D',label='nine') \n\n>>> ax.set_xlabel('Latent Feature 1',fontsize = 13) \n>>> ax.set_ylabel('Latent Feature 2',fontsize = 13) \n>>> ax.set_zlabel('Latent Feature 3',fontsize = 13) \n\n>>> ax.set_xlim3d(0,60) \n\n>>> plt.legend(loc='upper left', numpoints=1, ncol=3, fontsize=10, bbox_to_anchor=(0, 0)) \n\n>>> plt.show() \n```"]