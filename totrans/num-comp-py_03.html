<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">K-Nearest Neighbors and Naive Bayes</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we have learned about computationally intensive methods. In contrast, this chapter discusses the simple methods to balance it out! We will be covering the two techniques, called <strong>k-nearest neighbors</strong> (<strong>KNN</strong>)and Naive Bayes here. Before touching on KNN, we explained the issue with the curse of dimensionality with a simulated example. Subsequently, breast cancer medical examples have been utilized to predict whether the cancer is malignant or benign using KNN. In the final section of the chapter, Naive Bayes has been explained with spam/ham classification, which also involves the application of the <strong>natural language processing</strong> (<strong>NLP</strong>) techniques consisting of the following basic preprocessing and modeling steps:</p>
<ul>
<li>Punctuation removal</li>
<li>Word tokenization and lowercase conversion</li>
<li>Stopwords removal</li>
<li>Stemming</li>
<li>Lemmatization with POS tagging</li>
<li>Conversion of words into TF-IDF to create numerical representation of words</li>
<li>Application of the Naive Bayes model on TF-IDF vectors to predict if the message is either spam or ham on both train and test data</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-nearest neighbors</h1>
                </header>
            
            <article>
                
<p>K-nearest neighbors is a non-parametric machine learning model in which the model memorizes the training observation for classifying the unseen test data. It can also be called instance-based learning. This model is often termed as lazy learning, as it does not learn anything during the training phase like regression, random forest, and so on. Instead, it starts working only during the testing/evaluation phase to compare the given test observations with the nearest training observations, which will take significant time in comparing each test data point. Hence, this technique is not efficient on big data; also, performance does deteriorate when the number of variables is high due to the <strong>curse of dimensionality</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">KNN voter example</h1>
                </header>
            
            <article>
                
<p>KNN is explained better with the following short example. The objective is to predict the party for which voter will vote based on their neighborhood, precisely geolocation (latitude and longitude). Here we assume that we can identify the potential voter to which political party they would be voting based on majority voters did vote for that particular party in that vicinity so that they have a high probability to vote for the majority party. However, tuning the k-value (number to consider, among which majority should be counted) is the million-dollar question (as same as any machine learning algorithm):</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/b6e598c7-2e69-459d-b77a-32f57fb361f4.png" style="width:25.75em;height:20.08em;"/></div>
<p>In the preceding diagram, we can see that the voter of the study will vote for <strong>Party 2</strong>. As within the vicinity, one neighbor has voted for <strong>Party 1</strong> and the other voter voted for <strong>Party 3</strong>. But three voters voted for <strong>Party 2</strong>. In fact, by this way, KNN solves any given classification problem. Regression problems are solved by taking mean of its neighbors within the given circle or vicinity or k-value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Curse of dimensionality</h1>
                </header>
            
            <article>
                
<p>KNN completely depends on distance. Hence, it is worth studying about the curse of dimensionality to understand when KNN deteriorates its predictive power with the increase in the number of variables required for prediction. This is an obvious fact that high-dimensional spaces are vast. Points in high-dimensional spaces tend to be dispersing from each other more compared with the points in low-dimensional space. Though there are many ways to check the curve of dimensionality, here we are using uniform random values between zero and one generated for 1D, 2D, and 3D space to validate this hypothesis.</p>
<p>In the following lines of codes, the mean distance between 1,000 observations has been calculated with the change in dimensions. It is apparent that with the increase in dimensions, distance between points increases logarithmically, which gives us the hint that we need to have an exponential increase in data points with increase in dimensions in order to make machine learning algorithms work correctly:</p>
<pre><strong>&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; import pandas as pd 
 
# KNN Curse of Dimensionality 
&gt;&gt;&gt; import random,math</strong> </pre>
<p>The following code generates random numbers between zero and one from uniform distribution with the given dimension, which is equivalent of length of array or list:</p>
<pre><strong>&gt;&gt;&gt; def random_point_gen(dimension): 
...     return [random.random() for _ in range(dimension)]</strong> </pre>
<p>The following function calculates root mean sum of squares of Euclidean distances (2-norm) between points by taking the difference between points and sum the squares and finally takes the square root of total distance:</p>
<pre><strong>&gt;&gt;&gt; def distance(v,w): 
...     vec_sub = [v_i-w_i for v_i,w_i in zip(v,w)] 
...     sum_of_sqrs = sum(v_i*v_i for v_i in vec_sub) 
...     return math.sqrt(sum_of_sqrs)</strong> </pre>
<p>Both dimension and number of pairs are utilized for calculating the distances with the following code:</p>
<pre><strong>&gt;&gt;&gt; def random_distances_comparison(dimension,number_pairs): 
...     return [distance(random_point_gen(dimension),random_point_gen(dimension)) 
            for _ in range(number_pairs)] 
 
&gt;&gt;&gt; def mean(x): 
...     return sum(x) / len(x)</strong> </pre>
<p>The experiment has been done by changing dimensions from 1 to 201 with an increase of 5 dimensions to check the increase in distance:</p>
<pre><strong>&gt;&gt;&gt; dimensions = range(1, 201, 5)</strong></pre>
<p>Both minimum and average distances have been calculated to check, however, both illustrate the similar story:</p>
<pre><strong>&gt;&gt;&gt; avg_distances = [] 
&gt;&gt;&gt; min_distances = [] 
 
&gt;&gt;&gt; dummyarray = np.empty((20,4)) 
&gt;&gt;&gt; dist_vals = pd.DataFrame(dummyarray) 
&gt;&gt;&gt; dist_vals.columns = ["Dimension","Min_Distance","Avg_Distance","Min/Avg_Distance"] 
 
&gt;&gt;&gt; random.seed(34) 
&gt;&gt;&gt; i = 0 
&gt;&gt;&gt; for dims in dimensions: 
...     distances = random_distances_comparison(dims, 1000)   
...     avg_distances.append(mean(distances))     
...     min_distances.append(min(distances))      
     
...     dist_vals.loc[i,"Dimension"] = dims 
...     dist_vals.loc[i,"Min_Distance"] = min(distances) 
...     dist_vals.loc[i,"Avg_Distance"] = mean(distances) 
...     dist_vals.loc[i,"Min/Avg_Distance"] = min(distances)/mean(distances) 
                  
...     print(dims, min(distances), mean(distances), min(distances)*1.0 / mean( distances))</strong> 
<strong>...     i = i+1 
 
# Plotting Average distances for Various Dimensions 
&gt;&gt;&gt; import matplotlib.pyplot as plt 
&gt;&gt;&gt; plt.figure() 
&gt;&gt;&gt; plt.xlabel('Dimensions') 
&gt;&gt;&gt; plt.ylabel('Avg. Distance') 
&gt;&gt;&gt; plt.plot(dist_vals["Dimension"],dist_vals["Avg_Distance"]) 
&gt;&gt;&gt; plt.legend(loc='best') 
 
&gt;&gt;&gt; plt.show() </strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/befd95be-272c-43f4-81a1-8595f9aa4d17.png" style="width:37.17em;height:34.42em;"/></div>
<div class="packt_infobox">From the preceding graph, it is proved that with the increase in dimensions, mean distance increases logarithmically. Hence the higher the dimensions, the more data is needed to overcome the curse of dimensionality!</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Curse of dimensionality with 1D, 2D, and 3D example</h1>
                </header>
            
            <article>
                
<p>A quick analysis has been done to see how distance 60 random points are expanding with the increase in dimensionality. Initially, random points are drawn for one-dimension:</p>
<pre><strong># 1-Dimension Plot 
&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; import pandas as pd 
&gt;&gt;&gt; import matplotlib.pyplot as plt 
 
&gt;&gt;&gt; one_d_data = np.random.rand(60,1) 
&gt;&gt;&gt; one_d_data_df = pd.DataFrame(one_d_data) 
&gt;&gt;&gt; one_d_data_df.columns = ["1D_Data"] 
&gt;&gt;&gt; one_d_data_df["height"] = 1 
 
&gt;&gt;&gt; plt.figure() 
&gt;&gt;&gt; plt.scatter(one_d_data_df['1D_Data'],one_d_data_df["height"]) 
&gt;&gt;&gt; plt.yticks([]) 
&gt;&gt;&gt; plt.xlabel("1-D points") 
&gt;&gt;&gt; plt.show()</strong></pre>
<p>If we observe the following graph, all 60 data points are very nearby in one-dimension:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/e05c54d8-cedf-4930-90ac-361d35736b77.png" style="width:30.25em;height:28.17em;"/></div>
<p>Here we are repeating the same experiment in a 2D space, by taking 60 random numbers with <em>x</em> and <em>y</em> coordinate space and plotted them visually:</p>
<pre><strong># 2- Dimensions Plot 
&gt;&gt;&gt; two_d_data = np.random.rand(60,2) 
&gt;&gt;&gt; two_d_data_df = pd.DataFrame(two_d_data) 
&gt;&gt;&gt; two_d_data_df.columns = ["x_axis","y_axis"] 
 
&gt;&gt;&gt; plt.figure() 
&gt;&gt;&gt; plt.scatter(two_d_data_df['x_axis'],two_d_data_df["y_axis"]) 
&gt;&gt;&gt; plt.xlabel("x_axis");plt.ylabel("y_axis") 
&gt;&gt;&gt; plt.show()</strong>  </pre>
<p>By observing the 2D graph we can see that more gaps have been appearing for the same 60 data points:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/f9e699fd-c223-4755-948a-53ccbfef48c3.png" style="width:37.83em;height:32.00em;"/></div>
<p>Finally, 60 data points are drawn for 3D space. We can see a further increase in spaces, which is very apparent. This has proven to us visually by now that with the increase in dimensions, it creates a lot of space, which makes a classifier weak to detect the signal:</p>
<pre><strong># 3- Dimensions Plot 
&gt;&gt;&gt; three_d_data = np.random.rand(60,3) 
&gt;&gt;&gt; three_d_data_df = pd.DataFrame(three_d_data) 
&gt;&gt;&gt; three_d_data_df.columns = ["x_axis","y_axis","z_axis"] 
 
&gt;&gt;&gt; from mpl_toolkits.mplot3d import Axes3D 
&gt;&gt;&gt; fig = plt.figure() 
&gt;&gt;&gt; ax = fig.add_subplot(111, projection='3d') 
&gt;&gt;&gt; ax.scatter(three_d_data_df['x_axis'],three_d_data_df["y_axis"],three_d_data_df ["z_axis"]) 
&gt;&gt;&gt; plt.show()</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/d36eb8b9-0634-45b7-8c12-41eb2711b1a6.png" style="width:30.67em;height:29.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">KNN classifier with breast cancer Wisconsin data example</h1>
                </header>
            
            <article>
                
<p>Breast cancer data has been utilized from the UCI machine learning repository <a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29"><span class="URLPACKT">http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29</span></a> for illustration purposes. Here the task is to find whether the cancer is malignant or benign based on various collected features such as clump thickness and so on using the KNN classifier:</p>
<pre><strong># KNN Classifier - Breast Cancer 
&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; import pandas as pd 
&gt;&gt;&gt; from sklearn.metrics import accuracy_score,classification_report 
&gt;&gt;&gt; breast_cancer = pd.read_csv("Breast_Cancer_Wisconsin.csv") </strong></pre>
<p>The following are the first few rows to show how the data looks like. The <kbd>Class</kbd> value has class <kbd>2</kbd> and <kbd>4</kbd>. Value <kbd>2</kbd> and <kbd>4</kbd> represent benign and malignant class, respectively. Whereas all the other variables do vary between value <kbd>1</kbd> and <kbd>10</kbd>, which are very much categorical in nature:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="image-border" src="assets/66458e11-de44-4ba8-9bbd-d96eacae2b68.png"/></div>
<p>Only the <kbd>Bare_Nuclei</kbd> variable has some missing values, here we are replacing them with the most frequent value (category value <kbd>1</kbd>) in the following code:</p>
<pre><strong>&gt;&gt;&gt; breast_cancer['Bare_Nuclei'] = breast_cancer['Bare_Nuclei'].replace('?', np.NAN) 
&gt;&gt;&gt; breast_cancer['Bare_Nuclei'] = breast_cancer['Bare_Nuclei'].fillna(breast_cancer[ 'Bare_Nuclei'].value_counts().index[0])</strong> </pre>
<p>Use the following code to convert the classes to a <kbd>0</kbd> and <kbd>1</kbd> indicator for using in the classifier:</p>
<pre><strong>&gt;&gt;&gt; breast_cancer['Cancer_Ind'] = 0 
&gt;&gt;&gt; breast_cancer.loc[breast_cancer['Class']==4,'Cancer_Ind'] = 1</strong></pre>
<p>In the following code, we are dropping non-value added variables from analysis:</p>
<pre><strong>&gt;&gt;&gt; x_vars = breast_cancer.drop(['ID_Number','Class','Cancer_Ind'],axis=1) 
&gt;&gt;&gt; y_var = breast_cancer['Cancer_Ind'] 
&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler 
&gt;&gt;&gt; x_vars_stdscle = StandardScaler().fit_transform(x_vars.values) 
&gt;&gt;&gt; from sklearn.model_selection import train_test_split </strong></pre>
<p>As KNN is very sensitive to distances, here we are standardizing all the columns before applying algorithms:</p>
<pre><strong>&gt;&gt;&gt; x_vars_stdscle_df = pd.DataFrame(x_vars_stdscle, index=x_vars.index, columns=x_vars.columns) 
&gt;&gt;&gt; x_train,x_test,y_train,y_test = train_test_split(x_vars_stdscle_df,y_var, train_size = 0.7,random_state=42)</strong></pre>
<p>KNN classifier is being applied with neighbor value of <kbd>3</kbd> and <kbd>p</kbd> value indicates it is 2-norm, also known as Euclidean distance for computing classes:</p>
<pre><strong>&gt;&gt;&gt; from sklearn.neighbors import KNeighborsClassifier 
&gt;&gt;&gt; knn_fit = KNeighborsClassifier(n_neighbors=3,p=2,metric='minkowski') 
&gt;&gt;&gt; knn_fit.fit(x_train,y_train) 
 
&gt;&gt;&gt; print ("\nK-Nearest Neighbors - Train Confusion Matrix\n\n",pd.crosstab(y_train, knn_fit.predict(x_train),rownames = ["Actuall"],colnames = ["Predicted"]) )      
&gt;&gt;&gt; print ("\nK-Nearest Neighbors - Train accuracy:",round(accuracy_score(y_train, knn_fit.predict(x_train)),3)) 
&gt;&gt;&gt; print ("\nK-Nearest Neighbors - Train Classification Report\n", classification_report( y_train,knn_fit.predict(x_train))) 
 
&gt;&gt;&gt; print ("\n\nK-Nearest Neighbors - Test Confusion Matrix\n\n",pd.crosstab(y_test, knn_fit.predict(x_test),rownames = ["Actuall"],colnames = ["Predicted"]))       
&gt;&gt;&gt; print ("\nK-Nearest Neighbors - Test accuracy:",round(accuracy_score( y_test,knn_fit.predict(x_test)),3)) 
&gt;&gt;&gt; print ("\nK-Nearest Neighbors - Test Classification Report\n", classification_report(y_test,knn_fit.predict(x_test)))</strong> 
 </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/915405ac-f99e-4459-9f70-e50a7cd73812.png" style="width:35.17em;height:46.83em;"/></div>
<p>From the results, it is appearing that KNN is working very well in classifying malignant and benign classes well, obtaining test accuracy of 97.6 percent with 96 percent of recall on malignant class. The only deficiency of KNN classifier would be, it is computationally intensive during test phase, as each test observation will be compared with all the available observations in train data, which practically KNN does not learn a thing from training data. Hence, we are also calling it a lazy classifier!</p>
<p>The R code for KNN classifier is as follows:</p>
<pre><strong># KNN Classifier </strong><br/><strong>setwd("D:\\Book writing\\Codes\\Chapter 5") </strong><br/><strong>breast_cancer = read.csv("Breast_Cancer_Wisconsin.csv") </strong><br/><br/><strong># Column Bare_Nuclei have some missing values with "?" in place, we are replacing with median values </strong><br/><strong># As Bare_Nuclei is discrete variable </strong><br/><strong>breast_cancer$Bare_Nuclei = as.character(breast_cancer$Bare_Nuclei)</strong><br/><strong>breast_cancer$Bare_Nuclei[breast_cancer$Bare_Nuclei=="?"] = median(breast_cancer$Bare_Nuclei,na.rm = TRUE)</strong><br/><strong>breast_cancer$Bare_Nuclei = as.integer(breast_cancer$Bare_Nuclei)</strong><br/><strong># Classes are 2 &amp; 4 for benign &amp; malignant respectively, we # have converted # </strong><br/><strong>to zero-one problem, as it is easy to convert to work # around with models </strong><br/><strong>breast_cancer$Cancer_Ind = 0</strong><br/><strong>breast_cancer$Cancer_Ind[breast_cancer$Class==4]=1</strong><br/><strong>breast_cancer$Cancer_Ind = as.factor( breast_cancer$Cancer_Ind) </strong><br/><br/><strong># We have removed unique id number from modeling as unique # numbers does not provide value in modeling </strong><br/><strong># In addition, original class variable also will be removed # as the same has been replaced with derived variable</strong> <br/><br/><strong>remove_cols = c("ID_Number","Class") </strong><br/><strong>breast_cancer_new = breast_cancer[,!(names(breast_cancer) %in% remove_cols)] </strong><br/><br/><strong># Setting seed value for producing repetitive results </strong><br/><strong># 70-30 split has been made on the data </strong><br/><br/><strong>set.seed(123) </strong><br/><strong>numrow = nrow(breast_cancer_new) </strong><br/><strong>trnind = sample(1:numrow,size = as.integer(0.7*numrow)) </strong><br/><strong>train_data = breast_cancer_new[trnind,] </strong><br/><strong>test_data = breast_cancer_new[-trnind,] </strong><br/><br/><strong># Following is classical code for computing accuracy, # precision &amp; recall </strong><br/><br/><strong>frac_trzero = (table(train_data$Cancer_Ind)[[1]])/nrow(train_data)</strong><br/><strong>frac_trone = (table(train_data$Cancer_Ind)[[2]])/nrow(train_data)</strong><br/><br/><strong>frac_tszero = (table(test_data$Cancer_Ind)[[1]])/nrow(test_data)</strong><br/><strong>frac_tsone = (table(test_data$Cancer_Ind)[[2]])/nrow(test_data)</strong><br/><br/><strong>prec_zero &lt;- function(act,pred){ tble = table(act,pred)</strong><br/><strong>return( round( tble[1,1]/(tble[1,1]+tble[2,1]),4) ) } </strong><br/><br/><strong>prec_one &lt;- function(act,pred){ tble = table(act,pred)</strong><br/><strong>return( round( tble[2,2]/(tble[2,2]+tble[1,2]),4) ) } </strong><br/><br/><strong>recl_zero &lt;- function(act,pred){tble = table(act,pred) </strong><br/><strong>return( round( tble[1,1]/(tble[1,1]+tble[1,2]),4) ) } </strong><br/><br/><strong>recl_one &lt;- function(act,pred){ tble = table(act,pred) </strong><br/><strong>return( round( tble[2,2]/(tble[2,2]+tble[2,1]),4) ) } </strong><br/><br/><strong>accrcy &lt;- function(act,pred){ tble = table(act,pred) </strong><br/><strong>return( round((tble[1,1]+tble[2,2])/sum(tble),4)) }</strong> <br/><br/><strong># Importing Class package in which KNN function do present library(class) </strong><br/><br/><strong># Choosing sample k-value as 3 &amp; apply on train &amp; test data # respectively </strong><br/><br/><strong>k_value = 3 </strong><br/><strong>tr_y_pred = knn(train_data,train_data,train_data$Cancer_Ind,k=k_value)</strong><br/><strong>ts_y_pred = knn(train_data,test_data,train_data$Cancer_Ind,k=k_value) </strong><br/><br/><strong># Calculating confusion matrix, accuracy, precision &amp; # recall on train data </strong><br/><br/><strong>tr_y_act = train_data$Cancer_Ind;ts_y_act = test_data$Cancer_Ind</strong><br/><strong>tr_tble = table(tr_y_act,tr_y_pred) </strong><br/><strong>print(paste("Train Confusion Matrix")) </strong><br/><strong>print(tr_tble) </strong><br/><br/><strong>tr_acc = accrcy(tr_y_act,tr_y_pred) </strong><br/><strong>trprec_zero = prec_zero(tr_y_act,tr_y_pred); trrecl_zero = recl_zero(tr_y_act,tr_y_pred) </strong><br/><strong>trprec_one = prec_one(tr_y_act,tr_y_pred); trrecl_one = recl_one(tr_y_act,tr_y_pred)</strong> <br/><strong>trprec_ovll = trprec_zero *frac_trzero + trprec_one*frac_trone</strong><br/><strong>trrecl_ovll = trrecl_zero *frac_trzero + trrecl_one*frac_trone</strong><br/><br/><strong>print(paste("KNN Train accuracy:",tr_acc)) </strong><br/><strong>print(paste("KNN - Train Classification Report"))</strong><br/><strong>print(paste("Zero_Precision",trprec_zero,"Zero_Recall",trrecl_zero))</strong><br/><strong>print(paste("One_Precision",trprec_one,"One_Recall",trrecl_one))</strong><br/><strong>print(paste("Overall_Precision",round(trprec_ovll,4),"Overall_Recall",round(trrecl_ovll,4))) </strong><br/><br/><strong># Calculating confusion matrix, accuracy, precision &amp; # recall on test data </strong><br/><br/><strong>ts_tble = table(ts_y_act, ts_y_pred) </strong><br/><strong>print(paste("Test Confusion Matrix")) </strong><br/><strong>print(ts_tble) </strong><br/><br/><strong>ts_acc = accrcy(ts_y_act,ts_y_pred) </strong><br/><strong>tsprec_zero = prec_zero(ts_y_act,ts_y_pred); tsrecl_zero = recl_zero(ts_y_act,ts_y_pred) </strong><br/><strong>tsprec_one = prec_one(ts_y_act,ts_y_pred); tsrecl_one = recl_one(ts_y_act,ts_y_pred)</strong> <br/><br/><strong>tsprec_ovll = tsprec_zero *frac_tszero + tsprec_one*frac_tsone</strong><br/><strong>tsrecl_ovll = tsrecl_zero *frac_tszero + tsrecl_one*frac_tsone</strong><br/><br/><strong>print(paste("KNN Test accuracy:",ts_acc)) </strong><br/><strong>print(paste("KNN - Test Classification Report"))</strong><br/><strong>print(paste("Zero_Precision",tsprec_zero,"Zero_Recall",tsrecl_zero))</strong><br/><strong>print(paste("One_Precision",tsprec_one,"One_Recall",tsrecl_one))</strong><br/><strong>print(paste("Overall_Precision",round(tsprec_ovll,4),"Overall_Recall",round(tsrecl_ovll,4)))</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning of k-value in KNN classifier</h1>
                </header>
            
            <article>
                
<p>In the previous section, we just checked with only the k-value of three. Actually, in any machine learning algorithm, we need to tune the knobs to check where the better performance can be obtained. In the case of KNN, the only tuning parameter is k-value. Hence, in the following code, we are determining the best k-value with grid search:</p>
<pre><strong># Tuning of K- value for Train &amp; Test data 
&gt;&gt;&gt; dummyarray = np.empty((5,3)) 
&gt;&gt;&gt; k_valchart = pd.DataFrame(dummyarray) 
&gt;&gt;&gt; k_valchart.columns = ["K_value","Train_acc","Test_acc"] 
 
&gt;&gt;&gt; k_vals = [1,2,3,4,5] 
 
&gt;&gt;&gt; for i in range(len(k_vals)): 
...     knn_fit = KNeighborsClassifier(n_neighbors=k_vals[i],p=2,metric='minkowski') 
...     knn_fit.fit(x_train,y_train) 
 
...     print ("\nK-value",k_vals[i]) 
     
...     tr_accscore = round(accuracy_score(y_train,knn_fit.predict(x_train)),3) 
...     print ("\nK-Nearest Neighbors - Train Confusion</strong> <strong>Matrix\n\n",pd.crosstab( y_train, knn_fit.predict(x_train),rownames = ["Actuall"],colnames = ["Predicted"]) )      
...     print ("\nK-Nearest Neighbors - Train accuracy:",tr_accscore) 
...     print ("\nK-Nearest Neighbors - Train Classification Report\n", classification_report(y_train,knn_fit.predict(x_train))) 
 
...     ts_accscore = round(accuracy_score(y_test,knn_fit.predict(x_test)),3)     
...     print ("\n\nK-Nearest Neighbors - Test Confusion Matrix\n\n",pd.crosstab( y_test,knn_fit.predict(x_test),rownames = ["Actuall"],colnames = ["Predicted"]))       
...     print ("\nK-Nearest Neighbors - Test accuracy:",ts_accscore) 
...     print ("\nK-Nearest Neighbors - Test Classification Report\n",classification_report(y_test,knn_fit.predict(x_test))) 
     
...     k_valchart.loc[i, 'K_value'] = k_vals[i]       
...     k_valchart.loc[i, 'Train_acc'] = tr_accscore      
...     k_valchart.loc[i, 'Test_acc'] = ts_accscore                
 
# Ploting accuracies over varied K-values 
&gt;&gt;&gt; import matplotlib.pyplot as plt</strong> 
<strong>&gt;&gt;&gt; plt.figure() 
&gt;&gt;&gt; plt.xlabel('K-value') 
&gt;&gt;&gt; plt.ylabel('Accuracy') 
&gt;&gt;&gt; plt.plot(k_valchart["K_value"],k_valchart["Train_acc"]) 
&gt;&gt;&gt; plt.plot(k_valchart["K_value"],k_valchart["Test_acc"]) 
 
&gt;&gt;&gt; plt.axis([0.9,5, 0.92, 1.005]) 
&gt;&gt;&gt; plt.xticks([1,2,3,4,5]) 
 
&gt;&gt;&gt; for a,b in zip(k_valchart["K_value"],k_valchart["Train_acc"]): 
...     plt.text(a, b, str(b),fontsize=10) 
 
&gt;&gt;&gt; for a,b in zip(k_valchart["K_value"],k_valchart["Test_acc"]): 
...     plt.text(a, b, str(b),fontsize=10) 
     
&gt;&gt;&gt; plt.legend(loc='upper right')     
&gt;&gt;&gt; plt.show()</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/d729f290-0d04-4a49-9cd4-540513d9c272.png" style="width:37.50em;height:32.42em;"/></div>
<p>It appears that with less value of k-value, it has more overfitting problems due to the very high value of accuracy on train data and less on test data, with the increase in k-value more the train and test accuracies are converging and becoming more robust. This phenomenon illustrates the typical machine learning phenomenon. As for further analysis, readers are encouraged to try k-values higher than five and see how train and test accuracies are changing. The R code for tuning of k-value in KNN classifier is as follows:</p>
<pre><strong># Tuning of K-value on Train &amp; Test Data </strong><br/><strong>k_valchart = data.frame(matrix( nrow=5, ncol=3)) </strong><br/><strong>colnames(k_valchart) = c("K_value","Train_acc","Test_acc") </strong><br/><strong>k_vals = c(1,2,3,4,5) </strong><br/><br/><strong>i = 1</strong><br/><strong>for (kv in k_vals) { </strong><br/><strong>  tr_y_pred = knn(train_data,train_data,train_data$Cancer_Ind,k=kv)</strong><br/><strong>  ts_y_pred = knn(train_data,test_data,train_data$Cancer_Ind,k=kv)</strong><br/><strong>  tr_y_act = train_data$Cancer_Ind;ts_y_act = test_data$Cancer_Ind</strong><br/><strong>  tr_tble = table(tr_y_act,tr_y_pred) </strong><br/><strong>  print(paste("Train Confusion Matrix")) </strong><br/><strong>  print(tr_tble) </strong><br/><strong>  tr_acc = accrcy(tr_y_act,tr_y_pred) </strong><br/><strong>  trprec_zero = prec_zero(tr_y_act,tr_y_pred); trrecl_zero = recl_zero(tr_y_act, tr_y_pred) </strong><br/><strong>  trprec_one = prec_one(tr_y_act,tr_y_pred); trrecl_one = recl_one(tr_y_act,tr_y_pred) </strong><br/><strong>  trprec_ovll = trprec_zero *frac_trzero + trprec_one*frac_trone</strong><br/><strong>  trrecl_ovll = trrecl_zero *frac_trzero + trrecl_one*frac_trone</strong><br/><strong>  print(paste("KNN Train accuracy:",tr_acc)) </strong><br/><strong>  print(paste("KNN - Train Classification Report"))</strong><br/><br/><strong>print(paste("Zero_Precision",trprec_zero,"Zero_Recall",trrecl_zero))</strong><br/><strong>print(paste("One_Precision",trprec_one,"One_Recall",trrecl_one))</strong><br/><strong>print(paste("Overall_Precision",round(trprec_ovll,4),"Overall_Recall",round(trrecl_ovll,4))) </strong><br/><strong>  ts_tble = table(ts_y_act,ts_y_pred) </strong><br/><strong>  print(paste("Test Confusion Matrix")) </strong><br/><strong>  print(ts_tble)</strong><br/><strong>  ts_acc = accrcy(ts_y_act,ts_y_pred) </strong><br/><strong>  tsprec_zero = prec_zero(ts_y_act,ts_y_pred); tsrecl_zero = recl_zero(ts_y_act,ts_y_pred) </strong><br/><strong>  tsprec_one = prec_one(ts_y_act,ts_y_pred); tsrecl_one = recl_one(ts_y_act,ts_y_pred) </strong><br/><strong>  tsprec_ovll = tsprec_zero *frac_tszero + tsprec_one*frac_tsone</strong><br/><strong>  tsrecl_ovll = tsrecl_zero *frac_tszero + tsrecl_one*frac_tsone</strong><br/><br/><strong>  print(paste("KNN Test accuracy:",ts_acc)) </strong><br/><strong>  print(paste("KNN - Test Classification Report"))</strong><br/><br/><strong>print(paste("Zero_Precision",tsprec_zero,"Zero_Recall",tsrecl_zero))</strong><br/><strong>print(paste("One_Precision",tsprec_one,"One_Recall",tsrecl_one))</strong><br/><strong>print(paste("Overall_Precision",round(tsprec_ovll,4),"Overall_Recall",round(tsrecl_ovll,4)))</strong><br/><br/><strong>  k_valchart[i,1] =kv </strong><br/><strong>  k_valchart[i,2] =tr_acc </strong><br/><strong>  k_valchart[i,3] =ts_acc i = i+1 } </strong><br/><strong># Plotting the graph </strong><br/><strong>library(ggplot2) </strong><br/><strong>library(grid) </strong><br/><strong>ggplot(k_valchart, aes(K_value)) </strong><br/><strong>+ geom_line(aes(y = Train_acc, colour = "Train_Acc")) + </strong><br/><strong>geom_line(aes(y = Test_acc, colour = "Test_Acc"))+</strong><br/><strong>labs(x="K_value",y="Accuracy") + </strong><br/><strong>geom_text(aes(label = Train_acc, y = Train_acc), size = 3)+</strong><br/><strong>geom_text(aes(label = Test_acc, y = Test_acc), size = 3)</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes</h1>
                </header>
            
            <article>
                
<p>Bayes algorithm concept is quite old and exists from the 18th century. Thomas Bayes developed the foundational mathematical principles for determining the probability of unknown events from the known events. For example, if all apples are red in color and average diameter would be about 4 inches then, if at random one fruit is selected from the basket with red color and diameter of 3.7 inches, what is the probability that the particular fruit would be an apple? Naive term does assume independence of particular features in a class with respect to others. In this case, there would be no dependency between color and diameter. This independence assumption makes the Naive Bayes classifier most effective in terms of computational ease for particular tasks such as email classification based on words in which high dimensions of vocab do exist, even after assuming independence between features. Naive Bayes classifier performs surprisingly really well in practical applications.</p>
<p>Bayesian classifiers are best applied to problems in which information from a very high number of attributes should be considered simultaneously to estimate the probability of final outcome. Bayesian methods utilize all available evidence to consider for prediction even features have weak effects on the final outcome to predict. However, we should not ignore the fact that a large number of features with relatively minor effects, taken together its combined impact would form strong classifiers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Probability fundamentals</h1>
                </header>
            
            <article>
                
<p>Before diving into Naive Bayes, it would be good to reiterate the fundamentals. Probability of an event can be estimated from observed data by dividing the number of trails in which an event occurred with the total number of trails. For instance, if a bag contains red and blue balls and randomly picked <em>10</em> balls one by one with replacement and out of <em>10</em>, <em>3</em> red balls appeared in trails we can say that probability of red is <em>0.3</em>, <em>p<sub>red</sub> = 3/10 = 0.3</em>. Total probability of all possible outcomes must be 100 percent.</p>
<p>If a trail has two outcomes such as email classification either it is spam or ham and both cannot occur simultaneously, these events are considered as mutually exclusive with each other. In addition, if those outcomes cover all possible events, it would be called as <strong>exhaustive events</strong>. For example, in email classification if <em>P (spam) = 0.1</em>, we will be able to calculate <em>P (ham) = 1- 0.1 = 0.9</em>, these two events are mutually exclusive. In the following Venn diagram, all the email possible classes are represented (the entire universe) with the type of outcomes:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/0c902cba-e1f7-4e92-a1e4-79ad6f2499b8.png" style="width:22.83em;height:15.83em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Joint probability</h1>
                </header>
            
            <article>
                
<p>Though mutually exclusive cases are simple to work upon, most of the actual problems do fall under the category of non-mutually exclusive events. By using the joint appearance, we can predict the event outcome. For example, if emails messages present the word like <em>lottery</em>, which is very highly likely of being spam rather than ham. The following Venn diagram indicates the joint probability of spam with <em>lottery</em>. However, if you notice in detail, lottery circle is not contained completely within the spam circle. This implies that not all spam messages contain the word <em>lottery</em> and not every email with the word <em>lottery</em> is spam.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a4b84585-55e4-4155-8fec-f8d907e9e476.png" style="width:22.42em;height:17.67em;"/></div>
<p>In the following diagram, we have expanded the spam and ham category in addition to the <em>lottery</em> word in Venn diagram representation:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/6499a1f6-038f-48c8-ba33-b628ef50fb8b.png" style="width:34.00em;height:18.08em;"/></div>
<p>We have seen that 10 percent of all the emails are spam and 4 percent of emails have the word <em>lottery</em> and our task is to quantify the degree of overlap between these two proportions. In other words, we need to identify the joint probability of both <em>p(spam)</em> and <em>p(lottery)</em> occurring, which can be written as <em>p(spam ∩ lottery)</em>. In case if both the events are totally unrelated, they are called <strong>independent events</strong> and their respective value is <em>p(spam ∩ lottery) = p(spam) * p(lottery) = 0.1 * 0.04 = 0.004</em>, which is 0.4 percent of all messages are spam containing the word Lottery. In general, for independent events <em>P(A∩ B) = P(A) * P(B)</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding Bayes theorem with conditional probability</h1>
                </header>
            
            <article>
                
<p>Conditional probability provides a way of calculating relationships between dependent events using Bayes theorem. For example, <em>A</em> and <em>B</em> are two events and we would like to calculate <em>P(A\B)</em> can be read as the probability of an event occurring <em>A</em> given the fact that event <em>B</em> already occurred, in fact, this is known as <strong>conditional probability</strong>, the equation can be written as follows:</p>
<div style="padding-left: 150px" class="CDPAlignLeft CDPAlign"><img src="assets/0123470a-764e-4575-a679-9e30c8340e0e.png" style="width:17.08em;height:2.75em;"/></div>
<p>To understand better, we will now talk about the email classification example. Our objective is to predict whether an email is a spam given the word lottery and some other clues. In this case, we already knew the overall probability of spam, which is 10 percent also known as <strong>prior probability</strong>. Now suppose you have obtained an additional piece of information that probability of word lottery in all messages, which is 4 percent, also known as <strong>marginal likelihood</strong>. Now, we know the probability that <em>lottery</em> was used in previous spam messages and is called the <strong>likelihood</strong>.</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="image-border" src="assets/1d3a04f4-4cd8-42e8-b577-71e1f8e778e5.png" style="width:29.08em;height:11.50em;"/></div>
<p>By applying the Bayes theorem to the evidence, we can calculate the posterior probability that calculates the probability that the message is how likely a spam; given the fact that lottery was appearing in the message. On average if the probability is greater than 50 percent it indicates that the message is spam rather than ham.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a22f2b5e-d947-4383-a267-42b02ebbc4d6.png" style="width:33.58em;height:11.25em;"/></div>
<p>In the previous table, the sample frequency table that records the number of times <em>Lottery</em> appeared in spam and ham messages and its respective likelihood has been shown. Likelihood table reveals that <em>P(Lottery\Spam)= 3/22 = 0.13</em>, indicating that probability is 13 percent that a spam message contains the term <em>Lottery</em>. Subsequently we can calculate the <em>P(Spam ∩ Lottery) = P(Lottery\Spam) * P(Spam) = (3/22) * (22/100) = 0.03</em>. In order to calculate the posterior probability, we divide <em>P(Spam ∩ Lottery)</em> with <em>P(Lottery)</em>, which means <em>(3/22)*(22/100) / (4/100) = 0.75</em>. Therefore, the probability is 75 percent that a message is spam, given that message contains the word <em>Lottery</em>. Hence, don't believe in quick fortune guys!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes classification</h1>
                </header>
            
            <article>
                
<p>In the past example, we have seen with a single word called <em>lottery</em>, however, in this case, we will be discussing with a few more additional words such as <em>Million</em> and <em>Unsubscribe</em> to show how actual classifiers do work. Let us construct the likelihood table for the appearance of the three words (<em>W1</em>, <em>W2</em>, and <em>W3</em>), as shown in the following table for <em>100</em> emails:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a213ad61-ceab-4f9a-b03a-1d86c3651334.png" style="width:34.58em;height:8.42em;"/></div>
<p>When a new message is received, the posterior probability will be calculated to determine that email message is spam or ham. Let us assume that we have an email with terms <em>Lottery</em> and <em>Unsubscribe</em>, but it does not have word <em>Million</em> in it, with this details, what is the probability of spam?</p>
<p>By using Bayes theorem, we can define the problem as <em>Lottery = Yes</em>, <em>Million = No</em> and <em>Unsubscribe = Yes</em>:</p>
<div style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><img src="assets/7c004c3e-e6bc-43d3-9163-710c9b5798be.png" style="width:34.00em;height:3.08em;"/></div>
<p>Solving the preceding equations will have high computational complexity due to the dependency of words with each other. As a number of words are added, this will even explode and also huge memory will be needed for processing all possible intersecting events. This finally leads to intuitive turnaround with independence of words (<strong>cross-conditional independence</strong>) for which it got name of the Naive prefix for Bayes classifier. When both events are independent we can write <em>P(A ∩ B) = P(A) * P(B)</em>. In fact, this equivalence is much easier to compute with less memory requirement:</p>
<div style="padding-left: 30px" class="CDPAlignLeft CDPAlign"><img src="assets/7c4917c8-5737-4273-bf9d-8e906149c672.png" style="width:40.58em;height:2.92em;"/></div>
<p>In a similar way, we will calculate the probability for ham messages as well, as follows:</p>
<div style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><img src="assets/e0510142-46a4-4834-8928-885388599293.png" style="width:38.67em;height:2.92em;"/></div>
<p>By substituting the preceding likelihood table in the equations, due to the ratio of spam/ham we can just simply ignore the denominator terms in both the equations. Overall likelihood of spam is:</p>
<div style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><img src="assets/3008e654-9d2b-42b9-9f29-4cdd2f619bd8.png" style="width:34.42em;height:3.08em;"/></div>
<div style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><img src="assets/f26373a6-03d9-45f1-839a-e033b6d9ded8.png" style="width:35.42em;height:2.92em;"/></div>
<p>After calculating the ratio, <em>0.008864/0.004349 = 2.03</em>, which means that this message is two times more likely to be spam than ham. But we can calculate the probabilities as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>P(Spam) = 0.008864/(0.008864+0.004349) = 0.67</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>P(Ham) = 0.004349/(0.008864+0.004349) = 0.33</em></p>
<p>By converting likelihood values into probabilities, we can show in a presentable way for either to set-off some thresholds, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Laplace estimator</h1>
                </header>
            
            <article>
                
<p>In the previous calculation, all the values are nonzeros, which makes calculations well. Whereas in practice some words never appear in past for specific category and suddenly appear at later stages, which makes entire calculations as zeros.</p>
<p>For example, in the previous equation <em>W<sub>3</sub></em> did have a <em>0</em> value instead of <em>13</em>, and it will convert entire equations to <em>0</em> altogether:</p>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/4613c8e9-71b9-43dd-95e1-607855b3edf2.png" style="width:29.17em;height:2.83em;"/></div>
<p>In order to avoid this situation, Laplace estimator essentially adds a small number to each of the counts in the frequency table, which ensures that each feature has a nonzero probability of occurring with each class. Usually, Laplace estimator is set to <em>1</em>, which ensures that each class-feature combination is found in the data at least once:</p>
<div style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><img src="assets/4ed79e3d-5253-46f6-9efb-7ba0d5737898.png" style="width:29.83em;height:2.75em;"/></div>
<div class="packt_infobox">If you observe the equation carefully, value <em>1</em> is added to all three words in the numerator and at the same time, three has been added to all denominators to provide equivalence.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes SMS spam classification example</h1>
                </header>
            
            <article>
                
<p>Naive Bayes classifier has been developed using the SMS spam collection data available at <a href="http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/"><span class="URLPACKT">http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/</span></a>. In this chapter, various techniques available in NLP techniques have been discussed to preprocess prior to build the Naive Bayes model:</p>
<pre><strong>&gt;&gt;&gt; import csv 
 
&gt;&gt;&gt; smsdata = open('SMSSpamCollection.txt','r') 
&gt;&gt;&gt; csv_reader = csv.reader(smsdata,delimiter='\t') </strong></pre>
<p>The following <kbd>sys</kbd> package lines code can be used in case of any <kbd>utf-8</kbd> errors encountered while using older versions of Python, or else does not necessary with the latest version of Python 3.6:</p>
<pre><strong>&gt;&gt;&gt; import sys 
&gt;&gt;&gt; reload (sys) 
&gt;&gt;&gt; sys.setdefaultendocing('utf-8') </strong></pre>
<p>Normal coding starts from here as usual:</p>
<pre><strong>&gt;&gt;&gt; smsdata_data = [] 
&gt;&gt;&gt; smsdata_labels = [] 
 
&gt;&gt;&gt; for line in csv_reader: 
...     smsdata_labels.append(line[0]) 
...     smsdata_data.append(line[1]) 
 
&gt;&gt;&gt; smsdata.close()</strong> </pre>
<p>The following code prints the top 5 lines:</p>
<pre><strong>&gt;&gt;&gt; for i in range(5): 
...     print (smsdata_data[i],smsdata_labels[i])</strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/f57add7c-995c-4e5c-8979-de794701d32e.png"/></div>
<p>After getting preceding output run following code: </p>
<pre><strong>&gt;&gt;&gt; from collections import Counter 
&gt;&gt;&gt; c = Counter( smsdata_labels ) 
&gt;&gt;&gt; print(c)</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/99598fba-a2c0-42ac-ae0e-f3cd9e01cccd.png"/></div>
<p>Out of 5,572 observations, 4,825 are ham messages, which are about 86.5 percent and 747 spam messages are about remaining 13.4 percent.</p>
<p>Using NLP techniques, we have preprocessed the data for obtaining finalized word vectors to map with final outcomes spam or ham. Major preprocessing stages involved are:</p>
<ul>
<li><strong>Removal of punctuations</strong>: Punctuations needs to be removed before applying any further processing. Punctuations from the <kbd>string</kbd> library are <em>!"#$%&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~</em>, which are removed from all the messages.</li>
<li><strong>Word tokenization</strong>: Words are chunked from sentences based on white space for further processing.</li>
<li><strong>Converting words into lowercase</strong>: Converting to all lower case provides removal of duplicates, such as <em>Run</em> and <em>run</em>, where the first one comes at start of the sentence and the later one comes in the middle of the sentence, and so on, which all needs to be unified to remove duplicates as we are working on bag of words technique.</li>
<li><strong>Stop word removal</strong>: Stop words are the words that repeat so many times in literature and yet are not a differentiator in the explanatory power of sentences. For example: <em>I</em>, <em>me</em>, <em>you</em>, <em>this</em>, <em>that</em>, and so on, which needs to be removed before further processing.</li>
<li><strong>of length at least three</strong>: Here we have removed words with length less than three.</li>
<li><strong>Stemming of words</strong>: Stemming process stems the words to its respective root words. Example of stemming is bringing down running to run or runs to run. By doing stemming we reduce duplicates and improve the accuracy of the model.</li>
<li><strong>Part-of-speech (POS) tagging</strong>:  This applies the speech tags to words, such as noun, verb, adjective, and so on. For example, POS tagging for <em>running</em> is verb, whereas for <em>run</em> is noun. In some situation <em>running</em> is noun and lemmatization will not bring down the word to root word <em>run</em>, instead, it just keeps the <em>running</em> as it is. Hence, POS tagging is a very crucial step necessary for performing prior to applying the lemmatization operation to bring down the word to its root word.</li>
<li><strong>Lemmatization of words</strong>: Lemmatization is another different process to reduce the dimensionality. In lemmatization process, it brings down the word to root word rather than just truncating the words. For example, bring <em>ate</em> to its root word as <em>eat</em> when we pass the <em>ate</em> word into lemmatizer with the POS tag as verb.</li>
</ul>
<p>The <kbd>nltk</kbd> package has been utilized for all the preprocessing steps, as it consists of all the necessary NLP functionality in one single roof:</p>
<pre><strong>&gt;&gt;&gt; import nltk 
&gt;&gt;&gt; from nltk.corpus import stopwords 
&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer 
&gt;&gt;&gt; import string 
&gt;&gt;&gt; import pandas as pd 
&gt;&gt;&gt; from nltk import pos_tag 
&gt;&gt;&gt; from nltk.stem import PorterStemmer</strong>  </pre>
<p>Function has been written (preprocessing) consists of all the steps for convenience. However, we will be explaining all the steps in each section:</p>
<pre><strong>&gt;&gt;&gt; def preprocessing(text):</strong> </pre>
<p>The following line of the code splits the word and checks each character if it is in standard punctuations if so it will be replaced with blank and or else it just does not replace with blanks:</p>
<pre><strong>...     text2 = " ".join("".join([" " if ch in string.punctuation else ch for ch in text]).split())</strong> </pre>
<p>The following code tokenizes the sentences into words based on white spaces and put them together as a list for applying further steps:</p>
<pre><strong>...     tokens = [word for sent in nltk.sent_tokenize(text2) for word in 
              nltk.word_tokenize(sent)]</strong> </pre>
<p>Converting all the cases (upper, lower, and proper) into lowercase reduces duplicates in corpus:</p>
<pre><strong>...     tokens = [word.lower() for word in tokens]</strong> </pre>
<p>As mentioned earlier, stop words are the words that do not carry much weight in understanding the sentence; they are used for connecting words, and so on. We have removed them with the following line of code:</p>
<pre><strong>...     stopwds = stopwords.words('english') 
...     tokens = [token for token in tokens if token not in stopwds] </strong> </pre>
<p>Keeping only the words with length greater than <kbd>3</kbd> in the following code for removing small words, which hardly consists of much of a meaning to carry:</p>
<pre><strong>...     tokens = [word for word in tokens if len(word)&gt;=3]</strong> </pre>
<p>Stemming is applied on the words using <kbd>PorterStemmer</kbd> function, which stems the extra suffixes from the words:</p>
<pre><strong>...     stemmer = PorterStemmer() 
...     tokens = [stemmer.stem(word) for word in tokens] </strong> </pre>
<p>POS tagging is a prerequisite for lemmatization, based on whether the word is noun or verb, and so on, it will reduce it to the root word:</p>
<pre><strong>...     tagged_corpus = pos_tag(tokens)    </strong> </pre>
<p>The <kbd>pos_tag</kbd> function returns the part of speed in four formats for noun and six formats for verb. <kbd>NN</kbd> (noun, common, singular), <kbd>NNP</kbd> (noun, proper, singular), <kbd>NNPS</kbd> (noun, proper, plural), <kbd>NNS</kbd> (noun, common, plural), <kbd>VB</kbd> (verb, base form), <kbd>VBD</kbd> (verb, past tense), <kbd>VBG</kbd> (verb, present participle), <kbd>VBN</kbd> (verb, past participle), <kbd>VBP</kbd> (verb, present tense, not third person singular), <kbd>VBZ</kbd> (verb, present tense, third person singular):</p>
<pre><strong>...    Noun_tags = ['NN','NNP','NNPS','NNS'] 
...    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ'] </strong><br/><strong>...    lemmatizer = WordNetLemmatizer()</strong></pre>
<p>The <kbd>prat_lemmatize</kbd> function has been created only for the reasons of mismatch between the <kbd>pos_tag</kbd> function and intake values of the lemmatize function. If the tag for any word falls under the respective noun or verb tags category, <kbd>n</kbd> or <kbd>v</kbd> will be applied accordingly in the lemmatize function:</p>
<pre><strong>...     def prat_lemmatize(token,tag): 
...         if tag in Noun_tags: 
...             return lemmatizer.lemmatize(token,'n') 
...         elif tag in Verb_tags: 
...             return lemmatizer.lemmatize(token,'v') 
...         else: 
...             return lemmatizer.lemmatize(token,'n')</strong> </pre>
<p>After performing tokenization and applied all the various operations, we need to join it back to form stings and the following function performs the same:</p>
<pre><strong>...     pre_proc_text =  " ".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])              
...     return pre_proc_text</strong> </pre>
<p>The following step applies the preprocessing function to the data and generates new corpus:</p>
<pre><strong>&gt;&gt;&gt; smsdata_data_2 = [] 
&gt;&gt;&gt; for i in smsdata_data: 
...     smsdata_data_2.append(preprocessing(i)) </strong> </pre>
<p>Data will be split into train and test based on 70-30 split and converted to the NumPy array for applying machine learning algorithms:</p>
<pre><strong>&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; trainset_size = int(round(len(smsdata_data_2)*0.70)) 
&gt;&gt;&gt; print ('The training set size for this classifier is ' + str(trainset_size) + '\n') 
&gt;&gt;&gt; x_train = np.array([''.join(rec) for rec in smsdata_data_2[0:trainset_size]]) 
&gt;&gt;&gt; y_train = np.array([rec for rec in smsdata_labels[0:trainset_size]]) 
&gt;&gt;&gt; x_test = np.array([''.join(rec) for rec in smsdata_data_2[trainset_size+1:len( smsdata_data_2)]]) 
&gt;&gt;&gt; y_test = np.array([rec for rec in smsdata_labels[trainset_size+1:len( smsdata_labels)]])</strong> </pre>
<p>The following code converts the words into a vectorizer format and applies <strong>term frequency-inverse document frequency</strong> (<strong>TF-IDF</strong>) weights, which is a way to increase weights to words with high frequency and at the same time penalize the general terms such as <em>the</em>, <em>him</em>, <em>at</em>, and so on. In the following code, we have restricted to most frequent 4,000 words in the vocabulary, none the less we can tune this parameter as well for checking where the better accuracies are obtained:</p>
<pre><strong># building TFIDF vectorizer  
&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer 
&gt;&gt;&gt; vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),  stop_words='english',  
    max_features= 4000,strip_accents='unicode',  norm='l2')</strong> </pre>
<p>The TF-IDF transformation has been shown as follows on both train and test data. The <kbd>todense</kbd> function is used to create the data to visualize the content:</p>
<pre><strong>&gt;&gt;&gt; x_train_2 = vectorizer.fit_transform(x_train).todense() 
&gt;&gt;&gt; x_test_2 = vectorizer.transform(x_test).todense()</strong> </pre>
<p>Multinomial Naive Bayes classifier is suitable for classification with discrete features (example word counts), which normally requires large feature counts. However, in practice, fractional counts such as TF-IDF will also work well. If we do not mention any Laplace estimator, it does take the value of <em>1.0</em> means and it will add <em>1.0</em> against each term in numerator and total for denominator:</p>
<pre><strong>&gt;&gt;&gt; from sklearn.naive_bayes import MultinomialNB 
&gt;&gt;&gt; clf = MultinomialNB().fit(x_train_2, y_train) 
 
&gt;&gt;&gt; ytrain_nb_predicted = clf.predict(x_train_2) 
&gt;&gt;&gt; ytest_nb_predicted = clf.predict(x_test_2) 
 
&gt;&gt;&gt; from sklearn.metrics import classification_report,accuracy_score 
 
&gt;&gt;&gt; print ("\nNaive Bayes - Train Confusion Matrix\n\n",pd.crosstab(y_train, ytrain_nb_predicted,rownames = ["Actuall"],colnames = ["Predicted"]))       
&gt;&gt;&gt; print ("\nNaive Bayes- Train accuracy",round(accuracy_score(y_train, ytrain_nb_predicted),3)) 
&gt;&gt;&gt; print ("\nNaive Bayes  - Train Classification Report\n",classification_report(y_train, ytrain_nb_predicted)) 
 
&gt;&gt;&gt; print ("\nNaive Bayes - Test Confusion Matrix\n\n",pd.crosstab(y_test, ytest_nb_predicted,rownames = ["Actuall"],colnames = ["Predicted"]))       
&gt;&gt;&gt; print ("\nNaive Bayes- Test accuracy",round(accuracy_score(y_test, ytest_nb_predicted),3)) 
&gt;&gt;&gt; print ("\nNaive Bayes  - Test Classification Report\n",classification_report( y_test, ytest_nb_predicted))</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/34b61efc-2814-4234-8d08-21c2e68c1264.png" style="width:27.25em;height:36.75em;"/></div>
<p>From the previous results, it is appearing that Naive Bayes has produced excellent results of 96.6 percent test accuracy with significant recall value of 76 percent for spam and almost 100 percent for ham.</p>
<p>However, if we would like to check what are the top 10 features based on their coefficients from Naive Bayes, the following code will be handy for this:</p>
<pre><strong># printing top features  
&gt;&gt;&gt; feature_names = vectorizer.get_feature_names() 
&gt;&gt;&gt; coefs = clf.coef_ 
&gt;&gt;&gt; intercept = clf.intercept_ 
&gt;&gt;&gt; coefs_with_fns = sorted(zip(clf.coef_[0], feature_names)) 
 
&gt;&gt;&gt; print ("\n\nTop 10 features - both first &amp; last\n") 
&gt;&gt;&gt; n=10 
&gt;&gt;&gt; top_n_coefs = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1]) 
&gt;&gt;&gt; for (coef_1, fn_1), (coef_2, fn_2) in top_n_coefs: 
...     print('\t%.4f\t%-15s\t\t%.4f\t%-15s' % (coef_1, fn_1, coef_2, fn_2))</strong> </pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/b31d330c-79e0-405f-a60d-08a3940ea1b8.png"/></div>
<p>Though the R language is not a popular choice for NLP processing, here we have presented the code. Readers are encouraged to change the code and see how accuracies are changing for a better understanding of concepts. The R code for Naive Bayes classifier on SMS spam/ham data is as follows:</p>
<pre><strong># Naive Bayes </strong><br/><strong>smsdata = read.csv("SMSSpamCollection.csv",stringsAsFactors = FALSE) </strong><br/><strong># Try the following code for reading in case if you have </strong><br/><strong>#issues while reading regularly with above code </strong><br/><strong>#smsdata = read.csv("SMSSpamCollection.csv", </strong><br/><strong>#stringsAsFactors = FALSE,fileEncoding="latin1") </strong><br/><strong>str(smsdata) </strong><br/><strong>smsdata$Type = as.factor(smsdata$Type) </strong><br/><strong>table(smsdata$Type) </strong><br/><br/><strong>library(tm) </strong><br/><strong>library(SnowballC) </strong><br/><strong># NLP Processing </strong><br/><strong>sms_corpus &lt;- Corpus(VectorSource(smsdata$SMS_Details)) </strong><br/><strong>corpus_clean_v1 &lt;- tm_map(sms_corpus, removePunctuation)</strong><br/><strong>corpus_clean_v2 &lt;- tm_map(corpus_clean_v1, tolower) </strong><br/><strong>corpus_clean_v3 &lt;- tm_map(corpus_clean_v2, stripWhitespace)</strong><br/><strong>corpus_clean_v4 &lt;- tm_map(corpus_clean_v3, removeWords, stopwords())</strong><br/><strong>corpus_clean_v5 &lt;- tm_map(corpus_clean_v4, removeNumbers)</strong><br/><strong>corpus_clean_v6 &lt;- tm_map(corpus_clean_v5, stemDocument) </strong><br/><br/><strong># Check the change in corpus </strong><br/><strong>inspect(sms_corpus[1:3]) </strong><br/><strong>inspect(corpus_clean_v6[1:3]) </strong><br/><br/><strong>sms_dtm &lt;- DocumentTermMatrix(corpus_clean_v6) </strong><br/><br/><strong>smsdata_train &lt;- smsdata[1:4169, ] </strong><br/><strong>smsdata_test &lt;- smsdata[4170:5572, ] </strong><br/><br/><strong>sms_dtm_train &lt;- sms_dtm[1:4169, ] </strong><br/><strong>sms_dtm_test &lt;- sms_dtm[4170:5572, ] </strong><br/><br/><strong>sms_corpus_train &lt;- corpus_clean_v6[1:4169] </strong><br/><strong>sms_corpus_test &lt;- corpus_clean_v6[4170:5572]</strong><br/><br/><strong>prop.table(table(smsdata_train$Type))</strong><br/><strong>prop.table(table(smsdata_test$Type)) </strong><br/><strong>frac_trzero = (table(smsdata_train$Type)[[1]])/nrow(smsdata_train)</strong><br/><strong>frac_trone = (table(smsdata_train$Type)[[2]])/nrow(smsdata_train)</strong><br/><strong>frac_tszero = (table(smsdata_test$Type)[[1]])/nrow(smsdata_test)</strong><br/><strong>frac_tsone = (table(smsdata_test$Type)[[2]])/nrow(smsdata_test)</strong><br/><br/><strong>Dictionary &lt;- function(x) { </strong><br/><strong>  if( is.character(x) ) { </strong><br/><strong>    return (x) </strong><br/><strong>  } </strong><br/><strong>  stop('x is not a character vector') </strong><br/><strong>} </strong><br/><strong># Create the dictionary with at least word appears 1 time </strong><br/><strong>sms_dict &lt;- Dictionary(findFreqTerms(sms_dtm_train, 1)) </strong><br/><strong>sms_train &lt;- DocumentTermMatrix(sms_corpus_train,list(dictionary = sms_dict)) </strong><br/><strong>sms_test &lt;- DocumentTermMatrix(sms_corpus_test,list(dictionary = sms_dict)) </strong><br/><strong>convert_tofactrs &lt;- function(x) { </strong><br/><strong>  x &lt;- ifelse(x &gt; 0, 1, 0) </strong><br/><strong>  x &lt;- factor(x, levels = c(0, 1), labels = c("No", "Yes")) </strong><br/><strong>  return(x) </strong><br/><strong>} </strong><br/><strong>sms_train &lt;- apply(sms_train, MARGIN = 2, convert_tofactrs) </strong><br/><strong>sms_test &lt;- apply(sms_test, MARGIN = 2, convert_tofactrs) </strong><br/><br/><strong># Application of Naïve Bayes Classifier with laplace Estimator</strong><br/><strong>library(e1071) </strong><br/><strong>nb_fit &lt;- naiveBayes(sms_train, smsdata_train$Type,laplace = 1.0)</strong><br/><br/><strong>tr_y_pred = predict(nb_fit, sms_train) </strong><br/><strong>ts_y_pred = predict(nb_fit,sms_test) </strong><br/><strong>tr_y_act = smsdata_train$Type;ts_y_act = smsdata_test$Type </strong><br/><br/><strong>tr_tble = table(tr_y_act,tr_y_pred) </strong><br/><strong>print(paste("Train Confusion Matrix")) </strong><br/><strong>print(tr_tble) </strong><br/><br/><strong>tr_acc = accrcy(tr_y_act,tr_y_pred) </strong><br/><strong>trprec_zero = prec_zero(tr_y_act,tr_y_pred);  trrecl_zero = recl_zero(tr_y_act,tr_y_pred) </strong><br/><strong>trprec_one = prec_one(tr_y_act,tr_y_pred); trrecl_one = recl_one(tr_y_act,tr_y_pred) </strong><br/><strong>trprec_ovll = trprec_zero *frac_trzero + trprec_one*frac_trone</strong><br/><strong>trrecl_ovll = trrecl_zero *frac_trzero + trrecl_one*frac_trone</strong><br/><br/><strong>print(paste("Naive Bayes Train accuracy:",tr_acc)) </strong><br/><strong>print(paste("Naive Bayes - Train Classification Report"))</strong><br/><strong>print(paste("Zero_Precision",trprec_zero,"Zero_Recall",trrecl_zero))</strong><br/><strong>print(paste("One_Precision",trprec_one,"One_Recall",trrecl_one))</strong><br/><strong>print(paste("Overall_Precision",round(trprec_ovll,4),"Overall_Recall",round(trrecl_ovll,4))) </strong><br/><br/><strong>ts_tble = table(ts_y_act,ts_y_pred) </strong><br/><strong>print(paste("Test Confusion Matrix")) </strong><br/><strong>print(ts_tble) </strong><br/><br/><strong>ts_acc = accrcy(ts_y_act,ts_y_pred) </strong><br/><strong>tsprec_zero = prec_zero(ts_y_act,ts_y_pred); tsrecl_zero = recl_zero(ts_y_act,ts_y_pred) </strong><br/><strong>tsprec_one = prec_one(ts_y_act,ts_y_pred); tsrecl_one = recl_one(ts_y_act,ts_y_pred) </strong><br/><strong>tsprec_ovll = tsprec_zero *frac_tszero + tsprec_one*frac_tsone</strong><br/><strong>tsrecl_ovll = tsrecl_zero *frac_tszero + tsrecl_one*frac_tsone</strong><br/><br/><strong>print(paste("Naive Bayes Test accuracy:",ts_acc)) </strong><br/><strong>print(paste("Naive Bayes - Test Classification Report"))</strong><br/><strong>print(paste("Zero_Precision",tsprec_zero,"Zero_Recall",tsrecl_zero))</strong><br/><strong>print(paste("One_Precision",tsprec_one,"One_Recall",tsrecl_one))</strong><br/><strong>print(paste("Overall_Precision",round(tsprec_ovll,4),"Overall_Recall",round(tsrecl_ovll,4)))</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you have learned about KNN and Naive Bayes techniques, which require somewhat a little less computational power. KNN, in fact, is called a lazy learner, as it does not learn anything apart from comparing with training data points to classify them into class. Also, you have seen how to tune the k-value using grid search technique. Whereas explanation has been provided for Naive Bayes classifier, NLP examples have been provided with all the famous NLP processing techniques to give you a flavor of this field in a very crisp manner. Though in text processing, either Naive Bayes or SVM techniques could be used as these two techniques can handle data with high dimensionality, which is very relevant in NLP, as the number of word vectors is relatively high in dimensions and sparse at the same time.</p>
<p><span>In the next chapter, we will be covering the details of unsupervised learning, more precisely, clustering and principal component analysis models.</span></p>


            </article>

            
        </section>
    </body></html>