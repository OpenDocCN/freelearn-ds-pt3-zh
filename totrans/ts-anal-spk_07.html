<html><head></head><body>
<div epub:type="chapter" id="_idContainer125">
<h1 class="chapter-number" id="_idParaDest-133"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-134"><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.2.1">Building and Testing Models</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Having covered the data preparation and exploratory data analysis stages of time series analysis, we will now direct our focus to constructing predictive models for time series data. </span><span class="koboSpan" id="kobo.3.2">We will cover the diverse types of models and how to decide which one to choose. </span><span class="koboSpan" id="kobo.3.3">We will also learn how to train, tune, and </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">evaluate models.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">The concepts covered in this chapter will act as a practical guide to model development, providing essential building blocks for effective time series models and facilitating accurate predictions and insightful analyses. </span><span class="koboSpan" id="kobo.5.2">We will factor in common execution constraints faced in real-life projects and conclude with a comparison of the outcome of the different models to solve a </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">forecasting problem.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">We are going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">main topics:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.9.1">Model selection</span></span></li>
<li><span class="koboSpan" id="kobo.10.1">Development </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">and testing</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.12.1">Model comparison</span></span></li>
</ul>
<h1 id="_idParaDest-135"><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.13.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.14.1">The code for this chapter, which will be covered in the </span><em class="italic"><span class="koboSpan" id="kobo.15.1">Development and testing</span></em><span class="koboSpan" id="kobo.16.1"> section, can be found in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.17.1">ch7</span></strong><span class="koboSpan" id="kobo.18.1"> folder of the book’s GitHub repository at </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">this URL:</span></span></p>
<p><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch7"><span class="No-Break"><span class="koboSpan" id="kobo.20.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch7</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.21.1">.</span></span></p>
<h1 id="_idParaDest-136"><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.22.1">Model selection</span></h1>
<p><span class="koboSpan" id="kobo.23.1">The first step before developing a time series analysis model is to select which model to use. </span><span class="koboSpan" id="kobo.23.2">As discussed in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.24.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.25.1">, one of the key challenges of time series analysis is using the right model. </span><span class="koboSpan" id="kobo.25.2">This choice impacts, among other things, the accuracy, reliability, efficiency, and scalability of the analysis. </span><span class="koboSpan" id="kobo.25.3">This, in turn, ensures that the analysis leads to better-informed decisions and more effective outcomes while being scientifically robust and </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">practically useful.</span></span></p>
<p><span class="koboSpan" id="kobo.27.1">There are different types of models, each with its </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">own characteristics.</span></span></p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.29.1">Types of models</span></h2>
<p><span class="koboSpan" id="kobo.30.1">Time series analysis models can be categorized into statistical, classical Machine Learning (ML), and Deep Learning (</span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">DL) models:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.32.1">Statistical models</span></strong><span class="koboSpan" id="kobo.33.1"> for</span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.34.1"> time series analysis are based on statistical </span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.35.1">theories with </span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.36.1">assumptions about the characteristics of the time series, such as linearity and stationarity. </span><span class="koboSpan" id="kobo.36.2">Examples</span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.37.1"> of classical </span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.38.1">models include </span><strong class="bold"><span class="koboSpan" id="kobo.39.1">Autoregressive Moving Average</span></strong><span class="koboSpan" id="kobo.40.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.41.1">ARIMA</span></strong><span class="koboSpan" id="kobo.42.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.43.1">Seasonal Autoregressive Integrated Moving Average Exogenous</span></strong><span class="koboSpan" id="kobo.44.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.45.1">SARIMAX</span></strong><span class="koboSpan" id="kobo.46.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.47.1">Exponential Smoothing</span></strong><span class="koboSpan" id="kobo.48.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.49.1">ETS</span></strong><span class="koboSpan" id="kobo.50.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.51.1">Generalized Autoregressive Conditional Heteroskedasticity</span></strong><span class="koboSpan" id="kobo.52.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.53.1">GARCH</span></strong><span class="koboSpan" id="kobo.54.1">), and </span><a id="_idIndexMarker599"/><span class="No-Break"><span class="koboSpan" id="kobo.55.1">state-space models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.56.1">Classical Machine Learning models</span></strong><span class="koboSpan" id="kobo.57.1"> for </span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.58.1">time series analysis use algorithms that can learn from data without</span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.59.1"> explicit programming. </span><span class="koboSpan" id="kobo.59.2">These models can handle non-linear relationships. </span><span class="koboSpan" id="kobo.59.3">However, they often require more data for training compared to classical models. </span><span class="koboSpan" id="kobo.59.4">Examples of </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.60.1">Machine Learning models include linear </span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.61.1">regression, </span><strong class="bold"><span class="koboSpan" id="kobo.62.1">Support Vector Machines</span></strong><span class="koboSpan" id="kobo.63.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.64.1">SVMs</span></strong><span class="koboSpan" id="kobo.65.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">k-Nearest Neighbors</span></strong><span class="koboSpan" id="kobo.67.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.68.1">kNN</span></strong><span class="koboSpan" id="kobo.69.1">), random forests, and gradient </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">boosting machines.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.71.1">Deep Learning models</span></strong><span class="koboSpan" id="kobo.72.1"> use</span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.73.1"> neural networks </span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.74.1">with multiple layers to learn complex patterns in time series data. </span><span class="koboSpan" id="kobo.74.2">These models can handle non-linear relationships and long-term </span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.75.1">dependencies. </span><span class="koboSpan" id="kobo.75.2">They, however, require large datasets for training and significant computational resources. </span><span class="koboSpan" id="kobo.75.3">Examples of Deep Learning</span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.76.1"> models</span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.77.1"> include </span><strong class="bold"><span class="koboSpan" id="kobo.78.1">Long Short-Term Memory (LSTM) Networks</span></strong><span class="koboSpan" id="kobo.79.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.80.1">Convolutional Neural Networks</span></strong><span class="koboSpan" id="kobo.81.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.82.1">CNNs</span></strong><span class="koboSpan" id="kobo.83.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">Temporal Convolutional Networks</span></strong><span class="koboSpan" id="kobo.85.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.86.1">TCNs</span></strong><span class="koboSpan" id="kobo.87.1">), transformers, </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">and autoencoders.</span></span></li>
</ul>
<p class="callout-heading"><span class="koboSpan" id="kobo.89.1">Machine Learning and Deep Learning</span></p>
<p class="callout"><span class="koboSpan" id="kobo.90.1">Deep Learning</span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.91.1"> is a subset of Machine Learning</span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.92.1"> that uses deep neural networks. </span><span class="koboSpan" id="kobo.92.2">As is common practice, we are using the term classical Machine Learning here to refer to approaches and models that are not neural-network-based. </span><span class="koboSpan" id="kobo.92.3">The term Deep Learning is used for approaches and models using </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">neural networks.</span></span></p>
<p><span class="koboSpan" id="kobo.94.1">Each of the preceding categories and models has distinct characteristics and approaches that determine their applicability, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">explore next.</span></span></p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.96.1">Selection criteria</span></h2>
<p><span class="koboSpan" id="kobo.97.1">When to use which </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.98.1">model is based on several criteria. </span><span class="koboSpan" id="kobo.98.2">We touched on this briefly in the section on using the right model in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.99.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.100.1"> and when initially </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.101.1">discussing model selection in </span><a href="B18568_04.xhtml#_idTextAnchor087"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.102.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.103.1">. </span><span class="koboSpan" id="kobo.103.2">The applicability of a model to solve a time series analysis problem is dependent on factors such as the objectives of the analysis, the characteristics of the data, and the computation power and </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">time available.</span></span></p>
<p><span class="koboSpan" id="kobo.105.1">We will now dive deep into the details of these and other important factors for </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">model selection.</span></span></p>
<h3><span class="koboSpan" id="kobo.107.1">Types of use cases</span></h3>
<p><span class="koboSpan" id="kobo.108.1">Time series analysis broadly </span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.109.1">falls into use cases for forecasting, classification, and anomaly detection, as discussed in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.110.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.111.1">. </span><span class="koboSpan" id="kobo.111.2">We will briefly recap these types of use cases here, highlighting the frequently used models. </span><span class="koboSpan" id="kobo.111.3">We will go into further detail in the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">the chapter.</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.113.1">Forecasting</span></strong><span class="koboSpan" id="kobo.114.1">’s goal </span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.115.1">is to predict future values based</span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.116.1"> on patterns learned by the model from past values. </span><span class="koboSpan" id="kobo.116.2">As presented in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.117.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.118.1">, forecasting can be single or multi-steps, based </span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.119.1">on a single (</span><strong class="bold"><span class="koboSpan" id="kobo.120.1">univariate</span></strong><span class="koboSpan" id="kobo.121.1">) or multiple (</span><strong class="bold"><span class="koboSpan" id="kobo.122.1">multivariate</span></strong><span class="koboSpan" id="kobo.123.1">) time series. </span><span class="koboSpan" id="kobo.123.2">Commonly </span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.124.1">used models such as ARIMA, SARIMA, and </span><strong class="bold"><span class="koboSpan" id="kobo.125.1">Exponential Smoothing</span></strong><span class="koboSpan" id="kobo.126.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.127.1">ETS</span></strong><span class="koboSpan" id="kobo.128.1">) are chosen for their simplicity while giving </span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.129.1">strong performance in forecasting tasks. </span><span class="koboSpan" id="kobo.129.2">LSTM and Prophet, introduced in previous examples in the book, are preferred for more complex forecasting requirements where they can be </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">more effective.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.131.1">Pattern recognition</span></strong><span class="koboSpan" id="kobo.132.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.133.1">classification</span></strong><span class="koboSpan" id="kobo.134.1"> are used to</span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.135.1"> identify and </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.136.1">understand patterns and </span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.137.1">classify time series accordingly. </span><span class="koboSpan" id="kobo.137.2">Commonly used models are based on </span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.138.1">decomposition methods, such as </span><strong class="bold"><span class="koboSpan" id="kobo.139.1">Seasonal-Trend decomposition using LOESS</span></strong><span class="koboSpan" id="kobo.140.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.141.1">STL</span></strong><span class="koboSpan" id="kobo.142.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.143.1">Multiple STL</span></strong><span class="koboSpan" id="kobo.144.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.145.1">MSTL</span></strong><span class="koboSpan" id="kobo.146.1">), and </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.147.1">Fourier analysis. </span><span class="koboSpan" id="kobo.147.2">We spent some time on decomposition in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.148.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.149.1"> and </span><a href="B18568_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.150.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.151.1">. </span><span class="koboSpan" id="kobo.151.2">We briefly discussed Fourier analysis in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.152.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.153.1">, in addition to distance-based approaches, shapelets analysis, ensemble methods and </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">Deep Learning.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.155.1">Anomaly detection</span></strong><span class="koboSpan" id="kobo.156.1"> aims to</span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.157.1"> identify outliers or</span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.158.1"> anomalies in the time series. </span><span class="koboSpan" id="kobo.158.2">As presented in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.159.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.160.1">, this detection can be based on univariate or multivariate series and point, collective, or contextual analysis. </span><span class="koboSpan" id="kobo.160.2">What is initially flagged as an anomaly can turn out to be a novelty, in the sense of a new non-problematic pattern. </span><span class="koboSpan" id="kobo.160.3">Commonly used models are based on their capabilities for residual analysis, such as ARIMA. </span><span class="koboSpan" id="kobo.160.4">Machine learning models are frequently used as well, such as Isolation Forest or, when there is a high percentage of anomalies, specialized methods such</span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.161.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.162.1">Seasonal Hybrid Extreme Studentized Deviate</span></strong><span class="koboSpan" id="kobo.163.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.164.1">SH-ESD</span></strong><span class="koboSpan" id="kobo.165.1">). </span><span class="koboSpan" id="kobo.165.2">We saw a code example in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.166.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.167.1"> of Isolation Forest for anomaly</span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.168.1"> detection, in</span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.169.1"> addition to discussing supervised, unsupervised, semi-supervised, and </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">hybrid </span></span><span class="No-Break"><a id="_idIndexMarker629"/></span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">approaches.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.172.1">Another model selection criterion, which we will look at next, is the statistical nature of the </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">time series.</span></span></p>
<h3><span class="koboSpan" id="kobo.174.1">Nature of time series</span></h3>
<p><span class="koboSpan" id="kobo.175.1">The nature of the time series, that is, its </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.176.1">statistical properties, influences the choice of model. </span><span class="koboSpan" id="kobo.176.2">Models are researched and developed to work well, if at all, based on specific assumptions about the nature of the time series, which then determines their applicability. </span><span class="koboSpan" id="kobo.176.3">We will focus in this section on applicability and skip definitions, assuming that, by now, you are familiar with the terms we will use in this section, based on the introduction in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.177.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.178.1"> and code examples in </span><a href="B18568_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.179.1">Chapter 6</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.180.1">:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.181.1">Stationary</span></strong><span class="koboSpan" id="kobo.182.1"> time series</span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.183.1"> can be modeled with ARIMA, which assumes</span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.184.1"> stationarity. </span><span class="koboSpan" id="kobo.184.2">An example of a stationary time series is the daily percentage returns of a stock over a 3-year period. </span><span class="koboSpan" id="kobo.184.3">Assuming no significant structural changes in the market, stock returns tend to fluctuate around a stable mean with </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">consistent variance.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.186.1">Non-stationary time series can be converted to stationary, for example, by differencing, as seen in </span><a href="B18568_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.187.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.188.1">. </span><span class="koboSpan" id="kobo.188.2">The differenced series can then be used with such models. </span><span class="koboSpan" id="kobo.188.3">Alternatively, use Prophet, or Machine Learning models for non-stationary series. </span><span class="koboSpan" id="kobo.188.4">An example of a non-stationary time series is the monthly unemployment rate, which possibly has a trend, and cyclical patterns related to </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">economic conditions.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.190.1">Seasonal</span></strong><span class="koboSpan" id="kobo.191.1"> time series</span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.192.1"> require models that handle seasonality, such as </span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.193.1">SARIMA, ETS, Prophet, or Machine Learning models. </span><span class="koboSpan" id="kobo.193.2">We have seen this in action with the coding example in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.194.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.195.1"> to forecast temperature </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">using Prophet.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.197.1">Trends</span></strong><span class="koboSpan" id="kobo.198.1"> in </span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.199.1">time series can impact the performance of certain models, such as ARIMA. </span><span class="koboSpan" id="kobo.199.2">In this </span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.200.1">case, similarly to stationarity, we can remove the trend component by differencing, as per the code example in </span><a href="B18568_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.201.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.202.1">. </span><span class="koboSpan" id="kobo.202.2">ARIMA can then be used. </span><span class="koboSpan" id="kobo.202.3">Alternatively, use models that can handle trends, such as trend models, ETS, Prophet, or </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">Machine Learning.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.204.1">Volatility</span></strong><span class="koboSpan" id="kobo.205.1"> in time </span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.206.1">series can be </span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.207.1">handled with models such as </span><strong class="bold"><span class="koboSpan" id="kobo.208.1">Generalized Autoregressive Conditional Heteroskedasticity</span></strong><span class="koboSpan" id="kobo.209.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.210.1">GARCH</span></strong><span class="koboSpan" id="kobo.211.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.212.1">Stochastic Volatility GARCH</span></strong><span class="koboSpan" id="kobo.213.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.214.1">SV-GARCH</span></strong><span class="koboSpan" id="kobo.215.1">), or Machine Learning. </span><span class="koboSpan" id="kobo.215.2">Common </span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.216.1">use cases for these models are forecasting and risk management in highly volatile financial </span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.217.1">markets and </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">other domains.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.219.1">Linearity</span></strong><span class="koboSpan" id="kobo.220.1"> of the </span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.221.1">relationship in the</span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.222.1"> data means that linear models such as ARIMA are suitable. </span><span class="koboSpan" id="kobo.222.2">An example of a linear time series is daily temperature, where today’s temperature can be predicted by a linear combination of the temperatures from the previous two days plus some </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">random error.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.224.1">In the case of non-linear patterns, Machine Learning models with neural networks are preferable. </span><span class="koboSpan" id="kobo.224.2">An example of a non-linear time series is if a stock price follows one relationship if below a certain threshold (say 100) and follows a different relationship if it’s above </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">that threshold.</span></span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.226.1">The volume and frequency of data to analyze, discussed next, is another property of time series that influences </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">model selection.</span></span></p>
<h3><span class="koboSpan" id="kobo.228.1">Volume and frequency of data</span></h3>
<p><span class="koboSpan" id="kobo.229.1">The volume and frequency of data</span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.230.1"> impact the computational power required and the duration of the analysis. </span><span class="koboSpan" id="kobo.230.2">The combination of these factors determines the choice of model to use. </span><span class="koboSpan" id="kobo.230.3">We will discuss volume and frequency here, and the other two factors in the </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">following section:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.232.1">Small datasets</span></strong><span class="koboSpan" id="kobo.233.1"> can be analyzed with statistical models such as ARIMA and ETS. </span><span class="koboSpan" id="kobo.233.2">These are simple models that work well with smaller datasets. </span><span class="koboSpan" id="kobo.233.3">An example of a small dataset is the daily sales for a store over the past </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">few years.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.235.1">Large datasets</span></strong><span class="koboSpan" id="kobo.236.1"> are a good match for Machine Learning models such as gradient boosting and LSTM. </span><span class="koboSpan" id="kobo.236.2">This works in both ways: in terms of processing capability and scalability of ML models for large datasets, and the substantial amount of data needed for model training to avoid overfitting. </span><span class="koboSpan" id="kobo.236.3">ML models can learn complex patterns present in large datasets at the cost of more computational resources. </span><span class="koboSpan" id="kobo.236.4">Examples of large datasets are minute-by-minute stock prices or sensor data over, say, the past </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">5 years.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.238.1">As we will see in </span><a href="B18568_08.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.239.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.240.1">, we can scale models to large datasets by using the distributed computing capabilities of </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">Apache Spark:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.242.1">Low-frequency</span></strong><span class="koboSpan" id="kobo.243.1"> time series, such as</span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.244.1"> daily, weekly, monthly, quarterly, or annual, are usually small in size. </span><span class="koboSpan" id="kobo.244.2">As discussed before about small datasets, ARIMA and ETS are usually good choices for </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">such datasets.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.246.1">High-frequency</span></strong><span class="koboSpan" id="kobo.247.1"> time series</span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.248.1"> are likely to have rapid changes, noise, volatility, and heteroskedasticity, which can be handled with models such as GARCH, often used for financial </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">time series.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.250.1">If the analysis is required at a lower frequency than the data arrival rate, the high-frequency series can be converted to low frequency by resampling and aggregation, as discussed in </span><a href="B18568_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.251.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.252.1">. </span><span class="koboSpan" id="kobo.252.2">Resampling decreases the size of the dataset while smoothing out the noise and volatility. </span><span class="koboSpan" id="kobo.252.3">This opens the possibility of using models suited for low-frequency time series, as </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">discussed earlier.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.254.1">Diminishing value of high-frequency data</span></p>
<p class="callout"><span class="koboSpan" id="kobo.255.1">We discussed frequency here as pertaining to the time interval between consecutive data points in the time series, also referred to as </span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.256.1">granularity. </span><span class="koboSpan" id="kobo.256.2">Another consideration for high-frequency data is the requirement that the analysis also be done at high frequency. </span><span class="koboSpan" id="kobo.256.3">This is due to the quickly diminishing value of high-frequency data over time. </span><span class="koboSpan" id="kobo.256.4">Consider how real-time stock tick changes are critical at the moment they occur but become less relevant just a few hours later. </span><span class="koboSpan" id="kobo.256.5">In this scenario, the model must be capable of performing extremely rapid computations, potentially in </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">real time.</span></span></p>
<p><span class="koboSpan" id="kobo.258.1">Higher volume and frequency of data require more computational resources, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">cover next.</span></span></p>
<h3><span class="koboSpan" id="kobo.260.1">Computational constraints</span></h3>
<p><span class="koboSpan" id="kobo.261.1">Like any other project, time series</span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.262.1"> analysis occurs within a budget. </span><span class="koboSpan" id="kobo.262.2">This means that the amount of resources available, including the computing power to execute the analysis process, is constrained. </span><span class="koboSpan" id="kobo.262.3">At the same time, we know that higher volume and frequency of data require more computational resources. </span><span class="koboSpan" id="kobo.262.4">We also must factor in how fast the analysis needs to be completed for the outcome to be useful. </span><span class="koboSpan" id="kobo.262.5">With these constraints in mind, let’s investigate the choice </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">of model:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.264.1">Limited computation</span></strong><span class="koboSpan" id="kobo.265.1"> resources mean that we may have to consider a combination of dataset size reduction, with resampling, and simpler models such as ARIMA or ETS. </span><span class="koboSpan" id="kobo.265.2">Machine learning models, while better at detecting complex patterns and with larger datasets, usually require more </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">computation resources.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.267.1">Fast analysis</span></strong><span class="koboSpan" id="kobo.268.1"> requires using faster models for training and prediction. </span><span class="koboSpan" id="kobo.268.2">Models such as ARIMA or ETS are, again, good candidates for </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">smaller datasets.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.270.1">If fast analysis is required for a large dataset, options include </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">the following:</span></span></p><ul><li><span class="koboSpan" id="kobo.272.1">Scaling out using the distributed processing of Apache Spark clusters on large datasets, which we will cover in the </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">next chapter.</span></span></li><li><span class="koboSpan" id="kobo.274.1">Resampling to convert to a smaller dataset size, with the use of simpler models such as ARIMA </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">or ETS.</span></span></li><li><span class="koboSpan" id="kobo.276.1">Using Machine Learning models with the following caveats. </span><span class="koboSpan" id="kobo.276.2">The training and tuning stage will be slower for larger datasets. </span><span class="koboSpan" id="kobo.276.3">The prediction speed can be improved by using more computation resources, which of course comes at a higher cost. </span><span class="koboSpan" id="kobo.276.4">Note that training, tuning, and prediction speed can also be improved by using the distributed processing of Apache Spark, as we will see in the </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">next chapter.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.278.1">Cost of compute resources</span></strong> <a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.279.1">is another important factor that may limit the use of compute-intensive models. </span><span class="koboSpan" id="kobo.279.2">While the simpler statistical models can run on cheaper standard resources, Deep Learning models may require more expensive GPUs on </span><a id="_idIndexMarker648"/><span class="No-Break"><span class="koboSpan" id="kobo.280.1">high-performance hardware.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.281.1">After considering how computational requirements influence the choice of models, we will now consider how model accuracy, complexity, and interpretability determine which model </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">to use.</span></span></p>
<h3><span class="koboSpan" id="kobo.283.1">Model accuracy, complexity, and interpretability</span></h3>
<p><span class="koboSpan" id="kobo.284.1">Some of the other factors that are considered for model selection are model accuracy, complexity, </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">and interpretability:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.286.1">Model accuracy</span></strong><span class="koboSpan" id="kobo.287.1"> is </span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.288.1">wrongly seen as the determining factor for model selection in many cases. </span><span class="koboSpan" id="kobo.288.2">Accuracy has been presented at the end of the list of selection criteria on purpose to highlight the importance of considering other factors as well. </span><span class="koboSpan" id="kobo.288.3">The best model is not always the most accurate one. </span><span class="koboSpan" id="kobo.288.4">It is the one that delivers the most ROI for the </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">use case.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.290.1">When high accuracy is needed, especially in forecasting, more complex models such as SARIMAX or Deep Learning may be necessary. </span><span class="koboSpan" id="kobo.290.2">Hyperparameter tuning is used as part of the development process to further improve accuracy, but this comes at the cost of </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">additional computations.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.292.1">Complexity</span></strong><span class="koboSpan" id="kobo.293.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.294.1">interpretability</span></strong><span class="koboSpan" id="kobo.295.1"> usually conflict. </span><span class="koboSpan" id="kobo.295.2">The need for higher accuracy leads to the </span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.296.1">use of more complex models, which are then harder to interpret and often referred to as </span><a id="_idIndexMarker651"/><span class="No-Break"><span class="koboSpan" id="kobo.297.1">black boxes.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.298.1">If interpretability is crucial, prefer simpler models such as ARIMA or ETS, which have the added benefit of lower compute requirements. </span><span class="koboSpan" id="kobo.298.2">Tree-based models such as GBM </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.299.1">or </span><strong class="bold"><span class="koboSpan" id="kobo.300.1">Tree-Based Pipelines for Time Series</span></strong><span class="koboSpan" id="kobo.301.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.302.1">TSPi</span></strong><span class="koboSpan" id="kobo.303.1">) offer a good balance of accuracy and compute requirement, while simpler tree-based models </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">offer interpretability.</span></span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.305.1">If the data exhibits complex patterns and high accuracy is crucial, there may not be many options, and we may have to use complex models, with a trade-off on compute </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">and interpretability.</span></span></p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.307.1">Overview of model selection</span></h2>
<p><span class="koboSpan" id="kobo.308.1">To conclude on model selection, there are a </span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.309.1">few points </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">worth noting:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.311.1">Statistical models such as ARIMA are based on assumptions about the nature of the time series, requiring statistical tests and possibly additional pre-processing to convert the series before using </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">the model.</span></span></li>
<li><span class="koboSpan" id="kobo.313.1">Prophet and Machine Learning models are more broadly applicable but have additional complexity and </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">compute requirements.</span></span></li>
<li><span class="koboSpan" id="kobo.315.1">The models mentioned in this section are provided as examples applicable to the criteria discussed. </span><span class="koboSpan" id="kobo.315.2">Other models, from a growing list of publicly available models and approaches, can and should be tested. </span><span class="koboSpan" id="kobo.315.3">Finding the best model is an experimentation and iterative process, dependent on </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">one’s context.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.317.1">As we have seen in this section on selection criteria, several factors influence the choice of models and determine which ones to invest more effort in. </span><span class="koboSpan" id="kobo.317.2">Which factors are most important depends on the project context and the use case. </span><span class="koboSpan" id="kobo.317.3">The best model to choose is the one resulting in the highest ROI, requiring a trade-off between the different factors </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">discussed here.</span></span></p>
<p><span class="koboSpan" id="kobo.319.1">At this point, with the models selected, we are ready to move on to the next development step, which is to train the model on our time </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">series data.</span></span></p>
<h1 id="_idParaDest-140"><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.321.1">Development and testing</span></h1>
<p><span class="koboSpan" id="kobo.322.1">In this section, we will compare forecasting performance across different categories of models: statistical, classical Machine Learning, and Deep Learning. </span><span class="koboSpan" id="kobo.322.2">We will use six different models: SARIMA, LightGBM, LSTM, NBEATS, NHITS, and NeuralProphet. </span><span class="koboSpan" id="kobo.322.3">These models are chosen for their wide and proven adoption and ease of access </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">and use.</span></span></p>
<p><span class="koboSpan" id="kobo.324.1">We will proceed with the </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">following constraints:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.326.1">Use of the default model hyperparameters whenever possible for comparison and minimize tuning to a few cases, which will </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">be explained</span></span></li>
<li><span class="koboSpan" id="kobo.328.1">The complete execution, from data loading to model training, testing, and forecasting, will be limited to under </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">15 minutes</span></span></li>
<li><span class="koboSpan" id="kobo.330.1">The computing resource used will also be constrained to the Databricks Community Edition compute as per </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.331.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.332.1">.1</span></em><span class="koboSpan" id="kobo.333.1">, with 15.3 GB of memory and 2 </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">CPU cores</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer107">
<span class="koboSpan" id="kobo.335.1"><img alt="" src="image/B18568_07_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.336.1">Figure 7.1: Databricks Community Edition compute resource</span></p>
<p><span class="koboSpan" id="kobo.337.1">We all commonly face time and resource constraints in our real-life projects. </span><span class="koboSpan" id="kobo.337.2">This section also aims to give you the tools to work within </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">these limits.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.339.1">Single-threaded, multi-threaded, and clustering</span></p>
<p class="callout"><span class="koboSpan" id="kobo.340.1">We will use </span><strong class="source-inline"><span class="koboSpan" id="kobo.341.1">Pandas</span></strong><span class="koboSpan" id="kobo.342.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.343.1">NumPy</span></strong><span class="koboSpan" id="kobo.344.1"> in the </span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.345.1">code examples in this chapter. </span><strong class="source-inline"><span class="koboSpan" id="kobo.346.1">Pandas</span></strong><span class="koboSpan" id="kobo.347.1"> is single-threaded in terms </span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.348.1">of the use of a CPU core. </span><strong class="source-inline"><span class="koboSpan" id="kobo.349.1">NumPy</span></strong><span class="koboSpan" id="kobo.350.1"> is multi-threaded by default, so it makes use of multiple CPU cores in parallel. </span><span class="koboSpan" id="kobo.350.2">Both are bound to a single machine and do not leverage the multi-machine Spark clustering capability. </span><span class="koboSpan" id="kobo.350.3">We will address this limitation in </span><a href="B18568_08.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.351.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.352.1">, which covers scaling. </span><span class="koboSpan" id="kobo.352.2">As a lot of the existing code examples, you will find use </span><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">Pandas</span></strong><span class="koboSpan" id="kobo.354.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.355.1">NumPy</span></strong><span class="koboSpan" id="kobo.356.1">, it is important to start with these libraries as a foundation. </span><span class="koboSpan" id="kobo.356.2">We will then, in </span><a href="B18568_08.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.357.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.358.1">, cover how to convert the single-machine code to leverage Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">clustering capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.360.1">The time series data that will be used for this section is an extended version of that used in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.361.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.362.1"> for the energy consumption of a household. </span><span class="koboSpan" id="kobo.362.2">We will use the same time series for all the models we develop in the rest of this chapter. </span><span class="koboSpan" id="kobo.362.3">The dataset is in </span><strong class="source-inline"><span class="koboSpan" id="kobo.363.1">ts-spark_ch7_ds1_25mb.csv</span></strong><span class="koboSpan" id="kobo.364.1"> in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.365.1">ch7</span></strong><span class="koboSpan" id="kobo.366.1"> folder. </span><span class="koboSpan" id="kobo.366.2">As this is a new dataset, we will go through the steps of exploring the data as part of the </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">next section.</span></span></p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.368.1">Data exploration</span></h2>
<p><span class="koboSpan" id="kobo.369.1">In this section, we</span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.370.1"> want to check the stationarity, seasonality, and autocorrelation in the dataset. </span><span class="koboSpan" id="kobo.370.2">This is a crucial step in our understanding of the nature of the </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">time series.</span></span></p>
<p><span class="koboSpan" id="kobo.372.1">The code for this section is in </span><strong class="source-inline"><span class="koboSpan" id="kobo.373.1">ts-spark_ch7_1e_sarima_comm.dbc</span></strong><span class="koboSpan" id="kobo.374.1">. </span><span class="koboSpan" id="kobo.374.2">We import the code into Databricks Community Edition, as per the approach explained in the </span><em class="italic"><span class="koboSpan" id="kobo.375.1">Hands-on: Loading and visualizing time series</span></em><span class="koboSpan" id="kobo.376.1"> section of </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.377.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.378.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.379.1">The code URL is </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">as follows:</span></span></p>
<p><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc"><span class="No-Break"><span class="koboSpan" id="kobo.381.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc</span></span></a></p>
<p><span class="koboSpan" id="kobo.382.1">The first part of the code loads and prepares the data. </span><span class="koboSpan" id="kobo.382.2">We will not go into the details of this part here as we already covered data preparation in </span><a href="B18568_05.xhtml#_idTextAnchor103"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.383.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.384.1">, and you can refer to the code in the notebook. </span><span class="koboSpan" id="kobo.384.2">The data exploration part is, however, pertinent to this chapter, so let’s explore this further next, starting with the </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">stationarity check.</span></span></p>
<h3><span class="koboSpan" id="kobo.386.1">Stationarity</span></h3>
<p><span class="koboSpan" id="kobo.387.1">We can check whether the energy </span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.388.1">consumption time series is stationary</span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.389.1"> by running the </span><strong class="bold"><span class="koboSpan" id="kobo.390.1">Augmented Dickey-Fuller</span></strong><span class="koboSpan" id="kobo.391.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.392.1">ADF</span></strong><span class="koboSpan" id="kobo.393.1">) test</span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.394.1"> on the data with the </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.396.1">
from statsmodels.tsa.stattools import adfuller
# Perform Augmented Dickey-Fuller test
result = </span><strong class="bold"><span class="koboSpan" id="kobo.397.1">adfuller</span></strong><span class="koboSpan" id="kobo.398.1">(data_hr[-300:]['Global_active_power'])
# if Test statistic &lt; Critical Value and p-value &lt; 0.05
#   reject the Null hypothesis, time series does not have a unit root
#   series is stationary
# Extract and print the ADF test results
print('ADF Statistic:', result[0])
print('p-value:', result[1])
print('Critical Values:')
for key, value in result[4].items():
    print(f'   {key}: {value}')</span></pre> <p><span class="koboSpan" id="kobo.399.1">This gives the following </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">ADF statistics:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.401.1">
ADF Statistic: -6.615237252003429
p-value: 6.231223531550648e-09
Critical Values:
 1%: -3.4524113009049935
 5%: -2.8712554127251764
 10%: -2.571946570731871</span></pre> <p><span class="koboSpan" id="kobo.402.1">As the ADF statistic is </span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.403.1">less than the critical values and the p-value is less than 0.05, we </span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.404.1">can conclude that the time series </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">is stationary.</span></span></p>
<h3><span class="koboSpan" id="kobo.406.1">Seasonality</span></h3>
<p><span class="koboSpan" id="kobo.407.1">We can check on the</span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.408.1"> seasonality</span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.409.1"> with the </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.411.1">
from statsmodels.tsa.seasonal import seasonal_decompose
# Decompose the time series data into seasonal, trend, and residual 
# components
results = </span><strong class="bold"><span class="koboSpan" id="kobo.412.1">seasonal_decompose</span></strong><span class="koboSpan" id="kobo.413.1">(data_hr)
# Plot the last 300 data points of the seasonal component
results.seasonal[-300:].plot(figsize = (12,8));</span></pre> <p><span class="koboSpan" id="kobo.414.1">This gives the seasonal decomposition in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.415.1">Figure 7</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.416.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<span class="koboSpan" id="kobo.418.1"><img alt="" src="image/B18568_07_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.419.1">Figure 7.2: Seasonal decomposition</span></p>
<p><span class="koboSpan" id="kobo.420.1">As the pattern repeats every 24 hours, we can conclude that the time series has a </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">daily seasonality.</span></span></p>
<h3><span class="koboSpan" id="kobo.422.1">Autocorrelation</span></h3>
<p><span class="koboSpan" id="kobo.423.1">We can check on the </span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.424.1">autocorrelation</span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.425.1"> and partial autocorrelation with the </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.427.1">
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
# Plot ACF to identify autocorrelation in 'data_hr' DataFrame
</span><strong class="bold"><span class="koboSpan" id="kobo.428.1">plot_acf</span></strong><span class="koboSpan" id="kobo.429.1">(data_hr['Global_active_power'])
# Plot PACF to identify partial autocorrelation in 'data_hr' DataFrame
</span><strong class="bold"><span class="koboSpan" id="kobo.430.1">plot_pacf</span></strong><span class="koboSpan" id="kobo.431.1">(data_hr['Global_active_power'])
# Display the ACF and PACF plots
plt.show()</span></pre> <p><span class="koboSpan" id="kobo.432.1">This gives the autocorrelation plot in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.433.1">Figure 7</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.434.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer109">
<span class="koboSpan" id="kobo.436.1"><img alt="" src="image/B18568_07_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.437.1">Figure 7.3: Autocorrelation (y axis) at different lags (x axis)</span></p>
<p><span class="koboSpan" id="kobo.438.1">We can see the </span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.439.1">high autocorrelation</span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.440.1"> at the lower lag values, including lag 1, and at lag 12, as well as the effect of seasonality at lag 24. </span><span class="koboSpan" id="kobo.440.2">This makes sense when we consider the following typical patterns of energy consumption in </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">a household:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.442.1">Moments of active energy use, for example for cooking, washing, or use of the television, are likely to go over an hour (</span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">lag 1)</span></span></li>
<li><span class="koboSpan" id="kobo.444.1">The mornings and evenings (lag 12) are usually peaks </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">in activity</span></span></li>
<li><span class="koboSpan" id="kobo.446.1">Daily routines mean that we have similar periods of activities every 24 hours (</span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">lag 24)</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer110">
<span class="koboSpan" id="kobo.448.1"><img alt="" src="image/B18568_07_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.449.1">Figure 7.4: Partial autocorrelation</span></p>
<p><span class="koboSpan" id="kobo.450.1">The PACF plot shows high partial autocorrelation</span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.451.1"> at lag 1 and noticeable partial autocorrelation around lag 10 and lag 23. </span><span class="koboSpan" id="kobo.451.2">This is in line with the typical patterns of energy consumption in a household </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">we mentioned.</span></span></p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.453.1">Statistical model – SARIMA</span></h2>
<p><span class="koboSpan" id="kobo.454.1">The first model we will cover is SARIMA, which extends the </span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.455.1">ARIMA model by incorporating seasonal components. </span><span class="koboSpan" id="kobo.455.2">While ARIMA models address aspects such as autocorrelation, differencing for stationarity, and moving averages, SARIMA adds the handling of seasonal patterns in </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.457.1">The code for this section is in </span><strong class="source-inline"><span class="koboSpan" id="kobo.458.1">ts-spark_ch7_1e_sarima_comm.dbc</span></strong><span class="koboSpan" id="kobo.459.1">. </span><span class="koboSpan" id="kobo.459.2">We import the code into Databricks Community Edition, as per the approach explained in the </span><em class="italic"><span class="koboSpan" id="kobo.460.1">Hands-on: Loading and visualizing time series</span></em><span class="koboSpan" id="kobo.461.1"> section of </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.462.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.463.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.464.1">The code URL is </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">as follows:</span></span></p>
<p><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc"><span class="No-Break"><span class="koboSpan" id="kobo.466.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_sarima_comm.dbc</span></span></a></p>
<h3><span class="koboSpan" id="kobo.467.1">Development and tuning</span></h3>
<p><span class="koboSpan" id="kobo.468.1">For model development, we </span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.469.1">separated the last 48 hours of the dataset from </span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.470.1">the training data with the following code. </span><span class="koboSpan" id="kobo.470.2">This will be used for testing afterward. </span><span class="koboSpan" id="kobo.470.3">We use the rest </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">for training:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.472.1">
# Split the data into training and testing sets
# The last 48 observations are used for testing,
# the rest for training
train = data_hr[:-48]
test = data_hr[-48:]</span></pre> <p><span class="koboSpan" id="kobo.473.1">We will discuss two methods combining training and tuning to train the model and find the best parameters: </span><strong class="source-inline"><span class="koboSpan" id="kobo.474.1">auto_arima</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.475.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.476.1">ParameterGrid</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">.</span></span></p>
<h4><span class="koboSpan" id="kobo.478.1">Auto ARIMA</span></h4>
<p><span class="koboSpan" id="kobo.479.1">With the auto ARIMA </span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.480.1">approach, we </span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.481.1">want to automatically find the model parameters that minimize the </span><strong class="bold"><span class="koboSpan" id="kobo.482.1">Akaike Information Criterion</span></strong><span class="koboSpan" id="kobo.483.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.484.1">AIC</span></strong><span class="koboSpan" id="kobo.485.1">). </span><span class="koboSpan" id="kobo.485.2">This criterion is a statistical measure evaluating the trade-off </span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.486.1">between model complexity and goodness of fit. </span><span class="koboSpan" id="kobo.486.2">A lower AIC indicates a better model. </span><span class="koboSpan" id="kobo.486.3">We will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.487.1">pmdarima</span></strong><span class="koboSpan" id="kobo.488.1"> library to demonstrate the auto ARIMA approach. </span><span class="koboSpan" id="kobo.488.2">As this is a compute-intensive operation, and we want to keep to the time (15 minutes) and resource (Databricks Community Edition) constraints explained previously, we will limit the dataset to the last </span><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">300</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.490.1">data points.</span></span></p>
<p><span class="koboSpan" id="kobo.491.1">The code to use </span><strong class="source-inline"><span class="koboSpan" id="kobo.492.1">pmdarima</span></strong><span class="koboSpan" id="kobo.493.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.495.1">
import </span><strong class="bold"><span class="koboSpan" id="kobo.496.1">pmdarima</span></strong><span class="koboSpan" id="kobo.497.1"> as pm
# Create auto_arima model to automatically select the best ARIMA parameters
model = pm.</span><strong class="bold"><span class="koboSpan" id="kobo.498.1">auto_arima</span></strong><span class="koboSpan" id="kobo.499.1">(
    # Use the last 300 observations of the series for modeling:
    train[-</span><strong class="bold"><span class="koboSpan" id="kobo.500.1">300</span></strong><span class="koboSpan" id="kobo.501.1">:]["Global_active_power"],
    # Enable seasonal differencing:
    seasonal=True,
    # Set the seasonal period to 24
    # (e.g., 24 hours for daily data):
    m=24,
    # Set the degree of non-seasonal differencing to 0
    # (assumes data is already stationary):
    d=0,
    # Set the degree of seasonal differencing to 1:
    D=1,
    # Set the maximum value of AR (p) terms to consider:
    max_p=3,
    # Set the maximum value of MA (q) terms to consider:
    max_q=3,
    # Set the maximum value of seasonal AR (P) terms to consider:
    max_P=3,
    # Set the maximum value of seasonal MA (Q) terms to consider:
    max_Q=3,
    # Use AIC (Akaike Information Criterion) to select the best model:
    information_criterion='</span><strong class="bold"><span class="koboSpan" id="kobo.502.1">aic</span></strong><span class="koboSpan" id="kobo.503.1">',
    # Print fit information to see the progression of
    # the model fitting:
    trace=True,
    # Ignore models that fail to converge:
    error_action='ignore',
    # Use stepwise algorithm for efficient search of the model space:
    stepwise=True,
    # Suppress convergence warnings:
    suppress_warnings=True
)
# Print the summary of the fitted model
print(model.summary())</span></pre> <p><span class="koboSpan" id="kobo.504.1">The following code output shows the </span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.505.1">step-by-step search for the parameters minimizing the </span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.506.1">AIC. </span><span class="koboSpan" id="kobo.506.2">This will be the best set of model parameters to use with the ARIMA model to forecast this household’s </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">energy consumption:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.508.1">
Performing stepwise search to minimize aic
…
ARIMA(1,0,1)(2,1,0)[24] intercept : AIC=688.757, Time=9.37 sec
…
ARIMA(2,0,2)(2,1,0)[24] : AIC=681.750, Time=6.83 sec
…
ARIMA(1,0,1)(2,1,0)[24] : AIC=686.763, Time=6.02 sec
Best model: ARIMA(2,0,2)(2,1,0)[24]</span></pre> <p><span class="koboSpan" id="kobo.509.1">Note that while this is the best set of model parameters, we may find, given the time and resource constraints, that we may be able to find a better model with a longer run of </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">the algorithm.</span></span></p>
<h4><span class="koboSpan" id="kobo.511.1">ParameterGrid</span></h4>
<p><span class="koboSpan" id="kobo.512.1">With the </span><strong class="source-inline"><span class="koboSpan" id="kobo.513.1">ParameterGrid</span></strong><span class="koboSpan" id="kobo.514.1"> approach, we </span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.515.1">will sweep one by one through a list of parameter</span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.516.1"> combinations to find the model parameters that minimize </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">the AIC.</span></span></p>
<p><span class="koboSpan" id="kobo.518.1">The code to use </span><strong class="source-inline"><span class="koboSpan" id="kobo.519.1">ParameterGrid</span></strong><span class="koboSpan" id="kobo.520.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.522.1">
# Define parameter grid for SARIMAX model configuration
param_grid = {
    'order': [(0, 0, 0), (1, 0, 1), (2, 0, 0)],
    # Non-seasonal ARIMA orders
    'seasonal_order': [
        (0, 0, 0, 24),
        (2, 0, 1, 24),
        (2, 1, 1, 24)
    ],  # Seasonal ARIMA orders with period of 24
}
# Initialize variables to store the best AIC and
# corresponding parameters
best_aic = float("inf")
best_params = ["",""]
# Iterate over all combinations of parameters in the grid
for params in ParameterGrid(param_grid):
    print(
        f"order: {params['order']}, seasonal_order: {params['seasonal_order']}"
    )
    try:
        # Initialize and fit SARIMAX model with current parameters
        model = SARIMAX(
            train['Global_active_power'],
            order=params['order'],
            seasonal_order=params['seasonal_order'])
        model_fit = model.fit(disp=False)
        print(f"aic: {model_fit.aic}")
        # Update best parameters if current model has lower AIC
        if model_fit.aic &lt; best_aic:
            best_aic = model_fit.aic
            best_params = params
    except Exception as error:
        print("An error occurred:", error)
        continue</span></pre> <p><span class="koboSpan" id="kobo.523.1">While both auto</span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.524.1"> ARIMA and </span><strong class="source-inline"><span class="koboSpan" id="kobo.525.1">ParamaeterGrid</span></strong><span class="koboSpan" id="kobo.526.1"> are similar in terms of minimizing </span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.527.1">AIC, auto ARIMA is much simpler to use with only 1 line </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">of code.</span></span></p>
<p><span class="koboSpan" id="kobo.529.1">After the SARIMA model is trained, we will next test the </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">model forecasting.</span></span></p>
<h3><span class="koboSpan" id="kobo.531.1">Testing and forecasting</span></h3>
<p><span class="koboSpan" id="kobo.532.1">We use the model to forecast the test </span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.533.1">dataset with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.534.1">predict</span></strong><span class="koboSpan" id="kobo.535.1"> function, one period at </span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.536.1">a time, updating the model with the actual value after every time forecast. </span><span class="koboSpan" id="kobo.536.2">This iterative approach converts single-step forecasting in </span><strong class="source-inline"><span class="koboSpan" id="kobo.537.1">forecast_step</span></strong><span class="koboSpan" id="kobo.538.1"> into </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">multi-step forecasting:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.540.1">
def </span><strong class="bold"><span class="koboSpan" id="kobo.541.1">forecast_step</span></strong><span class="koboSpan" id="kobo.542.1">():
    # Predicts the next period with confidence intervals
    forecast, conf_int = model.</span><strong class="bold"><span class="koboSpan" id="kobo.543.1">predict</span></strong><span class="koboSpan" id="kobo.544.1">(
        n_periods=1, return_conf_int=True)
…
# Iterate over each observation in the test dataset
for obs in test['Global_active_power']:
    forecast, conf_int = </span><strong class="bold"><span class="koboSpan" id="kobo.545.1">forecast_step</span></strong><span class="koboSpan" id="kobo.546.1">()  # Forecast next step
    forecasts.append(forecast)  # Append forecast to list
…
    # Update the model with the new observation
    model.update(obs)</span></pre> <p><span class="koboSpan" id="kobo.547.1">We can then plot the forecast against the actual values in </span><em class="italic"><span class="koboSpan" id="kobo.548.1">Figures 7.5</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.549.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.550.1">7.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer111">
<span class="koboSpan" id="kobo.552.1"><img alt="" src="image/B18568_07_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.553.1">Figure 7.5: SARIMA Forecast vs Actuals (training and testing)</span></p>
<p><span class="koboSpan" id="kobo.554.1">We zoom in on the</span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.555.1"> testing period in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.556.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.557.1">.6</span></em><span class="koboSpan" id="kobo.558.1"> for a visual comparison of the </span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.559.1">forecast </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">and actuals.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer112">
<span class="koboSpan" id="kobo.561.1"><img alt="" src="image/B18568_07_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.562.1">Figure 7.6: SARIMA Forecast vs Actuals (zoom on test data)</span></p>
<p><span class="koboSpan" id="kobo.563.1">While visualizing the </span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.564.1">graphs gives us an idea of the forecasting capability of the</span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.565.1"> model, we need quantifiable metrics on how good the model is. </span><span class="koboSpan" id="kobo.565.2">These metrics will also allow us to compare forecasting accuracy with </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">other models.</span></span></p>
<p><span class="koboSpan" id="kobo.567.1">There are several metrics available for time series forecasting. </span><span class="koboSpan" id="kobo.567.2">We will show the use of the following three in this chapter, to highlight how different metrics serve </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">different objectives:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.569.1">Mean Squared Error</span></strong><span class="koboSpan" id="kobo.570.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.571.1">MSE</span></strong><span class="koboSpan" id="kobo.572.1">) measures </span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.573.1">the average squared differences between the forecasted (F) and actual (A) values. </span><span class="koboSpan" id="kobo.573.2">It works well when we want to penalize large errors. </span><span class="koboSpan" id="kobo.573.3">However, it is sensitive to outliers because the squaring of errors gives importance to </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">large discrepancies.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.575.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn mathvariant=&quot;italic&quot;&gt;1&lt;/mn&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;=&lt;/mo&gt;&lt;mn mathvariant=&quot;italic&quot;&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msubsup&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mn mathvariant=&quot;italic&quot;&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/2.png" style="vertical-align:-0.478em;height:1.502em;width:9.168em"/></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.576.1">Symmetric Mean Absolute Percentage Error</span></strong><span class="koboSpan" id="kobo.577.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.578.1">SMAPE</span></strong><span class="koboSpan" id="kobo.579.1">) is the average of the absolute differences between </span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.580.1">forecasted (F) and actual (A) values. </span><span class="koboSpan" id="kobo.580.2">It is expressed as a percentage over half of the sum of absolute values of actual and forecasted values. </span><span class="koboSpan" id="kobo.580.3">SMAPE adjusts to the scale of the data, making it suitable for comparisons across different datasets. </span><span class="koboSpan" id="kobo.580.4">Due to its symmetric scaling, it is less sensitive to </span><span class="No-Break"><span class="koboSpan" id="kobo.581.1">extreme values.</span></span></li>
</ul>
<p class="IMG---Figure"><span class="koboSpan" id="kobo.582.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mn mathvariant=&quot;italic&quot;&gt;100&lt;/mn&gt;&lt;mi&gt;%&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;=&lt;/mo&gt;&lt;mn mathvariant=&quot;italic&quot;&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msubsup&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;+&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;/&lt;/mo&gt;&lt;mn mathvariant=&quot;italic&quot;&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/3.png" style="vertical-align:-0.863em;height:2.395em;width:13.397em"/></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.583.1">Weighted Absolute Percentage Error</span></strong><span class="koboSpan" id="kobo.584.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.585.1">WAPE</span></strong><span class="koboSpan" id="kobo.586.1">) is a </span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.587.1">normalized measure of error, weighing the absolute errors by the actual values. </span><span class="koboSpan" id="kobo.587.2">It works well when dealing with data of varying magnitudes but is sensitive to </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">high-value errors.</span></span></li>
</ul>
<p class="IMG---Figure"><span class="koboSpan" id="kobo.589.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mi&gt;E&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;100&lt;/mml:mn&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;%&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/4.png" style="vertical-align:-0.918em;height:2.391em;width:11.762em"/></span></p>
<p><span class="koboSpan" id="kobo.590.1">We will see two different approaches to metrics calculation: metrics calculation functions included in the model library, and a separate dedicated metrics </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">calculation library.</span></span></p>
<h4><span class="koboSpan" id="kobo.592.1">Metric functions from the model library</span></h4>
<p><span class="koboSpan" id="kobo.593.1">In this approach, we want to</span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.594.1"> use the functions for metrics calculations already included in the model library. </span><span class="koboSpan" id="kobo.594.2">We will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">sklearn</span></strong><span class="koboSpan" id="kobo.596.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.597.1">pmdarima</span></strong><span class="koboSpan" id="kobo.598.1"> libraries for the metric calculations to demonstrate this in the </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.600.1">
from </span><strong class="bold"><span class="koboSpan" id="kobo.601.1">sklearn</span></strong><span class="koboSpan" id="kobo.602.1">.metrics import mean_squared_error
from </span><strong class="bold"><span class="koboSpan" id="kobo.603.1">pmdarima</span></strong><span class="koboSpan" id="kobo.604.1">.metrics import smape
# Calculate and print the mean squared error of the forecasts
print(f"Mean squared error: {</span><strong class="bold"><span class="koboSpan" id="kobo.605.1">mean_squared_error</span></strong><span class="koboSpan" id="kobo.606.1">(test['Global_active_power'], forecasts)}")
# Calculate and print the Symmetric Mean Absolute Percentage Error 
# (SMAPE)
print(f"SMAPE: {</span><strong class="bold"><span class="koboSpan" id="kobo.607.1">smape</span></strong><span class="koboSpan" id="kobo.608.1">(test['Global_active_power'], forecasts)}")</span></pre> <p><span class="koboSpan" id="kobo.609.1">This gives the </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">following results:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.611.1">
Mean squared error: 0.6131968222566936
SMAPE: 43.775868579535334</span></pre> <h4><span class="koboSpan" id="kobo.612.1">Separate metrics library</span></h4>
<p><span class="koboSpan" id="kobo.613.1">In this second approach for </span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.614.1">metrics calculation, we use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.615.1">SeqMetrics</span></strong><span class="koboSpan" id="kobo.616.1"> library, as in the </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.618.1">
from </span><strong class="bold"><span class="koboSpan" id="kobo.619.1">SeqMetrics</span></strong><span class="koboSpan" id="kobo.620.1"> import RegressionMetrics, plot_metrics
# Initialize the RegressionMetrics object with actual and
# predicted values
er = </span><strong class="bold"><span class="koboSpan" id="kobo.621.1">RegressionMetrics</span></strong><span class="koboSpan" id="kobo.622.1">(
    test['Global_active_power'], forecasts)
# Calculate all available regression metrics
metrics = er.calculate_all()
# Plot the calculated metrics using a color scheme
</span><strong class="bold"><span class="koboSpan" id="kobo.623.1">plot_metrics</span></strong><span class="koboSpan" id="kobo.624.1">(metrics, color="Blues")
# Display the Symmetric Mean Absolute Percentage Error (SMAPE)
print(f"Test SMAPE: {metrics['</span><strong class="bold"><span class="koboSpan" id="kobo.625.1">smape</span></strong><span class="koboSpan" id="kobo.626.1">']}")
# Display the Weighted Absolute Percentage Error (WAPE)
print(f"Test WAPE: {metrics['</span><strong class="bold"><span class="koboSpan" id="kobo.627.1">wape</span></strong><span class="koboSpan" id="kobo.628.1">']}")</span></pre> <p><span class="koboSpan" id="kobo.629.1">This gives the </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">following </span></span><span class="No-Break"><a id="_idIndexMarker692"/></span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">results:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.632.1">
Test SMAPE: 43.775868579535334
Test WAPE: 0.4202224470299464</span></pre> <p><span class="koboSpan" id="kobo.633.1">This library also provides a nice visualization of all the metrics calculated, as in </span><em class="italic"><span class="koboSpan" id="kobo.634.1">Figures 7.7</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.635.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.636.1">7.8</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<span class="koboSpan" id="kobo.638.1"><img alt="" src="image/B18568_07_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.639.1">Figure 7.7: SeqMetrics display of WAPE</span></p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<span class="koboSpan" id="kobo.640.1"><img alt="" src="image/B18568_07_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.641.1">Figure 7.8: SeqMetrics display of SMAPE</span></p>
<p><span class="koboSpan" id="kobo.642.1">After training and testing our first model, we can move on to the next model, which is a classical Machine </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">Learning model.</span></span></p>
<h2 id="_idParaDest-143"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.644.1">Classical Machine Learning model – LightGBM</span></h2>
<p><span class="koboSpan" id="kobo.645.1">The second model we will cover is </span><strong class="bold"><span class="koboSpan" id="kobo.646.1">Light Gradient Boosting Machine</span></strong><span class="koboSpan" id="kobo.647.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.648.1">LightGBM</span></strong><span class="koboSpan" id="kobo.649.1">), which is a free open source gradient</span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.650.1"> boosting model. </span><span class="koboSpan" id="kobo.650.2">It is based on the tree learning algorithm, designed to be efficient </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">and distributed.</span></span></p>
<p><span class="koboSpan" id="kobo.652.1">The code for this section is in </span><strong class="source-inline"><span class="koboSpan" id="kobo.653.1">ts-spark_ch7_1e_lgbm_comm.dbc</span></strong><span class="koboSpan" id="kobo.654.1">. </span><span class="koboSpan" id="kobo.654.2">We import the code into Databricks Community Edition, as per the approach explained in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.655.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.656.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.657.1">The code URL is </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">as</span></span><span class="No-Break"><a id="_idIndexMarker694"/></span><span class="No-Break"><span class="koboSpan" id="kobo.659.1"> follows:</span></span></p>
<p><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_lgbm_comm.dbc"><span class="No-Break"><span class="koboSpan" id="kobo.660.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_lgbm_comm.dbc</span></span></a></p>
<h3><span class="koboSpan" id="kobo.661.1">Development and tuning</span></h3>
<p><span class="koboSpan" id="kobo.662.1">For model </span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.663.1">development, we separated the last 48 hours of the dataset from the training data with the following code. </span><span class="koboSpan" id="kobo.663.2">This will be used for testing afterward. </span><span class="koboSpan" id="kobo.663.3">We use the rest </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">for training:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.665.1">
# Split the data into training and testing sets
# The last 48 observations are used for testing, the rest for training
train = data_hr[:-48]
test = data_hr[-48:]</span></pre> <p><span class="koboSpan" id="kobo.666.1">We will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.667.1">GridSearchCV</span></strong><span class="koboSpan" id="kobo.668.1"> method to find the best parameters for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.669.1">LGBMRegressor</span></strong><span class="koboSpan" id="kobo.670.1"> model. </span><strong class="source-inline"><span class="koboSpan" id="kobo.671.1">TimeSeriesSplit</span></strong><span class="koboSpan" id="kobo.672.1"> is used to split the training dataset for cross-validation, respecting the time series nature of </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">the dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.674.1">
# Define the parameter grid for LightGBM
param_grid = {
    'num_leaves': [30, 50, 100],
    'learning_rate': [0.1, 0.01, 0.001],
    'n_estimators': [50, 100, 200]
}
# Initialize LightGBM regressor
lgbm = lgb.</span><strong class="bold"><span class="koboSpan" id="kobo.675.1">LGBMRegressor</span></strong><span class="koboSpan" id="kobo.676.1">()
# Setup TimeSeriesSplit for cross-validation
tscv = </span><strong class="bold"><span class="koboSpan" id="kobo.677.1">TimeSeriesSplit</span></strong><span class="koboSpan" id="kobo.678.1">(n_splits=10)
# Configure and run GridSearchCV
gsearch = </span><strong class="bold"><span class="koboSpan" id="kobo.679.1">GridSearchCV</span></strong><span class="koboSpan" id="kobo.680.1">(
    estimator=lgbm,
    param_grid=param_grid,
    cv=tscv
)
gsearch.fit(X_train, y_train)
# Output the best parameters from Grid Search
print(f"Best Parameters: {gsearch.</span><strong class="bold"><span class="koboSpan" id="kobo.681.1">best_params</span></strong><span class="koboSpan" id="kobo.682.1">_}")</span></pre> <p><span class="koboSpan" id="kobo.683.1">We find the following </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">best parameters:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.685.1">
Best Parameters: {'learning_rate': 0.1, 'n_estimators': 50, 'num_leaves': 30}</span></pre> <p><span class="koboSpan" id="kobo.686.1">Based on the training dataset, this will be the best set of parameters to use with the LightGBM model to forecast this household’s energy consumption. </span><span class="koboSpan" id="kobo.686.2">We can then train the final model with </span><a id="_idIndexMarker696"/><span class="No-Break"><span class="koboSpan" id="kobo.687.1">these parameters:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.688.1">
final_model = lgb.LGBMRegressor(**</span><strong class="bold"><span class="koboSpan" id="kobo.689.1">best_params</span></strong><span class="koboSpan" id="kobo.690.1">)
final_model.fit(X_train, y_train)</span></pre> <p><span class="koboSpan" id="kobo.691.1">After the LightGBM model is trained, we will test the model </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">forecasting next.</span></span></p>
<h3><span class="koboSpan" id="kobo.693.1">Testing and forecasting</span></h3>
<p><span class="koboSpan" id="kobo.694.1">We use the model to forecast the test </span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.695.1">dataset with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.696.1">predict</span></strong><span class="koboSpan" id="kobo.697.1"> function. </span><span class="koboSpan" id="kobo.697.2">Note that in this case, we have not had the need to use iterative multi-step forecasting code. </span><span class="koboSpan" id="kobo.697.3">We have instead used the lag values as input features to </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">the model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.699.1">
# Predict on the test set
y_pred = final_model.</span><strong class="bold"><span class="koboSpan" id="kobo.700.1">predict</span></strong><span class="koboSpan" id="kobo.701.1">(X_test)</span></pre> <p><span class="koboSpan" id="kobo.702.1">We can then plot the </span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.703.1">forecast against the actual values in </span><em class="italic"><span class="koboSpan" id="kobo.704.1">Figures 7.8</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.705.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.706.1">7.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<span class="koboSpan" id="kobo.708.1"><img alt="" src="image/B18568_07_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.709.1">Figure 7.9: LightGBM Forecast vs Actuals (training and testing)</span></p>
<p><span class="koboSpan" id="kobo.710.1">We zoom in on the testing period in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.711.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.712.1">.9</span></em><span class="koboSpan" id="kobo.713.1"> for a visual comparison of the forecast </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">and actuals.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<span class="koboSpan" id="kobo.715.1"><img alt="" src="image/B18568_07_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.716.1">Figure 7.10: LightGBM Forecast vs Actuals (zoom on test data)</span></p>
<p><span class="koboSpan" id="kobo.717.1">Based on the forecast and</span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.718.1"> actuals, we can then measure the SMAPE and WAPE, getting the </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">following values:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.720.1">
Test SMAPE: 41.457989848314384
Test WAPE: 0.38978585281926825</span></pre> <p><span class="koboSpan" id="kobo.721.1">Now that we have trained and tested statistical and classical Machine Learning models, we can move on to a third type of model, which is a Deep </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">Learning model.</span></span></p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.723.1">Deep Learning model – NeuralProphet</span></h2>
<p><span class="koboSpan" id="kobo.724.1">The third model we will cover is </span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.725.1">NeuralProphet, which is a free open source Deep Learning model inspired by Prophet, which we used in previous chapters, and AR-Net. </span><span class="koboSpan" id="kobo.725.2">NeuralProphet is built </span><span class="No-Break"><span class="koboSpan" id="kobo.726.1">on PyTorch.</span></span></p>
<p><span class="koboSpan" id="kobo.727.1">The code for this section is in </span><strong class="source-inline"><span class="koboSpan" id="kobo.728.1">ts-spark_ch7_1e_nprophet_comm.dbc</span></strong><span class="koboSpan" id="kobo.729.1">. </span><span class="koboSpan" id="kobo.729.2">We import the code into Databricks Community Edition, as per the approach explained in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.730.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.731.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.732.1">The code URL is as </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">follows: </span></span><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_nprophet_comm.dbc"><span class="No-Break"><span class="koboSpan" id="kobo.734.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch7/ts-spark_ch7_1e_nprophet_comm.dbc</span></span></a></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.735.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.736.1">Note that the notebook for this example requires Databricks compute DBR 13.3 </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">LTS ML.</span></span></p>
<h3><span class="koboSpan" id="kobo.738.1">Development</span></h3>
<p><span class="koboSpan" id="kobo.739.1">We instantiate a </span><strong class="source-inline"><span class="koboSpan" id="kobo.740.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.741.1"> model, specifying with </span><strong class="source-inline"><span class="koboSpan" id="kobo.742.1">n_lag</span></strong><span class="koboSpan" id="kobo.743.1"> that we want to use the past 24 hours for the forecasting. </span><span class="koboSpan" id="kobo.743.2">We</span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.744.1"> then train (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.745.1">fit</span></strong><span class="koboSpan" id="kobo.746.1"> method) the model on the </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">training dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.748.1">
# Initialize and fit the Prophet model
# model = NeuralProphet()
model = </span><strong class="bold"><span class="koboSpan" id="kobo.749.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.750.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.751.1">n_lags</span></strong><span class="koboSpan" id="kobo.752.1">=24, quantiles=[0.05, 0.95])
metrics = model.</span><strong class="bold"><span class="koboSpan" id="kobo.753.1">fit</span></strong><span class="koboSpan" id="kobo.754.1">(train_df)</span></pre> <p><span class="koboSpan" id="kobo.755.1">With these two lines of code sufficient to train the model, we will next test the </span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">model forecasting.</span></span></p>
<h3><span class="koboSpan" id="kobo.757.1">Testing and forecasting</span></h3>
<p><span class="koboSpan" id="kobo.758.1">Before using the</span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.759.1"> model to forecast the test dataset, we need to prepare the data for NeuralProphet, similar to how we did previously for Prophet. </span><span class="koboSpan" id="kobo.759.2">The required format is to have a </span><strong class="source-inline"><span class="koboSpan" id="kobo.760.1">ds</span></strong><span class="koboSpan" id="kobo.761.1"> column for the date/time and </span><strong class="source-inline"><span class="koboSpan" id="kobo.762.1">y</span></strong><span class="koboSpan" id="kobo.763.1"> for the forecasting target. </span><span class="koboSpan" id="kobo.763.2">We can then use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.764.1">predict</span></strong><span class="koboSpan" id="kobo.765.1"> method. </span><span class="koboSpan" id="kobo.765.2">Note that in this case, we have not had the need to use iterative multi-step forecasting code. </span><span class="koboSpan" id="kobo.765.3">With the lag of 24 specified as a parameter in the previous code section, NeuralProphet uses a sliding window of the past 24 values to forecast the </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">next values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.767.1">
# Convert the DataFrame index to datetime,
# removing timezone information
test_df['</span><strong class="bold"><span class="koboSpan" id="kobo.768.1">ds</span></strong><span class="koboSpan" id="kobo.769.1">'] = test_df.index.to_pydatetime()
test_df['ds'] = test_df['ds'].apply(
    lambda x: x.replace(tzinfo=None))
# Rename the target variable for Prophet compatibility
test_df = test_df.rename(columns={'Global_active_power': '</span><strong class="bold"><span class="koboSpan" id="kobo.770.1">y</span></strong><span class="koboSpan" id="kobo.771.1">'})
# Use the trained model to make predictions on the test set
predictions_48h = model.</span><strong class="bold"><span class="koboSpan" id="kobo.772.1">predict</span></strong><span class="koboSpan" id="kobo.773.1">(test_df)</span></pre> <p><span class="koboSpan" id="kobo.774.1">We plot the </span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.775.1">forecast against the actual values in </span><em class="italic"><span class="koboSpan" id="kobo.776.1">Figures 7.12</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.777.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.778.1">7.13</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<span class="koboSpan" id="kobo.780.1"><img alt="" src="image/B18568_07_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.781.1">Figure 7.11: NeuralProphet Forecast vs Actuals (training and testing)</span></p>
<p><span class="koboSpan" id="kobo.782.1">We zoom in on the testing period in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.783.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.784.1">.13</span></em><span class="koboSpan" id="kobo.785.1"> for a visual comparison of the forecast </span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">and actuals.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<span class="koboSpan" id="kobo.787.1"><img alt="" src="image/B18568_07_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.788.1">Figure 7.12: NeuralProphet Forecast vs Actuals (zoom on test data)</span></p>
<p><span class="koboSpan" id="kobo.789.1">Based on the forecast and actuals, we can then measure the SMAPE and WAPE, getting the following values as a measurement of the accuracy of </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">the model:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.791.1">
Test SMAPE: 41.193985580947896
Test WAPE: 0.35355667972102317</span></pre> <p><span class="koboSpan" id="kobo.792.1">We will use these metrics to compare the different models we have used in this chapter, in the later </span><em class="italic"><span class="koboSpan" id="kobo.793.1">Model </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.794.1">comparison</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.795.1"> section.</span></span></p>
<p><span class="koboSpan" id="kobo.796.1">So far, we have trained and </span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.797.1">tested each type of model: a statistical, a classical Machine Learning, and a Deep Learning model. </span><span class="koboSpan" id="kobo.797.2">Other examples of commonly used models for time series are provided in the book’s </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">GitHub repository:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.799.1">Prophet: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.800.1">ts-spark_ch7_1e_prophet_comm.dbc</span></strong></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.801.1">LSTM: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.802.1">ts-spark_ch7_1e_lstm_comm1-cpu.dbc</span></strong></span></li>
<li><span class="koboSpan" id="kobo.803.1">NBEATS and </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">NHITS: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.805.1">ts-spark_ch7_1e_nbeats-nhits_comm.dbc</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.806.1">We encourage you to explore </span><span class="No-Break"><span class="koboSpan" id="kobo.807.1">these further.</span></span></p>
<p><span class="koboSpan" id="kobo.808.1">Having a working model is great but is not sufficient. </span><span class="koboSpan" id="kobo.808.2">We also need to be able to explain the model we are working with. </span><span class="koboSpan" id="kobo.808.3">We will cover </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">this next.</span></span></p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.810.1">Explainability</span></h2>
<p><span class="koboSpan" id="kobo.811.1">Explainability is a key </span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.812.1">requirement in many cases, such as for financial and regulated industries. </span><span class="koboSpan" id="kobo.812.2">We will look at how to do this now using a widely used method called </span><strong class="bold"><span class="koboSpan" id="kobo.813.1">Shapley Additive Explanations</span></strong><span class="koboSpan" id="kobo.814.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.815.1">SHAP</span></strong><span class="koboSpan" id="kobo.816.1">) to </span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.817.1">explain how the different features of the dataset contributed to </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">the prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.819.1">We will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.820.1">TreeExplainer</span></strong><span class="koboSpan" id="kobo.821.1"> function of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.822.1">shap</span></strong><span class="koboSpan" id="kobo.823.1"> library on the final model from the </span><em class="italic"><span class="koboSpan" id="kobo.824.1">Classical Machine Learning model – LightGBM</span></em><span class="koboSpan" id="kobo.825.1"> section to compute the SHAP values, which will give us the impact of each feature on the </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">model output.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.827.1">
import shap
# Initialize a SHAP TreeExplainer with the trained model
explainer = </span><strong class="bold"><span class="koboSpan" id="kobo.828.1">shap</span></strong><span class="koboSpan" id="kobo.829.1">.</span><strong class="bold"><span class="koboSpan" id="kobo.830.1">TreeExplainer</span></strong><span class="koboSpan" id="kobo.831.1">(final_model)
# Select features for SHAP analysis
X = data_hr[[
    'Global_active_power_lag1', 'Global_active_power_lag2',
    'Global_active_power_lag3', 'Global_active_power_lag4',
    'Global_active_power_lag5', 'Global_active_power_lag12',
    'Global_active_power_lag24', 'Global_active_power_lag24x7'
]]
# Compute SHAP values for the selected features
</span><strong class="bold"><span class="koboSpan" id="kobo.832.1">shap_values</span></strong><span class="koboSpan" id="kobo.833.1"> = explainer(X)
# Generate and display a summary plot of the SHAP values
shap.</span><strong class="bold"><span class="koboSpan" id="kobo.834.1">summary_plot</span></strong><span class="koboSpan" id="kobo.835.1">(shap_values, X)</span></pre> <p><span class="koboSpan" id="kobo.836.1">We can then plot the</span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.837.1"> feature importance in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.838.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.839.1">.10</span></em><span class="koboSpan" id="kobo.840.1">. </span><span class="koboSpan" id="kobo.840.2">As expected from the data exploration we did in the earlier section, lag 1 and lag 24 are the features contributing the most to </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">the forecasting.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<span class="koboSpan" id="kobo.842.1"><img alt="" src="image/B18568_07_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.843.1">Figure 7.13: SHAP – feature importance</span></p>
<p><span class="koboSpan" id="kobo.844.1">We can go further in the analysis </span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.845.1">by focusing on a specific forecast with the following code, where we want to explain the forecasting for the </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">first value:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.847.1">
# Plot a SHAP waterfall plot for the first observation's SHAP values # to visualize the contribution of each feature
shap.plots.</span><strong class="bold"><span class="koboSpan" id="kobo.848.1">waterfall</span></strong><span class="koboSpan" id="kobo.849.1">(shap_values[0])</span></pre> <p><span class="koboSpan" id="kobo.850.1">We can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.851.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.852.1">.11</span></em><span class="koboSpan" id="kobo.853.1"> the relative contributions of the features, again with pre-dominance of lag 1 and 24, and to a lesser extent lag 12. </span><span class="koboSpan" id="kobo.853.2">This is coherent with our analysis in the </span><em class="italic"><span class="koboSpan" id="kobo.854.1">Data exploration</span></em><span class="koboSpan" id="kobo.855.1"> section, where we established the pertinence of these lags in forecasting the energy consumption of </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">a household.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer123">
<span class="koboSpan" id="kobo.857.1"><img alt="" src="image/B18568_07_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.858.1">Figure 7.14: SHAP – feature importance (first observation)</span></p>
<h1 id="_idParaDest-146"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.859.1">Model comparison</span></h1>
<p><span class="koboSpan" id="kobo.860.1">Before </span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.861.1">concluding this chapter, we will compare all the models we have tested based on the metrics we measured and the code execution time. </span><span class="koboSpan" id="kobo.861.2">The results are shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.862.1">Table 7.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.863.1">.</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.864.1">Model</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.865.1">Type</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.866.1">SMAPE</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.867.1">WAPE</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.868.1">Training</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.869.1">Tuning</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.870.1">Testing</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.871.1">Total incl. </span><span class="koboSpan" id="kobo.871.2">data </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.872.1">prep.</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.873.1">NeuralProphet</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.874.1">DL/Mixed</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.875.1">41.19</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.876.1">0.35</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.877.1">60s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.878.1">-</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.879.1">1s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.880.1">90s</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.881.1">LightGBM</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.882.1">Classical ML</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.883.1">41.46</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.884.1">0.39</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.885.1">60s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.886.1">Included</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.887.1">Included</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.888.1">137s</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.889.1">SARIMA</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.890.1">Statistical</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.891.1">43.78</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.892.1">0.42</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.893.1">Included</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.894.1">420s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.895.1">180s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.896.1">662s</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.897.1">Prophet</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.898.1">Statistical/Mixed</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.899.1">47.60</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.900.1">0.41</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.901.1">2s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.902.1">-</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.903.1">1s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.904.1">70s</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.905.1">NHITS</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.906.1">DL</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.907.1">54.43</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.908.1">0.47</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.909.1">35s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.910.1">-</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.911.1">Included</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.912.1">433s</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.913.1">NBEATS</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.914.1">DL</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.915.1">54.91</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.916.1">0.48</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.917.1">35s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.918.1">-</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.919.1">Included</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.920.1">433s</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.921.1">LSTM</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.922.1">DL</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.923.1">55.08</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.924.1">0.48</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.925.1">722s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.926.1">-</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.927.1">4s</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.928.1">794s</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.929.1">Table 7.1: Model results comparison</span></p>
<p><span class="koboSpan" id="kobo.930.1">Here are a few observations on the </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">model accuracy:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.932.1">NeuralProphet and LightGBM gave the best forecasting accuracy with both the SMAPE and WAPE metrics. </span><span class="koboSpan" id="kobo.932.2">SARIMA was not very </span><span class="No-Break"><span class="koboSpan" id="kobo.933.1">far behind.</span></span></li>
<li><span class="koboSpan" id="kobo.934.1">The Deep Learning models, NBEATS, NHITS, and LSTM, did not have good forecasting accuracy when used as single-input models. </span><span class="koboSpan" id="kobo.934.2">We encourage you to explore further how they can be improved with </span><span class="No-Break"><span class="koboSpan" id="kobo.935.1">multiple inputs.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.936.1">The following is in regard to </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">execution time:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.938.1">In all the cases, we kept within the constraint of a total execution of 900s (15 minutes) with the 2 CPU cores on a single-node Databricks Community Edition cluster. </span><span class="koboSpan" id="kobo.938.2">This worked with the 25 MB dataset. </span><span class="koboSpan" id="kobo.938.3">We will see in </span><a href="B18568_08.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.939.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.940.1"> how to scale for </span><span class="No-Break"><span class="koboSpan" id="kobo.941.1">larger datasets.</span></span></li>
<li><span class="koboSpan" id="kobo.942.1">Prophet, NBEATS, and NHITS had the best execution time, with NeuralProphet and LightGBM coming after, still within 1 minute for training, tuning, </span><span class="No-Break"><span class="koboSpan" id="kobo.943.1">and testing.</span></span></li>
<li><span class="koboSpan" id="kobo.944.1">SARIMA had a relatively high execution time, even if we limited the dataset to the last 300 observations. </span><span class="koboSpan" id="kobo.944.2">This was due to the Auto ARIMA algorithm searching for the best hyperparameter, and then the multi-step iterative </span><span class="No-Break"><span class="koboSpan" id="kobo.945.1">forecasting code.</span></span></li>
<li><span class="koboSpan" id="kobo.946.1">LSTM had the longest execution time, which can be explained by the use of CPUs instead of GPUs, which</span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.947.1"> are much faster for </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">Deep Learning.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.949.1">The overall conclusion from this model comparison is that NeuralProphet and LightGBM are the best choices for the dataset we used, with minimal tuning, and for the compute and execution time constraint that </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">we set.</span></span></p>
<h1 id="_idParaDest-147"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.951.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.952.1">In this chapter, we have focused on the core topic of this book, which is the development of models for time series analysis, more specifically for forecasting. </span><span class="koboSpan" id="kobo.952.2">Starting with a review of the different types of models, we then looked at the important criteria guiding the choice of the right model to use. </span><span class="koboSpan" id="kobo.952.3">In the second part of the chapter, we put into practice the development and testing of several models, which we then compared on accuracy and </span><span class="No-Break"><span class="koboSpan" id="kobo.953.1">execution time.</span></span></p>
<p><span class="koboSpan" id="kobo.954.1">In the next chapter, we will expand on a topic where Apache Spark shines: scaling time series analysis to </span><span class="No-Break"><span class="koboSpan" id="kobo.955.1">big data.</span></span></p>
<h1 id="_idParaDest-148"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.956.1">Join our community on Discord</span></h1>
<p><span class="koboSpan" id="kobo.957.1">Join our community’s Discord space for discussions with the authors and </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">other readers:</span></span></p>
<p><a href="https://packt.link/ds"><span class="No-Break"><span class="koboSpan" id="kobo.959.1">https://packt.link/ds</span></span></a></p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<span class="koboSpan" id="kobo.960.1"><img alt="" src="image/ds_(1).jpg"/></span>
</div>
</div>
</div>


<div class="Content" epub:type="part" id="_idContainer126">
<h1 id="_idParaDest-149" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.1.1">Part 3:  Scaling to Production  and Beyond</span></h1>
</div>
<div id="_idContainer127">
<p><span class="koboSpan" id="kobo.2.1">In this last part, we will cover the considerations and practical examples of scaling and bringing to production the solutions covered in </span><em class="italic"><span class="koboSpan" id="kobo.3.1">Part 2</span></em><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">We then conclude the book with techniques to go further with Apache Spark and time series analysis. </span><span class="koboSpan" id="kobo.4.3">This guides you to using Databricks and generative AI as part of </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">your solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.6.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.7.1">following chapters:</span></span></p>
<ul>
<li><a href="B18568_08.xhtml#_idTextAnchor151"><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 8</span></em></a><span class="koboSpan" id="kobo.9.1">, </span><em class="italic"><span class="koboSpan" id="kobo.10.1">Going at Scale</span></em></li>
<li><a href="B18568_09.xhtml#_idTextAnchor169"><em class="italic"><span class="koboSpan" id="kobo.11.1">Chapter 9</span></em></a><span class="koboSpan" id="kobo.12.1">, </span><em class="italic"><span class="koboSpan" id="kobo.13.1">Going to Production</span></em></li>
<li><a href="B18568_10.xhtml#_idTextAnchor190"><em class="italic"><span class="koboSpan" id="kobo.14.1">Chapter 10</span></em></a><span class="koboSpan" id="kobo.15.1">, </span><em class="italic"><span class="koboSpan" id="kobo.16.1">Going Further with Apache Spark</span></em></li>
<li><a href="B18568_11.xhtml#_idTextAnchor211"><em class="italic"><span class="koboSpan" id="kobo.17.1">Chapter 11</span></em></a><span class="koboSpan" id="kobo.18.1">, </span><em class="italic"><span class="koboSpan" id="kobo.19.1">Recent Developments in Time Series Analysis</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer128">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer129">
</div>
</div>
</body></html>