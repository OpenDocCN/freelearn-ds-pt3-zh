<html><head></head><body>
		<div>
			<div class="Content" id="_idContainer090">
			</div>
		</div>
		<div class="Content" id="_idContainer091">
			<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/>3. Details of Logistic Regression and Feature Exploration</h1>
		</div>
		<div class="Content" id="_idContainer128">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter teaches you how to evaluate features quickly and efficiently, in order to know which ones will probably be most important for a machine learning model. Once we get a taste for this, we'll explore the inner workings of logistic regression so you can continue your journey to mastery of this fundamental technique. After reading this chapter, you will be able to make a correlation plot of many features and a response variable and interpret logistic regression as a linear model.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor059"/>Introduction</h1>
			<p>In the previous chapter, we developed a few example machine learning models using scikit-learn, to get familiar with how it works. However, the features we used, <strong class="source-inline">EDUCATION</strong> and <strong class="source-inline">LIMIT_BAL</strong>, were not chosen in a systematic way.</p>
			<p>In this chapter, we will start to develop techniques that can be used to assess features for their usefulness in modeling. This will enable you to make a quick pass over all candidate features, to have an idea of which will be the most important. For the most promising features, we will see how to create visual summaries that serve as useful communication tools.</p>
			<p>Next, we will begin our detailed examination of logistic regression. We'll learn why logistic regression is considered to be a linear model, even if the formulation involves some non-linear functions. We'll learn what a decision boundary is and see that as a key consequence of its linearity, the decision boundary of logistic regression could make it difficult to accurately classify the response variable. Along the way, we'll get more familiar with Python, by using list comprehensions and writing functions.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/>Examining the Relationships Between Features and the Response Variable</h1>
			<p>In order to make accurate predictions of the response variable, good features are necessary. We need features that are clearly linked to the response variable in some way. Thus far, we've examined the relationship between a couple of features and the response variable, either by calculating the <strong class="source-inline">groupby</strong>/<strong class="source-inline">mean</strong> of a feature and the response variable, or using individual features in a model and examining performance. However, we have not yet done a systematic exploration of how all the features relate to the response variable. We will do that now and begin to capitalize on all the hard work we put in when we were exploring the features and making sure the data quality was good.</p>
			<p>A popular way of getting a quick look at how all the features relate to the response variable, as well as how the features are related to each other, is by using a <strong class="bold">correlation plot</strong>. We will first create a correlation plot for the case study data, then discuss how to interpret it, along with some mathematical details.</p>
			<p>In order to create a correlation plot, the necessary inputs include all features that we plan to explore, as well as the response variable. Since we are going to use most of the column names from the DataFrame for this, a quick way to get the appropriate list in Python is to start with all the column names and remove those that we don't want. As a preliminary step, we start a new notebook for this chapter and load packages and the cleaned data from <em class="italic">Chapter 1</em>, <em class="italic">Data Exploration and Cleaning</em>, with this code:</p>
			<p class="source-code">import numpy as np #numerical computation</p>
			<p class="source-code">import pandas as pd #data wrangling</p>
			<p class="source-code">import matplotlib.pyplot as plt #plotting package</p>
			<p class="source-code">#Next line helps with rendering plots</p>
			<p class="source-code">%matplotlib inline</p>
			<p class="source-code">import matplotlib as mpl #add'l plotting functionality</p>
			<p class="source-code">import seaborn as sns #a fancy plotting package</p>
			<p class="source-code">mpl.rcParams['figure.dpi'] = 400 #high res figures</p>
			<p class="source-code">df = pd.read_csv('../../Data/Chapter_1_cleaned_data.csv')</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The path to your cleaned data file may be different, depending on where you saved it in <em class="italic">Chapter 1</em>,<em class="italic"> Data Exploration and Cleaning</em>. The code and the outputs presented in this section are also present in the reference notebook: <a href="https://packt.link/pMvWa">https://packt.link/pMvWa</a>.</p>
			<p>Notice that this notebook starts out in a very similar way to the previous chapter's notebook, except we also import the <strong class="bold">Seaborn</strong> package, which has many convenient plotting features that build on <strong class="bold">Matplotlib</strong>. Now let's make a list of all the columns of the DataFrame and look at the first and last five:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer092">
					<img alt="Figure 3.1: Get a list of column names&#13;&#10;" src="image/B16925_03_01.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1: Get a list of column names</p>
			<p>Recall that we are not to use the <strong class="source-inline">gender</strong> variable due to ethical concerns, and we learned that <strong class="source-inline">PAY_2</strong>, <strong class="source-inline">PAY_3</strong>,…, <strong class="source-inline">PAY_6</strong> are incorrect and should be ignored. Also, we are not going to examine the one-hot encoding we created from the <strong class="source-inline">EDUCATION</strong> variable, since the information from those columns is already included in the original feature, at least in some form. We will just use the <strong class="source-inline">EDUCATION</strong> feature directly. Finally, it makes no sense to use <strong class="source-inline">ID</strong> as a feature, since this is simply a unique account identifier and has nothing to do with the response variable. Let's make another list of column names that are neither features nor the response. We want to exclude these from our analysis:</p>
			<p class="source-code">items_to_remove = ['ID', 'SEX',\</p>
			<p class="source-code">                   'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\</p>
			<p class="source-code">                   'EDUCATION_CAT',\</p>
			<p class="source-code">                   'graduate school', 'high school', 'none',\</p>
			<p class="source-code">                   'others', 'university']</p>
			<p>To have a list of column names that consists only of the features and response we will use, we want to remove the names in <strong class="source-inline">items_to_remove</strong> from the current list contained in <strong class="source-inline">features_response</strong>. There are several ways to do this in Python. We will use this opportunity to learn about a particular way of building a list in Python, called a <strong class="bold">list comprehension</strong>. When people talk about certain constructions as being <strong class="bold">Pythonic</strong>, or idiomatic to the Python language, list comprehensions are often one of the things that are mentioned.</p>
			<p>What is a list comprehension? Conceptually, it is basically the same as a <strong class="source-inline">for</strong> loop. However, list comprehensions enable the creation of lists, which may be spread across several lines in an actual <strong class="source-inline">for</strong> loop, to be written in one line. They are also slightly faster than <strong class="source-inline">for</strong> loops, due to optimizations within Python. While this likely won't save us much time here, this is a good chance to become familiar with them. Here is an example list comprehension:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer093">
					<img alt="Figure 3.2: Example of a list comprehension&#13;&#10;" src="image/B16925_03_02.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2: Example of a list comprehension</p>
			<p>That's all there is to it.</p>
			<p>We can also use additional clauses to make the list comprehensions flexible. For example, we can use them to reassign the <strong class="source-inline">features_response</strong> variable with a list containing everything that's not in the list of strings we wish to remove:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer094">
					<img alt="Figure 3.3: Using a list comprehension to prune down the column names&#13;&#10;" src="image/B16925_03_03.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3: Using a list comprehension to prune down the column names</p>
			<p>The use of <strong class="source-inline">if</strong> and <strong class="source-inline">not in</strong> within the list comprehension is fairly self-explanatory. Easy readability in structures such as list comprehensions is one of the reasons for the popularity of Python.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Python documentation (<a href="https://docs.python.org/3/tutorial/datastructures.html">https://docs.python.org/3/tutorial/datastructures.html</a>) defines list comprehensions as the following: </p>
			<p class="callout"><em class="italic">"A list comprehension consists of brackets containing an expression followed by a for clause, then zero or more for or if clauses." </em></p>
			<p class="callout">Thus, list comprehensions can enable you to do things with less code, in a way that is usually pretty readable and understandable.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor061"/>Pearson Correlation</h2>
			<p>Now we are ready to create our correlation plot. Underlying a correlation plot is a <strong class="bold">correlation matrix</strong>, which we must calculate first. pandas makes this easy. We just need to select our columns of features and response values using the list we just created and call the <strong class="source-inline">.corr()</strong> method on these columns. As we calculate this, note that the type of correlation available to us in pandas is <strong class="bold">linear correlation</strong>, also known as <strong class="bold">Pearson correlation</strong>. Pearson correlation is used to measure the strength and direction (that is, positive or negative) of the linear relationship between two variables:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer095">
					<img alt="Figure 3.4: First five rows and columns of the correlation matrix&#13;&#10;" src="image/B16925_03_04.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4: First five rows and columns of the correlation matrix</p>
			<p>After creating the correlation matrix, notice that the row and column names are the same. Then, for each possible comparison between all pairs of features, as well as all features and the response, which we can't yet see here in the first five rows and columns, there is a number. This number is called the <strong class="bold">correlation</strong> between these two columns. All the correlations are between -1 and 1; a column has a correlation of 1 with itself (the diagonal of the correlation matrix), and there is repetition: each comparison appears twice since each column name from the original DataFrame appears as both a row and column in the correlation matrix. Before saying more about correlation, we'll use Seaborn to make a nice plot of it. Here is the plotting code, followed by the output (please see the notebook on GitHub for a color figure if you're reading in black and white; it's necessary here - <a href="https://packt.link/pMvWa">https://packt.link/pMvWa</a>):</p>
			<p class="source-code">sns.heatmap(corr,</p>
			<p class="source-code">            xticklabels=corr.columns.values,</p>
			<p class="source-code">            yticklabels=corr.columns.values,</p>
			<p class="source-code">            center=0)</p>
			<p>You should see the following output:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer096">
					<img alt="Figure 3.5: Heatmap of the correlation plot in Seaborn&#13;&#10;" src="image/B16925_03_05.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5: Heatmap of the correlation plot in Seaborn</p>
			<p>The Seaborn <strong class="source-inline">heatmap</strong> feature makes an obvious visualization of the correlation matrix, according to the color scale on the right of <em class="italic">Figure 3.5</em>, which is called a <strong class="bold">colorbar</strong>. Notice that when calling <strong class="source-inline">sns.heatmap</strong>, in addition to the matrix, we supplied the <strong class="bold">tick labels</strong> for the <em class="italic">x</em> and <em class="italic">y</em> axes, which are the features and response names, and indicated that the center of the colorbar should be 0, so that positive and negative correlation are distinguishable as red and blue, respectively.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you're reading the print version of this book, you can download and browse the color versions of some of the images in this chapter by visiting the following link: <a href="https://packt.link/veMmT">https://packt.link/veMmT</a>.</p>
			<p>What does this plot tell us? At a high level, if two features, or a feature and the response, are <strong class="bold">highly correlated</strong> with each other, you can say there is a strong association between them. Features that are highly correlated to the response will be good features to use for prediction. This high correlation could be positive or negative; we'll explain the difference shortly.</p>
			<p>To see the correlation with the response variable, we look along the bottom row, or equivalently, the last column. Here we see that the <strong class="source-inline">PAY_1</strong> feature is probably the most strongly correlated feature to the response variable. We can also see that a number of features are highly correlated to each other, in particular the <strong class="source-inline">BILL_AMT</strong> features. We will talk in the next chapter about the importance of features that are correlated with each other; this is important to know about for certain models, such as logistic regression, that make assumptions about the correlations between features. For now, we make the observation that <strong class="source-inline">PAY_1</strong> is likely going to be one of the best, most predictive features for our model. The other feature that looks like it may be important is <strong class="source-inline">LIMIT_BAL</strong>, which is negatively correlated. Depending on how astute your vision is, only these two really appear to be any color other than black (meaning 0 correlation) in the bottom row of <em class="italic">Figure 3.5</em>.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor062"/>Mathematics of Linear Correlation</h2>
			<p>What is linear correlation, mathematically speaking? If you've taken basic statistics, you are likely familiar with linear correlation already. Linear correlation works very similarly to linear regression. For two columns, <em class="italic">X</em> and <em class="italic">Y</em>, linear correlation <em class="italic">ρ</em> (the lowercase Greek letter "rho") is defined as the following:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer097">
					<img alt="Figure 3.6: Linear correlation equation&#13;&#10;" src="image/B16925_03_06.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6: Linear correlation equation</p>
			<p>This equation describes the <strong class="bold">expected value</strong> (<em class="italic">E</em>, which you can think of as the average) of the difference between the elements of <em class="italic">X</em> and their average, <em class="italic">µ</em><span class="subscript">x</span>, multiplied by the difference between the corresponding elements of <em class="italic">Y</em> and their average, <em class="italic">µ</em><span class="subscript">y</span>. The average for <em class="italic">E</em> is taken over pairs of <em class="italic">X</em>, <em class="italic">Y</em> values. You can imagine that if, when <em class="italic">X</em> is relatively large compared to its mean, <em class="italic">µ</em><span class="subscript">x</span>, <em class="italic">Y</em> also tends to be similarly large, then the terms of the multiplication in the numerator will both tend to be positive, leading to a positive product and <strong class="bold">positive correlation</strong> after the expected value, <em class="italic">E</em>, is taken. Similarly, if <em class="italic">Y</em> tends to be small when <em class="italic">X</em> is small, both terms in the numerator will be negative and again lead to positive correlation. Conversely, if <em class="italic">Y</em> tends to decrease as <em class="italic">X</em> increases, they will have <strong class="bold">negative correlation</strong>. The denominator (the product of the <strong class="bold">standard deviations</strong> of <em class="italic">X</em> and <em class="italic">Y</em>) serves to normalize linear correlation to the scale of <strong class="source-inline">[-1, 1]</strong>. Because Pearson correlation is adjusted for the mean and standard deviation of the data, the actual values of the data are not as important as the relationship between <em class="italic">X</em> and <em class="italic">Y</em>. <em class="italic">Stronger linear correlations are closer to 1 or -1. If there is no linear relation between X and Y, the correlation will be close to 0.</em></p>
			<p>It's worth noting that, while it is regularly used in this context by data science practitioners, Pearson correlation is not strictly appropriate for a binary response variable, as we have in the case study problem. Technically speaking, among other restrictions, Pearson correlation is only valid for <strong class="bold">continuous data</strong>, such as the data we used for our linear regression exercise in <em class="italic">Chapter 2</em>, <em class="italic">Introduction to Scikit-Learn and Model Evaluation</em>. However, Pearson correlation can still accomplish the purpose of giving a quick idea of the potential usefulness of features. It is also conveniently available in software libraries such as pandas.</p>
			<p>In data science in general, you will find that certain widely used techniques may be applied to data that violate their formal statistical assumptions. It is important to be aware of the formal assumptions underlying analytical methods. In fact, knowledge of these assumptions may be tested during interviews for data science jobs. However, in practice, as long as a technique can help us on our way to understanding the problem and finding an effective solution, it can still be a valuable tool.</p>
			<p>That being said, linear correlation will not be an effective measure of the predictive power of all features. In particular, it only picks up on linear relationships. Shifting our focus momentarily to a hypothetical regression problem, have a look at the following examples and discuss what you expect the linear correlations to be. Notice that the values of the data on the <em class="italic">x</em> and <em class="italic">y</em> axes are not labeled; this is because the location (mean) and standard deviation (scale) of data does not affect the Pearson correlation, only the relationship between the variables, which can be discerned by plotting them together:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer098">
					<img alt="Figure 3.7: Scatter plots of the relationship between example variables&#13;&#10;" src="image/B16925_03_07.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7: Scatter plots of the relationship between example variables</p>
			<p>For <em class="italic">examples A</em> and <em class="italic">B</em>, the actual Pearson correlations of these datasets are 0.96 and -0.97, respectively, according to the formula given previously. From looking at the plots, it's pretty clear that a correlation close to 1 or -1 has provided useful insight into the relationship between these variables. For <em class="italic">example C</em>, the correlation is 0.06. A correlation closer to 0 looks like an effective indication of the lack of an association here: the value of <em class="italic">Y</em> doesn't really seem to have much to do with the value of <em class="italic">X</em>. However, in <em class="italic">example D</em>, there is clearly some relationship between the variables. But the linear correlation is actually lower than the previous example, at 0.02. Here, <em class="italic">X</em> and <em class="italic">Y</em> tend to "move together" over smaller scales, but this is averaged out over all samples when the linear correlation is calculated.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code to generate the plots presented in this and the preceding section can be found here: <a href="https://packt.link/XrUJU">https://packt.link/XrUJU</a>.</p>
			<p>Ultimately, any summary statistic such as correlation that you may choose is only that: a summary. It could hide important details. For this reason, it is usually a good idea to visually examine the relationship between the features and response. This potentially takes up a lot of space on the page, so we won't demonstrate it here for all features in the case study. However, both pandas and Seaborn offer functions to create what's called a <strong class="bold">scatter plot matrix</strong>. A scatter plot matrix is similar to a correlation plot, but it actually shows all the data as a grid of scatter plots of all features and the response variable. This allows you to examine the data directly in a concise format. Since this could potentially be a lot of data and plots, you may need to downsample your data and look at a reduced number of features for the function to run efficiently.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor063"/>F-test</h2>
			<p>While Pearson correlation is theoretically valid for continuous response variables, the binary response variable for the case study data could be considered categorical data, with only two categories: 0 and 1. Among the different kinds of tests we can run, to see whether features are associated with a categorical response, is the <strong class="bold">ANOVA F-test</strong>, available in scikit-learn as <strong class="source-inline">f_classif</strong>. <strong class="bold">ANOVA</strong> stands for <strong class="bold">analysis of variance</strong>. The ANOVA F-test can be contrasted with the <strong class="bold">regression F-test</strong>, which is very similar to Pearson correlation, also available in scikit-learn as <strong class="source-inline">f_regression</strong>.</p>
			<p>We will do an ANOVA F-test using the candidate features for the case study data in the following exercise. You will see that the output consists of F-statistics, as well as <strong class="bold">p-values</strong>. How can we interpret this output? We will focus on the p-value, for reasons that will become clear in the exercise. The p-value is a useful concept across a wide variety of statistical measures. For instance, although we didn't examine them, each of the Pearson correlations calculated for the preceding correlation matrix has a corresponding p-value. There is a similar concept of a p-value corresponding to linear regression coefficients, logistic regression coefficients, and other measures.</p>
			<p>In the context of the F-test, the p-value answers the question: "For the samples in the positive class, how likely is it that the average value of this feature is the same as that of samples in the negative class?" If the data indicated that a feature has very different average values between the positive and negative classes, the following will be the case:</p>
			<ul>
				<li>It will be very unlikely that those average values are the same (low p-value).</li>
				<li>It will probably be a good feature in our model because it will help us discriminate between positive and negative classes.</li>
			</ul>
			<p>Keep these points in mind during the following exercise.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/>Exercise 3.01: F-test and Univariate Feature Selection</h2>
			<p>In this exercise, we'll use the F-test to examine the relationship between the features and response variable. We will use this method to do what is called <strong class="bold">univariate feature selection</strong>: the practice of testing features one by one against the response variable, to see which ones have predictive power. Perform the following steps to complete the exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Jupyter notebook for this exercise can be found here: <a href="https://packt.link/ZDPYf">https://packt.link/ZDPYf</a>. This notebook also contains the prerequisite steps of loading the cleaned data and importing the necessary libraries. These steps should be executed before step 1 of this exercise.</p>
			<ol>
				<li>Our first step in doing the ANOVA F-test is to separate out the features and response as NumPy arrays, taking advantage of the list we created, as well as integer indexing in pandas:<p class="source-code">X = df[features_response].iloc[:,:-1].values</p><p class="source-code">y = df[features_response].iloc[:,-1].values</p><p class="source-code">print(X.shape, y.shape)</p><p>The output should show the shapes of the features and response:</p><p class="source-code">(26664, 17) (26664, )</p><p>There are 17 features, and both the features and response arrays have the same number of samples as expected.</p></li>
				<li>Import the <strong class="source-inline">f_classif</strong> function and feed in the features and response:<p class="source-code">from sklearn.feature_selection import f_classif</p><p class="source-code">[f_stat, f_p_value] = f_classif(X, y)</p><p>There are two outputs from <strong class="source-inline">f_classif</strong>: the <strong class="bold">F-statistic</strong> and the <strong class="bold">p-value</strong>, for the comparison of each feature to the response variable. Let's create a new DataFrame containing the feature names and these outputs, to facilitate our inspection. One way to specify a new DataFrame is by using a <strong class="bold">dictionary</strong>, with <strong class="bold">key:value</strong> pairs of column names and the data to be contained in each column. We show the DataFrame sorted (ascending) on p-value.</p></li>
				<li>Use this code to create a DataFrame of feature names, F-statistics, and p-values, and show it sorted on p-value:<p class="source-code">f_test_df = pd.DataFrame({'Feature':features_response[:-1],</p><p class="source-code">                          'F statistic':f_stat,</p><p class="source-code">                          'p value':f_p_value})</p><p class="source-code">f_test_df.sort_values('p value')</p><p>The output should look like this:</p><div class="IMG---Figure" id="_idContainer099"><img alt="Figure 3.8: Results of the ANOVA F-test&#13;&#10;" src="image/B16925_03_08.jpg"/></div><p class="figure-caption">Figure 3.8: Results of the ANOVA F-test</p><p>Note that for every decrease in p-value, there is an increase in the F-statistic, so the information in these columns is identical in terms of ranking features.</p><p>The conclusions we can draw from the DataFrame of F-statistics and p-values are similar to what we observed in the correlation plot: <strong class="source-inline">PAY_1</strong> and <strong class="source-inline">LIMIT_BAL</strong> appear to be the most useful features. They have the smallest p-values, indicating the average values of these features are <strong class="bold">significantly different</strong> between the positive and negative classes, and these features will help predict which class a sample belongs to.</p><p>In scikit-learn, measures such as the F-test help us perform <strong class="bold">univariate feature selection</strong>. This may be helpful if you have a very large number of features, many of which may be totally useless, and would like a quick way to get a short list of which ones might be most useful. For example, if we wanted to retrieve only the 20% of features with the highest F-statistics, we could do this easily with the <strong class="source-inline">SelectPercentile</strong> class. Also note there is a similar class for the selection of the top "<em class="italic">k</em>" features (where <em class="italic">k</em> is any number you specify), called <strong class="source-inline">SelectKBest</strong>. Here we demonstrate how to select the top 20%.</p></li>
				<li>To select the top 20% of features according to the F-test, first import the <strong class="source-inline">SelectPercentile</strong> class:<p class="source-code">from sklearn.feature_selection import SelectPercentile</p></li>
				<li>Instantiate an object of this class, indicating we'd like to use the same feature selection criteria, ANOVA F-test, that we've already been considering in this exercise, and that we'd like to select the top 20% of features:<p class="source-code">selector = SelectPercentile(f_classif, percentile=20)</p></li>
				<li>Use the <strong class="source-inline">.fit</strong> method to fit the object on our features and response data, similar to how a model would be fit:<p class="source-code">selector.fit(X, y)</p><p>The output should appear like this:</p><p class="source-code">SelectPercentile(percentile=20)</p><p>There are several ways to access the selected features directly, which you may learn about in the scikit-learn documentation (that is, the <strong class="source-inline">.transform</strong> method, or in the same step as fitting with <strong class="source-inline">.fit_transform</strong>). However, these methods will return NumPy arrays, which don't tell you the names of the features that were selected, just the values. For that, you can use the <strong class="source-inline">.get_support</strong> method of the feature selector object, which will give you the column indices of the feature array that were selected. </p></li>
				<li>Capture the indices of the selected features in an array named <strong class="source-inline">best_feature_ix</strong>:<p class="source-code">best_feature_ix = selector.get_support()</p><p class="source-code">best_feature_ix</p><p>The output should appear as follows, indicating a logical index that can be used with an array of feature names, as well as values, assuming they're in the same order as the features array supplied to <strong class="source-inline">SelectPercentile</strong>:</p><p class="source-code">array([ True, False, False, False, True, False, False, False, False,</p><p class="source-code">           False, False, True, True, False, False, False, False])</p></li>
				<li>The feature names can be obtained using all but the last element (the <strong class="source-inline">name</strong> response variable) of our <strong class="source-inline">features_response</strong> list by indexing with <strong class="source-inline">:-1</strong>:<p class="source-code">features = features_response[:-1]</p></li>
				<li>Use the index array we created in <em class="italic">Step 7</em> with a list comprehension and the <strong class="source-inline">features</strong> list, to find the selected feature names, as follows:<p class="source-code">best_features = [features[counter]</p><p class="source-code">                 for counter in range(len(features))</p><p class="source-code">                 if best_feature_ix[counter]]</p><p class="source-code">best_features</p><p>The output should be as follows:</p><p class="source-code">['LIMIT_BAL', 'PAY_1', 'PAY_AMT1', 'PAY_AMT2']</p><p>In this code, the list comprehension has looped through the number of elements in the <strong class="source-inline">features</strong> array (<strong class="source-inline">len(features)</strong>) with the <strong class="source-inline">counter</strong> loop increment, using the <strong class="source-inline">best_feature_ix</strong> Boolean array, representing selected features, in the <strong class="source-inline">if</strong> statement to test whether each feature was selected and capturing the name if so.</p><p>The selected features agree with the top four rows of our DataFrame of F-test results, so the feature selection has worked as expected. While it's not strictly necessary to do things both ways, since they both lead to the same result, it's good to check your work, especially as you are learning new concepts. You should be aware that with convenient methods such as <strong class="source-inline">SelectPercentile</strong>, you don't get visibility of the F-statistics or p-values. However, in some situations, it may be more convenient to use these methods, as the p-values may not necessarily be important, outside of their utility in ranking features.</p></li>
			</ol>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor065"/>Finer Points of the F-test: Equivalence to the t-test for Two Classes and Cautions</h2>
			<p>When we use an F-test to look at the difference in means between just two groups, as we've done here for the binary classification problem of the case study, the test we are performing actually reduces to what's called a <strong class="bold">t-test</strong>. An F-test is extensible to three or more groups and so is useful for multiclass classification. A t-test just compares the means between two groups of samples, to see whether the difference in those means is <strong class="bold">statistically significant</strong>.</p>
			<p>While the F-test served our purposes here of univariate feature selection, there are a few cautions to keep in mind. Going back to the concept of formal statistical assumptions, for the F-test these include that the data is <strong class="bold">normally distributed</strong>. We have not checked this. Also, in comparing the same response variable, <strong class="source-inline">y</strong>, to many potential features from the matrix, <strong class="source-inline">X</strong>, we have performed what is known in statistics as <strong class="bold">multiple comparisons</strong>. In short, this means that by examining multiple features in comparison to the same response over and over, the odds increase that we'll find what we think is a "good feature" just by random chance. However, such features may not generalize to new data. There are statistical <strong class="bold">corrections for multiple comparisons</strong> that amount to adjusting the p-values to account for this.</p>
			<p>Even if we have not followed all the statistical rules that go along with these methods, we can still get useful results from them. The multiple comparisons correction is more of a concern when p-values are the ultimate quantity of interest, for example, when making statistical inferences. Here, p-values are just a means to an end of ranking the feature list. The order of this ranking would not change if the p-values were corrected for multiple comparisons.</p>
			<p>In addition to knowing which features are likely to be useful for modeling, it is good to have a deeper understanding of the important features. Consequently, we will do a detailed graphical exploration of these in the next exercise. We will also look at other methods for feature selection later that don't make the same assumptions as those we've introduced here and are more directly integrated with the predictive models that we will build.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor066"/>Hypotheses and Next Steps</h2>
			<p>According to our univariate feature exploration, the feature with the strongest association with the response variable is <strong class="source-inline">PAY_1</strong>. Does this make sense? What is the interpretation of <strong class="source-inline">PAY_1</strong>? <strong class="source-inline">PAY_1</strong> is the payment status of the account, in the most recent month. As we learned in the initial data exploration, there are some values that indicate that the account was in good standing: -2 means no account usage, -1 means balance paid in full, and 0 means at least the minimum payment was made. On the other hand, positive integer values indicate a delay of payment by that many months. Accounts with delayed payments last month were accounts that could be considered in default. This means that, essentially, this feature captures historical values of the response variable. Features such as this are extremely important as <em class="italic">one of the best predictors for just about any machine learning problem is historical data on the same thing you are trying to predict (that is, the response variable)</em>. This should make sense: people who have defaulted before are probably at the highest risk of defaulting again.</p>
			<p>How about <strong class="source-inline">LIMIT_BAL</strong>, the credit limit of accounts? Thinking about how credit limits are assigned, it is likely that our client has assessed how risky a borrower is when deciding their credit limit. Riskier clients should be given lower limits, so the creditor is less exposed. Therefore, we may expect to see a higher probability of default for accounts with lower values for <strong class="source-inline">LIMIT_BAL</strong>.</p>
			<p>What have we learned from our univariate feature selection exercise? We have an idea of what the most important features in our model are likely to be. And, from the correlation matrix, we have some idea of how they are related to the response variable. However, knowing the limitations of the tests we used, it is a good idea to visualize these features for a closer look at the relationship between the features and response variable. We have also started to develop <strong class="bold">hypotheses</strong> about these features: why do we think they are important? Now, by visualizing the relationships between the features and the response variable, we can determine whether our ideas are compatible with what we can see in the data.</p>
			<p>Such hypotheses and visualizations are often a key part of presenting your results to a client, who may be interested in how a model works, not just the fact that it does work.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor067"/>Exercise 3.02: Visualizing the Relationship Between the Features and Response Variable</h2>
			<p>In this exercise, you will further your knowledge of plotting functions from Matplotlib that you used earlier in this book. You'll learn how to customize graphics to better answer specific questions with the data. As you pursue these analyses, you will create insightful visualizations of how the <strong class="source-inline">PAY_1</strong> and <strong class="source-inline">LIMIT_BAL</strong> features relate to the response variable, which may possibly provide support for the hypotheses you formed about these features. This will be done by becoming more familiar with the Matplotlib <strong class="bold">Application Programming Interface</strong> (<strong class="bold">API</strong>), in other words, the syntax you use to interact with Matplotlib. Perform the following steps to complete the exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before beginning step 1 of this exercise, make sure that you have imported the necessary libraries and have loaded the correct dataframe. You can refer to the following notebook for the prerequisite steps along with the code for this exercise: <a href="https://packt.link/DOrZ9">https://packt.link/DOrZ9</a>.</p>
			<ol>
				<li value="1">Calculate a baseline for the response variable of the default rate across the whole dataset using pandas' <strong class="source-inline">.mean()</strong>:<p class="source-code">overall_default_rate = df['default payment next month'].mean()</p><p class="source-code">overall_default_rate</p><p>The output of this should be the following:</p><p class="source-code">0.2217971797179718</p><p>What would be a good way to visualize default rates for different values of the <strong class="source-inline">PAY_1</strong> feature? </p><p>Recall our observation that this feature is sort of like a hybrid categorical and numerical feature. We'll choose to plot it in a way that is typical for categorical features, due to the relatively small number of unique values. In <em class="italic">Chapter 1</em>, <em class="italic">Data Exploration and Cleaning</em>, we did <strong class="source-inline">value_counts</strong> of this feature as part of data exploration, then later we learned about <strong class="source-inline">groupby</strong>/<strong class="source-inline">mean</strong> when looking at the <strong class="source-inline">EDUCATION</strong> feature. <strong class="source-inline">groupby</strong>/<strong class="source-inline">mean</strong> would be a good way to visualize the default rate again here, for different payment statuses.</p></li>
				<li>Use this code to create a <strong class="source-inline">groupby</strong>/<strong class="source-inline">mean</strong> aggregation:<p class="source-code">group_by_pay_mean_y = df.groupby('PAY_1').agg(</p><p>{'default payment next month':np.mean})</p><p class="source-code">group_by_pay_mean_y </p><p>The output should look as follows:</p><div class="IMG---Figure" id="_idContainer100"><img alt="Figure 3.9: Mean of the response variable by groups of the PAY_1 feature&#13;&#10;" src="image/B16925_03_09.jpg"/></div><p class="figure-caption">Figure 3.9: Mean of the response variable by groups of the PAY_1 feature</p><p>Looking at these values, you may already be able to discern the trend. Let's go straight to plotting them. We'll take it step by step and introduce some new concepts. You should put all the code from <em class="italic">Steps 3</em> through <em class="italic">6</em> in a single code cell.</p><p>In Matplotlib, every plot exists on an axes, and within a <strong class="source-inline">figure</strong> window. By creating objects for <strong class="source-inline">axes</strong> and <strong class="source-inline">figure</strong>, you can directly access and change their properties, including axis labels and other kinds of annotation on the axes. </p></li>
				<li>Create an <strong class="source-inline">axes</strong> object in a variable also called <strong class="source-inline">axes</strong>, using the following code: <p class="source-code">axes = plt.axes()</p></li>
				<li>Plot the overall default rate as a red horizontal line. <p>Matplotlib makes this easy; you just have to indicate the <em class="italic">y</em> intercept of this line with the <strong class="source-inline">axhline</strong> function. Notice that instead of calling this function from <strong class="source-inline">plt</strong>, now we are calling it as a method on our <strong class="source-inline">axes</strong> object:</p><p class="source-code">axes.axhline(overall_default_rate, color='red')</p><p>Now, over this line, we want to plot the default rate within each group of <strong class="source-inline">PAY_1</strong> values. </p></li>
				<li>Use the <strong class="source-inline">plot</strong> method of the DataFrame of grouped data we created. Specify to include an <strong class="source-inline">'x'</strong> marker along the line plot, to not have a <strong class="source-inline">legend</strong> instance, which we'll create later, and that the <strong class="bold">parent axes</strong> of this plot should be the axes we are already working with (otherwise, pandas would erase what was already there and create new axes):<p class="source-code">group_by_pay_mean_y.plot(marker='x', legend=False, ax=axes)</p><p>This is all the data we want to plot. </p></li>
				<li>Set the <em class="italic">y</em>-axis label and create a <strong class="source-inline">legend</strong> instance (there are many possible options for controlling the legend appearance, but a simple way is to provide a list of strings, indicating the labels for the graphical elements in the order they were added to the axes):<p class="source-code">axes.set_ylabel('Proportion of credit defaults')</p><p class="source-code">axes.legend(['Entire dataset', 'Groups of PAY_1'])</p></li>
				<li>Executing all the code from <em class="italic">Steps 3</em> through <em class="italic">6</em> in a single code cell should result in the following plot:<div class="IMG---Figure" id="_idContainer101"><img alt="Figure 3.10: Credit default rates across the dataset&#13;&#10;" src="image/B16925_03_10.jpg"/></div><p class="figure-caption">Figure 3.10: Credit default rates across the dataset</p><p>Our visualization of payment statuses has revealed a clear, and probably expected, story: those who defaulted before are in fact more likely to default again. The default rate of accounts in good standing is well below the overall default rate, which we know from before is about 22%. However, over 30% of the accounts that were in default last month will be in default again next month, according to this. This is a good visual to share with our business partner as it shows the effect of what may be one of the most important features in our model.</p><p>Now we turn our attention to the feature ranked as having the second strongest association with the target variable: <strong class="source-inline">LIMIT_BAL</strong>. This is a numerical feature with many unique values. A good way to visualize features such as this, for a classification problem, is to plot multiple histograms on the same axis, with different colors for the different classes. As a way to separate the classes, we can index them from the DataFrame using logical arrays.</p></li>
				<li>Use this code to create logical masks for positive and negative samples:<p class="source-code">pos_mask = y == 1</p><p class="source-code">neg_mask = y == 0</p><p>To create our dual histogram plot, we'll make another <strong class="source-inline">axes</strong> object, then call the <strong class="source-inline">.hist</strong> method on it twice for the positive and negative class histograms. We supply a few additional keyword arguments: the first histogram will have black edges and white bars, while the second will use <strong class="source-inline">alpha</strong> to create transparency, so we can see both histograms in the places they overlap. Once we have the histograms, we rotate the <em class="italic">x</em>-axis tick labels to make them more legible and create several other annotations that should be self-explanatory. </p></li>
				<li>Use the following code to create the dual histogram plot with the aforementioned properties:<p class="source-code">axes = plt.axes()</p><p class="source-code">axes.hist(df.loc[neg_mask, 'LIMIT_BAL'],\</p><p class="source-code">          edgecolor='black', color='white')</p><p class="source-code">axes.hist(df.loc[pos_mask, 'LIMIT_BAL'],\</p><p class="source-code">          alpha=0.5, edgecolor=None, color='black')</p><p class="source-code">axes.tick_params(axis='x', labelrotation=45)</p><p class="source-code">axes.set_xlabel('Credit limit (NT$)')</p><p class="source-code">axes.set_ylabel('Number of accounts')</p><p class="source-code">axes.legend(['Not defaulted', 'Defaulted'])</p><p class="source-code">axes.set_title('Credit limits by response variable')</p><p>The plot should appear like this:	</p><div class="IMG---Figure" id="_idContainer102"><img alt="Figure 3.11: Dual histograms of credit limits&#13;&#10;" src="image/B16925_03_11.jpg"/></div><p class="figure-caption">Figure 3.11: Dual histograms of credit limits</p><p>While this plot has accomplished all the formatting we wished to present, it's not quite as interpretable as it could be. What we hope to gain from looking at it is some knowledge of how the credit limit may be a good way to distinguish between accounts that default and those that do not. However, the primary visual takeaway here is that the transparent histogram is bigger than the gray one. This is due to the fact that fewer accounts default than don't default. We already know this from examining the class fractions.</p><p>It would be more informative to show something about how the shapes of these histograms are different, not just their sizes. To emphasize this, we can make the total plotted area of the two histograms the same, by <strong class="bold">normalizing</strong> them. Matplotlib provides a keyword argument that makes this easy, creating what might be considered an empirical version of a <strong class="bold">probability mass function</strong>. This means that the integral or area contained within each histogram will be equal to 1 after normalization, since probabilities sum to 1.</p><p>After some experimentation, we decide to make a histogram with 16 bins. Since the maximum credit limit is NT$800,000, we use <strong class="source-inline">range</strong> with an increment of NT$50,000. Here is the code that you can use:</p><p class="source-code">df['LIMIT_BAL'].max()</p></li>
				<li>Create and display the histogram bin edges with this code:<p class="source-code">bin_edges = list(range(0,850000,50000))</p><p class="source-code">print(bin_edges)</p><p>The output should be as follows:</p><p class="source-code">[0, 50000, 100000, 150000, 200000, 250000, 300000, 350000, 40000, 450000,</p><p class="source-code">500000, 550000, 600000, 650000, 700000, 750000, 800000]</p><p>The plotting code for the normalized histograms is similar to before, with a few key changes: the use of the <strong class="source-inline">bins</strong> keyword to define bin edge locations, <strong class="source-inline">density=True</strong> to normalize the histograms, and changes to the plot annotations. The most complex part is that we need to adjust the <strong class="bold">y-axis tick labels</strong>, so that the heights of the histogram bins have the interpretation of proportions, which is more intuitive than the default output.</p><p><em class="italic">Y</em>-axis tick labels are the text labels displayed next to the ticks on the <em class="italic">y</em> axis and are usually simply the values of the ticks at those locations. However, you are able to manually change this if you want. </p><p class="callout-heading">Note</p><p class="callout">According to the Matplotlib documentation, for a normalized histogram, the bin heights are calculated by <em class="italic">"dividing the count by the number of observations times the bin width"</em> (<a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html">https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html</a>). So, we need to multiply the <em class="italic">y</em>-axis tick labels by the bin width of NT$50,000, for the bin heights to represent the proportion of the total number of samples in each bin. Notice the two lines where we get the tick locations of the <em class="italic">y</em>-axis, then set the labels to a modified version. The rounding to two decimal places with <strong class="source-inline">np.round</strong> is needed due to slight errors of floating-point arithmetic.</p></li>
				<li>Run this code to produce normalized histograms:<p class="source-code">mpl.rcParams['figure.dpi'] = 400 </p><p class="source-code">axes = plt.axes()</p><p class="source-code">axes.hist(</p><p class="source-code">    df.loc[neg_mask, 'LIMIT_BAL'],</p><p class="source-code">    bins=bin_edges, density=True,</p><p class="source-code">    edgecolor='black', color='white')</p><p class="source-code">axes.hist(</p><p class="source-code">    df.loc[pos_mask, 'LIMIT_BAL'],</p><p class="source-code">    bins=bin_edges, density=True, alpha=0.5,</p><p class="source-code">    edgecolor=None, color='black')</p><p class="source-code">axes.tick_params(axis='x', labelrotation=45)</p><p class="source-code">axes.set_xlabel('Credit limit (NT$)')</p><p class="source-code">axes.set_ylabel('Proportion of accounts')</p><p class="source-code">y_ticks = axes.get_yticks()</p><p class="source-code">axes.set_yticklabels(np.round(y_ticks*50000,2))</p><p class="source-code">axes.legend(['Not defaulted', 'Defaulted'])</p><p class="source-code">axes.set_title('Normalized distributions of '\</p><p class="source-code">               'credit limits by response variable')</p><p>The plot should look like this:</p><div class="IMG---Figure" id="_idContainer103"><img alt="Figure 3.12: Normalized dual histograms&#13;&#10;" src="image/B16925_03_12.jpg"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.12: Normalized dual histograms</p>
			<p>You can see that plots in Matplotlib are highly customizable. In order to view all the different things you can get from and set on Matplotlib axes, have a look here: <a href="https://matplotlib.org/stable/api/axes_api.html">https://matplotlib.org/stable/api/axes_api.html</a>.</p>
			<p>What can we learn from this plot? It looks like the accounts that default tend to have a higher proportion of lower credit limits. Accounts with credit limits less than NT$150,000 are relatively more likely to default, while the opposite is true for accounts with limits higher than this. We should ask ourselves, does this make sense? Our hypothesis was that the client would give riskier accounts lower limits. This intuition is compatible with the higher proportions of defaulters with lower credit limits that we observed here.</p>
			<p>Depending on how the model building goes, if the features we examined in this exercise turn out to be important for predictive modeling as we expect, it would be good to show these graphs to our client, as part of a presentation of our work. This would give the client insight into how the model works, as well as insights into their data.</p>
			<p>A key learning from this section is that effective visual presentations take substantial time to produce. It is good to budget some time in your project workflow for this. Convincing visuals are worth the effort since they should be able to quickly and effectively communicate important findings to the client. They are usually a better choice than adding lots of text to the materials that you create. Visual communication of quantitative concepts is a core data science skill.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>Univariate Feature Selection: What it Does and Doesn't Do</h1>
			<p>In this chapter, we have learned techniques for going through features one by one to see whether they have predictive power. This is a good first step, and if you already have features that are very predictive of the outcome variable, you may not need to spend much more time considering features before modeling. However, there are drawbacks to univariate feature selection. In particular, it does not consider the <strong class="bold">interactions</strong> between features. For example, what if the credit default rate is very high specifically for people with both a certain education level and a certain range of credit limit?</p>
			<p>Also, with the methods we used here, only the linear effects of features are captured. If a feature is more predictive when it's undergone some type of <strong class="bold">transformation</strong>, such as a <strong class="bold">polynomial</strong> or <strong class="bold">logarithmic</strong> transformation, or <strong class="bold">binning</strong> (<strong class="bold">discretization</strong>), linear techniques of univariate feature selection may not be effective. Interactions and transformations are examples of <strong class="bold">feature engineering</strong>, or creating new features, in these cases from existing features. The shortcomings of linear feature selection methods can be remedied by non-linear modeling techniques including decision trees and methods based on them, which we will examine later. But there is still value in looking for simple relationships that can be found by linear methods for univariate feature selection, and it is quick to do.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor069"/>Understanding Logistic Regression and the Sigmoid Function Using Function Syntax in Python</h2>
			<p>In this section, we will open the "black box" of logistic regression all the way: we will gain a comprehensive understanding of how it works. We'll start off by introducing a new programming concept: <strong class="bold">functions</strong>. At the same time, we'll learn about a mathematical function, the sigmoid function, which plays a key role in logistic regression.</p>
			<p>In the most basic sense, a function in computer programming is a piece of code that takes inputs and produces outputs. You have been using functions throughout the book: functions that were written by someone else. Any time that you use syntax such as this: <strong class="source-inline">output = do_something_to(input)</strong>, you have used a function. For example, NumPy has a function you can use to calculate the mean of the input:</p>
			<p class="source-code">np.mean([1, 2, 3, 4, 5])</p>
			<p class="source-code">3.0</p>
			<p>Functions <strong class="bold">abstract</strong> away the operations being performed so that, in our example, you don't need to see all the lines of code that it takes to calculate a mean, every time you need to do this. For many common mathematical functions, there are already pre-defined versions available in packages such as NumPy. You do not need to "reinvent the wheel." The implementations in popular packages are likely popular for a reason: people have spent time thinking about how to create them in the most efficient way. So, it would be wise to use them. However, since all the packages we are using are <strong class="bold">open source</strong>, if you are interested in seeing how the functions in the libraries we use are implemented, you are able to look at the code within any of them.</p>
			<p>Now, for the sake of illustration, let's learn Python function syntax by writing our own function for the arithmetic mean. Function syntax in Python is similar to <strong class="source-inline">for</strong> or <strong class="source-inline">if</strong> blocks, in that the body of a function is indented and the declaration of the function is followed by a colon. Here is the code for a function to compute the mean:</p>
			<p class="source-code">def my_mean(input_argument):</p>
			<p class="source-code">    output = sum(input_argument)/len(input_argument)</p>
			<p class="source-code">    return(output)</p>
			<p>After you execute the code cell with this definition, the function is available to you in other code cells in the notebook. Take the following example:</p>
			<p class="source-code">my_mean([1, 2, 3, 4, 5])</p>
			<p class="source-code">3.0</p>
			<p>The first part of defining a function, as shown here, is to start a line of code with <strong class="source-inline">def</strong>, followed by a space, followed by the name you'd like to call the function. After this come parentheses, inside which the names of the <strong class="bold">parameters</strong> of the function are specified. Parameters are names of the input variables, where these names are internal to the body of the function: the variable names defined as parameters are available within the function when it is <strong class="bold">called</strong> (used), but not outside the function. There can be more than one parameter; they would be comma-separated. After the parentheses comes a colon.</p>
			<p>The body of the function is indented and can contain any code that operates on the inputs. Once these operations are done, the last line should start with <strong class="source-inline">return</strong> and contain the output variable(s), comma-separated if there is more than one. We are leaving out many fine points in this very simple introduction to functions, but those are the essential parts you need to get started.</p>
			<p>The power of a function comes when you use it. Notice how after we define the function, in a separate code block we can <strong class="bold">call</strong> it by the name we've given it, and it operates on whatever inputs we <strong class="bold">pass</strong> it. It's as if we've copied and pasted all the code to this new location. But it looks much nicer than actually doing that. And if you are going to use the same code many times, a function can greatly reduce the overall length of your code.</p>
			<p>As a brief additional note, you can optionally specify the inputs using the parameter names explicitly, which can be clearer when there are many inputs:</p>
			<p class="source-code">my_mean(input_argument=[1, 2, 3])</p>
			<p class="source-code">2.0</p>
			<p>Now that we're familiar with the basics of Python functions, we are going to consider a mathematical function that's important to logistic regression, called <strong class="bold">sigmoid</strong>. This function may also be called the <strong class="bold">logistic function</strong>. The definition of sigmoid is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer104">
					<img alt="Figure 3.13: The sigmoid function&#13;&#10;" src="image/B16925_03_13.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13: The sigmoid function</p>
			<p>We will break down the different parts of this function. As you can see, the sigmoid function involves the <strong class="bold">irrational number e</strong>, which is also known as the base of the <strong class="bold">natural logarithm</strong>, in contrast to the base-10 logarithms we used earlier for data exploration. In order to compute <em class="italic">e</em><span class="superscript">-X</span> using Python, we don't actually need to perform the exponentiation manually. NumPy has a convenient function, <strong class="source-inline">exp</strong>, that takes <strong class="source-inline">e</strong> to the input exponent automatically. If you look at the documentation, you will see this process is called taking the "exponential," which sounds vague. But it is assumed to be understood that the base of the exponent is <em class="italic">e</em> in this case. In general, if you want to take an exponent in Python, such as 2<span class="superscript">3</span> ("two to the third power"), the syntax is two asterisks: <strong class="source-inline">2**3</strong>, which equals 8, for example.</p>
			<p>Consider how inputs may be passed to the <strong class="source-inline">np.exp</strong> function. Since NumPy's implementation is <strong class="bold">vectorized</strong>, this function can take individual numbers as well as arrays or matrices as input. To illustrate individual arguments, we compute the exponential of 1, which shows the approximate value of <em class="italic">e</em>, as well as <em class="italic">e0</em>, which of course equals 1, as does the zeroth power of any base:</p>
			<p class="source-code">np.exp(1)</p>
			<p class="source-code">2.718281828459045</p>
			<p class="source-code">np.exp(0)</p>
			<p class="source-code">1.0</p>
			<p>To illustrate the vectorized implementation of <strong class="source-inline">np.exp</strong>, we create an array of numbers using NumPy's <strong class="source-inline">linspace</strong> function. This function takes as input the starting and stopping points of a range, both inclusive, and the number of values you'd like within that range, to create an array of that many linearly spaced values. This function performs a somewhat similar role to Python's <strong class="source-inline">range</strong>, but can also produce decimal values:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer105">
					<img alt="Figure 3.14: Using np.linspace to make an array&#13;&#10;" src="image/B16925_03_14.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14: Using np.linspace to make an array</p>
			<p>Since <strong class="source-inline">np.exp</strong> is vectorized, it will compute the exponential of the whole array at once, in an efficient manner. Here is the code with output, to calculate the exponential of our <strong class="source-inline">X_exp</strong> array and examine the first five values:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer106">
					<img alt="Figure 3.15: NumPy's exp function&#13;&#10;" src="image/B16925_03_15.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15: NumPy's exp function</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor070"/>Exercise 3.03: Plotting the Sigmoid Function</h2>
			<p>In this exercise, we will use <strong class="source-inline">X_exp</strong> and <strong class="source-inline">Y_exp</strong>, created previously, to make a plot of what the exponential function looks like over the interval <strong class="source-inline">[-4, 4]</strong>. You need to have run all the code in <em class="italic">Figures 3.14</em> and <em class="italic">3.15</em> to have these variables available for this exercise. Then we will define a function for the sigmoid, create a plot of that, and consider how it is related to the exponential function. Perform the following steps to complete the exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before beginning step 1 of this exercise, make sure that you have imported the necessary libraries. The code for importing the libraries along with that for rest of the steps in the exercise can be found here: <a href="https://packt.link/Uq012">https://packt.link/Uq012</a>.</p>
			<ol>
				<li value="1">Use this code to plot the exponential function:<p class="source-code">plt.plot(X_exp, Y_exp)</p><p class="source-code">plt.title('Plot of $e^X$')</p><p>The plot should look like this:</p><div class="IMG---Figure" id="_idContainer107"><img alt="Figure 3.14: Using np.linspace to make an array&#13;&#10;" src="image/B16925_03_16.jpg"/></div><p class="figure-caption">Figure 3.16: Plotting the exponential function</p><p>Notice that in titling the plot, we've taken advantage of a kind of syntax called <strong class="bold">LaTeX</strong>, which enables the formatting of mathematical notation. We won't go into the details of LaTeX here, but suffice to say that it is very flexible. Note that enclosing part of the title string in dollar signs causes it to be rendered using LaTeX, and that superscript can be created using <strong class="source-inline">^</strong>.</p><p>Also note in <em class="italic">Figure 3.16</em> that many points spaced close together create the appearance of a smooth curve, but in fact, it is a graph of discrete points connected by line segments.</p><p><strong class="bold">What can we observe about the exponential function?</strong></p><p>It is never negative: as <em class="italic">X</em> approaches negative infinity, <em class="italic">Y</em> approaches 0.</p><p>As <em class="italic">X</em> increases, <em class="italic">Y</em> increases slowly at first, but very quickly "blows up." This is what is meant when people say "exponential growth" to signify a rapid increase.</p><p><strong class="bold">How can you think about the sigmoid in terms of the exponential? </strong></p><p>First, the sigmoid involves <em class="italic">e</em><span class="superscript">-X</span>, as opposed to <em class="italic">e</em><span class="superscript">X</span>. The graph of <em class="italic">e</em><span class="superscript">-X</span> is just the reflection of <em class="italic">e</em><span class="superscript">X</span> about the <em class="italic">y</em> axis. This can be plotted easily and annotated using curly braces for multiple-character superscript in the plot title.</p></li>
				<li>Run this code to see the plot of <em class="italic">e</em><span class="superscript">-X</span>:<p class="source-code">Y_exp = np.exp(-X_exp)</p><p class="source-code">plt.plot(X_exp, Y_exp)</p><p class="source-code">plt.title('Plot of $e^{-X}$')</p><p>The output should appear like this:</p><div class="IMG---Figure" id="_idContainer108"><img alt="Figure 3.17: Plot of exp(-X)&#13;&#10;" src="image/B16925_03_17.jpg"/></div><p class="figure-caption">Figure 3.17: Plot of exp(-X)</p><p>Now, in the sigmoid function, <em class="italic">e</em><span class="superscript">-X</span> is in the denominator, with 1 added to it. The numerator is 1. So, what happens to the sigmoid as <em class="italic">X</em> approaches negative infinity? We know that <em class="italic">e</em><span class="superscript">-X</span> "blows up," becoming very large. Overall, the denominator becomes very large and the fraction approaches 0. What about when <em class="italic">X</em> increases toward positive infinity? We can see that <em class="italic">e</em><span class="superscript">-X</span> becomes very close to 0. So, in this case, the sigmoid function would be approximately <em class="italic">1/1 = 1</em>. This should give you an intuition that the sigmoid function stays between 0 and 1. Let's now implement a sigmoid function in Python and use it to create a plot to see how reality matches this intuition.</p></li>
				<li>Define a sigmoid function like this:<p class="source-code">def sigmoid(X):</p><p class="source-code">    Y = 1 / (1 + np.exp(-X))</p><p class="source-code">    return Y</p></li>
				<li>Make a larger range of <em class="italic">x</em> values to plot over and plot the sigmoid. Use this code:<p class="source-code">X_sig = np.linspace(-7,7,141)</p><p class="source-code">Y_sig = sigmoid(X_sig)</p><p class="source-code">plt.plot(X_sig,Y_sig)</p><p class="source-code">plt.yticks(np.linspace(0,1,11))</p><p class="source-code">plt.grid()</p><p class="source-code">plt.title('The sigmoid function')</p><p>The plot should look like this:</p><div class="IMG---Figure" id="_idContainer109"><img alt="Figure 3.18: A sigmoid function plot&#13;&#10;" src="image/B16925_03_18.jpg"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.18: A sigmoid function plot</p>
			<p>This plot matches what we expected. Further, we can see that <strong class="source-inline">sigmoid(0) = 0.5</strong>. What is special about the sigmoid function? The output of this function is strictly bounded between 0 and 1. This is a good property for a function that should predict probabilities, which are also required to be between 0 and 1. Technically, probabilities can be exactly equal to 0 and 1, while the sigmoid never is. But the sigmoid can be close enough that this is not a practical limitation.</p>
			<p>Recall that we described logistic regression as producing <strong class="bold">predicted probabilities</strong> of class membership, as opposed to directly predicting class membership. This enables a more flexible implementation of logistic regression, allowing the selection of the threshold probability. The sigmoid function is the source of these predicted probabilities. Shortly, we will see how the different features are used in the calculation of the predicted probabilities.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor071"/>Scope of Functions</h2>
			<p>As you begin to use functions, you should develop an awareness of the concept of <strong class="bold">scope</strong>. Notice that when we wrote the <strong class="source-inline">sigmoid</strong> function, we created a variable, <strong class="source-inline">Y</strong>, inside the function. Variables created inside functions are different from those created outside functions. They are effectively created and destroyed within the function itself when it is called. These variables are said to be <strong class="bold">local</strong> in scope: local to the function. If you have been running all the code as written in this chapter in a single notebook in sequence, notice that you are not able to access the <strong class="source-inline">Y</strong> variable after using the <strong class="source-inline">sigmoid</strong> function:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer110">
					<img alt="Figure 3.19: The Y variable not in the scope of the notebook&#13;&#10;" src="image/B16925_03_19.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.19: The Y variable not in the scope of the notebook</p>
			<p>The <strong class="source-inline">Y</strong> variable is not in the <strong class="bold">global</strong> scope of the notebook. However, global variables created outside of functions are available within the local scope of functions, even if they are not inputted as parameters to the function. Here we demonstrate creating a variable outside of a function, which is global in scope, and then accessing it within a function. The function actually doesn't take any parameters at all, but as you can see, it can work with the value of the global variable to create an output:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer111">
					<img alt="Figure 3.20: Global variable available within the local scope of the function&#13;&#10;" src="image/B16925_03_20.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.20: Global variable available within the local scope of the function</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="bold">More details on scope</strong></p>
			<p class="callout">The scope of variables can potentially be confusing but is good to know when you start making more advanced use of functions. While this knowledge isn't required for the book, you may wish to get a more in-depth perspective on variable scope in Python here: <a href="https://nbviewer.jupyter.org/github/rasbt/python_reference/blob/master/tutorials/scope_resolution_legb_rule.ipynb">https://nbviewer.jupyter.org/github/rasbt/python_reference/blob/master/tutorials/scope_resolution_legb_rule.ipynb</a>.</p>
			<p class="callout"><strong class="bold">Sigmoid curves in scientific applications</strong></p>
			<p class="callout">Besides being fundamental to logistic regression, sigmoid curves are used in a variety of applications. In biology, they can be used to describe the growth of an organism, which starts slowly, then has a rapid phase, followed by a smooth tapering off as the final size is reached. Sigmoids can also be used to describe population growth, which has a similar trajectory, increasing rapidly but then slowing as the carrying capacity of the environment is reached.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor072"/>Why Is Logistic Regression Considered a Linear Model?</h2>
			<p>We mentioned previously that logistic regression is considered a <strong class="bold">linear model</strong>, while we were exploring whether the relationship between features and response resembled a linear relationship. Recall that we plotted <strong class="source-inline">groupby</strong>/<strong class="source-inline">mean</strong> of the <strong class="source-inline">EDUCATION</strong> feature in <em class="italic">Chapter 1</em>, <em class="italic">Data Exploration and Cleaning</em>, as well as for the <strong class="source-inline">PAY_1</strong> feature in this chapter, to see whether the default rates across values of these features exhibited a linear trend. While this is a good way to get a quick approximation of how "linear or not" these features may be, here we formalize the notion of why logistic regression is a linear model.</p>
			<p>A model is considered linear if the transformation of features that is used to calculate the prediction is a <strong class="bold">linear combination</strong> of the features. The possibilities for a linear combination are that each feature can be multiplied by a numerical constant, these terms can be added together, and an additional constant can be added. For example, in a simple model with two features, <em class="italic">X</em><span class="subscript">1</span> and <em class="italic">X</em><span class="subscript">2</span>, a linear combination would take the following form:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer112">
					<img alt="Figure 3.21: Linear combination of X1 and X2&#13;&#10;" src="image/B16925_03_21.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.21: Linear combination of X<span class="subscript">1</span> and X<span class="subscript">2</span></p>
			<p>The constants <em class="italic">𝜃</em><span class="subscript">i</span> can be any number, positive, negative, or zero, for <em class="italic">i = 0, 1, and 2</em> (although if a coefficient is 0, this removes a feature from the linear combination). A familiar example of a linear transformation of one variable is a straight line with the equation <em class="italic">y = mx + b</em>, as discussed <em class="italic">Chapter 2</em>, <em class="italic">Introduction to Scikit-Learn and Model Evaluation</em>. In this case, <em class="italic">𝜃</em><span class="subscript">o</span><em class="italic"> = b</em> and <em class="italic">𝜃</em><span class="subscript">1</span><em class="italic"> = m</em>. <em class="italic">𝜃</em><span class="subscript">o</span> is called the <strong class="bold">intercept</strong> of a linear combination, which should be familiar from algebra.</p>
			<p>What kinds of things are "not allowed" in linear transformations? Any other mathematical expressions besides what was just described, such as the following:</p>
			<ul>
				<li>Multiplying a feature by itself; for example, <em class="italic">X</em><span class="subscript">1</span><span class="superscript">2</span> or <em class="italic">X</em><span class="subscript">1</span><span class="superscript">3</span>. These are called polynomial terms.</li>
				<li>Multiplying features together; for example, <em class="italic">X</em><span class="subscript">1</span><em class="italic">X</em><span class="subscript">2</span>. These are called interactions.</li>
				<li>Applying non-linear transformations to features; for example, log and square root.</li>
				<li>Other complex mathematical functions.</li>
				<li>"If then" types of statements. For example, "if <em class="italic">X</em><span class="subscript">1</span><em class="italic"> &gt; a</em>, then <em class="italic">y = b</em>."</li>
			</ul>
			<p>However, while these transformations are not part of the basic formulation of a linear combination, they could be added to a linear model by <strong class="bold">engineering features</strong>, for example, defining a new feature, <em class="italic">X</em><span class="superscript">3</span> = <em class="italic">X</em><span class="subscript">1</span><span class="superscript">2</span>.</p>
			<p>Earlier, we learned that the predictions of logistic regression, which take the form of probabilities, are made using the sigmoid function. Taking another look here, we see that this function is clearly non-linear:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer113">
					<img alt="Figure 3.22: Non-linear sigmoid function&#13;&#10;" src="image/B16925_03_22.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.22: Non-linear sigmoid function</p>
			<p>Why, then, is logistic regression considered a linear model? It turns out that the answer to this question lies in a different formulation of the sigmoid equation, called the <strong class="source-inline">logit</strong> function. We can derive the <strong class="source-inline">logit</strong> function by solving the sigmoid function for <em class="italic">X</em>; in other words, finding the inverse of the sigmoid function. First, we set the sigmoid equal to <em class="italic">p</em>, which we interpret as the probability of observing the positive class, then solve for <em class="italic">X</em> as shown in the following:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer114">
					<img alt="Figure 3.23: Solving for X&#13;&#10;" src="image/B16925_03_23.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.23: Solving for X</p>
			<p>Here, we've used some laws of exponents and logs to solve for <em class="italic">X</em>. You may also see <strong class="source-inline">logit</strong> expressed as follows: </p>
			<div>
				<div class="IMG---Figure" id="_idContainer115">
					<img alt="Figure 3.24: The logit function&#13;&#10;" src="image/B16925_03_24.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.24: The logit function</p>
			<p>In this expression, the <strong class="bold">probability of failure</strong>, <em class="italic">q</em>, is expressed in terms of the <strong class="bold">probability of success</strong>, <em class="italic">p</em>; <em class="italic">q = 1 - p</em>, because probabilities sum to 1. Even though in our case, credit default would probably be considered a failure in the sense of real-world outcomes, the positive outcome (response variable = 1 in a binary problem) is conventionally considered "success" in mathematical terminology. The <strong class="source-inline">logit</strong> function is also called the <strong class="bold">log odds</strong>, because it is the natural logarithm of the <strong class="bold">odds ratio</strong>, <em class="italic">p/q</em>. Odds ratios may be familiar from the world of gambling, via phrases such as "the odds are 2 to 1 that team <em class="italic">a</em> will defeat team <em class="italic">b</em>."</p>
			<p>In general, what we've called capital <em class="italic">X</em> in these manipulations can stand for a linear combination of all the features. For example, this would be <em class="italic">X = </em><em class="italic">𝜃</em><span class="subscript">o</span><em class="italic">  + </em><em class="italic">𝜃</em><span class="subscript">1</span><em class="italic">X</em><span class="subscript">1</span><em class="italic"> + </em><em class="italic">𝜃</em><span class="subscript">2</span><em class="italic">X</em><span class="subscript">2</span> in our simple case of two features. Logistic regression is considered a linear model because the features included in <em class="italic">X</em> are, in fact, only subject to a linear combination when the response variable is considered to be the log odds. This is an alternative way of formulating the problem, as compared to the sigmoid equation.</p>
			<p>Putting the pieces together, the features <em class="italic">X</em><span class="subscript">1</span>, <em class="italic">X</em><span class="subscript">2</span>,…, <em class="italic">X</em><span class="subscript">j</span> look like this in the sigmoid equation version of logistic regression:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer116">
					<img alt="Figure 3.25: Sigmoid version of logistic regression&#13;&#10;" src="image/B16925_03_25.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.25: Sigmoid version of logistic regression</p>
			<p>But they look like this in the log odds version, which is why logistic regression is called a linear model:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer117">
					<img alt="Figure 3.26: Log odds version of logistic regression&#13;&#10;" src="image/B16925_03_26.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.26: Log odds version of logistic regression</p>
			<p>Because of this way of looking at logistic regression, ideally, the features of a logistic regression model would be <strong class="bold">linear in the log odds</strong> of the response variable. We will see what is meant by this in the following exercise.</p>
			<p>Logistic regression is part of a broader class of statistical models called <strong class="bold">Generalized Linear Models</strong> (<strong class="bold">GLMs</strong>). GLMs are connected to the fundamental concept of ordinary linear regression, which may have one feature (that is, the <strong class="bold">line of best fit</strong>, <em class="italic">y = mx + b</em>, for a single feature, <em class="italic">x</em>) or more than one in <strong class="bold">multiple linear regression</strong>. The mathematical connection between GLMs and linear regression is the <strong class="bold">link function</strong>. The link function of logistic regression is the logit function we just learned about. </p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>Exercise 3.04: Examining the Appropriateness of Features for Logistic Regression</h2>
			<p>In <em class="italic">Exercise 3.02</em>, <em class="italic">Visualizing the Relationship between the Features and Response Variable</em>, we plotted a <strong class="source-inline">groupby</strong>/<strong class="source-inline">mean</strong> of what might be the most important feature of the model, according to our exploration so far: the <strong class="source-inline">PAY_1</strong> feature. By grouping samples by the values of <strong class="source-inline">PAY_1</strong>, and then looking at the mean of the response variable, we are effectively looking at the probability, <em class="italic">p</em>, of default within each of these groups.</p>
			<p>In this exercise, we will evaluate the appropriateness of <strong class="source-inline">PAY_1</strong> for logistic regression. We will do this by examining the log odds of default within these groups to see whether the response variable is linear in the log odds, as logistic regression formally assumes. Perform the following steps to complete the exercise: </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before beginning step 1 of this exercise, make sure that you have imported the necessary libraries. You can refer to the following notebook for the prerequisite steps: <a href="https://packt.link/gtpF9">https://packt.link/gtpF9</a>.</p>
			<ol>
				<li value="1">Confirm you still have access to the variables from <em class="italic">Exercise 3.02</em>, <em class="italic">Visualizing the Relationship between the Features and Response Variable</em>, in your notebook by reviewing the DataFrame of the average value of the response variable for different values of <strong class="source-inline">PAY_1</strong> with this code:<p class="source-code">group_by_pay_mean_y</p><p>The output should be as follows:</p><div class="IMG---Figure" id="_idContainer118"><img alt="Figure 3.27: Rates of default within groups of PAY_1 values as probabilities of default&#13;&#10;" src="image/B16925_03_27.jpg"/></div><p class="figure-caption">Figure 3.27: Rates of default within groups of PAY_1 values as probabilities of default</p></li>
				<li>Extract the mean values of the response variable from these groups and put them in a variable, <strong class="source-inline">p</strong>, representing the probability of default:<p class="source-code">p = group_by_pay_mean_y['default payment next month'].values</p></li>
				<li>Create a probability, <strong class="source-inline">q</strong>, of not defaulting. Since there are only two possible outcomes in this binary problem, and probabilities of all outcomes always sum to 1, it is easy to calculate <strong class="source-inline">q</strong>. Also print the values of <strong class="source-inline">p</strong> and <strong class="source-inline">q</strong> to confirm:<p class="source-code">q = 1-p</p><p class="source-code">print(p)</p><p class="source-code">print(q)</p><p>The output should be as follows:</p><div class="IMG---Figure" id="_idContainer119"><img alt="Figure 3.28: Calculating q from p&#13;&#10;" src="image/B16925_03_28.jpg"/></div><p class="figure-caption">Figure 3.28: Calculating q from p</p></li>
				<li>Calculate the odds ratio from <strong class="source-inline">p</strong> and <strong class="source-inline">q</strong>, as well as the log odds, using the natural logarithm function from NumPy:<p class="source-code">odds_ratio = p/q</p><p class="source-code">log_odds = np.log(odds_ratio)</p><p class="source-code">log_odds</p><p>The output should look like this:</p><div class="IMG---Figure" id="_idContainer120"><img alt="Figure 3.29: Odds ratio and log odds&#13;&#10;" src="image/B16925_03_29.jpg"/></div><p class="figure-caption">Figure 3.29: Odds ratio and log odds</p></li>
				<li>In order to plot the log odds against the values of the feature, we can get the feature values from the index of the DataFrame containing <strong class="source-inline">groupby</strong>/<strong class="source-inline">mean</strong>. You can show the index like this:<p class="source-code">group_by_pay_mean_y.index</p><p>This should produce the following output:</p><p class="source-code">Int64Index([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8], dtype='int64', name='PAY_1')</p></li>
				<li>Create a similar plot to what we have already done, to show the log odds against the values of the feature. Here is the code:<p class="source-code">plt.plot(group_by_pay_mean_y.index, log_odds, '-x')</p><p class="source-code">plt.ylabel('Log odds of default')</p><p class="source-code">plt.xlabel('Values of PAY_1')</p><p>The plot should look like this:</p><div class="IMG---Figure" id="_idContainer121"><img alt="Figure 3.30: Log odds of default for values of PAY_1&#13;&#10;" src="image/B16925_03_30.jpg"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.30: Log odds of default for values of PAY_1</p>
			<p>We can see in this plot that the relationship between the log odds of the response variable and the <strong class="source-inline">PAY_1</strong> feature is not all that different from the relationship between the rate of default and this feature that we plotted in <em class="italic">Exercise 3.02</em>, <em class="italic">Visualizing the Relationship between the Features and Response Variable</em>. For this reason, if the "rate of default" is a simpler concept for you to communicate to the business partner, it may be preferable. However, in terms of understanding the workings of logistic regression, this plot shows exactly what is assumed to be linear.</p>
			<p><strong class="bold">Is a straight-line fit a good model for this data? </strong></p>
			<p>It certainly seems like a "line of best fit" drawn on this plot would go up from left to right. At the same time, this data doesn't seem like it would result in a truly linear process. One way to look at this data is that the values -2, -1, and 0 seem like they lie in a different regime of log odds than the others. <strong class="source-inline">PAY_1 = 1</strong> is sort of intermediate, and the rest are mostly larger. It may be that engineered features based on this variable, or different ways of encoding the categories represented by -2, -1, and 0, would be more effective for modeling. Keep this in mind as we proceed to model this data with logistic regression and then other approaches later in the book.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor074"/>From Logistic Regression Coefficients to Predictions Using Sigmoid</h2>
			<p>Before the next exercise, let's take a look at how the coefficients for logistic regression are used to calculate predicted probabilities, and ultimately make predictions for the class of the response variable.</p>
			<p>Recall that logistic regression predicts the probability of class membership, according to the sigmoid equation. In the case of two features with an intercept, the equation is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer122">
					<img alt="Figure 3.31: Sigmoid function to predict the probability of class membership for two features&#13;&#10;" src="image/B16925_03_31.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.31: Sigmoid function to predict the probability of class membership for two features</p>
			<p>When you call the <strong class="source-inline">.fit</strong> method of a logistic regression model object in scikit-learn using the training data, the <em class="italic">𝜃</em><span class="subscript">0</span>, <em class="italic">𝜃</em><span class="subscript">1</span>, and <em class="italic">𝜃</em><span class="subscript">2</span> parameters (intercept and coefficients) are estimated from this labeled training data. Effectively, scikit-learn figures out how to choose values for <em class="italic">𝜃</em><span class="subscript">0</span>, <em class="italic">𝜃</em><span class="subscript">1</span>, and <em class="italic">𝜃</em><span class="subscript">2</span>, so that it will classify as many training data points correctly as possible. We'll gain some insight into how this process works in the next chapter.</p>
			<p>When you call <strong class="source-inline">.predict</strong>, scikit-learn calculates predicted probabilities according to the fitted parameter values and the sigmoid equation. A given sample will then be classified as positive if <em class="italic">p ≥ 0.5</em>, and negative otherwise.</p>
			<p>We know that the plot of the sigmoid equation looks like the following, which we can connect to the equation in <em class="italic">Figure 3.31</em> by making the substitution <em class="italic">X = </em><em class="italic">𝜃</em><span class="subscript">0</span><em class="italic"> + </em><em class="italic">𝜃</em><span class="subscript">1</span><em class="italic">X</em><span class="subscript">1</span><em class="italic"> + </em><em class="italic">𝜃</em><span class="subscript">2</span><em class="italic">X</em><span class="subscript">2</span>:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer123">
					<img alt="Figure 3.32: Predictions and true classes plotted together&#13;&#10;" src="image/B16925_03_32.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.32: Predictions and true classes plotted together</p>
			<p>Notice here that if <em class="italic">X = </em><em class="italic">𝜃</em><span class="subscript">o</span><em class="italic"> + </em><em class="italic">𝜃</em><span class="subscript">1</span><em class="italic">X</em><span class="subscript">1</span><em class="italic"> + </em><em class="italic">𝜃</em><span class="subscript">2</span><em class="italic">X</em><span class="subscript">2</span><em class="italic"> ≥ 0</em> on the <em class="italic">x</em> axis, then the predicted probability would be <em class="italic">p ≥ 0.5</em> on the <em class="italic">y</em> axis and the sample would be classified as positive. Otherwise, <em class="italic">p </em><em class="italic">&lt;</em><em class="italic"> 0.5</em> and the sample would be classified as negative. We can use this observation to calculate a linear condition for positive prediction, in terms of the <em class="italic">X</em><span class="subscript">1</span> and <em class="italic">X</em><span class="subscript">2</span> features, using the coefficients and intercept. Solving the inequality for positive prediction, <em class="italic">X = </em><em class="italic">𝜃</em><span class="subscript">o</span><em class="italic"> + </em><em class="italic">𝜃</em><span class="subscript">1</span><em class="italic">X</em><span class="subscript">1</span><em class="italic"> + </em><em class="italic">𝜃</em><span class="subscript">2</span><em class="italic">X</em><span class="subscript">2</span><em class="italic"> ≥ 0</em>, for <em class="italic">X</em><span class="subscript">2</span>, we can obtain a linear inequality similar to a linear equation in <em class="italic">y = mx + b</em> form: <em class="italic">X</em><span class="subscript">2</span><em class="italic"> ≥ -(</em><em class="italic">𝜃</em><span class="subscript">1</span><em class="italic">/</em><em class="italic">𝜃</em><span class="subscript">2</span><em class="italic">)X</em><span class="subscript">1</span><em class="italic"> - (</em><em class="italic">𝜃</em><span class="subscript">o</span><em class="italic">/</em><em class="italic">𝜃</em><span class="subscript">2</span><em class="italic">)</em>.</p>
			<p>This will help to see the linear decision boundary of logistic regression in the <em class="italic">X</em><span class="subscript">1</span><em class="italic">-X</em><span class="subscript">2</span> <strong class="bold">feature space</strong> in the following exercise. </p>
			<p>We have now learned, from a theoretical and mathematical perspective, why logistic regression is considered a linear model. We also examined a single feature and considered whether the assumption of linearity was appropriate. It is also important to understand the assumption of linearity, in terms of how flexible and powerful we can expect logistic regression to be. We explore this in the following exercise.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/>Exercise 3.05: Linear Decision Boundary of Logistic Regression</h2>
			<p>In this exercise, we illustrate the concept of a <strong class="bold">decision boundary</strong> for a binary classification problem. We use synthetic data to create a clear example of how the decision boundary of logistic regression looks in comparison to the training samples. We start by generating two features, <em class="italic">X</em><span class="subscript">1</span> and <em class="italic">X</em><span class="subscript">2</span>, at random. Since there are two features, we can say that the data for this problem is two-dimensional. This makes it easy to visualize. The concepts we illustrate here generalize to cases of more than two features, such as the real-world datasets you're likely to see in your work; however, the decision boundary is harder to visualize in higher-dimensional spaces. </p>
			<p>Perform the following steps to complete the exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before beginning step 1 of this exercise, make sure that you have imported the necessary libraries. You can refer to the following notebook for the prerequisite steps: <a href="https://packt.link/35ge1">https://packt.link/35ge1</a>.</p>
			<ol>
				<li value="1">Generate the features using the following code:<p class="source-code">from numpy.random import default_rng</p><p class="source-code">rg = default_rng(4)</p><p class="source-code">X_1_pos = rg.uniform(low=1, high=7, size=(20,1))</p><p class="source-code">print(X_1_pos[0:3])</p><p class="source-code">X_1_neg = rg.uniform(low=3, high=10, size=(20,1))</p><p class="source-code">print(X_1_neg[0:3])</p><p class="source-code">X_2_pos = rg.uniform(low=1, high=7, size=(20,1))</p><p class="source-code">print(X_2_pos[0:3])</p><p class="source-code">X_2_neg = rg.uniform(low=3, high=10, size=(20,1))</p><p class="source-code">print(X_2_neg[0:3])</p><p>You don't need to worry too much about why we selected the values we did; the plotting we do later should make it clear. Notice, however, that we have assigned the true class at the same time, by defining here which points (<strong class="source-inline">X</strong><span class="subscript">1</span><strong class="source-inline">, X</strong><span class="subscript">2</span>) will be in the positive and negative classes. The result of this is that we have 20 samples each in the positive and negative classes, for a total of 40 samples, and that we have two features for each sample. We show the first three values of each feature for both the positive and negative classes.</p><p>The output should be the following:</p><div class="IMG---Figure" id="_idContainer124"><img alt="Figure 3.33: Generating synthetic data for a binary classification problem&#13;&#10;" src="image/B16925_03_33.jpg"/></div><p class="figure-caption">Figure 3.33: Generating synthetic data for a binary classification problem</p></li>
				<li>Plot this data, coloring the positive samples as red squares and the negative samples as blue <em class="italic">x</em>'s. The plotting code is as follows:<p class="source-code">plt.scatter(X_1_pos, X_2_pos, color='red', marker='s')</p><p class="source-code">plt.scatter(X_1_neg, X_2_neg, color='blue', marker='x')</p><p class="source-code">plt.xlabel(‚$X_1$')</p><p class="source-code">plt.ylabel(‚$X_2$')</p><p class="source-code">plt.legend(['Positive class', 'Negative class'])</p><p>The result should look like this:</p><div class="IMG---Figure" id="_idContainer125"><img alt="Figure 3.34: Generating synthetic data for a binary classification problem&#13;&#10;" src="image/B16925_03_34.jpg"/></div><p class="figure-caption">Figure 3.34: Generating synthetic data for a binary classification problem</p><p>In order to use our synthetic features with scikit-learn, we need to assemble them into a matrix. We use NumPy's <strong class="source-inline">block</strong> function for this, to create a 40 by 2 matrix. There will be 40 rows because there are 40 total samples, and 2 columns because there are 2 features. We will arrange things so that the features for the positive samples come in the first 20 rows and those for the negative samples after that. </p></li>
				<li>Create a 40 by 2 matrix and then show the shape and the first 3 rows:<p class="source-code">X = np.block([[X_1_pos, X_2_pos], [X_1_neg, X_2_neg]])</p><p class="source-code">print(X.shape)</p><p class="source-code">print(X[0:3])</p><p>The output should be as follows:</p><p class="source-code">(40, 2)</p><p class="source-code">[[6.65833663 5.15531227]</p><p class="source-code"> [4.06796532 5.6237829  ]</p><p class="source-code"> [6.85746223 2.14473103]]</p><p>We also need a response variable to go with these features. We know how we defined them, but we need an array of <strong class="source-inline">y</strong> values to let scikit-learn know. </p></li>
				<li>Create a vertical stack (<strong class="source-inline">vstack</strong>) of 20 ones and then 20 zeros to match our arrangement of the features and reshape to the way that scikit-learn expects. Here is the code:<p class="source-code">y = np.vstack((np.ones((20,1)), np.zeros((20,1)))).reshape(40,)</p><p class="source-code">print(y[0:5])</p><p class="source-code">print(y[-5:])</p><p>You will obtain the following output:</p><p class="source-code">[1. 1. 1. 1. 1.]</p><p class="source-code">[0. 0. 0. 0. 0.]</p><p>At this point, we are ready to fit a logistic regression model to this data with scikit-learn. We will use all of the data as training data and examine how well a linear model is able to fit the data. The next few steps should be familiar from your work in earlier chapters on how to instantiate a model class and fit the model. </p></li>
				<li>First, import the model class using the following code:<p class="source-code">from sklearn.linear_model import LogisticRegression</p></li>
				<li>Now instantiate, indicating the <strong class="source-inline">liblinear</strong> solver, and show the model object using the following code:<p class="source-code">example_lr = LogisticRegression(solver='liblinear')</p><p class="source-code">example_lr</p><p>The output should be as follows:</p><p class="source-code">LogisticRegression(solver='liblinear')</p><p>We'll discuss some of the different solvers available for logistic regression in scikit-learn in <em class="italic">Chapter 4</em>, <em class="italic">The Bias-Variance Trade-Off</em>, but for now we'll use this one.</p></li>
				<li>Now train the model on the synthetic data:<p class="source-code">example_lr.fit(X, y)</p><p><strong class="bold">How do the predictions from our fitted model look? </strong></p><p>We first need to obtain these predictions, by using the trained model's <strong class="source-inline">.predict</strong> method on the same samples we used for model training. Then, in order to add these predictions to the plot, we will create two lists of indices to use with the arrays, according to whether the prediction is 1 or 0. See whether you can understand how we've used a list comprehension, including an <strong class="source-inline">if</strong> statement, to accomplish this.</p></li>
				<li>Use this code to get predictions and separate them into indices of positive and negative class predictions. Show the indices of positive class predictions as a check:<p class="source-code">y_pred = example_lr.predict(X)</p><p class="source-code">positive_indices = [counter for counter in range(len(y_pred))</p><p class="source-code">                    if y_pred[counter]==1]</p><p class="source-code">negative_indices = [counter for counter in range(len(y_pred))</p><p class="source-code">                    if y_pred[counter]==0]</p><p class="source-code">positive_indices</p><p>The output should be as follows:</p><p class="source-code">[2, 3, 4, 5, 6, 7, 9, 11, 13, 15, 16, 17, 18, 19, 26, 34, 36]</p><p>From the indices of positive predictions, we can already tell that not every sample in the training data was classified correctly: the positive samples were the first 20 samples, but there are indices outside of that range here. You may have already guessed that a linear decision boundary would not be able to perfectly classify this data, based on examining it. Now let's put these predictions on the plot, in the form of squares and circles around each data point, colored according to positive and negative predictions, respectively: red for positive and blue for negative. </p><p>You can compare the color and shape of the inner symbols, the true labels of the data, to those of the outer symbols (predictions), to see which points were classified correctly or incorrectly.</p></li>
				<li>Here is the plotting code:<p class="source-code">plt.scatter(X_1_pos, X_2_pos, color='red', marker='s')</p><p class="source-code">plt.scatter(X_1_neg, X_2_neg, color='blue', marker='x')</p><p class="source-code">plt.scatter(X[positive_indices,0], X[positive_indices,1],</p><p class="source-code">            s=150, marker='s',</p><p class="source-code">            edgecolors='red', facecolors='none')</p><p class="source-code">plt.scatter(X[negative_indices,0], X[negative_indices,1],</p><p class="source-code">            s=150, marker='o',</p><p class="source-code">            edgecolors='blue', facecolors='none')</p><p class="source-code">plt.xlabel('$X_1$')</p><p class="source-code">plt.ylabel('$X_2$')</p><p class="source-code">plt.legend(['Positive class', 'Negative class',\</p><p class="source-code">            'Positive predictions', 'Negative predictions'])</p><p>The plot should appear as follows:</p><div class="IMG---Figure" id="_idContainer126"><img alt="Figure 3.35: Predictions and true classes plotted together&#13;&#10;" src="image/B16925_03_35.jpg"/></div><p class="figure-caption">Figure 3.35: Predictions and true classes plotted together</p><p>From the plot, it's apparent that the classifier struggles with data points that are close to where you may imagine the linear decision boundary to be; some of these may end up on the wrong side of that boundary. How might we figure out, and visualize, the actual location of the decision boundary? From the previous section, we know we can obtain the decision boundary of a logistic regression, in two-dimensional feature space, using the inequality <em class="italic">X</em><span class="subscript">2</span><em class="italic"> ≥ -(</em><em class="italic">𝜃</em><span class="subscript">1</span><em class="italic">/</em><em class="italic">𝜃</em><span class="subscript">2</span><em class="italic">)X</em><span class="subscript">1</span><em class="italic"> - (</em><em class="italic">𝜃</em><span class="subscript">0</span><em class="italic">/</em><em class="italic">𝜃</em><span class="subscript">2</span><em class="italic">)</em>. Since we've fitted the model here, we can retrieve the <em class="italic">𝜃</em><span class="subscript">1</span> and <em class="italic">𝜃</em><span class="subscript">2</span> coefficients, as well as the <em class="italic">𝜃</em><span class="subscript">0</span> intercept, to plug into this equation and create the plot.</p></li>
				<li>Use this code to get the coefficients from the fitted model and print them:<p class="source-code">theta_1 = example_lr.coef_[0][0]</p><p class="source-code">theta_2 = example_lr.coef_[0][1]</p><p class="source-code">print(theta_1, theta_2)</p><p>The output should look like this:</p><p class="source-code">-0.16472042583006558 -0.25675185949979507</p></li>
				<li>Use this code to get the intercept:<p class="source-code">theta_0 = example_lr.intercept_</p><p>Now use the coefficients and intercept to define the linear decision boundary. This captures the dividing line of the inequality, <em class="italic">X</em><span class="subscript">2</span><em class="italic"> ≥ -(</em><em class="italic">𝜃</em><span class="subscript">1</span><em class="italic">/</em><em class="italic">𝜃</em><span class="subscript">2</span><em class="italic">)X</em><span class="subscript">1</span><em class="italic"> - (</em><em class="italic">𝜃</em><span class="subscript">0</span><em class="italic">/</em><em class="italic">𝜃</em><span class="subscript">2</span><em class="italic">)</em>:</p><p class="source-code">X_1_decision_boundary = np.array([0, 10])</p><p class="source-code">X_2_decision_boundary = -(theta_1/theta_2)*X_1_decision_boundary\</p><p class="source-code">                        - (theta_0/theta_2)</p><p>To summarize the last few steps, after using the <strong class="source-inline">.coef_</strong> and <strong class="source-inline">.intercept_</strong> methods to retrieve the <em class="italic">𝜃</em><span class="subscript">1</span> and <em class="italic">𝜃</em><span class="subscript">2</span> model coefficients and the <em class="italic">𝜃</em><span class="subscript">0</span> intercept, we then used these to create a line defined by two points, according to the equation we described for the decision boundary.</p></li>
				<li>Plot the decision boundary using the following code, with some adjustments to assign the correct labels for the legend, and to move the legend to a location (<strong class="source-inline">loc</strong>) outside a plot that is getting crowded:<p class="source-code">pos_true = plt.scatter(X_1_pos, X_2_pos,</p><p class="source-code">                       color='red', marker='s',</p><p class="source-code">                       label='Positive class')</p><p class="source-code">neg_true = plt.scatter(X_1_neg, X_2_neg,</p><p class="source-code">                       color='blue', marker='x',</p><p class="source-code">                       label='Negative class')</p><p class="source-code">pos_pred = plt.scatter(X[positive_indices,0],</p><p class="source-code">                       X[positive_indices,1],</p><p class="source-code">                       s=150, marker='s',</p><p class="source-code">                       edgecolors='red', facecolors='none',</p><p class="source-code">                       label='Positive predictions')</p><p class="source-code">neg_pred = plt.scatter(X[negative_indices,0],</p><p class="source-code">                       X[negative_indices,1],</p><p class="source-code">                       s=150, marker='o',</p><p class="source-code">                       edgecolors='blue', facecolors='none',</p><p class="source-code">                       label='Negative predictions')</p><p class="source-code">dec = plt.plot(X_1_decision_boundary, X_2_decision_boundary,</p><p class="source-code">               'k-', label='Decision boundary')</p><p class="source-code">plt.xlabel('$X_1$')</p><p class="source-code">plt.ylabel('$X_2$')</p><p class="source-code">plt.legend(loc=[0.25, 1.05])</p><p>You will obtain the following plot:</p><div class="IMG---Figure" id="_idContainer127"><img alt="Figure 3.36: True classes, predicted classes, and the decision boundary&#13;&#10; of a logistic regression&#13;&#10;" src="image/B16925_03_36.jpg"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.36: True classes, predicted classes, and the decision boundary  of a logistic regression</p>
			<p><strong class="bold">How does the location of the decision boundary compare with where you thought it would be?</strong></p>
			<p><strong class="bold">Can you see how a linear decision boundary will never perfectly classify this data?</strong></p>
			<p>As a way around this, we could create <strong class="bold">engineered features</strong> from existing features here, such as polynomials or interactions, to allow for more complex, non-linear decision boundaries in a logistic regression. Or, we could use non-linear models such as random forest, which can also accomplish this, as we'll see later.</p>
			<p>As a final note here, this example was easily visualized in two dimensions since there are only two features. In general, the decision boundary can be described by a <strong class="bold">hyperplane</strong>, which is the generalization of a straight line to multi-dimensional spaces. However, the restrictive nature of the linear decision boundary is still a factor for hyperplanes.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/>Activity 3.01: Fitting a Logistic Regression Model and Directly Using the Coefficients</h2>
			<p>In this activity, we're going to train a logistic regression model on the two most important features we discovered in univariate feature exploration, as well as learning how to manually implement logistic regression using coefficients from the fitted model. This will show you how you could use logistic regression in a computing environment where scikit-learn may not be available, but the mathematical functions necessary to compute the sigmoid function are. On successful completion of the activity, you should observe that the calculated ROC AUC values using scikit-learn predictions and those obtained from manual predictions should be the same: approximately 0.63.</p>
			<p>Perform the following steps to complete the activity:</p>
			<ol>
				<li value="1">Create a train/test split (80/20) with <strong class="source-inline">PAY_1</strong> and <strong class="source-inline">LIMIT_BAL</strong> as features.</li>
				<li>Import <strong class="source-inline">LogisticRegression</strong>, with the default options, but set the solver to <strong class="source-inline">'liblinear'</strong>.</li>
				<li>Train on the training data and obtain predicted classes, as well as class probabilities, using the test data.</li>
				<li>Pull out the coefficients and intercept from the trained model and manually calculate predicted probabilities. You'll need to add a column of ones to your features, to multiply by the intercept.</li>
				<li>Using a threshold of <strong class="source-inline">0.5</strong>, manually calculate predicted classes. Compare this to the class predictions outputted by scikit-learn.</li>
				<li>Calculate the ROC AUC using both scikit-learn's predicted probabilities and your manually predicted probabilities, and compare them.<p class="callout-heading">Note</p><p class="callout">The Jupyter notebook containing the code for this activity can be found here: <a href="https://packt.link/4FHec">https://packt.link/4FHec</a>. This notebook contains only the Python code and corresponding outputs. The complete step-wise solution can be found via <a href="B16925_Solution_ePub.xhtml#_idTextAnchor153">this link</a>.</p></li>
			</ol>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor077"/>Summary</h1>
			<p>In this chapter, we have learned how to explore features one at a time, using univariate feature selection methods including Pearson correlation and an ANOVA F-test. While looking at features in this way does not always tell the whole story, since you are potentially missing out on important interactions between features, it is often a helpful step. Understanding the relationships between the most predictive features and the response variable, and creating effective visualizations around them, is a great way to communicate your findings to your client. We used customized plots, such as overlapping histograms created with Matplotlib, to create visualizations of the most important features.</p>
			<p>Then we began an in-depth description of how logistic regression works, exploring such topics as the sigmoid function, log odds, and the linear decision boundary. While logistic regression is one of the simplest classification models, and often is not as powerful as other methods, it is one of the most widely used and is the basis for more sophisticated models such as deep neural networks for classification. So, a detailed understanding of logistic regression can serve you well as you explore more advanced topics in machine learning. And, in some cases, a simple logistic regression may be all that's needed. All other things considered, the simplest model that satisfies the requirements is probably the best model. </p>
			<p>If you master the materials in this and the next chapter, you will be well prepared to use logistic regression in your work. In the next chapter, we'll build on the fundamentals we learned here, to see how coefficients are estimated for a logistic regression, as well as how logistic regression can be used effectively with large numbers of features and can also be used for feature selection.</p>
		</div>
		<div>
			<div class="Content" id="_idContainer129">
			</div>
		</div>
	</body></html>