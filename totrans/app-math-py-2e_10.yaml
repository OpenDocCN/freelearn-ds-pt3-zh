- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Improving Your Productivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will look at several topics that don’t fit within the categories
    that we discussed in the previous chapters of this book. Most of these topics
    are concerned with different ways to facilitate computing and otherwise optimize
    the execution of our code. Others are concerned with working with specific kinds
    of data or file formats.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this chapter is to provide you with some tools that, while not strictly
    mathematical in nature, often appear in mathematical problems. These include topics
    such as distributed computing and optimization – both help you to solve problems
    more quickly, validate data and calculations, load and store data from file formats
    commonly used in scientific computation, and incorporate other topics that will
    generally help you be more productive with your code.
  prefs: []
  type: TYPE_NORMAL
- en: In the first two recipes, we will cover packages that help keep track of units
    and uncertainties in calculations. These are very important for calculations that
    concern data that have a direct physical application. In the next recipe, we will
    look at loading and storing data from **Network Common Data Form** (**NetCDF**)
    files. NetCDF is a file format usually used for storing weather and climate data.
    In the fourth recipe, we’ll discuss working with geographical data, such as data
    that might be associated with weather or climate data. After that, we’ll discuss
    how we can run Jupyter notebooks from the terminal without having to start up
    an interactive session. Then, we will turn to validating data for the next recipe
    and then focus on performance with tools such as Cython and Dask. Finally, we
    will give a very short overview of some techniques for writing reproducible code
    for data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of units with Pint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accounting for uncertainty in calculations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and storing data from NetCDF files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with geographical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing a Jupyter notebook as a script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accelerating code with Cython
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributing computation with Dask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing reproducible code for data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires many different packages due to the nature of the recipes
    it contains. The list of packages we need is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uncertainties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: netCDF4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: xarray
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GeoPandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geoplot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jupyter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papermill
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cerberus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cython
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of these packages can be installed using your favorite package manager,
    such as `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To install the Dask package, we need to install the various extras associated
    with the package. We can do this using the following `pip` command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In addition to these Python packages, we will also need to install some supporting
    software. For the *Working with geographical data* recipe, the GeoPandas and Geoplot
    libraries have numerous lower-level dependencies that might need to be installed
    separately. Detailed instructions are given in the GeoPandas package documentation
    at [https://geopandas.org/install.html](https://geopandas.org/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: For the *Accelerating code with Cython* recipe, we will need to have a C compiler
    installed. Instructions on how to obtain the **GNU C compiler** (**GCC**) are
    given in the Cython documentation at [https://cython.readthedocs.io/en/latest/src/quickstart/install.html](https://cython.readthedocs.io/en/latest/src/quickstart/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found in the `Chapter 10` folder of the GitHub
    repository at [https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2010](https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2010).
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of units with Pint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Correctly keeping track of units in calculations can be very difficult, particularly
    if there are places where different units can be used. For example, it is very
    easy to forget to convert between different units – feet/inches into meters –
    or metric prefixes – converting 1 km into 1,000 m, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we’ll learn how to use the Pint package to keep track of units
    of measurement in calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we need the Pint package, which can be imported as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show you how to use the Pint package to keep track of units
    in calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create a `UnitRegistry` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To create a quantity with a unit, we multiply the number by the appropriate
    attribute of the registry object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can change the units of the quantity using one of the available conversion
    methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of these `print` statements is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We wrap a routine to make it expect an argument in seconds and output a result
    in meters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, when we call the `calc_depth` routine with a `minute` unit, it is automatically
    converted into seconds for the calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Pint package provides a wrapper class for numerical types that adds unit
    metadata to the type. This wrapper type implements all the standard arithmetic
    operations and keeps track of the units throughout these calculations. For example,
    when we divide a length unit by a time unit, we will get a speed unit. This means
    that you can use Pint to make sure the units are correct after a complex calculation.
  prefs: []
  type: TYPE_NORMAL
- en: The `UnitRegistry` object keeps track of all the units that are present in the
    session and handles things such as conversion between different unit types. It
    also maintains a reference system of measurements, which, in this recipe, is the
    standard international system with meters, kilograms, and seconds as base units,
    denoted by `mks`.
  prefs: []
  type: TYPE_NORMAL
- en: The `wraps` functionality allows us to declare the input and output units of
    a routine, which allows Pint to make automatic unit conversions for the input
    function – in this recipe, we converted from minutes into seconds. Trying to call
    a wrapped function with a quantity that does not have an associated unit, or an
    incompatible unit, will raise an exception. This allows runtime validation of
    parameters and automatic conversion into the correct units for a routine.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Pint package comes with a large list of preprogrammed units of measurement
    that cover most globally used systems. Units can be defined at runtime or loaded
    from a file. This means that you can define custom units or systems of units that
    are specific to the application that you are working with.
  prefs: []
  type: TYPE_NORMAL
- en: Units can also be used within different contexts, which allows for easy conversion
    between different unit types that would ordinarily be unrelated. This can save
    a lot of time in situations where you need to move between units fluidly at multiple
    points in a calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Accounting for uncertainty in calculations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most measuring devices are not 100% accurate and instead are accurate up to
    a certain amount, usually somewhere between 0 and 10%. For instance, a thermometer
    might be accurate to 1%, while a pair of digital calipers might be accurate up
    to 0.1%. The true value in both of these cases is unlikely to be exactly the reported
    value, although it will be fairly close. Keeping track of the uncertainty in a
    value is difficult, especially when you have multiple different uncertainties
    combined in different ways. Rather than keeping track of this by hand, it is much
    better to use a consistent library to do this for you. This is what the `uncertainties`
    package does.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to quantify the uncertainty of variables and
    see how these uncertainties propagate through a calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the `uncertainties` package, from which we will
    import the `ufloat` class and the `umath` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show you how to quantify uncertainty on numerical values
    in calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create an uncertain float value of `3.0` plus or minus `0.4`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we perform a calculation involving this uncertain value to obtain a new
    uncertain value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a new uncertain float value and apply the `sqrt` routine from
    the `umath` module and perform the reverse of the previous calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the result of the first calculation (*step 2*) is an uncertain
    float with a value of `44`, and ![](img/Formula_10_001.png) systematic error.
    This means that the true value could be anything between 32 and 56\. We cannot
    be more accurate than this with the measurements that we have.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `ufloat` class wraps around `float` objects and keeps track of the uncertainty
    throughout calculations. The library makes use of linear error propagation theory,
    which uses derivatives of non-linear functions to estimate the propagated error
    during calculations. The library also correctly handles correlation so that subtracting
    a value from itself gives zero with no error.
  prefs: []
  type: TYPE_NORMAL
- en: To keep track of uncertainties in standard mathematical functions, you need
    to use the versions that are provided in the `umath` module, rather than those
    defined in the Python Standard Library or a third-party package such as NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `uncertainties` package provides support for NumPy, and the Pint package
    mentioned in the previous recipe can be combined with `uncertainties` to make
    sure that units and error margins are correctly attributed to the final value
    of a calculation. For example, we could compute the units in the calculation from
    *step 2* of this recipe, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the `print` statement on the last line gives us `44+/-12` meters.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and storing data from NetCDF files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many scientific applications require that we start with large quantities of
    multi-dimensional data in a robust format. NetCDF is one example of a format used
    for data that’s developed by the weather and climate industry. Unfortunately,
    the complexity of the data means that we can’t simply use the utilities from the
    Pandas package, for example, to load this data for analysis. We need the `netcdf4`
    package to be able to read and import the data into Python, but we also need to
    use `xarray`. Unlike the Pandas library, `xarray` can handle higher-dimensional
    data while still providing a Pandas-like interface.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to load data from and store data in NetCDF
    files.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need to import the NumPy package as `np`, the Pandas
    package as `pd`, the Matplotlib `pyplot` module as `plt`, and an instance of the
    default random number generator from NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to import the `xarray` package under the `xr` alias. You will
    also need to install the Dask package, as described in the *Technical requirements*
    section, and the `netCDF4` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We don’t need to import either of these packages directly.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to load and store sample data in a NetCDF file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create some random data. This data consists of a range of
    dates, a list of location codes, and randomly generated numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a xarray `Dataset` object containing the data. The dates and
    locations are indexes, while the `steps` and `accumulated` variables are the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output from the `print` statement is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the mean over all the locations at each time index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we plot the mean accumulated values on a new set of axes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 - Plot of accumulated means over time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19085_10_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 - Plot of accumulated means over time
  prefs: []
  type: TYPE_NORMAL
- en: 'Save this dataset into a new NetCDF file using the `to_netcdf` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can load the newly created NetCDF file using the `load_dataset` routine
    from `xarray`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the loaded array contains all of the data that we added
    in the earlier steps. The important steps are *5* and *6*, where we store and
    load this `"``data.nc"` data.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `xarray` package provides the `DataArray` and `DataSet` classes, which are
    (roughly speaking) multi-dimensional equivalents of the Pandas `Series` and `DataFrame`
    objects. We’re using a dataset in this example because each index – a tuple of
    a date and location – has two pieces of data associated with it. Both of these
    objects expose a similar interface to their Pandas equivalents. For example, we
    can compute the mean along one of the axes using the `mean` method. The `DataArray`
    and `DataSet` objects also have a convenience method for converting into a Pandas
    `DataFrame` called `to_dataframe`. We used it in this recipe to convert the accumulated
    column from the *means* *Dataset* into a `DataFrame` for plotting, which isn’t
    really necessary because `xarray` has plotting features built into it.
  prefs: []
  type: TYPE_NORMAL
- en: The real focus of this recipe is on the `to_netcdf` method and the `load_dataset`
    routine. The former stores a `DataSet` object in a NetCDF format file. This requires
    the `netCDF4` package to be installed, as it allows us to access the relevant
    C library for decoding NetCDF-formatted files. The `load_dataset` routine is a
    general-purpose routine for loading data into a `DataSet` object from various
    file formats, including NetCDF (again, this requires the `netCDF4` package to
    be installed).
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `xarray` package has support for a number of data formats in addition to
    NetCDF, such as OPeNDAP, Pickle, GRIB, and other formats that are supported by
    Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Working with geographical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many applications involve working with geographical data. For example, when
    tracking global weather, we might want to plot the temperature as measured by
    various sensors around the world at their position on a map. For this, we can
    use the GeoPandas package and the Geoplot package, both of which allow us to manipulate,
    analyze, and visualize geographical data.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use the GeoPandas and Geoplot packages to load and visualize
    some sample geographical data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the GeoPandas package, the Geoplot package, and
    the Matplotlib `pyplot` package imported as `plt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to create a simple plot of the capital cities plotted on
    a map of the world using sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to load the sample data from the GeoPandas package, which contains
    the world geometry information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to load the data containing the name and position of each of
    the capital cities of the world:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can create a new figure and plot the outline of the world geometry
    using the `polyplot` routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we use the `pointplot` routine to add the positions of the capital
    cities on top of the world map. We also set the axes limits to make the whole
    world visible:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting plot of the positions of the capital cities of the world looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 - Plot of the world’s capital cities on a map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19085_10_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 - Plot of the world’s capital cities on a map
  prefs: []
  type: TYPE_NORMAL
- en: The plot shows a rough outline of the different countries of the world. Each
    of the capital cities is indicated by a marker. From this view, it is quite difficult
    to distinguish individual cities in central Europe.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GeoPandas package is an extension of Pandas that works with geographical
    data, while the Geoplot package is an extension of Matplotlib that’s used to plot
    geographical data. The GeoPandas package comes with a selection of sample datasets
    that we used in this recipe. `naturalearth_lowres` contains geometric figures
    that describe the boundaries of countries in the world. This data is not very
    high-resolution, as signified by its name, which means that some of the finer
    details of geographical features might not be present on the map (some small islands
    are not shown at all). `naturalearth_cities` contains the names and locations
    of the capital cities of the world. We’re using the `datasets.get_path` routine
    to retrieve the path for these datasets in the package data directory. The `read_file`
    routine imports the data into the Python session.
  prefs: []
  type: TYPE_NORMAL
- en: The Geoplot package provides some additional plotting routines specifically
    for plotting geographical data. The `polyplot` routine plots polygonal data from
    a GeoPandas DataFrame, which might describe the geographical boundaries of a country.
    The `pointplot` routine plots discrete points on a set of axes from a GeoPandas
    DataFrame, which, in this case, describe the positions of capital cities.
  prefs: []
  type: TYPE_NORMAL
- en: Executing a Jupyter notebook as a script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jupyter notebooks are a popular medium for writing Python code for scientific
    and data-based applications. A Jupyter notebook is really a sequence of blocks
    that is stored in a file in `ipynb` extension. Each block can be one of several
    different types, such as code or markdown. These notebooks are typically accessed
    through a web application that interprets the blocks and executes the code in
    a background kernel that then returns the results to the web application. This
    is great if you are working on a personal PC, but what if you want to run the
    code contained within a notebook remotely on a server? In this case, it might
    not even be possible to access the web interface provided by the Jupyter Notebook
    software. The `papermill` package allows us to parameterize and execute notebooks
    from the command line.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we’ll learn how to execute a Jupyter notebook from the command
    line using `papermill`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will need to have the `papermill` package installed, and
    also have a sample Jupyter notebook in the current directory. We will use the
    `sample.ipynb` notebook file stored in the code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to use the `papermill` command-line interface to execute
    a Jupyter notebook remotely:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we open the sample notebook, `sample.ipynb`, from the code repository
    for this chapter. The notebook contains three code cells that hold the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we open the folder containing the Jupyter notebook in the terminal and
    use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we open the output file, `output.ipynb`, which should now contain the
    notebook that’s been updated with the result of the executed code. The scatter
    plot that’s generated in the final block is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.3 - Scatter plot of the random data that was generated inside a
    Jupyter notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19085_10_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 - Scatter plot of the random data that was generated inside a Jupyter
    notebook
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the output of the `papermill` command is an entirely new notebook
    that copies the code and text content from the original and is populated with
    the output of running commands. This is useful for “freezing” the exact code that
    was used to generate the results.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `papermill` package provides a simple command-line interface that interprets
    and then executes a Jupyter notebook and stores the results in a new notebook
    file. In this recipe, we gave the first argument – the input notebook file – `sample.ipynb`,
    and the second argument – the output notebook file – `output.ipynb`. The tool
    then executes the code contained in the notebook and produces the output. The
    notebook’s file format keeps track of the results of the last run, so these results
    are added to the output notebook and stored at the desired location. In this recipe,
    this is a simple local file, but `papermill` can also store them in a cloud location
    such as **Amazon Web Services** (**AWS**) S3 storage or Azure data storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *step 2*, we added the `--kernel python3` option when using the `papermill`
    command-line interface. This option allows us to specify the kernel that is used
    to execute the Jupyter notebook. This might be necessary to prevent errors if
    `papermill` tries to execute the notebook with a kernel other than the one used
    to write the notebook. A list of available kernels can be found by using the following
    command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: If you get an error when executing a notebook, you could try changing to a different
    kernel.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Papermill also has a Python interface so that you can execute notebooks from
    within a Python application. This might be useful for building web applications
    that need to be able to perform long-running calculations on external hardware
    and where the results need to be stored in the cloud. It also has the ability
    to provide parameters to a notebook. To do this, we need to create a block in
    the notebook marked with the parameters tag with the default values. Updated parameters
    can then be provided through the command-line interface using the `-p` flag, followed
    by the name of the argument and the value.
  prefs: []
  type: TYPE_NORMAL
- en: Validating data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is often presented in a raw form and might contain anomalies or incorrect
    or malformed data, which will obviously present a problem for later processing
    and analysis. It is usually a good idea to build a validation step into a processing
    pipeline. Fortunately, the Cerberus package provides a lightweight and easy-to-use
    validation tool for Python.
  prefs: []
  type: TYPE_NORMAL
- en: For validation, we have to define a *schema*, which is a technical description
    of what the data should look like and the checks that should be performed on the
    data. For example, we can check the type and place bounds on the maximum and minimum
    values. Cerberus validators can also perform type conversions during the validation
    step, which allows us to plug data loaded directly from CSV files into the validator.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use Cerberus to validate data loaded from
    a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we need to import the `csv` module from the Python Standard
    Library ([https://docs.python.org/3/library/csv.html](https://docs.python.org/3/library/csv.html)),
    as well as the Cerberus package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We will also need the `sample.csv` file from the code repository ([https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2010](https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2010))
    for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following steps, we will validate a set of data that’s been loaded from
    CSV using the Cerberus package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to build a schema that describes the data we expect. To do this,
    we must define a simple schema for floating-point numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we build the schema for individual items. These will be the rows of our
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can define the schema for the whole document, which will contain a
    list of items:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a `Validator` object with the schema we just defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we load the data using a `DictReader` from the `csv` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we use the `validate` method on `validator` to validate the document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we retrieve the errors from the validation process from the `validator`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can print any error messages that appeared:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the error messages is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: This has identified four rows that do not conform to the schema that we set
    out, which limits the float values in “lower” and “upper” to those between `-1.0`
    and `1.0`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The schema that we created is a technical description of all the criteria that
    we need to check our data against. This will usually be defined as a dictionary
    with the name of the item as the key and a dictionary of properties, such as the
    type or bounds on the value in a dictionary, as the value. For example, in *step
    1*, we defined a schema for floating-point numbers that limits the numbers so
    that they’re between the values of -1 and 1\. Note that we include the `coerce`
    key, which specifies the type that the value should be converted into during the
    validation. This allows us to pass in data that’s been loaded from a CSV document,
    which only contains strings, without having to worry about its type.
  prefs: []
  type: TYPE_NORMAL
- en: The `validator` object takes care of parsing documents so that they’re validated
    and checking the data they contain against all the criteria described by the schema.
    In this recipe, we provided the schema to the `validator` object when it was created.
    However, we could also pass the schema into the `validate` method as a second
    argument. The errors are stored in a nested dictionary that mirrors the structure
    of the document.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerating code with Cython
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is often criticized for being a slow programming language – an endlessly
    debatable statement. Many of these criticisms can be addressed by using a high-performance
    compiled library with a Python interface – such as the scientific Python stack
    – to greatly improve performance. However, there are some situations where it
    is difficult to avoid the fact that Python is not a compiled language. One way
    to improve performance in these (fairly rare) situations is to write a C extension
    (or even rewrite the code entirely in C) to speed up the critical parts. This
    will certainly make the code run more quickly, but it might make it more difficult
    to maintain the package. Instead, we can use Cython, which is an extension of
    the Python language that is transpiled into C and compiled for great performance
    improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can consider some code that’s used to generate an image of
    the Mandelbrot set. For comparison, the pure Python code – which we assume is
    our starting point – is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason why this code is relatively slow in pure Python is fairly obvious:
    the nested loops. For demonstration purposes, let’s assume that we can’t vectorize
    this code using NumPy. A little preliminary testing shows that using these functions
    to generate the Mandelbrot set using 320 × 240 points and 255 steps takes approximately
    6.3 seconds. Your times may vary, depending on your system.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use Cython to greatly improve the performance of the
    preceding code in order to generate an image of the Mandelbrot set.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will need the NumPy package and the Cython package to be
    installed. You will also need a C compiler such as the GCC installed on your system.
    For example, on Windows, you can obtain a version of the GCC by installing MinGW.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to use Cython to greatly improve the performance of the
    code for generating an image of the Mandelbrot set:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a new file called `cython_mandel.pyx` in the `mandelbrot` folder. In
    this file, we will add some simple imports and type definitions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define a new version of the `in_mandel` routine using the Cython syntax.
    We add some declarations to the first few lines of this routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The rest of the function is identical to the Python version of the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define a new version of the `compute_mandel` function. We add two
    decorators to this function from the Cython package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we define the constants, just as we did in the original routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the `linspace` and `empty` routines from the NumPy package in exactly
    the same way as in the Python version. The only addition here is that we declare
    the `i` and `j` variables, which are of the `Int` type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The remainder of the definition is exactly the same as in the Python version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a new file called `setup.py` in the `mandelbrot` folder and
    add the following imports to the top of this file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, we define an extension module with the source pointing to the original
    `python_mandel.py` file. Set the name of this module to `hybrid_mandel`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we define a second extension module with the source set as the `cython_mandel.pyx`
    file that we just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we add both these extension modules to a list and call the `setup` routine
    to register these modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a new empty file called `__init__.py` in the `mandelbrot` folder to make
    this into a package that can be imported into Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the terminal inside the `mandelbrot` folder and use the following command
    to build the Cython extension modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, start a new file called `run.py` and add the following `import` statements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the various `compute_mandel` routines from each of the modules we have
    defined: `python_mandel` for the original; `hybrid_mandel` for the Cythonized
    Python code; and `cython_mandel` for the compiled pure Cython code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a simple timer decorator that we will use to test the performance of
    the routines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the `timer` decorator to each of the imported routines, and define some
    constants for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run each of the decorated routines with the constants we set previously. Record
    the output of the final call (the Cython version) in the `vals` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, plot the output of the Cython version to check that the routine computes
    the Mandelbrot set correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the `run.py` file will print the execution time of each of the routines
    to the terminal, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: These timings are not as good as in the first edition, which is likely due to
    the way Python is installed on the author’s PC. Your timings may vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot of the Mandelbrot set can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 - Image of the Mandelbrot set computed using Cython code'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19085_10_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 - Image of the Mandelbrot set computed using Cython code
  prefs: []
  type: TYPE_NORMAL
- en: This is what we expect for the Mandelbrot set. Some of the finer detail is visible
    around the boundary.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a lot happening in this recipe, so let’s start by explaining the overall
    process. Cython takes code that is written in an extension of the Python language
    and compiles it into C code, which is then used to produce a C extension library
    that can be imported into a Python session. In fact, you can even use Cython to
    compile ordinary Python code directly to an extension, although the results are
    not as good as when using the modified language. The first few steps in this recipe
    define the new version of the Python code in the modified language (saved as a
    `.pyx` file), which includes type information in addition to the regular Python
    code. In order to build the C extension using Cython, we need to define a setup
    file, and then we create a file that we run to produce the results.
  prefs: []
  type: TYPE_NORMAL
- en: The final compiled version of the Cython code runs considerably faster than
    its Python equivalent. The Cython-compiled Python code (hybrid, as we called it
    in this recipe) performs slightly better than the pure Python code. This is because
    the produced Cython code still has to work with Python objects with all of their
    caveats. By adding the typing information to the Python code, in the `.pyx` file,
    we start to see major improvements in performance. This is because the `in_mandel`
    function is now effectively defined as a C-level function that has no interaction
    with Python objects, and instead operates on primitive data types.
  prefs: []
  type: TYPE_NORMAL
- en: There are some small, but very important differences, between the Cython code
    and the Python equivalent. In *step 1*, you can see that we imported the NumPy
    package as usual but that we also used the `cimport` keyword to bring some C-level
    definitions into the scope. In *step 2*, we used the `cdef` keyword instead of
    the `def` keyword when we defined the `in_mandel` routine. This means that the
    `in_mandel` routine is defined as a C-level function that cannot be used from
    the Python level, which saves a significant amount of overhead when calling this
    function (which happens a lot).
  prefs: []
  type: TYPE_NORMAL
- en: The only other real differences regarding the definition of this function are
    the inclusion of some type declarations in the signature and the first few lines
    of the function. The two decorators we applied here disable the checking of bounds
    when accessing elements from a list (array). The `boundscheck` decorator disables
    checking whether the index is valid (between 0 and the size of the array), while
    the `wraparound` decorator disables the negative indexing. Both of these give
    a modest improvement to speed during execution, although they disable some of
    the safety features built into Python. In this recipe, it is OK to disable these
    checks because we are using a loop over the valid indices of the array.
  prefs: []
  type: TYPE_NORMAL
- en: The setup file is where we tell Python (and therefore Cython) how to build the
    C extension. The `cythonize` routine from Cython is the key here, as it triggers
    the Cython build process. In *steps 9* and *10*, we defined extension modules
    using the `Extension` class from `setuptools` so that we could define some extra
    details for the build; specifically, we set an environment variable for the NumPy
    compilation and added the `include` files for the NumPy C headers. This is done
    via the `define_macros` keyword argument for the `Extension` class. The terminal
    command we used in *step 13* uses `setuptools` to build the Cython extensions,
    and the addition of the `--inplace` flat means that the compiled libraries will
    be added to the current directory, rather than being placed in a centralized location.
    This is good for development.
  prefs: []
  type: TYPE_NORMAL
- en: 'The run script is fairly simple: import the routines from each of the defined
    modules – two of these are actually C extension modules – and time their execution.
    We have to be a little creative with the import aliases and routine names to avoid
    collisions.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cython is a powerful tool for improving the performance of some aspects of your
    code. However, you must always be careful to spend your time wisely while optimizing
    code. Using a profile such as cProfile that is provided in the Python Standard
    Library can be used to find the places where performance bottlenecks occur in
    your code. In this recipe, it was fairly obvious where the performance bottleneck
    was occurring. Cython is a good remedy to the problem in this case because it
    involves repetitive calls to a function inside a (double) `for` loop. However,
    it is not a universal fix for performance issues and, more often than not, the
    performance of code can be greatly improved by refactoring it so that it makes
    use of high-performance libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Cython is well integrated with Jupyter Notebook and can be used seamlessly in
    the code blocks of a notebook. Cython is also included in the Anaconda distribution
    of Python, so no additional setup is required for using Cython with Jupyter notebooks
    when it’s been installed using the Anaconda distribution.
  prefs: []
  type: TYPE_NORMAL
- en: There are alternatives to Cython when it comes to producing compiled code from
    Python. For example, the Numba package ([http://numba.pydata.org/](http://numba.pydata.org/))
    provides a **Just-in-Time** (**JIT**) compiler that optimizes Python code at runtime
    by simply placing a decorator on specific functions. Numba is designed to work
    with NumPy and other scientific Python libraries and can also be used to leverage
    GPUs to accelerate code.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a general-purpose JIT compiler for Python available through the
    `pyjion` package ([https://www.trypyjion.com/](https://www.trypyjion.com/)). This
    can be used in a variety of situations, unlike the Numba library, which is primarily
    for numerical code. The `jax` library discussed in [*Chapter 3*](B19085_03.xhtml#_idTextAnchor078)
    also has a JIT compiler built in, but this too is limited to numerical code.
  prefs: []
  type: TYPE_NORMAL
- en: Distributing computing with Dask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dask is a library that’s used for distributing computing across multiple threads,
    processes, or even computers in order to effectively perform computation on a
    huge scale. This can greatly improve performance and throughput, even if you are
    working on a single laptop computer. Dask provides replacements for most of the
    data structures from the Python scientific stack, such as NumPy arrays and Pandas
    DataFrames. These replacements have very similar interfaces, but under the hood,
    they are built for distributed computing so that they can be shared between multiple
    threads, processes, or computers. In many cases, switching to Dask is as simple
    as changing the `import` statement, and possibly adding a couple of extra method
    calls to start concurrent computations.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use Dask to do some simple computations
    on a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need to import the `dataframe` module from the Dask
    package. Following the convention set out in the Dask documentation, we will import
    this module under the `dd` alias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: We will also need the `sample.csv` file from the code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to use Dask to perform some computations on a DataFrame
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to load the data from `sample.csv` into a Dask DataFrame. The
    type of the `number` column is set to `"object"` because otherwise, Dask’s type
    inference will fail (since this column contains `None` but is otherwise integers):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we perform a standard calculation on the columns of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Unlike Pandas DataFrames, the result is not a new DataFrame. The `print` statement
    gives us the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: 'To actually get the result, we need to use the `compute` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is now shown as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: 'We compute the means of the final two columns in exactly the same way we would
    with a Pandas DataFrame, but we need to add a call to the `compute` method to
    execute the calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result, as printed, is exactly as we expect it to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dask builds a *task graph* for the computation, which describes the relationships
    between the various operations and calculations that need to be performed on the
    collection of data. This breaks down the steps of the calculation so that calculations
    can be done in the right order across the different workers. This task graph is
    then passed into a scheduler that sends the actual tasks to the workers for execution.
    Dask comes with several different schedulers: synchronous, threaded, multiprocessing,
    and distributed. The type of scheduler can be chosen in the call to the `compute`
    method or set globally. Dask will choose a sensible default if one is not given.'
  prefs: []
  type: TYPE_NORMAL
- en: The synchronous, threaded, and multiprocessing schedulers work on a single machine,
    while the distributed scheduler is for working with a cluster. Dask allows you
    to change between schedulers in a relatively transparent way, although for small
    tasks, you might not get any performance benefits because of the overhead of setting
    up more complicated schedulers.
  prefs: []
  type: TYPE_NORMAL
- en: The `compute` method is the key to this recipe. The methods that would ordinarily
    perform the computation on Pandas DataFrames now just set up a computation that
    is to be executed through the Dask scheduler. The computation isn’t started until
    the `compute` method is called. This is similar to the way that a `Future` (such
    as from the asyncio standard library package) is returned as a proxy for the result
    of an asynchronous function call, which isn’t fulfilled until the computation
    is complete.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dask provides interfaces for NumPy arrays, as well as the DataFrames shown in
    this recipe. There is also a machine learning interface called `dask_ml` that
    exposes similar capabilities to the `scikit-learn` package. Some external packages,
    such as `xarray`, also have a Dask interface. Dask can also work with GPUs to
    further accelerate computations and load data from remote sources, which is useful
    if the computation is distributed across a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Writing reproducible code for data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the fundamental principles of the scientific method is the idea that
    results should be reproducible and independently verifiable. Sadly, this principle
    is often undervalued in favor of “novel” ideas and results. As practitioners of
    data science, we have an obligation to do our part to make our analyses and results
    as reproducible as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Since data science is typically done entirely on computers – that is, it doesn’t
    usually involve instrumental errors involved in measurements – some might expect
    that all data science is inherently reproducible. This is certainly not the case.
    It is easy to overlook simple things such as seeding randomness (see [*Chapter
    3*](B19085_03.xhtml#_idTextAnchor078)) when using randomized hyperparameter searches
    or stochastic gradient descent-based optimization. Moreover, more subtle non-deterministic
    factors (such as use of threading or multiprocessing) can dramatically change
    results if you are not aware of them.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we’ll look at an example of a basic data analysis pipeline and
    implement some basic steps to make sure you can reproduce the results.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will need the NumPy package, imported as `np`, as usual,
    the Pandas package, imported as `pd`, the Matplotlib `pyplot` interface imported
    as `plt`, and the following imports from the `scikit-learn` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re going to simulate our data (rather than having to acquire it from elsewhere),
    so we need to set up an instance of the default random number generator with a
    seed value (for reproducibility):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: 'To generate the data, we define the following routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: We’re using this function in place of some other method of loading the data
    into Python, such as reading from a file or downloading from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow the steps below to create a very simple and reproducible data science
    pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to “load” our data using the `get_data` routine we defined previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since our data is acquired dynamically, it is a good idea to store the data
    alongside any results that we generate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to split the data into a training cohort and a testing cohort
    using the `train_test_split` routine from `scikit-learn`. We split the data 80/20
    (%) train/test, and make sure the random state is set so this can be repeated
    (although we will save the indices for reference in the next step):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we make sure that we save the indices of the training and test cohorts
    so we know precisely which observations were taken in each sample. We can use
    the indices along with the data stored in *step 2* to completely reconstruct the
    cohorts later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can set up and train the classifier. We’re using a simple `DecisionTreeClassifier`
    for this example, but this choice is not important. Since the training process
    involves some randomness, make sure to set the `random_state` keyword argument
    to seed this randomness:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before we go any further, it is a good idea to gather some information about
    the trained model and store it along with the results. The interesting information
    will vary from model to model. For this model, the feature importance information
    might be useful, so we record this in a CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can proceed to check the performance of our model. We’ll evaluate the
    model on both the training data and the test data, which we will later compare
    to the true labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Always save the results of this kind of prediction task (or regression, or
    any other final results that will in some way be part of the report). We convert
    these into `Series` objects first to make sure the indices are set correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE229]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE230]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE231]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can produce any graphics or metrics that will inform how we proceed
    with the analysis. Here, we’ll produce a confusion matrix plot for both training
    and testing cohorts and print out some accuracy summary scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE233]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE234]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE239]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE240]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE241]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE242]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE243]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE244]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE245]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting confusion matrices are shown in *Figure 10**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 - Confusion matrices for a simple classification task'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19085_10_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 - Confusion matrices for a simple classification task
  prefs: []
  type: TYPE_NORMAL
- en: 'The test results for this example are not spectacular, which should not be
    a surprise because we spent no time choosing the most appropriate model or tuning,
    and our sample size was pretty small. Producing an accurate model for this data
    was not the aim. In the current directory (wherever the script was run), there
    should be a number of new CSV files containing all the intermediate data we wrote
    to the disk: `data.csv`, `labels.csv`, `train_index.csv`, `test_index.csv`, `feature_importance.csv`,
    `train_predictions.csv`, and `test_predictions.csv`.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The are no definitive *right* answers when it comes to reproducibility, but
    there are certainly wrong answers. We’ve only touched on a few ideas of how to
    make your code more reproducible here, but there are many more things one can
    do. (See *There’s more…*). In the recipe, we really focused on storing intermediate
    values and results more than anything else. This is often overlooked in favor
    of producing plots and graphs – since these are usually the way results will be
    presented. However, we should not have to rerun the whole pipeline in order to
    change the styling of a plot. Storing intermediate values allows you to audit
    various parts of the pipeline and check that what you did was sensible and appropriate
    and that you can reproduce the results from these intermediate values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, a data science pipeline will consist of five steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data acquisition
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data preprocessing and feature selection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model and hyperparameter tuning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluation and results generation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the recipe, we replaced the data acquisition with a function that randomly
    generates data. As mentioned in the introduction, this step will usually involve
    loading data from disk (from CSV files or databases), downloading it from the
    internet, or gathering it directly from measurement devices. We cached the results
    of our data acquisition because we are assuming that this is an expensive operation.
    Of course, this is not always the case; if you load all of the data directly from
    disk (via a CSV file, for example) then there is obviously no need to store a
    second copy of this data. However, if you generate the data by querying a large
    database, then storing a flat copy of the data will dramatically improve the speed
    at which you can iterate on your pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Our preprocessing consists only of splitting the data into training and testing
    cohorts. Again, we store enough data after this step to recreate these cohorts
    independently later – we stored just the IDs corresponding to each cohort. Since
    we’re storing these sets, it isn’t totally necessary to seed the randomness in
    the `train_test_split` routine, but it is usually a good idea. If your preprocessing
    involves more intensive operations, then you might consider caching the processed
    data or the generated features that you will use in the pipeline (we will cover
    caching in more detail shortly). If your preprocessing step involves selecting
    features from the columns of your data, then you should absolutely save those
    selected features to disk alongside the results.
  prefs: []
  type: TYPE_NORMAL
- en: Our model is very simple and doesn’t have any (non-default) hyperparameters.
    If you have done some hyperparameter tuning, you should store these, along with
    any other metadata that you might need to reconstruct the model. Storing the model
    itself (via pickling or otherwise) can be useful but remember that a pickled model
    might not be readable by another party (for example, if they are using a different
    version of Python).
  prefs: []
  type: TYPE_NORMAL
- en: You should always store the numerical results from your model. It is impossible
    to compare plots and other summary figures when you’re checking that your results
    are the same on subsequent runs. Moreover, this allows you to quickly regenerate
    figures or values later should this be required. For example, if your analysis
    involves a binary classification problem, then storing the values used to generate
    a **Receiver Operating Characteristic** (**ROC**) curve is a good idea, even if
    one also produces a plot of the ROC curve and reports the area under the curve.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a lot we have not discussed here. First, let’s address an obvious point.
    Jupyter notebooks are a common medium for producing data science pipelines. This
    is fine, but users should understand that this format has several shortcomings.
    First, and probably most importantly, is the fact that Jupyter notebooks can be
    run out of order and that later cells might have non-trivial dependencies on earlier
    cells. To address this, make sure that you always run a notebook on a clean kernel
    in its entirety, rather than simply rerunning each cell in a current kernel (using
    tools such as Papermill from the *Executing a Jupyter notebook as a script* recipe,
    for example.) Second, the results stored inside the notebook might not correspond
    to the code that is written in the code cells. This happens when the notebook
    is run and the code is modified after the fact without a rerun. It might be a
    good idea to keep a master copy of the notebook without any stored results and
    make copies of this that are populated with results and are never modified further.
    Finally, Jupyter notebooks are often executed in environments where it is challenging
    to properly cache the results of intermediate steps. This is partially addressed
    by the internal caching mechanism inside the notebook, but this is not always
    totally transparent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s address two general concerns of reproducibility now: configuration and
    caching. Configuration refers to the collection of values that are used to control
    the setup and execution of the pipeline. We don’t have any obvious configuration
    values in the recipe except for the random seeds used in the `train_test_split`
    routine and the model (and the data generation, but let’s ignore this), and the
    percentage of values to take in the train/test split. These are hardcoded in the
    recipe, but this is probably not the best idea. At the very least, we want to
    be able to record the configuration used in any given run of the analysis. Ideally,
    the configuration should be loaded (exactly once) from a file and then finalized
    and cached before the pipeline runs. What this means is that the full configuration
    is loaded from one or more sources (config files, command-line arguments, or environmental
    variables), consolidated into a single source of truth, and then serialized into
    a machine- and human-readable format such as JSON alongside the results. This
    is so you know precisely what configuration was used to generate the results.'
  prefs: []
  type: TYPE_NORMAL
- en: Caching is the process of storing intermediate results so they can be reused
    later to decrease the running time on subsequent runs. In the recipe, we did store
    the intermediate results, but we didn’t build the mechanism to reuse the stored
    data if it exists and is valid. This is because the actual mechanism for checking
    and loading the cached values is complicated and somewhat dependent on the exact
    setup. Since our project is very small, it doesn’t necessarily make any sense
    to cache values. However, for larger projects that have multiple components, this
    absolutely makes a difference. When implementing a caching mechanism, you should
    build a system to check whether the cache is valid by, for example, using the
    SHA-2 hash of the code file and any data sources on which it depends.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to storing results, it is generally a good idea to store all
    the results together in a timestamped folder or similar. We don’t do this in the
    recipe, but it is relatively easy to achieve. For example, using the `datetime`
    and `pathlib` modules from the standard library, we can easily create a base path
    in which results can be stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: You must be a little careful if you are using multiprocessing to run multiple
    analyses in parallel since each new process will generate a new `RESULTS_OUT`
    global variable. A better option is to incorporate this into the configuration
    process, which would also allow the user to customize the output path.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the actual code in the script that we have discussed so far, there is
    a great deal one can do at the project level to make the code more reproducible.
    The first, and probably most important step, is to make the code available as
    far as possible, which includes specifying the license under which the code can
    be shared (if at all). Moreover, good code will be robust enough that it can be
    used for analyzing multiple data (obviously, the data should be of the same kind
    as the data originally used). Also important is making use of version control
    (Git, Subversion, and so on) to keep track of changes. This also helps distribute
    the code to other users. Finally, the code needs to be well documented and ideally
    have automated tests to check that the pipeline works as expected on an example
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: See also...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some additional sources of information about reproducible coding practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Turing Way*. Handbook on reproducible, ethical, and collaborative data
    science produced by the Alan Turing Institute. [https://the-turing-way.netlify.app/welcome](https://the-turing-way.netlify.app/welcome
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Review criteria for the Journal of Open Source Software: *Good practice guidelines
    to follow with your own code, even if it is not intended to be* *published*: [https://joss.readthedocs.io/en/latest/review_criteria.html](https://joss.readthedocs.io/en/latest/review_criteria.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes the 10th and final chapter of the book. Remember that we have
    barely scratched the surface of what is possible when doing mathematics with Python,
    and you should read the documentation and sources mentioned throughout this book
    for much more information about what these packages and techniques are capable
    of.
  prefs: []
  type: TYPE_NORMAL
