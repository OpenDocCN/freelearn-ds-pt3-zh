- en: '*Chapter 13*: Next Steps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the end of the book, and the beginning of your Dash journey! Even
    though we covered many topics, use cases, chart types, and interactivity features,
    the sky is the limit in terms of what you can build with Dash.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should be as comfortable building dashboards as you would be in
    creating presentations. You should be comfortable manipulating and expressing
    data using a variety of data visualization techniques and chart types.
  prefs: []
  type: TYPE_NORMAL
- en: But this book just sets you on the path, and there are many things to explore
    next, so we will cover some ideas and pointers on how to explore the topics we
    covered in more depth. We will also look at some aspects that weren't covered
    in the book that you might be interested in exploring.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following areas will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Expanding your data manipulation and preparation skills
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring more data visualization techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring other Dash components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating your own Dash components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operationalizing and visualizing machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing performance and using big data tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going large scale with Dash Enterprise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will not be doing any coding or deployment, so there won't be any technical
    requirements for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn new things and explore them generally from two directions: the
    top-down approach, where you want to do something, or are required to do something,
    and the bottom-up approach, where you start with the tools, and you want to explore
    the possibilities and what you can do with them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Top-down**: Because of a certain requirement or constraint, often you will
    be required to do something â€“ make something faster, better, or easier, for example.
    In order to solve these problems, or satisfy some requirement, you are required
    to learn something new. The value of this approach is mainly its practicality.
    You know what is useful and what is required, and this helps focus your mind and
    energy on the solution that you want, which is focused on solving a practical
    problem. At the same time, if you only focus on practical problems, you might
    be missing out on new techniques and approaches that you might learn that can
    make your practical life much easier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bottom-up**: This is the approach from the other direction. You start by
    learning something new just for the sake of exploration or curiosity. It could
    be something big, such as machine learning, or as small as learning about a new
    parameter in a function that you use every day. This suddenly opens up possibilities
    in your mind. It also expands your concept of what is possible. This is something
    that you can do proactively, regardless of the work requirements that you have.
    The famous quote, "The more I practice, the luckier I get," seems to fit this
    situation. The benefit of this approach is that you learn things properly and
    establish a solid theoretical understanding that allows you to be more in control
    of the techniques at hand. The drawback is that you might get too theoretical
    and lose touch with reality and forget what is really useful and what''s not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I find myself alternating between phases where sometimes I spend most of my
    time focused on solving a particular problem (top-down) and get very practical
    and produce practical solutions. I then go through a period of stagnation where
    my imagination doesn't work as much and I'm not that creative. I then slip into
    a more theoretical mode. Learning new things is very interesting and engaging
    in this phase. After learning enough new things and having established a good
    understanding of a certain topic, I find myself getting new ideas and get back
    to the practical mode, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: You can see what works for you. Let's now explore some specific topics that
    you might be interested in on your Dash journey.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding your data manipulation and preparation skills
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have read any introductory text on data science, you will have probably
    been told that data scientists spend the majority of their time cleaning data,
    reformatting it, and reshaping it.
  prefs: []
  type: TYPE_NORMAL
- en: As you have read this book, you will have probably seen this in action!
  prefs: []
  type: TYPE_NORMAL
- en: We saw several times how much code and mental effort, and importantly, domain
    knowledge, goes into just getting our data into a certain format. Once we have
    our data in a standardized format, for example, a long form (tidy) DataFrame,
    then our lives become easier.
  prefs: []
  type: TYPE_NORMAL
- en: You might want to learn more pandas and NumPy for a more complete set of techniques
    on reshaping your data however you want. As mentioned at the beginning of the
    chapter, learning new pandas techniques without a practical purpose in mind can
    help a lot in expanding your imagination. Learning regular expressions can help
    a lot in text analysis, because text is typically unstructured, and finding and
    extracting certain patterns can help a lot in your data cleaning process. Statistical
    and numeric techniques can definitely make a big difference. At the end of the
    day, we are basically crunching numbers here.
  prefs: []
  type: TYPE_NORMAL
- en: Improving data manipulation skills naturally leads us to easier and better visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring more data visualization techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw how easy it is to work with Plotly Express and how powerful it can be.
    We also saw the extensive options available for us. At the same time, we are constrained
    by the requirement to have our data in a certain format, which Plotly Express
    cannot help with. This is where we have to step in as data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: We covered four main chart types, and this is a very small subset of what's
    available. As mentioned at the beginning of the chapter, visualization works in
    a couple of ways. You might be required to produce a certain chart, so you end
    up having to learn about it. Or, you might learn about a new chart, and it then
    inspires you to better summarize certain types of data for certain use cases.
  prefs: []
  type: TYPE_NORMAL
- en: You might learn about new types of charts based on the geometric shapes/attributes
    they use, such as pie charts or dot plots. You can also explore them based on
    their usage; for example, there are statistical and financial charts. Many chart
    types boil down to basic shapes, such as dots, circles, rectangles, lines, and
    so on. The way they are displayed and combined makes them distinctive.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting visualization technique is using sub-plots. While we extensively
    used faceting in the book, facets are basically the same visualization for multiple
    subsets of the data we are analyzing. Sub-plots, on the other hand, allow you
    to create arrays of plots that could be independent of one another. This can help
    you produce rich reports in a single chart, where each sub-plot conveys a different
    aspect of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Having explored and mastered new visualization techniques and charts, you will
    probably want to put them in an app and make them interactive.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring other Dash components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We covered the basic Dash components and there are many others available. Keep
    in mind that there are three possible approaches here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dash_table`, which can offer quite complex functionality and spreadsheet-style
    options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DatePickerSingle` and `DatePickerRange`, which are self-explanatory. The `Interval`
    component allows you to execute code whenever a certain period of time elapses.
    The `Store` component allows you to store data in the user''s browser, in case
    you want to save some data to enhance the usability/functionality of your apps.
    There is also an `Upload` component for uploading files. These are all available
    under Dash Core Components. There are several other packages that are interesting
    for other use cases. For example, Dash Cytoscape is great for interactive graph
    (network) visualizations. We saw it several times when we used the visual debugger
    and saw how much it simplifies our understanding of our app. This has many applications
    for various industries. To enable users to draw on your charts, you can check
    out the image annotation options that Dash provides, as well as the Dash Canvas
    package. Together, they provide a wide array of options to let users literally
    draw on the charts with their mouse, using set shapes such as rectangles, or by
    simply dragging the mouse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explore some of the community components**: Since Dash is an open source
    project and has a mechanism for creating and incorporating new components, many
    people have created their own Dash components independently. One of those is Dash
    Bootstrap Components, which we have relied on in our work. There are many more
    and many new ones coming out all the time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This takes us to another topic, which is creating your own Dash component.
  prefs: []
  type: TYPE_NORMAL
- en: Creating your own Dash component
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's interesting to know that the official strategy for Dash is to be "React
    for Python, R, and Julia." As you might know, React is a very big JavaScript framework
    for building user interfaces. There is a massive library of open source React
    components, and Dash Core Components are basically React components made available
    in Python. This means that if there is any functionality that is not provided
    by Dash that you would like to have, you might consider developing it yourself,
    hiring a developer to build it, or you can also sponsor its development and have
    the Plotly team build it. Some of the components that we worked with were sponsored
    by clients who wanted to have certain functionality that wasn't available. This
    is one way to support Dash as well. It also benefits everyone who uses open source
    Dash.
  prefs: []
  type: TYPE_NORMAL
- en: There are clear instructions on how to create your own Dash components, and
    as a Dash developer, it's good to explore this option. It will certainly give
    you a deeper understanding of how the library works, and maybe you will end up
    creating a popular component yourself!
  prefs: []
  type: TYPE_NORMAL
- en: With all the data manipulation, visualization, and components, you have a rich
    vocabulary to do things beyond plotting points on a chart. Exploring what can
    be done with machine learning can give your models a big boost and can make them
    usable by others.
  prefs: []
  type: TYPE_NORMAL
- en: Operationalizing and visualizing machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning and deep learning are completely separate topics, of course,
    but with all the previously mentioned skills, you can take your machine learning
    to a new level. At the end of the day, you will use charts to express certain
    ideas about your data, and with a good interactive data visualization vocabulary,
    you can give your users many options to test different models and tune hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing performance and using big data tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a very important topic, and we always need to make sure that our apps
    perform at an acceptable level. We didn't tackle this in the book because the
    focus was mainly to learn how to create a Dash app with all the other details
    that make it work. We also worked with a very small dataset of a few megabytes.
    Still, even with a small dataset, it can be crucial to optimize it. Big data can
    be about handling a massive file, or it can be about a small file that needs to
    be handled a massive number of times.
  prefs: []
  type: TYPE_NORMAL
- en: These are some things that can be done to optimize performance, but big data
    is a separate topic altogether, so here are some hints and some areas to explore.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we know how our app will behave and what features we will be using, we
    can clean up some unnecessary code and data that might be hindering our app''s
    performance. Here are some ideas that can be done immediately to our app:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load the necessary data only**: We loaded the whole file, and for each callback,
    we queried the DataFrame separately. That can be wasteful. If we have a callback
    for population data only, for example, we can create a separate file (and then
    a separate subset) DataFrame that only contains relevant columns and query them
    only, instead of using the whole DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize data types**: Sometimes you need to load data that contains the
    same values repeated many times. For example, the poverty dataset contains many
    repetitions of country names. We can use the pandas categorical data type to optimize
    those values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Load the `sys` module and see the difference in size in bytes for a string
    (a country name) and an integer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the size of an integer value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see the big difference in size with the string taking almost three times
    the memory that an integer does. This is what the categorical data type basically
    does. It creates a dictionary mapping each unique value to an integer. It then
    uses integers to encode and represent those values, and you can imagine how much
    space this can save.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the poverty dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get a subset containing the country names column and check its memory usage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the column to the categorical data type and check the memory usage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With a simple command that encoded our countries as integers, we reduced the
    memory usage from 64.9 KB to 21.8 KB, making it about a third of the original
    size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another thing you might want to consider is to learn more about the available
    big data technologies and techniques. One of the most important projects right
    now is the Apache Arrow project. It is a collaboration between leaders from the
    database community, as well as from the data science community. One of the most
    important objectives of the project is to unify the effort across disciplines,
    and crucially, across programming languages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you want to read a CSV file, for example, you want it represented in memory
    as a DataFrame. Whether you are using R or Python, or any other language, you
    will be running very similar operations, such as sorting, selecting, filtering,
    and so on. There is a lot of effort duplicated, where each language implements
    its own DataFrame spec. From a performance perspective, it has been observed that
    a large percentage of computing power is wasted on converting objects from one
    language to another and reading and writing. This can also happen while saving
    an object to disk and then opening it in another language. This can cause a waste
    of resources, and in many cases forces many teams to have to choose a single language
    for easier communication and to reduce the wasted time and effort.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the goals of the Apache Arrow project is to create a single in-memory
    representation for data objects such as the DataFrame. This way, objects can be
    passed around across programming languages without having to make any conversions.
    You can imagine how much easier things can become. Also, there are big wins due
    to the collaborations across the programming languages and disciplines where a
    single specification is being used and maintained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each programming language can then implement its own libraries that are based
    on a single spec. For Python, the package is `pyarrow`, which is very interesting
    to explore. In many instances, it can be used on its own, and in others, it can
    integrate with pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A very interesting file format that is also part of the project is the `parquet`
    format. Just like CSV and JSON, `parquet` is language-agnostic. It is a file and
    can be opened with any language that has a `parquet` reader. And the good news
    is that pandas already supports it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the important features of `parquet` is massive compression that can reduce
    the size of your files drastically. So, it is ideal for long-term storage and
    efficiently utilizing space. Not only that, but it is very efficient to open and
    read those files because of the format. The file contains metadata about the file,
    as well as the schema. Files are also arranged into separate structures for efficient
    reading.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some techniques that `parquet` uses are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`parquet` mainly stores data in columns. Row-oriented formats are suitable
    for transactional processing. For example, when a user logs in to a website, we
    need to retrieve data about the user (a row), and potentially we want to write
    and update that row based on that user''s interactions. But in analytical processing,
    which is what we are interested in, if we want to analyze the average income per
    country, for example, we only need to read two columns from our dataset. If the
    data was arranged in columns, then we can jump from the beginning to the end of
    the column and extract it much faster than if it was a row. We can read other
    columns, but only if we want to do further analysis on them.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parquet` also performs dictionary encoding. There are several other encoding
    schemes that it uses. For example, there is delta encoding, which works best when
    you have large numbers. It saves the value of the first number in the column and
    only saves the difference between it and consecutive numbers. For example, if
    you had the following list: [1,000,000, 1,000,001, 1,000,002, 1,000,003], these
    numbers could be represented as [1,000,000, 1, 1, 1]. We saved the full value
    of the first element and only the difference between each element and the previous
    one. This can mean a lot of memory is saved. This can be really useful when using
    timestamps, for example, which can be represented as large integers with small
    differences between them, especially in time series data. When you want to read
    the list, the program can do the calculations and give you the original numbers.
    Several other encoding strategies are used, but this was just another example.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parquet` can split a file into multiple files and read and merge them from
    a folder that contains those files. Imagine a file with data about people, and
    this file has 10 million rows. One of the columns could be for gender, with "male"
    and "female" values. Now, if we split the file in two, with one for each value,
    we don''t even need to store the whole column anymore. We can simply include "female"
    in the filename, and if the column is requested, the program knows how to populate
    all the values for that column because they are all the same.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parquet` uses for the groups of data in columns is that it contains the minimum
    and maximum and some other statistics for each group. Imagine having a file with
    10 million rows, and you want to read only the values that are between 10 and
    20\. Assume that those rows are split into groups of one million each. Now, each
    group would have its minimum and maximum values in the header of that group. While
    scanning, if you come across a group where the maximum is six, then you know that
    your requested values won''t be in that group. You just skipped a million values
    by making a single comparison.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, if you have mastered all those techniques and can produce insightful visualizations,
    nice interactivity, and provide really helpful dashboards, you still might not
    have the experience (or desire) to handle Dash in large-scale deployments. This
    is where you might consider Dash Enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: Going large scale with Dash Enterprise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you have a deployment in a large organization with many users that has
    an existing infrastructure, several things might come up that you might not have
    considered or anticipated. Imagine your app is going to be used by hundreds of
    people in a company. How do you deal with access to the app? How do you manage
    passwords, and what exactly happens when someone resigns, or joins? Are you experienced
    enough in security that you are confident that you can handle such a large deployment?
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, data engineering takes on a much bigger role than previously.
    Storing the data efficiently and securely becomes more important. Scalability
    and managing it can be tricky if it is not your area of expertise. Your main job
    is to design and create something that helps in finding insights, rather than
    maintaining large-scale apps. In some cases, you might have the required skills
    but don't want to worry about those things and mainly want to focus on the interface,
    the models, and the visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: This is where Dash Enterprise can help. It's basically Dash, as you know it,
    but with many options that are specifically designed for large deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Dash Enterprise also offers a full online workbench with the popular IDEs and
    notebooks, so you can also collaborate online with co-workers. This might help
    if a large number of users are working together.
  prefs: []
  type: TYPE_NORMAL
- en: Professional services are also provided to large customers, in which case you
    get access to the people who built Dash and have worked with many organizations
    who have gone through many experiences similar to your own, and you can get the
    help you need from them in many important areas.
  prefs: []
  type: TYPE_NORMAL
- en: These were some ideas, but your creativity, domain knowledge, and hard work
    are what matter at the end, so let's summarize what we covered in this chapter
    and the book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started by focusing on the importance of basic skills in handling data. We
    emphasized the importance of mastering data manipulation and cleaning skills,
    which will allow you to format your data into the shape that you want and make
    it easy to analyze and visualize. We also mentioned various data visualization
    techniques and types of charts that can be explored to increase your fluency in
    expressing ideas visually.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed the other Dash components that we didn't cover in the book,
    as well as the community components that are constantly being developed. Eventually,
    you might decide to develop your own set of components and contribute additional
    functionality to the Dash ecosystem, and I look forward to `pip install dash-your-components`!
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed exploring machine learning and how we can make our models
    visual and interactive. Establishing data manipulation, visualization, and interactivity
    skills will help you a lot in making your models interpretable and usable, especially
    for a non-technical audience.
  prefs: []
  type: TYPE_NORMAL
- en: We explored some big data options and discussed one of the important projects,
    although there are many more to consider and explore.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we talked about the paid enterprise solution that Dash offers, which
    is Dash Enterprise; this solution might make sense when your project is part of
    a large organization or deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you very much for reading, and I hope you enjoyed the book. I look forward
    to seeing your app deployed online, with your own design, models, options, and
    customizations.
  prefs: []
  type: TYPE_NORMAL
