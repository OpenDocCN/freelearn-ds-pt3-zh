<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Python for Parallel Computing
                </header>
            
            <article>
                
<p class="mce-root">This chapter covers parallel computing and the module <kbd>mpi4py</kbd>. Complex and time-consuming computational tasks can often be divided into subtasks, which can be carried out simultaneously if there is capacity for it. When these subtasks are independent of each other, executing them in parallel can be e<span>specially </span>efficient. Situations where subtasks have to wait until another subtask is completed are less suited for parallel computing.</p>
<p>Consider the task of computing an integral of a function by a quadrature rule:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/83c9cea1-a108-4274-9dd2-2e25699390c0.png" style="width:19.83em;height:2.83em;"/></p>
<p>with <sub><img class="fm-editor-equation" src="assets/28ebd3f6-beef-4f20-b5f7-ac37ea65cc3f.png" style="width:13.92em;height:1.17em;"/></sub>. If the evaluation of <sub><img class="fm-editor-equation" src="assets/66a83318-be85-4810-b4d1-75b12ccc843c.png" style="width:0.75em;height:1.42em;"/></sub> <span>is time-consuming and <sub><img class="fm-editor-equation" src="assets/428d64f1-b6c7-491f-b9b4-fb7a1c4c9aaf.png" style="width:1.17em;height:1.08em;"/></sub> is large , </span>it would be advantageous to split the problem into two or several subtasks of smaller size:</p>
<p class="CDPAlignLeft CDPAlign">                                                       <img src="assets/ad5c2d76-a7b7-4216-b4c2-6ac07c065871.png" style="width:40.25em;height:5.25em;"/></p>
<p>We can use several computers and give each of them the necessary information so that they can perform their subtasks, or we can use a single computer with a so-called multicore architecture.</p>
<p>Once the subtasks are accomplished the results are communicated to the computer or processor that controls the entire process and performs the final additions.</p>
<p>We will use this as a guiding example in this chapter while covering the following topics:</p>
<ul>
<li>Multicore computers and computer clusters</li>
<li>Message passing interface (MPI)</li>
</ul>
<h1 id="uuid-75f9f8ce-49ad-491b-938c-6df14358ca04">18.1 Multicore computers and computer clusters</h1>
<p>Most of the modern computers are multicore computers. For example, the laptop used when writing this book has an Intel® i7-8565U processor that has four cores with two threads each.</p>
<p>What does this mean? Four cores on a processor allow performing four computational tasks in parallel. Four cores with two threads each are often counted as eight CPUs by system monitors. For the purposes of this chapter only the number of cores matters.</p>
<p>These cores share a common memory—the RAM of your laptop—and have individual memory in the form of cache memory:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/01e8633e-4624-46a6-89ec-57fe13f53752.png" style="width:16.67em;height:14.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 18.1: A multicore architecture with shared and local cache memory</div>
<p>The cache memory is used <span>optimally </span>by its core and is accessed at high speed, while the shared memory can be accessed by all cores of one CPU. On top, there is the computer's RAM memory and finally, the hard disk, which is also shared memory.</p>
<p>In the next section, we will see how a computational task can be distributed to individual cores and how results are received and further processed, for example, being stored in a file.</p>
<p>A different setting for parallel computing is the use of a computer cluster. Here, a task is divided into parallelizable subtasks that are sent to different computers, sometimes even over long distances. Here, communication time can matter substantially. The use of such a computer cluster makes sense <span>only </span>if the time for processing subtasks is large in relation to communication time.</p>
<h1 id="uuid-a049cfbd-e989-49fe-82c3-6fc5935aeb08">18.2 Message passing interface (MPI)</h1>
<p>Programming for several cores or on a computer cluster with distributed memory requires special techniques. We describe here <em>message passing</em> and related tools standardized by the MPI standard. These tools are similar in different programming languages, such as C, C++, and FORTRAN, and are realized in Python by the module <kbd>mpi4py</kbd>.</p>
<h2 id="uuid-4bc6fff1-2935-476d-ab44-78dda4db2a02" class="mce-root">18.2.1 Prerequisites</h2>
<p class="mce-root">You need to install this module first by executing the following in a terminal window:</p>
<pre>conda install mpi4py</pre>
<p>The module is imported by adding the following line to your Python script:</p>
<pre>import mpi4py as mpi</pre>
<p>The execution of a parallelized code is done from a terminal with the command <kbd>mpiexec</kbd>. Assuming that your code is stored in the file <kbd>script.py</kbd>, executing this code on a computer with a four-core CPU is done in the terminal window by running the following command:</p>
<pre>mpiexec -n 4 python script.py</pre>
<p>Alternatively, to execute the same script on a cluster with two computers, run the following in a terminal window:</p>
<pre>mpiexec --hostfile=hosts.txt python script.py</pre>
<p>You have to provide a file <kbd>hosts.txt</kbd> containing the names or IP addresses of the computers with the number of their cores you want to bind to a cluster:</p>
<pre># Content of hosts.txt<br/>192.168.1.25 :4 # master computer with 4 cores<br/>192.168.1.101:2 # worker computer with 2 cores</pre>
<p>The Python script, here <kbd>script.py</kbd>, has to be copied to all computers in the cluster.</p>
<h1 id="uuid-1e284496-6d19-43df-8abc-0a5633ca9e1b">18.3 Distributing tasks to different cores</h1>
<p>When executed on a multicore computer, we can think of it that <kbd>mpiexec</kbd> copies the given Python script to the number of cores and runs each copy. As an example, consider the one-liner script <kbd>print_me.py</kbd> with the command <kbd>print("Hello it's me")</kbd>, that, when executed with <kbd>mpiexec -n 4 print_me.py</kbd>, generates the same message on the screen <span>four times</span>, each sent from a different core.</p>
<p>In order to be able to execute different tasks on different cores, we have to be able to distinguish these cores in the script.</p>
<p>To this end, we create a so-called communicator instance, which organizes the communication between the <em>world</em>, that is, the input and output units like the screen, the keyboard, or a file, and the individual cores. Furthermore, the individual cores are given identifying numbers, called a rank:</p>
<pre>from mpi4py import MPI<br/>comm=MPI.COMM_WORLD  # making a communicator instance<br/>rank=comm.Get_rank() # querrying for the numeric identifyer of the core <br/>size=comm.Get_size() # the total number of cores assigned</pre>
<p>The communicator attribute size refers to the total number of processes specified in the <span>statement </span><kbd>mpiexec</kbd>.</p>
<p>Now we can give every core an individual computational task, as in the next script, which we might call <kbd>basicoperations.py</kbd>:</p>
<pre><br/>from mpi4py import MPI<br/>comm=MPI.COMM_WORLD # making a communicator instance<br/>rank=comm.Get_rank() # querrying for the numeric identifyer of the core <br/>size=comm.Get_size() # the total number of cores assigned <br/>a=15<br/>b=2<br/>if rank==0:<br/>    print(f'Core {rank} computes {a}+{b}={a+b}')<br/>if rank==1:<br/>    print(f'Core {rank} computes {a}*{b}={a*b}')<br/>if rank==2:<br/>    print(f'Core {rank} computes {a}**{b}={a**b}')</pre>
<p>This script is executed in the terminal by entering the following command:</p>
<pre>mpiexec -n 3 python basicoperations.py</pre>
<p>We obtain three messages:</p>
<pre>Core 0 computes 15+2=17<br/>Core 2 computes 15**2=225<br/>Core 1 computes 15*2=3</pre>
<p>All three processes got their individual tasks, which were executed in parallel. Clearly, printing the result to the screen is a bottleneck as the screen is shared by all three processes.</p>
<p>In the next section, we see how communication between the processes is done.</p>
<h2 id="uuid-5dc704ae-3ca9-404d-a31a-600c124832b7">18.3.1 Information exchange between processes</h2>
<p>There are different ways to send and receive information between processes:</p>
<ul>
<li>Point-to-point communication</li>
<li>One-to-all and all-to-one</li>
<li>All-to-all</li>
</ul>
<p>In this section, we will introduce point-to-point, one-to-all, and all-to-one communication.</p>
<p>Speaking to a neighbor and letting information pass along a street this way is an example from daily life of the first communication type from the preceding list, while the second can be illustrated by the daily news, spoken by one person and broadcast to a big group of listeners.One-to-all and all-to-one communication</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/13f24ef2-b180-4b43-b9b3-8498ba57e23a.png" style="width:22.75em;height:3.42em;"/>                    <img src="assets/1aea42da-4134-4564-beab-a2904d20fdf9.png" style="width:18.00em;height:13.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 18.2: Point-to-point communication and one-to-all communication</div>
<p>In the next subsections, we will study these different communication types in a computational context.</p>
<h2 id="uuid-15bc427f-8e9c-4315-99cf-e2f9aa84fd1a">18.3.2 Point-to-point communication</h2>
<p>Point-to-point communication directs information flow from one process to a designated receiving process. We first describe the methods and features by considering a ping-pong situation and a telephone-chain situation and explain the notion of blocking.</p>
<p>Point-to-point communication is applied in scientific computing, for instance in random-walk or particle-tracing applications on domains that are divided into a number of subdomains corresponding to the number of processes that can be carried out in parallel.</p>
<p>The ping-pong example assumes that we have two processors sending an integer back and forth to each other and increasing its value by one.</p>
<p>We start by creating a communicator object and checking that we have two processes available:</p>
<pre>from mpi4py import MPI<br/>comm=MPI.COMM_WORLD # making a communicator instance<br/>rank=comm.Get_rank() # querying for the numeric identifier of the core <br/>size=comm.Get_size() # the total number of cores assigned <br/>if not (size==2):<br/>    raise Exception(f"This examples requires two processes. \<br/>                    {size} processes given.")</pre>
<p>Then we send information back and forth between the two processes:</p>
<pre>count = 0<br/>text=['Ping','Pong']<br/>print(f"Rank {rank} activities:\n==================")<br/>while count &lt; 5:<br/>    if rank == count%2:<br/>        print(f"In round {count}: Rank {rank} says {text[count%2]}""<br/>               "and sends the ball to rank {(rank+1)%2}")<br/>        count += 1<br/>        comm.send(count, dest=(rank+1)%2)<br/>    elif rank == (count+1)%2:<br/>        count = comm.recv(source=(rank+1)%2)</pre>
<p>Information is sent by the method <kbd>send</kbd> of the communicator. Here, we provided it with the information that we want to send, along with the destination. The communicator takes care that the destination information is translated to a hardware address; either one core of the CPU in your machine or that of a host machine.</p>
<p>The other machine receives the information by the communicator method <kbd>comm.recv</kbd>. It requires information on where the information is expected from. Under the hood, it tells the sender that the information has been received by freeing the information buffer on the data channel. The sender awaits this signal before it can proceed.</p>
<p>The two statements <kbd>if rank == count%2</kbd> and <kbd>elif rank == (count+1)%2</kbd> ensure that the processors alternate their sending and receiving tasks.</p>
<p>Here is the output of this short script that we saved in a file called <kbd>pingpong.py</kbd> and executed with the following:</p>
<pre>mpiexec -n 2 python pingpong.py </pre>
<p>In the terminal, this produces the following output:</p>
<pre>Rank 0 activities:<br/>==================<br/>In round 0: Rank 0 says Ping and sends the ball to rank 1<br/>In round 2: Rank 0 says Ping and sends the ball to rank 1<br/>In round 4: Rank 0 says Ping and sends the ball to rank 1<br/>Rank 1 activities:<br/>==================<br/>In round 1: Rank 1 says Pong and sends the ball to rank 0<br/>In round 3: Rank 1 says Pong and sends the ball to rank 0</pre>
<p>What types of data can be sent or received? As the <span>commands </span><kbd>send</kbd> and <kbd>recv</kbd> communicate data in binary form, they <kbd>pickle</kbd> the data first (see <a href="f95f92d6-d8d1-46a6-bb5b-560714044c70.xhtml">Section 14.3</a>: <em>Pickling</em>). Most of the Python objects can be pickled, but not <kbd>lambda</kbd> functions for instance. It is also possible to pickle buffered data such as NumPy arrays, but a direct send of buffered data is more efficient, as we'll see in the next subsection.</p>
<p>Note that there might be reasons for sending and receiving functions between processes. As the <span>methods </span><kbd>send</kbd> and <kbd>recv</kbd>  <span>only </span>communicate references to functions, the references have to exist on the sending and receiving processors. Therefore the following Python script returns an error:</p>
<pre>from mpi4py import MPI<br/>comm=MPI.COMM_WORLD # making a communicator instance<br/>rank=comm.Get_rank() # querying for the numeric identifier of the core<br/>size=comm.Get_size() # the total number of cores assigned<br/><br/>if rank==0:<br/>    def func():<br/>        return 'Function called'<br/>    comm.send(func, dest=1)<br/>if rank==1:<br/>    f=comm.recv(source=0)    # &lt;&lt;&lt;&lt;&lt;&lt; This line reports an error<br/>    print(f())One-to-all and all-to-one communication</pre>
<p>The error message thrown by the <span>statement </span><kbd>recv</kbd> is <kbd>AttributeError: Can't get attribute 'func'</kbd>. This is caused by the fact that <kbd>f</kbd> refers to the function <kbd>func</kbd>, which is not defined for the processor with rank 1. The correct way is to define this function for both processors:</p>
<pre>from mpi4py import MPI<br/>comm=MPI.COMM_WORLD # making a communicator instance<br/>rank=comm.Get_rank() # querying for the numeric identifier of the core<br/>size=comm.Get_size() # the total number of cores assigned<br/><br/>def func():<br/>    return 'Function called'<br/>if rank==0:<br/>    comm.send(func, dest=1)<br/>if rank==1:<br/>    f=comm.recv(source=0) <br/>    print(f())</pre>
<h2 id="uuid-01b22b50-dbba-4d7d-9508-d0a1644ad6fc" class="mce-root">18.3.3 Sending NumPy arrays</h2>
<p>The <span>commands </span><kbd>send</kbd> and <kbd>recv</kbd> are high-level commands. That means they do under-the-hood work that saves the programmer time and avoids possible errors. They allocate memory after having internally deduced the datatype and the amount of buffer data needed for communication. This is done internally on a lower level based on C constructions.</p>
<p>NumPy arrays are objects that themselves make use of these C-buffer-like objects, so when sending and receiving NumPy arrays you can gain efficiency by using them in the lower-level communication counterparts <kbd>Send</kbd> and <kbd>Recv</kbd> (mind the capitalization!).</p>
<p>In the following example, we send an array from one processor to another:</p>
<p> </p>
<pre>from mpi4py import MPI<br/>comm=MPI.COMM_WORLD # making a communicator instance<br/>rank=comm.Get_rank() # querying for the numeric identifier of the core<br/>size=comm.Get_size() # the total number of cores assigned<br/>import numpy as np<br/><br/>if rank==0:<br/>    A = np.arange(700)<br/>    comm.Send(A, dest=1)<br/>if rank==1:<br/>    A = np.empty(700, dtype=int)  # This is needed for memory allocation <br/>                                  # of the buffer on Processor 1<br/>    comm.Recv(A, source=0)        # Note, the difference to recv in <br/>                                  # providing the data.<br/>    print(f'An array received with last element {A[-1]}')<br/> </pre>
<p>It is important to note, that on both processors, memory for the buffer has to be allocated. Here, this is done by creating on Processor 0 an array with the data and on Processor 1 an array with the same size and datatype but arbitrary data.</p>
<p>Also, we see a difference in the <span>command </span><kbd>recv</kbd> in the output. The command <kbd>Recv</kbd> returns the buffer via the first argument. This is possible as NumPy arrays are mutable.</p>
<h3 id="uuid-1861ca2c-cbef-42c8-838e-2356ca315244">18.3.4 Blocking and non-blocking communication</h3>
<p>The <span>commands </span><kbd>send</kbd> and <kbd>recv</kbd> and their buffer counterparts <kbd>Send</kbd> and <kbd>Recv</kbd> are so-called blocking commands. That means a <span>command </span><kbd>send</kbd> is completed when the corresponding send buffer is freed. When this will happen depends on several factors such as the particular communication architecture of your system and the amount of data that is to be communicated. Finally, the <span>command </span><kbd>send</kbd> is considered to be freed when the corresponding command <kbd>recv</kbd> has got all the information. Without such a <span>command </span><kbd>recv</kbd>, it will wait forever. This is called a deadlock situation.</p>
<p>The following script demonstrates a situation with the potential for deadlock. Both processes send simultaneously. If the amount of data to be communicated is too big to be stored the <span>command </span><kbd>send</kbd> is waiting for a corresponding <kbd>recv</kbd> to empty the pipe, but <kbd>recv</kbd> never is invoked due to the waiting state. That's a deadlock.</p>
<pre>from mpi4py import MPI<br/>comm=MPI.COMM_WORLD # making a communicator instance<br/>rank=comm.Get_rank() # querrying for the numeric identifier of the core<br/>size=comm.Get_size() # the total number of cores assigned<br/><br/><br/>if rank==0:<br/>    msg=['Message from rank 0',list(range(101000))]<br/>    comm.send(msg, dest=1)<br/>    print(f'Process {rank} sent its message')<br/>    s=comm.recv(source=1)<br/>    print(f'I am rank {rank} and got a {s[0]} with a list of \<br/>          length {len(s[1])}')<br/>if rank==1:<br/>    msg=['Message from rank 1',list(range(-101000,1))]<br/>    comm.send(msg,dest=0)<br/>    print(f'Process {rank} sent its message')<br/>    s=comm.recv(source=0)<br/>    print(f'I am rank {rank} and got a {s[0]} with a list of \<br/>          length {len(s[1])}')</pre>
<p class="mce-root">Note that executing this code might not cause a deadlock on your computer as the amount of data communicated is very small.</p>
<p class="mce-root">The straightforward remedy to avoid a deadlock, in this case, is to swap the order of the <span>commands </span><kbd>recv</kbd> and <kbd>send</kbd> on <em>one</em> of the processors:</p>
<div>
<pre>from mpi4py import MPI<br/>comm=MPI.COMM_WORLD # making a communicator instance<br/>rank=comm.Get_rank() # querrying for the numeric identifier of the core<br/>size=comm.Get_size() # the total number of cores assigned<br/><br/><br/>if rank==0:<br/>    msg=['Message from rank 0',list(range(101000))]<br/>    comm.send(msg, dest=1)<br/>    print(f'Process {rank} sent its message')<br/>    s=comm.recv(source=1)<br/>    print(f'I am rank {rank} and got a {s[0]} with a list of \<br/>          length {len(s[1])}')<br/>if rank==1:<br/>    s=comm.recv(source=0)<br/>    print(f'I am rank {rank} and got a {s[0]} with a list of \<br/>          length {len(s[1])}')<br/> msg=['Message from rank 1',list(range(-101000,1))]<br/>    comm.send(msg,dest=0)<br/>    print(f'Process {rank} sent its message')<br/>    print(f'I am rank {rank} and got a {s[0]} with a list of \<br/>          length {len(s[1])}')</pre></div>
<h2 id="uuid-14c4da56-1bd5-48a6-ab70-0030f9b0626e">18.3.5 One-to-all and all-to-one communication</h2>
<p>When a complex task depending on a larger amount of data is divided into subtasks, the data also has to be divided into portions relevant to the related subtask and the results have to be assembled and processed into a final result.</p>
<p>Let's consider as an example the scalar product of two vectors <img class="fm-editor-equation" src="assets/386ea4fe-1822-4d43-b6e6-60222a9b9311.png" style="width:4.75em;height:1.42em;"/> divided into <img class="fm-editor-equation" src="assets/4093f1e5-0763-4c6d-9c86-3d791dbe381b.png" style="width:1.17em;height:0.83em;"/> subtasks:</p>
<p class="CDPAlignLeft CDPAlign">                                  <img src="assets/2930f29a-640b-4af9-90a7-4ce61699133f.png" style="width:36.67em;height:8.67em;"/></p>
<p>with <img class="fm-editor-equation" src="assets/83232989-a3d1-4bce-8ab1-8c511977a52d.png" style="width:8.58em;height:1.33em;"/> All subtasks perform the same operations on portions of the initial data, the results have to be summed up, and possibly any remaining operations have to be carried out.</p>
<p>We have to perform the following steps:</p>
<ol>
<li>Creating the vectors <kbd>u</kbd> and <kbd>v</kbd></li>
<li>Dividing them into <em>m</em> subvectors with a balanced number of elements, that is, <sub><img class="fm-editor-equation" src="assets/dff7d037-6f3f-4a7e-91a6-866460521eb9.png" style="width:1.75em;height:1.17em;"/></sub> elements if <kbd>N</kbd> is divisible by <kbd>m</kbd>, otherwise some subvectors have more elements</li>
<li>Communicating each subvector to "its" processor</li>
<li>Performing the scalar product on the subvectors on each processor</li>
<li>Gathering all results</li>
<li>Summing up the results</li>
</ol>
<p><em>Steps 1</em>, <em>2</em>, and <em>6</em> are run on one processor, the so-called <em>root</em> processor. In the following example code, we choose the processor with rank 0 for these tasks. <em>Steps 3, 4,</em> and <em>5</em> are executed on all processors, including the root processor. For the communication in S<em>tep 3</em>, <kbd>mpi4py</kbd> provides the command <kbd>scatter</kbd>, and for recollecting the results the command <kbd>gather</kbd> is available.</p>
<h3 id="uuid-dcda5b57-4547-45c9-b235-3df9bc096dcb">Preparing the data for communication</h3>
<p>First, we will look into S<em>tep 2</em>. It is a nice exercise to write a script that splits a vector into <em>m</em> pieces with a balanced number of elements. Here is one suggestion for such a script, among many others:</p>
<pre>def split_array(vector, n_processors):<br/> # splits an array into a number of subarrays <br/> # vector one dimensional ndarray or a list<br/> # n_processors integer, the number of subarrays to be formed<br/> <br/>    n=len(vector)<br/>    n_portions, rest = divmod(n,n_processors) # division with remainder<br/>    # get the amount of data per processor and distribute the res on<br/>    # the first processors so that the load is more or less equally <br/>    # distributed<br/>    # Construction of the indexes needed for the splitting<br/>    counts = [0]+ [n_portions + 1 \<br/>                 if p &lt; rest else n_portions for p in range(n_processors)]<br/>    counts=numpy.cumsum(counts)<br/>    start_end=zip(counts[:-1],counts[1:]) # a generator<br/>    slice_list=(slice(*sl) for sl in start_end) # a generator comprehension<br/>    return [vector[sl] for sl in slice_list] # a list of subarrays</pre>
<p>As this chapter is one of the last in this book we have seen a lot of tools that can be used for this code. We worked with NumPy's cumulative sum, <kbd>cumsum</kbd>. We used the generator <kbd>zip</kbd>, unpacking arguments by the <span>operator </span><kbd>*</kbd>, and generator comprehension. We also tacitly introduced the data type <kbd>slice</kbd>, which allows us to do the splitting step in the last line in a very compact way.</p>
<h3 id="uuid-e861a08a-4fa0-4d83-9faa-481c97a02e16">The commands – scatter and gather</h3>
<p>Now we are ready to look at the entire script for our demo problem, the scalar product:</p>
<pre>from mpi4py import MPI<br/>import numpy as np<br/><br/>comm = MPI.COMM_WORLD<br/>rank = comm.Get_rank()<br/>nprocessors = comm.Get_size()<br/>import splitarray as spa <br/><br/>if rank == 0:<br/>    # Here we generate data for the example<br/>    n = 150<br/>    u = 0.1*np.arange(n)<br/>    v = - u<br/>    u_split = spa.split_array(u, nprocessors)<br/>    v_split = spa.split_array(v, nprocessors)<br/>else:<br/>    # On all processor we need variables with these names,<br/>    # otherwise we would get an Exception "Variable not defined" in <br/>    # the scatter command below<br/>    u_split = None<br/>    v_split = None<br/># These commands run now on all processors<br/>u_split = comm.scatter(u_split, root=0) # the data is portion wise <br/>                                        # distributed from root<br/>v_split = comm.scatter(v_split, root=0)<br/># Each processor computes its part of the scalar product<br/>partial_dot = u_split@v_split<br/># Each processor reports its result back to the root<br/>partial_dot = comm.gather(partial_dot,root=0)<br/><br/>if rank==0:<br/>    # partial_dot is a list of all collected results<br/>    total_dot=np.sum(partial_dot)<br/>    print(f'The parallel scalar product of u and v'<br/>        f'on {nprocessors} processors is {total_dot}.\n'<br/>        f'The difference to the serial computation is \<br/>        {abs(total_dot-u@v)}')</pre>
<p>If this script is stored in a file <kbd>parallel_dot.py</kbd> the command for execution with five processors is the following:</p>
<pre>mexec -n 5 python parallel_dot.py</pre>
<p>The result in this case is as follows:</p>
<pre class="mce-root">The parallel scalar product of u and v on 5 processors is -11137.75.<br/>The difference to the serial computation is 0.0</pre>
<p>This example demonstrates the use of <kbd>scatter</kbd> to send out specific information to each processor. To use this command the root processor has to provide a list with as many elements as available processors. Each element contains the data to be communicated to one of the processors including the root processor.</p>
<p>The reversing process is <kbd>gather</kbd>. When all processors completed this command the root processor is provided with a list with as many elements as available processors, each containing the resulting data of its corresponding processor.</p>
<p>In the final step, the root processor <span>again </span>works alone by postprocessing this result list. The example above it sums all list elements and displays the result.</p>
<p>The art of parallel programming is to avoid bottlenecks. Ideally, all processors should be busy and should start and stop simultaneously. That is why the workload is distributed more or less equally to the processors by the script <kbd>splitarray</kbd> that we described previously. Furthermore, the code should be organized in such a way that the start and end periods with the root processor working alone are short compared to the computationally intense part carried out by all processors simultaneously.</p>
<h3 id="uuid-b999b8b4-6aea-4068-a455-263a432c8587">A final data reduction operation – the command reduce</h3>
<p>The parallel scalar product example is typical for many other tasks in the way how results are handled: the amount of data coming from all processors is reduced to a single number in the last step. Here, the root processor sums up all partial results from the processors. The command <kbd>reduce</kbd> can be efficiently used for this task. We modify the preceding code by letting <kbd>reduce</kbd> do the gathering and summation in one step. Here, the last lines of the preceding code are modified in this way:</p>
<pre>......... modification of the script above .....<br/># Each processor reports its result back to the root<br/># and these results are summed up<br/>total_dot = comm.reduce(partial_dot, op=MPI.SUM, root=0)<br/><br/>if rank==0:<br/>   print(f'The parallel scalar product of u and v'<br/>         f' on {nprocessors} processors is {total_dot}.\n'<br/>         f'The difference to the serial computation \<br/>         is {abs(total_dot-u@v)}') </pre>
<p>Other frequently applied reducing operations are:</p>
<ul>
<li><kbd>MPI.MAX</kbd> or <kbd>MPI.MIN</kbd>: The maximum or minimum of the partial results</li>
<li><kbd>MPI.MAXLOC</kbd> or <kbd>MPI.MINLOC</kbd>: The argmax or argmin of the partial results</li>
<li><kbd>MPI.PROD</kbd>: The product of the partial results</li>
<li><kbd>MPI.LAND</kbd> or <kbd>MPI.LOR</kbd>: The logical and/logical or of the partial results</li>
</ul>
<h3 id="uuid-f6b02fd9-f93d-4f00-8106-706e23e6e852">Sending the same message to all</h3>
<p>Another collective command is the broadcasting command <kbd>bcast</kbd>. In contrast to <kbd>scatter</kbd> it is used to send the same data to all processors. Its call is similar to that of <kbd>scatter</kbd>:</p>
<pre>data = comm.bcast(data, root=0)</pre>
<p>but it is the total data and not a list of portioned data that is sent. Again, the root processor can be any processor. It is the processor that prepares the data to be broadcasted.</p>
<h3 id="uuid-b210c0cf-fb0f-4a50-a243-e3274fa8a7fa">Buffered data</h3>
<p>In an analogous manner, <kbd>mpi4py</kbd> provides the corresponding collective commands for buffer-like data such as NumPy arrays by capitalizing the command: <kbd>scatter</kbd>/<kbd>Scatter</kbd>, <kbd>gather</kbd>/<kbd>Gather</kbd>, <kbd>reduce</kbd>/<kbd>Reduce</kbd>, <kbd>bcast</kbd>/<kbd>Bcast</kbd>.</p>
<h1 id="uuid-be37a545-8ad3-47e0-a112-750321ad9e22">18.4 Summary</h1>
<p>In this chapter, we saw how to execute copies of the same script on different processors in parallel. Message passing allows the communication between these different processes. We saw <em>point-to-point </em>communication and the two different distribution type collective communications <em>one-to-all</em> and <em>all-to-one</em>. The commands presented in this chapter are provided by the Python module <kbd>mpi4py</kbd>, which is a Python wrapper to realize the MPI standard in C.</p>
<p>Having worked through this chapter, you are now able to work on your own scripts for parallel programming and you will find that we described only the most essential commands and concepts <span>here</span>. Grouping processes and tagging information are only two of those concepts that we left out. Many of these concepts are important for special and challenging applications, which are far too particular for this introduction.</p>


            </article>

            
        </section>
    </body></html>