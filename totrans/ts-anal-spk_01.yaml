- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What Are Time Series?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Time is the wisest counselor of all.” – Pericles
  prefs: []
  type: TYPE_NORMAL
- en: History is fascinating. It offers a profound narrative of our origins, the journey
    we are on, and the destination we strive toward. History equips us with learnings
    from the past to better face the future.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take, for example, the impact of meteorological data on history. Disruptions
    in weather patterns, starting in the Middle Ages and worsened by the Laki volcanic
    eruption in 1783, caused widespread hardship in France. This climatic upheaval
    contributed to the social unrest that ultimately led to the French Revolution
    in 1789\. (Find out more about this in the *Further* *reading* section.)
  prefs: []
  type: TYPE_NORMAL
- en: Time series embody this narrative with numbers echoing our past. **They are
    history quantified**, a numerical narrative of our collective past, with lessons
    for the future.
  prefs: []
  type: TYPE_NORMAL
- en: This book takes you on a comprehensive journey with time series, starting with
    foundational concepts, guiding you through practical data preparation and model
    building techniques, and culminating in advanced topics such as scaling, and deploying
    to production, while staying abreast of recent developments for cutting-edge applications
    across industries. By the end of this book, you will be equipped to build robust
    time series models, in combination with Apache Spark, to meet the requirements
    of the use cases in your industry.
  prefs: []
  type: TYPE_NORMAL
- en: As a start on this journey, this chapter introduces the fundamental concepts
    of time series data, exploring its sequential nature and the unique challenges
    it poses. The content covers key components such as trend and seasonality, providing
    a foundation to embark on time series analysis at scale using the Spark framework.
    This knowledge is crucial for data scientists and analysts as it forms the basis
    for leveraging Spark’s distributed computing capabilities in effectively analyzing
    and forecasting time-dependent data and making informed decisions in various domains
    such as finance, healthcare, and marketing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking time series into their components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional considerations with time series analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first part of the book, which sets the foundations, you can follow along
    without participating in hands-on examples (although it’s recommended). The latter
    part of the book will be more practice-driven. If you want to get hands-on from
    the beginning, the code for this chapter can be found in the GitHub repository
    of this book at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch1)ch1'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Refer to this GitHub repository for the latest revisions of the code, which
    will be commented on if updated post-publication. The updated code (if any) might
    differ from what is presented in the book's code sections.
  prefs: []
  type: TYPE_NORMAL
- en: The following hands-on sections will give you further details to get started
    with time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will develop an understanding of what time series are and
    some related terms. This will be illustrated by hands-on examples to visualize
    time series. We will look at different types of time series and what characterizes
    them. This knowledge of the nature of time series is necessary for us to choose
    the appropriate time series analysis approach in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with an example of a time series with the average temperature in
    Mauritius every year since 1950\. A short sample of the data is shown in *Table
    1.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Year** | **Average temperature** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1950 | 22.66 |'
  prefs: []
  type: TYPE_TB
- en: '| 1951 | 22.35 |'
  prefs: []
  type: TYPE_TB
- en: '| 1952 | 22.50 |'
  prefs: []
  type: TYPE_TB
- en: '| 1953 | 22.71 |'
  prefs: []
  type: TYPE_TB
- en: '| 1954 | 22.61 |'
  prefs: []
  type: TYPE_TB
- en: '| 1955 | 22.40 |'
  prefs: []
  type: TYPE_TB
- en: '| 1956 | 22.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 1957 | 22.53 |'
  prefs: []
  type: TYPE_TB
- en: '| 1958 | 22.71 |'
  prefs: []
  type: TYPE_TB
- en: '| 1959 | 22.49 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1.1: Sample time series data – average temperature'
  prefs: []
  type: TYPE_NORMAL
- en: While visualizing and explaining this example, we will be introduced to some
    terms related to time series. The code to visualize this dataset is covered in
    the hands-on section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In *the following figure*, we see the change in temperature over the years since
    1950\. If we focus on the period after 1980, we can observe the variations more
    closely, with similarly increasing temperatures over the years (trend – shown
    with a dashed line in both figures) to the current temperature.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Average temperature in Mauritius since 1950'
  prefs: []
  type: TYPE_NORMAL
- en: If the temperature continues to increase in the same way, we are heading to
    a warmer future, a manifestation of what is now widely accepted as global warming.
    At the same time as the temperature has been increasing over the years, it also
    goes up every summer and down during the winter months (**seasonality**). We will
    visualize this and other components of temperature time series in the hands-on
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: With the temperatures getting warmer over the years (**trend**), global warming
    has an impact (**causality**) on our planet and its inhabitants. This impact can
    also be represented with time series – for example, sea level or rainfall measurements.
    The consequences of global warming can be dramatic and irreversible, which further
    highlights the importance of understanding this trend.
  prefs: []
  type: TYPE_NORMAL
- en: These time-over-time readings of temperature form what we call a time series.
    Analysis and understanding of such a time series is critical for our future.
  prefs: []
  type: TYPE_NORMAL
- en: So, what is a time series in more general terms? It is simply a *chronological
    series of measurements together with the specific time at which it was generated
    by a source system*. In the example of temperature, the source system is the thermometer
    at a specific geographical location.
  prefs: []
  type: TYPE_NORMAL
- en: Time series can also be represented in an aggregated form, such as the average
    temperature every year, as shown in *Table 1.1*.
  prefs: []
  type: TYPE_NORMAL
- en: From this definition, illustrated with an example, let’s now probe further into
    the nature of time series. We will also cover in further detail in the rest of
    this book the terms introduced here, such as trend, seasonality, and causality.
  prefs: []
  type: TYPE_NORMAL
- en: Chronological order
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the beginning of the chapter, we mentioned chronological order while defining
    time series, this is because it is a major factor that differentiates the approach
    when working with time series data compared to other datasets. One of the main
    reasons why order matters is due to potential auto-correlation within time series,
    where measurement at time `t` is related to measurement at `n` time steps earlier
    (**lag**). Ignoring this order will make our analysis incomplete and even incorrect.
    We will look at the method to identify auto-correlation later, in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116)
    on exploratory data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that, in many cases with time series, auto-correlation tends
    to make measurements closer in time closer in value, as compared to measurements
    further apart in time.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason to respect chronological order is to avoid data leakage during
    model training. In some of the analysis and forecasting methods, we will be training
    models on past data to predict value at a future target date. We need to ensure
    that all data points used are prior to the target date. Data leakage during training,
    often tricky to spot with time series data, will invalidate the integrity of the
    approach and create models that perform misleadingly well during development,
    then not so well when faced with new unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Terms introduced here, such as auto-correlation, lags, and data leakage, will
    be further explained in the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Chronological order, discussed here, is one defining characteristic of time
    series. In the next section, we will highlight regularity or the lack of it, which
    is another characteristic.
  prefs: []
  type: TYPE_NORMAL
- en: Regular and irregular
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time series can be regular or irregular with regard to the interval of their
    measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Regular time series have values expected at regular intervals in time, say every
    minute, hour, month, and so on. This is usually the case for source systems generating
    a continuous value, which is then measured at a regular interval. This regularity
    is expected, but not guaranteed, as these time series can have gaps or values
    at zero, due to missing data points or just the measurement itself being zero.
    In this case, they will still be considered of a regular nature.
  prefs: []
  type: TYPE_NORMAL
- en: Irregular time series are when measurements are not generated at regular intervals
    at the source. This is usually the case of events occurring at irregular points
    in time, for which events some type of value is then measured. These irregular
    interval values can be resampled to a regular interval with a lower frequency—effectively
    turning into a regular time series. For example, an irregular event not occurring
    every minute may have a likelihood of occurring every hour and be considered regular
    in nature at the hourly rate.
  prefs: []
  type: TYPE_NORMAL
- en: This book will primarily focus on regular time series. After the regularity
    of time series, another characteristic we will consider in the next section is
    stationarity.
  prefs: []
  type: TYPE_NORMAL
- en: Stationary and non-stationary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering the statistical properties of time series over time, they can be
    further categorized as stationary or non-stationary.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stationary time series** are those for which statistical properties such
    as mean and variance do not vary over time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-stationary time series** have changing statistical properties. These
    time series can be converted to stationary by a combination of methods: for example,
    one or more orders of differencing to stabilize the mean and using the log value
    to stabilize the variance. This distinction is important as it will determine
    which analysis method can be used. For instance, if an analysis method is based
    on the assumption of stationary series, the above conversion can be applied to
    non-stationary data first. You will learn about the method to identify stationarity
    in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116) on exploratory data analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Converting a non-stationary time series to a stationary one removes the trend
    and seasonal components, which may not be what we want if we want to analyze these
    components.
  prefs: []
  type: TYPE_NORMAL
- en: This section was an important one to understand the underlying nature of time
    series, which is a prerequisite to identifying the right analysis method to use
    in the later part of this book. *Figure 1**.2* summarizes the types of time series
    and conversation operations that can be used.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Types of time series'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the theoretical part of this chapter. In the next section, we
    will have our first hands-on experience, setting up the coding environment along
    the way. We will start with visualizing and decomposing time series in this chapter.
    We will get into different types of time series analysis and when they are used
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hands-on: Loading and visualizing time series'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s go through the hands-on exercise to load a time series dataset and visualize
    it. We will try to create the visual representation we’ve already seen in *Figure
    1**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: Development environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to run the code, you will need a Python development environment where
    you can install Apache Spark and other required libraries. Specific libraries
    will be detailed, together with installation instructions, in the corresponding
    chapters when required.
  prefs: []
  type: TYPE_NORMAL
- en: PaaS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An easy way to get going with these requirements is by using Databricks Community
    Edition, which is free. This comes with a notebook-based development interface,
    as well as compute with pre-installed Spark and some other libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The instructions to sign up for Databricks Community Edition can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/getting-started/community-edition.html](https://docs.databricks.com/en/getting-started/community-edition.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Community Edition’s compute size is limited as it is a free cloud-based PaaS.
    You can also sign up for a 14-day free trial of Databricks, which, depending on
    the signup option you choose, may require you to first have an account with a
    cloud provider. Some cloud providers may have promotions with some free credits
    at the start. This will give you access to more resources than on Community Edition,
    for a limited time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sign up for the free trial to Databricks at the following URL: [https://www.databricks.com/try-databricks](https://www.databricks.com/try-databricks)'
  prefs: []
  type: TYPE_NORMAL
- en: The folks at Databricks are the original creators of Apache Spark, so you will
    be in a good place there.
  prefs: []
  type: TYPE_NORMAL
- en: The examples in the early chapters will use Community Edition and the open source
    version of Apache Spark. We will use the full Databricks platform in [*Chapter
    8*](B18568_08.xhtml#_idTextAnchor151) and [*Chapter 10*](B18568_10.xhtml#_idTextAnchor190).
  prefs: []
  type: TYPE_NORMAL
- en: Custom
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alternatively, you can build your own environment, setting up the full stack,
    for instance, in a Docker container. This will be covered in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063),
    *Introduction to* *Apache Spark*.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code for this section is in the following notebook file titled `ts-spark_ch1_1.dbc`
    in the `ch1` folder of this book’s GitHub repository, as per the *Technical* *requirements*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The location URL is as follows: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch1/ts-spark_ch1_1.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch1/ts-spark_ch1_1.dbc)'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the development and runtime environment are chosen, the other consideration
    is the dataset. The one we will be using is the observed annual average mean surface
    air temperature of Mauritius, available on the Climate Change Knowledge Portal
    at [https://climateknowledgeportal.worldbank.org/country/mauritius](https://climateknowledgeportal.worldbank.org/country/mauritius).
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset (in the file titled `ts-spark_ch1_ds1.csv`) is available
    in the `ch1` GitHub folder. It can be downloaded using the code mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will be working on the Databricks Community Edition workspace, which
    will be your own self-contained environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step-by-step: Loading and visualizing time series'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have everything set up, let’s get our hands on the first coding
    exercise. First, log in to Databricks Community Edition to import the code, create
    a cluster, and finally run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to Databricks Community Edition, shown in *Figure 1**.3*, using your
    credentials as specified during the signup process. Access the login page at the
    following URL: [https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refer to the *Development environment* section on how to sign up if you have
    not already done so.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18568_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Sign in to Databricks Community Edition'
  prefs: []
  type: TYPE_NORMAL
- en: Once in the workspace, click on **Create a notebook**. See *Figure 1**.4*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18568_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Create a notebook'
  prefs: []
  type: TYPE_NORMAL
- en: From here, we will get into the code, first importing the `ts-spark_ch1_1.dbc`
    notebook provided for [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016) on GitHub,
    as per *Figure 1**.5*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18568_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Import a notebook'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you can either download the file from the GitHub URL for [*Chapter
    1*](B18568_01.xhtml#_idTextAnchor016), provided in the *Technical requirements*
    section, to your local machines and then import it from there, or you can specify
    the following raw file URL for the import, as per *Figure* *1**.6*: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch1/ts-spark_ch1_1.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch1/ts-spark_ch1_1.dbc)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18568_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Import a notebook from the file or URL'
  prefs: []
  type: TYPE_NORMAL
- en: We get to the actual code at this point. You should now have a notebook with
    code as per *Figure 1**.7*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18568_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: Notebook with code'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s run the code. Click on **Run all** as per *Figure 1**.8*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18568_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: Run all code in the notebook'
  prefs: []
  type: TYPE_NORMAL
- en: In case you do not have a cluster already started, you will have to create and
    start a new one. Note that clusters are automatically terminated when not in use
    on Databricks Community Edition, in which case you will see the **Attached cluster
    is terminated** message, as per *Figure 1**.9*, and you will have to select another
    resource.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18568_01_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: Attached cluster is terminated'
  prefs: []
  type: TYPE_NORMAL
- en: From this point, you can either attach to another active cluster (non-terminated
    one) or choose to create a new resource as per *Figure 1**.10*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18568_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: Compute – Create new resource'
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will need to specify a name for the cluster and which version of Spark
    you want to use, as per *Figure 1**.11*. The recommendation here is to use the
    latest version unless, for portability to another environment reasons, you need
    the code to work with an earlier version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18568_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.11: Compute – Create, Attach, & Run'
  prefs: []
  type: TYPE_NORMAL
- en: Once the cluster is created and started, which may take a few minutes in this
    free environment, the code will run, and you will see the chart in *Figure 1**.1*,
    toward the beginning of the chapter, as output. The graphical library used to
    create and display the chart provides you with an interactive interface, allowing
    you – for instance – to zoom into a specific time period.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As this is the first hands-on, we have gone into the step-by-step details. In
    future hands-on sections, we will be focusing on specific datasets and code as
    the rest will be very similar. Additional instructions will be provided whenever
    they differ.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we have executed the code, let’s go over the main sections. We will
    keep it high level in this introductory section and go into further details in
    upcoming chapters once Apache Spark concepts have been introduced:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `import` statements add libraries for date format conversion and for drawing
    graphs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then use `spark.read` to read the CSV data file into a table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `spark.sql` statement chooses a subset of the dataset based on the year
    column, named `Category` in the source dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we plot the time series as well as the trendline based on **Ordinary
    Least Squares** (**OLS**) regression, as per *Figure 1**.1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The plotting library used, `plotly`, allows interactivity on the user interface,
    such as mouseover information on the data points and zooming in and out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From this point on, feel free to experiment with the code and the Databricks
    Community Edition environment, which we will be using for most of the initial
    chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you had your first introduction to time series and the coding
    environment, starting with a simple exercise. In the next section, we will go
    into detail about some of the concepts introduced so far and break down a time
    series into its components.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking a time series down into its components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section aims to further your understanding of a time series by analyzing
    its components and detailing several terms introduced so far. This will set you
    on track for the rest of the book, to use the right methods based on the nature
    of the time series you are analyzing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series models can be broken down into three main components: trend, seasonality,
    and residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>T</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>S</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>e</mi><mi>s</mi><mo>=</mo><mi>T</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>d</mi><mo>+</mo><mi>S</mi><mi>e</mi><mi>a</mi><mi>s</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>d</mi><mi>u</mi><mi>a</mi><mi>l</mi><mi>s</mi></mrow></mrow></math>](img/1.png)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical representations in this book will follow a simplified English
    notation, in favour of a broad audience. Refer to the following great resource
    on time series for mathematical formulations: *Forecasting: Principles and* *Practice*:
    [https://otexts.com/fpp3/](https://otexts.com/fpp3/).'
  prefs: []
  type: TYPE_NORMAL
- en: As you will see in the next hands-on section, this breakdown into components
    is derived from the model fitted to the time series data. For most real-life datasets,
    the breakdown is only an approximation of reality by the model. As such, each
    model will come up with its own identification and approximation of the components.
    The whole idea is to find the best model that fits the time series. This is what
    we will be building up to and covering in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133)
    on building and testing models.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go over each of the components, defining what they mean and visualizing
    them based on an example dataset, as in *Figure 1**.12*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_01_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.12: Time series decomposition'
  prefs: []
  type: TYPE_NORMAL
- en: Systematic and non-systematic components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The level, trend, seasonality, and cycle are called the **systematic** components.
    They represent the underlying structure of the time series, which can be modeled
    and hence forecast.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the systematic components, there is a **non-systematic** part
    that cannot be modeled, which is called residual, noise, or error. The goal of
    time series modeling is to find the model with the best match for the systematic
    components while minimizing the residuals.
  prefs: []
  type: TYPE_NORMAL
- en: We will now go into the details of each of the systematic and non-systematic
    parts.
  prefs: []
  type: TYPE_NORMAL
- en: Level
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Level**, also referred to as the base level, is the mean of the series, acting
    as a baseline on which the effects of the other components are added. Sometimes,
    it is explicitly added to the preceding formula as an additional component. However,
    the level is not always shown in the formula, as it may not be the primary focus
    of the analysis, or the decomposition method may implicitly account for it within
    other components.'
  prefs: []
  type: TYPE_NORMAL
- en: Trend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Trend** is the component indicating the general direction in which the values
    in the time series go over a time period: increasing, decreasing, or flat. This
    change can be linear, as in *Figure 1**.1* and *Figure 1**.12*, or non-linear.
    The trend itself can change at different points in time, as what we can refer
    to as trend changepoints. More broadly, changepoints refer to points on the timeline
    when the statistical properties of the time series change. This can have a significant
    impact on the model parameters or even the model we use to analyze the time series.'
  prefs: []
  type: TYPE_NORMAL
- en: Seasonalities and cycles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Seasonality** indicates changes to a time series at regular time intervals.
    This is usually due to seasonal calendar events. Using our example with temperature,
    every summer month the temperature goes up compared to the rest of the year, and
    down during the winter months, as can be seen in *Figure 1**.12*. Similarly, a
    time series for sales of gift items will likely show an increase in sales every
    Christmas period in its seasonality pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple seasonalities (intervals and amplitudes) can have a combined effect
    within the same time series, as illustrated in *Figure 1**.13*. For example, with
    temperatures, in addition to the ups and downs of summers and winters, the temperature
    goes up during the day and down every night.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.13: Multiple overlapping seasonalities (synthetic data)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cycles** are changes that happen at intervals, similar to seasonality, with
    the difference of being at irregular intervals. Cycles in time series are reflective
    of external cycles impacting the series. For example, recessions occur every certain
    number of years and have an impact on economic indicators. We don’t know when
    in advance and it is different from the seasonality of Christmas, which just occurs
    predictably every December 25.'
  prefs: []
  type: TYPE_NORMAL
- en: Remainders or residuals
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Remainders** or **residuals** are what remains once the model has accounted
    for trends, seasonalities, and cycles. Remainders can be modeled using **autoregression**
    (**AR**) or **moving average** (**MA**) methods. What is still residual at this
    point, also referred to as noise or error, is random in nature and is the part
    that can’t be modeled. You can visualize residuals in the topmost graph of *Figure
    1**.12*, as the distance between the data points and the modeled line. We will
    look at the method to test for residuals in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116)
    on exploratory data analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While with residuals only a component of the time series is random, a whole
    series can be completely random or can be a random walk. A completely random series
    will have no dependency on earlier time values, whereas for a random walk, the
    value at time `t` is dependent on the value at `t-1` (plus some drift and a random
    component).
  prefs: []
  type: TYPE_NORMAL
- en: Additive or multiplicative
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time series can be **additive** (the preceding formula) or **multiplicative**.
    In the first case, the seasonality and residual components are not dependent on
    the trend. In the second case, they change with the trend and can be seen as changing
    amplitude of the seasonal component – for example, higher peaks and lower troughs.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have gone through the components of time series, let’s put this
    into practice with code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hands-on: Decomposing time series'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To demonstrate `ts-spark_ch1_2fp.dbc`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The location URL is as follows: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch1/ts-spark_ch1_2fp.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch1/ts-spark_ch1_2fp.dbc)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset we will be using is the daily minimum temperature from 1981 to
    1990 in Melbourne, Australia, originally from the Australian Bureau of Meteorology,
    and available on Kaggle at the following URL: [https://www.kaggle.com/datasets/samfaraday/daily-minimum-temperatures-in-me](https://www.kaggle.com/datasets/samfaraday/daily-minimum-temperatures-in-me)'
  prefs: []
  type: TYPE_NORMAL
- en: A copy of the dataset is provided in the GitHub folder under the name `ts-spark_ch1_ds2.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will keep it high-level in this chapter, with selected extracts from the
    notebook, and go into further details in upcoming chapters once further concepts
    of forecasting models have been introduced:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `import` statements add libraries for forecasting models and for drawing
    graphs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The forecasting library used is `Prophet`, which is an open source library by
    Facebook. It is accessible to both experts and non-experts, providing automatic
    forecasting for time series data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We then use `spark.read` to read the CSV data file into a table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `spark.sql` statement converts the `date` and `daily_min_temperature` columns
    into the correct format and column name, which is required by `Prophet`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then use the `Prophet` library to create a forecasting model on the basis
    of a seasonality of 12 months and fit it to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model is then used to predict temperatures for future dates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we plot the components of the time series as identified by the model,
    as shown in *Figure 1**.12*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have had a basic discussion on components and forecasting, let’s
    explore the case of overlapping seasonalities.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple overlapping seasonalities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be going through the code to create the data visualization in *Figure
    1**.13*. The code for this section is in the notebook file named `ts-spark_ch1_3.dbc`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The location URL is as follows: [https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch1/ts-spark_ch1_3.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch1/ts-spark_ch1_3.dbc)'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is synthetic and generated as three different sine curves representing
    three overlapping seasonalities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is an extract from the notebook. Let’s look at it at a high
    level:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `import` statements add libraries for numerical calculations and for drawing
    graphs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: NumPy is an open source Python library for scientific computing significantly
    more efficient in terms of computation and memory use than standard Python. We
    will use it here for its mathematical functions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We then generate a number of sine curves, using `np.sin`, to represent different
    seasonalities and add them together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we plot the individual seasonalities as well as the combined one,
    as per *Figure 1**.13*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From here on, feel free to experiment with the full code in the notebooks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this section, we started our journey analyzing time series, probing the underlying
    structure, and paving the way for further analysis with the most appropriate method
    based on their nature. In the next section, we will cover several key considerations
    and challenges to factor into our journey.
  prefs: []
  type: TYPE_NORMAL
- en: Additional considerations with time series analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is probably the most important in this early part of the book.
    In the introductory section, we mentioned some key considerations for time series,
    such as the preservation of chronological order, regularity, and stationarity.
    Here, we map out the key challenges and additional considerations when analyzing
    time series in real-life projects. In doing so, it allows you to plan your learning
    and practice accordingly, with guidance in the relevant sections of this book
    as well as further reading.
  prefs: []
  type: TYPE_NORMAL
- en: According to *Hidden Technical Debt in Machine Learning Systems* a well-known
    paper published in 2015, only a fraction of the effort is with the code in advanced
    analytics projects. The rest of the time is mostly spent on other considerations
    such as data preparation and infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: The solutions to these challenges are very specific to your context. The aim
    in this chapter is to bring these considerations, as summarized in *Figure 1**.14*,
    to your awareness.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_01_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.14: Considerations and challenges with time series analysis'
  prefs: []
  type: TYPE_NORMAL
- en: While the majority of these considerations are shared in common with non-time-series
    analytics such as machine learning, time series analysis tends to be the most
    challenging of advanced analytics methods. We will go into detail on some of the
    solutions to these challenges in the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Facing data challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with all data science and machine learning projects, data is key. The analysis
    you run and the model you build are going to be only as good as the data. Data
    challenges are varied and very dependent on your own specific context and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will list some of the common ones here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Access** to data is probably where it all starts. For the purpose of this
    book, we will be using several freely accessible datasets, so this will not be
    an issue. In real-life projects, the ownership of the dataset you need may sit
    in another part of your organization or even with another organization altogether.
    In this case, you will have to go through the process of acquiring the dataset,
    potentially at a financial cost, and transferring it reliably, with acceptable
    speed and freshness. The transfer pipeline will have its own cost to build as
    well as the transfer cost itself. The transfer mechanism will have to be production
    grade to support operational requirements: robust, recoverable, monitored, and
    so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initially, your data access requirement will be for exploratory data analysis
    and model training. A batch dump may be sufficient. Moving to production, you
    may need access to the data in real or near-real time. The considerations then
    are vastly different.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once data is ingested, the next requirement is to store it in a secure and
    usable way. Using a specialized time series database is an option that is optimized
    for performance, though for the majority of cases, general-purpose storage is
    sufficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitivity** is another key aspect. Again, here, there will likely be different
    requirements in development and production. In many cases, though, a subset of
    production data is used in development and testing. Certain columns with **Personally
    Identifiable Information** (**PII**) will require masking or encryption to comply
    with regulations such as GDPR in Europe. In highly sensitive cases, the whole
    dataset may be encrypted. This can be a challenge for large-scale processing,
    as every access to data may require decryption and re-encryption. This will have
    a processing overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, end-to-end security and data governance will be high on your requirement
    list, and this starts from day one. You want to avoid security and compliance
    risks at all stages, including during development, even more so if you are dealing
    with sensitive data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The **volume** and **frequency** of data feeds at high volume in real or near-real
    time will require the right platform to enable quick processing without data loss.
    This may not be initially apparent in a pre-production environment due to the
    smaller scale. Performance and reliability issues then tend to surface late when
    ramping up in production. We will discuss scaling and streaming once we have introduced
    Apache Spark, which will help you avoid such issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality** is a challenge we will face very early on, as soon as data
    access is resolved, and we start working with the data during the exploratory
    phase and in development. Challenges include gaps in data, corrupt data, noisy
    data, and – even more pertinent for time series – delayed and out-of-order data.
    As mentioned in the earlier section, it is important to preserve the chronological
    order for time series data. We will go further into resolving data quality issues
    when we discuss data preparation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving on from the data challenges, the next area of focus is choosing the right
    approach and model for the problem that needs solving.
  prefs: []
  type: TYPE_NORMAL
- en: Using the right model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This may be more of a challenge for those new to time series. As we have seen
    so far, time series have different statistical properties. Some analysis and modeling
    methods are created based on assumptions about the statistical properties of time
    series, with stationarity as a common assumption. The method used will not work
    as intended or lead to misleading results if used with the incorrect type of time
    series. Handling multiple overlapping seasonalities, assuming you have identified
    them in the first place, can also be a challenge for some methods. *Figure 1**.14*
    gives a recap of the types of time series and analytical models. The choice of
    model will be discussed further in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133),
    *Building and* *Testing Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the right model is also very much dependent on what we want to achieve
    as an outcome, whether it is forecasting one or many time steps into the future,
    or analyzing one (univariate) or more (multivariate) series at the same time.
    For some domains, such as regulated industries, there is usually an additional
    requirement for explainability, which can be difficult with some models, such
    as black-box models. We will go further into the outcomes of time series analysis
    and choosing the right model, including for anomaly and pattern detection, in
    addition to predictive modeling, in the next chapter on why time series matter.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining spatial and temporal hierarchy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note that another key consideration is the hierarchy in which the data is collected
    and analyzed. This needs to be consistent between different levels. To illustrate
    this point, let’s use an example of time series forecasting of the sales volume
    of different products by a multi-store retailer. Spatial hierarchies here will
    likely be at product and product category levels, as well as at specific stores
    and regional levels. Temporal hierarchies will correspond to sales every hour,
    every day, every quarter, and so on. The challenge in this case is to ensure the
    consistency of forecasts for individual products and product categories, as well
    as, say, daily forecasts adding up and being consistent with the quarterly forecast.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the right model depends on the volume of data as we will see in our
    discussion on building models in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are primarily two factors impacting scalability: data volume and processing
    complexity. Earlier, we discussed data volume as a data challenge. Let’s consider
    processing complexity here. **Complexity** can arise from the extent of data transformations
    required to prepare the data for use, as well as the number, hierarchy, and size
    of models that need to be managed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Large number and complex hierarchy of models**: As you work on actual projects,
    it will not be uncommon for you to have to run tens to even thousands of models
    in parallel within a relatively short time period – say, if you work in a store
    and need to forecast the next day’s sales and stock level for each of the thousands
    of items sold in the store. This need for parallelism is one of the key reasons
    for using Apache Spark, as we will see further in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Size of the model**: Another requirement for scalability comes from the size
    of the model itself, which can be very large and have high compute requirements
    if we are using deep learning techniques with many layers and nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will dedicate a whole chapter to scaling later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Approaching real time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier, we identified high-frequency data as a significant data challenge.
    Approaching real time requires not just data-level adjustments but also a processing
    pipeline designed to handle such demands. Typically, models are trained on a batch
    of data collected over time, before being deployed for tasks such as forecasting
    or anomaly detection, where real-time processing becomes critical. For instance,
    in detecting fraudulent transactions, it’s essential to identify anomalies as
    close to the event occurrence as possible. A viable solution for near-instant
    data processing is Apache Spark Structured Streaming, a topic we’ll explore when
    we discuss Apache Spark later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Managing production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding considerations apply to the production environment as well. In
    addition, moving the developed solution into a production environment has several
    specific requirements. These can cause challenges if not managed properly.
  prefs: []
  type: TYPE_NORMAL
- en: Once the right model has been trained and is ready for use, the next step is
    to package it together with any required API wrapper, as well as the data pipeline
    and model-consuming application code. This means an end-to-end process involving
    DataOps, ModelOps, and DevOps. We will go into more on these in[*Chapter 9*](B18568_09.xhtml#_idTextAnchor169)when
    we discuss production.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and addressing drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once a model is in use, changes happen over time, resulting in the model not
    being fit for purpose anymore. These changes are broadly categorized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the nature of the dataset (**data drift**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in the relationship between input and output (**concept drift**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unexpected events such as COVID, or impactful events missed out during the modeling
    process (**sudden drift**, a type of concept drift)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These drifts will impact the model’s performance and, as such, need to be monitored.
    The solution in this case is usually to retrain the model on the new data or find
    a new model with better performance on the updated dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This section gave an overview of the considerations and challenges when working
    with time series. There are lots of commonalities with working on other datasets,
    so the guidance here will be useful in a broader context. As we saw in the introductory
    section, though, time series have their own set of specific considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series are everywhere, and this chapter gave us an introduction to what
    they are, their components, and the challenges in working with them. We started
    with some simple code to explore time series, setting the foundation for further
    practice in upcoming chapters. The concepts discussed in this first chapter will
    be built upon to get us to the point of analyzing time series at scale by the
    end of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the “what” for time series, in the next chapter, we
    will be looking at the “why,” which will pave the way to applications in various
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section serves as a repository of sources that can help you build on your
    understanding of the topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Climate Chaos Helped Spark the French* *Revolution*: [https://time.com/6107671/french-revolution-history-climate/](https://time.com/6107671/french-revolution-history-climate/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks Community Edition: [https://docs.databricks.com/en/getting-started/community-edition.html](https://docs.databricks.com/en/getting-started/community-edition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Climate Change Knowledge Portal: [https://climateknowledgeportal.worldbank.org/country/mauritius](https://climateknowledgeportal.worldbank.org/country/mauritius)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Forecasting: Principles and Practice* by Rob J Hyndman and George Athanasopoulos:
    [https://otexts.com/fpp3/](https://otexts.com/fpp3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hidden Technical Debt in Machine Learning Systems* (Sculley et al., 2015):
    [https://papers.neurips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf](https://papers.neurips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/ds](https://packt.link/ds)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ds_(1).jpg)'
  prefs: []
  type: TYPE_IMG
