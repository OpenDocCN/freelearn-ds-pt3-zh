<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer082">
<h1 class="chapter-number" id="_idParaDest-255"><a id="_idTextAnchor255"/>10</h1>
<h1 id="_idParaDest-256"><a id="_idTextAnchor256"/>Machine Learning for Bioinformatics</h1>
<p>Machine learning is used in a wide variety of contexts and computational biology is not an exception. Machine learning has countless applications in the field, probably the oldest and <a id="_idIndexMarker718"/>most known being the use of <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) to study population structure using genomics. There are many other potential applications as this is a burgeoning field. In this chapter, we are going to introduce machine learning concepts from a bioinformatics perspective.</p>
<p>Given that machine learning is a very complex topic that could easily fill a book, here we intend to take an intuitive approach that will allow you to broadly understand how some machine learning techniques can be useful to tackle biological problems. If you find these techniques useful, you will understand the fundamental concepts and can proceed to more detailed literature.</p>
<p>If you are using Docker, and because all the libraries in this chapter are fundamental for data analysis, they all can be found on the Docker image <strong class="source-inline">tiagoantao/bioinformatics_ml</strong>.</p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Introducing scikit-learn with a PCA example</li>
<li>Using clustering over PCA to classify samples</li>
<li>Exploring breast cancer traits using Decision Trees</li>
<li>Predicting breast cancer outcomes using Random Forests</li>
</ul>
<h1 id="_idParaDest-257"><a id="_idTextAnchor257"/>Introducing scikit-learn with a PCA example</h1>
<p>PCA is a <a id="_idIndexMarker719"/>statistical procedure <a id="_idIndexMarker720"/>that’s used to perform a reduction of the dimension of a number of variables to a smaller subset that is linearly uncorrelated. In <a href="B17942_06.xhtml#_idTextAnchor154"><em class="italic">Chapter 6</em></a>, we saw a PCA implementation based on using an external application. In this recipe, we will implement the same PCA for population genetics but will use the <strong class="source-inline">scikit-learn</strong> library. Scikit-learn is one of the fundamental Python libraries for machine learning and this recipe is an introduction to the library. PCA is a form of unsupervised machine learning – we don’t provide information about the class of the sample. We will discuss supervised techniques in the other recipes of this chapter.</p>
<p>As a reminder, we will compute PCA for 11 human populations from the HapMap project.</p>
<h2 id="_idParaDest-258"><a id="_idTextAnchor258"/>Getting ready</h2>
<p>You will <a id="_idIndexMarker721"/>need to run the first recipe from <a href="B17942_06.xhtml#_idTextAnchor154"><em class="italic">Chapter 6</em></a> in order to generate the <strong class="source-inline">hapmap10_auto_noofs_ld_12</strong> PLINK file (with alleles recorded as 1 and 2). From a <a id="_idIndexMarker722"/>population genetics perspective, we require LD-pruned markers to produce a reliable PCA. We will not risk using the offspring here because it would probably bias the result. Our recipe will require the <strong class="source-inline">pygenomics</strong> library, which can be installed using the following command:</p>
<p class="source-code">pip install pygenomics</p>
<p>The code is in the <strong class="source-inline">Chapter10/PCA.py</strong> notebook.</p>
<h2 id="_idParaDest-259"><a id="_idTextAnchor259"/>How to do it...</h2>
<p>Take a look at the following steps:</p>
<ol>
<li value="1">We start by loading the metadata for our samples. In our case, we will be loading the human population that each sample belongs to:<p class="source-code">import os</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">import numpy as np</p><p class="source-code">from genomics.popgen.pca import plot</p><p class="source-code">f = open('../Chapter06/relationships_w_pops_041510.txt')</p><p class="source-code">ind_pop = {}</p><p class="source-code">f.readline()  # header</p><p class="source-code">for l in f:</p><p class="source-code">    toks = l.rstrip().split('\t')</p><p class="source-code">    fam_id = toks[0]</p><p class="source-code">    ind_id = toks[1]</p><p class="source-code">    pop = toks[-1]</p><p class="source-code">    ind_pop['/'.join([fam_id, ind_id])] = pop</p><p class="source-code">f.close()</p></li>
<li>We now <a id="_idIndexMarker723"/>get the order <a id="_idIndexMarker724"/>of individuals along with the number of SNPs that we will be processing:<p class="source-code">f = open('../Chapter06/hapmap10_auto_noofs_ld_12.ped')</p><p class="source-code">ninds = 0</p><p class="source-code">ind_order = []</p><p class="source-code">for line in f:</p><p class="source-code">    ninds += 1</p><p class="source-code">    toks = line[:100].replace(' ', '\t').split('\t')</p><p class="source-code">    fam_id = toks[0]</p><p class="source-code">    ind_id = toks[1]</p><p class="source-code">    ind_order.append('%s/%s' % (fam_id, ind_id))</p><p class="source-code">nsnps = (len(line.replace(' ', '\t').split('\t')) - 6) // 2</p><p class="source-code">f.close()</p></li>
<li>We create the array that will be fed to the PCA:<p class="source-code">pca_array = np.empty((ninds, nsnps), dtype=int)</p><p class="source-code">print(pca_array.shape)</p><p class="source-code">f = open('../Chapter06/hapmap10_auto_noofs_ld_12.ped')</p><p class="source-code">for ind, line in enumerate(f):</p><p class="source-code">    snps = line.replace(' ', '\t').split('\t')[6:]</p><p class="source-code">    for pos in range(len(snps) // 2):</p><p class="source-code">        a1 = int(snps[2 * pos])</p><p class="source-code">        a2 = int(snps[2 * pos])</p><p class="source-code">        my_code = a1 + a2 - 2</p><p class="source-code">        pca_array[ind, pos] = my_code</p><p class="source-code">f.close()</p></li>
<li>Finally, we <a id="_idIndexMarker725"/>compute the PCA <a id="_idIndexMarker726"/>with up to eight components. We then get the 8-D coordinates for all samples using a <strong class="source-inline">transform</strong> method.<p class="source-code">my_pca = PCA(n_components=8)</p><p class="source-code">my_pca.fit(pca_array)</p><p class="source-code">trans = my_pca.transform(pca_array)</p></li>
<li>Finally, we plot the PCA:<p class="source-code">sc_ind_comp = {}</p><p class="source-code">for i, ind_pca in enumerate(trans):</p><p class="source-code">    sc_ind_comp[ind_order[i]] = ind_pca</p><p class="source-code">plot.render_pca_eight(sc_ind_comp, cluster=ind_pop)</p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer077">
<img alt="Figure 10.1 - PC1 to PC8 for our dataset as produced by scikit-learn " height="571" src="image/B17942_10_01.jpg" width="1070"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 - PC1 to PC8 for our dataset as produced by scikit-learn</p>
<h2 id="_idParaDest-260"><a id="_idTextAnchor260"/>There’s more...</h2>
<p>For publication <a id="_idIndexMarker727"/>in scientific journals, I would recommend using the recipe in <a href="B17942_06.xhtml#_idTextAnchor154"><em class="italic">Chapter 6</em></a>, simply because it’s based on a published <a id="_idIndexMarker728"/>and highly regarded method. That being said, the results from this code are qualitatively similar and cluster data in a very similar fashion (the inversion of direction on the vertical axis, if you compare it with the figure in <a href="B17942_06.xhtml#_idTextAnchor154"><em class="italic">Chapter 6</em></a>, is irrelevant when interpreting a PCA chart).</p>
<h1 id="_idParaDest-261"><a id="_idTextAnchor261"/>Using clustering over PCA to classify samples</h1>
<p>PCA in <a id="_idIndexMarker729"/>genomics allows us to see how samples cluster. In many cases, individuals from the same population will be in the same area of the chart. But we would like to go further and predict where new individuals fall in terms of populations. To do that, we will start with PCA data, as it does dimensionality reduction – making working with the data easier – and then apply a K-Means clustering algorithm to predict where new samples fall. We will use the same dataset as in the recipe above. We will use all our samples save one to train the algorithm, and then we will predict where the remaining sample falls.</p>
<p>K-Means clustering <a id="_idIndexMarker730"/>can be an example of a supervised algorithm. In these types of algorithms, we need a training dataset so that the algorithm is able to learn. After training the algorithm, it will be able to predict a certain outcome for new <a id="_idIndexMarker731"/>samples. In our case, we are hoping that we can predict the population.</p>
<p class="callout-heading">WARNING</p>
<p class="callout">The current recipe intends to serve as a gentle introduction to supervised algorithms and the concepts behind them. The way we are training the algorithm is far from optimal. The issue of properly training a supervised algorithm will be alluded to in the last recipe of this chapter.</p>
<h2 id="_idParaDest-262"><a id="_idTextAnchor262"/>Getting ready</h2>
<p>We will be using the same data as in the previous recipe. The code for this recipe can be found in <strong class="source-inline">Chapter10/Clustering.py</strong>.</p>
<h2 id="_idParaDest-263"><a id="_idTextAnchor263"/>How to do it...</h2>
<p>Let’s have a look:</p>
<ol>
<li value="1">We start by loading the population information – this is similar to what we did in the previous recipe:<p class="source-code">import os</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">import numpy as np</p><p class="source-code">from genomics.popgen.pca import plot</p><p class="source-code">f = open('../Chapter06/relationships_w_pops_041510.txt')</p><p class="source-code">ind_pop = {}</p><p class="source-code">f.readline()  # header</p><p class="source-code">for l in f:</p><p class="source-code">    toks = l.rstrip().split('\t')</p><p class="source-code">    fam_id = toks[0]</p><p class="source-code">    ind_id = toks[1]</p><p class="source-code">    pop = toks[-1]</p><p class="source-code">    ind_pop['/'.join([fam_id, ind_id])] = pop</p><p class="source-code">f.close()</p><p class="source-code"> </p><p class="source-code">f = open('../Chapter06/hapmap10_auto_noofs_ld_12.ped')</p><p class="source-code">ninds = 0</p><p class="source-code">ind_order = []</p><p class="source-code">for line in f:</p><p class="source-code">    ninds += 1</p><p class="source-code">    toks = line[:100].replace(' ', '\t').split('\t') #  for speed</p><p class="source-code">    fam_id = toks[0]</p><p class="source-code">    ind_id = toks[1]</p><p class="source-code">    ind_order.append('%s/%s' % (fam_id, ind_id))</p><p class="source-code">nsnps = (len(line.replace(' ', '\t').split('\t')) - 6) // 2</p><p class="source-code">print (nsnps)</p><p class="source-code">f.close()</p></li>
<li>We now <a id="_idIndexMarker732"/>load all sample data – SNPs – into a NumPy array:<p class="source-code">all_array = np.empty((ninds, nsnps), dtype=int)</p><p class="source-code">f = open('../Chapter06/hapmap10_auto_noofs_ld_12.ped')</p><p class="source-code">for ind, line in enumerate(f):</p><p class="source-code">    snps = line.replace(' ', '\t').split('\t')[6:]</p><p class="source-code">    for pos in range(len(snps) // 2):</p><p class="source-code">        a1 = int(snps[2 * pos])</p><p class="source-code">        a2 = int(snps[2 * pos])</p><p class="source-code">        my_code = a1 + a2 - 2</p><p class="source-code">        all_array[ind, pos] = my_code</p><p class="source-code">f.close()</p></li>
<li>We separate the array into two datasets, namely, a training case with all individuals except one, and a case to test with a single individual:<p class="source-code">predict_case = all_array[-1, :]</p><p class="source-code">pca_array = all_array[:-1,:]</p><p class="source-code"> </p><p class="source-code">last_ind = ind_order[-1]</p><p class="source-code">last_ind, ind_pop[last_ind]</p></li>
</ol>
<p>Our test case is individual Y076/NA19124, who we know belongs to the Yoruban population.</p>
<ol>
<li value="4">We now compute the PCA for the training set that we will use for K-Means clustering:<p class="source-code">my_pca = PCA(n_components=2)</p><p class="source-code">my_pca.fit(pca_array)</p><p class="source-code">trans = my_pca.transform(pca_array)</p><p class="source-code"> </p><p class="source-code">sc_ind_comp = {}</p><p class="source-code">for i, ind_pca in enumerate(trans):</p><p class="source-code">    sc_ind_comp[ind_order[i]] = ind_pca</p><p class="source-code">plot.render_pca(sc_ind_comp, cluster=ind_pop)</p></li>
</ol>
<p>Here is <a id="_idIndexMarker733"/>the output, which will be useful to check clustering results:</p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<img alt="Figure 10.2 - PC1 and PC2 with populations color-coded " height="463" src="image/B17942_10_02.jpg" width="894"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 - PC1 and PC2 with populations color-coded</p>
<ol>
<li value="5">Before we start computing K-means clustering, let’s write a function to plot the clustering <a id="_idIndexMarker734"/>surface from running the algorithm:<p class="source-code">def plot_kmeans_pca(trans, kmeans):</p><p class="source-code">    x_min, x_max = trans[:, 0].min() - 1, trans[:, 0].max() + 1</p><p class="source-code">    y_min, y_max = trans[:, 1].min() - 1, trans[:, 1].max() + 1</p><p class="source-code">    mesh_x, mesh_y = np.meshgrid(np.arange(x_min, x_max, 0.5), np.arange(y_min, y_max, 0.5))</p><p class="source-code"> </p><p class="source-code">    k_surface = kmeans.predict(np.c_[mesh_x.ravel(), mesh_y.ravel()]).reshape(mesh_x.shape)</p><p class="source-code">    fig, ax = plt.subplots(1,1, dpi=300)</p><p class="source-code">    ax.imshow(</p><p class="source-code">        k_surface, origin="lower", cmap=plt.cm.Pastel1,</p><p class="source-code">        extent=(mesh_x.min(), mesh_x.max(), mesh_y.min(), mesh_y.max()),</p><p class="source-code">    )</p><p class="source-code">    ax.plot(trans[:, 0], trans[:, 1], "k.", markersize=2)</p><p class="source-code">    ax.set_title("KMeans clustering of PCA data")</p><p class="source-code">    ax.set_xlim(x_min, x_max)</p><p class="source-code">    ax.set_ylim(y_min, y_max)</p><p class="source-code">    ax.set_xticks(())</p><p class="source-code">    ax.set_yticks(())</p><p class="source-code">    return ax</p></li>
<li>Let’s now fit the algorithm with our samples. Because we have 11 populations, we will train for 11 clusters:<p class="source-code">kmeans11 = KMeans(n_clusters=11).fit(trans)</p><p class="source-code">plot_kmeans_pca(trans, kmeans11)</p></li>
</ol>
<p>Here <a id="_idIndexMarker735"/>is the output:</p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<img alt="Figure 10.3 - The cluster surface for 11 clusters " height="767" src="image/B17942_10_03.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 - The cluster surface for 11 clusters</p>
<p>If you compare with the figure here, you can intuitively see that the clustering makes little sense: it doesn’t map to the known populations very well. One could argue that this clustering algorithm with 11 clusters is not very useful.</p>
<p class="callout-heading">TIP</p>
<p class="callout">There are many other clustering algorithms implemented in scikit-learn, and in several scenarios, they might perform better than K-means. You can find them at <a href="https://scikit-learn.org/stable/modules/clustering.xhtml">https://scikit-learn.org/stable/modules/clustering.xhtml</a>. It is doubtful that, in this specific case, any alternative would perform much better for 11 clusters.</p>
<ol>
<li value="7">While it <a id="_idIndexMarker736"/>seems K-means clustering cannot resolve the 11 populations, maybe it can still provide some predictions if we use a different number of clusters. Simply by looking at the chart, we see four separate blocks. What would be the result if we used four clusters? <p class="source-code">kmeans4 = KMeans(n_clusters=4).fit(trans)</p><p class="source-code">plot_kmeans_pca(trans, kmeans4)</p></li>
</ol>
<p>Here is the output:</p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="Figure 10.4 - The cluster surface for four clusters " height="947" src="image/B17942_10_04.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 - The cluster surface for four clusters</p>
<p>The four groups are now mostly clear. But do they make intuitive sense? If they do, we can <a id="_idIndexMarker737"/>make use of this clustering approach. And in fact, they do. The cluster on the left is composed of African populations, the top cluster European ones, and the bottom one East Asians. The middle one is the most cryptic as it contains both Gujarati and Mexican descendants, but that mix comes originally from PCA and it is not caused by clustering itself.</p>
<ol>
<li value="8">Let’s see how prediction does for the single case we left out:<p class="source-code">pca_predict = my_pca.transform([predict_case])</p><p class="source-code">kmeans4.predict(pca_predict)</p></li>
</ol>
<p>Our sample is predicted to be in cluster 1. We need to dig a little deeper now.</p>
<ol>
<li value="9">Let’s find <a id="_idIndexMarker738"/>out what cluster 1 means. We take the last individual from the training set, who is also a Yoruba, and see to which cluster he is allocated:<p class="source-code">last_train = ind_order[-2]</p><p class="source-code">last_train, ind_pop[last_train]</p><p class="source-code">kmeans4.predict(trans)[0]</p></li>
</ol>
<p>It is indeed cluster 1, so the prediction is correct.</p>
<h2 id="_idParaDest-264"><a id="_idTextAnchor264"/>There’s more...</h2>
<p>It is worth reiterating that we are trying to achieve an intuitive understanding of machine learning. At this stage, you should have a grasp of what you can gain from supervised learning, and also example usage of a clustering algorithm. There is much more to be said about the procedure to train a machine learning algorithm, something that we will partially unveil in the last recipe.</p>
<h1 id="_idParaDest-265"><a id="_idTextAnchor265"/>Exploring breast cancer traits using Decision Trees</h1>
<p>One of the <a id="_idIndexMarker739"/>first problems that we have when we receive a dataset is deciding what to start analyzing. At the very beginning, there is quite often a feeling of loss about what to do first. Here, we will present an <a id="_idIndexMarker740"/>exploratory approach based on Decision Trees. The big advantage of Decision Trees is that they will give us the rules that constructed the decision tree, allowing us a first tentative understanding of what is going on with our data.</p>
<p>In this example, we will be using a dataset with trait observations from patients with breast cancer. The dataset with 699 data entries includes information such as clump thickness, uniformity of cell size, or type of chromatin. The outcome is either a benign or malignant tumor. The features are encoded with values from 0 to 10. More information about the project can be found at <a href="http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29">http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29</a>.</p>
<h2 id="_idParaDest-266"><a id="_idTextAnchor266"/>Getting ready</h2>
<p>We are going to download the data along with the documentation:</p>
<p class="source-code">wget http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data</p>
<p class="source-code">wget http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names</p>
<p>The data <a id="_idIndexMarker741"/>file is formatted as a CSV file. Information about the content can be found in the second downloaded file.</p>
<p>The code <a id="_idIndexMarker742"/>for this recipe can be found in <strong class="source-inline">Chapter10/Decision_Tree.py</strong>.</p>
<h2 id="_idParaDest-267"><a id="_idTextAnchor267"/>How to do it...</h2>
<p>Follow these steps:</p>
<ol>
<li value="1">The first thing we do is to remove a small fraction of individuals that have incomplete data:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn import tree</p><p class="source-code">f = open('breast-cancer-wisconsin.data')</p><p class="source-code">w = open('clean.data', 'w')</p><p class="source-code">for line in f:</p><p class="source-code">    if line.find('?') &gt; -1:</p><p class="source-code">        continue</p><p class="source-code">    w.write(line)</p><p class="source-code">f.close()</p><p class="source-code">w.close()</p></li>
</ol>
<p class="callout-heading">TIP</p>
<p class="callout">Removing individuals with incomplete data is adequate in this case because they are a small fraction of the dataset, and we are only doing exploratory analysis. For cases with lots of missingness or when we are trying to do something more rigorous, you will have to use methods to deal with missing data, which we will not explore here.</p>
<ol>
<li value="2">We <a id="_idIndexMarker743"/>are now going to read <a id="_idIndexMarker744"/>the data, giving names to all columns:<p class="source-code">column_names = [</p><p class="source-code">    'sample_id', 'clump_thickness', 'uniformity_cell_size',</p><p class="source-code">    'uniformity_cell shape', 'marginal_adhesion',</p><p class="source-code">    'single_epithelial_cell_size', 'bare_nuclei',</p><p class="source-code">    'bland_chromatin', 'normal_nucleoli', 'mitoses',</p><p class="source-code">    'class'</p><p class="source-code">]</p><p class="source-code">samples = pd.read_csv('clean.data', header=None, names=column_names, index_col=0)</p></li>
<li>We will now separate the features from the outcome and recode the outcome using 0 and 1:<p class="source-code">training_input = samples.iloc[:,:-1]</p><p class="source-code">target = samples.iloc[:,-1].apply(lambda x: 0 if x == 2 else 1)</p></li>
<li>Let’s now create a Decision Tree based on this data with a max depth of 3: <p class="source-code">clf = tree.DecisionTreeClassifier(max_depth=3)</p><p class="source-code">clf.fit(training_input, target)</p></li>
<li>Let’s <a id="_idIndexMarker745"/>start by seeing which features are the most important: <p class="source-code">importances = pd.Series(</p><p class="source-code">    clf.feature_importances_ * 100,</p><p class="source-code">    index=training_input.columns).sort_values(ascending=False)</p><p class="source-code">importances</p></li>
</ol>
<p>The <a id="_idIndexMarker746"/>following are the features ranked by importance:</p>
<p class="source-code"><strong class="bold">uniformity_cell_size           83.972870</strong></p>
<p class="source-code"><strong class="bold">uniformity_cell shape           7.592903</strong></p>
<p class="source-code"><strong class="bold">bare_nuclei                     4.310045</strong></p>
<p class="source-code"><strong class="bold">clump_thickness                 4.124183</strong></p>
<p class="source-code"><strong class="bold">marginal_adhesion               0.000000</strong></p>
<p class="source-code"><strong class="bold">single_epithelial_cell_size     0.000000</strong></p>
<p class="source-code"><strong class="bold">bland_chromatin                 0.000000</strong></p>
<p class="source-code"><strong class="bold">normal_nucleoli                 0.000000</strong></p>
<p class="source-code"><strong class="bold">mitoses                         0.000000</strong></p>
<p>Remember that this is just exploratory analysis. In the next recipe, we will try to produce more reliable rankings. The reason why the bottom features are zero is we asked for a max depth of 3 and in that case, it is possible that not all features are used.</p>
<ol>
<li value="6">We can do some native analysis of the accuracy of our implementation:<p class="source-code">100 * clf.score(training_input, target)</p></li>
</ol>
<p>We get a performance of 96%. We shouldn’t be testing the algorithm with its own training set as this is quite circular. We will revisit that in the next recipe.</p>
<ol>
<li value="7">Finally, let’s <a id="_idIndexMarker747"/>plot the decision tree:<p class="source-code">fig, ax = plt.subplots(1, dpi=300)</p><p class="source-code">tree.plot_tree(clf,ax=ax, feature_names=training_input.columns, class_names=['Benign', 'Malignant'])</p></li>
</ol>
<p>This <a id="_idIndexMarker748"/>results in the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="Figure 10.5 - The decision tree for the breast cancer dataset " height="959" src="image/B17942_10_05.jpg" width="1491"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 - The decision tree for the breast cancer dataset</p>
<p>Let’s look at the root node to start: it has a criterion of <strong class="source-inline">uniformity_cell_size &lt; 2.5</strong> and a classification of benign. The main feature of splitting the tree is the uniformity of the cell size. The classification of benign at the top node comes simply from the fact that most samples on the dataset are benign. Now look at the right node from the root: it has 265 samples, most of which are malignant and with criteria of <strong class="source-inline">uniformity_cell_shape &lt; 2.5</strong>.</p>
<p>These <a id="_idIndexMarker749"/>rules allow you to have an initial <a id="_idIndexMarker750"/>understanding of what might be driving the dataset. Decision Trees are not very precise, so take these as your initial step.</p>
<h1 id="_idParaDest-268"><a id="_idTextAnchor268"/>Predicting breast cancer outcomes using Random Forests </h1>
<p>We are now going to predict the outcomes for some patients using Random Forests. A random <a id="_idIndexMarker751"/>forest is an ensemble method (it will use several instances of other machine learning algorithms) that uses <a id="_idIndexMarker752"/>many decision trees to arrive at robust conclusions about the data. We are going to use the same example as in the previous recipe: breast cancer traits and outcomes.</p>
<p>This recipe has two main goals: to introduce you to random forests and issues regarding the training of machine learning algorithms.</p>
<h2 id="_idParaDest-269"><a id="_idTextAnchor269"/>Getting ready</h2>
<p>The code for this recipe can be found in <strong class="source-inline">Chapter10/Random_Forest.py</strong>.</p>
<h2 id="_idParaDest-270"><a id="_idTextAnchor270"/>How to do it…</h2>
<p>Take a look at the code:</p>
<ol>
<li value="1">We start, as in the previous recipe, by getting rid of samples with missing information:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.tree import export_graphviz</p><p class="source-code">f = open('breast-cancer-wisconsin.data')</p><p class="source-code">w = open('clean.data', 'w')</p><p class="source-code">for line in f:</p><p class="source-code">    if line.find('?') &gt; -1:</p><p class="source-code">        continue</p><p class="source-code">    w.write(line)</p><p class="source-code">f.close()</p><p class="source-code">w.close()</p></li>
<li>We <a id="_idIndexMarker753"/>now load the <a id="_idIndexMarker754"/>cleaned data:<p class="source-code">column_names = [</p><p class="source-code">    'sample_id', 'clump_thickness', 'uniformity_cell_size',</p><p class="source-code">    'uniformity_cell shape', 'marginal_adhesion',</p><p class="source-code">    'single_epithelial_cell_size', 'bare_nuclei',</p><p class="source-code">    'bland_chromatin', 'normal_nucleoli', 'mitoses',</p><p class="source-code">    'class'</p><p class="source-code">]</p><p class="source-code">samples = pd.read_csv('clean.data', header=None, names=column_names, index_col=0)</p><p class="source-code">samples</p><p class="source-code"> </p></li>
<li>We separate the data read in features and outcomes:<p class="source-code">training_input = samples.iloc[:, :-1]</p><p class="source-code">target = samples.iloc[:, -1]</p></li>
<li>We create a classifier and fit the data to it:<p class="source-code">clf = RandomForestClassifier(max_depth=3, n_estimators=200)</p><p class="source-code">clf.fit(training_input, target)</p></li>
</ol>
<p>The most important parameter here is <strong class="source-inline">n_estimators</strong>: we are requesting the Forest be constructed with 200 trees.</p>
<ol>
<li value="5">We <a id="_idIndexMarker755"/>now rank the <a id="_idIndexMarker756"/>features in order of importance:<p class="source-code">importances = pd.Series(</p><p class="source-code">    clf.feature_importances_ * 100,</p><p class="source-code">    index=training_input.columns).sort_values(ascending=False)</p><p class="source-code">importances</p></li>
</ol>
<p>Here is the output:</p>
<p class="source-code"><strong class="bold">uniformity_cell_size           30.422515</strong></p>
<p class="source-code"><strong class="bold">uniformity_cell shape          21.522259</strong></p>
<p class="source-code"><strong class="bold">bare_nuclei                    18.410346</strong></p>
<p class="source-code"><strong class="bold">single_epithelial_cell_size    10.959655</strong></p>
<p class="source-code"><strong class="bold">bland_chromatin                 9.600714</strong></p>
<p class="source-code"><strong class="bold">clump_thickness                 3.619585</strong></p>
<p class="source-code"><strong class="bold">normal_nucleoli                 3.549669</strong></p>
<p class="source-code"><strong class="bold">marginal_adhesion               1.721133</strong></p>
<p class="source-code"><strong class="bold">mitoses                         0.194124</strong></p>
<p>The result is non-deterministic, meaning that you might have different results. Also, note that the Random Forest has quite different numbers than the Decision Tree of the previous recipe. This is to be expected as the Decision Tree is a single estimator where the Forest weighs 200 trees and is more reliable.</p>
<ol>
<li value="6">We can score this case:<p class="source-code">clf.score(training_input, target)</p></li>
</ol>
<p>I get a result of 97.95%. You might get a slightly different value as the algorithm is stochastic. As we said in the previous recipe, getting a score from the training set is <a id="_idIndexMarker757"/>quite circular and far from best practice.</p>
<ol>
<li value="7">In order <a id="_idIndexMarker758"/>to have a more realistic view of the accuracy of the algorithm, we need to separate our data into two parts – a training set and a test set:<p class="source-code">for test_size in [0.01, 0.1, 0.2, 0.5, 0.8, 0.9, 0.99]:</p><p class="source-code">    X_train, X_test, y_train, y_test = train_test_split(</p><p class="source-code">        trainning_input, target, test_size=test_size)</p><p class="source-code">    tclf = RandomForestClassifier(max_depth=3)</p><p class="source-code">    tclf.fit(X_train, y_train)</p><p class="source-code">    score = tclf.score(X_test, y_test)</p><p class="source-code">    print(f'{1 - test_size:.1%} {score:.2%}')</p></li>
</ol>
<p>The output is the following (remember that you will get different values):</p>
<p class="source-code"><strong class="bold">99.0% 71.43%</strong></p>
<p class="source-code"><strong class="bold">90.0% 94.20%</strong></p>
<p class="source-code"><strong class="bold">80.0% 97.81%</strong></p>
<p class="source-code"><strong class="bold">50.0% 97.66%</strong></p>
<p class="source-code"><strong class="bold">20.0% 96.89%</strong></p>
<p class="source-code"><strong class="bold">10.0% 94.80%</strong></p>
<p class="source-code"><strong class="bold">1.0% 92.02%</strong></p>
<p>If you <a id="_idIndexMarker759"/>only train with 1% of the <a id="_idIndexMarker760"/>data, you only get 71% accuracy, whereas if you train with more, the accuracy goes above 90%. Note that accuracy does not monotonically increase with the size of the training set. Deciding on the size of the training set is a complex affair with various issues causing unexpected side effects.</p>
<h2 id="_idParaDest-271"><a id="_idTextAnchor271"/>There’s more...</h2>
<p>We only scratched the surface of training and testing machine learning algorithms. For example, supervised datasets are normally split into 3, not 2 (training, test, and cross-validation). There are many more issues that you need to consider in order to train your algorithm and many more types of algorithms. In this chapter, we tried to develop basic intuition to understand machine learning, but this is nothing more than your starting point if you intend to follow this route.</p>
</div>
</div></body></html>