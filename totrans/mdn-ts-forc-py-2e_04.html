<html><head></head><body>
  <div id="_idContainer096" class="Basic-Text-Frame">
    <h1 class="chapterNumber">3</h1>
    <h1 id="_idParaDest-63" class="chapterTitle">Analyzing and Visualizing Time Series Data</h1>
    <p class="normal">In the previous chapter, we learned where to obtain time series datasets, as well as how to manipulate time series data using pandas, handle missing values, and so on. Now that we have the processed time series data, it’s time to understand the dataset, which data scientists call <strong class="keyWord">Exploratory Data Analysis</strong> (<strong class="keyWord">EDA</strong>). It is a process by which the data scientist analyzes the data by looking at<a id="_idIndexMarker153"/> aggregate statistics, feature distributions, visualizations, and so on to try and uncover patterns in the data that they can leverage in modeling. In this chapter, we will look at a couple of ways to analyze a time series dataset, a few specific techniques that are tailor-made for time series, and review some of the visualization techniques for time series data.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Components of a time series</li>
      <li class="bulletList">Visualizing time series data</li>
      <li class="bulletList">Decomposing a time series</li>
      <li class="bulletList">Detecting and treating outliers</li>
    </ul>
    <h1 id="_idParaDest-64" class="heading-1">Technical requirements</h1>
    <p class="normal">You will need to set up the <strong class="keyWord">Anaconda</strong> environment following the instructions in the <em class="italic">Preface</em> of the book to get a working environment with all the libraries and datasets required for the code in this book. Any additional library will be installed while running the notebooks.</p>
    <p class="normal">You will need to run the <code class="inlineCode">02-Preprocessing_London_Smart_Meter_Dataset.ipynb</code> notebook from <code class="inlineCode">Chapter02</code> folder.</p>
    <p class="normal">The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter03"><span class="url">https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter03</span></a>.</p>
    <h1 id="_idParaDest-65" class="heading-1">Components of a time series</h1>
    <p class="normal">Before we start analyzing and<a id="_idIndexMarker154"/> visualizing time series, we need to understand the structure of a time series. Any time series can contain some or all of the following components:</p>
    <ul>
      <li class="bulletList">Trend</li>
      <li class="bulletList">Seasonal</li>
      <li class="bulletList">Cyclical</li>
      <li class="bulletList">Irregular</li>
    </ul>
    <p class="normal">These components can be mixed in different ways, but two very commonly assumed ways are <em class="italic">additive</em> (<em class="italic">Y</em> = <em class="italic">Trend</em> + <em class="italic">Seasonal</em> + <em class="italic">Cyclical</em> + <em class="italic">Irregular</em>) and <em class="italic">multiplicative</em> (<em class="italic">Y</em> = <em class="italic">Trend</em> * <em class="italic">Seasonal</em> * <em class="italic">Cyclical</em> * <em class="italic">Irregular</em>), where <em class="italic">Y</em> is the time series.</p>
    <h2 id="_idParaDest-66" class="heading-2">The trend component</h2>
    <p class="normal">The <strong class="keyWord">trend</strong> is a<a id="_idIndexMarker155"/> long-term change in<a id="_idIndexMarker156"/> the mean of a time series. It is the smooth and steady movement of a time series in a particular direction. When the time series moves upward, we say there is an <em class="italic">upward or increasing trend</em>, while when it moves downward, we say there is a <em class="italic">downward or decreasing trend</em>. At the time of writing, if we think about the revenue of Tesla over the years, as shown in the following figure, we can see that it has been increasing consistently for the last few years:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_01.png" alt="Figure 3.1 – Tesla’s revenue in millions of USD "/></figure>
    <p class="packt_figref">Figure 3.1: Tesla’s revenue in millions of USD</p>
    <p class="normal">Looking at the <a id="_idIndexMarker157"/>preceding figure, we can say that Tesla’s revenue is having an increasing trend. The trend <a id="_idIndexMarker158"/>doesn’t need to be linear; it can also be non-linear.</p>
    <h2 id="_idParaDest-67" class="heading-2">The seasonal component</h2>
    <p class="normal">When a time series <a id="_idIndexMarker159"/>exhibits<a id="_idIndexMarker160"/> regular, repetitive, up-and-down fluctuations, we call that <strong class="keyWord">seasonality</strong>. For instance, retail sales typically shoot up during the holidays, especially during Christmas in Western countries. Similarly, electricity consumption peaks during the summer months in the tropics and the winter months in colder countries. In all these examples, you can see a specific up-and-down pattern repeating every year. Another example is sunspots, as shown in the following figure:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_02.png" alt="Figure 3.2 – Number of sunspots from 1749 to 2017 "/></figure>
    <p class="packt_figref">Figure 3.2: Number of sunspots from 1749 to 2017</p>
    <p class="normal">As <a id="_idIndexMarker161"/>you can see, sunspots <a id="_idIndexMarker162"/>peak every 11 years.</p>
    <h2 id="_idParaDest-68" class="heading-2">The cyclical component</h2>
    <p class="normal">The <strong class="keyWord">cyclical</strong> component <a id="_idIndexMarker163"/>is often confused with seasonality, but it stands apart<a id="_idIndexMarker164"/> due to a very subtle difference. Like seasonality, the cyclical component also exhibits a similar up-and-down pattern around the trend line, but the time over which this cycle moves isn’t fixed and is subject to a bit of variation around a general time frame. A good example of this is economic recession, which happens over a 10-year cycle. However, this doesn’t happen like clockwork; sometimes, it can be fewer or more than every 10 years.</p>
    <h2 id="_idParaDest-69" class="heading-2">The irregular component</h2>
    <p class="normal">This component is left after<a id="_idIndexMarker165"/> removing<a id="_idIndexMarker166"/> the trends, seasonality, and cyclicity from a time series. Traditionally, this component is considered <em class="italic">unpredictable</em> and is also called the <em class="italic">residual</em>, <em class="italic">error term</em>, <em class="italic">or noise term</em>. In common classical statistics-based models, the point of any “model” is to capture all the other components to the point that the only part that is not captured is the irregular component. </p>
    <p class="normal">In modern machine learning, we do not consider this component entirely unpredictable. We try to capture parts of this component by using exogenous variables. For instance, the irregular component of retail sales may be explained by the different promotional activities they run. When we have this additional information, the “unpredictable” component starts to become predictable again. But no matter how many additional variables you add to the model, there will always be some component, which is the irreducible error, that is left behind. This is the part of the time series which can never be explained no matter how strong the model is or how much additional <a id="_idIndexMarker167"/>information you <a id="_idIndexMarker168"/>add to the model.</p>
    <p class="normal">Now that we know what the different components of a time series are, let’s see how we can visualize them.</p>
    <h1 id="_idParaDest-70" class="heading-1">Visualizing time series data</h1>
    <p class="normal">In <em class="chapterRef">Chapter 2</em>, <em class="italic">Acquiring and Processing Time Series Data</em>, we learned how to prepare a data model as a first<a id="_idIndexMarker169"/> step toward analyzing a new dataset. If preparing a data model is like approaching someone you like and making that first contact, then EDA is<a id="_idIndexMarker170"/> like dating that person. At this point, you have the dataset, and you are trying to get to know them, trying to figure out what makes them tick, what the person likes and dislikes, and so on.</p>
    <p class="normal">EDA often employs visualization techniques to uncover patterns, spot anomalies, form and test hypotheses, and so on. Spending some time understanding your dataset will help you a lot when you are trying to squeeze out every last bit of performance from the models. You may understand what sort of features you must create, what kind of modeling techniques should be applied, and so on.</p>
    <p class="normal">In this chapter, we will cover a few visualization techniques that are well suited for time series datasets.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code for visualizing time series, use the <code class="inlineCode">01-Visualizing_Time_Series.ipynb</code> notebook in the <code class="inlineCode">Chapter03</code> folder.</p>
    </div>
    <h2 id="_idParaDest-71" class="heading-2">Line charts</h2>
    <p class="normal">This is the most basic and <a id="_idIndexMarker171"/>common <a id="_idIndexMarker172"/>visualization that is used for understanding a time series. We just plot the time on the <em class="italic">x</em>-axis and the time series value on the <em class="italic">y</em>-axis. Let’s see what it looks like if we plot one of the households from our dataset:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_03.png" alt="Figure 3.3 – Line plot of household MAC000193 "/></figure>
    <p class="packt_figref">Figure 3.3: Line plot of household MAC000193</p>
    <p class="normal">When you have a long time series with high variation, as we have, the line plot can get a bit chaotic. One of the options to get a macro view of the time series in terms of trends and movement is to plot a smoothed version of the time series. Let’s see what a rolling monthly average of the time series looks like:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_04.png" alt="Figure 3.4 – Rolling monthly average energy consumption of household MAC000193 "/></figure>
    <p class="packt_figref">Figure 3.4: Rolling monthly average energy consumption of household MAC000193</p>
    <p class="normal">We can see the macro patterns much more clearly now. The seasonality is clear—the series peaks in winter and troughs during summer. If you think about it critically, it makes sense. This is London we are talking about, and the energy consumption would be higher during the winter because of lower temperatures and subsequent heating system usage. </p>
    <p class="normal">For a household in the tropics, for example, the pattern may be reversed, with the peaks coming in summer when air conditioners come into play.</p>
    <p class="normal">Another use <a id="_idIndexMarker173"/>for the line <a id="_idIndexMarker174"/>chart is to visualize two or more time series together and investigate any correlations between them. In our case, let’s try plotting the temperature along with the energy consumption and see whether the hypothesis we have about temperature influencing energy consumption holds good:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_05.png" alt="Figure 3.5 – Temperature and energy consumption (zoomed-in plot at the bottom) "/></figure>
    <p class="packt_figref">Figure 3.5: Temperature and energy consumption (zoomed-in plot at the bottom)</p>
    <p class="normal">Here, we<a id="_idIndexMarker175"/> can see a <a id="_idIndexMarker176"/>clear negative correlation in yearly resolution between energy consumption and temperature. Winters show higher energy consumption on a macro scale. We can also see the daily patterns that are loosely correlated with temperature, but maybe because of other factors such as people coming back home <a id="_idIndexMarker177"/>after work and<a id="_idIndexMarker178"/> so on.</p>
    <p class="normal">There are a few other visualizations that are more suited to bringing out seasonality in a time series. Let’s take a look.</p>
    <h2 id="_idParaDest-72" class="heading-2">Seasonal plots</h2>
    <p class="normal">A <strong class="keyWord">seasonal plot</strong> is <a id="_idIndexMarker179"/>very similar to a line plot, but <a id="_idIndexMarker180"/>the key difference here is that the <em class="italic">x</em>-axis denotes the “seasons,” the <em class="italic">y</em>-axis denotes the time series value, and the different seasonal cycles are represented in different colors or line types. For instance, the yearly seasonality at a monthly resolution can be depicted with months on the <em class="italic">x</em>-axis and different years in different colors.</p>
    <p class="normal">Let’s see what this looks like for our household in question. Here, we have plotted the average monthly energy consumption across multiple years:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_06.png" alt="Figure 3.6 – Seasonal plot at a monthly resolution "/></figure>
    <p class="packt_figref">Figure 3.6: Seasonal plot at a monthly resolution</p>
    <p class="normal">We can instantly see the appeal in this visualization because it lets us visualize the seasonality pattern easily. We can see that the consumption goes down in the summer months and we can also see that it happens consistently across multiple years. In the two years that we have data for, we can see that in October, the behavior in 2013 slightly deviated from 2012. Maybe there is something else that can help us explain this difference—what about temperature? </p>
    <p class="normal">We can also plot the seasonal plots with another variable of interest, such <a id="_idIndexMarker181"/>as the <a id="_idIndexMarker182"/>temperature:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_07.png" alt="Figure 3.7 – Seasonal plot at a monthly resolution (energy consumption versus temperature) "/></figure>
    <p class="packt_figref">Figure 3.7: Seasonal plot at a monthly resolution (energy consumption versus temperature)</p>
    <p class="normal">Notice October? In October 2013, the temperature stayed warmer for one month more, hence why the energy consumption pattern was slightly different from last year.</p>
    <p class="normal">We can plot these kinds of plots at other resolutions as well, such as hourly seasonality. All we need to do is calculate the average consumption for each hour and day of the month and plat them with hours on the <em class="italic">x</em>-axis and different days of the month in different colors (<em class="italic">Figure 3.8</em> (top)). But when there are too many seasonal cycles to be plotted, it increases visual clutter. An alternative to a seasonal plot is a seasonal box plot.</p>
    <h2 id="_idParaDest-73" class="heading-2">Seasonal box plots</h2>
    <p class="normal">Instead of plotting the <a id="_idIndexMarker183"/>different seasonal cycles in different colors or line types, we <a id="_idIndexMarker184"/>can represent them as a box plot (<em class="italic">Figure 3.8</em> (bottom)). This instantly clears up the clutter in the plot. The additional benefit you get from this representation is that it lets us understand the variability across seasonal cycles:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_08.png" alt="Figure 3.8 – Seasonal plot (top) and seasonal box plot (bottom) at an hourly resolution "/></figure>
    <p class="packt_figref">Figure 3.8: Seasonal plot (top) and seasonal box plot (bottom) at an hourly resolution</p>
    <p class="normal">Here, we can<a id="_idIndexMarker185"/> see that the seasonal plot at this resolution is too cluttered to make out the pattern and the variation across seasonal cycles. However, the seasonal box plot is much more informative. The horizontal line in the box tells us about the median, the box is the <strong class="keyWord">interquartile range</strong> (<strong class="keyWord">IQR</strong>), and the points that are marked are the outliers. By looking at the <a id="_idIndexMarker186"/>medians, we can see that the peak consumption occurs from 9 A.M. onward. But the <a id="_idIndexMarker187"/>variability is also higher from 9 A.M. If you plot separate box plots for each week, for example, you will see that the patterns are slightly different on Sundays (additional visualizations are in the associated notebook).</p>
    <p class="normal">However, there is another visualization that lets you inspect these patterns along two dimensions.</p>
    <h2 id="_idParaDest-74" class="heading-2">Calendar heatmaps</h2>
    <p class="normal">Instead of having<a id="_idIndexMarker188"/> separate box plots or separate line charts for each week of the day, it would be useful if we could condense that information into a single plot. This is where <strong class="keyWord">calendar heatmaps</strong> come in. A <a id="_idIndexMarker189"/>heatmap visualization uses color gradients to represent data values, with different colors indicating varying intensities or frequencies. A calendar heatmap uses colored cells in a rectangular block to represent the information. Along the two sides of the rectangle, we can find two separate granularities of time, such as month and year. In each intersection, the cell is colored relative to the value of the time series at that intersection.</p>
    <p class="normal">Let’s look at the hourly average energy consumption across the different weekdays in a calendar heatmap (refer to the color images file:<a href="https://packt.link/gbp/9781835883181"><span class="url">https://packt.link/gbp/9781835883181</span></a>):</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_09.png" alt="Figure 3.9 – A calendar heatmap for energy consumption "/></figure>
    <p class="packt_figref">Figure 3.9: A calendar heatmap for energy consumption</p>
    <p class="normal">From the<a id="_idIndexMarker190"/> color scale on<a id="_idIndexMarker191"/> the right, we know that lighter colors mean higher values. We can see how Monday to Saturday have similar peaks—that is, once in the morning and once in the evening. However, Sunday has a slightly different pattern, with higher consumption throughout the day.</p>
    <p class="normal">So far, we’ve reviewed a lot of visualizations that can bring out seasonality. Now, let’s look at a visualization for inspecting autocorrelation.</p>
    <h2 id="_idParaDest-75" class="heading-2">Autocorrelation plot</h2>
    <p class="normal">If correlation<a id="_idIndexMarker192"/> indicates the strength and direction of the linear relationship between two variables, autocorrelation is the correlation between the values of a time series in successive periods. Most time series have a heavy dependence on the value in the previous period, and this is a critical component in a lot of the forecasting models we will be seeing as well. </p>
    <p class="normal">Something such as ARIMA (which we will briefly look at in <em class="chapterRef">Chapter 4</em>, <em class="italic">Setting a Strong Baseline Forecast</em>) is built on autocorrelation. So, it’s always helpful to just visualize and understand how<a id="_idIndexMarker193"/> strong the dependence on previous time steps is.</p>
    <p class="normal">This is where <strong class="keyWord">autocorrelation plots</strong> come in handy. In such plots, we have the different lags (<em class="italic">t-1</em>, <em class="italic">t-2</em>, <em class="italic">t-3</em>, and so on) on the <em class="italic">x</em>-axis and the correlations between <em class="italic">t</em> and the different lags on the <em class="italic">y</em>-axis. In addition to autocorrelation, we can also look at <strong class="keyWord">partial autocorrelation</strong>, which <a id="_idIndexMarker194"/>is <a id="_idIndexMarker195"/>very similar to autocorrelation but with one key difference: partial <a id="_idIndexMarker196"/>autocorrelation removes any indirect correlation that may be present before presenting the correlations. Let’s look at an example to understand this. If <em class="italic">t</em> is the current time step, let’s assume <em class="italic">t-1</em> is highly correlated to <em class="italic">t</em>. So, by extending this logic, <em class="italic">t-2</em> will be highly correlated with <em class="italic">t-1</em> and because of this correlation, the autocorrelation between <em class="italic">t</em> and <em class="italic">t-2</em> would be high. However, partial autocorrelation corrects this and extracts the correlation, which can be purely attributed to <em class="italic">t-2</em> and <em class="italic">t</em>.</p>
    <p class="normal">One thing we need to keep in mind is that the autocorrelation and partial autocorrelation analysis works best if the time series is stationary (we will talk about stationarity in detail in <em class="chapterRef">Chapter 6</em>, <em class="italic">Feature Engineering for Time Series Forecasting</em>).</p>
    <div class="packt_tip">
      <p class="normal"><strong class="keyWord">Best practice</strong>:</p>
      <p class="normal">There are many ways of making a series stationary, but a quick and dirty way is to use seasonal decomposition and just pick the residuals. It should be devoid of trends and seasonality, which are the major drivers of non-stationarity in a time series. But as we will see later in this book, this is not a foolproof method of making a series stationary in the truest sense.</p>
    </div>
    <p class="normal">Now, let’s see what these plots look like for our household from the dataset (after making it stationary):</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_10.png" alt="Figure 3.10 – Autocorrelation and partial autocorrelation plots "/></figure>
    <p class="packt_figref">Figure 3.10: Autocorrelation and partial autocorrelation plots</p>
    <p class="normal">Here, we can see that the first lag (<em class="italic">t-1</em>) has the most influence and that its influence quickly drops down to close to zero in the partial autocorrelation plot. This means that the energy consumption of a day is highly correlated with the energy consumption the day before.</p>
    <p class="normal">If you’ve seen such charts before, you would have seen an envelope over this showing the confidence intervals as a guide to selecting significant autocorrelations. While that is a good thumb of rule, it’s not included here because I don’t want you to use it as a rule. The relevance of the confidence <a id="_idIndexMarker197"/>intervals depends on some assumptions (normality and so on), which may not be<a id="_idIndexMarker198"/> satisfied all the time, especially in real-world use cases.</p>
    <p class="normal">With that, we’ve looked at the different components of a time series and learned how to visualize a few of them. Now, let’s see how we can decompose a time series into its components.</p>
    <h1 id="_idParaDest-76" class="heading-1">Decomposing a time series</h1>
    <p class="normal">Seasonal decomposition<a id="_idIndexMarker199"/> is the process by which we deconstruct a time series into its components—typically, trend, seasonality, and residuals. The general approach for decomposing a<a id="_idIndexMarker200"/> time series is as follows:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Detrending</strong>: Here, we <a id="_idIndexMarker201"/>estimate<a id="_idIndexMarker202"/> the <strong class="keyWord">trend component</strong> (which is the smooth change in the time series) and remove it<a id="_idIndexMarker203"/> from the<a id="_idIndexMarker204"/> time series, giving us a <strong class="keyWord">detrended time series</strong>.</li>
      <li class="numberedList"><strong class="keyWord">Deseasonalizing</strong>: Here, we<a id="_idIndexMarker205"/> estimate the seasonality component from the detrended time series. After removing the seasonal component, what is left<a id="_idIndexMarker206"/> is the residual.</li>
    </ol>
    <p class="normal">Let’s discuss them in detail.</p>
    <h2 id="_idParaDest-77" class="heading-2">Detrending</h2>
    <p class="normal"><strong class="keyWord">Detrending</strong> can be <a id="_idIndexMarker207"/>done in a<a id="_idIndexMarker208"/> few different <a id="_idIndexMarker209"/>ways. Two popular ways of doing it are by<a id="_idIndexMarker210"/> using <strong class="keyWord">moving averages</strong> and <strong class="keyWord">locally estimated scatterplot smoothing</strong> (<strong class="keyWord">LOESS</strong>) <strong class="keyWord">regression</strong>.</p>
    <h3 id="_idParaDest-78" class="heading-3">Moving averages</h3>
    <p class="normal">One of the easiest ways of <a id="_idIndexMarker211"/>estimating trends is by using a moving average along the time <a id="_idIndexMarker212"/>series. It can be seen as a window that is moved along the time series in steps, and at each step, the average of all the values in the window is recorded. This moving average is a smoothed-out time series and helps us estimate the slow change in a time series, which is the trend. The downside is that the technique is quite noisy. Even after smoothing out a time series using this technique, the extracted trend will not be smooth; it will be noisy. The noise should ideally reside with the residuals and not the trend (see the trend line shown in <em class="italic">Figure 3.13</em>).</p>
    <h3 id="_idParaDest-79" class="heading-3">LOESS</h3>
    <p class="normal">The <strong class="keyWord">LOESS</strong> algorithm, which is<a id="_idIndexMarker213"/> also called <em class="italic">locally weighted polynomial regression</em>, was developed by Bill Cleveland from the 70s to the 90s. It is a <a id="_idIndexMarker214"/>non-parametric method that is used to fit a smooth curve onto a noisy signal. We use an ordinal variable that moves between the time series as the independent variable and the time series signal as the dependent variable. </p>
    <p class="normal">For each value in the ordinal variable, the algorithm uses a fraction of the closest points and estimates a smoothed trend using only those points in a weighted regression. The weights in the weighted regression are the closest points to the point in question. This is given the highest weight and it decays as we move farther away from it. This gives us a very effective tool for modeling the smooth changes in the <a id="_idIndexMarker215"/>time series (trend) (see the trend line shown in <em class="italic">Figure 3.14</em>).</p>
    <h2 id="_idParaDest-80" class="heading-2">Deseasonalizing</h2>
    <p class="normal">The seasonality <a id="_idIndexMarker216"/>component can also be<a id="_idIndexMarker217"/> estimated in a few different ways. The two most popular ways of doing this are by using period-adjusted averages or a Fourier series.</p>
    <h3 id="_idParaDest-81" class="heading-3">Period adjusted averages</h3>
    <p class="normal">This is a pretty<a id="_idIndexMarker218"/> simple technique wherein we<a id="_idIndexMarker219"/> calculate a seasonality index for each period in the expected cycle by taking the average values of all such periods over all the cycles. To make that clear, let’s look at a monthly time series where we expect an annual seasonality in this time series. So, the up-and-down pattern would complete a full cycle in 12 months, or the seasonality period is 12. In other words, every 12 points in the time series have similar seasonal components. So, we take the average of all January values as the period-adjusted average for January. In the same way, we calculate the period average for all 12 months. At the end of the exercise, we have 12 period averages, and we can also calculate an <em class="italic">average</em> period average. Now, we can make these period averages into an index by either subtracting the average of all period averages from each of the period averages (for additive) or dividing the average of all period averages from each of the period averages (multiplicative).</p>
    <h3 id="_idParaDest-82" class="heading-3">Fourier series</h3>
    <p class="normal">In the late 1700s, Joseph Fourier, a<a id="_idIndexMarker220"/> mathematician and physicist, while studying heat <a id="_idIndexMarker221"/>flow, realized something profound—<em class="italic">any</em> periodic function can be broken down into a simple series of sine and cosine waves. Let’s dwell on that for a minute. Any periodic function, no matter the shape, curve, or absence of it, or how wildly it oscillates around the axis, can be broken down into a series of sine and cosine waves.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Additional information</strong>:</p>
      <p class="normal">For the more mathematically inclined, the original theory proposes to decompose any periodic function into an integral of exponentials. Using Euler’s identity, <img src="../Images/B22389_03_001.png" alt=""/>, we can consider them as a summation of sine and cosine waves. The <em class="italic">Further reading</em> section contains a few resources if you want to delve deeper and explore related concepts, such as the Fourier transform.</p>
    </div>
    <p class="normal">It is this <a id="_idIndexMarker222"/>property that we use to extract seasonality from a time series because seasonality is a<a id="_idIndexMarker223"/> periodic function, and any periodic function can be approximated by a combination of sine and cosine waves. The sine-cosine form of a Fourier series is as follows:</p>
    <p class="center"><img src="../Images/B22389_03_002.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">S</em><sub class="subscript-italic" style="font-style: italic;">N</sub> is the <em class="italic">N</em>-term approximation of the signal, <em class="italic">S</em>. Theoretically, when <em class="italic">N</em> is infinite, the resulting approximation is equal to the original signal. <em class="italic">P</em> is the maximum length of the cycle.</p>
    <p class="normal">We can use this Fourier series, or a few terms from the Fourier series, to model our seasonality. In our application, <em class="italic">P</em> is the maximum length of the cycle we are trying to model. For instance, for a yearly seasonality for monthly data, the maximum length of the cycle (<em class="italic">P</em>) is 12. <em class="italic">x</em> would be an ordinal variable that increases from <em class="italic">1</em> to <em class="italic">P</em>. In this example, <em class="italic">x</em> would be <em class="italic">1</em>, <em class="italic">2</em>, <em class="italic">3</em>, … <em class="italic">12</em>. Now, with these terms, all that is left to do is find <em class="italic">a</em><sub class="subscript-italic" style="font-style: italic;">n</sub> and <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">n</sub>, which we can do by regressing on the signal.</p>
    <p class="normal">We’ve seen that with the right combination of Fourier terms, we can replicate any signal. But the question is, should we? What we want to learn from data is a generalized seasonality profile that does well with unseen data as well. So, we use <em class="italic">N</em> as a hyperparameter to extract as complex a signal as we want from the data.</p>
    <p class="normal">This is a good time to brush up on your trigonometry and remember what sine and cosine waves look like. The first Fourier term (<em class="italic">n=1</em>) is your age-old sine and cosine waves, which complete one full cycle in the maximum cycle length (<em class="italic">P</em>). As we increase n, we get sine and cosine waves that have multiple cycles in the maximum cycle length (<em class="italic">P</em>). This can be seen in the following figure:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_11.png" alt="Figure 3.11 – Cosine Fourier terms (n=1, 2, 3) "/></figure>
    <p class="packt_figref">Figure 3.11: Cosine Fourier terms (n = 1, 2, 3)</p>
    <p class="normal">The sine and cosine<a id="_idIndexMarker224"/> waves are complementary to each other, as <a id="_idIndexMarker225"/>shown in the following figure:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_12.png" alt="Figure 3.12 – Sine and cosine Fourier terms (n=1) "/></figure>
    <p class="packt_figref">Figure 3.12: Sine and cosine Fourier terms (n = 1)</p>
    <p class="normal">Now, let’s <a id="_idIndexMarker226"/>see how we can <a id="_idIndexMarker227"/>use this in practice.</p>
    <h2 id="_idParaDest-83" class="heading-2">Implementations</h2>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code for decomposing time series, use the <code class="inlineCode">02-Decomposing_Time_Series.ipynb</code> notebook in the <code class="inlineCode">Chapter03</code> folder.</p>
    </div>
    <p class="normal">There are four implementations that we will cover here in the following subsections.</p>
    <h3 id="_idParaDest-84" class="heading-3">seasonal_decompose from statsmodel</h3>
    <p class="normal"><code class="inlineCode">statsmodels.tsa.seasonal</code> has a function<a id="_idIndexMarker228"/> called <code class="inlineCode">seasonal_decompose</code>. This is an implementation <a id="_idIndexMarker229"/>that uses moving averages for the trend component and period-adjusted averages for the seasonal component. It supports both additive and multiplicative modes of decomposition. However, it doesn’t tolerate missing values. Let’s see how we can use it:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Does not support missing values, so using imputed ts instead</span>
res = seasonal_decompose(ts, period=<span class="hljs-number">7</span>*<span class="hljs-number">48</span>, model=<span class="hljs-string">"additive"</span>, extrapolate_trend=<span class="hljs-string">"freq"</span>)
</code></pre>
    <p class="normal">A few key parameters to keep in mind are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">period</code> is the seasonal period you expect the pattern to repeat.</li>
      <li class="bulletList"><code class="inlineCode">model</code> takes <code class="inlineCode">additive</code> or <code class="inlineCode">multiplicative</code> as an argument to determine the type of decomposition.</li>
      <li class="bulletList"><code class="inlineCode">filt</code> takes in an array that is used as the weights in the moving average (convolution, to be specific). It can also be used to define the window over which we need our moving average. We can increase it to smooth out the trend component to some extent.</li>
      <li class="bulletList"><code class="inlineCode">extrapolate_trend</code> is a parameter that we can use to extend the trend component to both sides to avoid the missing values that are generated when applying the moving average filter.</li>
      <li class="bulletList"><code class="inlineCode">two_sided</code> is a parameter that lets us define how the moving averages are calculated. If <code class="inlineCode">True</code>, which it is by default, the moving average is calculated using the past as well as future values because the window for the moving average is centered. If <code class="inlineCode">False</code>, it only uses past values to calculate the moving average.</li>
    </ul>
    <p class="normal">Let’s see <a id="_idIndexMarker230"/>how well we have been able to decompose one of the time series in our datasets. We used <code class="inlineCode">period=7*48</code> to capture a <a id="_idIndexMarker231"/>weekday-hourly profile and <code class="inlineCode">filt=np.repeat(1/(30*48), 30*48)</code> to make the moving average over 30 days with uniform weights:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_13.png" alt="Figure 3.13 – Seasonal decomposition using statsmodels "/></figure>
    <p class="packt_figref">Figure 3.13: Seasonal decomposition using statsmodels</p>
    <p class="normal">We can’t see the seasonal pattern because it’s too small in the grand scale of the plot. The associated notebook has zoomed-in plots to help you understand the seasonal pattern. Even with a large window (for example, 20 days) of smoothing, the trend still has some noise in it. We may be able to reduce this a bit more by increasing the window, but there is a better alternative, as we will see now.</p>
    <h3 id="_idParaDest-85" class="heading-3">Seasonality and trend decomposition using LOESS (STL)</h3>
    <p class="normal">As we saw <a id="_idIndexMarker232"/>earlier, LOESS is much more suited for trend estimation.<strong class="keyWord"> STL</strong> is an implementation that uses <a id="_idIndexMarker233"/>LOESS for trend estimation and period averages for seasonality. Although <code class="inlineCode">statsmodels</code> has an implementation, we have reimplemented it for better performance and flexibility. </p>
    <p class="normal">This implementation can be found in this book’s GitHub repository under <code class="inlineCode">src.decomposition.seasonal.py</code>. It expects a <code class="inlineCode">pandas</code> DataFrame or series with a datetime index as an input. Let’s see how we can use this:</p>
    <pre class="programlisting code"><code class="hljs-code">stl = STL(seasonality_period=<span class="hljs-number">7</span>*<span class="hljs-number">48</span>, model = <span class="hljs-string">"additive"</span>)
res_new = stl.fit(ts_df.energy_consumption)
</code></pre>
    <p class="normal">The key parameters here are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">seasonality_period</code> is the seasonal period you expect the pattern to repeat.</li>
      <li class="bulletList"><code class="inlineCode">model</code> takes <code class="inlineCode">additive</code> or <code class="inlineCode">multiplicative</code> as an argument to determine the type of decomposition.</li>
      <li class="bulletList"><code class="inlineCode">lo_frac</code> is the fraction of the data that will be used to fit the LOESS regression.</li>
      <li class="bulletList"><code class="inlineCode">lo_delta</code> is the fractional distance within which we use linear interpolation instead of weighted regression. Using a non-zero <code class="inlineCode">lo_delta</code> significantly decreases computation time.</li>
    </ul>
    <p class="normal">Let’s see what this decomposition looks like. Here, we used <code class="inlineCode">seasonality_period=7*48</code> to capture a weekday-hourly profile:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_14.png" alt="Figure 3.14 – STL decomposition "/></figure>
    <p class="packt_figref">Figure 3.14: STL decomposition</p>
    <p class="normal">Let’s also look at <a id="_idIndexMarker234"/>the decomposition for just one month to see the extracted seasonality patterns clearer:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_15.png" alt="Figure 3.15 – STL decomposition (zoomed-in for a month) "/></figure>
    <p class="packt_figref">Figure 3.15: STL decomposition (zoomed-in for a month)</p>
    <p class="normal">The trend is<a id="_idIndexMarker235"/> smooth enough now and seasonality has also been captured. Here, we can clearly see the hourly peaks and valleys and the higher peaks on weekends. However, since we are relying on averages to derive the seasonality, it is also highly influenced by outliers. A few very high or very low values in the time series will skew your seasonality profile that’s been derived from period averages. Another disadvantage of this technique is that the “goodness” of the seasonality that’s been extracted suffers when the difference between the resolution of the data and the expected seasonality cycle is greater. For instance, when extracting a yearly seasonality on daily or sub-daily data, this would make the extracted seasonality very noisy. This technique will also not work if you have less than two cycles of the expected seasonality—for instance, if we want to extract a yearly seasonality, but we have less than 2 years of data.</p>
    <h3 id="_idParaDest-86" class="heading-3">Fourier decomposition</h3>
    <p class="normal">We can find the<a id="_idIndexMarker236"/> Python implementation for<a id="_idIndexMarker237"/> decomposing a time series using Fourier terms in <code class="inlineCode">src.decomposition.seasonal.py</code>. It uses LOESS for trend detection and Fourier terms for seasonality extraction. There are two ways we can use it. First, we can specify <code class="inlineCode">seasonality_period</code> as one of the <code class="inlineCode">pandas</code> datetime properties (such as <code class="inlineCode">hour</code> and week_of_day):</p>
    <pre class="programlisting code"><code class="hljs-code">stl = FourierDecomposition(seasonality_period=<span class="hljs-string">"hour"</span>, model = <span class="hljs-string">"additive"</span>, n_fourier_terms=<span class="hljs-number">5</span>)
res_new = stl.fit(pd.Series(ts.squeeze(), index=ts_df.index))
</code></pre>
    <p class="normal">Alternatively, we can create any custom seasonality array that’s the same length as the time series that has an ordinal representation of the seasonality. If it is an annual seasonality of daily data, the array would have a minimum value of <code class="inlineCode">1</code> and a maximum value of <code class="inlineCode">365</code> as it increases by one every day of the year:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Making a custom seasonality term</span>
ts_df[<span class="hljs-string">"dayofweek"</span>] = ts_df.index.dayofweek
ts_df[<span class="hljs-string">"hour"</span>] = ts_df.index.hour
<span class="hljs-comment">#Creating a sorted unique combination df</span>
map_df = ts_df[[<span class="hljs-string">"dayofweek"</span>,<span class="hljs-string">"hour"</span>]].drop_duplicates().sort_values([<span class="hljs-string">"</span><span class="hljs-string">dayofweek"</span>, <span class="hljs-string">"hour"</span>])
<span class="hljs-comment"># Assigning an ordinal variable to capture the order</span>
map_df[<span class="hljs-string">"map"</span>] = np.arange(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(map_df)+<span class="hljs-number">1</span>)
<span class="hljs-comment"># mapping the ordinal mapping back to the original df and getting the seasonality array</span>
seasonality = ts_df.merge(map_df, on=[<span class="hljs-string">"dayofweek"</span>,<span class="hljs-string">"hour"</span>], how=<span class="hljs-string">'</span><span class="hljs-string">left'</span>, validate=<span class="hljs-string">"many_to_one"</span>)[<span class="hljs-string">'map'</span>]
stl = FourierDecomposition(model = <span class="hljs-string">"additive"</span>, n_fourier_terms=<span class="hljs-number">50</span>)
res_new = stl.fit(pd.Series(ts, index=ts_df.index), seasonality=seasonality)
</code></pre>
    <p class="normal">The key parameters that<a id="_idIndexMarker238"/> are involved in this process are <a id="_idIndexMarker239"/>as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">seasonality_period</code> is the seasonality to be extracted from the <em class="italic">datetime index</em>. <code class="inlineCode">pandas</code> datetime properties, such as <code class="inlineCode">week_of_day</code> and <code class="inlineCode">month</code>, can be used to specify the most prominent seasonality. If left set to <code class="inlineCode">None</code>, you need to provide the seasonality array while calling <code class="inlineCode">fit</code>.</li>
      <li class="bulletList"><code class="inlineCode">model</code> takes <code class="inlineCode">additive</code> or <code class="inlineCode">multiplicative</code> as an argument to determine the type of decomposition.</li>
      <li class="bulletList"><code class="inlineCode">n_fourier_terms</code> determines the number of Fourier terms to be used to extract the seasonality. The more we increase this parameter, the more complex the seasonality that is extracted from the data.</li>
      <li class="bulletList"><code class="inlineCode">lo_frac</code> is the fraction of the data that will be used to fit the LOESS regression.</li>
      <li class="bulletList"><code class="inlineCode">lo_delta</code> is the fractional distance within which we use linear interpolation instead of weighted regression. Using a non-zero <code class="inlineCode">lo_delta</code> significantly decreases computation time.</li>
    </ul>
    <p class="normal">Let’s see the<a id="_idIndexMarker240"/> zoomed-in plot for the decomposition <a id="_idIndexMarker241"/>using <code class="inlineCode">FourierDecomposition</code>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_16.png" alt="Figure 3.16 – Decomposition using Fourier terms (zoomed-in for a month) "/></figure>
    <p class="packt_figref">Figure 3.16: Decomposition using Fourier terms (zoomed-in for a month)</p>
    <p class="normal">The trend is going to be the same as the STL one because we are using LOESS here as well. The seasonality profile may be slightly different and robust to outliers because we are doing regularized regression using the Fourier terms on the signal. Another advantage is that we have decoupled the resolution of the data and the expected seasonality. Now, extracting a yearly seasonality on sub-daily data is not as challenging as with period averages.</p>
    <p class="normal">So far, we have only seen techniques that extract one seasonality per series; mostly, we extract the major seasonality. So, what do we do when we have multiple seasonal patterns?</p>
    <h3 id="_idParaDest-87" class="heading-3">Multiple seasonality decomposition using LOESS (MSTL)</h3>
    <p class="normal">Time<a id="_idIndexMarker242"/> series with high-frequency data (such as daily or hourly data) are prone to exhibit multiple seasonal patterns. For instance, there may be an hourly seasonality pattern, a weekly seasonality pattern, and a yearly seasonality pattern. But if we extract only the dominant pattern and leave the rest to residuals, we are not doing justice to the decomposition. Kasun Bandara et al. proposed an extension of STL decomposition for multiple seasonality, known as <strong class="keyWord">MSTL</strong>, and a corresponding implementation is present in the R ecosystem. A very similar implementation in Python can be found in <code class="inlineCode">src.decomposition.seasonal.py</code>. In addition to MSTL, the implementation extracts multiple seasonality using Fourier terms.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Kasun Bandara et al. is cited in the <em class="italic">References</em> section as reference <em class="italic">1</em>.</p>
    </div>
    <p class="normal">Let’s look at an example of how we can use this:</p>
    <pre class="programlisting code"><code class="hljs-code">stl = MultiSeasonalDecomposition(seasonal_model=<span class="hljs-string">"fourier"</span>,seasonality_periods=[<span class="hljs-string">"day_of_year"</span>, <span class="hljs-string">"day_of_week"</span>, <span class="hljs-string">"hour"</span>], model = <span class="hljs-string">"additive"</span>, n_fourier_terms=<span class="hljs-number">10</span>)
res_new = stl.fit(pd.Series(ts, index=ts_df.index))
</code></pre>
    <p class="normal">The key parameters here are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">seasonality_periods</code> is the list of expected seasonalities. For <code class="inlineCode">stl</code>, it is a list of seasonal periods, while for <code class="inlineCode">FourierDecomposition</code>, it is a list of strings that denotes <code class="inlineCode">pandas</code> datetime properties.</li>
      <li class="bulletList"><code class="inlineCode">seasonality_model</code> takes <code class="inlineCode">fourier</code> or <code class="inlineCode">averages</code> as an argument to determine the type of seasonality decomposition.</li>
      <li class="bulletList"><code class="inlineCode">model</code> takes <code class="inlineCode">additive</code> or <code class="inlineCode">multiplicative</code> as an argument to determine the type of decomposition.</li>
      <li class="bulletList"><code class="inlineCode">n_fourier_terms</code> determines the number of Fourier terms to be used to extract the seasonality. As we increase this parameter, the more complex the seasonality that is extracted from the data.</li>
      <li class="bulletList"><code class="inlineCode">lo_frac</code> is the fraction of the data that will be used to fit the LOESS regression.</li>
      <li class="bulletList"><code class="inlineCode">lo_delta</code> is the fractional distance within which we use linear interpolation instead of weighted regression. Using a non-zero <code class="inlineCode">lo_delta</code> significantly decreases computation time.</li>
    </ul>
    <p class="normal">Let’s see what the <a id="_idIndexMarker243"/>decomposition looks like when using Fourier decomposition:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_17.png" alt="Figure 3.17 – Multiple seasonality decomposition using Fourier terms "/></figure>
    <p class="packt_figref">Figure 3.17: Multiple seasonality decomposition using Fourier terms</p>
    <p class="normal">Here, we can see that the <code class="inlineCode">day_of_week</code> seasonality has been extracted. To see the <code class="inlineCode">day_of_week</code> and <code class="inlineCode">hour</code> seasonal components, we need to zoom in a bit:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_18.png" alt="Figure 3.18 – Multiple seasonality decomposition using Fourier terms (zoomed-in for a month) "/></figure>
    <p class="packt_figref">Figure 3.18: Multiple seasonality decomposition using Fourier terms (zoomed-in for a month)</p>
    <p class="normal">Here, we can<a id="_idIndexMarker244"/> observe that the <code class="inlineCode">hour</code> seasonality has been extracted well and that it has also isolated the <code class="inlineCode">day_of_week</code> seasonal component, which peaks on weekends. The <strong class="keyWord">discrete step</strong> nature of the <code class="inlineCode">day_of_week</code> seasonal component is because the frequency of the data is half-hourly, and for 48 data points, <code class="inlineCode">day_of_week</code> will be the same.</p>
    <p class="normal">We have summarized the four techniques we’ve covered in the following table:</p>
    <table id="table001-1" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Implementation</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Trend</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Seasonal</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Supports Multiple Seasonality?</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Supports Missing?</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Seasonality decomposition</p>
          </td>
          <td class="table-cell">
            <p class="normal">Moving Averages</p>
          </td>
          <td class="table-cell">
            <p class="normal">Period-Adjusted Averages</p>
          </td>
          <td class="table-cell">
            <p class="normal">No</p>
          </td>
          <td class="table-cell">
            <p class="normal">No</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">STL</p>
          </td>
          <td class="table-cell">
            <p class="normal">LOESS</p>
          </td>
          <td class="table-cell">
            <p class="normal">Period-Adjusted Averages</p>
          </td>
          <td class="table-cell">
            <p class="normal">No</p>
          </td>
          <td class="table-cell">
            <p class="normal">Yes</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Fourier decomposition</p>
          </td>
          <td class="table-cell">
            <p class="normal">LOESS</p>
          </td>
          <td class="table-cell">
            <p class="normal">Fourier Terms</p>
          </td>
          <td class="table-cell">
            <p class="normal">No</p>
          </td>
          <td class="table-cell">
            <p class="normal">No</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Multiple seasonality decomposition</p>
          </td>
          <td class="table-cell">
            <p class="normal">LOESS</p>
          </td>
          <td class="table-cell">
            <p class="normal">Period-Adjusted Averages / Fourier Terms</p>
          </td>
          <td class="table-cell">
            <p class="normal">Yes</p>
          </td>
          <td class="table-cell">
            <p class="normal">No</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 3.1: Different seasonal decomposition techniques</p>
    <div class="note">
      <p class="normal">MSTL has also been implemented in statsmodels and the accompanying notebook has the code to use that. The key difference between statsmodels and the implementation bundled within the code for the book is that the one in the book also has the option for using Fourier-series-based decomposition.</p>
    </div>
    <p class="normal">Now, let’s understand and analyze a time series dataset.</p>
    <h1 id="_idParaDest-88" class="heading-1">Detecting and treating outliers</h1>
    <p class="normal">An <strong class="keyWord">outlier</strong>, as its<a id="_idIndexMarker245"/> name suggests, is an observation that lies at an abnormal distance from the rest of the observations. If we are looking at a <strong class="keyWord">data-generating process</strong> (<strong class="keyWord">DGP</strong>) as <a id="_idIndexMarker246"/>a stochastic process that generates the time series, the outliers are the points that have the least probability of being generated from the DGP. This can be for many reasons, including faulty measurement equipment, incorrect data entry, and black-swan events, to name a few. Being able to detect such outliers and <em class="italic">treat</em> them may help your forecasting model understand the data better.</p>
    <p class="normal">Outlier/anomaly detection is a specialized field itself in time series, but in this book, we are going to restrict ourselves to simpler techniques of identifying and treating outliers. This is because our main aim is not to detect outliers, but to clean the data for our forecasting models to perform better. If you want to learn more about anomaly detection, head over to the <em class="italic">Further reading</em> section for a few resources to get started.</p>
    <p class="normal">Now, let’s look at a few <a id="_idIndexMarker247"/>techniques for identifying outliers.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code for detecting outliers, use the <code class="inlineCode">03-Outlier_Detection.ipynb</code> notebook in the <code class="inlineCode">Chapter03</code> folder.</p>
    </div>
    <h2 id="_idParaDest-89" class="heading-2">Standard deviation</h2>
    <p class="normal">This is a<a id="_idIndexMarker248"/> rule of thumb that almost everyone who has worked with data for <a id="_idIndexMarker249"/>some time would have heard of—if <img src="../Images/B22389_03_003.png" alt=""/> is the mean of the time series and <img src="../Images/B22389_03_004.png" alt=""/> is the standard deviation, then anything that falls beyond <img src="../Images/B22389_03_005.png" alt=""/> is an <strong class="keyWord">outlier</strong>. The underlying theory is deeply rooted in statistics. If we assume that <a id="_idIndexMarker250"/>the values of the time series follow a normal distribution (which is a symmetrical distribution with very desirable properties), using probability theory, we can derive that 68% of the area under the normal distribution lies within one standard deviation on either side of the mean, about 95% of the area within two standard deviations, and about 99% of the area within three standard deviations. So, when we make the bounds as three standard deviations (by using the rule of thumb), what we are saying is that if any observation whose probability of belonging to the probability distribution is less than 1%, then they are an outlier. </p>
    <p class="normal">Moving slightly to more practical issues, this cutoff of three standard deviations is in no way sacrosanct. We need to try out different values of this multiple and determine the right multiple by subjectively evaluating the results we get. The higher the multiple is, the fewer outliers there will be.</p>
    <p class="normal">For highly seasonal data, the naïve way of applying the rule to the raw time series will not work well. In such cases, we must deseasonalize the data using any of the techniques we discussed earlier and then apply the outliers to the residuals. If we don’t do that, we may flag a seasonal peak as an outlier, which is not what we want.</p>
    <p class="normal">Another key assumption here is the normal distribution. However, in reality, a lot of the time series we come across may not be normal and hence the rule will lose its theoretical guarantees fast.</p>
    <h2 id="_idParaDest-90" class="heading-2">IQR</h2>
    <p class="normal">Another<a id="_idIndexMarker251"/> very similar technique is using the <a id="_idIndexMarker252"/>IQR instead of the standard deviation to define the bounds beyond which we mark the observations as outliers. A quantile arranges all your data in order and then splits it into equal parts, so each part has the same number of items. A quartile does the same but specifically divides your data into four equal parts. IQR is the difference between the 3<sup class="superscript">rd</sup> quartile (or the 75<sup class="superscript">th</sup> percentile or 0.75 quantile) and the 1<sup class="superscript">st</sup> quartile (or the 25<sup class="superscript">th</sup> percentile or 0.25 quantile). The upper and lower bounds are defined as follows:</p>
    <ul>
      <li class="bulletList"><em class="italic">Upper bound = Q3 + n x IQR</em></li>
      <li class="bulletList"><em class="italic">Lower bound = Q1 - n x IQR</em></li>
    </ul>
    <p class="normal">Here, <em class="italic">IQR</em> = <em class="italic">Q3</em>-<em class="italic">Q2</em>, and <em class="italic">n</em> is the multiple of IQRs that determines the width of the acceptable area.</p>
    <p class="normal">For datasets where we observe high occurrences of outliers and wild variations, this is slightly more robust than the standard deviation. This is because the standard deviation and the mean are highly influenced by individual points in the dataset. If <img src="../Images/B22389_03_006.png" alt=""/> was the rule of thumb in the earlier method, here, it is 1.5 times the IQR. This also ties back to the same normal distribution assumption, and 1.5 times the IQR is equivalent to ~ <img src="../Images/B22389_03_007.png" alt=""/> (<img src="../Images/B22389_03_008.png" alt=""/> to be exact). The point about deseasonalizing before applying the rule applies here as well. It applies to all the techniques we will see here.</p>
    <h2 id="_idParaDest-91" class="heading-2">Isolation Forest</h2>
    <p class="normal"><strong class="keyWord">Isolation Forest</strong> is an <a id="_idIndexMarker253"/>unsupervised anomaly detection algorithm based on decision trees. A<a id="_idIndexMarker254"/> typical anomaly detection algorithm models the <em class="italic">normal</em> points and profiles outliers as any points that do not fit the <em class="italic">normal.</em> However, Isolation Forest takes a different path and models the outliers directly. It does this by creating a forest of decision trees by randomly splitting the feature space. This technique works on the assumption that the outlier points fall in the outer periphery and are easier to fall into a leaf node of a tree. Therefore, you can find the outliers in short branches, whereas normal points, which are closer together, will require longer branches. The “anomaly score” of any point is determined by the depth of the tree to be traversed before reaching that particular point. scikit-learn has an implementation of the algorithm under <code class="inlineCode">sklearn.ensemble.IsolationForest</code>. Apart from the standard parameters for decision trees, the key parameter here is contamination. It is set to <code class="inlineCode">auto</code> by default but can be set to any value between <code class="inlineCode">0</code> and <code class="inlineCode">0.5</code>. This<a id="_idIndexMarker255"/> parameter specifies what percentage of the dataset you expect to be anomalous. </p>
    <p class="normal">But one thing we have to keep in mind is that <code class="inlineCode">IsolationForest</code> does not <a id="_idIndexMarker256"/>consider time at all and just highlights values that fall <em class="italic">outside the norm</em>.</p>
    <h2 id="_idParaDest-92" class="heading-2">Extreme studentized deviate (ESD) and seasonal ESD (S-ESD)</h2>
    <p class="normal">This<a id="_idIndexMarker257"/> statistics-based technique is more sophisticated <a id="_idIndexMarker258"/>than the basic <img src="../Images/B22389_03_009.png" alt=""/> technique but still uses the same assumption of normality. It is<a id="_idIndexMarker259"/> based on another <a id="_idIndexMarker260"/>statistical test, called Grubbs’s test, which is used to find a <em class="italic">single outlier</em> in a normally distributed dataset. ESD iteratively uses Grubbs’s test by identifying and removing an outlier at each step. It also adjusts the critical value based on the number of points left. For a more detailed understanding of the test, go to the <em class="italic">Further reading</em> section, where we have provided a couple of resources about ESD and S-ESD. In 2017, Hochenbaum et al. from Twitter Research proposed to use the generalized ESD with <a id="_idIndexMarker261"/>deseasonalization as a method of detecting outliers for time series.</p>
    <p class="normal">We have adapted an existing implementation of the algorithm for our use case, and it is available in this book’s GitHub repository. While all the other methods leave it to the user to determine the right level of outliers by tweaking a few parameters, S-ESD only takes in an upper bound on the number of expected outliers and then identifies the outliers independently. For instance, we set the upper bound to 800 and the algorithm identified ~400 outliers in the data we are working with.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check:</strong></p>
      <p class="normal">The research paper by Hochenbaum et al. is cited in the <em class="italic">References</em> section as reference <em class="italic">2</em>.</p>
    </div>
    <p class="normal">Let’s see how the outliers were detected using all the techniques we have reviewed:</p>
    <figure class="mediaobject"><img src="../Images/B22389_03_19.png" alt="Figure 3.19 – Outliers detected using different techniques "/></figure>
    <p class="packt_figref">Figure 3.19: Outliers detected using different techniques</p>
    <p class="normal">Now that we’ve learned how to detect outliers, let’s talk about how we can treat them and clean the dataset.</p>
    <h2 id="_idParaDest-93" class="heading-2">Treating outliers</h2>
    <p class="normal">The first<a id="_idIndexMarker262"/> question that we must answer is whether or not we should correct the outliers we have identified. The statistical tests that identify outliers automatically should go through another level of human verification. If we blindly “treat” outliers, we might be chopping off a valuable pattern that will help us forecast the time series. If you are only forecasting a handful of time series, then it still makes sense to look at the outliers and anchor them to reality by looking at the causes for such outliers.</p>
    <p class="normal">But when you have thousands of time series, a human can’t inspect all the outliers, so we will have to resort to automated techniques. A common practice is to replace an outlier with a heuristic such as the maximum, minimum, and 75th percentile. A better method is to consider the outliers as missing data and use any of the techniques we discussed earlier to impute the outliers.</p>
    <p class="normal">One thing we must keep in mind is that outlier correction is not a necessary step in forecasting, especially when using modern methods such as machine learning or deep learning. Whether <a id="_idIndexMarker263"/>we do outlier correction or not is something we have to experiment with and figure out.</p>
    <p class="normal">Well done! This was a pretty busy chapter, with a lot of concepts and code, so congratulations on finishing it. Feel free to head back and revise a few topics as needed.</p>
    <h1 id="_idParaDest-94" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we learned about the key components of a time series and familiarized ourselves with terms such as trend and seasonality. We also reviewed a few time series-specific visualization techniques that will come in handy during EDA. Then, we learned about techniques that let you decompose a time series into its components and saw techniques for detecting outliers in the data. Finally, we learned how to treat the identified outliers. Now, you are all set to start forecasting the time series, which we will start in the next chapter.</p>
    <h1 id="_idParaDest-95" class="heading-1">References</h1>
    <p class="normal">The following are the references for this chapter:</p>
    <ol>
      <li class="numberedList" value="1">Kasun Bandara and Rob J Hyndman and Christoph Bergmeir. (2021). <em class="italic">MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns</em>. arXiv:2107.13462 [stat.AP]. <a href="https://arxiv.org/abs/2107.13462"><span class="url">https://arxiv.org/abs/2107.13462</span></a>.</li>
      <li class="numberedList">Hochenbaum, J., Vallis, O., &amp; Kejariwal, A. (2017). <em class="italic">Automatic Anomaly Detection in the Cloud Via Statistical Learning</em>. ArXiv, abs/1704.07706. <a href="https://arxiv.org/abs/1704.07706"><span class="url">https://arxiv.org/abs/1704.07706</span></a>.</li>
    </ol>
    <h1 id="_idParaDest-96" class="heading-1">Further reading</h1>
    <p class="normal">To learn more about the topics that were covered in this chapter, take a look at the following resources:</p>
    <ul>
      <li class="bulletList">Fourier Series: <a href="https://www.setzeus.com/public-blog-post/the-fourier-series"><span class="url">https://www.setzeus.com/public-blog-post/the-fourier-series</span></a></li>
      <li class="bulletList">Fourier Series from Khan Academy: <a href="https://www.youtube.com/watch?v=UKHBWzoOKsY"><span class="url">https://www.youtube.com/watch?v=UKHBWzoOKsY</span></a></li>
      <li class="bulletList">Fourier Transform: <a href="https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/"><span class="url">https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/</span></a></li>
      <li class="bulletList">Ane Blázquez-García, Angel Conde, Usue Mori, and Jose A. Lozano. (2021). <em class="italic">A Review on Outlier/Anomaly Detection in Time Series Data</em>. arXiv:2002.04236. <a href="https://arxiv.org/abs/2002.04236"><span class="url">https://arxiv.org/abs/2002.04236</span></a></li>
      <li class="bulletList">Braei, M., &amp; Wagner, S. (2020). <em class="italic">Anomaly Detection in Univariate Time-series: A Survey on the State-of-the-Art</em>. ArXiv, abs/2004.00433. <a href="https://arxiv.org/abs/2004.00433"><span class="url">https://arxiv.org/abs/2004.00433</span></a></li>
      <li class="bulletList">Generalized ESD Test for Outliers: <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h3.htm"><span class="url">https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h3.htm</span></a></li>
    </ul>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
    <h1 class="heading-1">Leave a Review!</h1>
    <p class="normal">Thank you for purchasing this book from Packt Publishing—we hope you enjoy it! Your feedback is invaluable and helps us improve and grow. Once you’ve completed reading it, please take a moment to leave an Amazon review; it will only take a minute, but it makes a big difference for readers like you.</p>
    <p class="normal">Scan the QR or visit the link to receive a free ebook of your choice.</p>
    <p class="normal"><a href="Chapter_03.xhtml"><span class="url">https://packt.link/NzOWQ</span></a></p>
    <p class="normal"><img src="../Images/review1.jpg" alt="A qr code with black squares  Description automatically generated"/></p>
  </div>
</body></html>