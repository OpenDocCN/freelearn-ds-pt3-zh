<html><head></head><body>
		<div>
			<div class="Content" id="_idContainer130">
			</div>
		</div>
		<div class="Content" id="_idContainer131">
			<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/>4. The Bias-Variance Trade-Off</h1>
		</div>
		<div class="Content" id="_idContainer156">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we'll cover the remaining elements of logistic regression, including what happens when you call <strong class="source-inline">.fit</strong> to train the model, and the statistical assumptions you should be aware of when using this modeling technique. You will learn how to use L1 and L2 regularization with logistic regression to prevent overfitting and how to use the practice of cross-validation to decide the regularization strength. After reading this chapter, you will be able to use logistic regression in your work and employ regularization in the model fitting process to take advantage of the bias-variance trade-off and improve model performance on unseen data.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Introduction</h1>
			<p>In this chapter, we will introduce the remaining details of logistic regression left over from the previous chapter. In addition to being able to use scikit-learn to fit logistic regression models, you will gain insight into the gradient descent procedure, which is similar to the processes that are used "under the hood" (invisible to the user) to accomplish model fitting in scikit-learn. Finally, we'll complete our discussion of the logistic regression model by familiarizing ourselves with the formal statistical assumptions of this method.</p>
			<p>We begin our exploration of the foundational machine learning concepts of overfitting, underfitting, and the bias-variance trade-off by examining how the logistic regression model can be extended to address the overfitting problem. After reviewing the mathematical details of the regularization methods that are used to alleviate overfitting, you will learn a useful practice for tuning the hyperparameters of regularization: cross-validation. Through the methods of regularization and some simple feature engineering, you will gain an understanding of how to improve both overfitted and underfitted models.</p>
			<p>Although we are focusing on logistic regression in this chapter, the concepts of overfitting, underfitting, regularization, and the bias-variance trade-off are relevant to nearly all supervised modeling techniques in machine learning.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Estimating the Coefficients and Intercepts of Logistic Regression</h1>
			<p>In the previous chapter, we learned that the coefficients of a logistic regression model (each of which goes with a particular feature), as well as the intercept, are determined using the training data when the <strong class="source-inline">.fit</strong> method is called on a logistic regression model in scikit-learn. These numbers are called the <strong class="bold">parameters</strong> of the model, and the process of finding the best values for them is called parameter <strong class="bold">estimation</strong>. Once the parameters are found, the logistic regression model is essentially a finished product: with just these numbers, we can use a logistic regression model in any environment where we can perform common mathematical functions.</p>
			<p>It is clear that the process of parameter estimation is important, since this is how we can make a predictive model from our data. So, how does parameter estimation work? To understand this, the first step is to familiarize ourselves with the concept of a <strong class="bold">cost function</strong>. A cost function is a way of telling how far away the model predictions are from perfectly describing the data. The larger the difference between the model predictions and the actual data, then the larger the "cost" returned by the cost function. </p>
			<p>This is a straightforward concept for regression problems: the difference between predictions and true values can be used for the cost, after going through a transformation (such as absolute value or squaring) to make the value of the cost positive, and then averaging this over all the training samples.</p>
			<p>For classification problems, especially in fitting logistic regression models, a typical cost function is the <strong class="bold">log-loss</strong> function, also called cross-entropy loss. This is the cost function that scikit-learn uses, in a modified form, to fit logistic regression:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer132">
					<img alt="Figure 4.1: The log-loss function&#13;&#10;" src="image/B16925_4_1.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1: The log-loss function</p>
			<p>Here, there are <em class="italic">n</em> training samples, <em class="italic">y</em><span class="subscript">i </span>is the true label (0 or 1) of the <em class="italic">i</em><span class="superscript">th</span> sample, <em class="italic">p</em><span class="subscript">i</span> is the predicted probability that the label of the <em class="italic">i</em><span class="superscript">th</span> sample equals 1, and log is the natural logarithm. The summation notation (that is, the uppercase Greek letter, sigma) over all the training samples and division by <em class="italic">n</em> serve to take the average of this cost function over all training samples. With this in mind, take a look at the following graph of the natural logarithm function and consider what the interpretation of this cost function is:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer133">
					<img alt="Figure 4.2: Natural logarithm on the interval (0, 1)&#13;&#10;" src="image/B16925_4_2.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2: Natural logarithm on the interval (0, 1)</p>
			<p>To see how the log-loss cost function works, consider its value for a sample where the true label is 1, which is <em class="italic">y = 1</em> in this case, so the second part of the cost function, <em class="italic">(1 - y</em><span class="subscript">i</span><em class="italic">)log(1 - p</em><span class="subscript">i</span><em class="italic">)</em>, will be exactly equal to 0 and will not affect the value. Then the value of the cost function is <em class="italic">-y</em><span class="subscript">i</span><em class="italic">log(p</em><span class="subscript">i</span><em class="italic">) = -log(p</em><span class="subscript">i</span><em class="italic">)</em> since <em class="italic">y</em><span class="subscript">i</span><em class="italic"> = 1</em>. So, the cost for this sample is simply the negative of the natural logarithm of the predicted probability. Now since the true label for the sample is 1, consider how the cost function should behave. We expect that for predicted probabilities that are close to 1, the cost function will be small, representing a small error for predictions that are closer to the true value. For predictions that are closer to 0, it will be larger, since the cost function is supposed to take on larger values the more "wrong" the prediction is.</p>
			<p>From the graph of the natural logarithm in <em class="italic">Figure 4.2</em> we can see that for values of <em class="italic">p</em> that are closer to 0, the natural logarithm takes on increasingly negative values. This means the cost function will take on increasingly positive values, so that the cost of classifying a positive sample with a very low probability is relatively high, as it should be. Conversely, if the predicted probability is closer to 1, then the graph indicates the cost will be closer to 0 – again, this is as expected for a prediction that is "more correct." Therefore, the cost function behaves as expected for a positive sample. A similar observation can be made for samples where the true label is 0.</p>
			<p>Now we understand how the log-loss cost function works for logistic regression. But what does this have to do with how the coefficients and the intercept are determined? We will learn in the next section.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code for generating the plots presented in this section can be found here: <a href="https://packt.link/NeF8P">https://packt.link/NeF8P</a>.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor081"/>Gradient Descent to Find Optimal Parameter Values</h2>
			<p>The problem of finding the parameter values (coefficients and intercept) for a logistic regression model using a log-loss cost boils down to a problem of <strong class="bold">optimization</strong>: we would like to find the set of parameters that results in the <strong class="bold">minimum</strong> cost, since costs are higher for worse predictions. In other words, we want the set of parameters that is the "least wrong" on average over all of the training samples. This process is done for you automatically by the <strong class="source-inline">.fit</strong> method of the logistic regression model in scikit-learn. There are different solution techniques for finding the set of parameters with the lowest cost, and you can choose which one you would like to use with the <strong class="source-inline">solver</strong> keyword when you are instantiating the model class. All of these methods work somewhat differently. However, they are all based on the concept of <strong class="bold">gradient descent</strong>.</p>
			<p>The gradient descent process starts with an <strong class="bold">initial guess</strong>. The choice of the initial guess is not that important for logistic regression and you don't need to make it manually; this is handled by the <strong class="source-inline">solver</strong> keyword. However, for more advanced machine learning algorithms such as deep neural networks, selection of the initial guesses for parameters requires more attention.</p>
			<p>For the sake of illustration, we will consider a problem where there is only one parameter to estimate. We'll look at the value of a hypothetical cost function (<em class="italic">y = f(x) = x</em><span class="superscript">2</span><em class="italic"> – 2x</em>) and devise a gradient descent procedure to find the value of the parameter, <em class="italic">x</em>, for which the cost, <em class="italic">y</em>, is the lowest. Here, we choose some <em class="italic">x</em> values, create a function that returns the value of the cost function, and look at the value of the cost function over this range of parameters.</p>
			<p>The code to do this is as follows:</p>
			<p class="source-code">X_poly = np.linspace(-3,5,81)</p>
			<p class="source-code">print(X_poly[:5], '...', X_poly[-5:])</p>
			<p>Here is the output of the print statement:</p>
			<p class="source-code">[-3. -2.9 -2.8 -2.7 -2.6] ... [4.6 4.7 4.8 4.9 5. ]</p>
			<p>The remaining code snippet is as follows:</p>
			<p class="source-code">def cost_function(X):</p>
			<p class="source-code">    return X * (X-2)</p>
			<p class="source-code">y_poly = cost_function(X_poly)</p>
			<p class="source-code">plt.plot(X_poly, y_poly)</p>
			<p class="source-code">plt.xlabel('Parameter value')</p>
			<p class="source-code">plt.ylabel('Cost function')</p>
			<p class="source-code">plt.title('Error surface')</p>
			<p>The resulting plot should appear as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer134">
					<img alt="Figure 4.3: A cost function plot&#13;&#10;" src="image/B16925_4_3.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3: A cost function plot</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In the preceding code snippets, we assume that you would have imported the necessary libraries. You can refer to the following notebook for the complete code for the chapter including the import statement for the preceding snippets: <a href="https://packt.link/A4VyF">https://packt.link/A4VyF</a>.</p>
			<p>Looking at the <strong class="bold">error surface</strong> in <em class="italic">Figure 4.3</em>, which is the plot of the cost function over a range of parameter values, it's pretty evident what parameter value will result in the lowest value of the cost function: <em class="italic">x = 1</em>. In fact, with some calculus, you could easily confirm this by setting the derivative to zero and then solving for <em class="italic">x</em>, confirming that <em class="italic">x = 1</em> is the minimum. However, generally speaking, it is not always feasible to solve the problem so simply. In cases where it is necessary to use gradient descent, we don't always know how the entire error surface looks. Rather, after we've chosen the initial guess for the parameter, all we're able to know is the direction of the error surface in the immediate vicinity of that point.</p>
			<p><strong class="bold">Gradient descent</strong> is an iterative algorithm; starting from the initial guess, we try to find a new guess that lowers the cost function and continue with this until we've found a good solution. We are trying to move "downhill" on the error surface, but we only know which direction to move in and how far to move in that direction, based on the shape of the error surface in the immediate neighborhood of our current guess. In mathematical terms, we only know the value of the <strong class="bold">derivative</strong> (which is called the <strong class="bold">gradient</strong> in more than one dimension) at the parameter value of the current guess. If you have not studied calculus, you can think of the gradient as telling you which direction is downhill, and how steep the hill is from where you're standing. We use this information to "take a step" in the direction of decreasing error. How big a step we decide to take depends on the <strong class="bold">learning rate</strong>. Since the gradient declines toward the direction of decreasing error, we want to take a step in the direction that is the negative of the gradient.</p>
			<p>These notions can be formalized in the following equation. To get to the new guess, <em class="italic">x</em><span class="subscript">new</span>, from the current guess, <em class="italic">x</em><span class="subscript">old</span>, where <em class="italic">f'(x</em><span class="subscript">old</span><em class="italic">)</em> is the derivative (that is, the gradient) of the cost function at the current guess:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer135">
					<img alt="Figure 4.4: Equation to obtain the new guess from the current guess&#13;&#10;" src="image/B16925_4_4.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4: Equation to obtain the new guess from the current guess</p>
			<p>In the following graph, we can see the results of starting a gradient descent procedure from <em class="italic">x = 4.5</em>, with a learning rate of 0.75, and then optimizing <em class="italic">x</em> to attain the lowest value of the cost function:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer136">
					<img alt="Figure 4.5: The gradient descent path&#13;&#10;" src="image/B16925_4_5.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5: The gradient descent path</p>
			<p>Gradient descent also works in higher-dimensional spaces; in other words, with more than one parameter. However, you can only visualize up to a two-dimensional error surface (that is, two parameters at a time on a three-dimensional plot) on a single graph.</p>
			<p>Having described the workings of gradient descent, let's perform an exercise to implement the gradient descent algorithm, expanding on the example of this section.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code for generating the plots presented in this section can be found here: <a href="https://packt.link/NeF8P">https://packt.link/NeF8P</a>. If you're reading the print version of this book, you can download and browse the color versions of some of the images in this chapter by visiting the following link: <a href="https://packt.link/FAXBM">https://packt.link/FAXBM</a></p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/>Exercise 4.01: Using Gradient Descent to Minimize a Cost Function</h2>
			<p>In this exercise, our task is to find the best set of parameters in order to minimize the following hypothetical cost function: <em class="italic">y = f(x) = x</em><span class="superscript">2</span><em class="italic"> – 2x</em>. To do this, we will employ gradient descent, which was described in the preceding section. Perform the following steps to complete the exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before you begin this exercise, please make sure you have executed the prerequisite steps of importing the necessary libraries and loading the cleaned dataframe. These steps along with the code for this exercise can be found at <a href="https://packt.link/NeF8P">https://packt.link/NeF8P</a>.</p>
			<ol>
				<li>Create a function that returns the value of the cost function and look at the value of the cost function over a range of parameters. You can use the following code to do this (note that this repeats code from the preceding section):<p class="source-code">X_poly = np.linspace(-3,5,81)</p><p class="source-code">print(X_poly[:5], '...', X_poly[-5:])</p><p class="source-code">def cost_function(X):</p><p class="source-code">    return X * (X-2)</p><p class="source-code">y_poly = cost_function(X_poly)</p><p class="source-code">plt.plot(X_poly, y_poly)</p><p class="source-code">plt.xlabel('Parameter value')</p><p class="source-code">plt.ylabel('Cost function')</p><p class="source-code">plt.title('Error surface')</p><p>You will obtain the following plot of the cost function:</p><div class="IMG---Figure" id="_idContainer137"><img alt="Figure 4.6: A cost function plot&#13;&#10;" src="image/B16925_4_6.jpg"/></div><p class="figure-caption">Figure 4.6: A cost function plot</p></li>
				<li>Create a function for the value of the gradient. This is the analytical derivative of the cost function. Use this function to evaluate the gradient at the point <em class="italic">x = 4.5</em>, and then use this in combination with the learning rate to find the next step of the gradient descent process:<p class="source-code">def gradient(X):</p><p class="source-code">    return (2*X) - 2</p><p class="source-code">x_start = 4.5</p><p class="source-code">learning_rate = 0.75</p><p class="source-code">x_next = x_start - gradient(x_start)*learning_rate</p><p class="source-code">x_next</p><p class="callout-heading">Note</p><p class="callout">It doesn't matter if you haven't studied calculus and don't understand this part; you can just take it as a given that this is the function for the gradient. In some applications, it's not actually possible to calculate an analytical derivative, so this may need to be numerically approximated.</p><p>After running the cell with <strong class="source-inline">x_next</strong>, you will obtain the following output:</p><p class="source-code">-0.75</p><p>This is the next gradient descent step after <em class="italic">x = 4.5</em>.</p></li>
				<li>Plot the gradient descent path, from the starting point to the next point, using the following code:<p class="source-code">plt.plot(X_poly, y_poly)</p><p class="source-code">plt.plot([x_start, x_next],</p><p class="source-code">         [cost_function(x_start), cost_function(x_next)],</p><p class="source-code">         '-o')</p><p class="source-code">plt.xlabel('Parameter value')</p><p class="source-code">plt.ylabel('Cost function')</p><p class="source-code">plt.legend(['Error surface', 'Gradient descent path'])</p><p>You will obtain the following output:</p><div class="IMG---Figure" id="_idContainer138"><img alt="Figure 4.7: The first gradient descent path step&#13;&#10;" src="image/B16925_4_7.jpg"/></div><p class="figure-caption">Figure 4.7: The first gradient descent path step</p><p>Here, it appears as though we've taken a step in the right direction. However, it's clear that we've overshot where we want to be. It may be that our learning rate is too large, and consequently, we are taking steps that are too big. While tuning the learning rate will be a good way to converge toward an optimal solution more quickly, in this example, we can just continue illustrating the remainder of the process. Here, it looks like we may need to take a few more steps. In practice, gradient descent continues until the size of the steps become very small, or the change in the cost function becomes very small (you can specify how small by using the <strong class="source-inline">tol</strong> argument in the scikit-learn logistic regression), indicating that we're close enough to a good solution – that is, a <strong class="bold">local minimum</strong> of the cost function. For this example, we'll just take a total of 14 steps, or <strong class="bold">iterations</strong>, beyond the initial guess (note that you can also set the maximum number of iterations in scikit-learn with <strong class="source-inline">max_iter</strong>).</p></li>
				<li>Perform 14 iterations to converge toward the local minimum of the cost function by using the following code snippet (note that <strong class="source-inline">iterations = 15</strong>, but the endpoint is not included in the call to <strong class="source-inline">range()</strong>):<p class="source-code">iterations = 15</p><p class="source-code">x_path = np.empty(iterations,)</p><p class="source-code">x_path[0] = x_start</p><p class="source-code">for iteration_count in range(1,iterations):</p><p class="source-code">    derivative = gradient(x_path[iteration_count-1])</p><p class="source-code">    x_path[iteration_count] = x_path[iteration_count-1] \</p><p class="source-code">                              - (derivative*learning_rate)</p><p class="source-code">x_path</p><p>You will obtain the following output:</p><p class="source-code">array([ 4.5       , -0.75      ,  1.875     ,  0.5625    ,  1.21875   ,</p><p class="source-code">        0.890625  ,  1.0546875 ,  0.97265625,  1.01367188,  0.99316406,</p><p class="source-code">        1.00341797,  0.99829102,  1.00085449,  0.99957275,  1.00021362])</p><p>This <strong class="source-inline">for</strong> loop stores the successive estimates in the <strong class="source-inline">x_path</strong> array, using the current estimate to calculate the derivative and find the next estimate. From the resulting values of the gradient descent process, it looks like we've gotten very close (<strong class="source-inline">1.00021362</strong>) to the optimal solution of 1.</p></li>
				<li>Plot the gradient descent path using the following code:<p class="source-code">plt.plot(X_poly, y_poly)</p><p class="source-code">plt.plot(x_path, cost_function(x_path), '-o')</p><p class="source-code">plt.xlabel('Parameter value')</p><p class="source-code">plt.ylabel('Cost function')</p><p class="source-code">plt.legend(['Error surface', 'Gradient descent path'])</p><p>You will obtain the following output:</p><div class="IMG---Figure" id="_idContainer139"><img alt="Figure 4.8: The gradient descent path&#13;&#10;" src="image/B16925_4_8.jpg"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.8: The gradient descent path</p>
			<p>We encourage you to repeat the previous procedure with different learning rates in order to see how they affect the gradient descent path. With the right learning rate, it's possible to converge on a highly accurate solution very quickly. While the choice of learning rate can be important in different machine learning applications, for logistic regression, the problem is usually pretty easy to solve and you don't need to select a learning rate in scikit-learn.</p>
			<p>As you experimented with different learning rates, did you notice what happened when the learning rate was greater than one? In this case, the step that we take in the direction of the decreasing error is too large and we actually wind up with a higher error. This problem can compound itself and actually lead the gradient descent process away from the region of minimum error. On the other hand, if the step size is too small, it can take a very long time to find the desired solution.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/>Assumptions of Logistic Regression</h2>
			<p>Since it is a classical statistical model, similar to the F-test and Pearson correlation we already examined, logistic regression makes certain assumptions about the data. While it's not necessary to follow every one of these assumptions in the strictest possible sense, it's good to be aware of them. That way, if a logistic regression model is not performing very well, you can try to investigate and figure out why, using your knowledge of the ideal situation that logistic regression is intended for. You may find slightly different lists of the specific assumptions from different resources. However, those that are listed here are widely accepted.</p>
			<p><strong class="bold">Features Are Linear in the Log Odds</strong></p>
			<p>We learned about this assumption in the previous chapter, <em class="italic">Chapter 3</em>, <em class="italic">Details of Logistic Regression and Feature Exploration</em>. Logistic regression is a linear model, so it will only work well as long as the features are effective at describing a linear trend in the log odds. In particular, logistic regression won't capture interactions, polynomial features, or the discretization of features, on its own. You can, however, specify all of these as "new features" – even though they may be engineered from existing features.</p>
			<p>Remember from the previous chapter that the most important feature from univariate feature exploration, <strong class="source-inline">PAY_1</strong>, was not found to be linear in the log odds.</p>
			<p><strong class="bold">No Multicollinearity of Features</strong></p>
			<p>Multicollinearity means that features are correlated with each other. The worst violation of this assumption is when features are perfectly correlated with each other, such as one feature being identical to another, or when one feature equals another multiplied by a constant. We can investigate the correlation of features using the correlation plot that we're already familiar with from univariate feature selection. Here is the correlation plot from the previous chapter:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer140">
					<img alt="Figure 4.9: A correlation plot of features and the response&#13;&#10;" src="image/B16925_4_9.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9: A correlation plot of features and the response</p>
			<p>We can see from the correlation plot what perfect correlation looks like: since every feature and the response variable has a correlation of 1 with itself, we can see that a correlation of 1 is a light, cream color. From the color bar, which doesn't include -1, we know there are no correlations with that value.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Jupyter notebook containing the code and the corresponding plots presented in this section can be found here: <a href="https://packt.link/UOEMp">https://packt.link/UOEMp</a>.</p>
			<p>The clearest examples of correlated predictors in our case study data are the <strong class="source-inline">BILL_AMT</strong> features. It makes intuitive sense that bills might be similar from month to month for a given account. For instance, there may be an account that typically carries a balance of zero, or an account that has a large balance that is taking a while to pay off. Are any of the <strong class="source-inline">BILL_AMT</strong> features perfectly correlated? From <em class="italic">Figure 4.9</em>, it does not look like it. So, while these features may not contribute much independent information, we won't remove them at this point out of concern for multicollinearity.</p>
			<p><strong class="bold">The Independence of Observations</strong></p>
			<p>This is a common assumption in classical statistical models, including linear regression. Here, the observations (or samples) are assumed to be independent. Does this make sense with the case study data? We'd want to confirm with our client whether the same individual can hold multiple credit accounts across the dataset and consider what to do depending on how common it was. Let's assume we've been told that in our data each credit account belongs to a unique person, so we may assume independence of observations in this respect.</p>
			<p>Across different domains of data, some common violations of independence of observations are as follows:</p>
			<ul>
				<li><strong class="bold">Spatial autocorrelation</strong> of observations; for example, in natural phenomena such as soil types, where observations that are geographically close to each other may be similar to each other.</li>
				<li><strong class="bold">Temporal autocorrelation</strong> of observations, which may occur in time series data. Observations at the current point in time are usually assumed to be correlated to the most recent point(s) in time.</li>
			</ul>
			<p>However, these issues are not relevant to our case study data.</p>
			<p><strong class="bold">No Outliers</strong></p>
			<p>Outliers are observations where the value of the feature(s) or response are very far from most of the data or are different in some other way. A more appropriate term for an outlier observation of a feature value is a high leverage point, as the term "outlier" is usually applied to the response variable. However, in our binary classification problem, it's not possible to have an outlier value of the response variable, since it can only take on the values 0 and 1. In practice, you may see both of these terms used to refer to features.</p>
			<p>To see why these kinds of points can have an adverse effect on linear models in general, take a look at this synthetic linear data with 100 points and the line of best fit that results from linear regression:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer141">
					<img alt="Figure 4.10: “Well-behaved” linear data and a regression fit&#13;&#10;" src="image/B16925_4_10.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10: "Well-behaved" linear data and a regression fit</p>
			<p>Here, the model intuitively appears to be a good fit for the data. However, what if an outlier feature value is added? To illustrate this, we add a point with an x value that is very different from most of the observations and a y value that is in a similar range to the other observations. We then show the resulting regression line:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer142">
					<img alt="Figure 4.11: A plot showing what happens when an outlier is included&#13;&#10;" src="image/B16925_4_11.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11: A plot showing what happens when an outlier is included</p>
			<p>Due to the presence of a single high leverage point, the regression model fit for all the data is no longer a very good representation of much of the data. This shows the potential effect of just a single data point on linear models, especially if that point doesn't appear to follow the same trend as the rest of the data. </p>
			<p>There are methods for dealing with outliers. But a more fundamental question to ask is "Is data like this realistic?". If the data doesn't seem right, it is a good idea to ask the client whether the outliers are believable. If not, they should be excluded. However, if they do represent valid data, then non-linear models or other methods should be used.</p>
			<p>With our case study data, we did not observe outliers in the histograms that we plotted during feature exploration. Therefore, we don't have this concern.</p>
			<p><strong class="bold">How Many Features Should You Include?</strong></p>
			<p>This is not so much an assumption as it is guidance on model building. There is no clear-cut law that states how many features to include in a logistic regression model. However, a common rule of thumb is the "rule of 10," which states that for every 10 occurrences of the rarest outcome class, 1 feature may be added to the model. So, for example, in a binary logistic regression problem with 100 samples, if the class balance has 20% positive outcomes and 80% negative outcomes, then there are only 20 positive outcomes in total, and so only 2 features should be used in the model. A "rule of 20" has also been suggested, which would be a more stringent limit on the number of features to include (1 feature in our example).</p>
			<p>Another point to consider in the case of binary features, such as those that result from one-hot encoding, is how many samples will have a positive value for that feature. If the feature is very imbalanced, in other words, with very few samples containing either a 1 or a 0, it may not make sense to include it in the model.</p>
			<p>For the case study data, we are fortunate to have a relatively large number of samples and relatively balanced features, so these are not concerns.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code for generating the plots presented in this section can be found here: <a href="https://packt.link/SnX3y">https://packt.link/SnX3y</a>.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor084"/>The Motivation for Regularization: The Bias-Variance Trade-Off</h2>
			<p>We can extend the basic logistic regression model that we have learned about by using a powerful concept known as <strong class="bold">shrinkage</strong> or <strong class="bold">regularization</strong>. In fact, every logistic regression that you have fit so far in scikit-learn has used some amount of regularization. That is because it is a default option in the logistic regression model object. However, until now, we have ignored it.</p>
			<p>As you learn about these concepts in greater depth, you will also become familiar with a few foundational concepts in machine learning: <strong class="bold">overfitting</strong>, <strong class="bold">underfitting</strong>, and the <strong class="bold">bias-variance trade-off</strong>. A model is said to overfit the training data if the performance of the model on the training data (for example, the ROC AUC) is substantially better than the performance on a held-out test set. In other words, good performance on the training set does not generalize to the unseen test set. We started to discuss these concepts in <em class="italic">Chapter 2</em>, <em class="italic">Introduction to Scikit-Learn and Model Evaluation</em>, when we distinguished between model training and test scores.</p>
			<p>When a model is overfitted to the training data, it is said to have high <strong class="bold">variance</strong>. In other words, whatever variability exists in the training data, the model has learned this very well – in fact, too well. This will be reflected in a high model training score. However, when such a model is used to make predictions on new and unseen data, the performance is lower. Overfitting is more likely in the following circumstances:</p>
			<ul>
				<li>There are a large number of features available in relation to the number of samples. In particular, there may be so many possible features that it is cumbersome to directly inspect all of them, like we were able to do with the case study data.</li>
				<li>A complex model, that is, more complex than logistic regression, is used. These include models such as gradient boosting ensembles or neural networks.</li>
			</ul>
			<p>Under these circumstances, the model has an opportunity develop more complex <strong class="bold">hypotheses</strong> about the relationships between features and the response variable in the training data during model fitting, making overfitting more likely.</p>
			<p>In contrast, if a model is not fitting the training data very well, this is known as underfitting, and the model is said to have high <strong class="bold">bias</strong>.</p>
			<p>We can examine the differences between underfitting, overfitting, and the ideal that sits in between, by fitting polynomial models on some hypothetical data:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer143">
					<img alt="Figure 4.12: Quadratic data with underfit, overfit, and ideal models&#13;&#10;" src="image/B16925_4_12.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12: Quadratic data with underfit, overfit, and ideal models</p>
			<p>In <em class="italic">Figure 4.12</em>, we can see that including too few features, in this case, a linear model of <em class="italic">y</em> with just two features, a slope and an intercept, is clearly not a good representation of the data. This is known as an underfit model. However, if we include too many features, that is, many high-degree polynomial terms, such as <em class="italic">x</em><span class="superscript">2</span>, <em class="italic">x</em><span class="superscript">3</span>, <em class="italic">x</em><span class="superscript">4</span>,… <em class="italic">x</em><span class="superscript">10</span>, we can fit the training data almost perfectly. However, this is not necessarily a good thing. When we look at the results of the overfitted model in between the training data points, where new predictions may need to be made, we can see that the model is unstable and may not provide reliable predictions for data that was not in the training set. We can tell this just based on an intuitive understanding of the relationship between the features and the response variable, which we can get from visualizing the data.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code for generating the plots presented in this section can be found here: <a href="https://packt.link/SnX3y">https://packt.link/SnX3y</a>.</p>
			<p>The synthetic data for this example was generated by a second-degree (that is, quadratic) polynomial. Knowing this, we could easily find the ideal model by fitting a second-degree polynomial to the training data, as shown in <em class="italic">Figure 4.12</em>. </p>
			<p>In general, however, we won't know what the ideal model formulation is ahead of time. For this reason, we need to compare training and test scores to assess whether a model may be overfitting or underfitting.</p>
			<p>In some cases, it may be desirable to introduce some bias into the model training process, especially if this decreases overfitting and increases model performance on new, unseen data. In this way, it may be possible to leverage the bias-variance trade-off to improve a model. We can use <strong class="bold">regularization</strong> methods to accomplish this. Additionally, we may also be able to use these methods for <strong class="bold">variable selection</strong> as part of the modeling process. Using a predictive model to select variables is an alternative to the univariate feature selection methods that we've already explored. We begin to experiment with these concepts in the following exercise.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor085"/>Exercise 4.02: Generating and Modeling Synthetic Classification Data </h2>
			<p>In this exercise, we'll observe overfitting in practice by using a synthetic dataset. Consider yourself in the situation of having been given a binary classification dataset with many candidate features (200), where you don't have time to look through all of them individually. It's possible that some of these features are highly correlated or related in some other way. However, with this many variables, it can be difficult to effectively explore all of them. Additionally, the dataset has relatively few samples: only 1,000. We are going to generate this challenging dataset by using a feature of scikit-learn that allows you to create synthetic datasets for making conceptual explorations such as this. Perform the following steps to complete the exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before you begin this exercise, please make sure you have executed the prerequisite steps of importing the necessary libraries. These steps along with the code for this exercise can be found at <a href="https://packt.link/mIMsT">https://packt.link/mIMsT</a>.</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">make_classification</strong>, <strong class="source-inline">train_test_split</strong>, <strong class="source-inline">LogisticRegression</strong>, and <strong class="source-inline">roc_auc_score</strong> classes using the following code:<p class="source-code">from sklearn.datasets import make_classification</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.metrics import roc_auc_score</p><p>Notice that we've imported several familiar classes from scikit-learn, in addition to a new one that we haven't seen before: <strong class="source-inline">make_classification</strong>. This class does just what its name indicates – it makes data for a classification problem. Using the various keyword arguments, you can specify how many samples and features to include, and how many classes the response variable will have. There is also a range of other options that effectively control how "easy" the problem will be to solve.</p><p class="callout-heading">Note</p><p class="callout">For more information, refer to <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html</a>. Suffice to say that we've selected options here that make a reasonably easy-to-solve problem, with some curveballs thrown in. In other words, we expect high model performance, but we'll have to work a little bit to get it.</p></li>
				<li>Generate a dataset with two variables, <strong class="source-inline">x_synthetic</strong> and <strong class="source-inline">y_synthetic</strong>. <strong class="source-inline">x_synthetic</strong> has the 200 candidate features, and <strong class="source-inline">y_synthetic</strong> the response variable, each for 1,000 samples. Use the following code:<p class="source-code">X_synthetic, y_synthetic = make_classification(</p><p class="source-code">    n_samples=1000, n_features=200,</p><p class="source-code">    n_informative=3, n_redundant=10,</p><p class="source-code">    n_repeated=0, n_classes=2,</p><p class="source-code">    n_clusters_per_class=2,</p><p class="source-code">    weights=None, flip_y=0.01,</p><p class="source-code">    class_sep=0.8, hypercube=True,</p><p class="source-code">    shift=0.0, scale=1.0,</p><p class="source-code">    shuffle=True, random_state=24)</p></li>
				<li>Examine the shape of the dataset and the class fraction of the response variable using the following code:<p class="source-code">print(X_synthetic.shape, y_synthetic.shape)</p><p class="source-code">print(np.mean(y_synthetic))</p><p>You will obtain the following output:</p><p class="source-code">(1000, 200) (1000,)</p><p class="source-code">0.501</p><p>After checking the shape of the output, note that we've generated an almost perfectly balanced dataset: close to a 50/50 class balance. It is also important to note that we've generated all the features so that they have the same <strong class="source-inline">shift</strong> and <strong class="source-inline">scale</strong> – that is, a mean of 0 with a standard deviation of 1. Making sure that the features are on the same scale, or have roughly the same range of values, is a key point for using regularization methods – and we'll see why later. If the features in a raw dataset are on widely different scales, it is advisable to normalize them so that they are on the same scale. Scikit-learn has the functionality to make this easy, which we'll learn about in the activity at the end of this chapter.</p></li>
				<li>Plot the first few features as histograms to show that the range of values is the same using the following code:<p class="source-code">for plot_index in range(4):</p><p class="source-code">    plt.subplot(2, 2, plot_index+1)</p><p class="source-code">    plt.hist(X_synthetic[:, plot_index])</p><p class="source-code">    plt.title('Histogram for feature {}'.format(plot_index+1))</p><p class="source-code">plt.tight_layout()</p><p>You will obtain the following output:</p><div class="IMG---Figure" id="_idContainer144"><img alt="Figure 4.13: Histograms for the first 4 of 200 synthetic features&#13;&#10;" src="image/B16925_4_13.jpg"/></div><p class="figure-caption">Figure 4.13: Histograms for the first 4 of 200 synthetic features</p><p>Because we generated this dataset, we don't need to directly examine all 200 features to make sure that they're on the same scale. So, what are the possible concerns with this dataset? The data is balanced in terms of the class fractions of the response variable, so we don't need to undersample, oversample, or use other methods that are helpful for imbalanced data. What about relationships among the features themselves, and the features and response variable? There are a lot of these relationships and it is a challenge to investigate them all directly. Based on our rule of thumb (that is, 1 feature allowed for every 10 samples of the rarest class), 200 features is too many. We have 500 observations in the rarest class, so by that rule, we shouldn't have more than 50 features. It's possible that with so many features, the model training procedure will overfit. We will now start to learn how to use options in the scikit-learn logistic regression to prevent this.</p></li>
				<li>Split the data into training and test sets using an 80/20 split, and then instantiate a logistic regression model object using the following code:<p class="source-code">X_syn_train, X_syn_test, y_syn_train, y_syn_test = \</p><p class="source-code">train_test_split(X_synthetic, y_synthetic,\</p><p class="source-code">                 test_size=0.2, random_state=24)</p><p class="source-code">lr_syn = LogisticRegression(solver='liblinear', penalty='l1',</p><p class="source-code">                            C=1000, random_state=1)</p><p class="source-code">lr_syn.fit(X_syn_train, y_syn_train)</p><p>Notice here that we are specifying some new options in the logistic regression model, which, so far, we have not paid attention to. First, we specify the <strong class="source-inline">penalty</strong> argument to be <strong class="source-inline">l1</strong>. This means we are going to use <strong class="bold">L1 regularization</strong>, which is also known as <strong class="bold">lasso regularization</strong>. We'll discuss the mathematical definition of this shortly. Second, notice that we have set the <strong class="source-inline">C</strong> parameter to be equal to 1,000. <strong class="source-inline">C</strong> is the "inverse of regularization strength," according to the scikit-learn documentation (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>). This means that higher values of <strong class="source-inline">C</strong> correspond to less regularization. By choosing a relatively large number, such as 1,000, we are using relatively little regularization. The default value of <strong class="source-inline">C</strong> is 1. So, we are not really using much regularization here, rather, we are simply becoming familiar with the options to do so. Finally, we are using the <strong class="source-inline">liblinear</strong> solver, which we have used in the past. </p><p>Although we happen to be using scaled data here (all features have a mean of 0 and standard deviation of 1), it's worth noting at this point that among the various options we have available for solvers, <strong class="source-inline">liblinear</strong> is "robust to unscaled data." Also note that <strong class="source-inline">liblinear</strong> is one of only two solver options that support the L1 penalty – the other option being <strong class="source-inline">saga</strong>.</p><p class="callout-heading">Note</p><p class="callout">You can find out more information on available solvers at <a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</a>.</p></li>
				<li>Fit the logistic regression model on the training data using the following code:<p class="source-code">lr_syn.fit(X_syn_train, y_syn_train)</p><p>Here is the output:</p><p class="source-code">LogisticRegression(C=1000, penalty='l1', random_state=1, \</p><p class="source-code">                   solver='liblinear')</p></li>
				<li>Calculate the training score using this code by first getting predicted probabilities and then obtaining the ROC AUC:<p class="source-code">y_syn_train_predict_proba = lr_syn.predict_proba(X_syn_train)</p><p class="source-code">roc_auc_score(y_syn_train, y_syn_train_predict_proba[:,1])</p><p>The output should be as follows:</p><p class="source-code">0.9420000000000001</p></li>
				<li>Calculate the test score similar to how the training score was obtained:<p class="source-code">y_syn_test_predict_proba = lr_syn.predict_proba(X_syn_test)</p><p class="source-code">roc_auc_score(y_syn_test, y_syn_test_predict_proba[:,1])</p><p>The output should be as follows:</p><p class="source-code">0.8075807580758075</p><p>From these results, it's apparent that the logistic regression model has overfit the data. That is, the ROC AUC score on the training data is substantially higher than that of the test data.</p></li>
			</ol>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>Lasso (L1) and Ridge (L2) Regularization</h2>
			<p>Before applying regularization to a logistic regression model, let's take a moment to understand what regularization is and how it works. The two ways of regularizing logistic regression models in scikit-learn are called <strong class="bold">lasso</strong> (also known as <strong class="bold">L1</strong> regularization) and <strong class="bold">ridge</strong> (also known as <strong class="bold">L2</strong> regularization). When instantiating the model object from the scikit-learn class, you can choose <strong class="source-inline">penalty = 'l1'</strong> or <strong class="source-inline">'l2'</strong>. These are called "penalties" because the effect of regularization is to add a penalty, or a cost, for having larger values of the coefficients in a fitted logistic regression model.</p>
			<p>As we've already learned, coefficients in a logistic regression model describe the relationship between the log odds of the response and each of the features. Therefore, if a coefficient value is particularly large, then a small change in that feature will have a large effect on the prediction. When a model is being fit and is learning the relationship between features and the response variable, the model can start to learn the noise in the data. We saw this previously in <em class="italic">Figure 4.12</em>: if there are many features available when fitting a model, and there are no guardrails on the values that their coefficients can take, then the model fitting process may try to discover relationships between the features and the response variable that won't generalize to new data. In this way, the model becomes tuned to the unpredictable, random noise that accompanies real-world, imperfect data. Unfortunately, this only serves to increase the model's skill at predicting the training data, which is not our ultimate goal. Therefore, we should seek to root out such spurious relationships from the model.</p>
			<p>Lasso and ridge regularization use different mathematical formulations to accomplish this goal. These methods work by making changes to the cost function that is used for model fitting, which we introduced previously as the log-loss function. Lasso regularization uses what is called the <strong class="bold">1-norm</strong> (hence the term L1):</p>
			<div>
				<div class="IMG---Figure" id="_idContainer145">
					<img alt="Figure 4.14: Log-loss equation with lasso penalty&#13;&#10;" src="image/B16925_4_14.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.14: Log-loss equation with lasso penalty</p>
			<p>The 1-norm, which is the first term in the equation in <em class="italic">Figure 4.14</em>, is just the sum of the absolute values of the coefficients of the <em class="italic">m</em> different features. The absolute value is used because having a coefficient that's large in either the positive or negative directions can contribute to overfitting. So, what else is different about this cost function compared to the log-loss function that we saw earlier? Well, now there is a <em class="italic">C</em> factor that is multiplied by the fraction in front of the sum of the log-loss function. </p>
			<p>This is the "inverse of regularization strength," as described in the scikit-learn documentation (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>). Since this factor is in front of the term of the cost function that calculates the prediction error, as opposed to the term that does regularization, then making it larger makes the prediction error more important in the cost function, while regularization is made less important. In short, <em class="italic">larger values of C lead to less regularization</em> in the scikit-learn implementation.</p>
			<p>L2, or ridge regularization, is similar to L1, except that instead of the sum of absolute values of coefficients, ridge uses the sum of their squares, called the <strong class="bold">2-norm</strong>:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer146">
					<img alt="Figure 4.15: Log-loss equation with ridge penalty&#13;&#10;" src="image/B16925_4_15.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.15: Log-loss equation with ridge penalty</p>
			<p>Note that if you look at the cost functions for logistic regression in the scikit-learn documentation, the specific form is different than what is used here, but the overall idea is similar. Additionally, after you become comfortable with the concepts of lasso and ridge penalties, you should be aware that there is an additional regularization method called <strong class="bold">elastic-net</strong>, which is a combination lasso and ridge.</p>
			<p><strong class="bold">Why Are There Two Different Formulations of Regularization?</strong></p>
			<p>It may be that one or the other will provide better out-of-sample performance, so you may wish to test them both. There is another key difference in these methods: the L1 penalty also performs feature selection, in addition to regularization. It does this by setting some coefficient values to exactly zero during the regularization process, effectively removing features from the model. L2 regularization makes the coefficient values smaller but does not completely eliminate them. Not all solver options in scikit-learn support both L1 and L2 regularization, so you will need to select an appropriate solver for the regularization technique you want to use.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The mathematical details of why L1 regularization removes features but L2 doesn't are beyond the scope of this book. However, for a more thorough explanation of this topic and further reading in general, we recommend the very readable (and free) resource, <em class="italic">An Introduction to Statistical Learning by Gareth James</em>, et al. In particular, see <em class="italic">page 222</em> of the corrected 7th printing, for a helpful graphic on the difference between L1 and L2 regularization.</p>
			<p><strong class="bold">Intercepts and Regularization</strong></p>
			<p>We have not discussed intercepts very much, other than to note that we have been estimating them with our linear models, along with the coefficients that go with each feature. So, should you use an intercept? The answer is probably yes, until you've developed an advanced understanding of linear models and are certain that in a specific case you should not. However, such cases do exist, for example, in a linear regression where the features and the response variable have all been normalized to have a mean of zero.</p>
			<p>Intercepts don't go with any particular feature. Therefore, it doesn't make much sense to regularize them, as they shouldn't contribute to overfitting. Notice that in the regularization penalty term for L1, the summation starts with <em class="italic">j = 1</em>, and similarly for L2, we have skipped <em class="italic">σ</em><span class="subscript">0</span>, which is the intercept term.</p>
			<p>This is the ideal situation: not regularizing the intercept. However, some of the solvers in scikit-learn, such as <strong class="source-inline">liblinear</strong>, actually do regularize the intercept. There is an <strong class="source-inline">intercept_scaling</strong> option that you can supply to the model class to counteract this effect. We have not illustrated this here as, although it is theoretically incorrect, regularizing the intercept often does not have much effect on the model's predictive quality in practice.</p>
			<p><strong class="bold">Scaling and Regularization</strong></p>
			<p>As noted in the previous exercise, it is best practice to <strong class="bold">scale</strong> the data so that all the features have roughly the same range of values before using regularization. This is because the coefficients are all going to be subject to the same penalty in the cost function. If the range of values for a particular feature, such as <strong class="source-inline">LIMIT_BAL</strong> in our dataset, is much larger than other features, such as <strong class="source-inline">PAY_1</strong>, it may, in fact, be desirable to have a larger value for the coefficient of <strong class="source-inline">PAY_1</strong> and a smaller value for that of <strong class="source-inline">LIMIT_BAL</strong> in order to put their effects on the same scale in the linear combination of features and coefficients that are used for model prediction. Normalizing all the features before using regularization avoids complications such as this that arise simply from differences in scale.</p>
			<p>In fact, scaling your data may also be necessary, depending on which solver you are using. The different variations on the gradient descent process available in scikit-learn may or may not be able to work effectively with unscaled data.</p>
			<p><strong class="bold">The Importance of Selecting the Right Solver</strong></p>
			<p>As we've come to learn, the different solvers available for logistic regression in scikit-learn have different behaviors regarding the following:</p>
			<ul>
				<li>Whether they support both L1 and L2 regularization</li>
				<li>How they treat the intercept during regularization</li>
				<li>How they deal with unscaled data<p class="callout-heading">Note</p><p class="callout">There are other differences as well. A helpful table comparing these and other traits is available at <a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</a>. You can use this table to decide which solver is appropriate for your problem.</p></li>
			</ul>
			<p>To summarize this section, we have learned the mathematical foundations of lasso and ridge regularization. <em class="italic">These methods work by shrinking the coefficient values toward 0, and in the case of the lasso, setting some coefficients to exactly 0 and thus performing feature selection</em>. You can imagine that in our example of overfitting in <em class="italic">Figure 4.12</em>, if the complex, overfitted model had some coefficients shrunk toward 0, it would look more like the ideal model, which has fewer coefficients.</p>
			<p>Here is a plot of a regularized regression model, using the same high-degree polynomial features as the overfitted model, but with a ridge penalty:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer147">
					<img alt="Figure 4.16: An overfit model and regularized model using the same features&#13;&#10;" src="image/B16925_4_16.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.16: An overfit model and regularized model using the same features</p>
			<p>The regularized model looks similar to the ideal model, demonstrating the ability of regularization to correct overfitting. Note, however, that the regularized model should not be recommended for extrapolation. Here, we can see that the regularized model starts to increase toward the right side of <em class="italic">Figure 4.16</em>. This increase should be viewed with suspicion, as there is nothing in the training data that makes it clear that this would be expected. This is an example of the general view that the <em class="italic">extrapolation of model predictions outside the range of training data is not recommended</em>. However, it is clear from <em class="italic">Figure 4.16</em> that even if we didn't have knowledge of the model that was used to generate this synthetic data (as we typically don't have knowledge of the data-generating process in real-world predictive modeling work), we can still use regularization to reduce the effect of overfitting when a large number of candidate features are available.</p>
			<p><strong class="bold">Model and Feature Selection</strong></p>
			<p>L1 regularization is one way to use a model, such as logistic regression, to perform feature selection. Other methods include forward or backward <strong class="bold">stepwise selection</strong> from the pool of candidate features. Here is the high-level idea behind these methods: in the case of <strong class="bold">forward selection</strong>, features are added to the model one at a time, and the out-of-sample performance is observed along the way. At each iteration, the addition of all possible features from the candidate pool is considered, and the one resulting in the greatest increase in the out-of-sample performance is chosen. When adding additional features ceases to improve the model's performance, no more features need to be added from the candidates. In the case of <strong class="bold">backward selection</strong>, you first start with all the features in the model and determine which one you should remove: the one resulting in the smallest decrease in the out-of-sample performance. You can continue removing features in this way until the performance begins to decrease appreciably.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code for generating the plots presented in this section can be found here: <a href="https://packt.link/aUBMb">https://packt.link/aUBMb</a>.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/>Cross-Validation: Choosing the Regularization Parameter </h1>
			<p>By now, you may suspect that we could use regularization in order to decrease the overfitting we observed when we tried to model the synthetic data in <em class="italic">Exercise 4.02</em>, <em class="italic">Generating and Modeling Synthetic Classification Data</em>. The question is, how do we choose the regularization parameter, <em class="italic">C</em>? <em class="italic">C</em> is an example of a model <strong class="bold">hyperparameter</strong>. Hyperparameters are different from the parameters that are estimated when a model is trained, such as the coefficients and the intercept of a logistic regression. Rather than being estimated by an automated procedure like the parameters are, hyperparameters are input directly by the user as keyword arguments, typically when instantiating the model class. So, how do we know what values to choose?</p>
			<p>Hyperparameters are more difficult to estimate than parameters. This is because it is up to the data scientist to determine what the best value is, as opposed to letting an optimization algorithm find it. However, it is possible to programmatically choose hyperparameter values, which could be viewed as an optimization procedure in its own right. Practically speaking, in the case of the regularization parameter <em class="italic">C</em>, this is most commonly done by fitting the model on one set of data with a particular value of <em class="italic">C</em>, determining model training performance, and then assessing the out-of-sample performance on another set of data.</p>
			<p>We are already familiar with the concept of using model training and test sets. However, there is a key difference here; for instance, what would happen if we were to use the test set multiple times in order to see the effect of different values of <em class="italic">C</em>?</p>
			<p>It may occur to you that after the first time you use the unseen test set to assess the out-of-sample performance for a particular value of <em class="italic">C</em>, it is no longer an "unseen" test set. While only the training data was used for estimating the model parameters (that is, the coefficients and the intercept), now the test data is being used to estimate the hyperparameter <em class="italic">C</em>. Effectively, the test data has now become additional training data in the sense that it is being used to find a good value for the hyperparameter.</p>
			<p>For this reason, it is common to divide the data into three parts: a training set, a test set, and a <strong class="bold">validation set</strong>. The validation set serves multiple purposes:</p>
			<p><strong class="bold">Estimating Hyperparameters</strong></p>
			<p>The validation set can be repeatedly used to assess the out-of-sample performance with different hyperparameter values to select hyperparameters.</p>
			<p><strong class="bold">A Comparison of Different Models</strong></p>
			<p>In addition to finding hyperparameter values for a model, the validation set can be used to estimate the out-of-sample performance of different models; for example, if we wanted to compare logistic regression to random forest.</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="bold">Data Management Best Practices</strong></p>
			<p class="callout">As a data scientist, it's up to you to figure out how to divide up your data for different predictive modeling tasks. In the ideal case, you should reserve a portion of your data for the very end of the process, after you've already selected model hyperparameters and also selected the best model. This <strong class="bold">unseen test set</strong> is reserved for the last step, when it can be used to assess the endpoint of your model-building efforts, to see how the final model generalizes to new unseen data. When reserving the test set, it is good practice to make sure that the features and responses have similar characteristics to the rest of the data. In other words, the class fraction should be the same, and the distribution of features should be similar. This way, the test data should be representative of the data you built the model with.</p>
			<p>While model validation is a good practice, it raises the question of whether the particular split we choose for the training, validation, and test data has any effect on the outcomes that we are tracking. For example, perhaps the relationship between the features and the response variable is slightly different in the unseen test set that we have reserved, or in the validation set, versus the training set. It is likely impossible to eliminate all such variability, but we can use the method of <strong class="bold">cross-validation</strong> to avoid placing too much faith in one particular split of the data.</p>
			<p>Scikit-learn provides convenient functions to facilitate cross-validation analyses. These functions play a similar role to <strong class="source-inline">train_test_split</strong>, which we have already been using, although the default behavior is somewhat different. Let's get familiar with them now. First, import these two classes:</p>
			<p class="source-code">from sklearn.model_selection import StratifiedKFold</p>
			<p class="source-code">from sklearn.model_selection import KFold</p>
			<p>Similar to <strong class="source-inline">train_test_split</strong>, we need to specify what proportion of the dataset we would like to use for training versus testing. However, with cross-validation (specifically the <strong class="bold">k-fold cross-validation</strong> that was implemented in the classes we just imported), rather than specifying a proportion directly, we simply indicate how many folds we would like – that is, the "<strong class="bold">k folds</strong>." The idea here is that the data will be divided into <strong class="bold">k</strong> equal proportions. For example, if we specify 4 folds, then each fold will have 25% of the data. These folds will be the test data in four separate instances of model training, while the remaining 75% from each fold will be used to train the model. In this procedure, each data point gets used as training data a total of <em class="italic">k - 1</em> times, and as test data only once.</p>
			<p>When instantiating the class, we indicate the number of folds, whether or not to shuffle the data before splitting, and a random seed if we want repeatable results across different runs:</p>
			<p class="source-code">n_folds = 4</p>
			<p class="source-code">k_folds = KFold(n_splits=n_folds, shuffle=False)</p>
			<p>Here, we've instantiated an object with four folds and no shuffling. The way in which we use the object that is returned, which we've called <strong class="source-inline">k_folds</strong>, is by passing the features and response data that we wish to use for cross-validation, to the <strong class="source-inline">.split</strong> method of this object. This outputs an <strong class="bold">iterator</strong>, which means that we can loop through the output to get the different splits of training and test data. If we took the training data from our synthetic classification problem, <strong class="source-inline">X_syn_train</strong> and <strong class="source-inline">y_syn_train</strong>, we could loop through the splits like this:</p>
			<p class="source-code">for train_index, test_index in k_folds_iterator.split(X_syn_train,</p>
			<p class="source-code">                                                      y_syn_train):</p>
			<p>The iterator will return the row indices of <strong class="source-inline">X_syn_train</strong> and <strong class="source-inline">y_syn_train</strong>, which we can use to index the data. Inside this <strong class="source-inline">for</strong> loop, we can write code to use these indices to select data for repeatedly training and testing a model object with different subsets of the data. In this way, we can get a robust indication of the out-of-sample performance when using one particular hyperparameter value, and then repeat the whole process using another hyperparameter value. Consequently, the cross-validation loop may sit <strong class="bold">nested</strong> inside an outer loop over different hyperparameter values. We'll illustrate this in the following exercise.</p>
			<p>First though, what do these splits look like? If we were to simply plot the indices from <strong class="source-inline">train_index</strong> and <strong class="source-inline">test_index</strong> as different colors, we would get something that looks like this:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer148">
					<img alt="Figure 4.17: Training/test splits for k-folds with four folds and no shuffling&#13;&#10;" src="image/B16925_4_17.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.17: Training/test splits for k-folds with four folds and no shuffling</p>
			<p>Here, we see that with the options we've indicated for the <strong class="source-inline">KFold</strong> class, the procedure has simply taken the first 25% of the data, according to the order of rows, as the first test fold, then the next 25% of data for the second fold, and so on. But what if we wanted stratified folds? In other words, what if we wanted to ensure that the class fractions of the response variable were equal in every fold? While <strong class="source-inline">train_test_split</strong> allows this option as a keyword argument, there is a separate <strong class="source-inline">StratifiedKFold</strong> class that implements this for cross-validation. We can illustrate how the stratified splits will appear as follows:</p>
			<p class="source-code">k_folds = StratifiedKFold(n_splits=n_folds, shuffle=False)</p>
			<div>
				<div class="IMG---Figure" id="_idContainer149">
					<img alt="Figure 4.18: Training/test splits for stratified k-folds&#13;&#10;" src="image/B16925_4_18.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.18: Training/test splits for stratified k-folds</p>
			<p>In <em class="italic">Figure 4.18</em>, we can see that there has been some amount of "shuffling" between the different folds. The procedure has moved samples between folds as necessary to ensure that the class fractions in each fold are equal.</p>
			<p>Now, what if we want to shuffle the data to choose samples from throughout the range of indices for each test fold? First, why might we want to do this? Well, with the synthetic data that we've created for our problem, we can be certain that the data is in no particular order. However, in many real-world situations, the data we receive may be sorted in some way.</p>
			<p>For instance, perhaps the rows of the data have been ordered by the date an account was created, or by some other logic. Therefore, it can be a good idea to shuffle the data before splitting. This way, any traits that might have been used for sorting can be expected to be consistent throughout the folds. Otherwise, the data in different folds may have different characteristics, possibly leading to different relationships between features and response. </p>
			<p>This can lead to a situation where model performance is uneven between the folds. In order to "mix up" the folds throughout all the row indices of a dataset, all we need to do is set the <strong class="source-inline">shuffle</strong> parameter to <strong class="source-inline">True</strong>:</p>
			<p class="source-code">k_folds = StratifiedKFold(n_splits=n_folds, shuffle=True,</p>
			<p class="source-code">                          random_state=1)</p>
			<div>
				<div class="IMG---Figure" id="_idContainer150">
					<img alt="Figure 4.19: Training/test splits for stratified k-folds with shuffling&#13;&#10;" src="image/B16925_4_19.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.19: Training/test splits for stratified k-folds with shuffling</p>
			<p>With shuffling, the test folds are spread out randomly, and fairly evenly, across the indices of the input data.</p>
			<p>K-fold cross-validation is a widely used method in data science. However, the choice of how many folds to use depends on the particular dataset at hand. Using a smaller number of folds means that the amount of training data in each fold will be relatively small. Therefore, this increases the chances that the model will underfit, as models generally work better when trained on more data. It's a good idea to try a few different numbers of folds and see how the mean and the variability of the k-fold test score changes. Common numbers of folds can range anywhere from 4 or 5 to 10.</p>
			<p>In the event of a very small dataset, it may be necessary to use as much data as possible for training in the cross-validation folds. In this scenario, you can use a method called <strong class="bold">leave-one-out cross-validation</strong> (<strong class="bold">LOOCV</strong>). In LOOCV, the test set for each fold consists of a single sample. In other words, there will be as many folds as there are samples in the training data. For each iteration, the model is trained on all but one sample, and a prediction is made for that sample. The accuracy, or other performance metric, can then be constructed using these predictions.</p>
			<p>Other concerns that relate to the creation of a test set, such as choosing an out-of-time test set for problems where observations from the past must be used to predict future events, also apply to cross-validation.</p>
			<p>In <em class="italic">Exercise 4.02</em>, <em class="italic">Generating and Modeling Synthetic Classification Data</em>, we saw that fitting a logistic regression on our training data led to overfitting. Indeed, the test score (<em class="italic">ROC AUC = 0.81</em>) was substantially lower than the training score (<em class="italic">ROC AUC = 0.94</em>). We had essentially used very little or no regularization by setting the regularization parameter <em class="italic">C</em> to a relatively large value (1,000). Now we will see what happens when we vary <em class="italic">C</em> through a wide range of values.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code for generating the plots presented in this section can be found here: <a href="https://packt.link/37Zks">https://packt.link/37Zks</a>.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>Exercise 4.03: Reducing Overfitting on the Synthetic Data Classification Problem</h2>
			<p>This exercise is a continuation of <em class="italic">Exercise 4.02</em>, <em class="italic">Generating and Modeling Synthetic Classification Data</em>. Here, we will use a cross-validation procedure in order to find a good value for the hyperparameter <em class="italic">C</em>. We will do this by using only the training data, reserving the test data for after model building is complete. Be prepared – this is a long exercise – but it will illustrate a general procedure that you will be able to use with many different kinds of machine learning models, so it is worth the time spent here. Perform the following steps to complete the exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before you begin this exercise, you need to execute some prerequisite steps that can be found in the following notebook along with the code for this exercise:  <a href="https://packt.link/JqbsW">https://packt.link/JqbsW</a>.</p>
			<ol>
				<li value="1">Vary the value of the regularization parameter, <em class="italic">C</em>, to range from <em class="italic">C = 1000</em> to <em class="italic">C = 0.001</em>. You can use the following snippets to do this.<p>First, define exponents, which will be powers of 10, as follows: </p><p class="source-code">C_val_exponents = np.linspace(3,-3,13)</p><p class="source-code">C_val_exponents</p><p>Here is the output of the preceding code:</p><p class="source-code">array([ 3. ,  2.5,  2. ,  1.5,  1. ,  0.5,  0. , -0.5, -1. , -1.5, -2. , -2.5, -3. ])</p><p>Now, vary <em class="italic">C</em> by the powers of 10, as follows:</p><p class="source-code">C_vals = np.float(10)**C_val_exponents</p><p class="source-code">C_vals</p><p>Here is the output of the preceding code:</p><p class="source-code">array([1.00000000e+03, 3.16227766e+02, 1.00000000e+02, 3.16227766e+01,</p><p class="source-code">       1.00000000e+01, 3.16227766e+00, 1.00000000e+00, 3.16227766e-01,</p><p class="source-code">       1.00000000e-01, 3.16227766e-02, 1.00000000e-02, 3.16227766e-03,</p><p class="source-code">       1.00000000e-03])</p><p>It's generally a good idea to vary the regularization parameter by powers of 10, or by using a similar strategy, as training models can take a substantial amount of time, especially when using k-fold cross-validation. This gives you a good idea of how a wide range of <em class="italic">C</em> values impacts the bias-variance trade-off, without needing to train a very large number of models. In addition to the integer powers of 10, we also include points on the log<span class="subscript">10</span> scale that are about halfway between. If it seems like there is some interesting behavior in between these relatively widely spaced values, you can add more granular values for <em class="italic">C</em> in a smaller part of the range of possible values.</p></li>
				<li>Import the <strong class="source-inline">roc_curve</strong> class:<p class="source-code">from sklearn.metrics import roc_curve</p><p>We'll continue to use the ROC AUC score for assessing, training, and testing performance. Now that we have several values of <em class="italic">C</em> to try and several folds (in this case four) for the cross-validation, we will want to store the training and test scores for each fold and for each value of <em class="italic">C</em>.</p></li>
				<li>Define a function that takes the <strong class="source-inline">k_folds</strong> cross-validation splitter, the array of <em class="italic">C</em> values (<strong class="source-inline">C_vals</strong>), the model object (<strong class="source-inline">model</strong>), and the features and response variable (<strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>, respectively) as inputs, to explore different amounts of regularization with k-fold cross-validation. Use the following code:<p class="source-code">def cross_val_C_search(k_folds, C_vals, model, X, Y):</p><p class="callout-heading">Note</p><p class="callout">The function we started in this step will return the ROC AUCs and ROC curve data. The return block will be written during a later step in the exercise. For now, you can simply write the preceding code as is, because we will be defining <strong class="source-inline">k_folds</strong>, <strong class="source-inline">C_vals</strong>, <strong class="source-inline">model</strong>, <strong class="source-inline">X</strong>, and <strong class="source-inline">Y</strong> as we progress in the exercise.</p></li>
				<li>Within this function block, create a NumPy array to hold model performance data, with dimensions <strong class="source-inline">n_folds</strong> by <strong class="source-inline">len(C_vals)</strong>:<p class="source-code">n_folds = k_folds.n_splits</p><p class="source-code">cv_train_roc_auc = np.empty((n_folds, len(C_vals)))</p><p class="source-code">cv_test_roc_auc = np.empty((n_folds, len(C_vals)))</p><p>Next, we'll store the arrays of true and false positive rates and thresholds that go along with each of the test ROC AUC scores in a <strong class="bold">list of lists</strong>.</p><p class="callout-heading">Note</p><p class="callout">This is a convenient way to store all this model performance information, as a list in Python can contain any kind of data, including another list. Here, each item of the inner lists in the <strong class="bold">list of lists</strong> will be a tuple holding the arrays of TPR, FPR, and the thresholds for each of the folds, for each of the C values. Tuples are an ordered collection data type in Python, similar to lists, but unlike lists they are immutable: the items in a tuple can't be changed after the tuple is created. When a function returns multiple values, like the roc_curve function of scikit-learn, these values can be output to a single variable, which will be a tuple of those values. This way of storing results should be more obvious when we access these arrays later in order to examine them.</p></li>
				<li>Create a list of empty lists using <strong class="source-inline">[[]]</strong> and <strong class="source-inline">*len(C_vals)</strong> as follows:<p class="source-code">cv_test_roc = [[]]*len(C_vals)</p><p>Using <strong class="source-inline">*len(C_vals)</strong> indicates that there should be a list of tuples of metrics (TPR, FPR, thresholds) for each value of <em class="italic">C</em>.</p><p>We have learned how to loop through the different folds for cross-validation in the preceding section. What we need to do now is write an outer loop in which we will nest the cross-validation loop.</p></li>
				<li>Create an outer loop for training and testing each of the k-folds for each value of <em class="italic">C</em>:<p class="source-code">for c_val_counter in range(len(C_vals)):</p><p class="source-code">    #Set the C value for the model object</p><p class="source-code">    model.C = C_vals[c_val_counter]</p><p class="source-code">    #Count folds for each value of C</p><p class="source-code">    fold_counter = 0</p><p>We can reuse the same model object that we have already, and simply set a new value of <em class="italic">C</em> within each run of the loop. Inside the loop of <em class="italic">C</em> values, we run the cross-validation loop. We begin by yielding the training and test data row indices for each split.</p></li>
				<li>Obtain the training and test indices for each fold:<p class="source-code">for train_index, test_index in k_folds.split(X, Y):</p></li>
				<li>Index the features and response variable to obtain the training and test data for this fold using the following code:<p class="source-code">X_cv_train, X_cv_test = X[train_index], X[test_index]</p><p class="source-code">y_cv_train, y_cv_test = Y[train_index], Y[test_index]</p><p>The training data for the current fold is then used to train the model.</p></li>
				<li>Fit the model on the training data, as follows:<p class="source-code">model.fit(X_cv_train, y_cv_train)</p><p>This will effectively "reset" the model from whatever the previous coefficients and intercept were to reflect the training on this new data.</p><p>The training and test ROC AUC scores are then obtained, as well as the arrays of TPRs, FPRs, and thresholds that go along with the test data.</p></li>
				<li>Obtain the training ROC AUC score:<p class="source-code">y_cv_train_predict_proba = model.predict_proba(X_cv_train)</p><p class="source-code">cv_train_roc_auc[fold_counter, c_val_counter] = \</p><p class="source-code">roc_auc_score(y_cv_train, y_cv_train_predict_proba[:,1])</p></li>
				<li>Obtain the test ROC AUC score:<p class="source-code">y_cv_test_predict_proba = model.predict_proba(X_cv_test)</p><p class="source-code">cv_test_roc_auc[fold_counter, c_val_counter] = \</p><p class="source-code">roc_auc_score(y_cv_test, y_cv_test_predict_proba[:,1])</p></li>
				<li>Obtain the test ROC curves for each fold using the following code:<p class="source-code">this_fold_roc = roc_curve(y_cv_test, y_cv_test_predict_proba[:,1])</p><p class="source-code">cv_test_roc[c_val_counter].append(this_fold_roc)</p><p>We will use a fold counter to keep track of the folds that are incremented, and once outside the cross-validation loop, we print a status update to standard output. Whenever performing long computational procedures, it's a good idea to periodically print the status of the job so that you can monitor its progress and confirm that things are still working correctly. This cross-validation procedure will likely take only a few seconds on your laptop, but for longer jobs this can be especially reassuring.</p></li>
				<li>Increment the fold counter using the following code:<p class="source-code">fold_counter += 1</p></li>
				<li>Write the following code to indicate the progress of execution for each value of <em class="italic">C</em>:<p class="source-code">print('Done with C = {}'.format(lr_syn.C))</p></li>
				<li>Write the code to return the ROC AUCs and ROC curve data and finish the function:<p class="source-code">return cv_train_roc_auc, cv_test_roc_auc, cv_test_roc</p><p>Note that we will continue to use the split into four folds that we illustrated previously, but you are encouraged to try this procedure with different numbers of folds to compare the effect.</p><p>We have covered a lot of material in the preceding steps. You may want to take a few moments to review this with your classmates in order to make sure that you understand each part. Running the function is comparatively simple. That is the beauty of a well-designed function – all the complicated parts get abstracted away, allowing you to concentrate on usage. </p></li>
				<li>Run the function we've designed to examine cross-validation performance, with the <em class="italic">C</em> values that we previously defined, and by using the model and data we were working with in the previous exercise. Use the following code:<p class="source-code">cv_train_roc_auc, cv_test_roc_auc, cv_test_roc = \</p><p class="source-code">cross_val_C_search(k_folds, C_vals, lr_syn, X_syn_train, y_syn_train)</p><p>When you run this code, you should see the following output populate below the code cell as the cross-validation is completed for each value of <em class="italic">C</em>:</p><p class="source-code">Done with C = 1000.0</p><p class="source-code">Done with C = 316.22776601683796</p><p class="source-code">Done with C = 100.0</p><p class="source-code">Done with C = 31.622776601683793</p><p class="source-code">Done with C = 10.0</p><p class="source-code">Done with C = 3.1622776601683795</p><p class="source-code">Done with C = 1.0</p><p class="source-code">Done with C = 0.31622776601683794</p><p class="source-code">Done with C = 0.1</p><p class="source-code">Done with C = 0.03162277660168379</p><p class="source-code">Done with C = 0.01</p><p class="source-code">Done with C = 0.0031622776601683794</p><p class="source-code">Done with C = 0.001</p><p>So, what do the results of the cross-validation look like? There are a few ways to examine this. It is useful to look at the performance of each fold individually, so that you can see how variable the results are. </p><p>This tells you how different subsets of your data perform as test sets, leading to a general idea of the range of performance you can expect from the unseen test set. What we're interested in here is whether or not we are able to use regularization to alleviate the overfitting that we saw. We know that using <em class="italic">C = 1,000</em> led to overfitting – we know this from comparing the training and test scores. But what about the other <em class="italic">C</em> values that we've tried? A good way to visualize this will be to plot the training and test scores on the <em class="italic">y-axis</em> and the values of <em class="italic">C</em> on the <em class="italic">x-axis</em>. </p></li>
				<li>Loop over each of the folds to view their results individually by using the following code:<p class="source-code">for this_fold in range(k_folds.n_splits):</p><p class="source-code">    plt.plot(C_val_exponents, cv_train_roc_auc[this_fold], '-o',\</p><p class="source-code">             color=cmap(this_fold),\</p><p class="source-code">             label='Training fold {}'.format(this_fold+1))</p><p class="source-code">    plt.plot(C_val_exponents, cv_test_roc_auc[this_fold], '-x',\</p><p class="source-code">             color=cmap(this_fold),\</p><p class="source-code">             label='Testing fold {}'.format(this_fold+1))</p><p class="source-code">plt.ylabel('ROC AUC')</p><p class="source-code">plt.xlabel('log$_{10}$(C)')</p><p class="source-code">plt.legend(loc = [1.1, 0.2])</p><p class="source-code">plt.title('Cross validation scores for each fold')</p><p>You will obtain the following output:</p><div class="IMG---Figure" id="_idContainer151"><img alt="Figure 4.20: The training and test scores for each fold and C-value&#13;&#10;" src="image/B16925_4_20.jpg"/></div><p class="figure-caption">Figure 4.20: The training and test scores for each fold and C-value</p><p>We can see that for each fold of the cross-validation, as <em class="italic">C</em> decreases, the training performance also decreases. However, at the same time, the test performance increases. For some folds and values of <em class="italic">C</em>, the test ROC AUC score actually exceeds that of the training data, while for others, these two metrics simply come closer together. In all cases, we can say that the <em class="italic">C</em> values of 10<span class="superscript">-1.5</span> and 10<span class="superscript">-2</span> appear to have a similar test performance, which is substantially higher than the test performance of <em class="italic">C = 10</em><span class="superscript">3</span>. So, it appears that regularization has successfully addressed our overfitting problem.</p><p>But what about the lower values of <em class="italic">C</em>? For values that are lower than 10<span class="superscript">-2</span>, the ROC AUC metric suddenly drops to 0.5. As you know, this value means that the classification model is essentially useless, performing no better than a coin flip. You are encouraged to check on this later when exploring how regularization affects the coefficient values; however, this is what happens when so much L1 regularization is applied that all model coefficients shrink to 0. Obviously, such models are not useful to us, as they encode no information about the relationship between the features and response variable.</p><p>Looking at the training and test performance of each k-fold split is helpful for gaining insights into the variability of model performance that may be expected when the model is scored on new, unseen data. But in order to summarize the results of the k-folds procedure, a common approach is to average the performance metric over the folds, for each value of the hyperparameter being considered. We'll perform this in the next step. </p></li>
				<li>Plot the mean of training and test ROC AUC scores for each <em class="italic">C</em> value using the following code:<p class="source-code">plt.plot(C_val_exponents, np.mean(cv_train_roc_auc, axis=0), \</p><p class="source-code">         '-o', label='Average training score')</p><p class="source-code">plt.plot(C_val_exponents, np.mean(cv_test_roc_auc, axis=0), \</p><p class="source-code">         '-x', label='Average testing score')</p><p class="source-code">plt.ylabel('ROC AUC')</p><p class="source-code">plt.xlabel('log$_{10}$(C)')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.title('Cross validation scores averaged over all folds')</p><div class="IMG---Figure" id="_idContainer152"><img alt="Figure 4.21: The average training and test scores across cross-validation folds&#13;&#10;" src="image/B16925_4_21.jpg"/></div><p class="figure-caption">Figure 4.21: The average training and test scores across cross-validation folds</p><p>From this plot, it's clear that <em class="italic">C = 10</em><span class="superscript">-1.5</span> and <em class="italic">10</em><span class="superscript">-2</span> are the best values of <em class="italic">C</em>. There is little or no overfitting here, as the average training and test scores are nearly the same. You could search a finer grid of <em class="italic">C</em> values (that is <em class="italic">C = 10</em><span class="superscript">-1.1</span><em class="italic">,</em> <em class="italic">10</em><span class="superscript">-1.2</span>, and so on) in order to more precisely locate a <em class="italic">C</em> value. However, from our graph, we can see that either <em class="italic">C = 10</em><span class="superscript">-1.5</span> or <em class="italic">C = 10</em><span class="superscript">-2</span> will likely be good solutions. We will move forward with <em class="italic">C = 10</em><span class="superscript">-1.5</span>.</p><p>Examining the summary metric of ROC AUC is a good way to get a quick idea of how models will perform. However, for any real-world business application, you will often need to choose a specific threshold, which goes along with specific true and false positive rates. These will be needed to use the classifier to make the required "yes" or "no" decision, which, in our case study, is a prediction of whether an account will default. For this reason, it is useful to look at the ROC curves across the different folds of the cross-validation. To facilitate this, the preceding function has been designed to return the true and false positive rates, and thresholds, for each test fold and value of <em class="italic">C</em>, in the <strong class="source-inline">cv_test_roc</strong> list of lists. First, we need to find the index of the outer list that corresponds to the <em class="italic">C</em> value that we've chosen, <em class="italic">10</em><span class="superscript">-1.5</span>.</p><p>To accomplish this, we could simply look at our list of <em class="italic">C</em> values and count by hand, but it's safer to do this programmatically by finding the index of the non-zero element of a Boolean array, as is shown in the next step.</p></li>
				<li>Use a Boolean array to find the index where <em class="italic">C = 10</em><span class="superscript">-1.5</span> and convert it to an integer data type with the following code:<p class="source-code">best_C_val_bool = C_val_exponents == -1.5</p><p class="source-code">best_C_val_bool.astype(int)</p><p>Here is the output of the preceding code:</p><p class="source-code">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])</p></li>
				<li>Convert the integer version of the Boolean array into a single integer index using the <strong class="source-inline">nonzero</strong> function with this code:<p class="source-code">best_C_val_ix = np.nonzero(best_C_val_bool.astype(int)) best_C_val_ix[0][0]</p><p>Here is the output of the preceding code:</p><p class="source-code">9</p><p>We have now successfully located the <em class="italic">C</em> value that we wish to use.</p></li>
				<li>Access the true and false positive rates in order to plot the ROC curves for each fold:<p class="source-code">for this_fold in range(k_folds_n_splits):</p><p class="source-code">    fpr = cv_test_roc[best_C_val_ix[0][0]][this_fold][0]</p><p class="source-code">    tpr = cv_test_roc[best_C_val_ix[0][0]][this_fold][1]</p><p class="source-code">    plt.plot(fpr, tpr, label='Fold {}'.format(this_fold+1))</p><p class="source-code">plt.xlabel('False positive rate')</p><p class="source-code">plt.ylabel('True positive rate')</p><p class="source-code">plt.title('ROC curves for each fold at C = $10^{-1.5}$')</p><p class="source-code">plt.legend()</p><p>You will obtain the following output:</p><div class="IMG---Figure" id="_idContainer153"><img alt="Figure 4.22: ROC curves for each fold&#13;&#10;" src="image/B16925_4_22.jpg"/></div><p class="figure-caption">Figure 4.22: ROC curves for each fold</p><p>It appears that there is a fair amount of variability in the ROC curves. For example, if, for some reason, we want to limit the false positive rate to 40%, then from the plot it appears that we may be able to achieve a true positive rate of anywhere from approximately 60% to 80%. You can find the exact values by examining the arrays that we have plotted. This gives you an idea of how much variability in performance can be expected when deploying the model on new data. Generally, the more training data that is available, then the less variability there will be between the folds of cross-validation, so this could also be a sign that it would be a good idea to collect additional data, especially if the variability between training folds seems unacceptably high. You also may wish to try different numbers of folds with this procedure so as to see the effect on the variability of results between folds.</p><p>While normally we would try other models on our synthetic data problem, such as a random forest or support vector machine, if we imagine that in cross-validation, logistic regression proved to be the best model, we would decide to make this our final choice. When the final model is selected, all the training data can be used to fit the model, using the hyperparameters chosen with cross-validation. It's best to use as much data as possible in model fitting, as models typically work better when trained on more data.</p></li>
				<li>Train the logistic regression on all the training data from our synthetic problem and compare the training and test scores, using the held-out test set as shown in the following steps. <p class="callout-heading">Note</p><p class="callout">This is the final step in the model selection process. You should only use the unseen test set after your choice of model and hyperparameters are considered finished, otherwise it will not be "unseen."</p></li>
				<li>Set the <em class="italic">C</em> value and train the model on all the training data with this code:<p class="source-code">lr_syn.C = 10**(-1.5)</p><p class="source-code">lr_syn.fit(X_syn_train, y_syn_train)</p><p>Here is the output of the preceding code:</p><p class="source-code">LogisticRegression(C=0.03162277660168379, penalty='l1', \</p><p class="source-code">                   random_state=1, solver='liblinear'))</p></li>
				<li>Obtain predicted probabilities and the ROC AUC score for the training data with this code:<p class="source-code">y_syn_train_predict_proba = lr_syn.predict_proba(X_syn_train)</p><p class="source-code">roc_auc_score(y_syn_train, y_syn_train_predict_proba[:,1])</p><p>Here is the output of the preceding code:</p><p class="source-code">0.8802812499999999</p></li>
				<li>Obtain predicted probabilities and the ROC AUC score for the test data with this code:<p class="source-code">y_syn_test_predict_proba = lr_syn.predict_proba(X_syn_test)</p><p class="source-code">roc_auc_score(y_syn_test, y_syn_test_predict_proba[:,1])</p><p>Here is the output of the preceding code:</p><p class="source-code">0.8847884788478848</p><p>Here, we can see that by using regularization, the model training and test scores are similar, indicating that the overfitting problem has been greatly reduced. The training score is lower since we have introduced bias into the model at the expense of variance. However, this is OK, since the test score, which is the most important part, is higher. The out-of-sample test score is what matters for predictive capability. You are encouraged to check that these training and test scores are similar to those from the cross-validation procedure by printing the values from the arrays that we plotted previously; you should find that they are.</p><p class="callout-heading">Note</p><p class="callout">In a real-world project, before delivering this model to your client for production use, you may wish to train the model on all the data that you were given, including the unseen test set. This follows the idea that the more data a model has seen, the better it is likely to perform in practice. However, some practitioners prefer to only use models that have been tested, meaning you would deliver the model trained only on the training data, not including the test set.</p><p>We know that L1 regularization works by decreasing the magnitude (that is, absolute value) of coefficients of the logistic regression. It can also set some coefficients to zero, therefore performing feature selection. In the next step, we will determine how many coefficients were set to zero.</p></li>
				<li>Access the coefficients of the trained model and determine how many do not equal zero (<strong class="source-inline">!= 0</strong>) with this code:<p class="source-code">sum((lr_syn.coef_ != 0)[0])</p><p>The output should be as follows:</p><p class="source-code">2</p><p>This code takes the sum of a Boolean array indicating the locations of non-zero coefficients, so it shows how many coefficients in the model did not get set to zero by L1 regularization. Only 2 of the 200 features were selected!</p></li>
				<li>Examine the value of the intercept using this code:<p class="source-code">lr_syn.intercept_</p><p>The output should be as follows:</p><p class="source-code">array([0.])</p><p>This shows that the intercept was regularized to 0.</p></li>
			</ol>
			<p>In this exercise, we accomplished several goals. We used the k-fold cross-validation procedure to tune the regularization hyperparameter. We saw the power of regularization for reducing overfitting, and in the case of L1 regularization in logistic regression, selecting features.</p>
			<p>Many machine learning algorithms offer some type of feature selection capability. Many also require the tuning of hyperparameters. The function here that loops over hyperparameters, and performs cross-validation, is a powerful concept that generalizes to other models. Scikit-learn offers functionality to make this process easier; in particular, the <strong class="source-inline">sklearn.model_selection.GridSearchCV</strong> procedure, which applies cross-validation to a grid search over hyperparameters. A <strong class="bold">grid search</strong> can be helpful when there are multiple hyperparameters to tune, by looking at all combinations of the ranges of different hyperparameters that you specify. A <strong class="bold">randomized grid search</strong> can speed up this process by randomly choosing a smaller number of combinations when an exhaustive grid search would take too long. Once you are comfortable with the concepts illustrated here, you are encouraged to streamline your workflow with convenient functions like these.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor089"/>Options for Logistic Regression in Scikit-Learn</h2>
			<p>We have used and discussed most of the options that you may supply to scikit-learn when instantiating or tuning the hyperparameters of a <strong class="source-inline">LogisticRegression</strong> model class. Here, we list them all and provide some general advice on their usage:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer154">
					<img alt="Figure 4.23: A complete list of options for the logistic regression model in scikit-learn&#13;&#10;" src="image/B16925_4_23.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.23: A complete list of options for the logistic regression model in scikit-learn</p>
			<p>If you are in doubt regarding which option to use for logistic regression, we recommend you consult the scikit-learn documentation for further guidance (<a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</a>). Some options, such as the regularization parameter <em class="italic">C</em>, or the choice of a penalty for regularization, will need to be explored through the cross-validation process. Here, as with many choices to be made in data science, there is no universal approach that will apply to all datasets. The best way to see which options to use with a given dataset is to try several of them and see which gives the best out-of-sample performance. Cross-validation offers you a robust way to do this.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor090"/>Scaling Data, Pipelines, and Interaction Features in Scikit-Learn</h2>
			<p><strong class="bold">Scaling Data</strong></p>
			<p>Compared to the synthetic data we were just working with, the case study data is relatively large. If we want to use L1 regularization, then according to the official documentation (<a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</a>), we ought to use the <strong class="source-inline">saga</strong> solver. However, this solver is not robust to unscaled datasets. Hence, we need to be sure to scale the data. This is also a good idea whenever doing regularization, so all the features are on the same scale and are equally penalized by the regularization process. A simple way to make sure that all the features have the same scale is to put them all through the transformation of subtracting the minimum and dividing by the range from minimum to maximum. This transforms each feature so that it will have a minimum of 0 and a maximum of 1. To instantiate the <strong class="source-inline">MinMaxScaler</strong> scaler that does this, we can use the following code: </p>
			<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p>
			<p class="source-code">min_max_sc = MinMaxScaler()</p>
			<p><strong class="bold">Pipelines</strong></p>
			<p>Previously, we used a logistic regression model in the cross-validation loop. However, now that we're scaling data, what new considerations are there? The scaling is effectively "learned" from the minimum and maximum values of the training data. After this, a logistic regression model would be trained on data scaled by the extremes of the model training data. However, we won't know the minimum and maximum values of the new, unseen data. So, following the philosophy of making cross-validation an effective indicator of model performance on unseen data, we need to use the minimum and maximum values of the training data in each cross-validation fold in order to scale the test data in that fold, before making predictions on the test data. Scikit-learn has the functionality to facilitate the combination of several training and test steps for situations such as this: the <strong class="source-inline">Pipeline</strong>. Our pipeline will consist of two steps: the scaler and the logistic regression model. These can both be fit on the training data and then used to make predictions on the test data. The process of fitting a pipeline is executed as a single step in the code, so all the parts of the pipeline are fit at once in this sense. Here is how a <strong class="source-inline">Pipeline</strong> is instantiated:</p>
			<p class="source-code">from sklearn.pipeline import Pipeline</p>
			<p class="source-code">scale_lr_pipeline = Pipeline(steps=[('scaler', min_max_sc), \</p>
			<p class="source-code">                                    ('model', lr)])</p>
			<p><strong class="bold">Interaction Features</strong></p>
			<p>Considering the case study data, do you think a logistic regression model with all possible features would be overfit or underfit? You can think about this from the perspective of rules of thumb, such as the "rule of 10," and the number of features (17) versus samples (26,664) that we have. Alternatively, you can consider all the work we've done so far with this data. For instance, we've had a chance to visualize all the features and ensure they make sense. Since there are relatively few features, and we have relatively high confidence that they are high quality because of our data exploration work, we are in a different situation than with the synthetic data exercises in this chapter, where we had a large number of features about which we knew relatively little. So, it may be that overfitting will be less of an issue with our case study at this point, and the benefits of regularization may not be significant.</p>
			<p>In fact, it may be that we will underfit the model using only the 17 features that came with the data. One strategy to deal with this is to engineer new features. Some simple feature engineering techniques we've discussed include interaction and polynomial features. Polynomials may not make sense given the way in which some of the data has been encoded; for example, <em class="italic">-1</em><span class="superscript">2</span><em class="italic"> = 1</em>, which may not be sensible for <strong class="source-inline">PAY_1</strong>. However, we may wish to try creating interaction features to capture the relationships between features. <strong class="source-inline">PolynomialFeatures</strong> can be used to create interaction features only, without polynomial features. The example code is as follows:</p>
			<p class="source-code">make_interactions = PolynomialFeatures(degree=2, \</p>
			<p class="source-code">                                       interaction_only=True, \</p>
			<p class="source-code">                                       include_bias=False)</p>
			<p>Here, <strong class="source-inline">degree</strong> represents the degree of the polynomial features, <strong class="source-inline">interaction_only</strong> takes a Boolean value (setting it to <strong class="source-inline">True</strong> indicates that only interaction features will be created), and so does <strong class="source-inline">include_bias</strong>, which adds an intercept to the model (the default value is <strong class="source-inline">False</strong>, which is correct here as the logistic regression model will add an intercept).</p>
			<h2 id="_idParaDest-92">Activity 4.01: <a id="_idTextAnchor091"/>Cross-Validation and Feature Engineering with the Case Study Data</h2>
			<p>In this activity, we'll apply the knowledge of cross-validation and regularization that we've learned in this chapter to the case study data. We'll perform basic feature engineering. In order to estimate parameters for the regularized logistic regression model for the case study data, which is larger in size than the synthetic data that we've worked with, we'll use the <strong class="source-inline">saga</strong> solver. In order to use this solver, and for the purpose of regularization, we'll need to <strong class="bold">scale</strong> our data as part of the modeling process, leading us to the use of <strong class="source-inline">Pipeline</strong> class in scikit-learn. Once you have completed the activity, you should obtain an improved cross-validation test performance with the use of interaction features, as shown in the following diagram:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer155">
					<img alt="Figure 4.24: Improved model test performance&#13;&#10;" src="image/B16925_4_24.jpg"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.24: Improved model test performance</p>
			<p>Perform the following steps to complete the activity:</p>
			<ol>
				<li value="1">Select the features from the DataFrame of the case study data.<p>You can use the list of feature names that we've already created in this chapter, but be sure not to include the response variable, which would be a very good (but entirely inappropriate) feature!</p></li>
				<li>Make a training/test split using a random seed of 24.<p>We'll use this going forward and reserve this test data as the unseen test set. By specifying the random seed, we can easily create separate notebooks with other modeling approaches using the same training data.</p></li>
				<li>Instantiate <strong class="source-inline">MinMaxScaler</strong> to scale the data.</li>
				<li>Instantiate a logistic regression model with the <strong class="source-inline">saga</strong> solver, L1 penalty, and set <strong class="source-inline">max_iter</strong> to <strong class="source-inline">1000</strong> as we want the solver to have enough iterations to find a good solution.</li>
				<li>Import the <strong class="source-inline">Pipeline</strong> class and create a pipeline with the scaler and the logistic regression model, using the names <strong class="source-inline">'scaler'</strong> and <strong class="source-inline">'model'</strong> for the steps, respectively.</li>
				<li>Use the <strong class="source-inline">get_params</strong> and <strong class="source-inline">set_params</strong> methods to see how to view the parameters from each stage of the pipeline and change them.</li>
				<li>Create a smaller range of <em class="italic">C</em> values to test with cross-validation, as these models will take longer to train and test with more data than our previous exercise; we recommend <em class="italic">C = [10</em><span class="superscript">2</span><em class="italic">, 10, 1, 10</em><span class="superscript">-1</span><em class="italic">, 10</em><span class="superscript">-2</span><em class="italic">, 10</em><span class="superscript">-3</span><em class="italic">]</em>.</li>
				<li>Make a new version of the <strong class="source-inline">cross_val_C_search</strong> function called <strong class="source-inline">cross_val_C_search_pipe</strong>. Instead of the <strong class="source-inline">model</strong> argument, this function will take a <strong class="source-inline">pipeline</strong> argument. The changes inside the function will be to set the <em class="italic">C</em> value using <strong class="source-inline">set_params(model__C = &lt;value you want to test&gt;)</strong> on the pipeline, replacing the model with the pipeline for the <strong class="source-inline">fit</strong> and <strong class="source-inline">predict_proba</strong> methods, and accessing the <em class="italic">C</em> value using <strong class="source-inline">pipeline.get_params()['model__C']</strong> for the printed status update.</li>
				<li>Run this function as in the previous exercise, but using the new range of <em class="italic">C</em> values, the pipeline you created, and the features and response variable from the training split of the case study data.<p>You may see warnings here, or in later steps, regarding the non-convergence of the solver; you could experiment with the <strong class="source-inline">tol</strong> or <strong class="source-inline">max_iter</strong> options to try and achieve convergence, although the results you obtain with <strong class="source-inline">max_iter = 1000</strong> are likely to be sufficient.</p></li>
				<li>Plot the average training and test ROC AUC across folds for each <em class="italic">C</em> value.</li>
				<li>Create interaction features for the case study data and confirm that the number of new features makes sense.</li>
				<li>Repeat the cross-validation procedure and observe the model performance when using interaction features.<p>Note that this will take substantially more time, due to the larger number of features, but it will probably take less than 10 minutes. So, does the average cross-validation test performance improve with the interaction features? Is regularization useful?</p><p class="callout-heading">Note</p><p class="callout">The Jupyter notebook containing the Python code for this activity can be found at <a href="https://packt.link/ohGgX">https://packt.link/ohGgX</a>. Detailed step-wise solution to this activity can be found via <a href="B16925_Solution_ePub.xhtml#_idTextAnchor155">this link</a>.</p></li>
			</ol>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Summary</h1>
			<p>In this chapter, we introduced the final details of logistic regression and continued to understand how to use scikit-learn to fit logistic regression models. We gained more visibility into how the model fitting process works by learning about the concept of a cost function, which is minimized by the gradient descent procedure to estimate parameters during model fitting.</p>
			<p>We also learned of the need for regularization by introducing the concepts of underfitting and overfitting. In order to reduce overfitting, we saw how to adjust the cost function to regularize the coefficients of a logistic regression model using an L1 or L2 penalty. We used cross-validation to select the amount of regularization by tuning the regularization hyperparameter. To reduce underfitting, we saw how to do some simple feature engineering with interaction features for the case study data.</p>
			<p>We are now familiar with some of the most important concepts in machine learning. We have, so far, only used a very basic classification model: logistic regression. However, as you increase your toolbox of models that you know how to use, you will find that the concepts of overfitting and underfitting, the bias-variance trade-off, and hyperparameter tuning will come up again and again. These ideas, as well as convenient scikit-learn implementations of the cross-validation functions that we wrote in this chapter, will help us through our exploration of more advanced prediction methods.</p>
			<p>In the next chapter, we will learn about decision trees, an entirely different type of predictive model than logistic regression, and the random forests that are based on them. However, we will use the same concepts that we learned here, cross-validation and hyperparameter search, to tune these models.</p>
		</div>
		<div>
			<div class="Content" id="_idContainer157">
			</div>
		</div>
	</body></html>