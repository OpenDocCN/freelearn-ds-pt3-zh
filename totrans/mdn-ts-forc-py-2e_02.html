<html><head></head><body>
  <div id="_idContainer028" class="Basic-Text-Frame">
    <h1 class="chapterNumber">1</h1>
    <h1 id="_idParaDest-21" class="chapterTitle">Introducing Time Series</h1>
    <p class="normal">Welcome to <em class="italic">Modern Time Series Forecasting with Python</em>! This book is intended<a id="_idIndexMarker000"/> for data scientists or <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>) engineers who want to level up their time series analysis skills by learning new and advanced techniques from the ML world. <strong class="keyWord">Time series analysis</strong> is something that is commonly overlooked<a id="_idIndexMarker001"/> in regular ML books, courses, and so on. They typically start with classification, touch upon regression, and then move on. But it is also something that is immensely valuable and ubiquitous in business. We look at the world from a three-dimensional perspective. Time is the hidden dimension that we rarely think about, but is all-pervasive. And as long as time is one of the four dimensions in the world we live in, time series data is all-pervasive too.</p>
    <p class="normal">Analyzing time series data unlocks a lot of value for a business. Time series analysis isn’t new—it’s been around since the 1920s. But in the current age of data, the time series that are collected by businesses are growing larger and wider by the minute. Combined with an explosion in the quantum of data collected and the renewed interest in ML, the landscape of time series analysis also changed considerably. This book attempts to take you beyond<a id="_idIndexMarker002"/> classical statistical methods such as <strong class="keyWord">AutoRegressive Integrated Moving Average</strong> (<strong class="keyWord">ARIMA</strong>) and introduce to you the latest techniques from the ML world in time series analysis.</p>
    <p class="normal">We are going to start with some fundamental concepts and quickly scale up to more complex topics. In this chapter, we’re going to cover the following main topics:</p>
    <ul>
      <li class="bulletList">What is a time series?</li>
      <li class="bulletList">Data-generating process (DGP)</li>
      <li class="bulletList">What can we forecast?</li>
      <li class="bulletList">Forecasting terminology and notation</li>
    </ul>
    <h1 id="_idParaDest-22" class="heading-1">Technical requirements</h1>
    <p class="normal">You will need to set up the <strong class="keyWord">Anaconda</strong> environment by following the instructions in the <em class="italic">Preface</em> of the book to get a working environment with all the libraries and datasets required for the code in this book. Any additional library will be installed while running the notebooks.</p>
    <p class="normal">The associated code for the chapter can be found at <a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter01"><span class="url">https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter01</span></a>.</p>
    <h1 id="_idParaDest-23" class="heading-1">What is a time series?</h1>
    <p class="normal">To keep it simple, a <strong class="keyWord">time series</strong> is a set of observations<a id="_idIndexMarker003"/> taken sequentially in time. The focus is on the word <em class="italic">time</em>. If we keep taking the same observation at different points in time, we will get a time series. For example, if you keep recording the number of bars of chocolate you have in a month, you’ll end up with a time series of your chocolate consumption. Suppose you are recording your weight at the beginning of every month. You get another time series of your weight. Is there any relation between the two time series? Most likely, yeah. But we will be able to analyze that scientifically by the end of this book.</p>
    <p class="normal">A few other examples of time series are the weekly closing price of a stock that you follow, daily rainfall or snowfall in your city, and hourly readings of your pulse rate from your smartwatch.</p>
    <h2 id="_idParaDest-24" class="heading-2">Types of time series</h2>
    <p class="normal">There are two types of time series data based on time intervals, as outlined here:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Regular time series</strong>: This is the most common type<a id="_idIndexMarker004"/> of time series, where<a id="_idIndexMarker005"/> we have observations coming in at regular intervals of time, such as every hour or every month. For example, if we take a time series of temperature in a city, we will get the time series in a regular interval (whichever frequency we choose for observation).</li>
      <li class="bulletList"><strong class="keyWord">Irregular time series</strong>: There are a few time series<a id="_idIndexMarker006"/> where we do not have observations<a id="_idIndexMarker007"/> at regular intervals of time. For example, consider we have a sequence of readings from lab tests of a patient. We see an observation in the time series only when the patient heads to the clinic and carries out the lab test, and this may not happen at regular intervals.</li>
    </ul>
    <div class="note">
      <p class="normal">This book only focuses on regular time series, which are evenly spaced in time. Irregular time series are slightly more advanced and require specialized techniques to handle them. A couple of survey papers on the topic is a good way to get started on irregular time series, and you can find them in the <em class="italic">Further reading</em> section of this chapter.</p>
    </div>
    <h2 id="_idParaDest-25" class="heading-2">Main areas of application for time series analysis</h2>
    <p class="normal">There are broadly three important areas of application<a id="_idIndexMarker008"/> for time series analysis, outlined as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Time series forecasting</strong>: Predicting<a id="_idIndexMarker009"/> the future values of a time series, given the past values—for example, predict the next day’s temperature using the last 5 years of temperature data. This use case is one of the most popular and important ones because any kind of planning we need to do needs some visibility into the future. For instance, planning how many chocolates to produce next month needs a forecast of expected demand.</li>
      <li class="bulletList"><strong class="keyWord">Time series classification</strong>: Sometimes, instead of predicting the future value of the time series, we <a id="_idIndexMarker010"/>may also want to predict an action based on past values. For example, given historical<a id="_idIndexMarker011"/> measurements from an <strong class="keyWord">electroencephalogram</strong> (<strong class="keyWord">EEG</strong>; tracking electrical activity in the brain) or an <strong class="keyWord">electrocardiogram</strong> (<strong class="keyWord">EKG</strong>; tracking electrical activity in the heart), we need to predict<a id="_idIndexMarker012"/> whether the result of an EEG or an EKG is normal or abnormal.</li>
      <li class="bulletList"><strong class="keyWord">Outlier detection</strong>: There<a id="_idIndexMarker013"/> are some situations where we only want to detect if something is going wrong or if something is out of the ordinary. In such cases, we need to use classification or forecasting, but instead, we can do outlier detection. For instance, the wearable tech on your body records accelerometer readings across time and can use outlier detection to identify falls or accidents.</li>
      <li class="bulletList"><strong class="keyWord">Interpretation and causality</strong>: You can use time-series analysis to understand the whats and <a id="_idIndexMarker014"/>whys of the time series based on past values, understand the relationships between several related time series, or derive causal inferences based on time series data. For example, we have a time series of market share for a brand and another time series of advertising spend. Using interpretation and causality techniques, we can start to understand how much advertising investment<a id="_idIndexMarker015"/> is affecting the market share and possibly take appropriate action.</li>
    </ul>
    <div class="note">
      <p class="normal">The focus of this book is predominantly on <em class="italic">time series forecasting</em>, but the techniques that you learn will help you approach <em class="italic">time series classification</em> problems also, with minimal changes in the approach. <em class="italic">Interpretation</em> is also addressed, although only briefly, but <em class="italic">causality</em> is an area that this book does not address because it warrants a whole different approach.</p>
    </div>
    <p class="normal">Now that we have an overview of the time series landscape, let’s build a mental model of how time series data is generated.</p>
    <h1 id="_idParaDest-26" class="heading-1">Data-generating process (DGP)</h1>
    <p class="normal">We have seen that time series data<a id="_idIndexMarker016"/> is a collection of observations made sequentially along the time dimension. Any time series is, in turn, generated by some kind of <em class="italic">mechanism</em>. For example, time series data of daily shipments of your favorite chocolate from the manufacturing plant is affected by a lot of factors, such as the time of the year (holiday season, for example), the availability of cocoa, the uptime of the machines working on the plant, and so on. In statistics, this underlying process that generates the time series is referred to as the <strong class="keyWord">DGP</strong>. Time series data is produced by stochastic and deterministic processes. The deterministic processes involve quantities that evolve in a predictable manner over time. An example of this is the radioactive decay of an element, where the remaining quantity diminishes according to a precise mathematical formula, leading to a consistent reduction over time. But most of the interesting time series (from a forecasting perspective) are generated by a stochastic process. A stochastic process is a way to describe how things change over time in a random but somewhat predictable manner, like how the weather changes daily with some patterns and probabilities involved. So, let’s discuss more about time series generated from stochastic processes.</p>
    <p class="normal">If we had complete and perfect knowledge of reality, all we would need to do would be to put this DGP together in a mathematical form and you would get the most accurate forecast possible. But sadly, nobody has complete and perfect knowledge of reality. So, what we try to do is approximate the DGP, mathematically, as much as possible so that our imitation of the DGP gives us the best possible forecast (or any other output we want from the analysis). This imitation is called a <strong class="keyWord">model</strong> that provides a useful approximation of the DGP.</p>
    <p class="normal">But we must remember<a id="_idIndexMarker017"/> that the model is not the DGP, but a representation of some essential aspects of reality. For example, let’s consider an aerial view of Bengaluru and a map of Bengaluru, as represented here:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01_01.png" alt="Figure 1.1 – An aerial view of Bengaluru (left) and a map of Bengaluru (right) "/></figure>
    <p class="packt_figref">Figure 1.1: An aerial view of Bengaluru (left) and a map of Bengaluru (right)</p>
    <p class="normal">The map of Bengaluru is certainly useful—we can use it to go from point A to point B. But a map of Bengaluru is not the same as a photo of Bengaluru. It doesn’t showcase the bustling nightlife or the insufferable traffic. A map is just a model that represents some useful features of a location, such as roads and places. The following diagram might help us internalize the concept and remember it:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01_02.png" alt="Figure 1.2 – DGP, model, and time series "/></figure>
    <p class="packt_figref">Figure 1.2: DGP, model, and time series</p>
    <p class="normal">Naturally, the next question would be this: <em class="italic">Do we have a useful model?</em> Every model has limitations and challenges. As we have seen, a map of Bengaluru does not perfectly represent Bengaluru. But if our purpose is to navigate Bengaluru, then a map is a very useful model. What if we want to understand the culture? A map doesn’t give you a flavor of that. So, now, the same model that was useful is utterly useless in the new context.</p>
    <p class="normal">Different kinds of models are required in different situations and for different objectives. For example, the best model for forecasting may not be the same as the best model for making a causal inference.</p>
    <p class="normal">We can use the concept<a id="_idIndexMarker018"/> of DGPs to generate multiple synthetic time series of varying degrees of complexity.</p>
    <h2 id="_idParaDest-27" class="heading-2">Generating synthetic time series</h2>
    <p class="normal">Synthetic time series, or artificial time<a id="_idIndexMarker019"/> series, are excellent tools with which you can understand the time series space, experiment with different techniques, and even test new models or modeling setups. These time series are designed to be predictable, even though a bit challenging. Let’s take a look at a few practical examples where we can generate a few time series using a set of fundamental building blocks. You can get creative and mix and match any of these components, or even add them together to generate<a id="_idIndexMarker020"/> a time series of arbitrary complexity.</p>
    <h3 id="_idParaDest-28" class="heading-3">White and red noise</h3>
    <p class="normal">An extreme case of a stochastic process<a id="_idIndexMarker021"/> that generates<a id="_idIndexMarker022"/> a time series is a <strong class="keyWord">white noise</strong> process. It has a sequence of random numbers with zero mean and constant variance. This is also one of the most popular assumptions of noise in a time series.</p>
    <p class="normal">Let’s see how we can generate such a time series and plot it:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Generate the time axis with sequential numbers upto 200</span>
time = np.arange(<span class="hljs-number">200</span>)
<span class="hljs-comment"># Sample 200 hundred random values</span>
values = np.random.randn(<span class="hljs-number">200</span>)*<span class="hljs-number">100</span>
plot_time_series(time, values, <span class="hljs-string">"White Noise"</span>)
</code></pre>
    <p class="normal">Here is the output:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01_03.png" alt="Figure 1.3 – White noise process "/></figure>
    <p class="packt_figref">Figure 1.3: White noise process</p>
    <p class="normal"><strong class="keyWord">Red noise</strong>, on the other hand, has zero mean<a id="_idIndexMarker023"/> and constant variance<a id="_idIndexMarker024"/> but is serially correlated in time. This serial correlation or redness is parameterized by a correlation coefficient <em class="italic">r</em>, such that:</p>
    <p class="center"><img src="../Images/B22389_01_001.png" alt=""/></p>
    <p class="normal">where <em class="italic">w</em> is a random sample from a white noise distribution.</p>
    <p class="normal">Let’s see how we can generate that, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Setting the correlation coefficient</span>
r = <span class="hljs-number">0.4</span>
<span class="hljs-comment"># Generate the time axis</span>
time = np.arange(<span class="hljs-number">200</span>)
<span class="hljs-comment"># Generate white noise</span>
white_noise = np.random.randn(<span class="hljs-number">200</span>)*<span class="hljs-number">100</span>
<span class="hljs-comment"># Create Red Noise by introducing correlation between subsequent values in the white noise</span>
values = np.zeros(<span class="hljs-number">200</span>)
<span class="hljs-keyword">for</span> i, v <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(white_noise):
    <span class="hljs-keyword">if</span> i==<span class="hljs-number">0</span>:
        values[i] = v
    <span class="hljs-keyword">else</span>:
        values[i] = r*values[i-<span class="hljs-number">1</span>]+ np.sqrt((<span class="hljs-number">1</span>-np.power(r,<span class="hljs-number">2</span>))) *v
plot_time_series(time, values, <span class="hljs-string">"Red Noise Process"</span>)
</code></pre>
    <p class="normal">Here is the output:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01_04.png" alt="Figure 1.4 – Red noise process "/></figure>
    <p class="packt_figref">Figure 1.4: Red noise process</p>
    <h3 id="_idParaDest-29" class="heading-3">Cyclical or seasonal signals</h3>
    <p class="normal">Among the most common <a id="_idIndexMarker025"/>signals you see in time series<a id="_idIndexMarker026"/> are seasonal or cyclical signals. Therefore, you can introduce seasonality into your generated series in a few ways.</p>
    <p class="normal">Let’s take the help of a very useful library to generate the rest of the time series—<code class="inlineCode">TimeSynth</code>. For more information, refer to <a href="https://github.com/TimeSynth/TimeSynth"><span class="url">https://github.com/TimeSynth/TimeSynth</span></a>.</p>
    <p class="normal">This is a useful library for generating time series. It has all kinds of DGPs that you can mix and match and create an authentic synthetic time series.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">For the exact code and usage, please refer to the associated Jupyter notebooks.</p>
    </div>
    <p class="normal">Let’s see how we can use a sinusoidal function to create cyclicity. There is a helpful function in <code class="inlineCode">TimeSynth</code> called <code class="inlineCode">generate_timeseries</code> that helps us combine signals and generate time series. Have a look at the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Sinusoidal Signal with Amplitude=1.5 &amp; Frequency=0.25</span>
signal_1 =ts.signals.Sinusoidal(amplitude=<span class="hljs-number">1.5</span>, frequency=<span class="hljs-number">0.25</span>)
<span class="hljs-comment">#Sinusoidal Signal with Amplitude=1 &amp; Frequency=0. 5</span>
signal_2 = ts.signals.Sinusoidal(amplitude=<span class="hljs-number">1</span>, frequency=<span class="hljs-number">0.5</span>)
<span class="hljs-comment">#Generating the time series</span>
samples_1, regular_time_samples, signals_1, errors_1 = generate_timeseries(signal=signal_1)
samples_2, regular_time_samples, signals_2, errors_2 = generate_timeseries(signal=signal_2)
plot_time_series(regular_time_samples,
                 [samples_1, samples_2],
                 <span class="hljs-string">"Sinusoidal Waves"</span>,
                 legends=[<span class="hljs-string">"Amplitude = 1.5 | Frequency = 0.25"</span>, <span class="hljs-string">"Amplitude = 1 | Frequency = 0.5"</span>])
</code></pre>
    <p class="normal">Here is the output:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01_05.png" alt="Figure 1.5 – Sinusoidal waves "/></figure>
    <p class="packt_figref">Figure 1.5: Sinusoidal waves</p>
    <p class="normal">Note the two sinusoidal waves<a id="_idIndexMarker027"/> are different with respect<a id="_idIndexMarker028"/> to the frequency (how fast the time series crosses zero) and amplitude (how far away from zero the time series travels).</p>
    <p class="normal"><code class="inlineCode">TimeSynth</code> also has another signal called <code class="inlineCode">PseudoPeriodic</code>. This is like the <code class="inlineCode">Sinusoidal</code> class, but the frequency and amplitude have some stochasticity. We can see in the following code snippet that this is more realistic than the vanilla sine and cosine waves from the <code class="inlineCode">Sinusoidal</code> class:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># PseudoPeriodic signal with Amplitude=1 &amp; Frequency=0.25</span>
signal = ts.signals.PseudoPeriodic(amplitude=<span class="hljs-number">1</span>, frequency=<span class="hljs-number">0.25</span>)
<span class="hljs-comment">#Generating Timeseries</span>
samples, regular_time_samples, signals, errors = generate_timeseries(signal=signal)
plot_time_series(regular_time_samples,
                 samples,
                 <span class="hljs-string">"Pseudo Periodic"</span>)
</code></pre>
    <p class="normal">Here is <a id="_idIndexMarker029"/>the<a id="_idIndexMarker030"/> output:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01_06.png" alt="Figure 1.6 – Pseudo-periodic signal "/></figure>
    <p class="packt_figref">Figure 1.6: Pseudo-periodic signal</p>
    <h3 id="_idParaDest-30" class="heading-3">Autoregressive signals</h3>
    <p class="normal">Another very <a id="_idIndexMarker031"/>popular signal<a id="_idIndexMarker032"/> in the real world is an <strong class="keyWord">autoregressive (AR) signal</strong>. We will go into this in more detail in <em class="chapterRef">Chapter 4</em>, <em class="italic">Setting a Strong Baseline Forecast</em>, but for now, an AR signal refers to when the value of a time series for the current timestep is dependent on the values of the time series in the previous timesteps. This serial correlation is a key property of the AR signal, and it is parametrized by a few parameters, outlined as follows:</p>
    <ul>
      <li class="bulletList">Order of serial correlation—or, in other words, the number of previous timesteps the signal is dependent on</li>
      <li class="bulletList">Coefficients to combine the previous timesteps</li>
    </ul>
    <p class="normal">Let’s see how we can generate<a id="_idIndexMarker033"/> an AR signal and see what<a id="_idIndexMarker034"/> it looks like, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># We have re-implemented the class in src because of a bug in TimeSynth</span>
<span class="hljs-keyword">from</span> src.synthetic_ts.autoregressive <span class="hljs-keyword">import</span> AutoRegressive
<span class="hljs-comment"># Autoregressive signal with parameters 1.5 and -0.75</span>
<span class="hljs-comment"># y(t) = 1.5*y(t-1) - 0.75*y(t-2)</span>
signal= AutoRegressive(ar_param=[<span class="hljs-number">1.5</span>, -<span class="hljs-number">0.75</span>])
<span class="hljs-comment">#Generate Timeseries</span>
samples, regular_time_samples, signals, errors = generate_timeseries(signal=signal)
plot_time_series(regular_time_samples,
                 samples,
                 <span class="hljs-string">"Auto Regressive"</span>)
</code></pre>
    <p class="normal">Here is the output:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01_07.png" alt="Figure 1.7 – AR signal "/></figure>
    <p class="packt_figref">Figure 1.7: AR signal</p>
    <h3 id="_idParaDest-31" class="heading-3">Mix and match</h3>
    <p class="normal">There are many more components<a id="_idIndexMarker035"/> that you can use to create your DGP and thereby generate a time series, but let’s quickly look at how we can combine the components we have already seen to generate a realistic time series.</p>
    <p class="normal">Let’s use a pseudo-periodic signal with white noise and combine it with an AR signal, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Generating Pseudo Periodic Signal</span>
pseudo_samples, regular_time_samples, _, _ = generate_timeseries(signal=ts.signals.PseudoPeriodic(amplitude=<span class="hljs-number">1</span>, frequency=<span class="hljs-number">0.25</span>), noise=ts.noise.GaussianNoise(std=<span class="hljs-number">0.3</span>))
<span class="hljs-comment"># Generating an Autoregressive Signal</span>
ar_samples, regular_time_samples, _, _ = generate_timeseries(signal= AutoRegressive(ar_param=[<span class="hljs-number">1.5</span>, -<span class="hljs-number">0.75</span>]))
<span class="hljs-comment"># Combining the two signals using a mathematical equation</span>
ts = pseudo_samples*<span class="hljs-number">2</span>+ar_samples
plot_time_series(regular_time_samples,
                 ts,
                 <span class="hljs-string">"Pseudo Periodic with AutoRegression and White Noise"</span>)
</code></pre>
    <p class="normal">Here is the output:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01_08.png" alt="Figure 1.8 – Pseudo-periodic signal with AR and white noise "/></figure>
    <p class="packt_figref">Figure 1.8: Pseudo-periodic signal with AR and white noise</p>
    <h2 id="_idParaDest-32" class="heading-2">Stationary and non-stationary time series</h2>
    <p class="normal">In time series, <strong class="keyWord">stationarity</strong> is of great significance and is a key assumption<a id="_idIndexMarker036"/> in many modeling<a id="_idIndexMarker037"/> approaches. Ironically, many (if not most) real-world time series are non-stationary. So, let’s understand what a stationary time series is from a layman’s point of view.</p>
    <p class="normal">There are multiple ways to look at stationarity, but one of the clearest and most intuitive ways is to think of the probability distribution or the data distribution of a time series. We call a time series stationary when the probability distribution remains the same at every point in time. In other words, if you pick different windows in time, the data distribution across all those windows should be the same.</p>
    <p class="normal">A standard Gaussian distribution is defined by two parameters—the mean and the variance. So, there are two ways the stationarity assumption can be broken, as outlined here:</p>
    <ul>
      <li class="bulletList">Change in mean over time</li>
      <li class="bulletList">Change in variance over time</li>
    </ul>
    <p class="normal">Let’s look at these assumptions in detail and understand them better.</p>
    <h3 id="_idParaDest-33" class="heading-3">Change in mean over time</h3>
    <p class="normal">This is the most popular<a id="_idIndexMarker038"/> way a non-stationary time series presents itself. If there is an upward/downward trend in the time series, the mean across two windows of time would not be the same.</p>
    <p class="normal">Another way non-stationarity manifests itself is in the form of seasonality. Suppose we are looking at the time series of average temperature measurements per month for the last 5 years. From our experience, we know that temperature peaks during summer and falls in winter. So, when we take the mean temperature of winter and the mean temperature of summer, they will be different.</p>
    <p class="normal">Let’s generate a time series with trend and seasonality and see how it manifests:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Sinusoidal Signal with Amplitude=1 &amp; Frequency=0.25</span>
signal=ts.signals.Sinusoidal(amplitude=<span class="hljs-number">1</span>, frequency=<span class="hljs-number">0.25</span>)
<span class="hljs-comment"># White Noise with standard deviation = 0.3</span>
noise=ts.noise.GaussianNoise(std=<span class="hljs-number">0.3</span>)
<span class="hljs-comment"># Generate the time series</span>
sinusoidal_samples, regular_time_samples, _, _ = generate_timeseries(signal=signal, noise=noise)
<span class="hljs-comment"># Regular_time_samples is a linear increasing time axis and can be used as a trend</span>
trend = regular_time_samples*<span class="hljs-number">0.4</span>
<span class="hljs-comment"># Combining the signal and trend</span>
ts = sinusoidal_samples+trend
plot_time_series(regular_time_samples,
                 ts,
                 <span class="hljs-string">"Sinusoidal with Trend and White Noise"</span>)
</code></pre>
    <p class="normal">Here is the output:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01_09.png" alt="Figure 1.9 – Sinusoidal signal with trend and white noise "/></figure>
    <p class="packt_figref">Figure 1.9: Sinusoidal signal with trend and white noise</p>
    <p class="normal">If you examine the<a id="_idIndexMarker039"/> time series in <em class="italic">Figure 1.9</em>, you will be able to see a definite trend and the seasonality, which together make the mean of the data distribution change wildly across different windows of time.</p>
    <h3 id="_idParaDest-34" class="heading-3">Change in variance over time</h3>
    <p class="normal">Non-stationarity can also present<a id="_idIndexMarker040"/> itself in the fluctuating variance of a time series. If the time series starts off with low variance and as time progresses, the variance keeps getting bigger and bigger, we have a non-stationary time series. In statistics, there is a scary<a id="_idIndexMarker041"/> name for this phenomenon—<strong class="keyWord">heteroscedasticity</strong>. The Air Passengers dataset, which is the “iris dataset” of time series (the most popular, over-used, and useless) is a classic example of a heteroscedastic time series. Let’s look at the plot:</p>
    <figure class="mediaobject"><img src="../Images/B22389_01_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 1.10: Air Passengers dataset—Example of a heteroscedastic time series</p>
    <p class="normal">In the figure, you can see that the seasonal peaks keep getting wider and wider as we move through time, and this is a classic sign that the time series is heteroscedastic. But not all heteroscedastic time series are easy to spot. We have statistical tests to check for each of the stationarity cases, which we will cover in <em class="chapterRef">Chapter 7</em>, <em class="italic">Target Transformations for Time Series Forecasting</em>.</p>
    <p class="normal">This book just tries to give<a id="_idIndexMarker042"/> you an understanding of stationary and non-stationary time series. There is a lot of statistical theory and depth in this discussion that we are skipping to keep our focus on the practical aspects of time series.</p>
    <p class="normal">Armed with the mental model of the DGP, we are at the right place to think about another important question: <em class="italic">what can we forecast?</em></p>
    <h1 id="_idParaDest-35" class="heading-1">What can we forecast?</h1>
    <p class="normal">Before we move ahead, there<a id="_idIndexMarker043"/> is another aspect of time series forecasting that we have to understand—<em class="italic">the predictability of a time series</em>. The most basic assumption when we forecast a time series is that the future depends on the past. But not all time series are equally predictable.</p>
    <p class="normal">Let’s take a look at a few examples and try to rank these in order of predictability (from easiest to hardest), as follows:</p>
    <ul>
      <li class="bulletList">High tide next Monday</li>
      <li class="bulletList">Lottery numbers next Sunday</li>
      <li class="bulletList">The stock price of Tesla next Friday</li>
    </ul>
    <p class="normal">Intuitively, it is very easy<a id="_idIndexMarker044"/> for us to rank them. High tide next Monday is going to be the easiest to predict because it is so predictable, the stock price of Tesla next Friday is going to be difficult to predict, but not impossible, and the lottery numbers are going to be very hard to predict because they are pretty much random.</p>
    <div class="note">
      <p class="normal">However, for people thinking that they can forecast stock prices with the advanced techniques covered in the book and get rich, that (most likely) won’t happen. Although it is worthy of a lengthy discussion, we can summarize the key points in a short paragraph.</p>
      <p class="normal">Share prices are not a function of their past values but an anticipation of their future values, and this thereby violates our first assumption while forecasting. And if that is not bad enough, financial stock prices typically have a very low signal-to-noise ratio. The final<a id="_idIndexMarker045"/> wrench in the process is the <strong class="keyWord">efficient market hypothesis</strong> (<strong class="keyWord">EMH</strong>). This seemingly innocent hypothesis proclaims that all known information about a stock price is already factored into the price of the stock. The implication of the hypothesis is that if you can forecast accurately, many others will also be able to do that, and thereby the market price of the stock already reflects the change in price that this forecast brought about.</p>
      <p class="normal">The M6 competition chose to tackle this problem head-on to evaluate if the EMH holds true by conducting a year-long forecasting and investment strategy competition. Although not conclusive, the results show that the EMH holds true for the vast majority of the participants, barring a few top teams. And even in that, they found out that in the top teams, there was no significant correlation between forecasting accuracy and the selection of stocks into the portfolio, i.e. the teams weren’t choosing stocks which they were able to forecast better (a link to the full report is provided in the <em class="italic">Further reading</em> section).</p>
    </div>
    <p class="normal">Coming back to the topic at hand—predictability—three main factors<a id="_idIndexMarker046"/> form a mental model for this, as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Understanding the DGP</strong>: The better you understand the DGP, the higher the predictability of a time series.</li>
      <li class="bulletList"><strong class="keyWord">Amount of data</strong>: The more data you have, the better your predictability is.</li>
      <li class="bulletList"><strong class="keyWord">Adequately repeating pattern</strong>: For any mathematical model to work well, there should be an adequately repeating pattern in your time series. The more repeatable the pattern is, the better your predictability is.</li>
    </ul>
    <p class="normal">Even though you have a mental <a id="_idIndexMarker047"/>model of how to think about predictability, we will look at more concrete ways of assessing the predictability of time series in <em class="chapterRef">Chapter 3</em>,<em class="italic"> Analyzing and Visualizing Time Series Data</em>, but the key takeaway is that not all time series are equally predictable.</p>
    <p class="normal">In order to fully follow the discussion in the coming chapters, we need to establish a standard notation and learn the terminology that is specific to time series analysis.</p>
    <h1 id="_idParaDest-36" class="heading-1">Forecasting terminology</h1>
    <p class="normal">There are a few terms that will help you understand this book as well as other literature on time series. These terms are described in more detail here:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Forecasting</strong></li>
    </ul>
    <p class="normal-one">Forecasting is the prediction<a id="_idIndexMarker048"/> of future values of a time series using the known past values of the time series and/or some other related variables. This is very similar to prediction in ML, where we use a model to predict unseen data.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Multivariate forecasting</strong></li>
    </ul>
    <p class="normal-one">Multivariate time series<a id="_idIndexMarker049"/> consist of more than one time series variable that is not only dependent on its past values but also has some dependency on the other variables. For example, a set<a id="_idIndexMarker050"/> of macroeconomic indicators, such as <strong class="keyWord">gross domestic product</strong> (<strong class="keyWord">GDP</strong>) and inflation, of a particular country can be considered a multivariate time series. The aim of multivariate forecasting is to come up with a model that captures the interrelationship between the different variables along with its relationship with its past and forecast all the time series together in the future.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Explanatory forecasting</strong></li>
    </ul>
    <p class="normal-one">In addition to the past values<a id="_idIndexMarker051"/> of a time series, we might use some other information to predict the future values of a time series. For example, when predicting retail store sales, information regarding promotional offers (both historical and future ones) is usually helpful. This type of forecasting, which uses information other than its own history, is called explanatory forecasting.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Backtesting</strong></li>
    </ul>
    <p class="normal-one">Setting aside a validation<a id="_idIndexMarker052"/> set from your training data to evaluate your models is a practice that is common in the ML world. Backtesting is the time series equivalent of validation, whereby you use the history to evaluate a trained model. We will cover the different ways of doing validation and cross-validation for time series data later.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">In-sample and out-sample</strong></li>
    </ul>
    <p class="normal-one">Again drawing parallels with ML, in-sample refers to training data and out-sample refers to unseen or testing data. When you hear in-sample metrics, this refers to metrics<a id="_idIndexMarker053"/> calculated on training data, and out-sample metrics refers to metrics calculated<a id="_idIndexMarker054"/> on testing data.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Exogenous and endogenous variables</strong></li>
    </ul>
    <p class="normal-one">Exogenous variables are parallel time series<a id="_idIndexMarker055"/> variables that are not modeled directly for output but used to help us model the time series that we are interested in. Typically, exogenous variables are not affected by other variables in the system. Endogenous variables<a id="_idIndexMarker056"/> are variables that are affected by other variables in the system. A purely endogenous variable is a variable that is entirely dependent on the other variables in the system. Relaxing the strict assumptions a bit, we can consider the target variable as the endogenous variable and the explanatory regressors we include in the model as exogenous variables.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Forecast combination</strong></li>
    </ul>
    <p class="normal-one">Forecast combinations in the time series<a id="_idIndexMarker057"/> world are similar to ensembles from the ML world. Forecast combination is a process by which we combine multiple forecasts by using a function, either learned or heuristic-based, such as a simple average<a id="_idIndexMarker058"/> of three forecast models.</p>
    <p class="normal">There are a lot more terms that are specific to time series, some of which we will be covering throughout the book. But these terms should be a good starting point to give you basic familiarity in the field.</p>
    <h1 id="_idParaDest-37" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we had our first look at time series as we discussed the different types of time series, looked at how a DGP generates a time series, and saw how we can think about the important question: <em class="italic">how well can we forecast a time series?</em> We also had a quick review of the terminology required to understand the rest of the book. In the next chapter, we will be getting our hands dirty and will learn how to acquire and process time series data. If you have not set up the environment yet, take a break and put some time into doing that.</p>
    <h1 id="_idParaDest-38" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList"><em class="italic">A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series: From Discretization to Attention and Invariance</em> by S.N. Shukla and B.M. Marlin (2020): <a href="https://arxiv.org/abs/2012.00168"><span class="url">https://arxiv.org/abs/2012.00168</span></a></li>
      <li class="bulletList"><em class="italic">Learning from Irregularly-Sampled Time Series: A Missing Data Perspective</em> by S.C. Li and B.M. Marlin (2020), ICML: <a href="https://arxiv.org/abs/2008.07599"><span class="url">https://arxiv.org/abs/2008.07599</span></a></li>
      <li class="bulletList"><em class="italic">The M6 forecasting competition: Bridging the gap between forecasting and investment decisions by Spyros Makridakis et al. (2023)</em>:<em class="italic"> </em><a href="https://arxiv.org/abs/2310.13357"><span class="url">https://arxiv.org/abs/2310.13357</span></a></li>
    </ul>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
  </div>
</body></html>