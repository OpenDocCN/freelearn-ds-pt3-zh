- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going Further with Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we leveraged open source components to bring time series
    analysis to production. This requires significant effort to set up and manage
    the platform. In this chapter, we will answer such challenges by using Databricks
    as a cloud-based managed **platform-as-a-service** (**PaaS**) solution to go further
    with Apache Spark. We will use an end-to-end example of time series analysis built
    on Databricks using advanced features such as Delta Live Tables with a streaming
    pipeline, AutoML, Unity Catalog, and AI/BI dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Databricks components and setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring, security, and governance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use code examples to explore the deployment of a scalable
    end-to-end solution for time series analysis on Databricks, starting with the
    setup of the environment in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter is at this URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch10](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch10)'
  prefs: []
  type: TYPE_NORMAL
- en: Databricks components and setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using a Databricks environment, as in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151),
    for the platform infrastructure. Follow the instructions in the *Technical requirements*
    section of [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151) on setting up the Databricks
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Workspace, folders, and notebooks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the environment is set up, follow the instructions in the links provided
    here to import the notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate the Databricks workspace: [https://docs.databricks.com/en/workspace/index.html](https://docs.databricks.com/en/workspace/index.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a folder named `ts_spark` and a sub-folder named `ch10`: [https://docs.databricks.com/en/workspace/workspace-objects.html#folders](https://docs.databricks.com/en/workspace/workspace-objects.html#folders)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the notebooks for this example into the `ch10` folder: [https://docs.databricks.com/en/notebooks/notebook-export-import.html#import-a-notebook](https://docs.databricks.com/en/notebooks/notebook-export-import.html#import-a-notebook)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are eight notebooks in all, and they can be imported from the following
    URLs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_dlt_features.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_dlt_features.dbc)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_evaluate_forecast.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_evaluate_forecast.dbc)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_generate_forecast.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_generate_forecast.dbc)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training.dbc)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training_automl.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_model_training_automl.dbc)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_reset.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_reset.dbc)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_data.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_data.dbc)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_model.dbc](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch10/ts_spark_ch10_update_model.dbc)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With the notebooks imported, we can next set up the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use the Databricks **Machine Learning Runtime** (**MLR**) or serverless
    compute for the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The MLR cluster comes preloaded with common libraries used for **machine learning**
    (**ML**). It instantiates virtual machines in your cloud provider account. You
    will be charged for the virtual machine by the cloud provider. Choose a small
    instance with minimal CPU and memory to minimize this cost when creating the cluster.
    This will be sufficient for the example in this chapter. Refer to the *Technical
    requirements* section of [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151) on setting
    up the Databricks cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The MLR cluster has libraries required for the AutoML example, which we will
    cover in a later section. You can skip the code execution for this example if
    you do not want to incur the MLR-related cloud provider cost for the associated
    virtual machines. We will provide an alternative workflow that does not use AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the cloud provider cost for these virtual machines is
    above the free allowance that you get with the free cloud provider trial account.
    This means that you will have to upgrade to a paid cloud provider account and
    the cost will be charged to the credit card you specified when creating the account.
    The virtual machines and cloud infrastructure costs are not incurred from the
    free Databricks trial account.
  prefs: []
  type: TYPE_NORMAL
- en: The serverless cluster is included in your Databricks cost as the underlying
    virtual machines are fully managed by Databricks. This means it does not incur
    separate cloud provider costs. Serverless clusters do, however, require the installation
    of ML libraries at the time of writing, as you will see in the code example. Databricks
    may provide serverless clusters with pre-loaded ML libraries in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Databricks has started to include serverless in the free trial account at the
    time of writing. This means that your use of serverless clusters to execute the
    code in this chapter will be free if within the time and cost limit of the Databricks
    free trial account. This may be subject to change in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more information on MLR and serverless clusters in the following
    resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/machine-learning/databricks-runtime-ml.html](https://docs.databricks.com/en/machine-learning/databricks-runtime-ml.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/compute/serverless/index.html](https://docs.databricks.com/en/compute/serverless/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the clusters covered, we will next configure the data pipeline using Delta
    Live Tables.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming with Delta Live Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Databricks **Delta Live Tables** (**DLT**) is a low-code declarative solution
    to build data pipelines. In our example, we will use DLT for the feature engineering
    pipeline, getting data from the source files, checking the data quality, and transforming
    it into features that can be used to train the time series model. You can find
    more information on DLT at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.databricks.com/discover/pages/getting-started-with-delta-live-tables](https://www.databricks.com/discover/pages/getting-started-with-delta-live-tables)'
  prefs: []
  type: TYPE_NORMAL
- en: We will go into the details of the DLT configuration in the *Implementing* *workflows*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Databricks workflows are the equivalent of the Airflow DAGs that we used in
    [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087) and [*Chapter 9*](B18568_09.xhtml#_idTextAnchor169).
    You can find more information on workflows, also referred to as **jobs**, at the
    following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/jobs/index.html](https://docs.databricks.com/en/jobs/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: We will now go into the details of *jobs* configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code example in this chapter includes four workflows. These are implemented
    as jobs in Databricks. The best way to visualize the jobs is from the **Workflows**
    > **Jobs** > **Tasks** views in Databricks, as per *Figures 10.1*, *10.2*, *10.3*,
    and *10.4*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The jobs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ts-spark_ch10_1a_ingest_and_train` – This job is for data ingestion, feature
    engineering, and model training, and is shown in *Figure 10**.1*. It includes
    the following tasks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reset`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dlt_features`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_training`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18568_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Data ingestion, feature engineering, and model training job'
  prefs: []
  type: TYPE_NORMAL
- en: '`ts-spark_ch10_1b_ingest_and_train_automl` – The second job, shown in *Figure
    10**.2*, is another version of the first job, with the difference being the use
    of AutoML, which will be explained in the *Training with* *AutoML* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18568_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Data ingestion, feature engineering, and model training (AutoML)
    job'
  prefs: []
  type: TYPE_NORMAL
- en: '`ts-spark_ch10_2b_ingest_and_forecast` – This job ingests new data, retrains
    the model, and generates and evaluates forecasts, and is shown in *Figure 10**.3*.
    It includes the following tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dlt_features`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update_model`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_forecast`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update_data`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`evaluate_forecast`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18568_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Ingest new data, retrain model, and generate forecasts job'
  prefs: []
  type: TYPE_NORMAL
- en: '`ts-spark_ch10_2a_update_iteration` – This job, shown in *Figure 10**.4*, calls
    the preceding one multiple times to ingest new data. It simulates what happens
    in the real world whereby the previous end-to-end workflow is launched at a regular
    interval, say, daily or weekly, with new data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18568_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Multiple calls to ingest and process new data job'
  prefs: []
  type: TYPE_NORMAL
- en: Modularity and task separation
  prefs: []
  type: TYPE_NORMAL
- en: As in [*Chapter 9*](B18568_09.xhtml#_idTextAnchor169), we have broken the jobs
    into multiple tasks to illustrate the best practice of modularization. This facilitates
    independent code change, scaling, and task reruns. The ownership of the tasks
    can be with different teams. The jobs in this example can be further split in
    your own implementation, depending on your requirement to launch the tasks separately.
  prefs: []
  type: TYPE_NORMAL
- en: We will explain each of these jobs and related tasks in detail in the upcoming
    sections, starting with the ingestion and training job.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up the jobs required for this chapter, follow the instructions on creating
    jobs and configuring tasks at these links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/jobs/configure-job.html#create-a-new-job](https://docs.databricks.com/en/jobs/configure-job.html#create-a-new-job)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/jobs/configure-task.html#configure-and-edit-databricks-tasks](https://docs.databricks.com/en/jobs/configure-task.html#configure-and-edit-databricks-tasks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to the tables in the upcoming sections for the configuration when creating
    the jobs and related tasks, replacing `<USER_LOGIN>` with your own Databricks
    user login.
  prefs: []
  type: TYPE_NORMAL
- en: Ingest and train
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `ts-spark_ch10_1a_ingest_and_train` job, shown in *Figure 10**.1*, will
    be detailed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 10.1* shows the configuration for the `ts_spark_ch10_1a_ingest_and_train`
    job to use when following the instructions in the previously provided URL. Note
    that for simplicity, we have given each task the same name as the code notebook
    or pipeline that it runs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Job** | `ts_spark_ch10_1a_ingest_and_train` |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 1** | Task name | `ts_spark_ch10_reset` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Notebook |'
  prefs: []
  type: TYPE_TB
- en: '|  | Source | Workspace |'
  prefs: []
  type: TYPE_TB
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_reset`
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | Serverless |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 2** | Task name | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Pipeline |'
  prefs: []
  type: TYPE_TB
- en: '|  | Pipeline | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Trigger a full refresh on the pipeline | R |'
  prefs: []
  type: TYPE_TB
- en: '|  | Depends on | `ts_spark_ch10_reset` |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 3** | Task name | `ts_spark_ch10_model_training` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Notebook |'
  prefs: []
  type: TYPE_TB
- en: '|  | Source | Workspace |'
  prefs: []
  type: TYPE_TB
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_model_training`
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | Serverless |'
  prefs: []
  type: TYPE_TB
- en: '|  | Depends on | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.1: Job configuration – ts_spark_ch10_1a_ingest_and_train'
  prefs: []
  type: TYPE_NORMAL
- en: reset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `reset` task does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Resets the Databricks catalog, `ts_spark`, which is used for this example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloads the data files from the GitHub location for this chapter to the volumes
    created in the `ts_spark` catalog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this task is in the `ts_spark_ch10_reset` notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Catalog and volume
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks’ Unity Catalog provides data governance and management in Databricks.
    It organizes data into a three-level hierarchy: catalogs, schemas (equivalent
    to databases), and tables, views, or volumes. Tabular data is stored in tables
    and views, while files are stored in volumes. In our code example, we are using
    a separate catalog, `ts_spark`, and volumes to store the data files.'
  prefs: []
  type: TYPE_NORMAL
- en: dlt_features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This task is used for data ingestion and feature engineering. It is implemented
    as the `ts_spark_ch10_dlt_features` DLT pipeline, as shown in *Figure 10**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Feature engineering pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find and zoom in on a digital version of *Figure 10**.5* here: [https://packt.link/D9OXb](https://packt.link/D9OXb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up the DLT pipeline required for this chapter, follow the instructions
    on creating the pipeline at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/delta-live-tables/configure-pipeline.html#configure-a-new-delta-live-tables-pipeline](https://docs.databricks.com/en/delta-live-tables/configure-pipeline.html#configure-a-new-delta-live-tables-pipeline)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you will have to create the `ts_spark` catalog before you can set
    up the DLT pipeline. Refer to the following instructions to create the `ts_spark`
    catalog via the Catalog Explorer: [https://docs.databricks.com/aws/en/catalogs/create-catalog?language=Catalog%C2%A0Explorer](https://docs.databricks.com/aws/en/catalogs/create-catalog?language=Catalog%C2%A0Explorer)'
  prefs: []
  type: TYPE_NORMAL
- en: Refer to *Table 10.2* for the configuration when creating the pipeline, replacing
    `<USER_LOGIN>` with your own Databricks user login.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pipeline** | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: '| **General** | Pipeline name | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Serverless | R |'
  prefs: []
  type: TYPE_TB
- en: '|  | Pipeline mode | Triggered |'
  prefs: []
  type: TYPE_TB
- en: '| **Source code** | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_dlt_features`
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Destination** | Storage options | `Unity Catalog` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Default catalog / Default schema | `ts_spark /` `ch10` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.2: DLT configuration – ts_spark_ch10_dlt_features'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this pipeline task is in the `ts_spark_ch10_dlt_features` notebook.
    It has the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Read historical data from files in the `vol01_hist` volume using Auto Loader,
    check the data, and store the data in the `raw_hist_power_consumption` streaming
    table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Auto Loader
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks Auto Loader, also referred to as `cloudfiles` in code, enables the
    efficient incremental ingestion of new data files as they arrive in a cloud storage
    location. You can find more information on Auto Loader at the following link:
    [https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html](https://docs.databricks.com/en/ingestion/cloud-object-storage/auto-loader/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Data quality checks
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks DLT can include data quality checks to ensure data integrity based
    on quality constraints within data pipelines. You can find more information on
    data quality checks in DLT at the following link: [https://docs.databricks.com/en/delta-live-tables/expectations.html](https://docs.databricks.com/en/delta-live-tables/expectations.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Read update data from files in the `vol01_upd` volume using Auto Loader, check
    the data, and store the data in the `raw_upd_power_consumption` streaming table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read raw historical data from the `raw_hist_power_consumption` streaming table,
    transform the data, and store the result in the `curated_hist_power_consumption`
    streaming table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read raw update data from the `raw_upd_power_consumption` streaming table, transform
    the data, and store the result in the `curated_upd_power_consumption` streaming
    table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append data from the `curated_hist_power_consumption` and `curated_upd_power_consumption`
    streaming tables, storing the combined result in the `curated_all_power_consumption`
    streaming table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read curated data from the `curated_all_power_consumption` streaming table,
    use Tempo to calculate the `features_aggr_power_consumption` materialized view.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tempo
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks Tempo is an open source project simplifying time series data manipulation
    within Apache Spark. You can find more information on Tempo at the following link:
    [https://databrickslabs.github.io/tempo/](https://databrickslabs.github.io/tempo/).'
  prefs: []
  type: TYPE_NORMAL
- en: Read aggregated data from the `features_aggr_power_consumption` materialized
    view, and use Tempo to do an `AsOf` join with the `curated_all_power_consumption`
    streaming table. Then, store the result in the `features_gnlr_power_consumption`
    materialized view.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps correspond to the data transformation stages of the medallion approach,
    which was discussed in the *Data processing and storage* section of [*Chapter
    4*](B18568_04.xhtml#_idTextAnchor087).
  prefs: []
  type: TYPE_NORMAL
- en: model_training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This task is used to train a Prophet model using the features calculated in
    the prior `dlt_features` task. The code for `model_training` is in the `ts_spark_ch10_model_training`
    notebook. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the features from `features_aggr_power_consumption`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rename the `Date` column to `ds` and `hourly_Global_active_power` to `y`. These
    column names are required by Prophet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start an MLflow run to track the training in MLflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the Prophet model to the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the model to Unity Catalog, setting the alias as `Champion`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that this notebook shows a simplified model training, which is sufficient
    to illustrate the training step in the example in this chapter. It does not include
    the full model experimentation process and hyperparameter tuning, which we covered
    in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133).
  prefs: []
  type: TYPE_NORMAL
- en: Training with AutoML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another approach to model training is to use Databricks AutoML to find the best
    model for the given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML is a feature within Databricks that automates the process of developing
    ML models. It has tasks such as data profiling, feature engineering, model selection,
    and hyperparameter tuning. This enables users to quickly generate baseline models
    for regression, classification, and forecasting problems. With its “glass box”
    approach, AutoML provides the underlying code for each model, which differs from
    “black box” approaches that do not show the code details. AutoML can be used from
    the UI, as shown in *Figure 10**.6*, or programmatically, as in the example provided
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Databricks AutoML'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more information on AutoML here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.databricks.com/product/automl](https://www.databricks.com/product/automl).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ts-spark_ch10_1b_ingest_and_train_automl` job is an example of how to
    include AutoML programmatically in the training task. The code for this task is
    in the `ts_spark_ch10_model_training_automl` notebook. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the features from `features_aggr_power_consumption`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call the `databricks.automl.forecast` function, which takes care of renaming
    the columns, starting an MLflow run to track the training, and finding the best
    model for forecasting based on the specified `primary_metric` (`mdape` is used
    in the example).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the model to Unity Catalog, setting the alias as `Champion`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The configuration for the `ts_spark_ch10_1b_ingest_and_train_automl` job is
    shown in *Table 10.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Job (optional)** | `ts_spark_ch10_1b_ingest_and_train_automl` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 1** | Task name | `ts_spark_ch10_reset` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Notebook |'
  prefs: []
  type: TYPE_TB
- en: '|  | Source | Workspace |'
  prefs: []
  type: TYPE_TB
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_reset`
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | Serverless |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 2** | Task name | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Pipeline |'
  prefs: []
  type: TYPE_TB
- en: '|  | Pipeline | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Trigger a full refresh on the pipeline | R |'
  prefs: []
  type: TYPE_TB
- en: '|  | Depends on | `ts_spark_ch10_reset` |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 3** | Task name | `ts_spark_ch10_model_training_automl` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Notebook |'
  prefs: []
  type: TYPE_TB
- en: '|  | Source | Workspace |'
  prefs: []
  type: TYPE_TB
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_model_training_automl`
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | *Refer to the earlier section on clusters to choose the compute
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Depends on | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.3: Job configuration – ts_spark_ch10_1b_ingest_and_train_automl'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in addition to simplifying the steps compared to the previous training
    approach without AutoML, we also get to find the best model.
  prefs: []
  type: TYPE_NORMAL
- en: Ingest and forecast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `ts-spark_ch10_2b_ingest_and_forecast` job, shown in *Figure 10**.3*, will
    be detailed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The configuration for the `ts_spark_ch10_2b_ingest_and_forecast` job is shown
    in *Table 10.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Job** | `ts_spark_ch10_2b_ingest_and_forecast` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Job parameters | Key: `upd_iter`Value: `1` |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 1** | Task name | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Pipeline |'
  prefs: []
  type: TYPE_TB
- en: '|  | Pipeline | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Trigger a full refresh on the pipeline | R |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 2** | Task name | `ts_spark_ch10_update_model` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Notebook |'
  prefs: []
  type: TYPE_TB
- en: '|  | Source | Workspace |'
  prefs: []
  type: TYPE_TB
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_update_model`
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | Serverless |'
  prefs: []
  type: TYPE_TB
- en: '|  | Depends on | `ts_spark_ch10_dlt_features` |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 3** | Task name | `ts_spark_ch10_generate_forecast` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Notebook |'
  prefs: []
  type: TYPE_TB
- en: '|  | Source | Workspace |'
  prefs: []
  type: TYPE_TB
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_generate_forecast`
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | Serverless |'
  prefs: []
  type: TYPE_TB
- en: '|  | Depends on | `ts_spark_ch10_update_model` |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 4** | Task name | `ts_spark_ch10_update_data` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Notebook |'
  prefs: []
  type: TYPE_TB
- en: '|  | Source | Workspace |'
  prefs: []
  type: TYPE_TB
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_update_data`
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | Serverless |'
  prefs: []
  type: TYPE_TB
- en: '|  | Depends on | `ts_spark_ch10_generate_forecast` |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 5** | Task name | `ts_spark_ch10_evaluate_forecast` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Notebook |'
  prefs: []
  type: TYPE_TB
- en: '|  | Source | Workspace |'
  prefs: []
  type: TYPE_TB
- en: '|  | Path (notebook) | `/``Workspace/Users/<USER_LOGIN>/ts-spark/ch10/ts_spark_ch10_
    evaluate_forecast` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Compute | Serverless |'
  prefs: []
  type: TYPE_TB
- en: '|  | Depends on | `ts_spark_ch10_update_data` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.4: Job configuration – ts_spark_ch10_2b_ingest_and_forecast'
  prefs: []
  type: TYPE_NORMAL
- en: dlt_features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This task is the same `ts_spark_ch10_dlt_features` DLT pipeline, shown in *Figure
    10**.5*, as used in the earlier *Ingest and train* section, where it was used
    to process historical data. The difference is here we will be calling this pipeline
    to process new data files from the `vol01_upd` volume.
  prefs: []
  type: TYPE_NORMAL
- en: update_model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This task is used to train a Prophet model using the features calculated in
    the prior `dlt_features` task. The code for `update_model` is in the `ts_spark_ch10_update_model`
    notebook. This task is similar to the task discussed in the *model_training* section,
    the difference being that we now have new data to include in the training. The
    steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the features from `features_aggr_power_consumption`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rename the `Date` column to `ds` and `hourly_Global_active_power` to `y`. These
    column names are required by Prophet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the Prophet model to the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the model to Unity Catalog, setting the alias as `Champion`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the latest model updated, we can use it to forecast next.
  prefs: []
  type: TYPE_NORMAL
- en: generate_forecast
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This task uses the previously trained model to generate and store forecasts.
    The code for `generate_forecast` is in the `ts_spark_ch10_generate_forecast` notebook.
    The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the `Champion` model from Unity Catalog.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a forecast for the next 24 hours.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the forecasts, together with the model’s name and version, in the `forecast`
    table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the forecast generated, we can compare the forecasted time period against
    the actuals, which we will get next.
  prefs: []
  type: TYPE_NORMAL
- en: update_data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This task simply copies the data file for the new time period from the `vol01_upd_src`
    volume to `vol01_upd`. The code for `update_data` is in the `ts_spark_ch10_update_data`
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: evaluate_forecast
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This task calculates and stores the forecasting accuracy metrics. The code
    for `evaluate_forecast` is in the `ts_spark_ch10_evaluate_forecast` notebook.
    The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Join the `features_aggr_power_consumption` actuals table to the previously created
    `forecast` table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the `mdape` metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the calculated metrics with the model’s name and version in the `forecast_metrics`
    table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the data quality check results in the `dq_results` table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the forecast evaluated, we can report on the outcome and metrics. We will
    cover this in the *User interface* section. Before we get to this, let’s detail
    how we will orchestrate the multiple iterations of new data arrival and corresponding
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Updating iterations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `ts-spark_ch10_2a_update_iteration` job, shown in *Figure 10**.4*, simulates
    what happens in the real world whereby we have new data to process at a regular
    interval, say, daily or weekly. It calls the `ts-spark_ch10_2b_ingest_and_forecast`
    job seven times, corresponding to one week of daily new data. Every call results
    in the end-to-end processing of a new data file, as described in the previous
    *Ingest and* *forecast* section.
  prefs: []
  type: TYPE_NORMAL
- en: The configuration for the `ts_spark_ch10_2a_update_iterations` job is shown
    in *Table 10.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Job** | `ts_spark_ch10_2a_update_iterations` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 1** | Task name | `ts_spark_ch10_update_iterations` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | For each |'
  prefs: []
  type: TYPE_TB
- en: '|  | Inputs | [1,2,3,4,5,6,7] |'
  prefs: []
  type: TYPE_TB
- en: '| **Task 2 (Add a task to** **loop over)** | Task name | `ts_spark_ch10_update_iterations_iteration`
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Type | Run Job |'
  prefs: []
  type: TYPE_TB
- en: '|  | Job | `ts_spark_ch10_2b_ingest_and_forecast` |'
  prefs: []
  type: TYPE_TB
- en: '|  | Job Parameters | Key: `upd_iter`Value: `{{input}}` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.5: Job configuration – ts_spark_ch10_2a_update_iterations'
  prefs: []
  type: TYPE_NORMAL
- en: Starting the jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the jobs configured and explained, we will now start these jobs, which
    will execute the code for this chapter. You can find more information on running
    jobs here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/jobs/run-now.html](https://docs.databricks.com/en/jobs/run-now.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Proceed in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on `ts-spark_ch10_1a_ingest_and_train`. Wait for the job to complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on `ts-spark_ch10_2a_update_iteration`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the jobs launched and executed, we can review their status, as will be
    explained in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring, security, and governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087) in the *From
    DataOps to ModelOps to DevOps* section and in [*Chapter 9*](B18568_09.xhtml#_idTextAnchor169)
    in the *Governance and security* section, a key requirement for workloads in a
    production environment and with sensitive data is to have proper monitoring, security,
    and governance in place. This is greatly facilitated by leveraging the built-in
    functionalities of a managed platform such as Databricks with Unity Catalog. The
    alternative approach, if we were to develop and test our own custom-built platform,
    requires considerable time and effort to robustly meet these requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The monitoring of the jobs can be done from the `ts-spark_ch10_2b_ingest_and_forecast`
    job. We can see the different runs, their parameters, durations, and statuses,
    among other information useful for monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Databricks Workflows – Jobs – Runs'
  prefs: []
  type: TYPE_NORMAL
- en: The monitoring of the `ts_spark_ch10_dlt_features` DLT pipeline can be done
    from the **Workflows** > **Pipelines** page, as shown in *Figure 10**.8*. We can
    see the different stages, data checks, durations, and statuses, among other information
    useful for monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Databricks DLT pipelines'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more information on observability, monitoring, and alerting here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/delta-live-tables/observability.html](https://docs.databricks.com/en/delta-live-tables/observability.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.databricks.com/blog/lakehouse-monitoring-unified-solution-quality-data-and-ai](https://www.databricks.com/blog/lakehouse-monitoring-unified-solution-quality-data-and-ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/lakehouse-monitoring/index.html](https://docs.databricks.com/en/lakehouse-monitoring/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/aws/en/lakehouse-monitoring/monitor-alerts](https://docs.databricks.com/aws/en/lakehouse-monitoring/monitor-alerts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As shown in *Figure 10**.9*, setting access permissions to tables and other
    objects requires just a few clicks when using Unity Catalog.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Databricks Unity Catalog – setting permission'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to define fine-grained access control at a more granular
    row or column level within tables, as per the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.databricks.com/resources/demos/videos/governance/access-controls-with-unity-catalog](https://www.databricks.com/resources/demos/videos/governance/access-controls-with-unity-catalog)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more information on security here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/security/index.html](https://docs.databricks.com/en/security/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An important consideration for governance is being able to track the lineage
    of data assets, as shown in *Figure 10**.10*. We can see here the source of the
    data, the multiple intermediate stages, and the final tables where the data is
    stored. Unity Catalog tracks this automatically in Databricks to give us real-time
    visibility into the data flows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Databricks Unity Catalog – lineage view'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find and zoom in on a digital version of *Figure* *10**.10* here:'
  prefs: []
  type: TYPE_NORMAL
- en: https://packt.link/D6DyC
  prefs: []
  type: TYPE_NORMAL
- en: 'We have touched only briefly on governance and security with Databricks Unity
    Catalog. You can find more information here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.databricks.com/product/unity-catalog](https://www.databricks.com/product/unity-catalog)'
  prefs: []
  type: TYPE_NORMAL
- en: With an understanding of how to leverage a platform such as Databricks for monitoring,
    security, and governance, we will next uncover how to present the outcome of time
    series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks UI — AI/BI dashboards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to presenting the outcome of the time series analysis we have
    done so far, Databricks provides a few options for the user interface with AI/BI
    dashboards, Genie spaces, AI-based chatbots, and Lakehouse Apps. We will cover
    AI/BI dashboards in this section and the other options in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We have been using various graphs extensively throughout this book to represent
    data and the outcome of analysis. This has required us to execute code in notebooks
    to create the graphs. This works well when we are able to write code and have
    an execution environment. When this is not the case, a common way to present the
    data and the outcome of the analysis is with a reporting dashboard. This is possible
    with Databricks AI/BI dashboards, as shown in *Figure 10**.11*.
  prefs: []
  type: TYPE_NORMAL
- en: The Databricks AI/BI dashboard is a solution integrated into the Databricks
    platform to create reports and dashboards. It has AI-powered capabilities to assist
    with the creation of queries and data visualizations. The dashboards can be published
    and shared for consumption.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Databricks AI/BI dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install this dashboard in your own environment, first, download it from
    the following location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/blob/main/ch10/ts_spark_ch10.lvdash.json](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/blob/main/ch10/ts_spark_ch10.lvdash.json)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dashboard file can then be imported into your own environment by following
    the instructions here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/dashboards/index.html#import-a-dashboard-file](https://docs.databricks.com/en/dashboards/index.html#import-a-dashboard-file)'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need a SQL Warehouse in order to run the dashboard. Refer to the following
    instructions to create a SQL Warehouse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/aws/en/compute/sql-warehouse/create](https://docs.databricks.com/aws/en/compute/sql-warehouse/create).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this dashboard, we have brought together, in a combined view, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Graph of the actual and forecasted values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of records that failed and passed the data quality checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics for different model versions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find more information on AI/BI dashboards at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.databricks.com/blog/introducing-aibi-intelligent-analytics-real-world-data](https://www.databricks.com/blog/introducing-aibi-intelligent-analytics-real-world-data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/en/dashboards/index.html](https://docs.databricks.com/en/dashboards/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the end-to-end example of time series analysis on a managed Spark platform,
    this chapter has shown how to leverage the out-of-the-box features of Databricks
    to go further with Apache Spark. We have gone from data ingestion with a streaming
    pipeline to feature engineering and model training and to inferencing and reporting
    while ensuring that monitoring, security, and governance are in place. By combining
    pre-built features on Databricks with our own custom code, we were able to implement
    a solution that can be extended to further use cases.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to our last chapter, where we will expand on some of the recent
    developments in time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/ds](https://packt.link/ds)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ds_(1).jpg)'
  prefs: []
  type: TYPE_IMG
