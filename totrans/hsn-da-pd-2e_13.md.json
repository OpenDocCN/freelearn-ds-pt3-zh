["```py\n>>> %matplotlib inline\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> import pandas as pd\n>>> import seaborn as sns\n```", "```py\n>>> red_wine = pd.read_csv('data/winequality-red.csv')\n```", "```py\n>>> def plot_quality_scores(df, kind):\n...     ax = df.quality.value_counts().sort_index().plot.barh(\n...         title=f'{kind.title()} Wine Quality Scores',\n...         figsize=(12, 3)\n...     ) \n...     ax.axes.invert_yaxis()\n...     for bar in ax.patches:\n...         ax.text(\n...             bar.get_width(),\n...             bar.get_y() + bar.get_height()/2,\n...             f'{bar.get_width()/df.shape[0]:.1%}',\n...             verticalalignment='center'\n...         )\n...     plt.xlabel('count of wines')\n...     plt.ylabel('quality score')\n...  \n...     for spine in ['top', 'right']:\n...         ax.spines[spine].set_visible(False)\n... \n...     return ax\n>>> plot_quality_scores(red_wine, 'red')\n```", "```py\n>>> red_wine.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1599 entries, 0 to 1598\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   fixed acidity         1599 non-null   float64\n 1   volatile acidity      1599 non-null   float64\n 2   citric acid           1599 non-null   float64\n 3   residual sugar        1599 non-null   float64\n 4   chlorides             1599 non-null   float64\n 5   free sulfur dioxide   1599 non-null   float64\n 6   total sulfur dioxide  1599 non-null   float64\n 7   density               1599 non-null   float64\n 8   pH                    1599 non-null   float64\n 9   sulphates             1599 non-null   float64\n 10  alcohol               1599 non-null   float64\n 11  quality               1599 non-null   int64  \ndtypes: float64(11), int64(1)\nmemory usage: 150.0 KB\n```", "```py\n>>> red_wine.describe()\n```", "```py\n>>> red_wine['high_quality'] = pd.cut(\n...     red_wine.quality, bins=[0, 6, 10], labels=[0, 1]\n... )\n>>> red_wine.high_quality.value_counts(normalize=True)\n0    0.86429\n1    0.13571\nName: high_quality, dtype: float64\n```", "```py\n>>> red_wine = pd.read_csv('data/winequality-red.csv')\n>>> white_wine = \\\n...     pd.read_csv('data/winequality-white.csv', sep=';')\n```", "```py\n>>> wine = pd.concat([\n...     white_wine.assign(kind='white'),\n...     red_wine.assign(kind='red')\n... ])\n>>> wine.sample(5, random_state=10)\n```", "```py\n>>> wine.kind.value_counts()\nwhite    4898\nred      1599\nName: kind, dtype: int64\n```", "```py\n>>> import math\n>>> chemical_properties = [col for col in wine.columns\n...                        if col not in ['quality', 'kind']]\n>>> melted = \\\n...     wine.drop(columns='quality').melt(id_vars=['kind'])\n>>> fig, axes = plt.subplots(\n...     math.ceil(len(chemical_properties) / 4), 4, \n...     figsize=(15, 10)\n... )\n>>> axes = axes.flatten()\n>>> for prop, ax in zip(chemical_properties, axes):\n...     sns.boxplot(\n...         data=melted[melted.variable.isin([prop])], \n...         x='variable', y='value', hue='kind', ax=ax\n...     ).set_xlabel('')\n>>> for ax in axes[len(chemical_properties):]:\n...     ax.remove() # remove the extra subplots\n>>> plt.suptitle(\n...     'Comparing Chemical Properties of Red and White Wines'\n... )\n>>> plt.tight_layout()\n```", "```py\n>>> planets = pd.read_csv('data/planets.csv')\n```", "```py\n>>> fig = plt.figure(figsize=(7, 7))\n>>> sns.heatmap(\n...     planets.drop(columns='discoveryyear').corr(), \n...     center=0, vmin=-1, vmax=1, square=True, annot=True,\n...     cbar_kws={'shrink': 0.8}\n... )\n```", "```py\n>>> planets.eccentricity.min(), planets.eccentricity.max()\n(0.0, 0.956) # circular and elliptical eccentricities\n>>> planets.eccentricity.hist()\n>>> plt.xlabel('eccentricity')\n>>> plt.ylabel('frequency')\n>>> plt.title('Orbit Eccentricities')\n```", "```py\n>>> planets[[\n...     'period', 'eccentricity', 'semimajoraxis', 'mass'\n... ]].info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4094 entries, 0 to 4093\nData columns (total 4 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   period         3930 non-null   float64\n 1   eccentricity   1388 non-null   float64\n 2   semimajoraxis  1704 non-null   float64\n 3   mass           1659 non-null   float64\ndtypes: float64(4)\nmemory usage: 128.1 KB\n```", "```py\n>>> planets[[\n...     'period', 'eccentricity', 'semimajoraxis', 'mass'\n... ]].dropna().shape\n(1222, 4)\n```", "```py\n>>> planets[[\n...     'period', 'eccentricity', 'semimajoraxis', 'mass'\n... ]].describe()\n```", "```py\n>>> sns.scatterplot(\n...     x=planets.semimajoraxis, y=planets.period, \n...     hue=planets.list, alpha=0.5\n... )\n>>> plt.title('period vs. semimajoraxis')\n>>> plt.legend(title='') \n```", "```py\n>>> fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n>>> in_solar_system = (planets.list == 'Solar System')\\\n...     .rename('in solar system?')\n>>> sns.scatterplot(\n...     x=planets.semimajoraxis, y=planets.period, \n...     hue=in_solar_system, ax=ax\n... )\n>>> ax.set_yscale('log')\n>>> solar_system = planets[planets.list == 'Solar System']\n>>> for planet in solar_system.name:\n...     data = solar_system.query(f'name == \"{planet}\"')\n...     ax.annotate(\n...         planet, \n...         (data.semimajoraxis, data.period), \n...         (7 + data.semimajoraxis, data.period),\n...         arrowprops=dict(arrowstyle='->')\n...     )\n>>> ax.set_title('log(orbital period) vs. semi-major axis')\n```", "```py\n>>> import numpy as np\n>>> import pandas as pd\n>>> planets = pd.read_csv('data/planets.csv')\n>>> red_wine = pd.read_csv('data/winequality-red.csv')\n>>> wine = pd.concat([\n...     pd.read_csv(\n...         'data/winequality-white.csv', sep=';'\n...     ).assign(kind='white'), \n...     red_wine.assign(kind='red')\n... ])\n```", "```py\nshuffled = \\\n    planets.reindex(np.random.permutation(planets.index))\ntrain_end_index = int(np.ceil(shuffled.shape[0] * .75))\ntraining = shuffled.iloc[:train_end_index,]\ntesting = shuffled.iloc[train_end_index:,]\n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> X = planets[['eccentricity', 'semimajoraxis', 'mass']]\n>>> y = planets.period\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.25, random_state=0\n... )\n```", "```py\n>>> X.shape, y.shape # original data\n((4094, 3), (4094,))\n>>> X_train.shape, y_train.shape # training data\n((3070, 3), (3070,))\n>>> X_test.shape, y_test.shape # testing data\n((1024, 3), (1024,))\n```", "```py\n>>> X_train.head()\n      eccentricity  semimajoraxis  mass\n1390           NaN            NaN   NaN\n2837           NaN            NaN   NaN\n3619           NaN         0.0701   NaN\n1867           NaN            NaN   NaN\n1869           NaN            NaN   NaN\n```", "```py\n>>> y_train.head()\n1390     1.434742\n2837    51.079263\n3619     7.171000\n1867    51.111024\n1869    62.869161\nName: period, dtype: float64\n```", "```py\n>>> from sklearn.preprocessing import StandardScaler\n>>> standardized = StandardScaler().fit_transform(X_train)\n# examine some of the non-NaN values\n>>> standardized[~np.isnan(standardized)][:30]\narray([-5.43618156e-02,  1.43278593e+00,  1.95196592e+00,\n        4.51498477e-03, -1.96265630e-01,  7.79591646e-02, \n        ...,\n       -2.25664815e-02,  9.91013258e-01, -7.48808523e-01,\n       -4.99260165e-02, -8.59044215e-01, -5.49264158e-02])\n```", "```py\n>>> from sklearn.preprocessing import MinMaxScaler\n>>> normalized = MinMaxScaler().fit_transform(X_train)\n# examine some of the non-NaN values\n>>> normalized[~np.isnan(normalized)][:30]\narray([2.28055906e-05, 1.24474091e-01, 5.33472803e-01,\n       1.71374569e-03, 1.83543340e-02, 1.77824268e-01, \n       ...,\n       9.35966714e-04, 9.56961137e-02, 2.09205021e-02, \n       1.50201619e-04, 0.00000000e+00, 6.59028789e-06])\n```", "```py\n>>> np.where(wine.kind == 'red', 1, 0)\narray([0, 0, 0, ..., 1, 1, 1])\n```", "```py\n>>> from sklearn.preprocessing import LabelEncoder\n>>> pd.Series(LabelEncoder().fit_transform(pd.cut(\n...     red_wine.quality, \n...     bins=[-1, 3, 6, 10], \n...     labels=['0-3 (low)', '4-6 (med)', '7-10 (high)']\n... ))).value_counts()\n1    1372\n2     217\n0      10\ndtype: int64\n```", "```py\n>>> planets.list.value_counts()\nConfirmed planets                    3972\nControversial                          97\nRetracted planet candidate             11\nSolar System                            9\nKepler Objects of Interest              4\nPlanets in binary systems, S-type       1\nName: list, dtype: int64\n```", "```py\n>>> pd.get_dummies(planets.list).head()\n```", "```py\n>>> pd.get_dummies(planets.list, drop_first=True).head()\n```", "```py\n>>> from sklearn.preprocessing import LabelBinarizer\n>>> LabelBinarizer().fit_transform(planets.list)\narray([[1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0], \n       [1, 0, 0, 0, 0, 0],\n       ..., \n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0]])\n```", "```py\n>>> planets[['semimajoraxis', 'mass', 'eccentricity']].tail()\n      semimajoraxis    mass  eccentricity\n4089        0.08150  1.9000         0.000\n4090        0.04421  0.7090         0.038\n4091            NaN  0.3334         0.310\n4092            NaN  0.4000         0.270\n4093            NaN  0.4200         0.160\n```", "```py\n>>> from sklearn.impute import SimpleImputer\n>>> SimpleImputer().fit_transform(\n...     planets[['semimajoraxis', 'mass', 'eccentricity']]\n... )\narray([[ 1.29      , 19.4       ,  0.231     ],\n       [ 1.54      , 11.2       ,  0.08      ],\n       [ 0.83      ,  4.8       ,  0.        ],\n       ...,\n       [ 5.83796389,  0.3334    ,  0.31      ],\n       [ 5.83796389,  0.4       ,  0.27      ],\n       [ 5.83796389,  0.42      ,  0.16      ]])\n```", "```py\n>>> from sklearn.impute import KNNImputer\n>>> KNNImputer().fit_transform(\n...     planets[['semimajoraxis', 'mass', 'eccentricity']]\n... )\narray([[ 1.29    , 19.4     ,  0.231   ],\n       [ 1.54    , 11.2     ,  0.08    ],\n       [ 0.83    ,  4.8     ,  0.      ],\n       ...,\n       [ 0.404726,  0.3334  ,  0.31    ],\n       [ 0.85486 ,  0.4     ,  0.27    ],\n       [ 0.15324 ,  0.42    ,  0.16    ]])\n```", "```py\n>>> from sklearn.impute import MissingIndicator\n>>> MissingIndicator().fit_transform(\n...     planets[['semimajoraxis', 'mass', 'eccentricity']]\n... )\narray([[False, False, False],\n       [False, False, False],\n       [False, False, False],\n       ...,\n       [ True, False, False],\n       [ True, False, False],\n       [ True, False, False]])\n```", "```py\n>>> from sklearn.preprocessing import FunctionTransformer\n>>> FunctionTransformer(\n...     np.abs, validate=True\n... ).fit_transform(X_train.dropna())\narray([[0.51   , 4.94   , 1.45   ],\n       [0.17   , 0.64   , 0.85   ],\n       [0.08   , 0.03727, 1.192  ],\n       ...,\n       [0.295  , 4.46   , 1.8    ],\n       [0.34   , 0.0652 , 0.0087 ],\n       [0.3    , 1.26   , 0.5    ]])\n```", "```py\n>>> from sklearn.compose import ColumnTransformer \n>>> from sklearn.impute import KNNImputer\n>>> from sklearn.preprocessing import (\n...     MinMaxScaler, StandardScaler\n... )\n>>> ColumnTransformer([\n...     ('impute', KNNImputer(), [0]),\n...     ('standard_scale', StandardScaler(), [1]),\n...     ('min_max', MinMaxScaler(), [2])\n... ]).fit_transform(X_train)[10:15] \narray([[ 0.17      , -0.04747176,  0.0107594 ],\n       [ 0.08      , -0.05475873,  0.01508851],\n       [ 0.15585591,         nan,  0.13924042],\n       [ 0.15585591,         nan,         nan],\n       [ 0.        , -0.05475111,  0.00478471]])\n```", "```py\n>>> from sklearn.compose import make_column_transformer\n>>> from sklearn.preprocessing import (\n...     OneHotEncoder, StandardScaler\n... )\n>>> categorical = [\n...     col for col in planets.columns\n...     if col in [\n...         'list', 'name', 'description', \n...         'discoverymethod', 'lastupdate'\n...     ]\n... ]\n>>> numeric = [\n...     col for col in planets.columns\n...     if col not in categorical\n... ]\n>>> make_column_transformer(\n...     (StandardScaler(), numeric),\n...     (OneHotEncoder(sparse=False), categorical)\n... ).fit_transform(planets.dropna())\narray([[ 3.09267587, -0.2351423 , -0.40487424, ...,  \n         0.        ,  0.        ],\n       [ 1.432445  , -0.24215395, -0.28360905, ...,  \n         0.        ,  0.        ],\n       [ 0.13665505, -0.24208849, -0.62800218, ...,  \n         0.        ,  0.        ],\n       ...,\n       [-0.83289954, -0.76197788, -0.84918988, ...,  \n         1.        ,  0.        ],\n       [ 0.25813535,  0.38683239, -0.92873984, ...,  \n         0.        ,  0.        ],\n       [-0.26827931, -0.21657671, -0.70076129, ...,  \n         0.        ,  1.        ]])\n```", "```py\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.linear_model import LinearRegression\n>>> Pipeline([\n...     ('scale', StandardScaler()), ('lr', LinearRegression())\n... ])\nPipeline(steps=[('scale', StandardScaler()), \n                ('lr', LinearRegression())])\n```", "```py\n>>> from sklearn.compose import ColumnTransformer \n>>> from sklearn.impute import KNNImputer\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import (\n...     MinMaxScaler, StandardScaler\n... )\n>>> ColumnTransformer([\n...     ('impute', Pipeline([\n...         ('impute', KNNImputer()),\n...         ('scale', StandardScaler())\n...     ]), [0]),\n...     ('standard_scale', StandardScaler(), [1]),\n...     ('min_max', MinMaxScaler(), [2])\n... ]).fit_transform(X_train)[10:15]\narray([[ 0.13531604, -0.04747176,  0.0107594 ],\n       [-0.7257111 , -0.05475873,  0.01508851],\n       [ 0.        ,         nan,  0.13924042],\n       [ 0.        ,         nan,         nan],\n       [-1.49106856, -0.05475111,  0.00478471]])\n```", "```py\n>>> from sklearn.pipeline import make_pipeline\n>>> make_pipeline(StandardScaler(), LinearRegression())\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('linearregression', LinearRegression())])\n```", "```py\n>>> from sklearn.cluster import KMeans\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> kmeans_pipeline = Pipeline([\n...     ('scale', StandardScaler()), \n...     ('kmeans', KMeans(8, random_state=0))\n... ])\n```", "```py\n>>> kmeans_data = planets[['semimajoraxis', 'period']].dropna()\n>>> kmeans_pipeline.fit(kmeans_data)\nPipeline(steps=[('scale', StandardScaler()),\n                ('kmeans', KMeans(random_state=0))])\n```", "```py\n>>> fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n>>> sns.scatterplot(\n...     x=kmeans_data.semimajoraxis, \n...     y=kmeans_data.period, \n...     hue=kmeans_pipeline.predict(kmeans_data),\n...     ax=ax, palette='Accent'\n... )\n>>> ax.set_yscale('log')\n>>> solar_system = planets[planets.list == 'Solar System']\n>>> for planet in solar_system.name:\n...     data = solar_system.query(f'name == \"{planet}\"')\n...     ax.annotate(\n...         planet, \n...         (data.semimajoraxis, data.period), \n...         (7 + data.semimajoraxis, data.period),\n...         arrowprops=dict(arrowstyle='->')\n...     )\n>>> ax.get_legend().remove()\n>>> ax.set_title('KMeans Clusters')\n```", "```py\nimport matplotlib.pyplot as plt\ndef elbow_point(data, pipeline, kmeans_step_name='kmeans',    \n                k_range=range(1, 11), ax=None):\n    \"\"\"\n    Plot the elbow point to find an appropriate k for\n    k-means clustering.\n    Parameters:\n        - data: The features to use\n        - pipeline: The scikit-learn pipeline with `KMeans`\n        - kmeans_step_name: Name of `KMeans` step in pipeline\n        - k_range: The values of `k` to try\n        - ax: Matplotlib `Axes` to plot on.\n    Returns: \n        A matplotlib `Axes` object\n    \"\"\"\n    scores = []\n    for k in k_range:\n        pipeline.named_steps[kmeans_step_name].n_clusters = k\n        pipeline.fit(data)\n        # score is -1*inertia so we multiply by -1 for inertia\n        scores.append(pipeline.score(data) * -1)\n    if not ax:\n        fig, ax = plt.subplots()\n    ax.plot(k_range, scores, 'bo-')\n    ax.set_xlabel('k')\n    ax.set_ylabel('inertias')\n    ax.set_title('Elbow Point Plot')\n    return ax\n```", "```py\n>>> from ml_utils.elbow_point import elbow_point\n>>> ax = elbow_point(\n...     kmeans_data, \n...     Pipeline([\n...         ('scale', StandardScaler()), \n...         ('kmeans', KMeans(random_state=0))\n...     ])\n... )\n>>> ax.annotate(\n...     'possible appropriate values for k', xy=(2, 900), \n...     xytext=(2.5, 1500), arrowprops=dict(arrowstyle='->')\n... )\n>>> ax.annotate(\n...     '', xy=(3, 3480), xytext=(4.4, 1450), \n...     arrowprops=dict(arrowstyle='->')\n... )\n```", "```py\n>>> fig = plt.figure(figsize=(8, 6))\n>>> outside = fig.add_axes([0.1, 0.1, 0.9, 0.9])\n>>> inside = fig.add_axes([0.6, 0.2, 0.35, 0.35])\n```", "```py\n>>> scaled = kmeans_pipeline_2.named_steps['scale']\\ \n...     .fit_transform(kmeans_data) \n>>> cluster_distances = kmeans_pipeline_2\\\n...     .fit_transform(kmeans_data)\n```", "```py\n>>> for ax, data, title, axes_labels in zip(\n...     [outside, inside], [scaled, cluster_distances], \n...     ['Visualizing Clusters', 'Cluster Distance Space'], \n...     ['standardized', 'distance to centroid']\n... ):\n...     sns.scatterplot(\n...         x=data[:,0], y=data[:,1], ax=ax, alpha=0.75, s=100,\n...         hue=kmeans_pipeline_2.named_steps['kmeans'].labels_\n...     )\n... \n...     ax.get_legend().remove()\n...     ax.set_title(title)\n...     ax.set_xlabel(f'semimajoraxis ({axes_labels})')\n...     ax.set_ylabel(f'period ({axes_labels})')\n...     ax.set_ylim(-1, None)\n```", "```py\n>>> cluster_centers = kmeans_pipeline_2\\\n...     .named_steps['kmeans'].cluster_centers_\n>>> for color, centroid in zip(\n...     ['blue', 'orange'], cluster_centers\n... ):\n...     outside.plot(*centroid, color=color, marker='x')\n...     outside.annotate(\n...         f'{color} center', xy=centroid, \n...         xytext=centroid + [0, 5], \n...         arrowprops=dict(arrowstyle='->')\n...     )\n```", "```py\n>>> from sklearn.metrics import silhouette_score\n>>> silhouette_score(\n...     kmeans_data, kmeans_pipeline.predict(kmeans_data)\n... )\n0.7579771626036678\n```", "```py\n>>> from sklearn.metrics import davies_bouldin_score\n>>> davies_bouldin_score(\n...     kmeans_data, kmeans_pipeline.predict(kmeans_data)\n... )\n0.4632311032231894 \n```", "```py\n>>> from sklearn.metrics import calinski_harabasz_score\n>>> calinski_harabasz_score(\n...     kmeans_data, kmeans_pipeline.predict(kmeans_data)\n... )\n21207.276781867335\n```", "```py\n>>> data = planets[\n...     ['semimajoraxis', 'period', 'mass', 'eccentricity']\n... ].dropna()\n>>> X = data[['semimajoraxis', 'mass', 'eccentricity']]\n>>> y = data.period\n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.25, random_state=0\n... )\n```", "```py\n>>> from sklearn.linear_model import LinearRegression\n>>> lm = LinearRegression().fit(X_train, y_train)\n```", "```py\n# get intercept\n>>> lm.intercept_\n-622.9909910671811 \n# get coefficients\n>>> [(col, coef) for col, coef in \n...  zip(X_train.columns, lm.coef_)]\n[('semimajoraxis', 1880.4365990440929),\n ('mass', -90.18675916509196),\n ('eccentricity', -3201.078059333091)] \n```", "```py\n>>> preds = lm.predict(X_test)\n```", "```py\n>>> fig, axes = plt.subplots(1, 1, figsize=(5, 3))\n>>> axes.plot(\n...     X_test.semimajoraxis, y_test, 'ob',\n...     label='actuals', alpha=0.5\n... )\n>>> axes.plot(\n...     X_test.semimajoraxis, preds, 'or', \n...     label='predictions', alpha=0.5\n... )\n>>> axes.set(xlabel='semimajoraxis', ylabel='period')\n>>> axes.legend()\n>>> axes.set_title('Linear Regression Results')\n```", "```py\n>>> np.corrcoef(y_test, preds)[0][1]\n0.9692104355988059\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef plot_residuals(y_test, preds):\n    \"\"\"\n    Plot residuals to evaluate regression.\n    Parameters:\n        - y_test: The true values for y\n        - preds: The predicted values for y\n    Returns:\n        Subplots of residual scatter plot and residual KDE\n    \"\"\"\n    residuals = y_test – preds\n    fig, axes = plt.subplots(1, 2, figsize=(15, 3))\n    axes[0].scatter(np.arange(residuals.shape[0]), residuals)\n    axes[0].set(xlabel='Observation', ylabel='Residual')\n    residuals.plot(kind='kde', ax=axes[1])\n    axes[1].set_xlabel('Residual')\n    plt.suptitle('Residuals')\n    return axes\n```", "```py\n>>> from ml_utils.regression import plot_residuals\n>>> plot_residuals(y_test, preds)\n```", "```py\n>>> lm.score(X_test, y_test)\n0.9209013475842684 \n```", "```py\n>>> from sklearn.metrics import r2_score\n>>> r2_score(y_test, preds)\n0.9209013475842684 \n```", "```py\nfrom sklearn.metrics import r2_score\ndef adjusted_r2(model, X, y):\n    \"\"\"\n    Calculate the adjusted R^2.\n    Parameters:\n        - model: Estimator object with a `predict()` method\n        - X: The values to use for prediction.\n        - y: The true values for scoring.\n    Returns: \n        The adjusted R^2 score.\n    \"\"\"\n    r2 = r2_score(y, model.predict(X))\n    n_obs, n_regressors = X.shape\n    adj_r2 = \\\n        1 - (1 - r2) * (n_obs - 1)/(n_obs - n_regressors - 1)\n    return adj_r2\n```", "```py\n>>> from ml_utils.regression import adjusted_r2\n>>> adjusted_r2(lm, X_test, y_test)\n0.9201155993814631 \n```", "```py\n>>> from sklearn.metrics import explained_variance_score\n>>> explained_variance_score(y_test, preds)\n0.9220144218429371 \n```", "```py\n>>> from sklearn.metrics import mean_absolute_error\n>>> mean_absolute_error(y_test, preds)\n1369.441817073533 \n```", "```py\n>>> from sklearn.metrics import mean_squared_error\n>>> np.sqrt(mean_squared_error(y_test, preds))\n3248.499961928374 \n```", "```py\n>>> from sklearn.metrics import median_absolute_error\n>>> median_absolute_error(y_test, preds)\n759.8613358335442 \n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> red_y = red_wine.pop('high_quality')\n>>> red_X = red_wine.drop(columns='quality')\n>>> r_X_train, r_X_test, \\\n... r_y_train, r_y_test = train_test_split(\n...     red_X, red_y, test_size=0.1, random_state=0,\n...     stratify=red_y\n... )\n```", "```py\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.linear_model import LogisticRegression\n>>> red_quality_lr = Pipeline([\n...     ('scale', StandardScaler()), \n...     ('lr', LogisticRegression(\n...         class_weight='balanced', random_state=0\n...     ))\n... ])\n```", "```py\n>>> red_quality_lr.fit(r_X_train, r_y_train)\nPipeline(steps=[('scale', StandardScaler()),\n                ('lr', LogisticRegression(\n                     class_weight='balanced',\n                     random_state=0))])\n```", "```py\n>>> quality_preds = red_quality_lr.predict(r_X_test)\n```", "```py\n>>> from sklearn.model_selection import train_test_split \n>>> wine_y = np.where(wine.kind == 'red', 1, 0)\n>>> wine_X = wine.drop(columns=['quality', 'kind'])\n>>> w_X_train, w_X_test, \\\n... w_y_train, w_y_test = train_test_split(\n...     wine_X, wine_y, test_size=0.25, \n...     random_state=0, stratify=wine_y\n... )\n```", "```py\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.pipeline import Pipeline\n>>> from sklearn.preprocessing import StandardScaler\n>>> white_or_red = Pipeline([\n...     ('scale', StandardScaler()), \n...     ('lr', LogisticRegression(random_state=0))\n... ]).fit(w_X_train, w_y_train)\n```", "```py\n>>> kind_preds = white_or_red.predict(w_X_test)\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\ndef confusion_matrix_visual(y_true, y_pred, class_labels, \n                            normalize=False, flip=False, \n                            ax=None, title=None, **kwargs):\n    \"\"\"\n    Create a confusion matrix heatmap\n    Parameters:\n        - y_test: The true values for y\n        - preds: The predicted values for y\n        - class_labels: What to label the classes.\n        - normalize: Whether to plot the values as percentages.\n        - flip: Whether to flip the confusion matrix. This is \n          helpful to get TP in the top left corner and TN in \n          the bottom right when dealing with binary \n          classification with labels True and False.\n        - ax: The matplotlib `Axes` object to plot on.\n        - title: The title for the confusion matrix\n        - kwargs: Additional keyword arguments to pass down.\n    Returns: A matplotlib `Axes` object.\n    \"\"\"\n    mat = confusion_matrix(y_true, y_pred)\n    if normalize:\n        fmt, mat = '.2%', mat / mat.sum()\n    else:\n        fmt = 'd'\n    if flip:\n        class_labels = class_labels[::-1]\n        mat = np.flip(mat)\n    axes = sns.heatmap(\n        mat.T, square=True, annot=True, fmt=fmt,\n        cbar=True, cmap=plt.cm.Blues, ax=ax, **kwargs\n    )\n    axes.set(xlabel='Actual', ylabel='Model Prediction')\n    tick_marks = np.arange(len(class_labels)) + 0.5\n    axes.set_xticks(tick_marks)\n    axes.set_xticklabels(class_labels)\n    axes.set_yticks(tick_marks)\n    axes.set_yticklabels(class_labels, rotation=0)\n    axes.set_title(title or 'Confusion Matrix')\n    return axes\n```", "```py\n>>> from ml_utils.classification import confusion_matrix_visual\n>>> confusion_matrix_visual(\n...     r_y_test, quality_preds, ['low', 'high']\n... )\n```", "```py\n>>> from ml_utils.classification import confusion_matrix_visual\n>>> confusion_matrix_visual(\n...     w_y_test, kind_preds, ['white', 'red']\n... )\n```", "```py\n>>> red_quality_lr.score(r_X_test, r_y_test)\n0.775\n```", "```py\n>>> from sklearn.metrics import zero_one_loss\n>>> zero_one_loss(r_y_test, quality_preds)\n0.22499999999999998\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> print(classification_report(r_y_test, quality_preds))\n              precision    recall  f1-score   support\n           0       0.95      0.78      0.86       138\n           1       0.35      0.73      0.47        22\n    accuracy                           0.78       160\n   macro avg       0.65      0.75      0.66       160\nweighted avg       0.86      0.78      0.80       160\n```", "```py\n>>> from sklearn.metrics import classification_report\n>>> print(classification_report(w_y_test, kind_preds))\n              precision    recall  f1-score   support\n           0       0.99      1.00      0.99      1225\n           1       0.99      0.98      0.98       400\n    accuracy                           0.99      1625\n   macro avg       0.99      0.99      0.99      1625\nweighted avg       0.99      0.99      0.99      1625\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import auc, roc_curve\ndef plot_roc(y_test, preds, ax=None):\n    \"\"\"\n    Plot ROC curve to evaluate classification.\n    Parameters:\n        - y_test: The true values for y\n        - preds: The predicted values for y as probabilities\n        - ax: The `Axes` object to plot on\n    Returns: \n        A matplotlib `Axes` object.\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(1, 1)\n    fpr, tpr, thresholds = roc_curve(y_test, preds)\n    ax.plot(\n        [0, 1], [0, 1], color='navy', lw=2, \n        linestyle='--', label='baseline'\n    )\n    ax.plot(fpr, tpr, color='red', lw=2, label='model')\n    ax.legend(loc='lower right')\n    ax.set_title('ROC curve')\n    ax.set_xlabel('False Positive Rate (FPR)')\n    ax.set_ylabel('True Positive Rate (TPR)')\n    ax.annotate(\n        f'AUC: {auc(fpr, tpr):.2}', xy=(0.5, 0),\n        horizontalalignment='center'\n    )\n    return ax\n```", "```py\n>>> from ml_utils.classification import plot_roc\n>>> plot_roc(\n...     w_y_test, white_or_red.predict_proba(w_X_test)[:,1]\n... )\n```", "```py\n>>> from ml_utils.classification import plot_roc\n>>> plot_roc(\n...     r_y_test, red_quality_lr.predict_proba(r_X_test)[:,1]\n... )\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (\n    auc, average_precision_score, precision_recall_curve\n)\ndef plot_pr_curve(y_test, preds, positive_class=1, ax=None):\n    \"\"\"\n    Plot precision-recall curve to evaluate classification.\n    Parameters:\n        - y_test: The true values for y\n        - preds: The predicted values for y as probabilities\n        - positive_class: Label for positive class in the data\n        - ax: The matplotlib `Axes` object to plot on\n    Returns: A matplotlib `Axes` object.\n    \"\"\"\n    precision, recall, thresholds = \\\n        precision_recall_curve(y_test, preds)\n    if not ax:\n        fig, ax = plt.subplots()\n    ax.axhline(\n        sum(y_test == positive_class) / len(y_test), \n        color='navy', lw=2, linestyle='--', label='baseline'\n    )\n    ax.plot(\n        recall, precision, color='red', lw=2, label='model'\n    )\n    ax.legend()\n    ax.set_title(\n        'Precision-recall curve\\n'\n        f\"\"\"AP: {average_precision_score(\n            y_test, preds, pos_label=positive_class\n        ):.2} | \"\"\"\n        f'AUC: {auc(recall, precision):.2}'\n    )\n    ax.set(xlabel='Recall', ylabel='Precision')\n    ax.set_xlim(-0.05, 1.05)\n    ax.set_ylim(-0.05, 1.05)\n    return ax\n```", "```py\n>>> from ml_utils.classification import plot_pr_curve\n>>> plot_pr_curve(\n...     r_y_test, red_quality_lr.predict_proba(r_X_test)[:,1]\n... )\n```", "```py\n>>> from ml_utils.classification import plot_pr_curve\n>>> plot_pr_curve(\n...     w_y_test, white_or_red.predict_proba(w_X_test)[:,1]\n... )\n```"]