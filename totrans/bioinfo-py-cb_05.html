<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer035">
<h1 class="chapter-number" id="_idParaDest-105"><a id="_idTextAnchor104"/>4</h1>
<h1 id="_idParaDest-106"><a id="_idTextAnchor105"/>Advanced NGS Data Processing</h1>
<p>If you work with <strong class="bold">next-generation sequencing</strong> (<strong class="bold">NGS</strong>) data, you know that quality analysis and <a id="_idIndexMarker283"/>processing are two of the great time-sinks in getting results. In the first part of this chapter, we will delve deeper into NGS analysis by using a dataset that includes information about relatives – in our case, a mother, a father, and around 20 offspring. This is a common technique for performing quality analysis, as pedigree information will allow us to make inferences on the number of errors that our filtering rules might produce. We will also take the opportunity to use the same dataset to find genomic features based on existing annotations.</p>
<p>The last recipe of this chapter will delve into another advanced topic using NGS data: metagenomics. We will QIIME2, a Python package for metagenomics, to analyze data. </p>
<p>If you are using Docker, please use the tiagoantao/bioinformatics_base image. The QIIME2 content has a special setup process that will be discussed in the relevant recipe.</p>
<p>In this chapter, there are the following recipes:</p>
<ul>
<li>Preparing a dataset for analysis</li>
<li>Using Mendelian error information for quality control</li>
<li>Exploring data with standard statistics</li>
<li>Finding genomic features from sequencing annotations</li>
<li>Doing metagenomics with QIIME2</li>
</ul>
<h1 id="_idParaDest-107"><a id="_idTextAnchor106"/>Preparing a dataset for analysis</h1>
<p>Our starting <a id="_idIndexMarker284"/>point will be a VCF file (or equivalent) with <a id="_idIndexMarker285"/>calls made by a genotyper (<strong class="bold">Genome Analysis Toolkit</strong> (<strong class="bold">GATK</strong>) in our case), including annotations. As we will be filtering NGS data, we need reliable decision criteria to call a site. So, how do we get that information? Generally, we can’t, but if we need to do so, there are three basic approaches:</p>
<ul>
<li>Using a more robust sequencing technology for comparison – for example, using Sanger sequencing to verify NGS datasets. This is cost-prohibitive and can only be done for a few loci.</li>
<li>Sequencing closely related individuals, for example, two parents and their offspring. In this case, we use Mendelian inheritance rules to decide whether a certain call is acceptable or not. This was the strategy used by both the Human Genome Project and the Anopheles gambiae 1000 Genomes project.</li>
<li>Finally, we can use simulations. This setup is not only quite complex but also of dubious reliability. It’s more of a theoretical option.</li>
</ul>
<p>In this chapter, we will <a id="_idIndexMarker286"/>use the second option, based on the Anopheles gambiae 1000 Genomes project. This project makes available information based on crosses between mosquitoes. A cross will include the parents (mother and father) and up to 20 offspring.</p>
<p>In this recipe, we are going to prepare our data for usage in the later recipes.</p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor107"/>Getting ready</h2>
<p>We will download our files in HDF5 format for faster processing. Please be advised that these files are quite big; you will need a good network connection and plenty of disk space:</p>
<p class="source-code">wget -c ftp://ngs.sanger.ac.uk/production/ag1000g/phase1/AR3/variation/main/hdf5/ag1000g.phase1.ar3.pass.3L.h5</p>
<p class="source-code">wget -c ftp://ngs.sanger.ac.uk/production/ag1000g/phase1/AR3/variation/main/hdf5/ag1000g.phase1.ar3.pass.2L.h5</p>
<p>The files have four crosses with around 20 offspring each. We will use chromosome arms 3L and 2L. At this stage, we also compute Mendelian errors (a subject of the next recipe, so we will delay a detailed discussion until then).</p>
<p>The relevant notebook is <strong class="source-inline">Chapter04/Preparation.py</strong>. There is also a local sample metadata file in the directory called <strong class="source-inline">samples.tsv</strong>.</p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor108"/>How to do it…</h2>
<p>After downloading the <a id="_idIndexMarker287"/>data, follow these steps:</p>
<ol>
<li>First, start with a few imports:<p class="source-code">import pickle</p><p class="source-code">import gzip</p><p class="source-code">import random</p><p class="source-code">import numpy as np</p><p class="source-code">import h5py</p><p class="source-code">import pandas as pd</p></li>
<li>Let’s get the sample metadata:<p class="source-code">samples = pd.read_csv('samples.tsv', sep='\t')</p><p class="source-code">print(len(samples))</p><p class="source-code">print(samples['cross'].unique())</p><p class="source-code">print(samples[samples['cross'] == 'cross-29-2'][['id', 'function']])</p><p class="source-code">print(len(samples[samples['cross'] == 'cross-29-2']))</p><p class="source-code">print(samples[samples['function'] == 'parent'])</p></li>
</ol>
<p>We also print some basic information about the cross we are going to use and all the parents.</p>
<ol>
<li value="3">We prepare to deal with chromosome arm 3L based on its HDF5 file:<p class="source-code">h5_3L = h5py.File('ag1000g.crosses.phase1.ar3sites.3L.h5', 'r')</p><p class="source-code">samples_hdf5 = list(map(lambda sample: sample.decode('utf-8'), h5_3L['/3L/samples']))</p><p class="source-code">calldata_genotype = h5_3L['/3L/calldata/genotype']</p><p class="source-code">MQ0 = h5_3L['/3L/variants/MQ0']</p><p class="source-code">MQ = h5_3L['/3L/variants/MQ']</p><p class="source-code">QD = h5_3L['/3L/variants/QD']</p><p class="source-code">Coverage = h5_3L['/3L/variants/Coverage']</p><p class="source-code">CoverageMQ0 = h5_3L['/3L/variants/CoverageMQ0']</p><p class="source-code">HaplotypeScore = h5_3L['/3L/variants/HaplotypeScore']</p><p class="source-code">QUAL = h5_3L['/3L/variants/QUAL']</p><p class="source-code">FS = h5_3L['/3L/variants/FS']</p><p class="source-code">DP = h5_3L['/3L/variants/DP']</p><p class="source-code">HRun = h5_3L['/3L/variants/HRun']</p><p class="source-code">ReadPosRankSum = h5_3L['/3L/variants/ReadPosRankSum']</p><p class="source-code">my_features = {</p><p class="source-code">    'MQ': MQ,</p><p class="source-code">    'QD': QD,</p><p class="source-code">    'Coverage': Coverage,</p><p class="source-code">    'HaplotypeScore': HaplotypeScore,</p><p class="source-code">    'QUAL': QUAL,</p><p class="source-code">    'FS': FS,</p><p class="source-code">    'DP': DP,</p><p class="source-code">    'HRun': HRun,</p><p class="source-code">    'ReadPosRankSum': ReadPosRankSum</p><p class="source-code">}</p><p class="source-code">num_features = len(my_features)</p><p class="source-code">num_alleles = h5_3L['/3L/variants/num_alleles']</p><p class="source-code">is_snp = h5_3L['/3L/variants/is_snp']</p><p class="source-code">POS = h5_3L['/3L/variants/POS']</p></li>
<li>The <a id="_idIndexMarker288"/>code to compute Mendelian errors is the following:<p class="source-code">#compute mendelian errors (biallelic)</p><p class="source-code">def compute_mendelian_errors(mother, father, offspring):</p><p class="source-code">    num_errors = 0</p><p class="source-code">    num_ofs_problems = 0</p><p class="source-code">    if len(mother.union(father)) == 1:</p><p class="source-code">        # Mother and father are homogenous and the            same for ofs in offspring:</p><p class="source-code">            if len(ofs) == 2:</p><p class="source-code">                # Offspring is het</p><p class="source-code">                num_errors += 1</p><p class="source-code">                num_ofs_problems += 1</p><p class="source-code">            elif len(ofs.intersection(mother)) == 0:</p><p class="source-code">                # Offspring is homo, but opposite from parents</p><p class="source-code">                num_errors += 2</p><p class="source-code">                num_ofs_problems += 1</p><p class="source-code">    elif len(mother) == 1 and len(father) == 1:</p><p class="source-code">        # Mother and father are homo and different</p><p class="source-code">        for ofs in offspring:</p><p class="source-code">            if len(ofs) == 1:</p><p class="source-code">                # Homo, should be het</p><p class="source-code">                num_errors += 1</p><p class="source-code">                num_ofs_problems += 1</p><p class="source-code">    elif len(mother) == 2 and len(father) == 2:</p><p class="source-code">        # Both are het, individual offspring can be anything</p><p class="source-code">        pass</p><p class="source-code">    else:</p><p class="source-code">        # One is het, the other is homo</p><p class="source-code">        homo = mother if len(mother) == 1 else father</p><p class="source-code">        for ofs in offspring:</p><p class="source-code">            if len(ofs) == 1 and ofs.intersection(homo):</p><p class="source-code">                # homo, but not including the allele from parent that is homo</p><p class="source-code">                num_errors += 1</p><p class="source-code">                num_ofs_problems += 1</p><p class="source-code">    return num_errors, num_ofs_problems</p></li>
</ol>
<p>We will <a id="_idIndexMarker289"/>discuss this in the next recipe, <em class="italic">Using Mendelian error information for quality control</em>.</p>
<ol>
<li value="5">We now define a support generator and function to select acceptable positions and accumulate basic data:<p class="source-code">def acceptable_position_to_genotype():</p><p class="source-code">    for i, genotype in enumerate(calldata_genotype):</p><p class="source-code">        if is_snp[i] and num_alleles[i] == 2:</p><p class="source-code">            if len(np.where(genotype == -1)[0]) &gt; 1:</p><p class="source-code">                # Missing data</p><p class="source-code">                continue</p><p class="source-code">            yield i</p><p class="source-code">def acumulate(fun):</p><p class="source-code">    acumulator = {}</p><p class="source-code">    for res in fun():</p><p class="source-code">        if res is not None:</p><p class="source-code">            acumulator[res[0]] = res[1]</p><p class="source-code">    return acumulator</p></li>
<li>We now need to find <a id="_idIndexMarker290"/>the indexes of our cross (mother, father, and 20 offspring) on the HDF5 file:<p class="source-code">def get_family_indexes(samples_hdf5, cross_pd):</p><p class="source-code">    offspring = []</p><p class="source-code">    for i, individual in cross_pd.T.iteritems():</p><p class="source-code">        index = samples_hdf5.index(individual.id)</p><p class="source-code">        if individual.function == 'parent':</p><p class="source-code">            if individual.sex == 'M':</p><p class="source-code">                father = index</p><p class="source-code">            else:</p><p class="source-code">                mother = index</p><p class="source-code">        else:</p><p class="source-code">            offspring.append(index)</p><p class="source-code">    return {'mother': mother, 'father': father, 'offspring': offspring}</p><p class="source-code">cross_pd = samples[samples['cross'] == 'cross-29-2']</p><p class="source-code">family_indexes = get_family_indexes(samples_hdf5, cross_pd)</p></li>
<li>Finally, we will <a id="_idIndexMarker291"/>actually compute Mendelian errors and save them to disk:<p class="source-code">mother_index = family_indexes['mother']</p><p class="source-code">father_index = family_indexes['father']</p><p class="source-code">offspring_indexes = family_indexes['offspring']</p><p class="source-code">all_errors = {}</p><p class="source-code">def get_mendelian_errors():</p><p class="source-code">    for i in acceptable_position_to_genotype():</p><p class="source-code">        genotype = calldata_genotype[i]</p><p class="source-code">        mother = set(genotype[mother_index])</p><p class="source-code">        father = set(genotype[father_index])</p><p class="source-code">        offspring = [set(genotype[ofs_index]) for ofs_index in offspring_indexes]</p><p class="source-code">        my_mendelian_errors = compute_mendelian_errors(mother, father, offspring)</p><p class="source-code">        yield POS[i], my_mendelian_errors</p><p class="source-code">mendelian_errors = acumulate(get_mendelian_errors)</p><p class="source-code">pickle.dump(mendelian_errors, gzip.open('mendelian_errors.pickle.gz', 'wb'))</p></li>
<li>We will now <a id="_idIndexMarker292"/>generate an efficient NumPy array with annotations and Mendelian error information:<p class="source-code">ordered_positions = sorted(mendelian_errors.keys())</p><p class="source-code">ordered_features = sorted(my_features.keys())</p><p class="source-code">num_features = len(ordered_features)</p><p class="source-code">feature_fit = np.empty((len(ordered_positions), len(my_features) + 2), dtype=float)</p><p class="source-code">for column, feature in enumerate(ordered_features):  # 'Strange' order</p><p class="source-code">    print(feature)</p><p class="source-code">    current_hdf_row = 0</p><p class="source-code">    for row, genomic_position in enumerate(ordered_positions):</p><p class="source-code">        while POS[current_hdf_row] &lt; genomic_position:</p><p class="source-code">            current_hdf_row +=1</p><p class="source-code">        feature_fit[row, column] = my_features[feature][current_hdf_row]</p><p class="source-code">for row, genomic_position in enumerate(ordered_positions):</p><p class="source-code">    feature_fit[row, num_features] = genomic_position</p><p class="source-code">    feature_fit[row, num_features + 1] = 1 if mendelian_errors[genomic_position][0] &gt; 0 else 0</p><p class="source-code">np.save(gzip.open('feature_fit.npy.gz', 'wb'), feature_fit, allow_pickle=False, fix_imports=False)</p><p class="source-code">pickle.dump(ordered_features, open('ordered_features', 'wb'))</p></li>
</ol>
<p>Buried in this code is one of the most important decisions of the whole chapter: how do we weigh Mendelian errors? In our case, we only store a 1 if there is any kind of error, and we store a 0 if there is none. An alternative would be to count the number of errors – as we have up to 20 offspring, that would require some sophisticated statistical analysis that we will not be doing here.</p>
<ol>
<li value="9">Changing <a id="_idIndexMarker293"/>gears, let’s extract some information from chromosome arm 2L now:<p class="source-code">h5_2L = h5py.File('ag1000g.crosses.phase1.ar3sites.2L.h5', 'r')</p><p class="source-code">samples_hdf5 = list(map(lambda sample: sample.decode('utf-8'), h5_2L['/2L/samples']))</p><p class="source-code">calldata_DP = h5_2L['/2L/calldata/DP']</p><p class="source-code">POS = h5_2L['/2L/variants/POS']</p></li>
<li>Here, we are only interested in the parents:<p class="source-code">def get_parent_indexes(samples_hdf5, parents_pd):</p><p class="source-code">    parents = []</p><p class="source-code">    for i, individual in parents_pd.T.iteritems():</p><p class="source-code">        index = samples_hdf5.index(individual.id)</p><p class="source-code">        parents.append(index)</p><p class="source-code">    return parents</p><p class="source-code">parents_pd = samples[samples['function'] == 'parent']</p><p class="source-code">parent_indexes = get_parent_indexes(samples_hdf5, parents_pd)</p></li>
<li>We extract the sample DP for each parent:<p class="source-code">all_dps = []</p><p class="source-code">for i, pos in enumerate(POS):</p><p class="source-code">    if random.random() &gt; 0.01:</p><p class="source-code">        continue</p><p class="source-code">    pos_dp = calldata_DP[i]</p><p class="source-code">    parent_pos_dp = [pos_dp[parent_index] for parent_index in parent_indexes]</p><p class="source-code">    all_dps.append(parent_pos_dp + [pos])</p><p class="source-code">all_dps = np.array(all_dps)</p><p class="source-code">np.save(gzip.open('DP_2L.npy.gz', 'wb'), all_dps, allow_pickle=False, fix_imports=False)</p></li>
</ol>
<p>Now, we have <a id="_idIndexMarker294"/>prepared the dataset for analysis in this chapter.</p>
<h1 id="_idParaDest-110"><a id="_idTextAnchor109"/>Using Mendelian error information for quality control</h1>
<p>So, how <a id="_idIndexMarker295"/>can we infer the quality of calls using Mendelian inheritance rules? Let’s look at expectations for different genotypical configurations of the parents:</p>
<ul>
<li>For a certain potential bi-allelic SNP, if the mother is AA and the father is also AA, then all offspring will be AA.</li>
<li>If the mother is AA and the father TT, then all offspring will have to be heterozygous (AT). They always get an A from the mother, and they always get a T from the father.</li>
<li>If the mother is AA and the father is AT, then the offspring can be either AA or AT. They always get an A from the mother, but they can get either an A or a T from the father.</li>
<li>If both the mother and the father are heterozygous (AT), then the offspring can be anything. In theory, there is not much we can do here.</li>
</ul>
<p>In practice, we can ignore mutations, which is safe to do with most eukaryotes. The number <a id="_idIndexMarker296"/>of mutations (noise, from our perspective) is several orders of magnitude lower than the signal we are looking for.</p>
<p>In this recipe, we are going to do a small theoretical study of the distribution and Mendelian errors, and further process the data for downstream analysis based on errors. The relevant notebook file is <strong class="source-inline">Chapter04/Mendel.py</strong>.</p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor110"/>How to do it…</h2>
<ol>
<li value="1">We will need a few imports:<p class="source-code">import random</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
<li>Before we do any empirical analysis, let’s try to understand what information we can extract from the case where the mother is AA and the father is AT. Let’s answer the question, <em class="italic">If we have 20 offspring, what is the probability of all of them being heterozygous?</em>:<p class="source-code">num_sims = 100000</p><p class="source-code">num_ofs = 20</p><p class="source-code">num_hets_AA_AT = []</p><p class="source-code">for sim in range(num_sims):</p><p class="source-code">    sim_hets = 0</p><p class="source-code">    for ofs in range(20):</p><p class="source-code">        sim_hets += 1 if random.choice([0, 1]) == 1 else 0</p><p class="source-code">    num_hets_AA_AT.append(sim_hets)</p><p class="source-code">    </p><p class="source-code">fig, ax = plt.subplots(1,1, figsize=(16,9))</p><p class="source-code">ax.hist(num_hets_AA_AT, bins=range(20))</p><p class="source-code">print(len([num_hets for num_hets in num_hets_AA_AT if num_hets==20]))</p></li>
</ol>
<p>We <a id="_idIndexMarker297"/>get the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<img alt="Figure 4.1 - Results from 100,000 simulations: the number of offspring that are heterozygous for certain loci where the mother is AA and the father is heterozygous " height="518" src="image/B17942_04_1.jpg" width="943"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 - Results from 100,000 simulations: the number of offspring that are heterozygous for certain loci where the mother is AA and the father is heterozygous</p>
<p>Here, we have done 100,000 simulations. In my case (this is stochastic, so your result might vary), I got exactly zero simulations where all offspring were heterozygous. Indeed, these are permutations with repetition, so the probability of all being heterozygous is <img alt="" height="64" src="image/Formula_04_001.png" width="44"/> or 9.5367431640625e-07 – not very likely. So, even if for a single offspring, we can have AT or AA; for 20, it is very unlikely that all of them are of the same type. This is the information we can use for a less naive interpretation of Mendelian errors.</p>
<ol>
<li value="3">Let’s <a id="_idIndexMarker298"/>repeat the analysis where the mother and the father are both AT:<p class="source-code">num_AAs_AT_AT = []</p><p class="source-code">num_hets_AT_AT = []</p><p class="source-code">for sim in range(num_sims):</p><p class="source-code">    sim_AAs = 0</p><p class="source-code">    sim_hets = 0</p><p class="source-code">    for ofs in range(20):</p><p class="source-code">        derived_cnt = sum(random.choices([0, 1], k=2))</p><p class="source-code">        sim_AAs += 1 if derived_cnt == 0 else 0</p><p class="source-code">        sim_hets += 1 if derived_cnt == 1 else 0</p><p class="source-code">    num_AAs_AT_AT.append(sim_AAs)</p><p class="source-code">    num_hets_AT_AT.append(sim_hets)</p><p class="source-code">fig, ax = plt.subplots(1,1, figsize=(16,9))</p><p class="source-code">ax.hist([num_hets_AT_AT, num_AAs_AT_AT], histtype='step', fill=False, bins=range(20), label=['het', 'AA'])</p><p class="source-code">plt.legend()</p></li>
</ol>
<p>The output is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<img alt="Figure 4.2 - Results from 100,000 simulations: the number of offspring that are AA or heterozygous for a certain locus where both parents are also heterozygous " height="515" src="image/B17942_04_2.jpg" width="940"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 - Results from 100,000 simulations: the number of offspring that are AA or heterozygous for a certain locus where both parents are also heterozygous</p>
<p>In this case, we have also permutations with repetition, but we have four possible values, not two: AA, AT, TA, and TT. We end up with the same <a id="_idIndexMarker299"/>probability for all individuals being AT: 9.5367431640625e-07. It’s even worse (twice as bad, in fact) for all of them being homozygous of the same type (all TT or all AA).</p>
<ol>
<li value="4">OK, after this probabilistic prelude, let’s get down to more data-moving stuff. The first thing that we will do is check how many errors we have. Let’s load the data from the previous recipe:<p class="source-code">import gzip</p><p class="source-code">import pickle</p><p class="source-code">import random</p><p class="source-code">import numpy as np</p><p class="source-code">mendelian_errors = pickle.load(gzip.open('mendelian_errors.pickle.gz', 'rb'))</p><p class="source-code">feature_fit = np.load(gzip.open('feature_fit.npy.gz', 'rb'))</p><p class="source-code">ordered_features = np.load(open('ordered_features', 'rb'))</p><p class="source-code">num_features = len(ordered_features)</p></li>
<li>Let’s see how many errors we have:<p class="source-code">print(len(mendelian_errors), len(list(filter(lambda x: x[0] &gt; 0,mendelian_errors.values()))))</p></li>
</ol>
<p>The output is as follows:</p>
<p class="source-code">(10905732, 541688)</p>
<p>Not many of the calls have Mendelian errors – only around 5%, great.</p>
<ol>
<li value="6">Let’s create <a id="_idIndexMarker300"/>a balanced set where roughly half of the set has errors. For that, we will randomly drop a lot of good calls. First, we compute the fraction of errors:<p class="source-code">total_observations = len(mendelian_errors)</p><p class="source-code">error_observations = len(list(filter(lambda x: x[0] &gt; 0,mendelian_errors.values())))</p><p class="source-code">ok_observations = total_observations - error_observations</p><p class="source-code">fraction_errors = error_observations/total_observations</p><p class="source-code">print (total_observations, ok_observations, error_observations, 100*fraction_errors)</p><p class="source-code">del mendelian_errors</p></li>
<li>We use that information to get a set of accepted entries: all the errors plus an approximately <a id="_idIndexMarker301"/>equal quantity of OK calls. We print the number of entries at the end (this will vary as the OK list is stochastic):<p class="source-code">prob_ok_choice = error_observations / ok_observations</p><p class="source-code">def accept_entry(row):</p><p class="source-code">    if row[-1] == 1:</p><p class="source-code">        return True</p><p class="source-code">    return random.random() &lt;= prob_ok_choice</p><p class="source-code">accept_entry_v = np.vectorize(accept_entry, signature='(i)-&gt;()')</p><p class="source-code">accepted_entries = accept_entry_v(feature_fit)</p><p class="source-code">balanced_fit = feature_fit[accepted_entries]</p><p class="source-code">del feature_fit</p><p class="source-code">balanced_fit.shape</p><p class="source-code">len([x for x in balanced_fit if x[-1] == 1]), len([x for x in balanced_fit if x[-1] == 0])</p></li>
<li>Finally, we save it:<p class="source-code">np.save(gzip.open('balanced_fit.npy.gz', 'wb'), balanced_fit, allow_pickle=False, fix_imports=False)</p></li>
</ol>
<h2 id="_idParaDest-112"><a id="_idTextAnchor111"/>There’s more…</h2>
<p>With regards to Mendelian errors and their impact on cost functions, let’s think about the following case: the mother is AA, the father is AT, and all offspring are AA. Does this mean that the father is wrongly called, or that we failed to detect a few heterozygous offspring? From this reasoning, it’s probably the father that is wrongly called. This has an impact in <a id="_idIndexMarker302"/>terms of some more refined Mendelian error estimation functions: it’s probably more costly to have a few offspring wrong than just a single sample (the father) wrong. In this case, you might think it’s trivial (the probability of having no heterozygous offspring is so low that it’s probably the father), but if you have 18 offspring AA and two AT, is it still “trivial”? This is not just a theoretical problem, because it severely impacts the design of a proper cost function.</p>
<p>Our function in a previous recipe, <em class="italic">Preparing the dataset for analysis</em>, is naive but is enough for the level of refinement that will allow us to have some interesting results further down the road.</p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor112"/>Exploring the data with standard statistics</h1>
<p>Now that <a id="_idIndexMarker303"/>we have the insights for our Mendelian error analysis, let’s explore the data in order to get more insights that might help us to better filter the data. You can find this content in <strong class="source-inline">Chapter04/Exploration.py</strong>.</p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor113"/>How to do it…</h2>
<ol>
<li value="1">We start, as usual, with the necessary imports:<p class="source-code">import gzip</p><p class="source-code">import pickle</p><p class="source-code">import random</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import pandas as pd</p><p class="source-code">from pandas.plotting import scatter_matrix</p></li>
<li>Then <a id="_idIndexMarker304"/>we load the data. We will use pandas to navigate it:<p class="source-code">fit = np.load(gzip.open('balanced_fit.npy.gz', 'rb'))</p><p class="source-code">ordered_features = np.load(open('ordered_features', 'rb'))</p><p class="source-code">num_features = len(ordered_features)</p><p class="source-code">fit_df = pd.DataFrame(fit, columns=ordered_features + ['pos', 'error'])</p><p class="source-code">num_samples = 80</p><p class="source-code">del fit</p></li>
<li>Let’s ask pandas to show a histogram of all annotations:<p class="source-code">fig,ax = plt.subplots(figsize=(16,9))</p><p class="source-code">fit_df.hist(column=ordered_features, ax=ax)</p></li>
</ol>
<p>The following histogram is generated:</p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<img alt="Figure 4.3 - Histogram of all annotations for a dataset with roughly 50% of errors " height="536" src="image/B17942_04_3.jpg" width="962"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 - Histogram of all annotations for a dataset with roughly 50% of errors</p>
<ol>
<li value="4">For some <a id="_idIndexMarker305"/>annotations, we do not get interesting information. We can try to zoom in, for example, with DP:<p class="source-code">fit_df['MeanDP'] = fit_df['DP'] / 80</p><p class="source-code">fig, ax = plt.subplots()</p><p class="source-code">_ = ax.hist(fit_df[fit_df['MeanDP']&lt;50]['MeanDP'], bins=100)</p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer030">
<img alt="Figure 4.4 - Histogram zooming in on an area of interest for DP " height="252" src="image/B17942_04_4.jpg" width="391"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 - Histogram zooming in on an area of interest for DP</p>
<p>We are actually dividing DP by the number of samples in order to get a more meaningful number.</p>
<ol>
<li value="5">We will <a id="_idIndexMarker306"/>split the dataset in two, one for the errors and the other for the positions with no Mendelian errors:<p class="source-code">errors_df = fit_df[fit_df['error'] == 1]</p><p class="source-code">ok_df = fit_df[fit_df['error'] == 0]</p></li>
<li>Let’s have a look at <strong class="source-inline">QUAL</strong> and split it on 0.005, and check how we get errors and correct calls split:<p class="source-code">ok_qual_above_df = ok_df[ok_df['QUAL']&gt;0.005]</p><p class="source-code">errors_qual_above_df = errors_df[errors_df['QUAL']&gt;0.005]</p><p class="source-code">print(ok_df.size, errors_df.size, ok_qual_above_df.size, errors_qual_above_df.size)</p><p class="source-code">print(ok_qual_above_df.size / ok_df.size, errors_qual_above_df.size / errors_df.size)</p></li>
</ol>
<p>The result is as follows:</p>
<p class="source-code">6507972 6500256 484932 6114096</p>
<p class="source-code">0.07451353509203788 0.9405931089483245</p>
<p>Clearly, <strong class="source-inline">['QUAL']&gt;0.005</strong> gets lots of errors, while not getting lots of OK positions. This is positive, as we have some hope for filtering it.</p>
<ol>
<li value="7">Let’s do the same with QD:<p class="source-code">ok_qd_above_df = ok_df[ok_df['QD']&gt;0.05]</p><p class="source-code">errors_qd_above_df = errors_df[errors_df['QD']&gt;0.05]</p><p class="source-code">print(ok_df.size, errors_df.size, ok_qd_above_df.size, errors_qd_above_df.size)</p><p class="source-code">print(ok_qd_above_df.size / ok_df.size, errors_qd_above_df.size / errors_df.size)</p></li>
</ol>
<p>Again, we have <a id="_idIndexMarker307"/>some interesting results:</p>
<p class="source-code">6507972 6500256 460296 5760288</p>
<p class="source-code">0.07072802402960554 0.8861632526472804</p>
<ol>
<li value="8">Let’s take an area where there are fewer errors and study the relationships between annotations on errors. We will plot annotations pairwise:<p class="source-code">not_bad_area_errors_df = errors_df[(errors_df['QUAL']&lt;0.005)&amp;(errors_df['QD']&lt;0.05)]</p><p class="source-code">_ = scatter_matrix(not_bad_area_errors_df[['FS', 'ReadPosRankSum', 'MQ', 'HRun']], diagonal='kde', figsize=(16, 9), alpha=0.02)</p></li>
</ol>
<p>The preceding code generates the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<img alt="Figure 4.5 - Scatter matrix of annotations of errors for an area of the search space " height="545" src="image/B17942_04_5.jpg" width="948"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 - Scatter matrix of annotations of errors for an area of the search space</p>
<ol>
<li value="9">And now do <a id="_idIndexMarker308"/>the same on the good calls:<p class="source-code">not_bad_area_ok_df = ok_df[(ok_df['QUAL']&lt;0.005)&amp;(ok_df['QD']&lt;0.05)]</p><p class="source-code">_ = scatter_matrix(not_bad_area_ok_df[['FS', 'ReadPosRankSum', 'MQ', 'HRun']], diagonal='kde', figsize=(16, 9), alpha=0.02)</p></li>
</ol>
<p>The output is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<img alt="Figure 4.6 - Scatter matrix of annotations of good calls for an area of the search space " height="544" src="image/B17942_04_6.jpg" width="948"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 - Scatter matrix of annotations of good calls for an area of the search space</p>
<ol>
<li value="10">Finally, let’s see <a id="_idIndexMarker309"/>how our rules would perform on the complete dataset (remember that we are using a dataset roughly composed of 50% errors and 50% OK calls):<p class="source-code">all_fit_df = pd.DataFrame(np.load(gzip.open('feature_fit.npy.gz', 'rb')), columns=ordered_features + ['pos', 'error'])</p><p class="source-code">potentially_good_corner_df = all_fit_df[(all_fit_df['QUAL']&lt;0.005)&amp;(all_fit_df['QD']&lt;0.05)]</p><p class="source-code">all_errors_df=all_fit_df[all_fit_df['error'] == 1]</p><p class="source-code">print(len(all_fit_df), len(all_errors_df), len(all_errors_df) / len(all_fit_df))</p></li>
</ol>
<p>We get the following:</p>
<p class="source-code">10905732 541688 0.04967002673456491</p>
<p>Let’s remember that there are roughly 10.9 million markers in our full dataset, with around 5% errors.</p>
<ol>
<li value="11">Let’s get <a id="_idIndexMarker310"/>some statistics on our <strong class="source-inline">good_corner</strong>:<p class="source-code">potentially_good_corner_errors_df = potentially_good_corner_df[potentially_good_corner_df['error'] == 1]</p><p class="source-code">print(len(potentially_good_corner_df), len(potentially_good_corner_errors_df), len(potentially_good_corner_errors_df) / len(potentially_good_corner_df))</p><p class="source-code">print(len(potentially_good_corner_df)/len(all_fit_df))</p></li>
</ol>
<p>The output is as follows:</p>
<p class="source-code">9625754 32180 0.0033431147315836243</p>
<p class="source-code">0.8826325458942141</p>
<p>So, we reduced the error rate to 0.33% (from 5%), while having only reduced to 9.6 million markers.</p>
<h2 id="_idParaDest-115"><a id="_idTextAnchor114"/>There’s more…</h2>
<p>Is a reduction in error from 5% to 0.3% while losing 12% of markers good or bad? Well, it depends on what analysis you want to do next. Maybe your method is resilient to loss of markers but not too many errors, in which case this might help. But if it is the other way around, maybe you prefer to have the complete dataset even if it has more errors. If you apply different methods, maybe you will use different datasets from method to method. In the specific case of this Anopheles dataset, there is so much data that reducing the size will probably be fine for almost anything. But if you have fewer markers, you will have to assess your needs in terms of markers and quality.</p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor115"/>Finding genomic features from sequencing annotations</h1>
<p>We will <a id="_idIndexMarker311"/>conclude this chapter <a id="_idIndexMarker312"/>and this book with a simple recipe that suggests that sometimes you can learn important things from simple unexpected results, and that apparent quality issues might mask important biological questions.</p>
<p>We will plot read depth – <strong class="source-inline">DP</strong> – across chromosome arm 2L for all the parents on our crosses. The recipe can be found in <strong class="source-inline">Chapter04/2L.py</strong>.</p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor116"/>How to do it…</h2>
<p>We’ll get started with the following steps:</p>
<ol>
<li value="1">Let’s start with the usual imports:<p class="source-code">from collections import defaultdict</p><p class="source-code">import gzip</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pylab as plt</p></li>
<li>Let’s load the data that we saved in the first recipe:<p class="source-code">num_parents = 8</p><p class="source-code">dp_2L = np.load(gzip.open('DP_2L.npy.gz', 'rb'))</p><p class="source-code">print(dp_2L.shape)</p></li>
<li>And let’s print the median DP for the whole chromosome arm, and a part of it in the middle for all parents:<p class="source-code">for i in range(num_parents):</p><p class="source-code">    print(np.median(dp_2L[:,i]), np.median(dp_2L[50000:150000,i]))</p></li>
</ol>
<p>The output is as follows:</p>
<p class="source-code">17.0 14.0</p>
<p class="source-code">23.0 22.0</p>
<p class="source-code">31.0 29.0</p>
<p class="source-code">28.0 24.0</p>
<p class="source-code">32.0 27.0</p>
<p class="source-code">31.0 31.0</p>
<p class="source-code">25.0 24.0</p>
<p class="source-code">24.0 20.0</p>
<p>Interestingly, the median for the whole chromosome sometimes does not hold for <a id="_idIndexMarker313"/>that big region in the middle, so let’s dig further.</p>
<ol>
<li value="4">We <a id="_idIndexMarker314"/>will print the median DP for 200,000 kbp windows across the chromosome arm. Let’s start with the window code:<p class="source-code">window_size = 200000</p><p class="source-code">parent_DP_windows = [defaultdict(list) for i in range(num_parents)]</p><p class="source-code">def insert_in_window(row):</p><p class="source-code">    for parent in range(num_parents):</p><p class="source-code">        parent_DP_windows[parent][row[-1] // window_size].append(row[parent])</p><p class="source-code">insert_in_window_v = np.vectorize(insert_in_window, signature='(n)-&gt;()')</p><p class="source-code">_ = insert_in_window_v(dp_2L)</p></li>
<li>Let’s plot it:<p class="source-code">fig, axs = plt.subplots(2, num_parents // 2, figsize=(16, 9), sharex=True, sharey=True, squeeze=True)</p><p class="source-code">for parent in range(num_parents):</p><p class="source-code">    ax = axs[parent // 4][parent % 4]</p><p class="source-code">    parent_data = parent_DP_windows[parent]</p><p class="source-code">    ax.set_ylim(10, 40)</p><p class="source-code">    ax.plot(*zip(*[(win*window_size, np.mean(lst)) for win, lst in parent_data.items()]), '.')</p></li>
<li>The <a id="_idIndexMarker315"/>following plot <a id="_idIndexMarker316"/>shows the output:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer033">
<img alt="Figure 4.7 - Median DP per window for all parents of the dataset on chromosome arm 2L " height="537" src="image/B17942_04_7.jpg" width="930"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 - Median DP per window for all parents of the dataset on chromosome arm 2L</p>
<p>You will notice that for some mosquitoes, for example, the ones on the first and last columns, there <a id="_idIndexMarker317"/>is a clear drop of <a id="_idIndexMarker318"/>DP in the middle of the chromosome arm. In some of them, such as in the third column, there is a bit of drop – not so pronounced. And for the bottom parent of the second column, there is no drop at all.</p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor117"/>There’s more…</h2>
<p>The preceding pattern has a biological cause that ends up having consequences for sequencing: Anopheles mosquitoes might have a big chromosomal inversion in the middle of arm 2L. Karyotypes that are not the same as those on the reference genome used to make the calls are harder to call due to evolutionary divergence. These make the number of sequencer reads in that area lower. This is very specific to this species, but you might expect other kinds of features to appear in other organisms.</p>
<p>A more <a id="_idIndexMarker319"/>widely known case is <strong class="bold">Copy Number Variation</strong> (<strong class="bold">CNV</strong>): if a reference genome has only a copy of a feature, but the individual that you are sequencing has <strong class="source-inline">n</strong>, then you can expect to see a DP of <strong class="source-inline">n</strong> times the median across the genome.</p>
<p>But, in the general case, it is a good idea to be on the lookout for <em class="italic">strange</em> results throughout the analysis. Sometimes, that is the hallmark of an interesting biological feature, as <a id="_idIndexMarker320"/>it is here. Either that, or it’s a pointer to a mistake: for example, <strong class="bold">Principal Components Analysis</strong> (<strong class="bold">PCA</strong>) can be used to find mislabeled samples (as they might cluster in the wrong group).</p>
<h1 id="_idParaDest-119"><a id="_idTextAnchor118"/>Doing metagenomics with QIIME 2 Python API</h1>
<p>Wikipedia says <a id="_idIndexMarker321"/>that metagenomics is the study of genetic material that’s recovered directly from environmental samples. Note that “environment” here should be interpreted broadly: in the case of our example, we will deal with gastrointestinal microbiomes in a study of a fecal microbiome transplant in children with gastrointestinal problems. The study is one of the tutorials of QIIME 2, which is one of the most widely used applications for data analysis in metagenomics. QIIME 2 has several interfaces: a GUI, a command line, and a Python API called the Artifact API.</p>
<p>Tomasz Kościółek has an outstanding tutorial for using the Artifact API based on the most well-developed (client-based, not artifact-based) tutorial on QIIME 2, the <em class="italic">“Moving Pictures” tutorial</em> (<a href="http://nbviewer.jupyter.org/gist/tkosciol/29de5198a4be81559a075756c2490fde">http://nbviewer.jupyter.org/gist/tkosciol/29de5198a4be81559a075756c2490fde</a>). Here, we will create a Python version of the fecal microbiota transplant study that’s available, as with the client interface, at <a href="https://docs.qiime2.org/2022.2/tutorials/fmt/">https://docs.qiime2.org/2022.2/tutorials/fmt/</a>. You should get familiar with it as we won’t go into the details of the biology here. I do follow a more convoluted route than Tomasz: this will allow you to get a bit more acquainted with QIIME 2 Python internals. After you get this experience, you will probably want to follow Tomasz’s route, not mine. However, the experience you get here will make you more comfortable and confident with QIIME’s internals.</p>
<h2 id="_idParaDest-120"><a id="_idTextAnchor119"/>Getting ready</h2>
<p>This recipe is <a id="_idIndexMarker322"/>slightly more complicated to set up. We will have to create a <strong class="source-inline">conda</strong> environment where packages from QIIME 2 are segregated from packages from all other applications. The steps that you need to follow are simple.</p>
<p>On OS X, use the following code to create a new <strong class="source-inline">conda</strong> environment:</p>
<p class="source-code">wget wget https://data.qiime2.org/distro/core/qiime2-2022.2-py38-osx-conda.yml</p>
<p class="source-code">conda env create -n qiime2-2022.2 --file qiime2-2022.2-py38-osx-conda.yml</p>
<p>On Linux, use the following code to create the environment:</p>
<p class="source-code">wget wget https://data.qiime2.org/distro/core/qiime2-2022.2-py38-linux-conda.yml</p>
<p class="source-code">conda env create -n qiime2-2022.2 --file qiime2-2022.2-py38-linux-conda.yml</p>
<p>If these instructions do not work, check the QIIME 2 website for an updated version (<a href="https://docs.qiime2.org/2022.2/install/native">https://docs.qiime2.org/2022.2/install/native</a>). QIIME 2 is updated regularly.</p>
<p>At this stage, you need to enter the QIIME 2 <strong class="source-inline">conda</strong> environment by using <strong class="source-inline">source activate qiime2-2022.2</strong>. If you want to get to the standard <strong class="source-inline">conda</strong> environment, use <strong class="source-inline">source deactivate</strong> instead. We will want to install <strong class="source-inline">jupyter lab</strong> and <strong class="source-inline">jupytext</strong>:</p>
<p class="source-code">conda install jupyterlab jupytext</p>
<p>You might want to install other packages you want inside QIIME 2’s environment using <strong class="source-inline">conda install</strong>.</p>
<p>To prepare <a id="_idIndexMarker323"/>for Jupyter execution, you should install the QIIME 2 extension, as follows:</p>
<pre class="source-code">
jupyter serverextension enable --py qiime2 --sys-prefix</pre>
<p class="callout-heading">TIP</p>
<p class="callout">The extension is highly interactive and allows you to look at data from different viewpoints that cannot be captured in this book. The downside is that it won’t work in <strong class="source-inline">nbviewer</strong> (some cell outputs won’t be visible with the static viewer). Remember to interact with the outputs from the extension, since many are dynamic.</p>
<p>You can now start Jupyter. The notebook can be found in the <strong class="source-inline">Chapter4/QIIME2_Metagenomics.py</strong> file.</p>
<p class="callout-heading">WARNING</p>
<p class="callout">Due to the fluidity of package installation with QIIME, we don’t provide a Docker environment for it. This means that if you are working from our Docker installation you will have to download the recipe and install the packages manually.</p>
<p>You can find the instructions to get the data of both the Notebook files and the QIIME 2 tutorial.</p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor120"/>How to do it...</h2>
<p>Let’s take a look at the following steps:</p>
<ol>
<li value="1">Let’s start by checking what plugins are available:<p class="source-code">import pandas as pd</p><p class="source-code">from qiime2.metadata.metadata import Metadata</p><p class="source-code">from qiime2.metadata.metadata import CategoricalMetadataColumn</p><p class="source-code">from qiime2.sdk import Artifact</p><p class="source-code">from qiime2.sdk import PluginManager</p><p class="source-code">from qiime2.sdk import Result</p><p class="source-code">pm = PluginManager()</p><p class="source-code">demux_plugin = pm.plugins['demux']</p><p class="source-code">#demux_emp_single = demux_plugin.actions['emp_single']</p><p class="source-code">demux_summarize = demux_plugin.actions['summarize']</p><p class="source-code">print(pm.plugins)</p></li>
</ol>
<p>We are <a id="_idIndexMarker324"/>also accessing the demultiplexing plugin and its summarize action.</p>
<ol>
<li value="2">Let’s take a peek at the summarize action, namely <strong class="source-inline">inputs</strong>, <strong class="source-inline">outputs</strong>, and <strong class="source-inline">parameters</strong>:<p class="source-code">print(demux_summarize.description)</p><p class="source-code">demux_summarize_signature = demux_summarize.signature</p><p class="source-code">print(demux_summarize_signature.inputs)</p><p class="source-code">print(demux_summarize_signature.parameters)</p><p class="source-code">print(demux_summarize_signature.outputs)</p></li>
</ol>
<p>The output will be as follows:</p>
<p class="source-code">Summarize counts per sample for all samples, and generate interactive positional quality plots based on `n` randomly selected sequences.</p>
<p class="source-code"> OrderedDict([('data', ParameterSpec(qiime_type=SampleData[JoinedSequencesWithQuality | PairedEndSequencesWithQuality | SequencesWithQuality], view_type=&lt;class 'q2_demux._summarize._visualizer._PlotQualView'&gt;, default=NOVALUE, description='The demultiplexed sequences to be summarized.'))])</p>
<p class="source-code"> OrderedDict([('n', ParameterSpec(qiime_type=Int, view_type=&lt;class 'int'&gt;, default=10000, description='The number of sequences that should be selected at random for quality score plots. The quality plots will present the average positional qualities across all of the sequences selected. If input sequences are paired end, plots will be generated for both forward and reverse reads for the same `n` sequences.'))])</p>
<p class="source-code"> OrderedDict([('visualization', ParameterSpec(qiime_type=Visualization, view_type=None, default=NOVALUE, description=NOVALUE))])</p>
<ol>
<li value="3">We will <a id="_idIndexMarker325"/>now load the first dataset, demultiplex it, and visualize some demultiplexing statistics:<p class="source-code">seqs1 = Result.load('fmt-tutorial-demux-1-10p.qza')</p><p class="source-code">sum_data1 = demux_summarize(seqs1)</p><p class="source-code">sum_data1.visualization</p></li>
</ol>
<p>Here is a part of the output from the QIIME extension for Juypter:</p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<img alt="Figure 4.8 - A part of the output of the QIIME2 extension for Jupyter " height="518" src="image/B17942_04_8.jpg" width="620"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 - A part of the output of the QIIME2 extension for Jupyter</p>
<p>Remember <a id="_idIndexMarker326"/>that the extension is iterative and provides substantially more information than only this chart.</p>
<p class="callout-heading">TIP</p>
<p class="callout">The original data for this recipe is supplied in QIIME 2 format. Obviously, you will have your own original data in some other format (probably FASTQ) – see the <em class="italic">There’s more...</em> section for a way to load a standard format.</p>
<p class="callout">QIIME 2’s <strong class="source-inline">.qza</strong> and <strong class="source-inline">.qzv</strong> formats are simply zipped files. You can have a look at the content with <strong class="source-inline">unzip</strong>.</p>
<p>The chart will be similar to in the QIIME CLI tutorial, but be sure to check the interactive quality plot of our output.</p>
<ol>
<li value="4">Let’s do the same for the second dataset:<p class="source-code">seqs2 = Result.load('fmt-tutorial-demux-2-10p.qza')</p><p class="source-code">sum_data2 = demux_summarize(seqs2)</p><p class="source-code">sum_data2.visualization</p></li>
<li>Let’s <a id="_idIndexMarker327"/>use the DADA2 (<a href="https://github.com/benjjneb/dada2">https://github.com/benjjneb/dada2</a>) plugin <a id="_idIndexMarker328"/>for quality control:<p class="source-code">dada2_plugin = pm.plugins['dada2']</p><p class="source-code">dada2_denoise_single = dada2_plugin.actions['denoise_single']</p><p class="source-code">qual_control1 = dada2_denoise_single(demultiplexed_seqs=seqs1,</p><p class="source-code">                                    trunc_len=150, trim_left=13)</p><p class="source-code">qual_control2 = dada2_denoise_single(demultiplexed_seqs=seqs2,</p><p class="source-code">                                    trunc_len=150, trim_left=13)</p></li>
<li>Let’s extract some statistics from denoising (first set):<p class="source-code">metadata_plugin = pm.plugins['metadata']</p><p class="source-code">metadata_tabulate = metadata_plugin.actions['tabulate']</p><p class="source-code">stats_meta1 = metadata_tabulate(input=qual_control1.denoising_stats.view(Metadata))</p><p class="source-code">stats_meta1.visualization</p></li>
</ol>
<p>Again, the result can be found online in the QIIME 2 CLI version of the tutorial.</p>
<ol>
<li value="7">Now, let’s do the same for the second set:<p class="source-code">stats_meta2 = metadata_tabulate(input=qual_control2.denoising_stats.view(Metadata))</p><p class="source-code">stats_meta2.visualization</p></li>
<li>Now, merge <a id="_idIndexMarker329"/>the denoised data:<p class="source-code">ft_plugin = pm.plugins['feature-table']</p><p class="source-code">ft_merge = ft_plugin.actions['merge']</p><p class="source-code">ft_merge_seqs = ft_plugin.actions['merge_seqs']</p><p class="source-code">ft_summarize = ft_plugin.actions['summarize']</p><p class="source-code">ft_tab_seqs = ft_plugin.actions['tabulate_seqs']</p><p class="source-code">table_merge = ft_merge(tables=[qual_control1.table, qual_control2.table])</p><p class="source-code">seqs_merge = ft_merge_seqs(data=[qual_control1.representative_sequences, qual_control2.representative_sequences])</p></li>
<li>Then, gather some quality statistics from the merge:<p class="source-code">ft_sum = ft_summarize(table=table_merge.merged_table)</p><p class="source-code">ft_sum.visualization</p></li>
<li>Finally, let’s get some information about the merged sequences:<p class="source-code">tab_seqs = ft_tab_seqs(data=seqs_merge.merged_data)</p><p class="source-code">tab_seqs.visualization</p></li>
</ol>
<h2 id="_idParaDest-122"><a id="_idTextAnchor121"/>There’s more...</h2>
<p>The preceding code does not show you how to import data. The actual code will vary from case <a id="_idIndexMarker330"/>to case (single-end data, paired-end data, or already-demultiplexed data), but for the main QIIME 2 tutorial, <em class="italic">Moving Pictures</em>, assuming that you have downloaded the single-end, non-demultiplexed data and barcodes into a directory called <strong class="source-inline">data</strong>, you can do the following:</p>
<pre class="source-code">
data_type = 'EMPSingleEndSequences'
conv = Artifact.import_data(data_type, 'data')
conv.save('out.qza')</pre>
<p>As stated in the preceding code, if you look on GitHub for this notebook, the static <strong class="source-inline">nbviewer</strong> system will not be able to render the notebook correctly (you have to run it yourself). This is far from perfect; it is not interactive, since the quality is not great, but at least it lets you get an idea of the output without running the code.</p>
</div>
<div>
<div id="_idContainer036">
</div>
</div>
</div></body></html>