["```py\nln -sf /Applications/Docker.app/Contents/Resources/bin/docker-credential-ecr-login /usr/local/bin/docker-credential-ecr-login\n```", "```py\n% netstat -an | grep LISTEN\n```", "```py\n         ports:\n          - '7077:7077'\n          - '8080:8080'\n    ```", "```py\n         ports:\n          - '7077:7077'\n          - '8070:8080'\n    ```", "```py\n    git clone https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git\n    cd Time-Series-Analysis-with-Spark/ch3\n    ```", "```py\n    xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\n    ```", "```py\n    xcode-select --install\n    ```", "```py\n    make up\n    ```", "```py\ndocker-compose up -d\n[+] Running 4/4\n...\n ✔ Container ts-spark-env-spark-master-1    Started\n ✔ Container ts-spark-env-jupyter-1         Started\n ✔ Container ts-spark-env-spark-worker-1-1  Started\n ✔ Container ts-spark-env-ts-spark-env-spark-master-1), which is where the cluster manager runs, and two worker nodes (ts-spark-env-spark-worker-1-1 and ts-spark-env-spark-worker-2-1). In addition, there is a separate node (ts-spark-env-jupyter-1) for a notebook environment, called Jupyter Notebook, similar to what you have used in the previous chapters on Databricks Community Edition. In this deployment, this Jupyter node is also the driver node.\nLet’s now validate the environment that we have just deployed.\nAccessing the UIs\nWe will now access the UIs of the different components as a quick way to validate the deployment:\n\n1.  We start with Jupyter Notebook at the following local URL: [http://localhost:8888/lab](http://localhost:8888/lab)\n\nNote\nYou will need to change the network port in the preceding URL if you need to modify it as discussed in the *Network* *ports* section.\nThis will open the web page as per *Figure 3**.5*.\n![](img/B18568_03_5.jpg)\n\nFigure 3.5: Jupyter Notebook\n\n1.  The next (and important) UI is for the Apache Spark master node, accessible via the following local URL: [http://localhost:8080/](http://localhost:8080/)\n\n    *Figure 3**.6* shows this master node UI, as well as the worker nodes connected.\n\n![](img/B18568_03_6.jpg)\n\nFigure 3.6: Spark master node UI\nWe now have our own Apache Spark cluster running.\nAs a final step to conclude this chapter, you can stop the containers with the following command:\n\n```", "```py\n\n If you do not intend to use it further, you can additionally delete the Docker containers created with the Delete action as explained here: [https://docs.docker.com/desktop/use-desktop/container/#container-actions](https://docs.docker.com/desktop/use-desktop/container/#container-actions)\nSummary\nIn this chapter, we dove deep into the Apache Spark architecture, its key components, and its features. The key concepts, how it works, and what makes it such a great tool were explained. We then deployed a multi-node cluster representing an example architecture. The concepts presented in this chapter, while essential, cover only a part of an Apache Spark project. We will view such a project end to end in the next chapter.\nFurther reading\nThis section serves as a repository of sources that can help you build on your understanding of the topic:\n\n*   Apache Spark official web page: [https://spark.apache.org/](https://spark.apache.org/)\n*   *Mastering Apache Spark* (Packt Publishing) by Timothy Chen, Mike Frampton, and Tim Seear\n*   *Azure Databricks Cookbook* (Packt Publishing) by Phani Raj and Vinod Jaiswal\n*   Google Trends comparison: [https://trends.google.com/trends/explore?date=2009-01-01%202024-08-28&q=%2Fm%2F0bs2j8q,%2Fm%2F0ndhxqz,%2Fm%2F0fdjtq&hl=en](https://trends.google.com/trends/explore?date=2009-01-01%202024-08-28&q=%2Fm%2F0bs2j8q,%2Fm%2F0ndhxqz,%2Fm%2F0fdjtq&hl=en)\n*   Cluster Overview: [https://spark.apache.org/docs/latest/cluster-overview.html](https://spark.apache.org/docs/latest/cluster-overview.html)\n*   Spark Connect: [https://spark.apache.org/docs/latest/spark-connect-overview.html](https://spark.apache.org/docs/latest/spark-connect-overview.html)\n*   Docker Compose: [https://docs.docker.com/compose/](https://docs.docker.com/compose/)\n*   Make and Makefile: [https://www.gnu.org/software/make/manual/make.html](https://www.gnu.org/software/make/manual/make.html)\n*   Jupyter: [https://jupyter.org/](https://jupyter.org/)\n\nJoin our community on Discord\nJoin our community’s Discord space for discussions with the authors and other readers:\n[https://packt.link/ds](https://packt.link/ds)\n![](img/ds_(1).jpg)\n\n```"]