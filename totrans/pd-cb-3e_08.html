<html><head></head><body>
  <div id="_idContainer110" class="Basic-Text-Frame">
    <h1 class="chapterNumber">8</h1>
    <h1 id="_idParaDest-224" class="chapterTitle">Group By</h1>
    <p class="normal">One of the most fundamental tasks during data analysis involves splitting data into independent groups before performing a calculation on each group. This methodology has been around for quite some time, but has more recently been referred to as <em class="italic">split-apply-combine</em>.</p>
    <p class="normal">Within the <em class="italic">apply</em> step of the <em class="italic">split-apply-combine</em> paradigm, it is additionally helpful to know whether we are trying to perform a <em class="italic">reduction</em> (also referred to as an aggregation) or a <em class="italic">transformation</em>. The former reduces the values in a group down to <em class="italic">one value</em> whereas the latter attempts to maintain the shape of the group.</p>
    <p class="normal">To illustrate, here is what <em class="italic">split-apply-combine</em> looks like for a reduction: </p>
    <figure class="mediaobject"><img src="../Images/B31091_08_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.1: Split-apply-combine paradigm for a reduction</p>
    <p class="normal">Here is the same paradigm for a <em class="italic">transformation</em>:</p>
    <figure class="mediaobject"><img src="../Images/B31091_08_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.2: Split-apply-combine paradigm for a transformation </p>
    <p class="normal">In pandas, the <code class="inlineCode">pd.DataFrame.groupby</code> method is responsible for splitting, applying a function of your choice, and combining the results back together for you as an end user.</p>
    <p class="normal">We are going to cover the following recipes in this chapter:</p>
    <ul>
      <li class="bulletList">Group by basics</li>
      <li class="bulletList">Grouping and calculating multiple columns</li>
      <li class="bulletList">Group by apply</li>
      <li class="bulletList">Window operations</li>
      <li class="bulletList">Selecting the highest rated movies by year</li>
      <li class="bulletList">Comparing the best hitter in baseball across years</li>
    </ul>
    <h1 id="_idParaDest-225" class="heading-1">Group by basics</h1>
    <p class="normal">True mastery of the<a id="_idIndexMarker426"/> pandas group by mechanisms is a powerful skill for any data analyst. With pandas, you can easily summarize data, find patterns within different groups, and compare groups to one another. The number of algorithms you can apply alongside a group by are endless in theory, giving you as an analyst tons of flexibility to explore your data.</p>
    <p class="normal">In this first recipe, we are going to start with a very simple summation against different groups in an intentionally small dataset. While this example is overly simplistic, a solid theoretical understanding of how group by works is important as you look toward real-world applications.</p>
    <h2 id="_idParaDest-226" class="heading-2">How to do it</h2>
    <p class="normal">To get familiarized with how group by works in code, let’s create some sample data that matches our starting point in <em class="italic">Figures 8.1</em> and <em class="italic">8.2</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.DataFrame([
    ["group_a", <span class="hljs-number">0</span>],
    ["group_a", <span class="hljs-number">2</span>],
    ["group_b", <span class="hljs-number">1</span>],
    ["group_b", <span class="hljs-number">3</span>],
    ["group_b", <span class="hljs-number">5</span>],
], columns=["group", "value"])
df = df.convert_dtypes(dtype_backend="numpy_nullable")
df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">     group      value
0    group_a    0
1    group_a    2
2    group_b    1
3    group_b    3
4    group_b    5
</code></code></pre>
    <p class="normal">Our <code class="inlineCode">pd.DataFrame</code> has two distinct groups: <code class="inlineCode">group_a</code> and <code class="inlineCode">group_b</code>. As you can see, the <code class="inlineCode">group_a</code> rows are<a id="_idIndexMarker427"/> associated with <code class="inlineCode">value</code> data of <code class="inlineCode">0</code> and <code class="inlineCode">2</code>, whereas the <code class="inlineCode">group_b</code> rows are associated with <code class="inlineCode">value</code> data of <code class="inlineCode">1</code>, <code class="inlineCode">3</code>, and <code class="inlineCode">5</code>. Summing the values within each <code class="inlineCode">group</code> should therefore yield a result of <code class="inlineCode">2</code> and <code class="inlineCode">9</code>, respectively.</p>
    <p class="normal">To express this with pandas, you are going to use the <code class="inlineCode">pd.DataFrame.groupby</code> method, which accepts as an argument the group name(s). In our case, this is the <code class="inlineCode">group</code> column. This technically returns a <code class="inlineCode">pd.core.groupby.DataFrameGroupBy</code> object that exposes a <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.sum</code> method for summation:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group      value
group_a    2
group_b    9
</code></code></pre>
    <p class="normal">Don’t worry if you find the method name <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.sum</code> to be verbose; it is, but you will never have to write it out by hand. We are going to refer to it here by its technical name for the sake of completeness, but as an end user, you will always follow the form you can see here:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby(&lt;GROUP_OR_GROUPS&gt;)
</code></code></pre>
    <p class="normal">This is what you will follow to get your <code class="inlineCode">pd.core.groupby.DataFrameGroupBy</code> object.</p>
    <p class="normal">By default, <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.sum</code> is considered an <em class="italic">aggregation</em>, so each group is <em class="italic">reduced</em> down to a single row during the <em class="italic">apply</em> phase of <em class="italic">split-apply-combine</em>, much like we see in <em class="italic">Figure 8.1</em>.</p>
    <p class="normal">Instead of calling <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.sum</code> directly, we could have alternatively used the <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.agg</code> method, providing it with the argument of <code class="inlineCode">"sum"</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group").agg("<span class="hljs-built_in">sum</span>")
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group    value
group_a  2
group_b  9
</code></code></pre>
    <p class="normal">The explicitness <a id="_idIndexMarker428"/>of <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.agg</code> is useful when compared side by side with the <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.transform</code> method, which will perform a <em class="italic">transformation</em> (see <em class="italic">Figure 8.2</em> again) instead of a <em class="italic">reduction</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group").transform("<span class="hljs-built_in">sum</span>")
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    value
0       2
1       2
2       9
3       9
4       9
</code></code></pre>
    <p class="normal"><code class="inlineCode">pd.core.groupby.DataFrameGroupBy.transform</code> guarantees to return a like-indexed object to the caller, which makes it ideal for performing calculations like <code class="inlineCode">% of group</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df[["value"]].div(df.groupby("group").transform("<span class="hljs-built_in">sum</span>"))
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">     value
0    0.000000
1    1.000000
2    0.111111
3    0.333333
4    0.555556
</code></code></pre>
    <p class="normal">When applying a reduction algorithm, <code class="inlineCode">pd.DataFrame.groupby</code> will take the unique values of the group(s) and use them to form a new row <code class="inlineCode">pd.Index</code> (or <code class="inlineCode">pd.MultiIndex</code>, in the case of multiple groups). If you would prefer not to have the grouped labels create a new index, keeping them as columns instead, you can pass <code class="inlineCode">as_index=False</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group", as_index=<span class="hljs-literal">False</span>).<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">   group    value
0  group_a      2
1  group_b      9
</code></code></pre>
    <p class="normal">You should also note that the name of any non-grouping columns will not be altered when performing a group by operation. For example, even though we start with a <code class="inlineCode">pd.DataFrame</code> containing <a id="_idIndexMarker429"/>a column named <em class="italic">value</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">   group    value
0  group_a      0
1  group_a      2
2  group_b      1
3  group_b      3
4  group_b      5
</code></code></pre>
    <p class="normal">The fact that we then group by the <code class="inlineCode">group</code> column and sum the <code class="inlineCode">value</code> column does not change its name in the result; it is still just <code class="inlineCode">value</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group      value
group_a    2
group_b    9
</code></code></pre>
    <p class="normal">This can be confusing or ambiguous if you apply other algorithms to your groups, like <code class="inlineCode">min</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group").<span class="hljs-built_in">min</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group      value
group_a    0
group_b    1
</code></code></pre>
    <p class="normal">Our column is still just called <code class="inlineCode">value</code>, even though in one instance, we are taking the <em class="italic">sum of value</em> and in the other instance, we are taking the <em class="italic">min of value</em>.</p>
    <p class="normal">Fortunately, there is a way to control this by using the <code class="inlineCode">pd.NamedAgg</code> class. When calling <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.agg</code>, you can provide keyword arguments where each argument key dictates the desired column name and the argument value is a <code class="inlineCode">pd.NamedAgg</code>, which dictates the aggregation as well as the original column it is applied to.</p>
    <p class="normal">For instance, if we wanted to apply a <code class="inlineCode">sum</code> aggregation to our <code class="inlineCode">value</code> column, and have the result shown as <code class="inlineCode">sum_of_value</code>, we could write the following:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group").agg(sum_of_value=pd.NamedAgg(column="value", aggfunc="<span class="hljs-built_in">sum</span>"))
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group           sum_of_value
group_a         2
group_b         9
</code></code></pre>
    <h2 id="_idParaDest-227" class="heading-2">There’s more…</h2>
    <p class="normal">Although this recipe<a id="_idIndexMarker430"/> focused mainly on summation, pandas offers quite a few other built-in <em class="italic">reduction</em> algorithms that can be applied to a <code class="inlineCode">pd.core.groupby.DataFrameGroupBy</code> object, such as the following:</p>
    <table id="table001-2" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">any</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">all</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">sum</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">prod</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">idxmin</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">idxmax</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">min</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">max</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">mean</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">median</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">var</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">std</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">sem</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">skew</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">first</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">last</code></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 8.1: Commonly used GroupBy reduction algorithms</p>
    <p class="normal">Likewise, there are some built-in <em class="italic">transformation</em> functions that you can use:</p>
    <table id="table002-1" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal">cumprod</p>
          </td>
          <td class="table-cell">
            <p class="normal">cumsum</p>
          </td>
          <td class="table-cell">
            <p class="normal">cummin</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">cummax</p>
          </td>
          <td class="table-cell">
            <p class="normal">rank</p>
          </td>
          <td class="table-cell"/>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 8.2: Commonly used GroupBy transformation algorithms</p>
    <p class="normal">Functionally, there is no difference between calling these functions directly as methods of <code class="inlineCode">pd.core.groupby.DataFrameGroupBy</code> versus providing them as an argument to <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.agg</code> or <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.transform</code>. You will get the same performance and result by doing the following:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group").<span class="hljs-built_in">max</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group      value
group_a    2
group_b    5
</code></code></pre>
    <p class="normal">The preceding code<a id="_idIndexMarker431"/> snippet will yield the same results as this one:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group").agg("<span class="hljs-built_in">max</span>")
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group      value
group_a    2
group_b    5
</code></code></pre>
    <p class="normal">You could argue that the latter approach signals a clearer intent, especially considering that <code class="inlineCode">max</code> can be used as a transformation just as well as an aggregation:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group").transform("<span class="hljs-built_in">max</span>")
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    value
0       2
1       2
2       5
3       5
4       5
</code></code></pre>
    <p class="normal">In practice, both styles are commonplace, so you should be familiar with the different approaches.</p>
    <h1 id="_idParaDest-228" class="heading-1">Grouping and calculating multiple columns</h1>
    <p class="normal">Now that we have<a id="_idIndexMarker432"/> the basics down, let’s <a id="_idIndexMarker433"/>take a look at a <code class="inlineCode">pd.DataFrame</code> that contains more columns of data. Generally, your <code class="inlineCode">pd.DataFrame</code> objects will contain many columns with potentially different data types, so knowing how to select and work with them all through the context of <code class="inlineCode">pd.core.groupby.DataFrameGroupBy</code> is important.</p>
    <h2 id="_idParaDest-229" class="heading-2">How to do it</h2>
    <p class="normal">Let’s create a <code class="inlineCode">pd.DataFrame</code> that shows the <code class="inlineCode">sales</code> and <code class="inlineCode">returns</code> of a hypothetical <code class="inlineCode">widget</code> across different <code class="inlineCode">region</code> and <code class="inlineCode">month</code> values:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.DataFrame([
    ["North", "Widget A", "Jan", <span class="hljs-number">10</span>, <span class="hljs-number">2</span>],
    ["North", "Widget B", "Jan", <span class="hljs-number">4</span>, <span class="hljs-number">0</span>],
    ["South", "Widget A", "Jan", <span class="hljs-number">8</span>, <span class="hljs-number">3</span>],
    ["South", "Widget B", "Jan", <span class="hljs-number">12</span>, <span class="hljs-number">8</span>],
    ["North", "Widget A", "Feb", <span class="hljs-number">3</span>, <span class="hljs-number">0</span>],
    ["North", "Widget B", "Feb", <span class="hljs-number">7</span>, <span class="hljs-number">0</span>],
    ["South", "Widget A", "Feb", <span class="hljs-number">11</span>, <span class="hljs-number">2</span>],
    ["South", "Widget B", "Feb", <span class="hljs-number">13</span>, <span class="hljs-number">4</span>],
], columns=["region", "widget", "month", "sales", "returns"])
df = df.convert_dtypes(dtype_backend="numpy_nullable")
df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">     region     widget     month   sales     returns
0    North      Widget A   Jan     10        2
1    North      Widget B   Jan      4        0
2    South      Widget A   Jan      8        3
3    South      Widget B   Jan     12        8
4    North      Widget A   Feb      3        0
5    North      Widget B   Feb      7        0
6    South      Widget A   Feb     11        2
7    South      Widget B   Feb     13        4
</code></code></pre>
    <p class="normal">To calculate the total <code class="inlineCode">sales</code> and <code class="inlineCode">returns</code> for each <code class="inlineCode">widget</code>, your first attempt at doing so may look like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("widget").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">widget    region                month         sales  returns
Widget A  NorthSouthNorthSouth  JanJanFebFeb     32        7
Widget B  NorthSouthNorthSouth  JanJanFebFeb     36       12
</code></code></pre>
    <p class="normal">While <code class="inlineCode">sales</code> and <code class="inlineCode">returns</code> look good, the <code class="inlineCode">region</code> and <code class="inlineCode">month</code> columns also ended up being summed, using the same summation logic that Python would when working with strings:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">"North" + "South" + "North" + "South"
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">NorthSouthNorthSouth
</code></code></pre>
    <p class="normal">Unfortunately, this default behavior is usually undesirable. I personally find it rare to ever want strings to be concatenated like this, and when dealing with large <code class="inlineCode">pd.DataFrame</code> objects, it can be prohibitively expensive to do so.</p>
    <p class="normal">One way to avoid this<a id="_idIndexMarker434"/> issue is to be more<a id="_idIndexMarker435"/> explicit about the columns you would like to aggregate by selecting them after the <code class="inlineCode">df.groupby("widget")</code> call:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("widget")[["sales", "returns"]].agg("<span class="hljs-built_in">sum</span>")
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">widget        sales    returns
Widget A      32        7
Widget B      36       12
</code></code></pre>
    <p class="normal">Alternatively, you could reach for the <code class="inlineCode">pd.NamedAgg</code> class we introduced back in the <em class="italic">Group by basics</em> recipe. Though more verbose, the use of <code class="inlineCode">pd.NamedAgg</code> gives you the benefit of being able to rename the columns you would like to see in the output (i.e., instead of <code class="inlineCode">sales</code>, you may want to see <code class="inlineCode">sales_total</code>):</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("widget").agg(
    sales_total=pd.NamedAgg(column="sales", aggfunc="<span class="hljs-built_in">sum</span>"),
    returns_total=pd.NamedAgg(column="returns", aggfunc="<span class="hljs-built_in">sum</span>"),
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">widget            sales_total     returns_total
Widget A          32               7
Widget B          36              12
</code></code></pre>
    <p class="normal">Another feature of <code class="inlineCode">pd.core.groupby.DataFrameGroupBy</code> worth reviewing here is its ability to deal with multiple <code class="inlineCode">group</code> arguments. By providing a list, you can expand your grouping to cover both <code class="inlineCode">widget</code> and <code class="inlineCode">region</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby(["widget", "region"]).agg(
    sales_total=pd.NamedAgg("sales", "<span class="hljs-built_in">sum</span>"),
    returns_total=pd.NamedAgg("returns", "<span class="hljs-built_in">sum</span>"),
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">widget      region          sales_total     returns_total
Widget A    North           13               2
            South           19               5
Widget B    North           11               0
            South           25              12
</code></code></pre>
    <p class="normal">With <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.agg</code>, there is no limitation on how many functions can be applied. For instance, if you want to see the <code class="inlineCode">sum</code>, <code class="inlineCode">min</code>, and <code class="inlineCode">mean</code> of <code class="inlineCode">sales</code> and <code class="inlineCode">returns</code> within <a id="_idIndexMarker436"/>each <code class="inlineCode">widget</code> and <code class="inlineCode">region</code>, you <a id="_idIndexMarker437"/>could simply write the following:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby(["widget", "region"]).agg(
    sales_total=pd.NamedAgg("sales", "<span class="hljs-built_in">sum</span>"),
    returns_total=pd.NamedAgg("returns", "<span class="hljs-built_in">sum</span>"),
    sales_min=pd.NamedAgg("sales", "<span class="hljs-built_in">min</span>"),
    returns_min=pd.NamedAgg("returns", "<span class="hljs-built_in">min</span>"),
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">                    sales_total   returns_total   sales_min   returns_min
widget     region 
Widget A   North            13                2           3             0
           South            19                5           8             2
Widget B   North            11                0           4             0
           South            25               12          12             4
</code></code></pre>
    <h2 id="_idParaDest-230" class="heading-2">There’s more…</h2>
    <p class="normal">While the built-in reduction functions and transformation functions that work out of the box with a group by are useful, there may still be times when you need to roll with your own custom function. This can be particularly useful when you find an algorithm to be <code class="inlineCode">good enough</code> for what you are attempting in your local analysis, but when it may be difficult to generalize to all use cases.</p>
    <p class="normal">A commonly requested function in pandas that is not provided out of the box with a group by is <code class="inlineCode">mode</code>, even though there is a <code class="inlineCode">pd.Series.mode</code> method. With <code class="inlineCode">pd.Series.mode</code>, the type<a id="_idIndexMarker438"/> returned is <a id="_idIndexMarker439"/>always a <code class="inlineCode">pd.Series</code>, regardless of whether there is only one value that appears most frequently:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.Series([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]).mode()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0    1
dtype: int64
</code></code></pre>
    <p class="normal">This is true even if there are two or more elements that appear most frequently:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.Series([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]).mode()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0    1
1    2
dtype: int64
</code></code></pre>
    <p class="normal">Given that there is a <code class="inlineCode">pd.Series.mode</code>, why does pandas not offer a similar function when doing a group by? From a pandas developer perspective, the reason is simple; there is no single way to interpret what a group by should return.</p>
    <p class="normal">Let’s think through this in more detail with the following example, where <code class="inlineCode">group_a</code> contains two values that appear with the same frequency (42 and 555), whereas <code class="inlineCode">group_b</code> only contains the value 0:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.DataFrame([
    ["group_a", <span class="hljs-number">42</span>],
    ["group_a", <span class="hljs-number">555</span>],
    ["group_a", <span class="hljs-number">42</span>],
    ["group_a", <span class="hljs-number">555</span>],
    ["group_b", <span class="hljs-number">0</span>],
], columns=["group", "value"])
df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">  group    value
0 group_a     42
1 group_a    555
2 group_a     42
3 group_a    555
4 group_b      0
</code></code></pre>
    <p class="normal">The question we need to answer is <em class="italic">what should the mode return for group_a?</em> One possible solution would be to return a list (or any Python sequence) that holds both 42 and 555. The downside to this approach is that your returned dtype would be <code class="inlineCode">object</code>, the pitfalls of which we covered back in <em class="chapterRef">Chapter 3</em>, <em class="italic">Data Types</em>.</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.Series([[<span class="hljs-number">42</span>, <span class="hljs-number">555</span>], <span class="hljs-number">0</span>], index=pd.Index(["group_a", "group_b"], name="group"))
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group
group_a    [42, 555]
group_b            0
dtype: object
</code></code></pre>
    <p class="normal">A second expectation would be for pandas to just <em class="italic">choose one</em> of the values. Of course, this begs the question as to <em class="italic">how</em> pandas should make that decision – would the value 42 or 555 be more<a id="_idIndexMarker440"/> appropriate<a id="_idIndexMarker441"/> for <code class="inlineCode">group_a</code> and how can that be determined in a general case?</p>
    <p class="normal">A third expectation would be to return something where the label <code class="inlineCode">group_a</code> appears twice in the resulting row index after aggregation. However, no other group by aggregations work this way, so we would be introducing new and potentially unexpected behavior by reducing to this:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.Series(
    [<span class="hljs-number">42</span>, <span class="hljs-number">555</span>, <span class="hljs-number">0</span>],
    index=pd.Index(["group_a", "group_a", "group_b"], name="group")
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group
group_a     42
group_a    555
group_b      0
dtype: int64
</code></code></pre>
    <p class="normal">Rather than trying to solve for all of these expectations and codify it as part of the API, pandas leaves it entirely up to you how you would like to implement a <code class="inlineCode">mode</code> function, as long as you adhere to the expectations that aggregations reduce to a single value per group. This eliminates the third expectation we just outlined as a possibility, at least until we talk about <strong class="keyWord">Group by</strong> apply later in this chapter.</p>
    <p class="normal">To that end, if we wanted <a id="_idIndexMarker442"/>to roll with<a id="_idIndexMarker443"/> our own custom mode functions, they may end up looking something like:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">scalar_or_list_mode</span>(<span class="hljs-params">ser: pd.Series</span>):
    result = ser.mode()
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(result) &gt; <span class="hljs-number">1</span>:
        <span class="hljs-keyword">return</span> result.tolist()
    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">len</span>(result) == <span class="hljs-number">1</span>:
        <span class="hljs-keyword">return</span> result.iloc[<span class="hljs-number">0</span>]
    <span class="hljs-keyword">return</span> pd.NA
<span class="hljs-keyword">def</span> <span class="hljs-title">scalar_or_bust_mode</span>(<span class="hljs-params">ser: pd.Series</span>):
    result = ser.mode()
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(result) == <span class="hljs-number">0</span>:
        <span class="hljs-keyword">return</span> pd.NA
    <span class="hljs-keyword">return</span> result.iloc[<span class="hljs-number">0</span>]
</code></code></pre>
    <p class="normal">Since these are both aggregations, we can use them in the context of a <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.agg</code> operation:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby("group").agg(
    scalar_or_list=pd.NamedAgg(column="value", aggfunc=scalar_or_list_mode),
    scalar_or_bust=pd.NamedAgg(column="value", aggfunc=scalar_or_bust_mode),
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    scalar_or_list    scalar_or_bust
group                                
group_a   [42, 555]              42
group_b          0                0
</code></code></pre>
    <h1 id="_idParaDest-231" class="heading-1">Group by apply</h1>
    <p class="normal">During our discussion <a id="_idIndexMarker444"/>on algorithms and how to apply them back in <em class="italic">Chapter 5</em>, <em class="italic">Algorithms and How to Apply Them</em>, we came across the Apply function, which is both powerful and terrifying at the same time. An equivalent function for group by exists as <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.apply</code> with all of the same caveats. Generally, this function is overused, and you should opt for <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.agg</code> or <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.transform</code> instead. However, for the cases where you don’t really want an <em class="italic">aggregation</em> or a <em class="italic">transformation</em>, but something in between, using <code class="inlineCode">apply</code> is your only option.</p>
    <p class="normal">Generally, <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.apply</code> should only be used as a last resort. It can produce sometimes ambiguous behavior and is rather prone to breakage across releases of pandas.</p>
    <h2 id="_idParaDest-232" class="heading-2">How to do it</h2>
    <p class="normal">In the <em class="italic">There’s more…</em> section of the previous recipe, we mentioned how it is not possible to start with a <code class="inlineCode">pd.DataFrame</code> of the following:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.DataFrame([
    ["group_a", <span class="hljs-number">42</span>],
    ["group_a", <span class="hljs-number">555</span>],
    ["group_a", <span class="hljs-number">42</span>],
    ["group_a", <span class="hljs-number">555</span>],
    ["group_b", <span class="hljs-number">0</span>],
], columns=["group", "value"])
df = df.convert_dtypes(dtype_backend="numpy_nullable")
df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">  group    value
0 group_a     42
1 group_a    555
2 group_a     42
3 group_a    555
4 group_b      0
</code></code></pre>
    <p class="normal">And to produce the following output, using a custom <code class="inlineCode">mode</code> algorithm supplied to <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.agg</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.Series(
    [<span class="hljs-number">42</span>, <span class="hljs-number">555</span>, <span class="hljs-number">0</span>],
    index=pd.Index(["group_a", "group_a", "group_b"], name="group"),
    dtype=pd.Int64Dtype(),
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group
group_a     42
group_a    555
group_b      0
dtype: Int64
</code></code></pre>
    <p class="normal">The reason for this is straightforward; an aggregation expects you to reduce to a single value per group label. Repeating the label <code class="inlineCode">group_a</code> twice in the output is a non-starter for an aggregation. Similarly, a transformation would expect you to produce a result that shares the same row index as the calling <code class="inlineCode">pd.DataFrame</code>, which is not what we are after either.</p>
    <p class="normal"><code class="inlineCode">pd.core.groupby.DataFrameGroupBy.apply</code> is the in-between method that can get us closer to the desired<a id="_idIndexMarker445"/> result, which you can see in the following code. As a technical aside, the <code class="inlineCode">include_groups=False</code> argument is passed to suppress any deprecation warnings about behavior in pandas 2.2. In subsequent versions, you may not need this:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">mode_for_apply</span>(<span class="hljs-params">df: pd.DataFrame</span>):
    <span class="hljs-keyword">return</span> df["value"].mode()
df.groupby("group").apply(mode_for_apply, include_groups=<span class="hljs-literal">False</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group      
group_a  0     42
         1    555
group_b  0      0
Name: value, dtype: Int64
</code></code></pre>
    <p class="normal">It is important to note that we annotated the parameter of the <code class="inlineCode">mode_for_apply</code> function as a <code class="inlineCode">pd.DataFrame</code>. With aggregations and transformations, user-defined functions receive just a single <code class="inlineCode">pd.Series</code> of data at a time, but with apply, you get an entire <code class="inlineCode">pd.DataFrame</code>. For a more detailed look at what is going on, you can add <code class="inlineCode">print</code> statements to the user-defined function:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">mode_for_apply</span>(<span class="hljs-params">df: pd.DataFrame</span>):
    <span class="hljs-built_in">print</span>(f"\nThe data passed to apply <span class="hljs-keyword">is</span>:\n{df}")
    <span class="hljs-keyword">return</span> df["value"].mode()
df.groupby("group").apply(mode_for_apply, include_groups=<span class="hljs-literal">False</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">The data passed to apply is:
  value
0     42
1    555
2     42
3    555
The data passed to apply is:
  value
4      0
group     
group_a  0     42
        1    555
group_b  0      0
Name: value, dtype: Int64
</code></code></pre>
    <p class="normal">Essentially, <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.apply</code> passes a <code class="inlineCode">pd.DataFrame</code> of data to the user-defined function, excluding the column(s) that are used for grouping. From there, it will look at the return type of the user-defined function and try to infer the best possible output shape it can. In this particular instance, because our <code class="inlineCode">mode_for_apply</code> function returns a <code class="inlineCode">pd.Series</code>, <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.apply</code> has determined that the best output shape should have a <code class="inlineCode">pd.MultiIndex</code>, where the first level of the index is the<a id="_idIndexMarker446"/> group value and the second level contains the row index from the <code class="inlineCode">pd.Series</code> returned by the <code class="inlineCode">mode_for_apply</code> function.</p>
    <p class="normal">Where <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.apply</code> gets overused is in the fact that it can change its shape to look like an aggregation when it detects that the functions it applies reduce to a single value:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">sum_values</span>(<span class="hljs-params">df: pd.DataFrame</span>):
    <span class="hljs-keyword">return</span> df["value"].<span class="hljs-built_in">sum</span>()
df.groupby("group").apply(sum_values, include_groups=<span class="hljs-literal">False</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">group
group_a    1194
group_b       0
dtype: int64
</code></code></pre>
    <p class="normal">It is a trap to use it in this way, however. Even if it can infer a reasonable shape for some outputs, the rules for how it determines that are implementation details, for which you pay a performance penalty or run the risk of code breakage across pandas releases. If you know your<a id="_idIndexMarker447"/> functions will reduce to a single value, always opt for <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.agg</code> in lieu of <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.apply</code>, leaving the latter only for extreme use cases.</p>
    <h1 id="_idParaDest-233" class="heading-1">Window operations</h1>
    <p class="normal">Window operations <a id="_idIndexMarker448"/>allow you to calculate values over a sliding partition (or “window”) of values. Commonly, these operations are used to calculate things like “rolling 90-day average,” but they are flexible enough to extend to any algorithm of your choosing.</p>
    <p class="normal">While not technically a group by operation, window operations are included here as they share a similar API and work with “groups” of data. The only difference to a group by call is that, instead of forming groups from unique value sets, a window operation creates its group by iterating over each value of a pandas object and looking at a particular number of preceding (and sometimes following) values.</p>
    <h2 id="_idParaDest-234" class="heading-2">How to do it</h2>
    <p class="normal">To get a feel for how window operations work, let’s start with a simple <code class="inlineCode">pd.Series</code> where each element is an increasing power of 2:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser = pd.Series([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>], dtype=pd.Int64Dtype())
ser
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0     0
1     1
2     2
3     4
4     8
5    16
dtype: Int64
</code></code></pre>
    <p class="normal">The first type of window operation you will come across is the “rolling window,” accessed via the <code class="inlineCode">pd.Series.rolling</code> method. When calling this method, you need to tell pandas the desired size of your window <em class="italic">n</em>. The pandas library starts at each element and looks backward <em class="italic">n-1</em> records to form the “window”:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.rolling(<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0     NaN
1     1.0
2     3.0
3     6.0
4    12.0
5    24.0
dtype: float64
</code></code></pre>
    <p class="normal">You may notice that we started with a <code class="inlineCode">pd.Int64Dtype()</code> but ended up with a <code class="inlineCode">float64</code> type after the rolling window operation. Unfortunately, the pandas window operations do not work well with the pandas extension system in at least version 2.2 (see issue #50449), so for the time being, we need to cast the result back into the proper data type:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.rolling(<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>().astype(pd.Int64Dtype())
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0    &lt;NA&gt;
1       1
2       3
3       6
4      12
5      24
dtype: Int64
</code></code></pre>
    <p class="normal">So, what is going on<a id="_idIndexMarker449"/> here? Essentially, you can think of a rolling window operation as iterating through the <code class="inlineCode">pd.Series</code> values. While doing so, it looks backward to try and collect enough values to fulfill the desired window size, which we have specified as 2.</p>
    <p class="normal">After collecting two elements in each window, pandas will apply the specified aggregation function (in our case, summation). The result of that aggregation in each window is then used to piece back together the result:</p>
    <figure class="mediaobject"><img src="../Images/B31091_08_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.3: Rolling window with sum aggregation</p>
    <p class="normal">In the case of our very first record, which cannot form a window with two elements, pandas returns a <a id="_idIndexMarker450"/>missing value. If you want the rolling calculation to just sum up as many elements as it can, even if the window size cannot be reached, you can pass an argument to <code class="inlineCode">min_periods=</code> that dictates the minimum number of elements within each window required to perform the aggregation:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.rolling(<span class="hljs-number">2</span>, min_periods=<span class="hljs-number">1</span>).<span class="hljs-built_in">sum</span>().astype(pd.Int64Dtype())
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0     0
1     1
2     3
3     6
4    12
5    24
dtype: Int64
</code></code></pre>
    <p class="normal">By default, rolling window operations look backward to try and fulfill your window size requirements. You can also “center” them instead so that pandas looks both forward and backward.</p>
    <p class="normal">The effect of this is better seen with an odd window size. Note the difference when we expand our call so far with a window size of <code class="inlineCode">3</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.rolling(<span class="hljs-number">3</span>).<span class="hljs-built_in">sum</span>().astype(pd.Int64Dtype())
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0    &lt;NA&gt;
1    &lt;NA&gt;
2       3
3       7
4      14
5      28
dtype: Int64
</code></code></pre>
    <p class="normal">Compared to the<a id="_idIndexMarker451"/> same call with an argument of <code class="inlineCode">center=True</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.rolling(<span class="hljs-number">3</span>, center=<span class="hljs-literal">True</span>).<span class="hljs-built_in">sum</span>().astype(pd.Int64Dtype())
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0    &lt;NA&gt;
1       3
2       7
3      14
4      28
5    &lt;NA&gt;
dtype: Int64
</code></code></pre>
    <p class="normal">Instead of looking at the current and preceding two values, usage of <code class="inlineCode">center=True</code> tells pandas to take the current value, one prior, and one following to form a window.</p>
    <p class="normal">Another type of window function is the “expanding window”, which looks at all prior values encountered. The syntax for that is straightforward; simply replace your call to <code class="inlineCode">pd.Series.rolling</code> with <code class="inlineCode">pd.Series.expanding</code> and follow that up with your desired aggregation function. An expanding summation is similar to the <code class="inlineCode">pd.Series.cumsum</code> method you have seen before, so for demonstration purposes, let’s pick a different aggregation function, like <code class="inlineCode">mean</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.expanding().mean().astype(pd.Float64Dtype())
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0         0.0
1         0.5
2         1.0
3        1.75
4         3.0
5    5.166667
dtype: Float64
</code></code></pre>
    <p class="normal">Visually represented, an <a id="_idIndexMarker452"/>expanding window calculation looks as follows (for brevity, not all of the <code class="inlineCode">pd.Series</code> elements are shown):</p>
    <figure class="mediaobject"><img src="../Images/B31091_08_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.4: Expanding window with mean aggregation</p>
    <h2 id="_idParaDest-235" class="heading-2">There’s more…</h2>
    <p class="normal">In <em class="italic">Chapter 9</em>, <em class="italic">Temporal Data Types and Algorithms</em>, we will dive deeper<a id="_idIndexMarker453"/> into some of the very nice features pandas can offer when dealing with temporal data. Before we get there, it is worth noting that group by and rolling/expanding window functions work very naturally with such data, allowing you to concisely perform calculations like, “N day moving averages” “year-to-date X,” “quarter-to-date X,” etc.</p>
    <p class="normal">To see how that works, let’s take another look at the Nvidia stock performance dataset we started with back in <em class="chapterRef">Chapter 5</em>, <em class="keystroke">Algorithms and How to Apply Them</em>, originally as part of the <em class="italic">Calculating a trailing stop order price</em> recipe:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_csv(
    "data/NVDA.csv",
    usecols=["Date", "Close"],
    parse_dates=["Date"],
    dtype_backend="numpy_nullable",
).set_index("Date")
df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">      Date        Close
2020-01-02    59.977501
2020-01-03    59.017502
2020-01-06    59.264999
2020-01-07    59.982498
2020-01-08    60.095001
…             …
2023-12-22   488.299988
2023-12-26   492.790009
2023-12-27   494.170013
2023-12-28   495.220001
2023-12-29   495.220001
1006 rows × 1 columns
</code></code></pre>
    <p class="normal">With rolling window functions, we can easily add 30, 60, and 90-day moving averages. A subsequent call <a id="_idIndexMarker454"/>to <code class="inlineCode">pd.DataFrame.plot</code> also makes this easy to visualize:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
plt.ion()
df.assign(
    ma30=df["Close"].rolling(<span class="hljs-number">30</span>).mean().astype(pd.Float64Dtype()),
    ma60=df["Close"].rolling(<span class="hljs-number">60</span>).mean().astype(pd.Float64Dtype()),
    ma90=df["Close"].rolling(<span class="hljs-number">90</span>).mean().astype(pd.Float64Dtype()),
).plot()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_08_05.png" alt=""/></figure>
    <p class="normal">For “year-to-date” and “quarter-to-date” calculations, we can use a combination of group by and expanding window <a id="_idIndexMarker455"/>functions. For “year-to-date” min, max, and mean close values, we can start by forming a group by object to split our data into yearly buckets, and from there, we can make a call to <code class="inlineCode">.expanding()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby(pd.Grouper(freq="YS")).expanding().agg(
    ["<span class="hljs-built_in">min</span>", "<span class="hljs-built_in">max</span>", "mean"]
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">          Close
                        min        max        mean
Date       Date
2020-01-01  2020-01-02  59.977501  59.977501  59.977501
            2020-01-03  59.017502  59.977501  59.497501
            2020-01-06  59.017502  59.977501  59.420001
            2020-01-07  59.017502  59.982498  59.560625
            2020-01-08  59.017502  60.095001  59.667500
…           …          …          …          …
2023-01-01  2023-12-22  142.649994  504.089996  363.600610
            2023-12-26  142.649994  504.089996  364.123644
            2023-12-27  142.649994  504.089996  364.648024
            2023-12-28  142.649994  504.089996  365.172410
            2023-12-29  142.649994  504.089996  365.692600
1006 rows × 3 columns
</code></code></pre>
    <p class="normal">The <code class="inlineCode">pd.Grouper(freq="YS")</code> takes our row index, which contains datetimes, and groups them by the start of the year within which they fall. After the grouping, the call to <code class="inlineCode">.expanding()</code> performs <a id="_idIndexMarker456"/>the min/max aggregations, only looking as far back as the start of each year. The effects of this are once again easier to see with a visualization:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby(pd.Grouper(freq="YS")).expanding().agg(
    ["<span class="hljs-built_in">min</span>", "<span class="hljs-built_in">max</span>", "mean"]
).droplevel(axis=<span class="hljs-number">1</span>, level=<span class="hljs-number">0</span>).reset_index(level=<span class="hljs-number">0</span>, drop=<span class="hljs-literal">True</span>).plot()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_08_06.png" alt=""/></figure>
    <p class="normal">For a more granular<a id="_idIndexMarker457"/> view, you can calculate the expanding min/max close prices per quarter by changing the <code class="inlineCode">freq=</code> argument from <code class="inlineCode">YS</code> to <code class="inlineCode">QS</code> in <code class="inlineCode">pd.Grouper</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby(pd.Grouper(freq="QS")).expanding().agg(
    ["<span class="hljs-built_in">min</span>", "<span class="hljs-built_in">max</span>", "mean"]
).reset_index(level=<span class="hljs-number">0</span>, drop=<span class="hljs-literal">True</span>).plot()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_08_07.png" alt=""/></figure>
    <p class="normal">A <code class="inlineCode">MS</code> <code class="inlineCode">freq=</code> argument <a id="_idIndexMarker458"/>gets you down to the monthly level:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby(pd.Grouper(freq="MS")).expanding().agg(
    ["<span class="hljs-built_in">min</span>", "<span class="hljs-built_in">max</span>", "mean"]
).reset_index(level=<span class="hljs-number">0</span>, drop=<span class="hljs-literal">True</span>).plot()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_08_08.png" alt=""/></figure>
    <h1 id="_idParaDest-236" class="heading-1">Selecting the highest rated movies by year</h1>
    <p class="normal">One of the most basic<a id="_idIndexMarker459"/> and common operations to perform during data analysis is to select rows containing the largest value of some column within a group. Applied to our movie dataset, this could mean finding the highest-rated film of each year or the highest-grossing film by content rating. To accomplish these tasks, we need to sort the groups as well as the column used to rank each member of the group, and then extract the highest member of each group.</p>
    <p class="normal">In this recipe, we will find the highest-rated film of each year using a combination of <code class="inlineCode">pd.DataFrame.sort_values</code> and <code class="inlineCode">pd.DataFrame.drop_duplicates</code>.</p>
    <h2 id="_idParaDest-237" class="heading-2">How to do it</h2>
    <p class="normal">Start by reading in the movie dataset and slim it down to just the three columns we care about: <code class="inlineCode">movie_title</code>, <code class="inlineCode">title_year</code>, and <code class="inlineCode">imdb_score</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_csv(
    "data/movie.csv",
    usecols=["movie_title", "title_year", "imdb_score"],
    dtype_backend="numpy_nullable",
)
df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">   movie_title                                  title_year  imdb_score
0  Avatar                                        2009.0        7.9
1  Pirates of the Caribbean: At World's End      2007.0        7.1
2  Spectre                                       2015.0        6.8
3  The Dark Knight Rises                         2012.0        8.5
4  Star Wars: Episode VII - The Force Awakens     &lt;NA&gt;        7.1
…                                                 …          …
4911  Signed Sealed Delivered                    2013.0        7.7
4912  The Following                               &lt;NA&gt;        7.5
4913  A Plague So Pleasant                       2013.0        6.3
4914  Shanghai Calling                           2012.0        6.3
4915  My Date with Drew                          2004.0        6.6
4916 rows × 3 columns
</code></code></pre>
    <p class="normal">As you can see, the <code class="inlineCode">title_year</code> column gets interpreted as a floating point value, but years should always<a id="_idIndexMarker460"/> be whole numbers. We could correct that by assigning the proper data type directly to our column:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df["title_year"] = df["title_year"].astype(pd.Int16Dtype())
df.head(<span class="hljs-number">3</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    movie_title                                title_year  imdb_score
0   Avatar                                     2009        7.9
1   Pirates of the Caribbean: At World's End   2007        7.1
2   Spectre                                    2015        6.8
</code></code></pre>
    <p class="normal">Alternatively, we could have passed the desired data type as the <code class="inlineCode">dtype=</code> argument in <code class="inlineCode">pd.read_csv</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_csv(
    "data/movie.csv",
    usecols=["movie_title", "title_year", "imdb_score"],
    dtype={"title_year": pd.Int16Dtype()},
    dtype_backend="numpy_nullable",
)
df.head(<span class="hljs-number">3</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    movie_title                                 title_year  imdb_score
0   Avatar                                      2009         7.9
1   Pirates of the Caribbean: At World's End    2007         7.1
2   Spectre                                     2015         6.8
</code></code></pre>
    <p class="normal">With our data cleansing out of the way, we can now turn our focus to answering the question of “what is the highest rated movie each year?”. There are a few ways we can calculate this, but let’s start with the approach you see most commonly.</p>
    <p class="normal">When you perform a group by in pandas, the order in which rows appear in the original <code class="inlineCode">pd.DataFrame</code> is respected as rows are bucketed into different groups. Knowing this, many users will answer this question by first sorting their dataset across <code class="inlineCode">title_year</code> and <code class="inlineCode">imdb_score</code>. After the sort, you can group by the <code class="inlineCode">title_year</code> column, select just the <code class="inlineCode">movie_title</code> column, and <a id="_idIndexMarker461"/>chain in a call to <code class="inlineCode">pd.DataFrameGroupBy.last</code> to select the last value from each group:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.sort_values(["title_year", "imdb_score"]).groupby(
    "title_year"
)[["movie_title"]].agg(top_rated_movie=pd.NamedAgg("movie_title", "last"))
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">title_year                                    top_rated_movie
1916         Intolerance: Love's Struggle Throughout the Ages
1920                           Over the Hill to the Poorhouse
1925                                           The Big Parade
1927                                               Metropolis
1929                                            Pandora's Box
…                                                           …
2012                                         Django Unchained
2013                  Batman: The Dark Knight Returns, Part 2
2014                                           Butterfly Girl
2015                                          Running Forever
2016                                     Kickboxer: Vengeance
91 rows × 1 columns
</code></code></pre>
    <p class="normal">A slightly more succinct approach can be had if you use <code class="inlineCode">pd.DataFrameGroupBy.idxmax</code>, which selects the row index value corresponding to the highest movie rating each year. This would require you to set the index to the <code class="inlineCode">movie_title</code> up front:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.set_index("movie_title").groupby("title_year").agg(
    top_rated_movie=pd.NamedAgg("imdb_score", "idxmax")
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">title_year                                   top_rated_movie
1916        Intolerance: Love's Struggle Throughout the Ages
1920                          Over the Hill to the Poorhouse
1925                                          The Big Parade
1927                                              Metropolis
1929                                           Pandora's Box
…                                                          …
2012                                   The Dark Knight Rises
2013                 Batman: The Dark Knight Returns, Part 2
2014                                  Queen of the Mountains
2015                                         Running Forever
2016                                    Kickboxer: Vengeance
91 rows × 1 columns
</code></code></pre>
    <p class="normal">Our results appear<a id="_idIndexMarker462"/> mostly the same, although we can see that the two approaches disagreed on what the highest rated movie was in the years 2012 and 2014. A closer look at these titles reveals the root cause:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df[df["movie_title"].isin({
    "Django Unchained",
    "The Dark Knight Rises",
    "Butterfly Girl",
    "Queen of the Mountains",
})]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">			movie_title 			title_year 	imdb_score
3 			The Dark Knight Rises 	2012 		8.5
293 			Django Unchained 		2012 		8.5
4369 		Queen of the Mountains 	2014 		8.7
4804 		Butterfly Girl			2014 		8.7
</code></code></pre>
    <p class="normal">In case of a tie, each method has its own way of choosing a value. Neither approach is right or wrong per se, but if you wanted finer control over that, you would have to reach for <strong class="keyWord">Group by apply</strong>.</p>
    <p class="normal">Let’s assume we wanted to aggregate the values so that when there is no tie, we get back a string, but in case of a tie we get a sequence of strings. To do this, you should define a function that accepts a <code class="inlineCode">pd.DataFrame</code>. This <code class="inlineCode">pd.DataFrame</code> will contain the values associated with each unique grouping column, which is <code class="inlineCode">title_year</code> in our case.</p>
    <p class="normal">Within the body of the function, you can figure out what the top movie rating is, find all movies with that rating, and return back either a single movie title (when there are no ties) or a set of <a id="_idIndexMarker463"/>movies (in case of a tie):</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">top_rated</span>(<span class="hljs-params">df: pd.DataFrame</span>):
    top_rating = df["imdb_score"].<span class="hljs-built_in">max</span>()
    top_rated = df[df["imdb_score"] == top_rating]["movie_title"].unique()
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(top_rated) == <span class="hljs-number">1</span>:
        <span class="hljs-keyword">return</span> top_rated[<span class="hljs-number">0</span>]
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> top_rated
df.groupby("title_year").apply(
    top_rated, include_groups=<span class="hljs-literal">False</span>
).to_frame().rename(columns={<span class="hljs-number">0</span>: "top_rated_movie(s)"})
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">title_year                                   top_rated_movie(s)
1916           Intolerance: Love's Struggle Throughout the Ages
1920                             Over the Hill to the Poorhouse
1925                                             The Big Parade
1927                                                 Metropolis
1929                                              Pandora's Box
…                                                             …
2012                  [The Dark Knight Rises, Django Unchained]
2013                    Batman: The Dark Knight Returns, Part 2
2014                   [Queen of the Mountains, Butterfly Girl]
2015                                            Running Forever
2016                                       Kickboxer: Vengeance
91 rows × 1 columns
</code></code></pre>
    <h1 id="_idParaDest-238" class="heading-1">Comparing the best hitter in baseball across years</h1>
    <p class="normal">In the <em class="italic">Finding the baseball </em><em class="italic"><a id="_idIndexMarker464"/></em><em class="italic">players best at…</em> recipe back in <em class="italic">Chapter 5</em>, <em class="italic">Algorithms and How to Apply Them</em>, we worked with a dataset that had already aggregated the performance of players from the years 2020-2023. However, comparing players based on their performance across multiple years is rather difficult. Even on a year-to-year basis, statistics that appear elite one year can be considered just “very good” in other years. The reasons for the variation in statistics across years can be debated, but likely come down to some combination of strategy, equipment, weather, and just pure statistical chance.</p>
    <p class="normal">For this recipe, we are going to work with a more granular dataset that goes down to the game level. From there, we are going to aggregate the data up to a yearly summary, and from there calculate a common baseball statistic known as the <em class="italic">batting average</em>. </p>
    <p class="normal">For those unfamiliar, a batting average is calculated by taking the number of <em class="italic">hits</em> a player produces (i.e., how many times they swung a bat at a baseball, and reached base as a result) as a percentage of their total <em class="italic">at bats</em> (i.e., how many times they came to bat, excluding <em class="italic">walks</em>).</p>
    <p class="normal">So what constitutes a good batting average? As you will see, the answer to that question is a moving target, having shifted even within the past twenty years. In the early 2000s, a batting average between .260-.270 (i.e., getting a hit in 26%-27% of at bats) was considered middle of the road for professionals. Within recent years, that number has fallen somewhere in the range of .240-.250.</p>
    <p class="normal">As such, to try and compare the <em class="italic">best hitters</em> from each year to one another, we cannot solely look at the batting average. A league-leading batting average of .325 in a year when the league itself<a id="_idIndexMarker465"/> averaged .240 is likely more impressive than a league-leading batting average of .330 in a year where the overall league averaged around .260.</p>
    <h2 id="_idParaDest-239" class="heading-2">How to do it</h2>
    <p class="normal">Once again, we are going to use data collected from <code class="inlineCode">retrosheet.org</code>, with the following legal disclaimer:</p>
    <div class="note">
      <p class="normal"> The information used here was obtained free of charge from and is copyrighted by Retrosheet. Interested parties may contact Retrosheet at <a href="https://www.retrosheet.org"><span class="url">www.retrosheet.org</span></a>.</p>
    </div>
    <p class="normal">For this recipe we are going to use “box score” summaries from every regular season game played in the years 2000-2023:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_parquet("data/mlb_batting_lines.parquet")
df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    year     game       starttime   …   cs  gidp  int
0   2015  ANA201504100   7:12PM   …    0    0    0
1   2015  ANA201504100   7:12PM   …    0    0    0
2   2015  ANA201504100   7:12PM   …    0    0    0
3   2015  ANA201504100   7:12PM   …    0    0    0
4   2015  ANA201504100   7:12PM   …    0    0    0
…     …          …          …   …    …    …    …
1630995 2013  WAS201309222   7:06PM   …    0    0    0
1630996 2013  WAS201309222   7:06PM   …    0    0    0
1630997 2013  WAS201309222   7:06PM   …    0    0    0
1630998 2013  WAS201309222   7:06PM   …    0    0    0
1630999 2013  WAS201309222   7:06PM   …    0    0    0
1631000 rows × 26 columns
</code></code></pre>
    <p class="normal">A box score summarizes the performance of every player in a <em class="italic">game</em>. We could therefore single in on a <a id="_idIndexMarker466"/>particular game that was played in Baltimore on April 10, 2015, and see how batters performed:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">bal = df[df["game"] == "BAL201504100"]
bal.head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">      year      game       starttime   …  cs  gidp  int
2383  2015  BAL201504100   3:11PM   …   0    0    0
2384  2015  BAL201504100   3:11PM   …   0    0    0
2385  2015  BAL201504100   3:11PM   …   0    0    0
2386  2015  BAL201504100   3:11PM   …   0    0    0
2387  2015  BAL201504100   3:11PM   …   0    0    0
5 rows × 26 columns
</code></code></pre>
    <p class="normal">In that game alone we see a total of 75 at bats (<em class="italic">ab</em>), 29 hits (<em class="italic">h</em>) and two home runs (<em class="italic">hr</em>):</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">bal[["ab", "h", "hr"]].<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">ab    75
h     29
hr     2
dtype: Int64
</code></code></pre>
    <p class="normal">With a basic understanding of what a box score is and what it shows, let’s turn our focus toward calculating the batting average every player produces each year. The individual player is notated in the <code class="inlineCode">id</code> column of our dataset, and since we want to see the batting average over the course of an entire season, we can use the combination of <code class="inlineCode">year</code> and <code class="inlineCode">id</code> as our argument to <code class="inlineCode">pd.DataFrame.groupby</code>. Afterward, we can apply a summation to the at bats (<code class="inlineCode">ab</code>) and hits (<code class="inlineCode">h</code>) columns:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby(["year", "<span class="hljs-built_in">id</span>"]).agg(
    total_ab=pd.NamedAgg(column="ab", aggfunc="<span class="hljs-built_in">sum</span>"),
    total_h=pd.NamedAgg(column="h", aggfunc="<span class="hljs-built_in">sum</span>"),
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">year  id        total_ab  total_h
2000  abboj002     215       59
      abbok002     157       34
      abbop001       5        2
      abreb001     576      182
      acevj001       1        0
…     …           …        …
2023  zavas001     175       30
      zerpa001       0        0
      zimmb002       0        0
      zunig001       0        0
      zunim001     124       22
31508 rows × 2 columns
</code></code></pre>
    <p class="normal">To turn those totals into a batting average, we can chain in a division using <code class="inlineCode">pd.DataFrame.assign</code>. After that, a call to <code class="inlineCode">pd.DataFrame.drop</code> will let us solely focus on the batting average, dropping<a id="_idIndexMarker467"/> the <code class="inlineCode">total_ab</code> and <code class="inlineCode">total_h</code> columns we no longer need:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">(
    df.groupby(["year", "<span class="hljs-built_in">id</span>"]).agg(
        total_ab=pd.NamedAgg(column="ab", aggfunc="<span class="hljs-built_in">sum</span>"),
        total_h=pd.NamedAgg(column="h", aggfunc="<span class="hljs-built_in">sum</span>"))
    .assign(avg=<span class="hljs-keyword">lambda</span> x: x["total_h"] / x["total_ab"])
    .drop(columns=["total_ab", "total_h"])
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">year  id        avg
2000  abboj002  0.274419
      abbok002  0.216561
      abbop001  0.400000
      abreb001  0.315972
      acevj001  0.000000
…     …         …
2023  zavas001  0.171429
      zerpa001  NaN
      zimmb002  NaN
      zunig001  NaN
      zunim001  0.177419
31508 rows × 1 columns
</code></code></pre>
    <p class="normal">Before we continue, we have to consider some data quality issues that may arise when calculating averages. Over the course of a baseball season, teams may use players who only appear in very niche situations, yielding a low number of plate appearances. In some instances, a batter may not even register an “at bat” for the season, so using that as a divisor has a chance of dividing by 0, which will produce <code class="inlineCode">NaN</code>. In cases where a batter has a non-zero amount of at bats on the season, but still has relatively few, a small sample size can severely skew their batting average.</p>
    <p class="normal">Major League Baseball has strict rules for determining how many plate appearances it takes for a batter to qualify for records within a given year. Without following the rule exactly, and without<a id="_idIndexMarker468"/> having to calculate plate appearances in our dataset, we can proxy this by setting a requirement of at least 400 at bats over the course of a season:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">(
    df.groupby(["year", "<span class="hljs-built_in">id</span>"]).agg(
        total_ab=pd.NamedAgg(column="ab", aggfunc="<span class="hljs-built_in">sum</span>"),
        total_h=pd.NamedAgg(column="h", aggfunc="<span class="hljs-built_in">sum</span>"))
    .loc[<span class="hljs-keyword">lambda</span> df: df["total_ab"] &gt; <span class="hljs-number">400</span>]
    .assign(avg=<span class="hljs-keyword">lambda</span> x: x["total_h"] / x["total_ab"])
    .drop(columns=["total_ab", "total_h"])
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">year  id        avg
2000  abreb001  0.315972
      alfoe001  0.323529
      alicl001  0.294444
      alomr001  0.309836
      aloum001  0.354626
…     …         …
2023  walkc002  0.257732
      walkj003  0.276190
      wittb002  0.276131
      yelic001  0.278182
      yoshm002  0.288641
4147 rows × 1 columns
</code></code></pre>
    <p class="normal">We can summarize this further by finding the average and maximum <code class="inlineCode">batting_average</code> per season, and we can even use <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.idxmax</code> to identify the player who<a id="_idIndexMarker469"/> achieved the best average:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">averages = (
    df.groupby(["year", "<span class="hljs-built_in">id</span>"]).agg(
        total_ab=pd.NamedAgg(column="ab", aggfunc="<span class="hljs-built_in">sum</span>"),
        total_h=pd.NamedAgg(column="h", aggfunc="<span class="hljs-built_in">sum</span>"))
    .loc[<span class="hljs-keyword">lambda</span> df: df["total_ab"] &gt; <span class="hljs-number">400</span>]
    .assign(avg=<span class="hljs-keyword">lambda</span> x: x["total_h"] / x["total_ab"])
    .drop(columns=["total_ab", "total_h"])
)
averages.groupby("year").agg(
    league_mean_avg=pd.NamedAgg(column="avg", aggfunc="mean"),
    league_max_avg=pd.NamedAgg(column="avg", aggfunc="<span class="hljs-built_in">max</span>"),
    batting_champion=pd.NamedAgg(column="avg", aggfunc="idxmax"),
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">year  league_mean_avg  league_max_avg  batting_champion
2000  0.284512         0.372414         (2000, heltt001)
2001  0.277945         0.350101         (2001, walkl001)
2002  0.275713         0.369727         (2002, bondb001)
2003  0.279268         0.358714         (2003, pujoa001)
2004  0.281307         0.372159         (2004, suzui001)
2005  0.277350         0.335017         (2005, lee-d002)
2006  0.283609         0.347409         (2006, mauej001)
2007  0.281354         0.363025         (2007, ordom001)
2008  0.277991         0.364465         (2008, jonec004)
2009  0.278010         0.365201         (2009, mauej001)
2010  0.271227         0.359073         (2010, hamij003)
2011  0.269997         0.344406         (2011, cabrm001)
2012  0.269419         0.346405         (2012, cabrm002)
2013  0.268789         0.347748         (2013, cabrm001)
2014  0.267409         0.340909         (2014, altuj001)
2015  0.268417         0.337995         (2015, cabrm001)
2016  0.270181         0.347826         (2016, lemad001)
2017  0.268651         0.345763         (2017, altuj001)
2018  0.261824         0.346154         (2018, bettm001)
2019  0.269233         0.335341         (2019, andet001)
2021  0.262239         0.327731         (2021, turnt001)
2022  0.255169         0.326454         (2022, mcnej002)
2023  0.261457         0.353659         (2023, arral001)
</code></code></pre>
    <p class="normal">As we can see, the mean batting average fluctuates each year, with those numbers having been higher back toward the year 2000. In the year 2005, the mean batting average was .277, with the best hitter (lee-d002, or Derrek Lee) having hit .335. The best hitter in 2019 (andet001, or Tim Anderson) also averaged .335, but the overall league was down around .269. Therefore, a strong argument could be made that Tim Anderson’s 2019 season was more impressive than Derrek Lee’s 2005 season, at least through the lens of batting average.</p>
    <p class="normal">While taking the mean can be useful, it doesn’t tell the full story of what goes on within a given season. We would probably like to get a better feel for the overall distribution of batting averages across each season, for which a visualization is in order. The violin plot we discovered back in the <em class="italic">Plotting movie ratings by decade with seaborn</em> recipe can help us understand this in more detail.</p>
    <p class="normal">First let’s set up our seaborn import, and have Matplotlib draw plots as soon as possible:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
plt.ion()
</code></code></pre>
    <p class="normal">Next, we will want to <a id="_idIndexMarker470"/>make a few considerations for seaborn. Seaborn does not make use of <code class="inlineCode">pd.MultiIndex</code>, so we are going to move our index values to columns with a call to <code class="inlineCode">pd.DataFrame.reset_index</code>. Additionally, seaborn can easily misinterpret discrete <em class="italic">year</em> values like 2000, 2001, 2002, and so on for a continuous range, which we can solve by turning that column into a categorical data type. </p>
    <p class="normal">The <code class="inlineCode">pd.CategoricalDtype</code> we want to construct is also ideally ordered, so that pandas can ensure the year 2000 is followed by 2001, which is followed by 2002, and so on:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">sns_df = averages.reset_index()
years = sns_df["year"].unique()
cat = pd.CategoricalDtype(<span class="hljs-built_in">sorted</span>(years), ordered=<span class="hljs-literal">True</span>)
sns_df["year"] = sns_df["year"].astype(cat)
sns_df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    year      id        avg
0   2000  abreb001  0.315972
1   2000  alfoe001  0.323529
2   2000  alicl001  0.294444
3   2000  alomr001  0.309836
4   2000  aloum001  0.354626
…     …      …         …
4142 2023  walkc002  0.257732
4143 2023  walkj003  0.276190
4144 2023  wittb002  0.276131
4145 2023  yelic001  0.278182
4146 2023  yoshm002  0.288641
4147 rows × 3 columns
</code></code></pre>
    <p class="normal">23 years of data on a single<a id="_idIndexMarker471"/> plot may take up a lot of space, so let’s just look at the years 2000-2009 first:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">mask = (sns_df["year"] &gt;= <span class="hljs-number">2000</span>) &amp; (sns_df["year"] &lt; <span class="hljs-number">2010</span>)
fig, ax = plt.subplots()
sns.violinplot(
    data=sns_df[mask],
    ax=ax,
    x="avg",
    y="year",
    order=sns_df.loc[mask, "year"].unique(),
)
ax.set_xlim(<span class="hljs-number">0.15</span>, <span class="hljs-number">0.4</span>)
plt.show()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_08_09.png" alt=""/></figure>
    <p class="normal">We intentionally made<a id="_idIndexMarker472"/> the call to <code class="inlineCode">plt.subplots()</code> and used <code class="inlineCode">ax.set_xlim(0.15, 0.4)</code> so that the x-axis would not change when plotting the remaining years:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">mask = sns_df["year"] &gt;= <span class="hljs-number">2010</span>
fig, ax = plt.subplots()
sns.violinplot(
    data=sns_df[mask],
    ax=ax,
    x="avg",
    y="year",
    order=sns_df.loc[mask, "year"].unique(),
)
ax.set_xlim(<span class="hljs-number">0.15</span>, <span class="hljs-number">0.4</span>)
plt.show()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_08_10.png" alt=""/></figure>
    <p class="normal">While some years show <a id="_idIndexMarker473"/>skew in the data (e.g., 2014 skewing right and 2018 skewing left), we can generally imagine the distribution of this data as an approximation of a normal distribution. Therefore, to try and better compare the peak performances across different years, we can use a technique whereby we <em class="italic">normalize</em> data within each season. Rather than thinking in terms of absolute batting averages like .250, we instead think of how far beyond the norm within a season a batter’s performance is.</p>
    <p class="normal">More specifically, we can use Z-score normalization, which would appear as follows when mathematically represented:</p>
    <p class="center"><img src="../Images/B31091_08_001.png" alt=""/></p>
    <p class="normal">Here, <code class="inlineCode"><img src="../Images/B31091_08_002.png" alt=""/></code> is the mean and <code class="inlineCode"><img src="../Images/B31091_08_003.png" alt=""/></code> is the standard deviation.</p>
    <p class="normal">Calculating this in pandas is rather trivial; all we need to do is define our custom <code class="inlineCode">normalize</code> function and use that as an argument to <code class="inlineCode">pd.core.groupby.DataFrameGroupBy.transform</code> to assign each combination of year and player their normalized batting average. Using that in subsequent group by operations allows us to better <a id="_idIndexMarker474"/>compare the peak performance each year across different years:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">normalize</span>(<span class="hljs-params">ser: pd.Series</span>) -&gt; pd.Series:
    <span class="hljs-keyword">return</span> (ser - ser.mean()) / ser.std()
(
    averages.assign(
        normalized_avg=averages.groupby("year").transform(normalize)
    )
    .groupby("year").agg(
        league_mean_avg=pd.NamedAgg(column="avg", aggfunc="mean"),
        league_max_avg=pd.NamedAgg(column="avg", aggfunc="<span class="hljs-built_in">max</span>"),
        batting_champion=pd.NamedAgg(column="avg", aggfunc="idxmax"),
        max_normalized_avg=pd.NamedAgg(column="normalized_avg", aggfunc="<span class="hljs-built_in">max</span>"),
    )
    .sort_values(by="max_normalized_avg", ascending=<span class="hljs-literal">False</span>)
).head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">year  league_mean_avg  league_max_avg  batting_champion      max_normalized_avg
2023  0.261457         0.353659        (2023, arral001)                3.714121
2004  0.281307         0.372159        (2004, suzui001)                3.699129
2002  0.275713         0.369727        (2002, bondb001)                3.553521
2010  0.271227         0.359073        (2010, hamij003)                3.379203
2008  0.277991         0.364465        (2008, jonec004)                3.320429
</code></code></pre>
    <p class="normal">According to this analysis, the 2023 season by Luis Arráez is the most impressive batting average performance since the year 2000. His <code class="inlineCode">league_max_avg</code> achieved that year may appear as the lowest out of our top five, but so was the <code class="inlineCode">league_mean_avg</code> in 2023.</p>
    <p class="normal">As you can see from this recipe, effective use of pandas’ Group By functionality allows you to more fairly evaluate records within different groups. Our example used professional baseball players <a id="_idIndexMarker475"/>within a season, but that same methodology could be extended to evaluate users within different age groups, products within different product lines, stocks within different sectors, and so on. Simply put, the possibilities for exploring your data with group by are endless!</p>
    <h1 id="_idParaDest-240" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/pandas"><span class="url">https://packt.link/pandas</span></a></p>
    <p class="normal"><img src="../Images/QR_Code5040900042138312.png" alt=""/></p>
  </div>
</body></html>