["```py\nfrom collections import namedtuple\nFeatureConfig = namedtuple(\n    \"FeatureConfig\",\n    [\n        \"target\",\n        \"index_cols\",\n        \"static_categoricals\",\n        \"static_reals\",\n        \"time_varying_known_categoricals\",\n        \"time_varying_known_reals\",\n        \"time_varying_unknown_reals\",\n        \"group_ids\"\n    ],\n) \n```", "```py\nfeat_config = FeatureConfig(\n    target=\"energy_consumption\",\n    index_cols=[\"LCLid\", \"timestamp\"],\n    static_categoricals=[\n        \"LCLid\",\n        \"stdorToU\",\n        \"Acorn\",\n        \"Acorn_grouped\",\n        \"file\",\n    ],\n    static_reals=[],\n    time_varying_known_categoricals=[\n        \"holidays\",\n        \"timestamp_Dayofweek\",\n    ],\n    time_varying_known_reals=[\"apparentTemperature\"],\n    time_varying_unknown_reals=[\"energy_consumption\"],\n    group_ids=[\"LCLid\"],\n) \n```", "```py\ntraining = TimeSeriesDataSet(\n    train_df,\n    time_idx=\"time_idx\",\n    target=feat_config.target,\n    group_ids=feat_config.group_ids,\n    max_encoder_length=max_encoder_length,\n    max_prediction_length=max_prediction_length,\n    time_varying_unknown_reals=[\n        \"energy_consumption\",\n    ],\n    target_normalizer=GroupNormalizer(\n        groups=feat_config.group_ids, transformation=None\n    )\n) \n```", "```py\n# Defining the validation dataset with the same parameters as training\nvalidation = TimeSeriesDataSet.from_dataset(training, pd.concat([val_history,val_df]).reset_index(drop=True), stop_randomization=True)\n# Defining the test dataset with the same parameters as training\ntest = TimeSeriesDataSet.from_dataset(training, pd.concat([hist_df, test_df]).reset_index(drop=True), stop_randomization=True) \n```", "```py\ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\nval_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0) \n```", "```py\n# Testing the dataloader\nx, y = next(iter(train_dataloader))\nprint(\"\\nsizes of x =\")\nfor key, value in x.items():\n    print(f\"\\t{key} = {value.size()}\")\nprint(\"\\nsize of y =\")\nprint(f\"\\ty = {y[0].size()}\") \n```", "```py\nclass SimpleRNNModel(SingleStepRNN):\n    def __init__(\n        self,\n        rnn_type: str,\n        input_size: int,\n        hidden_size: int,\n        num_layers: int,\n        bidirectional: bool,\n    ):\n        super().__init__(rnn_type, input_size, hidden_size, num_layers, bidirectional)\n    def forward(self, x: Dict):\n        # Using the encoder continuous which has the history window\n        x = x[\"encoder_cont\"] # x --> (batch_size, seq_len, input_size)\n        # Processing through the RNN\n        x, _ = self.rnn(x)  # --> (batch_size, seq_len, hidden_size)\n        # Using a FC layer on last hidden state\n        x = self.fc(x[:,-1,:])  # --> (batch_size, seq_len, 1)\n        return x \n```", "```py\nmodel = SingleStepRNNModel.from_dataset(\n    training,\n    network_callable=SimpleRNNModel,\n    model_params=model_params,\n    **other_params\n) \n```", "```py\n    trainer = pl.Trainer(\n        auto_select_gpus=True,\n        gpus=-1,\n        min_epochs=1,\n        max_epochs=20,\n        callbacks=[\n            pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4*3),\n            pl.callbacks.ModelCheckpoint(\n                monitor=\"val_loss\", save_last=True, mode=\"min\", auto_insert_metric_name=True\n            ),\n        ],\n        val_check_interval=2000,\n        log_every_n_steps=2000,\n    ) \n    ```", "```py\n    trainer.fit(\n        model,\n        train_dataloaders=train_dataloader,\n        val_dataloaders=val_dataloader,\n    ) \n    ```", "```py\n    best_model_path = trainer.checkpoint_callback.best_model_path\n    best_model = SingleStepRNNModel.load_from_checkpoint(best_model_path) \n    ```", "```py\npred, index = best_model.predict(test, return_index=True, show_progress_bar=True) \n```", "```py\ndef forward(self, x: Dict):\n    # Step 2 in Figure 15.5\n    x_cont = torch.cat([x[\"encoder_cont\"],x[\"decoder_cont\"]], dim=1)\n    # Step 3 in Figure 15.5\n    x_cont[:,:,-1] = torch.roll(x_cont[:,:,-1], 1, dims=1)\n    x = x_cont\n    # Step 4 in Figure 15.5\n    x = x[:,1:,:] # x -> (batch_size, seq_len, input_size)\n    # Processing through the RNN\n    x, _ = self.rnn(x)  # --> (batch_size, seq_len, hidden_size)\n    # Using a FC layer on last hidden state\n    x = self.fc(x[:,-1,:])  # --> (batch_size, seq_len, 1)\n    return x \n```", "```py\ndef __init__(\n    self,\n    rnn_type: str,\n    input_size: int,\n    hidden_size: int,\n    num_layers: int,\n    bidirectional: bool,\n    embedding_sizes = []\n):\n    super().__init__(rnn_type, input_size, hidden_size, num_layers, bidirectional)\n    self.embeddings = torch.nn.ModuleList(\n        [torch.nn.Embedding(card, size) for card, size in embedding_sizes]\n    ) \n```", "```py\n# Finding the cardinality using the categorical encoders in the dataset\ncardinality = [len(training.categorical_encoders[c].classes_) for c in training.categoricals]\n# using the cardinality list to create embedding sizes\nembedding_sizes = [\n    (x, min(50, (x + 1) // 2))\n    for x in cardinality\n] \n```", "```py\ndef forward(self, x: Dict):\n    # Using the encoder and decoder sequence\n    x_cont = torch.cat([x[\"encoder_cont\"],x[\"decoder_cont\"]], dim=1)\n    # Roll target by 1\n    x_cont[:,:,-1] = torch.roll(x_cont[:,:,-1], 1, dims=1)\n    # Combine the encoder and decoder categoricals\n    cat = torch.cat([x[\"encoder_cat\"],x[\"decoder_cat\"]], dim=1)\n    # if there are categorical features\n    if cat.size(-1)>0:\n        # concatenating all the embedding vectors\n        x_cat = torch.cat([emb(cat[:,:,i]) for i, emb in enumerate(self.embeddings)], dim=-1)\n        # concatenating continuous and categorical\n        x = torch.cat([x_cont, x_cat], dim=-1)\n    else:\n        x = x_cont\n    # dropping first timestep\n    x = x[:,1:,:] # x --> (batch_size, seq_len, input_size)\n    # Processing through the RNN\n    x, _ = self.rnn(x)  # --> (batch_size, seq_len, hidden_size)\n    # Using a FC layer on last hidden state\n    x = self.fc(x[:,-1,:])  # --> (batch_size, seq_len, 1)\n    return x \n```", "```py\nn_bins= 10\n# Calculating the length of each LCLid\ncounts = train_df.groupby(\"LCLid\")['timestamp'].count()\n# Binning the counts and renaming\nout, bins = pd.cut(counts, bins=n_bins, retbins=True)\nout = out.cat.rename_categories({\n    c:f\"bin_{i}\" for i, c in enumerate(out.cat.categories)\n}) \n```", "```py\n# TimeSeriesDataset stores a df as the index over which it samples\ndf = training.index.copy()\n# Adding a bin column to it to represent the bins we have created\ndf['bins'] = [f\"bin_{i}\" for i in np.digitize(df[\"count\"].values, bins)]\n# Calculate Weights as inverse counts of the bins\nweights = 1/df['bins'].value_counts(normalize=True)\n# Assigning the weights back to the df so that we have an array of\n# weights in the same shape as the index over which we are going to sample\nweights = weights.reset_index().rename(columns={\"index\":\"bins\", \"bins\":\"weight\"})\ndf = df.merge(weights, on='bins', how='left')\nprobabilities = df.weight.values \n```", "```py\nfrom torch.utils.data import WeightedRandomSampler\nsampler = WeightedRandomSampler(probabilities, len(probabilities)) \n```", "```py\ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0, sampler=sampler) \n```"]