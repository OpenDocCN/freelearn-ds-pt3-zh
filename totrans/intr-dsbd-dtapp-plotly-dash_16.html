<html><head></head><body>
		<div id="_idContainer191">
			<h1 id="_idParaDest-174"><em class="italic"><a id="_idTextAnchor174"/>Chapter 13</em>: Next Steps</h1>
			<p>Welcome to the end of the book, and the beginning of your Dash journey! Even though we covered many topics, use cases, chart types, and interactivity features, the sky is the limit in terms of what you can build with Dash.</p>
			<p>By now, you should be as comfortable building dashboards as you would be in creating presentations. You should be comfortable manipulating and expressing data using a variety of data visualization techniques and chart types.</p>
			<p>But this book just sets you on the path, and there are many things to explore next, so we will cover some ideas and pointers on how to explore the topics we covered in more depth. We will also look at some aspects that weren't covered in the book that you might be interested in exploring.</p>
			<p>The following areas will be covered in this chapter:</p>
			<ul>
				<li>Expanding your data manipulation and preparation skills</li>
				<li>Exploring more data visualization techniques</li>
				<li>Exploring other Dash components</li>
				<li>Creating your own Dash components</li>
				<li>Operationalizing and visualizing machine learning models</li>
				<li>Enhancing performance and using big data tools</li>
				<li>Going large scale with Dash Enterprise</li>
			</ul>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor175"/>Technical requirements</h1>
			<p>We will not be doing any coding or deployment, so there won't be any technical requirements for this chapter. </p>
			<p>You can learn new things and explore them generally from two directions: the top-down approach, where you want to do something, or are required to do something, and the bottom-up approach, where you start with the tools, and you want to explore the possibilities and what you can do with them:</p>
			<ul>
				<li><strong class="bold">Top-down</strong>: Because of a certain requirement or constraint, often you will be required to do something – make something faster, better, or easier, for example. In order to solve these problems, or satisfy some requirement, you are required to learn something new. The value of this approach is mainly its practicality. You know what is useful and what is required, and this helps focus your mind and energy on the solution that you want, which is focused on solving a practical problem. At the same time, if you only focus on practical problems, you might be missing out on new techniques and approaches that you might learn that can make your practical life much easier.</li>
				<li><strong class="bold">Bottom-up</strong>: This is the approach from the other direction. You start by learning something new just for the sake of exploration or curiosity. It could be something big, such as machine learning, or as small as learning about a new parameter in a function that you use every day. This suddenly opens up possibilities in your mind. It also expands your concept of what is possible. This is something that you can do proactively, regardless of the work requirements that you have. The famous quote, "The more I practice, the luckier I get," seems to fit this situation. The benefit of this approach is that you learn things properly and establish a solid theoretical understanding that allows you to be more in control of the techniques at hand. The drawback is that you might get too theoretical and lose touch with reality and forget what is really useful and what's not.</li>
			</ul>
			<p>I find myself alternating between phases where sometimes I spend most of my time focused on solving a particular problem (top-down) and get very practical and produce practical solutions. I then go through a period of stagnation where my imagination doesn't work as much and I'm not that creative. I then slip into a more theoretical mode. Learning new things is very interesting and engaging in this phase. After learning enough new things and having established a good understanding of a certain topic, I find myself getting new ideas and get back to the practical mode, and so on.</p>
			<p>You can see what works for you. Let's now explore some specific topics that you might be interested in on your Dash journey.</p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor176"/>Expanding your data manipulation and preparation skills</h1>
			<p>If you have read any introductory text on data science, you will have probably been told that data scientists spend the majority of their time cleaning data, reformatting it, and reshaping it.</p>
			<p>As you have read this book, you will have probably seen this in action!</p>
			<p>We saw several times how much code and mental effort, and importantly, domain knowledge, goes into just getting our data into a certain format. Once we have our data in a standardized format, for example, a long form (tidy) DataFrame, then our lives become easier.</p>
			<p>You might want to learn <a id="_idIndexMarker645"/>more pandas and NumPy for a more complete set of techniques on reshaping your data however you want. As mentioned at the beginning of the chapter, learning new pandas techniques without a practical purpose in <a id="_idIndexMarker646"/>mind can help a lot in expanding your imagination. Learning regular expressions can help a lot in text analysis, because text is typically unstructured, and finding and extracting certain patterns can help a lot in your data cleaning process. Statistical and numeric techniques can definitely make a big difference. At the end of the day, we are basically crunching numbers here.</p>
			<p>Improving data manipulation skills naturally leads us to easier and better visualization.</p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor177"/>Exploring more data visualization techniques</h1>
			<p>We saw how easy it is to <a id="_idIndexMarker647"/>work with Plotly Express and how powerful it can be. We also saw the extensive options available for us. At the same time, we are constrained by the requirement to have our data in a certain format, which Plotly Express cannot help with. This is where we have to step in as data scientists.</p>
			<p>We covered four main chart types, and this is a very small subset of what's available. As mentioned at the beginning of the chapter, visualization works in a couple of ways. You might be required to produce a certain chart, so you end up having to learn about it. Or, you might learn about a new chart, and it then inspires you to better summarize certain types of data for certain use cases.</p>
			<p>You might learn about new types of charts based on the geometric shapes/attributes they use, such as pie charts or dot plots. You can also explore them based on their usage; for example, there are statistical and financial charts. Many chart types boil down to basic shapes, such as dots, circles, rectangles, lines, and so on. The way they are displayed and combined makes them distinctive.</p>
			<p>Another interesting <a id="_idIndexMarker648"/>visualization technique is using sub-plots. While we extensively used faceting in the book, facets are basically the same visualization for multiple subsets of the data we are analyzing. Sub-plots, on the other hand, allow you to create arrays of plots that could be independent of one another. This can help you produce rich reports in a single chart, where each sub-plot conveys a different aspect of your data.</p>
			<p>Having explored and mastered new visualization techniques and charts, you will probably want to put them in an app and make them interactive.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor178"/>Exploring other Dash components</h1>
			<p>We covered the basic <a id="_idIndexMarker649"/>Dash components and there are many others available. Keep in mind that there are three possible approaches here: </p>
			<ul>
				<li><strong class="bold">Go deeper into components you know</strong>: Although we covered many components, there is always more to explore – more options to learn, use, and gain experience in. You probably want to go deeper into <strong class="source-inline">dash_table</strong>, which can offer quite complex functionality and spreadsheet-style options.</li>
				<li><strong class="bold">Explore other Dash components</strong>: There are several other components that we didn't cover that could enhance your apps. Two of those are <strong class="source-inline">DatePickerSingle</strong> and <strong class="source-inline">DatePickerRange</strong>, which are self-explanatory. The <strong class="source-inline">Interval</strong> component allows you to execute code whenever a certain period of time elapses. The <strong class="source-inline">Store</strong> component allows you to store data in the user's browser, in case you want to save some data to enhance the usability/functionality of your apps. There is also an <strong class="source-inline">Upload</strong> component for uploading files. These are all available under Dash Core Components. There are several other packages that are interesting for other use cases. For example, Dash Cytoscape is great for interactive graph (network) visualizations. We saw it several times when we used the visual debugger and saw how much it simplifies our understanding of our app. This has many applications for various industries. To enable users to draw on your charts, you can check out the image annotation options that Dash provides, as well as the Dash Canvas package. Together, they provide a wide array of options to let users literally draw on the charts with their mouse, using set shapes such as rectangles, or by simply dragging the mouse.</li>
				<li><strong class="bold">Explore some of the community components</strong>: Since Dash is an open source project and has a mechanism for creating and incorporating new components, many <a id="_idIndexMarker650"/>people have created their own Dash components independently. One of those is Dash Bootstrap Components, which we have relied on in our work. There are many more and many new ones coming out all the time.</li>
			</ul>
			<p>This takes us to another topic, which is creating your own Dash component. </p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor179"/>Creating your own Dash component</h1>
			<p>It's interesting to know that the <a id="_idIndexMarker651"/>official strategy for Dash is to be "React for Python, R, and Julia." As you might know, React is a very big JavaScript framework for building user interfaces. There is a massive library of open source React components, and Dash Core Components are basically React components made available in Python. This means that if there is any functionality that is not provided by Dash that you would like to have, you might consider developing it yourself, hiring a developer to build it, or you can also sponsor its development and have the Plotly team build it. Some of the components that we worked with were sponsored by clients who wanted to have certain functionality that wasn't available. This is one way to support Dash as well. It also benefits everyone who uses open source Dash.</p>
			<p>There are clear instructions on how to create your own Dash components, and as a Dash developer, it's good to explore this option. It will certainly give you a deeper understanding of how the library works, and maybe you will end up creating a popular component yourself!</p>
			<p>With all the data manipulation, visualization, and components, you have a rich vocabulary to do things beyond <a id="_idIndexMarker652"/>plotting points on a chart. Exploring what can be done with machine learning can give your models a big boost and can make them usable by others.</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor180"/>Operationalizing and visualizing machine learning models</h1>
			<p>Machine learning and deep learning are <a id="_idIndexMarker653"/>completely separate topics, of course, but with all the previously mentioned skills, you can take your machine learning to a new level. At the end of the day, you will use charts to express certain ideas <a id="_idIndexMarker654"/>about your data, and with a good interactive data visualization vocabulary, you can give your users many options to test different models and tune hyperparameters.</p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor181"/>Enhancing performance and using big data tools</h1>
			<p>This is a very important topic, and we always need to make sure that our apps perform at an acceptable level. We didn't tackle this in the book because the focus was mainly to learn how to create a Dash app with all the other details that make it work. We also worked with a very small dataset of a few megabytes. Still, even with a small dataset, it can be crucial to optimize it. Big data can be about handling a massive file, or it can be about a small file that needs to be handled a massive number of times.</p>
			<p>These are some things that can be done to optimize performance, but big data is a separate topic altogether, so here are some hints and some areas to explore.</p>
			<p>Once we know how our app will <a id="_idIndexMarker655"/>behave and what features we will be using, we can clean up some unnecessary code and data that might be hindering our app's performance. Here are some ideas that can be done immediately to our app:</p>
			<ul>
				<li><strong class="bold">Load the necessary data only</strong>: We loaded the whole file, and for each callback, we queried the DataFrame separately. That can be wasteful. If we have a callback for population data only, for example, we can create a separate file (and then a separate subset) DataFrame that only contains relevant columns and query them only, instead of using the whole DataFrame.</li>
				<li><strong class="bold">Optimize data types</strong>: Sometimes you <a id="_idIndexMarker656"/>need to load data that contains the same values repeated many times. For example, the poverty dataset contains many repetitions of country names. We can use the pandas categorical data type to optimize those values:</li>
			</ul>
			<ol>
				<li>Load the <strong class="source-inline">sys</strong> module and see the difference in size in bytes for a string (a country name) and an integer: <p class="source-code">import sys</p><p class="source-code">sys.getsizeof('Central African Republic')</p><p class="source-code"><strong class="bold"> 73</strong></p></li>
				<li>Get the size of an integer value:<p class="source-code">sys.getsizeof(150)</p><p class="source-code"><strong class="bold"> 28</strong></p></li>
				<li>We can see the big difference in size with the string taking almost three times the memory that an integer does. This is what the categorical data type basically does. It creates a dictionary mapping each unique value to an integer. It then uses integers to encode and represent those values, and you can imagine how much space this can save.</li>
				<li>Load the poverty dataset:<p class="source-code">import pandas as pd</p><p class="source-code">poverty = pd.DataFrame('data/poverty.csv')</p></li>
				<li>Get a subset containing the country names column and check its memory usage: <p class="source-code"><strong class="bold">poverty[['Country Name']].info()</strong></p><p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p><p class="source-code">RangeIndex: 8287 entries, 0 to 8286</p><p class="source-code">Data columns (total 1 columns):</p><p class="source-code">  #   Column        Non-Null Count  Dtype </p><p class="source-code">---  ------        --------------  ----- </p><p class="source-code"> 0   Country Name  8287 non-null   object</p><p class="source-code">dtypes: object(1)</p><p class="source-code"><strong class="bold"> memory usage: 64.9+ KB</strong></p></li>
				<li>Convert the column to the <a id="_idIndexMarker657"/>categorical data type and check the memory usage:<p class="source-code"><strong class="bold">poverty['Country Name'].astype('category').to_frame().info()</strong></p><p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p><p class="source-code">RangeIndex: 8287 entries, 0 to 8286</p><p class="source-code">Data columns (total 1 columns):</p><p class="source-code"> #   Column        Non-Null Count  Dtype   </p><p class="source-code">---  ------        --------------  -----   </p><p class="source-code"> 0   Country Name  8287 non-null   category</p><p class="source-code">dtypes: category(1)</p><p class="source-code"><strong class="bold">memory usage: 21.8 KB</strong></p></li>
				<li>With a simple command that encoded our countries as integers, we reduced the memory usage from 64.9 KB to 21.8 KB, making it about a third of the original size.</li>
				<li>Another thing you might <a id="_idIndexMarker658"/>want to consider is to learn more about the available big data technologies and techniques. One of the <a id="_idIndexMarker659"/>most important projects right now is the Apache Arrow project. It is a collaboration between leaders from the database community, as well as from the data science community. One of the most important objectives of the project is to unify the effort across disciplines, and crucially, across programming languages.</li>
				<li>When you want to read a CSV file, for example, you want it represented in memory as a DataFrame. Whether you are using R or Python, or any other language, you will be running very similar operations, such as sorting, selecting, filtering, and so on. There is a lot of effort duplicated, where each language implements its own DataFrame spec. From a performance perspective, it has been observed that a large percentage of computing power is wasted on converting <a id="_idIndexMarker660"/>objects from one language to another and reading and writing. This can also happen while saving an object to disk and then opening it in another language. This can cause a waste of resources, and in many cases forces many teams to have to choose a single language for easier communication and to reduce the wasted time and effort.</li>
				<li>One of the goals of the <a id="_idIndexMarker661"/>Apache Arrow project is to create a single in-memory representation for data objects such as the DataFrame. This way, objects can be passed around across programming languages without having to make any conversions. You can imagine how much easier things can become. Also, there are big wins due to the collaborations across the programming languages and disciplines where a single specification is being used and maintained.</li>
				<li>Each programming language can then implement its own libraries that are based on a single spec. For Python, the package is <strong class="source-inline">pyarrow</strong>, which is very interesting to explore. In many instances, it can be used on its own, and in others, it can integrate with pandas.</li>
				<li>A very interesting file format that is also part of the project is the <strong class="source-inline">parquet</strong> format. Just like CSV and JSON, <strong class="source-inline">parquet</strong> is language-agnostic. It is a file and can be opened with any language that has a <strong class="source-inline">parquet</strong> reader. And the good news is that pandas already supports it.</li>
				<li>One of the important features of <strong class="source-inline">parquet</strong> is massive compression that can reduce the size of your files drastically. So, it is ideal for long-term storage and efficiently utilizing space. Not only that, but it is very efficient to open and read those files because of the format. The file contains metadata about the file, as well as the schema. Files are also arranged into separate structures for efficient reading. </li>
				<li>Some <a id="_idIndexMarker662"/>techniques that <strong class="source-inline">parquet</strong> uses are as follows: <ul><li><strong class="bold">Column orientation</strong>: In contrast to CSV, for example, which is a row-oriented format, <strong class="source-inline">parquet</strong> mainly stores <a id="_idIndexMarker663"/>data in columns. Row-oriented formats are suitable for transactional processing. For example, when a user logs in to a website, we need to retrieve data about the user (a row), and potentially we want to write and update that row based on that user's interactions. But in analytical processing, which is what we are interested in, if we want to analyze the average income per country, for example, we only need to read two columns from our dataset. If the data was arranged in columns, then we can jump from the beginning to the end of the column and extract it much faster than if it was a row. We can read other columns, but only if we want to do further analysis on them.</li><li><strong class="bold">Encoding</strong>: Just like the example <a id="_idIndexMarker664"/>we saw with pandas categorical type, <strong class="source-inline">parquet</strong> also performs dictionary encoding. There are several other encoding schemes that it uses. For example, there is delta encoding, which works best when you have large numbers. It saves the value of the first number in the column and only saves the difference between it and consecutive numbers. For example, if you had the following list: [1,000,000, 1,000,001, 1,000,002, 1,000,003], these numbers could be represented as [1,000,000, 1, 1, 1]. We saved the full value of the first element and only the difference between each element and the previous one. This can mean a lot of memory is saved. This can be really useful when using timestamps, for example, which can be represented as large integers with small differences between them, especially in time series data. When you want to read the list, the program can do the calculations and give you the original numbers. Several other encoding strategies are used, but this was just another example.</li><li><strong class="bold">Partitioning</strong>: Another interesting technique is that <strong class="source-inline">parquet</strong> can split a file into multiple files and read and merge them from a folder that contains those files. Imagine a file with data <a id="_idIndexMarker665"/>about people, and this file has 10 million rows. One of the columns could be for gender, with "male" and "female" values. Now, if we split the file in two, with one for each value, we don't even need to store the whole column anymore. We can simply include "female" in the filename, and if the column is requested, the program knows how to populate all the values for that column because they are all the same.</li><li><strong class="bold">Summary statistics</strong>: Another technique that <strong class="source-inline">parquet</strong> uses for the groups of data in columns is that it contains the <a id="_idIndexMarker666"/>minimum and maximum and some other statistics for each group. Imagine having a file with 10 million rows, and you want to read only the values that are between 10 and 20. Assume that those rows are split into groups of one million each. Now, each group would have its minimum and maximum values in the header of that group. While scanning, if you come across a group where the maximum is six, then you know that your requested values won't be in that group. You just skipped a million values by making a single comparison. </li></ul></li>
			</ol>
			<p>Now, if you have mastered all those techniques and can produce insightful visualizations, nice interactivity, and provide really helpful dashboards, you still might not have the experience (or desire) to handle Dash in large-scale deployments. This is where you might consider Dash Enterprise.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor182"/>Going large scale with Dash Enterprise</h1>
			<p>When you have a deployment in a <a id="_idIndexMarker667"/>large organization with many users that has an existing infrastructure, several things might come up that you might not have considered or anticipated. Imagine your app is going to be used by hundreds of people in a company. How do you deal with access to the app? How do you manage passwords, and what exactly happens when someone resigns, or joins? Are you experienced enough in security that you are confident that you can handle such a large deployment?</p>
			<p>In these cases, data engineering takes on a much bigger role than previously. Storing the data efficiently and securely becomes more important. Scalability and managing it can be tricky if it is not your area of expertise. Your main job is to design and create something that helps in finding insights, rather than maintaining large-scale apps. In some cases, you might have the required skills but don't want to worry about those things and mainly want to focus on the interface, the models, and the visualizations.</p>
			<p>This is where <a id="_idIndexMarker668"/>Dash Enterprise can help. It's basically Dash, as you know it, but with many options that are specifically designed for large deployments. </p>
			<p>Dash Enterprise also offers a full online workbench with the popular IDEs and notebooks, so you can also collaborate online with co-workers. This might help if a large number of users are working together.</p>
			<p>Professional services are also provided to large customers, in which case you get access to the people who built Dash and have worked with many organizations who have gone through many experiences similar to your own, and you can get the help you need from them in many important areas.</p>
			<p>These were some ideas, but your creativity, domain knowledge, and hard work are what matter at the end, so let's summarize what we covered in this chapter and the book.</p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor183"/>Summary</h1>
			<p>We started by focusing on the importance of basic skills in handling data. We emphasized the importance of mastering data manipulation and cleaning skills, which will allow you to format your data into the shape that you want and make it easy to analyze and visualize. We also mentioned various data visualization techniques and types of charts that can be explored to increase your fluency in expressing ideas visually.</p>
			<p>We also discussed the other Dash components that we didn't cover in the book, as well as the community components that are constantly being developed. Eventually, you might decide to develop your own set of components and contribute additional functionality to the Dash ecosystem, and I look forward to <strong class="source-inline">pip install dash-your-components</strong>!</p>
			<p>We then discussed exploring machine learning and how we can make our models visual and interactive. Establishing data manipulation, visualization, and interactivity skills will help you a lot in making your models interpretable and usable, especially for a non-technical audience.</p>
			<p>We explored some big data options and discussed one of the important projects, although there are many more to consider and explore. </p>
			<p>Finally, we talked about the paid enterprise solution that Dash offers, which is Dash Enterprise; this solution might make sense when your project is part of a large organization or deployment.</p>
			<p>Thank you very much for reading, and I hope you enjoyed the book. I look forward to seeing your app deployed online, with your own design, models, options, and customizations.</p>
		</div>
	</body></html>