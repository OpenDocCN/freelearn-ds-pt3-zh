- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automate Data Cleaning with User-Defined Functions, Classes, and Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of great reasons to write code that is reusable. When we
    step back from the particular data-cleaning problem at hand and consider its relationship
    to very similar problems, we can actually improve our understanding of the key
    issues involved. We are also more likely to address a task systematically when
    we set our sights more on solving it for the long term than on the before-lunch
    solution. This has the additional benefit of helping us to disentangle the substantive
    issues from the mechanics of data manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: We will create several modules to accomplish routine data-cleaning tasks in
    this chapter. The functions and classes in these modules are examples of code
    that can be reused across DataFrames, or for one DataFrame over an extended period
    of time. These functions handle many of the tasks we discussed in the first eleven
    chapters, but in a manner that allows us to reuse our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the recipes in this chapter cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Functions for getting a first look at our data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functions for displaying summary statistics and frequencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functions for identifying outliers and unexpected values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functions for aggregating or combining data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classes that contain the logic for updating Series values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classes that handle non-tabular data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functions for checking overall data quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pre-processing data with pipelines: a simple example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pre-processing data with pipelines: a more complicated example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need pandas, NumPy, and Matplotlib to complete the recipes in this
    chapter. I used pandas 2.1.4, but the code will run on pandas 1.5.3 or later.
  prefs: []
  type: TYPE_NORMAL
- en: The code in this chapter can be downloaded from the book’s GitHub repository,
    [https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Data-Cleaning-Cookbook-Second-Edition).
  prefs: []
  type: TYPE_NORMAL
- en: Functions for getting a first look at our data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first few steps we take after we import our data into a pandas DataFrame
    are pretty much the same regardless of the characteristics of the data. We almost
    always want to know the number of columns and rows and the column data types,
    and to see the first few rows. We also might want to view the index and check
    whether there is a unique identifier for DataFrame rows. These discrete, easily
    repeatable tasks are good candidates for a collection of functions we can organize
    into a module.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create a module with functions that give us a good first
    look at any pandas DataFrame. A module is simply a collection of Python code that
    we can import into another Python program. Modules are easy to reuse because they
    can be referenced by any program with access to the folder where the module is
    saved.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create two files in this recipe: one with a function we will use to look
    at our data and another to call that function. Let’s call the file with the function
    we will use `basicdesciptives.py` and place it in a subfolder called `helperfunctions`.'
  prefs: []
  type: TYPE_NORMAL
- en: We work with the **National Longitudinal Surveys** (**NLS**) data in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: The NLS, administered by the United States Bureau of Labor Statistics, is a
    collection of longitudinal surveys of individuals who were in high school in 1997
    when the surveys started. Participants were surveyed each year through 2023\.
    The surveys are available for public use at [nlsinfo.org](https://nlsinfo.org).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will create a function to take an initial look at a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Create the `basicdescriptives.py` file with the function we want.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `getfirstlook` function will return a dictionary with summary information
    on a DataFrame. Save the file in the `helperfunctions` subfolder as `basicdescriptives.py`.
    (You can also just download the code from the GitHub repository.) Also, create
    a function (`displaydict`) to pretty up the display of a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Create a separate file, `firstlook.py`, to call the `getfirstlook` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `pandas`, `os`, and `sys` libraries, and load the NLS data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Import the `basicdescriptives` module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, append the `helperfunctions` subfolder to the Python path. We can then
    import `basicdescriptives`. We use the same name as the name of the file to import
    the module. We create an alias, `bd`, to make it easier to access the functions
    in the module later. (We can use `importlib`, commented out here, if we need to
    reload `basicdescriptives` because we have made some changes in the code in that
    module.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Take a first look at the NLS data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can just pass the DataFrame to the `getfirstlook` function in the `basicdescriptives`
    module to get a quick summary of the NLS data. The `displaydict` function gives
    us prettier printing of the dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Pass values to the `nrows` and `uniqueids` parameters of `getfirstlook`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The two parameters default to values of 5 and `None` respectively, unless we
    provide values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Work with some of the returned dictionary keys and values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can also display selected key values from the dictionary returned from `getfirstlook`.
    Show the number of rows and data types, and check to see whether each row has
    a `uniqueid` instance (`dfinfo[''nrows''] == dfinfo[''uniqueids'']`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a closer look at how the function works and how we call it.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost all of the action in this recipe is in the `getfirstlook` function, which
    we look at in *step 1*. We place the `getfirstlook` function in a separate file
    that we name `basicdescriptives.py`, which we can import as a module with that
    name (minus the extension).
  prefs: []
  type: TYPE_NORMAL
- en: We could have typed the function into the file we were using and called it from
    there. By putting it in a module instead, we can call it from any file that has
    access to the folder where the module is saved. When we import the `basicdescriptives`
    module in *step 3*, we load all of the code in `basicdescriptives`, allowing us
    to call all functions in that module.
  prefs: []
  type: TYPE_NORMAL
- en: The `getfirstlook` function returns a dictionary with useful information about
    the DataFrame that is passed to it. We see the first five rows, the number of
    columns and rows, the data types, and the index. By passing a value to the `uniqueid`
    parameter, we also get the number of unique values for the column.
  prefs: []
  type: TYPE_NORMAL
- en: By adding keyword parameters (`nrows` and `uniqueid`) with default values, we
    improve the flexibility of `getfirstlook`, without increasing the amount of effort
    it takes to call the function when we do not need the extra functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In the first call, in *step 4*, we do not pass values for `nrows` or `uniqueid`,
    sticking with the default values. In *step 5*, we indicate that we only want two
    rows displayed and that we want to examine unique values for `originalid`.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The point of this recipe, and the ones that follow it, is not to provide code
    that you can download and run on your own data, though you are certainly welcome
    to do that. I am mainly trying to demonstrate how you can collect your favorite
    approaches to data cleaning in handy modules, and how this allows for easy code
    reuse. The specific code here is just a serving suggestion, if you will.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we use a combination of positional and keyword parameters, the positional
    parameters must go first.
  prefs: []
  type: TYPE_NORMAL
- en: Functions for displaying summary statistics and frequencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the first few days of working with a DataFrame, we try to get a good
    sense of the distribution of continuous variables and counts for categorical variables.
    We also often do counts by selected groups. Although pandas and NumPy have many
    built-in methods for these purposes—`describe`, `mean`, `valuecounts`, `crosstab`,
    and so on—data analysts often have preferences for how they work with these tools.
    If, for example, an analyst finds that they usually need to see more percentiles
    than those generated by `describe`, they can use their own function instead. We
    will create user-defined functions for displaying summary statistics and frequencies
    in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be working with the `basicdescriptives` module again in this recipe.
    All of the functions we will define are saved in that module. We will continue
    to work with the NLS data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use functions we create to generate summary statistics and counts:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the `gettots` function in the `basicdescriptives` module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The function takes a pandas DataFrame and creates a dictionary with selected
    summary statistics. It returns a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Import the `pandas`, `os`, and `sys` libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Do this from a different file, which you can call `taking_measure.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `basicdescriptives` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Show summary statistics for continuous variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `gettots` function from the `basicdescriptives` module that we created
    in *step 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Create a function to count missing values by columns and rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `getmissings` function will take a DataFrame and a parameter for showing
    percentages or counts. It returns two Series, one with the missing values for
    each column and the other with missing values by row. Save the function in the
    `basicdescriptives` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Call the `getmissings` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Call it first with `byrowperc` (the second parameter) set to `True`. This will
    show the percentage of rows with the associated number of missing values. For
    example, the `missingbyrows` value shows that 73% of rows have 0 missing values
    for `weeksworked20` and `weeksworked21`. Call it again, leaving `byrowperc` at
    its default value of `False`, to get counts instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Create a function to calculate frequencies for all categorical variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `makefreqs` function loops through all columns with the category data type
    in the passed DataFrame, running `value_counts` on each one. The frequencies are
    saved to the file indicated by `outfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Call the `makefreqs` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First change the data type of each object column to `category`. This call runs
    `value_counts` on category data columns in the NLS DataFrame and saves the frequencies
    to `nlsfreqs.txt` in the `views` subfolder of the current folder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Create a function to get counts by groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `getcnts` function counts the number of rows for each combination of column
    values in `cats`, a list of column names. It also counts the number of rows for
    each combination of column values excluding the final column in `cats`. This provides
    a total across all values of the final column. (The next step shows what this
    looks like.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Pass the `maritalstatus` and `colenroct00` columns to the `getcnts` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This returns a DataFrame with counts for each column value combination, as
    well as counts for all combinations excluding the last column. This is used to
    calculate percentages within groups. For example, 669 respondents were divorced
    and 560 of those (or 84%) were not enrolled in college in October 2000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `rowsel` parameter of `getcnts` to limit the output to specific rows.
    This will show only the not-enrolled-in-college rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These steps demonstrate how to create functions and use them to generate summary
    statistics and frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *step 1*, we created a function, `gettots`, that calculated descriptive statistics
    for all columns in a DataFrame, returning those results in a summary DataFrame.
    Most of the statistics can be generated with the `describe` method, but we add
    a few statistics—the 15^(th) percentile, the 85^(th) percentile, and the interquartile
    range. We call that function twice in *step 4*, the first time for the SAT verbal
    and math scores and the second time for all weeks worked columns.
  prefs: []
  type: TYPE_NORMAL
- en: '*Steps 5* and *6* create and call a function that shows the number of missing
    values for each column in the passed DataFrame. The function also counts missing
    values for each row, displaying the frequency of missing values. The frequency
    of missing values by row can also be displayed as a percentage of all rows by
    passing a value of `True` to the `byrowperc` parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Steps 7* and *8* produce a text file with frequencies for all categorical
    variables in the passed DataFrame. We just loop through all columns with the category
    data type and run `value_counts`. Since often the output is long, we save it to
    a file. It is also good to have frequencies saved somewhere for later reference.'
  prefs: []
  type: TYPE_NORMAL
- en: The `getcnts` function we create in *step 9* and call in *steps 10* and *11*
    is a tad idiosyncratic. pandas has a very useful `crosstab` function, which I
    use frequently. But I often need a no-fuss way to look at group counts and percentages
    for subgroups within groups. The `getcnts` function does that.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A function can be very helpful even when it does not do very much. There is
    not much code in the `getmissings` function, but I check for missing values so
    frequently that the small time savings are significant cumulatively. It also reminds
    me to check for missing values by column and by row.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We explore pandas’ tools for generating summary statistics and frequencies in
    *Chapter 3*, *Taking the Measure of Your Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Functions for identifying outliers and unexpected values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If I had to pick one data-cleaning area where I find reusable code most beneficial,
    it would be in the identification of outliers and unexpected values. This is because
    our prior assumptions often lead us to the central tendency of a distribution,
    rather than to the extremes. Quickly—think of a cat. Unless you were thinking
    about a particular cat in your life, an image of a generic feline between 8 and
    10 pounds probably came to mind; not one that is 6 pounds or 22 pounds.
  prefs: []
  type: TYPE_NORMAL
- en: We often need to be more deliberate to elevate extreme values to consciousness.
    This is where having a standard set of diagnostic functions to run on our data
    is very helpful. We can run these functions even if nothing in particular triggers
    us to run them. This recipe provides examples of functions that we can use regularly
    to identify outliers and unexpected values.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will create two files in this recipe, one with the functions we will use
    to check for outliers and another with the code we will use to call those functions.
    Let’s call the file with the functions we will use `outliers.py`, and place it
    in a subfolder called `helperfunctions`.
  prefs: []
  type: TYPE_NORMAL
- en: You will need the `matplotlib` and `scipy` libraries, in addition to pandas,
    to run the code in this recipe. You can install `matplotlib` and `scipy` by entering
    `pip install matplotlib` and `pip install scipy` in a Terminal client or in Windows
    PowerShell. You will also need the `pprint` utility, which you can install with
    `pip install pprint`.
  prefs: []
  type: TYPE_NORMAL
- en: We will work with the NLS and COVID-19 data in this recipe. The COVID-19 data
    has one row per country, with cumulative cases and deaths for that country.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: Our World in Data provides COVID-19 public use data at [https://ourworldindata.org/covid-cases](https://ourworldindata.org/covid-cases).
    The dataset includes total cases and deaths, tests administered, hospital beds,
    and demographic data such as median age, gross domestic product, and diabetes
    prevalence. The dataset used in this recipe was downloaded on March 3, 2024.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create and call functions to check the distribution of variables, list extreme
    values, and visualize a distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `pandas`, `os`, `sys`, and `pprint` libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Also, load the NLS and COVID-19 data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Create a function to show some important properties of a distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `getdistprops` function takes a Series and generates measures of central
    tendency, shape, and spread. The function returns a dictionary with these measures.
    It also handles situations where the Shapiro test for normality does not return
    a value. It will not add keys for `normstat` and `normpvalue` when that happens.
    Save the function in a file named `outliers.py` in the `helperfunctions` subfolder
    of the current directory. (Also load the `pandas`, `matplotlib`, `scipy`, and
    `math` libraries we will need for this and other functions in this module.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Pass the total cases per million in population series to the `getdistprops`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `skew` and `kurtosis` values suggest that the distribution of `total_cases_pm`
    has a positive skew and shorter tails than a normally distributed variable. The
    Shapiro test of normality (`normpvalue`) confirms this. (Use `pprint` to improve
    the display of the dictionary returned by `getdistprops`.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Create a function to list the outliers in a DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `getoutliers` function iterates over all columns in `sumvars`. It determines
    outlier thresholds for those columns, setting them at 1.5 times the interquartile
    range (the distance between the first and third quartiles) below the first quartile
    or above the third quartile. It then selects all rows with values above the high
    threshold or below the low threshold. It adds columns that indicate the variable
    examined (`varname`) for outliers and the threshold levels. It also includes columns
    in the `othervars` list in the DataFrame it returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Call the `getoutlier` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass a list of columns to check for outliers (`sumvars`) and another list of
    columns to include in the returned DataFrame (`othervars`). Show the count of
    outliers for each variable and view the outliers for SAT math:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Create a function to generate histograms and boxplots.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `makeplot` function takes a Series, title, and label for the *x*-axis.
    The default plot is set as a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the `makeplot` function to create a histogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Frequencies of SAT math values'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `makeplot` function to create a boxplot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following boxplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18596_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: Show the median, interquartile range, and outlier thresholds with
    a boxplot'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding steps show how we can develop reusable code to check for outliers
    and unexpected values.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by getting the key attributes of a distribution, including the mean,
    median, standard deviation, skew, and kurtosis. We do this by passing a Series
    to the `getdistprop` function in *step 3*, getting back a dictionary with these
    measures.
  prefs: []
  type: TYPE_NORMAL
- en: The function in *step 4* selects rows where one of the columns in `sumvars`
    has a value that is an outlier. It also includes the values for the columns in
    `othervars` and the threshold amounts in the DataFrame it returns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a function in *step 6* that makes it easier to create a simple histogram
    or boxplot. The functionality of `matplotlib` is great, but it can take a minute
    to remind ourselves of the syntax when we just want to create a simple histogram
    or boxplot. We can avoid that by defining a function with a few routine parameters:
    Series, title, and *x*-label. We call that function in *steps 7* and *8*.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We do not want to do too much work with a continuous variable before getting
    a good sense of how its values are distributed; what is the central tendency and
    shape of the distribution? If we run something like the functions in this recipe
    for key continuous variables, we would be off to a good start.
  prefs: []
  type: TYPE_NORMAL
- en: The relatively painless portability of Python modules makes this pretty easy
    to do. If we wanted to use the `outliers` module that we used in this example,
    we would just need to save the `outliers.py` file to a folder that our program
    can access, add that folder to the Python path, and import it.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, when we are inspecting an extreme value, we want to have a better idea
    of the context of other variables that might explain why the value is extreme.
    For example, a height of 178 centimeters is not an outlier for an adult male,
    but it definitely is for a 9-year-old. The DataFrame produced in *steps 4* and
    *5* provides us with both the outlier values and other data that might be relevant.
    Saving the data to an Excel file makes it easy to inspect outlier rows later or
    share that data with others.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We go into a fair bit of detail on detecting outliers and unexpected values
    in *Chapter 4*, *Identifying Outliers in Subsets of Data*. We examine histograms,
    boxplots, and many other visualizations in *Chapter 5*, *Using Visualizations
    for the Identification of Unexpected Values*.
  prefs: []
  type: TYPE_NORMAL
- en: Functions for aggregating or combining data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most data analysis projects require some reshaping of data. We may need to aggregate
    by group or combine data vertically or horizontally. We have to do similar tasks
    each time we prepare our data for this reshaping. We can routinize some of these
    tasks with functions, improving both the reliability of our code and our efficiency
    in getting the work done. We sometimes need to check for mismatches in merge-by
    columns before doing a merge, check for unexpected changes in values in panel
    data from one period to the next before aggregating, or concatenate a number of
    files at once and verify that data has been combined accurately.
  prefs: []
  type: TYPE_NORMAL
- en: These are just a few examples of the kind of data aggregation and combining
    tasks that might lend themselves to a more generalized coding solution. In this
    recipe, we define functions that can help with these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will work with the COVID-19 daily data in this recipe. This data comprises
    new cases and new deaths for each country by day. We will also work with land
    temperature data for several countries in 2023\. The data for each country is
    in a separate file and has one row per weather station in that country for each
    month.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: The land temperature DataFrame has the average temperature readings (in °C)
    in 2023 from over 12,000 stations across the world, though a majority of the stations
    are in the United States. The raw data was retrieved from the Global Historical
    Climatology Network integrated database. It is made available for public use by
    the United States National Oceanic and Atmospheric Administration at [https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-monthly).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use functions to aggregate data, combine data vertically, and check
    merge-by values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas`, `os`, and `sys` libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a function (`adjmeans`) to aggregate values by period for a group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sort the values in the passed DataFrame by group (`byvar`) and then `period`.
    Convert the DataFrame values to a NumPy array. Loop through the values, do a running
    tally of the `var` column, and set the running tally back to 0 when you reach
    a new value for `byvar`. Before aggregating, check for extreme changes in values
    from one period to the next. The `changeexclude` parameter indicates the size
    of a change from one period to the next that should be considered extreme. The
    `excludetype` parameter indicates whether the `changeexclude` value is an absolute
    amount or a percentage of the `var` column’s mean. Save the function in a file
    called `combineagg.py` in the `helperfunctions` subfolder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `combineagg` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Call the `adjmeans` function to summarize panel data by group and time period.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Indicate that we want a summary of `new_cases` by `location`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the `adjmeans` function again, this time excluding values where `new_cases`
    goes up or down by more than 5,000 from one day to the next. Notice some reduction
    in the counts for some countries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a function to check values for merge-by columns on one file but not another.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `checkmerge` function does an outer join of two DataFrames passed to it,
    using the third and fourth parameters for the merge-by columns for the first and
    second DataFrames respectively. It then does a crosstab that shows the number
    of rows with merge-by values in both DataFrames and those in one DataFrame but
    not the other. It also shows up to 20 rows of data for merge-by values found in
    just one file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Call the `checkmerge` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check a merge between the `countries` land temperatures DataFrame (which has
    one row per country) and the `locations` DataFrame (which has one row for each
    weather station in each country). The crosstab shows that 27,472 merge-by column
    values are in both DataFrames, two are in the `countries` file and not in the
    `locations` file, and one is in the `locations` file but not the `countries` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Create a function that concatenates all CSV files in a folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This function loops through all of the filenames in the specified folder. It
    uses the `endswith` method to check that the filename has a CSV file extension.
    It then loads the DataFrame and prints out the number of rows. Finally, it uses
    `concat` to append the rows of the new DataFrame to the rows already appended.
    If column names on a file are different, it prints those column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Use the `addfiles` function to concatenate all of the `countries` land temperature
    files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It looks like the file for Oman (`ltoman`) is slightly different. It does not
    have the `latabs` column. Notice that the counts for each country in the combined
    DataFrame match the number of rows for each country file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The preceding steps demonstrate how we can systematize some of our messy data
    reshaping work. I am sure you can think of a number of other functions that might
    be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that in the `adjmeans` function we define in *step 2*,
    we actually do not append our summary of the `var` column values until we get
    to the next `byvar` column value. This is because there is no way to tell that
    we are on the last row for any `byvar` value until we get to the next `byvar`
    value. That is not a problem because we append the summary to `rowlist` right
    before we reset the value to `0`. This also means that we need to do something
    special to output the totals for the last `byvar` value since no next `byvar`
    value is reached. We do this with a final append after the loop is complete.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 5*, we call the `adjmeans` function we defined in *step 2*. Since we
    do not set a value for the `changeexclude` parameter, the function will include
    all values in the aggregation. This will give us the same results as we would
    get using `groupby` with an aggregation function. When we pass an argument to
    `changeexclude`, however, we determine which rows to exclude from the aggregation.
    In *step 6*, the fifth argument in the call to `adjmeans` indicates that we should
    exclude new case values that are more than 5,000 cases higher or lower than the
    value for the previous day.
  prefs: []
  type: TYPE_NORMAL
- en: The function in *step 9* works well when the data files to be concatenated have
    the same, or nearly the same, structure. We print an alert when the column names
    are different, as *step 10* shows. The `latabs` column is not in the Oman file.
    This means that in the concatenated file, `latabs` will be missing for all of
    the rows for Oman.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `adjmeans` function does a fairly straightforward check of each new value
    to be aggregated before including it in the total. But we could imagine much more
    complicated checks. We could even have made a call to another function within
    the `adjmeans` function where we are deciding whether to include the row.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We examine combining DataFrames vertically and horizontally in *Chapter 10*,
    *Addressing Data Issues When Combining DataFrames*.
  prefs: []
  type: TYPE_NORMAL
- en: Classes that contain the logic for updating Series values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We sometimes work with a particular dataset for an extended period of time,
    occasionally years. The data might be updated regularly, for a new month or year,
    or with additional individuals, but the data structure might be fairly stable.
    If that dataset also has a large number of columns, we might be able to improve
    the reliability and readability of our code by implementing classes.
  prefs: []
  type: TYPE_NORMAL
- en: When we create classes, we define the attributes and methods of objects. When
    I use classes for my data-cleaning work, I tend to conceptualize a class as representing
    my unit of analysis. So, if my unit of analysis is a student, then I have a student
    class. Each instance of a student created by that class might have birth date
    and gender attributes and a course registration method. I might also create a
    subclass for alumni that inherits methods and attributes from the student class.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning for the NLS DataFrame could be implemented nicely with classes.
    The dataset has been relatively stable for 25 years, both in terms of the variables
    and the allowable values for each variable. We explore how to create a respondent
    class for NLS survey responses in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need to create a `helperfunctions` subfolder in your current directory
    to run the code in this recipe. We will save the file (`respondent.py`) for our
    new class in that subfolder.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will define a respondent class to create several new Series based on the
    NLS data:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `pandas`, `os`, `sys`, and `pprint` libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We store this code in a different file than we will save the respondent class.
    Let’s call this file `class_cleaning.py`. We will instantiate respondent objects
    from this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Create a `Respondent` class and save it to `respondent.py` in the `helperfunctions`
    subfolder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we call our class (instantiate a class object), the `__init__` method runs
    automatically. (There is a double underscore before and after `init`.) The `__init__`
    method has `self` as the first parameter, as any instance method does. The `__init__`
    method of this class also has a `respdict` parameter, which expects a dictionary
    of values from the NLS data. In later steps, we will instantiate a respondent
    object once for each row of data in the NLS DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: The `__init__` method assigns the passed `respdict` value to `self.respdict`
    to create an instance variable that we can reference in other methods. Finally,
    we increment a counter, `respondentcnt`. We will be able to use this later to
    confirm the number of instances of `respondent` that we created. We also import
    the `math` and `datetime` modules because we will need them later. (Notice that
    class names are capitalized by convention.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Add a method for counting the number of children.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is a very simple method that just adds the number of children living with
    the respondent to the number of children not living with the respondent, to get
    the total number of children. It uses the `childathome` and `childnotathome` key
    values in the `self.respdict` dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Add a method for calculating average weeks worked across the 25 years of the
    survey.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use dictionary comprehension to create a dictionary (`workdict`) of the weeks-worked
    keys that do not have missing values. Sum the values in `workdict` and divide
    that by the length of `workdict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Add a method for calculating age as of a given date.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This method takes a date string (`bydatestring`) to use for the end date of
    the age calculation. We use the `datetime` module to convert the `date` string
    to a `datetime` object, `bydate`. We subtract the birth year value in `self.respdict`
    from the year of `bydate`, subtracting 1 from that calculation if the birth date
    has not happened yet that year. (We only have birth month and birth year in the
    NLS data, so we choose 15 as a midpoint.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Add a method to create a flag if the respondent ever enrolled at a 4-year college.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use dictionary comprehension to check whether any college enrollment values
    are at a 4-year college:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Import the respondent class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we are ready to instantiate some `Respondent` objects! Let’s do that from
    the `class_cleaning.py` file we started in *step 1*. We start by importing the
    respondent class. (This step assumes that `respondent.py` is in the `helperfunctions`
    subfolder.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Load the NLS data and create a list of dictionaries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `to_dict` method to create the list of dictionaries (`nls97list`).
    Each row from the DataFrame will be a dictionary with column names as keys. Show
    part of the first dictionary (the first row):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Loop through the list, creating a `respondent` instance each time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We pass each dictionary to the respondent class, `rp.Respondent(respdict)`.
    Once we have created a respondent object (`resp`), we can then use all of the
    instance methods to get the values we need. We create a new dictionary with those
    values returned by instance methods. We then append that dictionary to `analysisdict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Pass the dictionary to the pandas `DataFrame` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, check the number of items in `analysislist` and the number of instances
    created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: These steps demonstrated how to create a class in Python, how to pass data to
    a class, how to create an instance of a class, and how to call the methods of
    the class to update variable values.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key work in this recipe is done in *step 2*. It creates the respondent class
    and sets us up well for the remaining steps. We pass a dictionary with the values
    for each row to the class’s `__init__` method. The `__init__` method assigns that
    dictionary to an instance variable that will be available to all of the class’s
    methods (`self.respdict = respdict`).
  prefs: []
  type: TYPE_NORMAL
- en: '*Steps 3* through *6* use that dictionary to calculate the number of children,
    average weeks worked per year, age, and college enrollment. *Steps 4* and *6*
    show how helpful dictionary comprehensions are when we need to test for the same
    value over many keys. The dictionary comprehensions select the relevant keys,
    `weeksworked##`, `colenroct##`, and `colenrfeb##`, and allow us to inspect the
    values of those keys. This is incredibly useful when we have data that is untidy
    in this way, as survey data often is.'
  prefs: []
  type: TYPE_NORMAL
- en: In *step 8*, we create a list of dictionaries with the `to_dict` method. It
    has the expected number of list items, 8,984, the same as the number of rows in
    the DataFrame. We use `pprint` to show what the dictionary looks like for the
    first list item. The dictionary has keys for the column names and values for the
    column values.
  prefs: []
  type: TYPE_NORMAL
- en: We iterate over the list in *step 9*, creating a new respondent object and passing
    the list item. We call the methods to get the values we want, except for `originalid`,
    which we can pull directly from the dictionary. We create a dictionary (`newdict`)
    with those values, which we append to a list (`analysislist`).
  prefs: []
  type: TYPE_NORMAL
- en: In *step 10*, we create a pandas DataFrame from the list (`analysislist`) we
    created in *step 9*. We do this by passing the list to the pandas DataFrame method.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We pass dictionaries to the class rather than data rows, which is also a possibility.
    We do this because navigating a NumPy array is more efficient than looping over
    a DataFrame with `itertuples` or `iterrows`. We do not lose much of the functionality
    needed for our class when we work with dictionaries rather than DataFrame rows.
    We are still able to use functions such as `sum` and `mean` and count the number
    of values meeting certain criteria.
  prefs: []
  type: TYPE_NORMAL
- en: It is hard to avoid having to iterate over data with this conceptualization
    of a respondent class. This respondent class is consistent with our understanding
    of the unit of analysis, the survey respondent. That is also, unsurprisingly,
    how the data comes to us. But iterating over data one row at a time is resource-intensive,
    even with more efficient NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: I would argue, however, that you gain more than you lose by constructing a class
    like this one when working with data with many columns and with a structure that
    does not change much over time. The most important advantage is that it matches
    our intuition about the data and focuses our work on understanding the data for
    each respondent. I also think we find that when we construct the class well, we
    do far fewer passes through the data than we otherwise might.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We examine navigating over DataFrame rows and NumPy arrays in *Chapter 9*, *Fixing
    Messy Data When Aggregating*.
  prefs: []
  type: TYPE_NORMAL
- en: This was a very quick introduction to working with classes in Python. If you
    would like to learn more about object-oriented programming in Python, I would
    recommend *Python 3 Object-Oriented Programming*, *Third Edition* by Dusty Phillips.
  prefs: []
  type: TYPE_NORMAL
- en: Classes that handle non-tabular data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data scientists increasingly receive non-tabular data, often in the form of
    JSON or XML files. The flexibility of JSON and XML allows organizations to capture
    complicated relationships between data items in one file. A one-to-many relationship
    stored in two tables in an enterprise data system can be represented well in JSON
    by a parent node for the one side and child nodes for data on the many side.
  prefs: []
  type: TYPE_NORMAL
- en: When we receive JSON data we often start by trying to normalize it. Indeed,
    we do that in a couple of recipes in this book. We try to recover the one-to-one
    and one-to-many relationships in the data obfuscated by the flexibility of JSON.
    But there is another way to work with such data, one that has many advantages.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of normalizing the data, we can create a class that instantiates objects
    at the appropriate unit of analysis, and use the methods of the class to navigate
    the many side of one-to-many relationships. For example, if we get a JSON file
    that has student nodes and then multiple child nodes for each course taken by
    a student, we would usually normalize that data by creating a student file and
    a course file, with student ID as the merge-by column on both files. An alternative,
    which we explore in this recipe, would be to leave the data as it is, create a
    student class, and create methods that do calculations on the child nodes, such
    as calculating total credits taken.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try that with this recipe using data from the Cleveland Museum of Art
    that has collection items, one or more nodes for media citations for each item,
    and one or more nodes for each creator of the item.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe assumes you have the `requests` and `pprint` libraries. If they
    are not installed, you can install them with `pip`. From Terminal, or PowerShell
    (in Windows), enter `pip install requests` and `pip install pprint`.
  prefs: []
  type: TYPE_NORMAL
- en: I show here the structure of the JSON file that is created when using the `collections`
    API of the Cleveland Museum of Art (I have abbreviated the JSON file to save space).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '**Data note**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Cleveland Museum of Art provides an API for public access to this data:
    [https://openaccess-api.clevelandart.org/](https://openaccess-api.clevelandart.org/).
    Much more than the citations and creators data used in this recipe is available
    with the API.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create a collection item class that summarizes the data we need on creators
    and media citations:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the `pandas`, `json`, `pprint`, and `requests` libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s first create a file that we will use to instantiate collection item objects
    and call it `class_cleaning_json.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Create a `Collectionitem` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We pass a dictionary for each collection item to the `__init__` method of the
    class, which runs automatically when an instance of the class is created. We assign
    the collection item dictionary to an instance variable. Save the class as `collectionitem.py`
    in the `helperfunctions` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Create a method to get the birth year of the first creator for each collection
    item.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remember that collection items can have multiple creators. This means that
    the `creators` key has one or more list items as values, and these items are themselves
    dictionaries. To get the birth year of the first creator, then, we need `[''creators''][0][''birth_year'']`.
    We also need to allow for the birth year key to be missing, so we test for that
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Create a method to get the birth years for all creators.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use list comprehension to loop through all the `creators` items. This will
    return the birth years as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a method to count the number of creators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a method to count the number of media citations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Import the `collectionitem` module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We do this from the `class_cleaning_json.py` file we created in *step 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Load the art museum’s collections data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This returns a list of dictionaries. We just pull a subset of the museum collections
    data with African American artists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Loop through the `camcollections` list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a collection item instance for each item in `camcollections`. Pass each
    item, which is a dictionary of collections, creators, and citation keys, to the
    class. Call the methods we have just created and assign the values they return
    to a new dictionary (`newdict`). Append that dictionary to a list (`analysislist`).
    (Some of the values can be pulled directly from the dictionary, such as with `title=colldict['title']`,
    since we do not need to change the value in any way.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Create an analysis DataFrame with the new list of dictionaries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Confirm that we are getting the correct counts, and print the dictionary for
    the first item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: These steps give a sense of how we can use classes to handle non-tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe demonstrated how to work directly with a JSON file, or any file
    with implied one-to-many or many-to-many relationships. We created a class at
    the unit of analysis (a collection item, in this case) and then created methods
    to summarize multiple nodes of data for each collection item.
  prefs: []
  type: TYPE_NORMAL
- en: The methods we created in *steps 3* to *6* are satisfyingly straightforward.
    When we first look at the structure of the data, displayed in the *Getting ready*
    section of this recipe, it is hard not to feel that it will be really difficult
    to clean. It looks like anything goes. But it turns out to have a fairly reliable
    structure. We can count on one or more child nodes for `creators` and `citations`.
    Each `creators` and `citations` node also has child nodes, which are key and value
    pairs. These keys are not always present, so we need to first check to see whether
    they are present before trying to grab their values. We did this in *step 3*.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I go into some detail about the advantages of working directly with JSON files
    in *Chapter 2*, *Anticipating Data Cleaning Issues When Working with HTML, JSON,
    and Spark Data*. I think the museum’s collections data is a good example of why
    we might want to stick with the JSON if we can. The structure of the data actually
    makes sense, even if it is in a very different form. There is always a danger
    when we try to normalize it that we will miss some aspects of its structure.
  prefs: []
  type: TYPE_NORMAL
- en: Functions for checking overall data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can tighten up our data quality checks by being more explicit and upfront
    about what we are evaluating. We likely have some expectations about the distribution
    of variable values, about the range of allowable values, and about the number
    of missing values very early in a data analysis project. This may come from documentation,
    our knowledge of the underlying real-world processes represented by the data,
    or our understanding of statistics. It is a good idea to have a routine for delineating
    those initial assumptions, testing them, and then revising assumptions throughout
    a project. This recipe will demonstrate what that process might look like.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set up data quality targets for each variable of interest. This includes
    allowable values and thresholds for missing values for categorical variables.
    It also includes ranges of values; missing value, skewness, and kurtosis thresholds;
    and checking for outliers for numeric values. We will check unique identifier
    variables for duplication and for missing values. We start with the assumptions
    in this CSV file about variables on the NLS file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![dct](img/B18596_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Data checks for selected NLS columns'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.3* shows our initial assumptions. For example, for `maritalstatus`,
    we assume the category values *Divorced|Married|Never-married|Separated|Widowed*,
    and that no more than 20% of values will be missing. For `nightlyhrssleep`, a
    numeric variable, we assume that values will be between 3 and 9, that no more
    than 30% of values will be missing, and that it will have a skew and kurtosis
    close to that of a normal distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: We also indicate that we want to check for outliers. The final column is a flag
    we can use if we only want to do data checks for a few variables. Here we indicate
    that we want to do checks for `maritalstatus`, `originalid`, `highestgradecompleted`,
    `gpaenglish`, and `nightlyhrssleep`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will work again with the NLS data in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use our predefined data-checking targets to analyze selected variables in
    the NLS data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the functions we will need for data checking and save them in the `helperfunctions`
    subfolder with the name `runchecks.py`. The following two functions, `checkcats`
    and `checkoutliers`, will be used to test values in a list and outliers respectively.
    We will see how that works in subsequent steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define a function to run all of our checks, `runchecks`, which will
    take a DataFrame (`df`), our data targets (`dc`), a list of numerical columns
    (`numvars`), a list of categorical columns (`catvars`), and a list of identifier
    columns (`idvars`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Within the `runchecks` function, we loop over the categorical variable columns
    in our data checks. We get the values of all targets for the variable with `dcvals
    = dc.loc[col]`. We create a NumPy array, `compcat`, from the category values.
    We then compare that array to all of the values for that column in the passed
    DataFrame (`df[col].dropna().str.strip().unique()`). If there is a category in
    one array but not the other (`valuediff`) we print that to the console. We also
    calculate the missing-value percentage. If it is beyond the threshold we specified,
    we print a message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s now look at the loop for checking the numeric variables. We create a NumPy
    array from the range value in our data-checking targets, `range = np.fromstring(dcvals.range,
    sep='|')`. The first element of `range` is the lower end of the range. The second
    element is the upper end. We then get the min and max values for the variable
    from the DataFrame and compare those with the range indicated in the target file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We calculate the missing-values percentage and print if it exceeds the threshold
    we set in the data-checking target file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We show outliers if the `showoutliers` flag is set to `Y`. We use the `checkoutliers`
    function we set up earlier, which uses a simple interquartile range calculation
    to determine outliers. Finally, we check the skew and kurtosis to get an indication
    of how far from normally distributed the variable might be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'We do a couple of straightforward checks for variables identified as id variables
    in the targets file. We look to see if the variable is duplicated and check for
    missing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we are ready to run the data checks. We start by loading the NLS DataFrame
    and the data-checking targets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We import the `runchecks` module we just created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s mess up some of the id variable values for testing the code. We also fix
    logical missing values for `highestgradecompleted`, setting them to actual missing
    values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We select just those targets flagged to be included. We then create categorical
    variable, numeric variable, and id variable lists based on the data-checking targets
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we are ready to run the checks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![datachecksoutput](img/B18596_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: Running the checks'
  prefs: []
  type: TYPE_NORMAL
- en: We see that `maritalstatus` has more missings (26%) than the 20% threshold we
    set. `highestgradecompleted` and `gpaoverall` have values that exceed the anticipated
    range. The kurtosis for both variables is low. `nightlyhrssleep` has outliers
    substantially below and above the interquartile range. 31 respondents have `nightlyhrssleep`
    of 2 or less. 27 respondents have very high `nightlyhrssleep`, of 12 or more.
  prefs: []
  type: TYPE_NORMAL
- en: These steps show how we can use our prior domain knowledge and understanding
    of statistics to better target our investigation of data quality.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created a CSV file with our data-checking targets. We used that file in our
    checks of the NLS data. We did that by passing both the NLS DataFrame and the
    data-checking targets to `runchecks`. Code in `runchecks` loops through the column
    names from the data-checking file and does checks based on the type of variable.
  prefs: []
  type: TYPE_NORMAL
- en: The targets for each variable are defined by `dcvals = dc.loc[col]`, which grabs
    all of the target values for that row in the target file. We can then refer to
    `dcvals.missingthreshold` to get the missing-values threshold, for example. We
    then compare the percentage of missing values (`df[col].isnull().sum()/df.shape[0]`)
    to the missing threshold, and print a message if the missing-value percentage
    is greater than the threshold. We do the same type of checking for range of values,
    skew, outliers, and so on, depending on the type of variable.
  prefs: []
  type: TYPE_NORMAL
- en: We can add new variables to the data-checking targets file without changing
    any of the code in `runchecks`. We can also change target values.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is sometimes important to be proactive with our data checks. There is a difference
    between displaying some sample statistics and frequency distributions to get a
    general sense of the data, and marshaling our domain knowledge and understanding
    of statistics for careful examination of data quality. A more intentional approach
    might require us to occasionally step away from our Python development environment
    for a moment to reflect on our expectations for data values and their distribution.
    Setting up initial data quality targets, and revisiting them regularly, can help
    us do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-processing data with pipelines: a simple example'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When doing predictive analysis, we often need to fold all of our pre-processing
    and feature engineering into a pipeline, including scaling, encoding, and handling
    outliers and missing values. We discussed the reasons why we might need to incorporate
    all of our data preparation into a data pipeline in *Chapter 8*, *Encoding, Transforming,
    and Scaling Features*. The main takeaway from that chapter is that pipelines are
    critical when we are building explanatory models and need to avoid data leakage.
    This can be trickier still when we are using *k*-fold cross-validation for model
    validation, since testing and training DataFrames change during evaluation. Cross-validation
    has become the norm when constructing predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-fold cross-validation trains our model on all but one of the *k* folds,
    or parts, leaving one out for testing. This is repeated *k* times, each time excluding
    a different fold for testing. Performance metrics are then based on the average
    scores across the *k* folds.'
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of pipelines is that they help us ensure reproducible results,
    as they are often intended to take our analysis from raw data to model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Although this recipe demonstrates how to use a pipeline all the way through
    model evaluation, we will not go into detail there. A good resource for both model
    evaluation and pipelines with scikit-learn tools is the book *Data Cleaning and
    Exploration with Machine Learning*, also written by me.
  prefs: []
  type: TYPE_NORMAL
- en: We start with a relatively simple example here, a model with two numeric features
    and one numeric target. We work on a much more complicated example in the next
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will work with scikit-learn’s pipeline tool in this recipe, and a few other
    modules for encoding and scaling the data, and imputing values for missing data.
    We will work with land temperature data again in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by loading the `scikit-learn` modules we will be using in this recipe
    for transforming our data. We will use `StandardScaler` to standardize our features,
    `SimpleImputer` to impute values for missing data, and `make_pipeline` to pull
    all of our pre-processing together. We also use `train_test_split` to create training
    and testing DataFrames. I’ll discuss the other modules as we use them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the land temperature data and create training and testing DataFrames.
    We will try to model temperature as a function of latitude and elevation. Given
    the very different ranges of the `latabs` and `elevation` variables, scaling will
    be important:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For an introduction to `train_test_split`, see *Chapter 8*, *Encoding, Transforming,
    and Scaling Features*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set up *k*-fold cross-validation. We indicate that we want five folds and
    for the data to be shuffled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we are ready to set up our pipeline. The pipeline will do standard scaling,
    impute the mean when values are missing, and then run a linear regression model.
    Both features will be handled in the same way.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After constructing the pipeline, and instantiating a *k*-fold cross-validation
    object, we are ready to run the pre-processing, estimate the model, and generate
    evaluation metrics. We pass the pipeline to the `cross_validate` function, as
    well as our training data. We also pass the `Kfold` object we created in *step
    3*. We get a pretty decent *R*-squared value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We used scikit-learn’s `make_pipeline` to create a pipeline with just three
    steps: apply standard scaling, impute values for missing data based on the mean
    for that variable, and fit a linear regression model. What is so helpful about
    pipelines is that they automatically feed a transformation from one step into
    the next step. This is easy to do, once we get the hang of it, despite the complication
    of *k*-fold cross-validation.'
  prefs: []
  type: TYPE_NORMAL
- en: We can imagine for a moment how messy it would be to write our own code to do
    this when the training and testing DataFrames are changing, as with *k*-fold cross-validation.
    Even something as simple as using the mean for imputation is tricky. We would
    need to take a new mean for the training data each time the training data changed.
    Our pipeline handles all of this for us.
  prefs: []
  type: TYPE_NORMAL
- en: That was a relatively straightforward example, with just a couple of features
    that we could handle in the same way. We also did not bother to check for outliers
    or scale the target variable. We use a pipeline to handle a much trickier modeling
    project in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-processing data with pipelines: a more complicated example'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have ever built a data pipeline, you know that it can be a little messy
    when you are working with several different data types. For example, we might
    need to impute the median for missing values with continuous features and the
    most frequent value for categorical features. We might also need to transform
    our target variable. We explore how to apply different pre-processing to different
    variables in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will work with a fair number of scikit-learn modules in this recipe. Although
    this can be confusing at first, you quickly become grateful that scikit-learn
    has a tool to do pretty much anything you need. Scikit-learn also allows us to
    add our own transformations to a pipeline if we need to do so. I demonstrate how
    to construct our own transformer in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We will work with wage and employment data from the NLS.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by loading the libraries we used in the previous recipe. Then we add
    the `ColumnTransformer` and `TransformedTargetRegressor` classes. We will use
    those classes to transform our features and target respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The column transformer is quite flexible. We can even use it with pre-processing
    functions we have defined ourselves. The code block below imports the `OutlierTrans`
    class from the `preprocfunc` module in the `helperfunctions` subfolder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `OutlierTrans` class identifies missing values by distance from the interquartile
    range. This is a technique we demonstrated in *Chapter 4*, *Identifying Outliers
    in Subsets of Data*, and have used multiple times in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To work in a scikit-learn pipeline our class has to have `fit` and `transform`
    methods. We also need to inherit the `BaseEstimator` and `TransformerMixin` classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this class, almost all of the action happens in the `transform` method.
    Any value that is more than 1.5 times the interquartile range above the third
    quartile or below the first quartile is assigned missing, though that threshold
    can be changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: Our `OutlierTrans` class can be used later in our pipeline in the same way we
    use the `StandardScaler` and other transformers. We will do that later.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to load the data that needs to be processed. We will work with
    the NLS wage data. Wage income will be our target, and we will use high school
    GPA, mother’s and father’s highest grade completed, parent income, gender, weeks
    worked, and whether the individual completed a bachelor’s degree as features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create lists of features to handle in different ways here. That will be helpful
    later when we instruct our pipeline to carry out different operations on numerical,
    categorical, and binary features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: Now we can set up a column transformer. We first create pipelines for handling
    numerical data (`standtrans`), categorical data, and binary data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the numerical data (continuous), we want to assign outlier values to missing.
    Here we pass a value of 2 to the `threshold` parameter of `OutlierTrans`, indicating
    that we want values 2 times the interquartile range above or below that range
    to be set to missing. Recall that it is common to use 1.5, so we are being somewhat
    conservative.
  prefs: []
  type: TYPE_NORMAL
- en: We do one-hot encoding of the `gender` column, essentially creating a dummy
    variable. We drop the last category to avoid the **dummy variable trap**, as discussed
    in *Chapter 8*, *Encoding, Transforming, and Scaling Features*.
  prefs: []
  type: TYPE_NORMAL
- en: We then create a `ColumnTransformer` object, passing to it the three pipelines
    we just created, and indicating which features to use with which pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do not worry about missing values yet for the numeric variables. We will
    handle them later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: We can now add the column transformer to a pipeline that also includes the linear
    model that we would like to run. We add KNN imputation to the pipeline to handle
    missing values for the numeric values. We have already handled missing values
    for the categorical variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also need to scale the target, which cannot be done in our pipeline. We use
    scikit-learn’s `TransformedTargetRegressor` for that. We pass the pipeline we
    just created to the target regressor’s `regressor` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: Let’s do *k*-fold cross-validation using this pipeline. We can pass our pipeline,
    via the target regressor `ttr`, to the `cross_validate` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These scores are not very good, though that was not quite the point of this
    exercise. The key takeaway here is that we typically want to fold most of the
    pre-processing we will do into a pipeline. This is the best way to avoid data
    leakage. The column transformer is an extremely flexible tool, allowing us to
    apply different transformations to different features.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created several different pipelines to pre-process our data before fitting
    our model, one for numeric data, one for categorical data, and one for binary
    data. The column transformer helps us by allowing us to apply different pipelines
    to different columns. We set up the column transformer in *step 5*.
  prefs: []
  type: TYPE_NORMAL
- en: We created another pipeline in *step 6*. That pipeline actually begins with
    the column transformer. Then, the dataset that results from the column transformer’s
    pre-processing is passed to the KNN imputer to handle missing values from the
    numeric columns, and then to the linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: It is good to note that we are able to add transformations to a scikit-learn
    pipeline, even ones we have designed ourselves, because they inherit the `BaseEstimator`
    and `TransformerMixin` classes, as we saw in *step 3*.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is one additional thing that is very cool, and useful, about pipelines
    that was not demonstrated in this example. If you have ever had to generate predictions
    based on variables that have been scaled or transformed in some way, you likely
    remember how much of a nuisance that can be. Well, pipelines handle that for us,
    generating predictions in the appropriate units.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This really just scratches the surface of what can be done with pipelines. For
    a fuller discussion, see the book *Data Cleaning and Exploration with Machine
    Learning*, also written by me.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There was a fair bit packed into this chapter, covering several approaches to
    automating our data-cleaning work. We created functions for showing the structure
    of our data and generating descriptive statistics. We created functions for restructuring
    and aggregating our data. We also developed Python classes for handling data cleaning
    when we have a large number of variables, each requiring very different treatment.
    We also saw how Python classes can make it easier to work directly with a JSON
    file. We examined being more intentional with our data cleaning by checking our
    data against predefined targets. Finally, we explored how to automate our data
    cleaning with pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Leave a review!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoyed this book? Help readers like you by leaving an Amazon review. Scan the
    QR code below to get a free eBook of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Review_copy.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/New_Packt_Logo1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[packt.com](https://www.packt.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  prefs: []
  type: TYPE_NORMAL
- en: Why subscribe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve your learning with Skill Plans built especially for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a free eBook or video every month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully searchable for easy access to vital information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy and paste, print, and bookmark content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At [www.packt.com](https://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  prefs: []
  type: TYPE_NORMAL
- en: Other Books You May Enjoy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/978-1-80324-659-8.png)](https://www.packtpub.com/product/azure-data-factory-cookbook-second-edition/9781803246598)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure Data Factory Cookbook – Second Edition**'
  prefs: []
  type: TYPE_NORMAL
- en: Dmitry Foshin
  prefs: []
  type: TYPE_NORMAL
- en: Dimtry Anoshin
  prefs: []
  type: TYPE_NORMAL
- en: Tonya Chernyshova
  prefs: []
  type: TYPE_NORMAL
- en: Xenia Ireton
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 978-1-80324-659-8'
  prefs: []
  type: TYPE_NORMAL
- en: Create an orchestration and transformation job in ADF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop, execute, and monitor data flows using Azure Synapse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create big data pipelines using Databricks and Delta tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with big data in Azure Data Lake using Spark Pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Migrate on-premises SSIS jobs to ADF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate ADF with commonly used Azure services such as Azure ML, Azure Logic
    Apps, and Azure Functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run big data compute jobs within HDInsight and Azure Databricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy data from AWS S3 and Google Cloud Storage to Azure Storage using ADF’s
    built-in connectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/978-1-80461-442-6.png)](https://www.packtpub.com/product/data-engineering-with-aws-second-edition/9781804614426)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Engineering with AWS – Second Edition**'
  prefs: []
  type: TYPE_NORMAL
- en: Gareth Eagar
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 978-1-80461-442-6'
  prefs: []
  type: TYPE_NORMAL
- en: Seamlessly ingest streaming data with Amazon Kinesis Data Firehose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize, denormalize, and join datasets with AWS Glue Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Amazon S3 events to trigger a Lambda process to transform a file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load data into a Redshift data warehouse and run queries with ease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize and explore data using Amazon QuickSight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract sentiment data from a dataset using Amazon Comprehend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build transactional data lakes using Apache Iceberg with Amazon Athena
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how a data mesh approach can be implemented on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packt is searching for authors like you
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](https://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  prefs: []
  type: TYPE_NORMAL
- en: Share your thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you’ve finished *Python Data Cleaning Cookbook, Second Edition*, we’d love
    to hear your thoughts! If you purchased the book from Amazon, please [click here
    to go straight to the Amazon review page](https://packt.link/r/1803239875) for
    this book and share your feedback or leave a review on the site that you purchased
    it from.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
