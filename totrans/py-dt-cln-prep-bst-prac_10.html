<html><head></head><body>
		<div id="_idContainer095">
			<h1 id="_idParaDest-186" class="chapter-number"><a id="_idTextAnchor223"/><span class="koboSpan" id="kobo.1.1">10</span></h1>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor224"/><span class="koboSpan" id="kobo.2.1">Handling Categorical Features</span></h1>
			<p><span class="koboSpan" id="kobo.3.1">Handling categorical features involves representing and processing information that isn’t inherently numerical. </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">Categorical features</span></strong><span class="koboSpan" id="kobo.5.1"> are </span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.6.1">attributes that can take on a limited, fixed number of values or categories, and they often define distinct categories or groups within a dataset, such as types of products, genres of books, or customer segments. </span><span class="koboSpan" id="kobo.6.2">Effectively managing categorical data is crucial because most </span><strong class="bold"><span class="koboSpan" id="kobo.7.1">machine learning</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.9.1">ML</span></strong><span class="koboSpan" id="kobo.10.1">) algorithms require </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">numerical inputs.</span></span></p>
			<p><span class="koboSpan" id="kobo.12.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">following topics:</span></span></p>
			<ul>
				<li><span class="No-Break"><span class="koboSpan" id="kobo.14.1">Label encoding</span></span></li>
				<li><span class="No-Break"><span class="koboSpan" id="kobo.15.1">One-hot encoding</span></span></li>
				<li><span class="koboSpan" id="kobo.16.1">Target encoding (</span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">mean encoding)</span></span></li>
				<li><span class="No-Break"><span class="koboSpan" id="kobo.18.1">Frequency encoding</span></span></li>
				<li><span class="No-Break"><span class="koboSpan" id="kobo.19.1">Binary encoding</span></span></li>
			</ul>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor225"/><span class="koboSpan" id="kobo.20.1">Technical requirements</span></h1>
			<p><span class="koboSpan" id="kobo.21.1">The complete code for this chapter can be found in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">GitHub repository:</span></span></p>
			<p><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter10"><span class="No-Break"><span class="koboSpan" id="kobo.23.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter10</span></span></a><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-Best-Practices/tree/main/chapter10 "/></p>
			<p><span class="koboSpan" id="kobo.24.1">Let's install the necessary libraries we will use in </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">this chapter:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.26.1">
pip install scikit-learn==1.5.0
pip install matplotlib==3.9.0
pip install seaborn==0.13.2
pip install category_encoders==2.6.3</span></pre>			<h1 id="_idParaDest-189"><a id="_idTextAnchor226"/><span class="koboSpan" id="kobo.27.1">Label encoding</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.28.1">Label encoding</span></strong><span class="koboSpan" id="kobo.29.1"> is a </span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.30.1">technique for handling categorical data by converting each category into a unique integer. </span><span class="koboSpan" id="kobo.30.2">It’s suitable for categorical features with ordinal relationships, where there is a clear ranking or order among </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">the categories.</span></span></p>
			<p><span class="koboSpan" id="kobo.32.1">For example, when dealing with educational levels such as “high school,” “bachelor’s,” “master’s,” and “Ph.D.,” label encoding can be used because there’s a clear order from the least to most advanced level </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">of education.</span></span></p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor227"/><span class="koboSpan" id="kobo.34.1">Use case – employee performance analysis</span></h2>
			<p><span class="koboSpan" id="kobo.35.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.36.1">Human Resources</span></strong><span class="koboSpan" id="kobo.37.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.38.1">HR</span></strong><span class="koboSpan" id="kobo.39.1">) department </span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.40.1">wants to analyze employee performance data to understand the relationship between</span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.41.1"> employee ratings and other factors such as salary, years of experience, and department. </span><span class="koboSpan" id="kobo.41.2">They plan to use ML to predict employee ratings based on </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">these factors.</span></span></p>
			<h3><span class="koboSpan" id="kobo.43.1">The data</span></h3>
			<p><span class="koboSpan" id="kobo.44.1">Let’s </span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.45.1">have a quick look at the data we have available for the </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">performance analysis:</span></span></p>
			<ul>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.47.1">Employee Rating</span></strong><span class="koboSpan" id="kobo.48.1">: Categorical feature with </span><strong class="source-inline"><span class="koboSpan" id="kobo.49.1">Poor</span></strong><span class="koboSpan" id="kobo.50.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.51.1">Satisfactory</span></strong><span class="koboSpan" id="kobo.52.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.53.1">Good</span></strong><span class="koboSpan" id="kobo.54.1">, and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.55.1">Excellent</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.56.1"> values</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.57.1">Salary</span></strong><span class="koboSpan" id="kobo.58.1">: Numeric feature representing </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">employee salaries</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.60.1">Years of Experience</span></strong><span class="koboSpan" id="kobo.61.1">: Numeric feature indicating the number of years an employee </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">has worked</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.63.1">Department</span></strong><span class="koboSpan" id="kobo.64.1">: Categorical feature specifying the department in which the </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">employee works</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.66.1">Let’s have a look at the original DataFrame before </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">the encoding:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.68.1">
Employee   Rating  Salary  Years of Experience  Department
0            Poor   35000                    2          HR
1            Good   50000                    5          IT
2    Satisfactory   42000                    3     Finance
3       Excellent   60000                    8          IT
4            Good   52000                    6   Marketing</span></pre>			<p><span class="koboSpan" id="kobo.69.1">Having </span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.70.1">understood the data, we can move to the objective of the </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">use case.</span></span></p>
			<h3><span class="koboSpan" id="kobo.72.1">Objective of the use case</span></h3>
			<p><span class="koboSpan" id="kobo.73.1">The </span><a id="_idIndexMarker725"/><span class="koboSpan" id="kobo.74.1">use case’s objective is to encode the </span><strong class="source-inline"><span class="koboSpan" id="kobo.75.1">Employee Rating</span></strong><span class="koboSpan" id="kobo.76.1"> feature using label encoding to prepare the data for ML analysis. </span><span class="koboSpan" id="kobo.76.2">Let’s see how we can do this using scikit-learn the complete code can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/1a.label_encoding.py"><span class="No-Break"><span class="koboSpan" id="kobo.78.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/1a.label_encoding.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.79.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.80.1">Let’s import the </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">required libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.82.1">
import pandas as pd
from sklearn.preprocessing import LabelEncoder</span></pre></li>				<li><span class="koboSpan" id="kobo.83.1">Let’s create a sample dataset and turn it into </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">a DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.85.1">
data = {
    'Employee Rating': ['Poor', 'Good', 'Satisfactory', 'Excellent', 'Good'],
    'Salary': [35000, 50000, 42000, 60000, 52000],
    'Years of Experience': [2, 5, 3, 8, 6],
    'Department': ['HR', 'IT', 'Finance', 'IT', 'Marketing']
}
df = pd.DataFrame(data)</span></pre></li>				<li><span class="koboSpan" id="kobo.86.1">Initialize a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.87.1">LabelEncoder</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.88.1"> class:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.89.1">
label_encoder = LabelEncoder()</span></pre></li>				<li><span class="koboSpan" id="kobo.90.1">Apply label encoding to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.91.1">Employee </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.92.1">Rating</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.93.1"> column:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.94.1">
df['Employee Rating (Encoded)'] = label_encoder.fit_transform(df['Employee Rating'])</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.95.1">Let’s have a look </span><a id="_idIndexMarker726"/><span class="koboSpan" id="kobo.96.1">at the encoded output </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">we created.</span></span></p>
			<h3><span class="koboSpan" id="kobo.98.1">Encoded output</span></h3>
			<p><span class="koboSpan" id="kobo.99.1">In this</span><a id="_idIndexMarker727"/><span class="koboSpan" id="kobo.100.1"> use case, label encoding is applied to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.101.1">Employee Rating</span></strong><span class="koboSpan" id="kobo.102.1"> feature to convert it into numeric values while preserving the ordinal relationship. </span><span class="koboSpan" id="kobo.102.2">The following table shows the output of the </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">encoding operation.</span></span></p>
			<table id="table001-7" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.104.1">Employee Rating</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.105.1">Salary</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.106.1">Years </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.107.1">of Experience</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.108.1">Department</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.109.1">Employee </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.110.1">Rating (Encoded)</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.111.1">1</span></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.112.1">Poor</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.113.1">35000</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.114.1">2</span></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.115.1">HR</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.116.1">2</span></strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.117.1">2</span></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.118.1">Good</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.119.1">50000</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.120.1">5</span></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.121.1">IT</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.122.1">1</span></strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.123.1">3</span></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.124.1">Satisfactory</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.125.1">42000</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.126.1">3</span></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.127.1">FINANCE</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.128.1">3</span></strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.129.1">4</span></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.130.1">Excellent</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">60000</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.132.1">8</span></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.133.1">IT</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.134.1">0</span></strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.135.1">5</span></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.136.1">Good</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.137.1">52000</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.138.1">6</span></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.139.1">MARKETING</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline"><span class="koboSpan" id="kobo.140.1">1</span></strong></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.141.1">Table 10.1 – Output dataset after label encoding</span></p>
			<p><span class="koboSpan" id="kobo.142.1">As you can see, an </span><strong class="source-inline"><span class="koboSpan" id="kobo.143.1">Employee Rating (Encoded)</span></strong><span class="koboSpan" id="kobo.144.1"> feature has been added, and all the items are now numeric. </span><span class="koboSpan" id="kobo.144.2">Let’s have a look at the distribution graphs for the </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">encoded column:</span></span></p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<span class="koboSpan" id="kobo.146.1"><img src="image/B19801_10_1.jpg" alt="Figure 10.1 – Distribution before and after the encoding"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.147.1">Figure 10.1 – Distribution before and after the encoding</span></p>
			<p><span class="koboSpan" id="kobo.148.1">As we</span><a id="_idIndexMarker728"/><span class="koboSpan" id="kobo.149.1"> can see, there are no changes in the distribution before and after encoding. </span><span class="koboSpan" id="kobo.149.2">Label encoding converts categorical labels into numerical values </span><em class="italic"><span class="koboSpan" id="kobo.150.1">while preserving the original data distribution</span></em><span class="koboSpan" id="kobo.151.1">. </span><span class="koboSpan" id="kobo.151.2">It simply assigns unique integer values to each category </span><em class="italic"><span class="koboSpan" id="kobo.152.1">without altering their frequency</span></em><span class="koboSpan" id="kobo.153.1">. </span><span class="koboSpan" id="kobo.153.2">However, visually, the labels on the </span><em class="italic"><span class="koboSpan" id="kobo.154.1">x</span></em><span class="koboSpan" id="kobo.155.1"> axis will change from categorical values to numerical values, but the counts (or frequencies) of each label will remain </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">the same.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.157.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.158.1">If the data is shuffled or the order of the categories changes between different runs of the encoder, the encoded values might differ. </span><span class="koboSpan" id="kobo.158.2">This is because the assignment of integers to categories can depend on the order in which they appear. </span><span class="koboSpan" id="kobo.158.3">Also, if you initialize a new instance of the label encoder each time, the mapping of categories to integers might vary as well. </span><span class="koboSpan" id="kobo.158.4">For consistent results, you should fit the encoder once and then use it for </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">transforming data.</span></span></p>
			<p><span class="koboSpan" id="kobo.160.1">The encoded values can then be used as input features for an ML model to predict employee ratings based on salary, years of experience, and department. </span><span class="koboSpan" id="kobo.160.2">Let’s now discuss some things to keep in mind when encoding features with a </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">label encoder.</span></span></p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor228"/><span class="koboSpan" id="kobo.162.1">Considerations for label encoding</span></h2>
			<p><span class="koboSpan" id="kobo.163.1">When </span><a id="_idIndexMarker729"/><span class="koboSpan" id="kobo.164.1">performing label encoding, especially on large datasets, there are several important considerations to keep in mind. </span><span class="koboSpan" id="kobo.164.2">Ensure that categorical features have a meaningful order. </span><span class="koboSpan" id="kobo.164.3">If there’s no natural order among the categories, label encoding might not be appropriate. </span><span class="koboSpan" id="kobo.164.4">Label encoding assigns </span><em class="italic"><span class="koboSpan" id="kobo.165.1">integer values to categories based on alphabetical order</span></em><span class="koboSpan" id="kobo.166.1">. </span><span class="koboSpan" id="kobo.166.2">This introduces a potential issue if the categories do not have an inherent order, but the model might interpret the numerical values as ordered. </span><span class="koboSpan" id="kobo.166.3">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.167.1">Poor</span></strong><span class="koboSpan" id="kobo.168.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.169.1">Good</span></strong><span class="koboSpan" id="kobo.170.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.171.1">Excellent</span></strong><span class="koboSpan" id="kobo.172.1"> might be encoded as </span><strong class="source-inline"><span class="koboSpan" id="kobo.173.1">2</span></strong><span class="koboSpan" id="kobo.174.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.175.1">1</span></strong><span class="koboSpan" id="kobo.176.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.177.1">0</span></strong><span class="koboSpan" id="kobo.178.1">, respectively, but </span><strong class="source-inline"><span class="koboSpan" id="kobo.179.1">Poor</span></strong><span class="koboSpan" id="kobo.180.1"> is not inherently greater than </span><strong class="source-inline"><span class="koboSpan" id="kobo.181.1">Good</span></strong><span class="koboSpan" id="kobo.182.1">. </span><span class="koboSpan" id="kobo.182.2">This is exactly what happened in the use case presented previously. </span><span class="koboSpan" id="kobo.182.3">What we could have done to ensure that the label encoding reflects the correct ordinal order (that is, </span><strong class="source-inline"><span class="koboSpan" id="kobo.183.1">Poor</span></strong><span class="koboSpan" id="kobo.184.1"> &lt; </span><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">Satisfactory</span></strong><span class="koboSpan" id="kobo.186.1"> &lt; </span><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">Good</span></strong><span class="koboSpan" id="kobo.188.1"> &lt; </span><strong class="source-inline"><span class="koboSpan" id="kobo.189.1">Excellent</span></strong><span class="koboSpan" id="kobo.190.1">) was to manually set the order by specifying the desired mapping, as shown next the complete code can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/1b.label_encoding_forced.py"><span class="No-Break"><span class="koboSpan" id="kobo.192.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/1b.label_encoding_forced.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.193.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.194.1">Define the correct order of categories </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">with prefixes:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.196.1">
ordered_categories = {
    'Poor': '1.Poor',
    'Satisfactory': '2.Satisfactory',
    'Good': '3.Good',
    'Excellent': '4.Excellent'
}</span></pre></li>				<li><span class="koboSpan" id="kobo.197.1">Map the </span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">Employee Rating</span></strong><span class="koboSpan" id="kobo.199.1"> column to the </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">prefixed categories:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.201.1">
df['Employee Rating Ordered'] = df['Employee Rating'].map(ordered_categories)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.202.1">The resulting DataFrame is presented </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">as follows:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.204.1">  Employee Rating Ordered  Employee Rating (Encoded)
0                    Poor                          0
1                    Good                          2
2            Satisfactory                          1
3               Excellent                          3
4                    Good                          2</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.205.1">Always keep consistency in mind when encoding </span><em class="italic"><span class="koboSpan" id="kobo.206.1">across training and test datasets</span></em><span class="koboSpan" id="kobo.207.1">. </span><span class="koboSpan" id="kobo.207.2">The encoder should be fitted </span><em class="italic"><span class="koboSpan" id="kobo.208.1">on the training data</span></em><span class="koboSpan" id="kobo.209.1"> and used to transform both training and test datasets. </span><span class="koboSpan" id="kobo.209.2">This prevents issues where unseen categories in the test set lead to errors or incorrect encoding. </span><span class="koboSpan" id="kobo.209.3">Follow the next steps as </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">best practice:</span></span></p></li>				<li><span class="koboSpan" id="kobo.211.1">Apply</span><a id="_idIndexMarker730"/><span class="koboSpan" id="kobo.212.1"> label encoding to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.213.1">Employee </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.214.1">Rating</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.215.1"> column:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.216.1">
df['Employee Rating (Encoded)'] = label_encoder.fit_transform(df['Employee Rating'])</span></pre></li>				<li><span class="koboSpan" id="kobo.217.1">Save </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">the encoder:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.219.1">
joblib.dump(label_encoder, 'label_encoder.pkl')</span></pre></li>				<li><span class="koboSpan" id="kobo.220.1">Load the encoder (in another script </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">or session):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.222.1">
loaded_encoder = joblib.load('label_encoder.pkl')</span></pre></li>				<li><span class="koboSpan" id="kobo.223.1">Transform the </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">new data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.225.1">
df['Employee Rating (Encoded)'] = loaded_encoder.transform(df['Employee Rating'])</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.226.1">The last point to mention, important when dealing with large datasets, is that label encoding is generally more memory-efficient than one-hot encoding, which can create many </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">binary columns.</span></span></p>
			<p><span class="koboSpan" id="kobo.228.1">While label encoding is a straightforward approach for converting categorical data into numerical form, it can inadvertently introduce ordinal relationships between categories that don’t inherently exist. </span><span class="koboSpan" id="kobo.228.2">To avoid this issue and ensure that each category is treated independently, one-hot encoding is often a more </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">suitable method.</span></span></p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor229"/><span class="koboSpan" id="kobo.230.1">One-hot encoding</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.231.1">One-hot encoding</span></strong><span class="koboSpan" id="kobo.232.1"> is a</span><a id="_idIndexMarker731"/><span class="koboSpan" id="kobo.233.1"> technique used to convert categorical data into a binary matrix (1s and 0s). </span><span class="koboSpan" id="kobo.233.2">Each category is transformed into a new column, and a 1 is placed in the column corresponding to the category present for each observation, while all other columns get a 0. </span><span class="koboSpan" id="kobo.233.3">This method is particularly useful when dealing with categorical data where there is </span><strong class="bold"><span class="koboSpan" id="kobo.234.1">no ordinal relationship </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.235.1">among categories</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">.</span></span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor230"/><span class="koboSpan" id="kobo.237.1">When to use one-hot encoding</span></h2>
			<p><span class="koboSpan" id="kobo.238.1">One-hot encoding is </span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.239.1">suitable for categorical data that lacks a natural order or ranking among categories. </span><span class="koboSpan" id="kobo.239.2">Here are some scenarios where it </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">is appropriate:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.241.1">Nominal categorical data</span></strong><span class="koboSpan" id="kobo.242.1">: When dealing with nominal data, where categories are distinct and have no </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">inherent order.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.244.1">Algorithms that don’t handle ordinal data</span></strong><span class="koboSpan" id="kobo.245.1">: Some ML algorithms (for example, decision trees and random forests) are not designed to handle ordinal data correctly. </span><span class="koboSpan" id="kobo.245.2">One-hot encoding ensures that each category is treated as a </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">separate entity.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.247.1">Preventing misinterpretation</span></strong><span class="koboSpan" id="kobo.248.1">: To prevent a model from assuming any ordinal relationship that doesn’t exist, one-hot encoding is used to represent categorical data as </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">binary values.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.250.1">Next, let’s look at a use case where we can use </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">one-hot encoding.</span></span></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor231"/><span class="koboSpan" id="kobo.252.1">Use case – customer churn prediction</span></h2>
			<p><span class="koboSpan" id="kobo.253.1">A</span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.254.1"> telecommunications company is experiencing high customer churn and wants to build an ML model to predict which</span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.255.1"> customers are likely to leave their service. </span><span class="koboSpan" id="kobo.255.2">They have collected data on customer demographics, contract details, and </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">services used.</span></span></p>
			<h3><span class="koboSpan" id="kobo.257.1">The data</span></h3>
			<p><span class="koboSpan" id="kobo.258.1">Let’s have a </span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.259.1">quick look at the data we have available for </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">the analysis:</span></span></p>
			<ul>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.261.1">Contract Type</span></strong><span class="koboSpan" id="kobo.262.1">: Categorical feature with </span><strong class="source-inline"><span class="koboSpan" id="kobo.263.1">Month-to-Month</span></strong><span class="koboSpan" id="kobo.264.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.265.1">One Year</span></strong><span class="koboSpan" id="kobo.266.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">Two </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.268.1">Year</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.269.1"> values</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.270.1">Internet Service</span></strong><span class="koboSpan" id="kobo.271.1">: Categorical feature with </span><strong class="source-inline"><span class="koboSpan" id="kobo.272.1">DSL</span></strong><span class="koboSpan" id="kobo.273.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.274.1">Fiber Optic</span></strong><span class="koboSpan" id="kobo.275.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.276.1">No Internet </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.277.1">Service</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.278.1"> values</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">Payment Method</span></strong><span class="koboSpan" id="kobo.280.1">: Categorical feature with </span><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">Electronic Check</span></strong><span class="koboSpan" id="kobo.282.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">Mailed Check</span></strong><span class="koboSpan" id="kobo.284.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">Bank Transfer</span></strong><span class="koboSpan" id="kobo.286.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">Credit </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">Card</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.289.1"> values</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.290.1">Let’s have a look at the sample data for the </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">use case:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.292.1">
  Customer ID  Contract Type Internet Service Payment Method
0           1 Month-to-Month              DSL  Electronic Check
1           2       One Year      Fiber Optic  Mailed Check
2           3 Month-to-Month              DSL Bank Transfer
3           4       Two Year      Fiber Optic   Credit Card</span></pre>			<p><span class="koboSpan" id="kobo.293.1">Having understood the data, we can move to the objective of the </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">use case.</span></span></p>
			<h3><span class="koboSpan" id="kobo.295.1">Objective of the use case</span></h3>
			<p><span class="koboSpan" id="kobo.296.1">The </span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.297.1">objective of the use case is to encode the categorical features using one-hot encoding to prepare the data for ML analysis. </span><span class="koboSpan" id="kobo.297.2">The code for this example can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/2.one_hot_encoding.py"><span class="No-Break"><span class="koboSpan" id="kobo.299.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/2.one_hot_encoding.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.300.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.301.1">Follow the </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">next steps:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.303.1">Initialize a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.304.1">OneHotEncoder</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.305.1"> class:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.306.1">
one_hot_encoder = OneHotEncoder(sparse_output=False, drop='first')</span></pre></li>				<li><span class="koboSpan" id="kobo.307.1">Fit and transform the </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">categorical columns:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.309.1">
encoded_columns = one_hot_encoder.fit_transform(df[['Contract Type', 'Internet Service', 'Payment Method']])</span></pre></li>				<li><span class="koboSpan" id="kobo.310.1">Create a new DataFrame with the one-hot </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">encoded columns:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.312.1">
encoded_df = pd.DataFrame(encoded_columns, columns=one_hot_encoder.get_feature_names_out(['Contract Type', 'Internet Service', 'Payment Method']))</span></pre></li>				<li><span class="koboSpan" id="kobo.313.1">Concatenate </span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.314.1">the one-hot encoded DataFrame with the </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">original DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.316.1">
df_encoded = pd.concat([df, encoded_df], axis=1)</span></pre></li>				<li><span class="koboSpan" id="kobo.317.1">Drop the original categorical columns as they are </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">now encoded:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.319.1">
df_encoded = df_encoded.drop(['Contract Type', 'Internet Service', 'Payment Method'], axis=1)</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.320.1">Let’s have a look at the encoded output </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">we created.</span></span></p>
			<h3><span class="koboSpan" id="kobo.322.1">Encoded output</span></h3>
			<p><span class="koboSpan" id="kobo.323.1">In this </span><a id="_idIndexMarker738"/><span class="koboSpan" id="kobo.324.1">use case, we are preparing customer data for a churn prediction model. </span><span class="koboSpan" id="kobo.324.2">Categorical features such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.325.1">Contract Type</span></strong><span class="koboSpan" id="kobo.326.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.327.1">Internet Service</span></strong><span class="koboSpan" id="kobo.328.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.329.1">Payment Method</span></strong><span class="koboSpan" id="kobo.330.1"> are one-hot encoded to convert them into binary representations suitable for ML. </span><span class="koboSpan" id="kobo.330.2">These encoded features can be used to train a predictive model that helps the telecommunications company identify customers at risk of churning and take proactive measures to </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">retain them.</span></span></p>
			<p><span class="koboSpan" id="kobo.332.1">Let’s see with some plots how the distribution of the features changes when we are applying the encoding. </span><span class="koboSpan" id="kobo.332.2">Let’s have a look at the original distribution before the </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">encoding first:</span></span></p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<span class="koboSpan" id="kobo.334.1"><img src="image/B19801_10_2.jpg" alt="Figure 10.2 – Distribution before the one-hot encoding"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.335.1">Figure 10.2 – Distribution before the one-hot encoding</span></p>
			<p><span class="koboSpan" id="kobo.336.1">After</span><a id="_idIndexMarker739"/><span class="koboSpan" id="kobo.337.1"> encoding, each value of the categorical variables is turned into a unique column, showcasing binary values (0 or 1) reflecting the presence of that category in each row of the dataset. </span><span class="koboSpan" id="kobo.337.2">Let’s see the distribution graphs for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.338.1">Contract </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.339.1">Type</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.340.1"> column:</span></span></p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<span class="koboSpan" id="kobo.341.1"><img src="image/B19801_10_3.jpg" alt="Figure 10.3 – Distribution after the one-hot encoding for the Contract Type feature"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.342.1">Figure 10.3 – Distribution after the one-hot encoding for the Contract Type feature</span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.343.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.344.1">Visualizing the original categorical data helps understand the data distribution and identify any imbalances. </span><span class="koboSpan" id="kobo.344.2">Visualizing the encoded columns ensures that the transformation has been applied correctly. </span><span class="koboSpan" id="kobo.344.3">Each binary column should only have values of 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">or 1.</span></span></p>
			<p><span class="koboSpan" id="kobo.346.1">Let’s now discuss some things to keep in mind when encoding features with a </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">one-hot encoder.</span></span></p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor232"/><span class="koboSpan" id="kobo.348.1">Considerations for one-hot encoding</span></h2>
			<p><span class="koboSpan" id="kobo.349.1">When </span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.350.1">performing one-hot encoding, especially on large datasets, there are several important considerations to keep </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">in mind:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.352.1">One-hot encoding can significantly increase the dimensionality of your dataset, especially when you have many categories. </span><span class="koboSpan" id="kobo.352.2">This can lead to the “curse of dimensionality,” which can be problematic for </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">some algorithms.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.354.1">Collinearity</span></strong><span class="koboSpan" id="kobo.355.1">: Since each category is represented as a separate binary column, there can be collinearity between these columns. </span><span class="koboSpan" id="kobo.355.2">This means some columns might be highly correlated, which can affect the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">linear models.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.357.1">Handling missing values</span></strong><span class="koboSpan" id="kobo.358.1">: Decide how to handle missing values in categorical features before applying one-hot encoding. </span><span class="koboSpan" id="kobo.358.2">You may choose to create a separate column for missing values or use </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">imputation techniques.</span></span></li>
				<li><span class="koboSpan" id="kobo.360.1">Handling one-hot encoding on large datasets can be challenging due to the increase in the number of features and the potential for high memory usage. </span><span class="koboSpan" id="kobo.360.2">Process the data in smaller batches if the dataset is too large to fit </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">into memory.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.362.1">Moving from one-hot encoding to target encoding can be particularly beneficial when dealing with high-cardinality categorical features. </span><span class="koboSpan" id="kobo.362.2">Let’s explore target encoding in </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">more detail.</span></span></p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor233"/><span class="koboSpan" id="kobo.364.1">Target encoding (mean encoding)</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.365.1">Target encoding</span></strong><span class="koboSpan" id="kobo.366.1">, also known</span><a id="_idIndexMarker741"/><span class="koboSpan" id="kobo.367.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.368.1">mean encoding</span></strong><span class="koboSpan" id="kobo.369.1">, is a technique used for</span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.370.1"> encoding categorical features by replacing each category with the </span><strong class="bold"><span class="koboSpan" id="kobo.371.1">mean</span></strong><span class="koboSpan" id="kobo.372.1"> of the target variable (or another relevant aggregation function) for that category. </span><span class="koboSpan" id="kobo.372.2">This method is particularly useful for classification tasks when dealing with </span><strong class="bold"><span class="koboSpan" id="kobo.373.1">high-cardinality categorical features</span></strong><span class="koboSpan" id="kobo.374.1">, where one-hot encoding would result in a significant increase </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">in dimensionality.</span></span></p>
			<p><span class="koboSpan" id="kobo.376.1">In more detail, target encoding replaces categorical values with the mean (or other aggregation metric) of the target variable for each category. </span><span class="koboSpan" id="kobo.376.2">It leverages the relationship between the categorical feature and the target variable to encode </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">the information.</span></span></p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor234"/><span class="koboSpan" id="kobo.378.1">When to use target encoding</span></h2>
			<p><span class="koboSpan" id="kobo.379.1">When you</span><a id="_idIndexMarker743"/><span class="koboSpan" id="kobo.380.1"> have categorical features with many unique categories, using one-hot encoding might lead to a high-dimensional dataset. </span><span class="koboSpan" id="kobo.380.2">Target encoding can be an effective alternative in </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">such cases.</span></span></p>
			<p><span class="koboSpan" id="kobo.382.1">If there’s a strong relationship between the categorical feature and the target variable, target encoding can capture this relationship and potentially improve </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">predictive power.</span></span></p>
			<p><span class="koboSpan" id="kobo.384.1">You can also use target encoding when you have memory constraints and need to reduce the dimensionality of your dataset, as target encoding doesn’t create </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">additional columns.</span></span></p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor235"/><span class="koboSpan" id="kobo.386.1">Use case – sales prediction for retail stores</span></h2>
			<p><span class="koboSpan" id="kobo.387.1">A </span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.388.1">retail chain with multiple stores wants to build an ML model to predict daily sales for each store. </span><span class="koboSpan" id="kobo.388.2">They have collected </span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.389.1">data on various features, including the </span><strong class="source-inline"><span class="koboSpan" id="kobo.390.1">Store Type</span></strong><span class="koboSpan" id="kobo.391.1"> feature, which has a high cardinality. </span><span class="koboSpan" id="kobo.391.2">Instead of using one-hot encoding, which would result in a large number of features, the retail chain decides to use target encoding to encode the </span><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">Store </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.393.1">Type</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.394.1"> feature.</span></span></p>
			<h3><span class="koboSpan" id="kobo.395.1">The data</span></h3>
			<p><span class="koboSpan" id="kobo.396.1">Let’s</span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.397.1"> have a quick look at the data we have available for </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">the analysis:</span></span></p>
			<ul>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.399.1">Store Type</span></strong><span class="koboSpan" id="kobo.400.1">: The type of store (categorical variable with </span><strong class="source-inline"><span class="koboSpan" id="kobo.401.1">Type A</span></strong><span class="koboSpan" id="kobo.402.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.403.1">Type B</span></strong><span class="koboSpan" id="kobo.404.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.405.1">Type C</span></strong><span class="koboSpan" id="kobo.406.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.407.1">Type </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">D</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.409.1"> values)</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.410.1">Number of Employees</span></strong><span class="koboSpan" id="kobo.411.1">: The number of employees working at the store (</span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">integer variable)</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">Advertising Budget</span></strong><span class="koboSpan" id="kobo.414.1">: The budget allocated for advertising by the store (continuous variable </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">in dollars)</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">Daily Sales</span></strong><span class="koboSpan" id="kobo.417.1">: The sales made by the store in a day (target variable </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">in dollars)</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.419.1">Let’s have a look at a sample of the data for the </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">use case:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.421.1">
    Store Type  Number of Employees  Advertising Budget   Daily Sales
0       Type C                   21        23117.964192  16195.682148
1       Type D                   13         9017.567238    851.127834
2       Type A                   37        39945.667889  19274.801963
3       Type C                   24        34990.429063  14670.084345
4       Type C                   17        11817.711027   6442.646360</span></pre>			<p><span class="koboSpan" id="kobo.422.1">Having</span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.423.1"> understood the data, we can move to the objective of the </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">use case.</span></span></p>
			<h3><span class="koboSpan" id="kobo.425.1">Objective of the use case</span></h3>
			<p><span class="koboSpan" id="kobo.426.1">The </span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.427.1">use case’s objective is to encode the categorical features using target encoding to prepare the data for ML modeling. </span><span class="koboSpan" id="kobo.427.2">Let’s see how we can do this using scikit-learn. </span><span class="koboSpan" id="kobo.427.3">The code for this example can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/3.target_encoding.py"><span class="No-Break"><span class="koboSpan" id="kobo.429.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/3.target_encoding.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.430.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.431.1">Make sure you have installed and imported the libraries mentioned in the </span><em class="italic"><span class="koboSpan" id="kobo.432.1">Technical requirements</span></em><span class="koboSpan" id="kobo.433.1"> section at the beginning of the chapter. </span><span class="koboSpan" id="kobo.433.2">Once that is done, </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">let’s begin:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.435.1">Let’s create a synthetic dataset of sample </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">size 1000:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.437.1">
np.random.seed(42)
n_samples = 1000</span></pre></li>				<li><span class="koboSpan" id="kobo.438.1">Generate some </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">random data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.440.1">
data = {
  'Store Type': np.random.choice(['Type A', 'Type B', 'Type C', 'Type D'], size=n_samples),
  'Number of Employees': np.random.randint(5, 50, size=n_samples),
  'Advertising Budget': np.random.uniform(1000, 50000, size=n_samples),
  'Daily Sales': np.random.uniform(500, 20000, size=n_samples)
}</span></pre></li>				<li><span class="koboSpan" id="kobo.441.1">Put</span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.442.1"> the data into </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">a DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.444.1">
df = pd.DataFrame(data)</span></pre></li>				<li><span class="koboSpan" id="kobo.445.1">Define a target variable </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">and features:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.447.1">
X = df.drop(columns=['Daily Sales']) # Features
y = df['Daily Sales'] # Target variable</span></pre></li>				<li><span class="koboSpan" id="kobo.448.1">Split the data into training and </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">testing sets:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.450.1">
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></pre></li>				<li><span class="koboSpan" id="kobo.451.1">Initialize a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.452.1">TargetEncoder</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.453.1"> class:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.454.1">
target_encoder = TargetEncoder(cols=['Store Type'])</span></pre></li>				<li><span class="koboSpan" id="kobo.455.1">Fit and transform on the </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">training data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.457.1">
X_train_encoded = target_encoder.fit_transform(X_train, y_train)</span></pre></li>			</ol>
			<p class="callout-heading"><span class="koboSpan" id="kobo.458.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.459.1">In the code provided on GitHub for this section, we are using the data and the encoded feature to train a random forest regressor model and calculate validation metrics. </span><span class="koboSpan" id="kobo.459.2">If you are interested, explore the code file </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/3.target_encoding.py"><span class="No-Break"><span class="koboSpan" id="kobo.461.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/3.target_encoding.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.462.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.463.1">This encoding technique helps capture the relationship between different store types and daily sales, so let’s have a look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">encoded output.</span></span></p>
			<h3><span class="koboSpan" id="kobo.465.1">Encoded output</span></h3>
			<p><span class="koboSpan" id="kobo.466.1">Let’s have a</span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.467.1"> look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">encoded data:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.469.1">
       Store Type  Number of Employees  Advertising Budget
29   10025.134200                   37        43562.535230
535  10190.055174                   12         1940.421564
695  10025.134200                   14        47945.600526
557  10190.055174                   23        19418.525972
836  10560.489044                   27        35683.919764</span></pre>			<p><span class="koboSpan" id="kobo.470.1">Let’s focus on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.471.1">Store Type</span></strong><span class="koboSpan" id="kobo.472.1"> column, which is now encoded into numerical values. </span><span class="koboSpan" id="kobo.472.2">We can see the difference in more detail before and after the encoding in the </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">following graphs:</span></span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<span class="koboSpan" id="kobo.474.1"><img src="image/B19801_10_4.jpg" alt="Figure 10.4 – Distribution of store type before and after the encoding"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.475.1">Figure 10.4 – Distribution of store type before and after the encoding</span></p>
			<p><span class="koboSpan" id="kobo.476.1">Target encoding can be advantageous in this scenario as it efficiently encodes the categorical feature, making it suitable for regression tasks such as sales prediction while avoiding the dimensionality issues associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">one-hot encoding.</span></span></p>
			<p><span class="koboSpan" id="kobo.478.1">Let’s now discuss some things to keep in mind when encoding features with a </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">target encoder.</span></span></p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor236"/><span class="koboSpan" id="kobo.480.1">Considerations for target encoding</span></h2>
			<p><span class="koboSpan" id="kobo.481.1">When performing </span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.482.1">target encoding, especially on large datasets, there are several important considerations to keep </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">in mind:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.484.1">Overfitting</span></strong><span class="koboSpan" id="kobo.485.1">: Target encoding can lead to overfitting if not applied carefully or if some categories have only a few samples. </span><span class="koboSpan" id="kobo.485.2">To mitigate this, techniques such as smoothing or adding regularization terms are </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">often used.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.487.1">Smoothing (regularization)</span></strong><span class="koboSpan" id="kobo.488.1">: Smoothing involves blending the mean of the target variable for each category with a global mean. </span><span class="koboSpan" id="kobo.488.2">This reduces the impact of extreme values or noise in the training data. </span><span class="koboSpan" id="kobo.488.3">The formula for smoothed target encoding is often </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">the following:</span></span></li>
			</ul>
			<p><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.490.1">smoothed_mean =(n*category_mean+m*global_mean)/(n+m)</span></span></span></p>
			<p class="list-inset"><span class="koboSpan" id="kobo.491.1">Here, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">the following:</span></span></p>
			<ul>
				<li><em class="italic"><span class="koboSpan" id="kobo.493.1">n</span></em><span class="koboSpan" id="kobo.494.1"> is the number of observations in </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">the category.</span></span></li>
				<li><em class="italic"><span class="koboSpan" id="kobo.496.1">m</span></em><span class="koboSpan" id="kobo.497.1"> is a hyperparameter that controls the strength </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">of smoothing.</span></span></li>
			</ul>
			<p class="list-inset"><span class="koboSpan" id="kobo.499.1">Adjusting the value of </span><em class="italic"><span class="koboSpan" id="kobo.500.1">m</span></em><span class="koboSpan" id="kobo.501.1"> allows you to control the level of regularization. </span><span class="koboSpan" id="kobo.501.2">Smaller values of </span><em class="italic"><span class="koboSpan" id="kobo.502.1">m</span></em><span class="koboSpan" id="kobo.503.1"> give more weight to the category’s actual mean, while larger values give more weight to the </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">global mean.</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.505.1">Cross-validation</span></strong><span class="koboSpan" id="kobo.506.1">: Perform target encoding within each fold of a cross-validation scheme. </span><span class="koboSpan" id="kobo.506.2">This helps ensure that the encoding is based on a portion of the data independent of the one being predicted. </span><span class="koboSpan" id="kobo.506.3">Cross-validation can provide a more reliable estimate of the target variable’s distribution for </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">each category.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.508.1">Leave-one-out encoding</span></strong><span class="koboSpan" id="kobo.509.1">: In this approach, you compute the mean of the target variable for a category excluding the current observation. </span><span class="koboSpan" id="kobo.509.2">It can be more robust to overfitting because it considers the effect of the category without including the target value of the instance </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">being encoded.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.511.1">Adding noise</span></strong><span class="koboSpan" id="kobo.512.1">: Introducing a small amount of random noise to the encoded values can help </span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.513.1">reduce overfitting. </span><span class="koboSpan" id="kobo.513.2">This is often referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.514.1">Bayesian </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.515.1">target encoding</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">.</span></span></li>
				<li><span class="koboSpan" id="kobo.517.1">Be cautious</span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.518.1"> of data leakage. </span><span class="koboSpan" id="kobo.518.2">It’s crucial to calculate the mean on the training dataset only and apply the same encoding to the validation and </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">test datasets.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.520.1">Compute encoding statistics on training data only</span></strong><span class="koboSpan" id="kobo.521.1">: Calculate the encoding statistics (for example, mean) based solely on the training dataset. </span><span class="koboSpan" id="kobo.521.2">This ensures that the model is trained on </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">unbiased information.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.523.1">Apply the same encoding to all datasets</span></strong><span class="koboSpan" id="kobo.524.1">: Once you’ve calculated the encoding statistics on the training data, use the same encoding when preprocessing the validation and test datasets. </span><span class="koboSpan" id="kobo.524.2">Do not recalculate the statistics separately for </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">these datasets.</span></span></li>
				<li><span class="koboSpan" id="kobo.526.1">While target encoding can improve model performance, it may reduce the interpretability of your model, as the original categorical values </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">are lost.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.528.1">After exploring target encoding, another effective technique for handling high-cardinality categorical features is frequency encoding. </span><span class="koboSpan" id="kobo.528.2">Frequency encoding replaces each category with its frequency or count in the dataset, which can help capture the inherent importance of each category and maintain the overall distribution of the data. </span><span class="koboSpan" id="kobo.528.3">Let’s deep dive into frequency encoding and its advantages in processing </span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">categorical variables.</span></span></p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor237"/><span class="koboSpan" id="kobo.530.1">Frequency encoding</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.531.1">Frequency encoding</span></strong><span class="koboSpan" id="kobo.532.1">, also known as </span><strong class="bold"><span class="koboSpan" id="kobo.533.1">count encoding</span></strong><span class="koboSpan" id="kobo.534.1">, is a</span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.535.1"> technique used </span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.536.1">for encoding categorical features by replacing each category with its corresponding frequency or count in the dataset. </span><span class="koboSpan" id="kobo.536.2">In this encoding method, the more frequent a category is, the higher its encoded value. </span><span class="koboSpan" id="kobo.536.3">Frequency encoding can be a valuable tool in certain situations where the frequency of occurrence of a category carries </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">valuable information.</span></span></p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor238"/><span class="koboSpan" id="kobo.538.1">When to use frequency encoding</span></h2>
			<p><span class="koboSpan" id="kobo.539.1">Frequency </span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.540.1">encoding can be considered in the </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">following scenarios:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.542.1">Informative frequency</span></strong><span class="koboSpan" id="kobo.543.1">: The frequency or count of categories is informative and has a direct or indirect relationship with the target variable. </span><span class="koboSpan" id="kobo.543.2">For example, in a customer churn prediction problem, the frequency of product purchases by a customer may correlate with their likelihood </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">to churn.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.545.1">Efficiency</span></strong><span class="koboSpan" id="kobo.546.1">: You need an efficient encoding method that requires minimal computational resources and memory compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">one-hot encoding.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.548.1">This encoding method often works well with tree-based models such as decision trees, random forests, and gradient boosting, as these models can effectively capture the relationship between the encoded frequency and the </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">target variable.</span></span></p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor239"/><span class="koboSpan" id="kobo.550.1">Use case – customer product preference analysis</span></h2>
			<p><span class="koboSpan" id="kobo.551.1">A retail</span><a id="_idIndexMarker757"/><span class="koboSpan" id="kobo.552.1"> company wants to analyze customer product preferences based on their purchase history. </span><span class="koboSpan" id="kobo.552.2">They have a dataset with information about customer purchases, including the product category they buy </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">the most.</span></span></p>
			<h3><span class="koboSpan" id="kobo.554.1">The data</span></h3>
			<p><span class="koboSpan" id="kobo.555.1">In this </span><a id="_idIndexMarker758"/><span class="koboSpan" id="kobo.556.1">example, we will use frequency encoding on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.557.1">Product Category</span></strong><span class="koboSpan" id="kobo.558.1"> feature to determine the most frequently purchased product categories by customers. </span><span class="koboSpan" id="kobo.558.2">This encoding method allows the retail company to analyze customer preferences and understand how to optimize product recommendations or marketing strategies based on popular </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">product categories.</span></span></p>
			<p><span class="koboSpan" id="kobo.560.1">Let’s have a look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">sample dataset:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.562.1">
  Customer ID Product Category  Total Purchases
0            1      Electronics                5
1            2         Clothing                2
2            3      Electronics                3
3            4            Books                8
4            5            Books                7
5            6         Clothing                4</span></pre>			<p><span class="koboSpan" id="kobo.563.1">Having </span><a id="_idIndexMarker759"/><span class="koboSpan" id="kobo.564.1">understood the data, we can move to the objective of the </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">use case.</span></span></p>
			<h3><span class="koboSpan" id="kobo.566.1">Objective of the use case</span></h3>
			<p><span class="koboSpan" id="kobo.567.1">The use case’s objective </span><a id="_idIndexMarker760"/><span class="koboSpan" id="kobo.568.1">is to encode the categorical features using frequency encoding to prepare the data for ML modeling. </span><span class="koboSpan" id="kobo.568.2">Let’s see how we can do that </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">using scikit-learn:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.570.1">Let’s create a </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">sample dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.572.1">
data = {
    'Customer ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Product Category': ['Electronics', 'Clothing', 'Electronics', 'Books', 'Books', 'Clothing', 'Electronics', 'Books', 'Clothing', 'Books'],
    'Total Purchases': [5, 2, 3, 8, 7, 4, 2, 5, 1, 6]
}
df = pd.DataFrame(data)</span></pre></li>				<li><span class="No-Break"><span class="koboSpan" id="kobo.573.1">Define features:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.574.1">
X = df[['Customer ID', 'Product Category', 'Total Purchases']]</span></pre></li>				<li><span class="koboSpan" id="kobo.575.1">Split the data into training and </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">testing sets:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.577.1">
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)</span></pre></li>				<li><span class="koboSpan" id="kobo.578.1">Initialize a </span><strong class="source-inline"><span class="koboSpan" id="kobo.579.1">CountEncoder</span></strong><span class="koboSpan" id="kobo.580.1"> class for </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.581.1">Product Category</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.583.1">
count_encoder = CountEncoder(cols=['Product Category'])</span></pre></li>				<li><span class="koboSpan" id="kobo.584.1">Fit and transform the </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">training data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.586.1">
X_train_encoded = count_encoder.fit_transform(X_train)</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.587.1">The </span><a id="_idIndexMarker761"/><span class="koboSpan" id="kobo.588.1">company wants to encode this categorical feature using frequency encoding to understand which product categories are most frequently purchased. </span><span class="koboSpan" id="kobo.588.2">Let’s have a look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.589.1">encoded data.</span></span></p>
			<h3><span class="koboSpan" id="kobo.590.1">Encoded output</span></h3>
			<p><span class="koboSpan" id="kobo.591.1">Let’s have a look </span><a id="_idIndexMarker762"/><span class="koboSpan" id="kobo.592.1">at the </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">encoded data:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.594.1">
   Customer ID  Product Category  Total Purchases
5            6                 1                4
0            1                 3                5
7            8                 4                5
2            3                 3                3
9           10                 4                6</span></pre>			<p><span class="koboSpan" id="kobo.595.1">Let’s focus on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.596.1">Product Category</span></strong><span class="koboSpan" id="kobo.597.1"> feature, which is now encoded into numerical values based on frequency. </span><span class="koboSpan" id="kobo.597.2">We can see the difference in more detail before and after the encoding on the </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">following graphs:</span></span></p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<span class="koboSpan" id="kobo.599.1"><img src="image/B19801_10_5.jpg" alt="Figure 10.5 – Distribution of Product Category before and after the encoding"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.600.1">Figure 10.5 – Distribution of Product Category before and after the encoding</span></p>
			<p><span class="koboSpan" id="kobo.601.1">The </span><a id="_idIndexMarker763"/><span class="koboSpan" id="kobo.602.1">first subplot shows the distribution of </span><strong class="source-inline"><span class="koboSpan" id="kobo.603.1">Product Category</span></strong><span class="koboSpan" id="kobo.604.1"> in the training set before encoding. </span><span class="koboSpan" id="kobo.604.2">The second subplot shows the distribution of the encoded </span><strong class="source-inline"><span class="koboSpan" id="kobo.605.1">Product Category</span></strong><span class="koboSpan" id="kobo.606.1"> feature in the training set after encoding. </span><span class="koboSpan" id="kobo.606.2">As we can see, each category in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.607.1">Product Category</span></strong><span class="koboSpan" id="kobo.608.1"> column is replaced</span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.609.1"> by the </span><strong class="bold"><span class="koboSpan" id="kobo.610.1">frequency count</span></strong><span class="koboSpan" id="kobo.611.1"> of that category within the </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">training set.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.613.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.614.1">Frequency encoding preserves information about the prevalence of each category in </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">the dataset.</span></span></p>
			<p><span class="koboSpan" id="kobo.616.1">Let’s now discuss some things to keep in mind when encoding features with a </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">frequency encoder.</span></span></p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor240"/><span class="koboSpan" id="kobo.618.1">Considerations for frequency encoding</span></h2>
			<p><span class="koboSpan" id="kobo.619.1">When</span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.620.1"> performing frequency encoding, there are several important considerations to keep </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">in mind:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.622.1">Frequency encoding can lead to overfitting, especially if the dataset is small or if there are categories with very few observations. </span><span class="koboSpan" id="kobo.622.2">This is because the model might learn to rely too heavily on the frequency counts, which may not generalize well to </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">new data.</span></span></li>
				<li><span class="koboSpan" id="kobo.624.1">When two or more categories have the same frequency, they will end up with the same encoded value. </span><span class="koboSpan" id="kobo.624.2">This can be a limitation if those categories have different effects on the </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">target variable.</span></span></li>
				<li><span class="koboSpan" id="kobo.626.1">Frequency</span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.627.1"> encoding is generally not suitable for linear models, as it does not create a linear relationship between the encoded values and the target variable. </span><span class="koboSpan" id="kobo.627.2">It may be necessary to normalize the encoded values to a similar scale, especially if you’re using linear models that are sensitive to </span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">feature scaling.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.629.1">Overall, frequency encoding is straightforward to implement and does not expand the feature space, unlike one-hot encoding, making it efficient for handling high-cardinality features without creating many </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">new columns.</span></span></p>
			<p><span class="koboSpan" id="kobo.631.1">While frequency encoding offers simplicity and efficiency for handling high-cardinality features, another effective technique is binary encoding. </span><span class="koboSpan" id="kobo.631.2">Binary encoding represents categories as binary numbers, providing a more compact representation than one-hot encoding and preserving ordinal relationships. </span><span class="koboSpan" id="kobo.631.3">Let’s explore how binary encoding can further enhance the processing of </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">categorical variables.</span></span></p>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor241"/><span class="koboSpan" id="kobo.633.1">Binary encoding</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.634.1">Binary encoding</span></strong><span class="koboSpan" id="kobo.635.1"> is a</span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.636.1"> technique used for encoding categorical features by converting each category into binary code. </span><span class="koboSpan" id="kobo.636.2">Each unique category is represented by a unique binary pattern, where each digit (0 or 1) in the pattern corresponds to the presence or absence of that category. </span><span class="koboSpan" id="kobo.636.3">Binary encoding is particularly useful for handling high-cardinality categorical features while </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">reducing dimensionality.</span></span></p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor242"/><span class="koboSpan" id="kobo.638.1">When to use binary encoding</span></h2>
			<p><span class="koboSpan" id="kobo.639.1">Binary encoding </span><a id="_idIndexMarker768"/><span class="koboSpan" id="kobo.640.1">can be considered in the </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">following scenarios:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.642.1">Dimensionality reduction</span></strong><span class="koboSpan" id="kobo.643.1">: You want to reduce the dimensionality of the dataset while still capturing information contained within the categorical feature. </span><span class="koboSpan" id="kobo.643.2">Binary encoding is particularly useful in </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">this scenario.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.645.1">Efficiency</span></strong><span class="koboSpan" id="kobo.646.1">: You need an efficient encoding method that results in a compact representation of </span><a id="_idIndexMarker769"/><span class="koboSpan" id="kobo.647.1">categorical data and can be easily processed by </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">ML algorithms.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.649.1">Let’s look at a </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">use case.</span></span></p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor243"/><span class="koboSpan" id="kobo.651.1">Use case – customer subscription prediction</span></h2>
			<p><span class="koboSpan" id="kobo.652.1">A subscription-based service </span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.653.1">provider </span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.654.1">wants to predict whether customers will subscribe to a premium plan based on various features, including the </span><strong class="source-inline"><span class="koboSpan" id="kobo.655.1">Country</span></strong><span class="koboSpan" id="kobo.656.1"> feature, which has a high cardinality. </span><span class="koboSpan" id="kobo.656.2">Binary encoding will be used to efficiently encode the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.657.1">Country</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.658.1"> feature.</span></span></p>
			<h3><span class="koboSpan" id="kobo.659.1">The data</span></h3>
			<p><span class="koboSpan" id="kobo.660.1">Let’s have a </span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.661.1">look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">sample dataset:</span></span></p>
			<ul>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.663.1">Country</span></strong><span class="koboSpan" id="kobo.664.1">: This categorical feature represents the country of the customers. </span><span class="koboSpan" id="kobo.664.2">It helps to understand if the location influences the </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">subscription status.</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.666.1">Age</span></strong><span class="koboSpan" id="kobo.667.1">: This numerical feature represents the age of the customers. </span><span class="koboSpan" id="kobo.667.2">Age can be a significant factor in determining the likelihood of a customer subscribing to </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">a service.</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.669.1">Income</span></strong><span class="koboSpan" id="kobo.670.1">: This numerical feature represents the annual income of the customers. </span><span class="koboSpan" id="kobo.670.2">Income can indicate the financial capability of the customers to subscribe to </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">a service.</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.672.1">Subscription</span></strong><span class="koboSpan" id="kobo.673.1">: This binary target variable indicates whether a customer has subscribed to a service. </span><span class="koboSpan" id="kobo.673.2">This is the target variable that we want to predict using the </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">other features.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.675.1">Let’s have a look at a sample of the data for the </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">use case:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.677.1">
  Country  Age  Income  Subscription
0     USA   25   50000             1
1  Canada   30   60000             0
2     USA   35   70000             1
3  Canada   40   80000             0
4  Mexico   45   90000             1</span></pre>			<p><span class="koboSpan" id="kobo.678.1">The</span><a id="_idIndexMarker773"/><span class="koboSpan" id="kobo.679.1"> distribution of </span><strong class="source-inline"><span class="koboSpan" id="kobo.680.1">Country</span></strong><span class="koboSpan" id="kobo.681.1"> can be seen in the </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">following plot:</span></span></p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<span class="koboSpan" id="kobo.683.1"><img src="image/B19801_10_6.jpg" alt="Figure 10.6 – Distribution of Country before and after the encoding"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.684.1">Figure 10.6 – Distribution of Country before and after the encoding</span></p>
			<h3><span class="koboSpan" id="kobo.685.1">Objective of the use case</span></h3>
			<p><span class="koboSpan" id="kobo.686.1">The</span><a id="_idIndexMarker774"/><span class="koboSpan" id="kobo.687.1"> goal of this analysis is to predict the Subscription status of customers based on their country, age, and income. </span><span class="koboSpan" id="kobo.687.2">We use binary encoding for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.688.1">Country</span></strong><span class="koboSpan" id="kobo.689.1"> feature to convert it from a categorical variable to a numerical format that can be used in the ML algorithm. </span><span class="koboSpan" id="kobo.689.2">The code for this use case can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/5.binary_encoding.py"><span class="No-Break"><span class="koboSpan" id="kobo.691.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/5.binary_encoding.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.692.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.693.1">Follow the </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">next steps:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.695.1">Let’s create a </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">sample dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.697.1">
data = {
    'Country': ['USA', 'Canada', 'USA', 'Canada', 'Mexico', 'USA', 'Mexico', 'Canada'],
    'Age': [25, 30, 35, 40, 45, 50, 55, 60],
    'Income': [50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000],
    'Subscription': [1, 0, 1, 0, 1, 0, 1, 0]
}
df = pd.DataFrame(data)</span></pre></li>				<li><span class="koboSpan" id="kobo.698.1">Apply</span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.699.1"> binary encoding to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.700.1">Country</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.701.1"> feature:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.702.1">
encoder = BinaryEncoder(cols=['Country'])
df_encoded = encoder.fit_transform(df)</span></pre></li>				<li><span class="koboSpan" id="kobo.703.1">Display the </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">encoded DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.705.1">
print(df_encoded)</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.706.1">Let’s have a look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">encoded data.</span></span></p>
			<h3><span class="koboSpan" id="kobo.708.1">Encoded output</span></h3>
			<p><span class="koboSpan" id="kobo.709.1">In this</span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.710.1"> example, binary encoding is applied to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.711.1">Country</span></strong><span class="koboSpan" id="kobo.712.1"> feature, as we can see in the </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">following output:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.714.1">
   Country_0  Country_1  Age  Income  Subscription
0          0          1   25   50000             1
1          1          0   30   60000             0
2          0          1   35   70000             1
3          1          0   40   80000             0
4          1          1   45   90000             1
5          0          1   50  100000             0
6          1          1   55  110000             1
7          1          0   60  120000             0</span></pre>			<p><span class="koboSpan" id="kobo.715.1">As we </span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.716.1">can see from the encoded output, the binary digits are split into separate columns. </span><span class="koboSpan" id="kobo.716.2">Let’s also have a look at the changes in the distribution after </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">the encoding:</span></span></p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<span class="koboSpan" id="kobo.718.1"><img src="image/B19801_10_7.jpg" alt="Figure 10.7 – Distribution of Country encoded feature"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.719.1">Figure 10.7 – Distribution of Country encoded feature</span></p>
			<p><span class="koboSpan" id="kobo.720.1">Let’s now discuss some things to keep in mind when encoding features with a </span><span class="No-Break"><span class="koboSpan" id="kobo.721.1">binary encoder.</span></span></p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor244"/><span class="koboSpan" id="kobo.722.1">Considerations for binary encoding</span></h2>
			<p><span class="koboSpan" id="kobo.723.1">When performing</span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.724.1"> binary encoding, there are several important considerations to keep </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">in mind:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.726.1">Binary encoding doesn’t provide direct interpretability for encoded features. </span><span class="koboSpan" id="kobo.726.2">The encoded binary patterns may not have a clear meaning, unlike one-hot encoding, where each binary feature corresponds to a </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">specific category.</span></span></li>
				<li><span class="koboSpan" id="kobo.728.1">The binary representation can become complex for categories with very high cardinality as the number of binary digits increases logarithmically with the number </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">of categories.</span></span></li>
				<li><span class="koboSpan" id="kobo.730.1">Some ML algorithms, particularly linear models, may not work well with binary-encoded features. </span><span class="koboSpan" id="kobo.730.2">Careful evaluation of algorithm compatibility </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">is necessary.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.732.1">Now that we’ve explored the nuances of the different encoding methods, let’s transition to summarizing their </span><a id="_idIndexMarker779"/><span class="koboSpan" id="kobo.733.1">key differences and considerations for practical application in </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">ML workflows.</span></span></p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor245"/><span class="koboSpan" id="kobo.735.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.736.1">Throughout this chapter, we explored various techniques for encoding categorical variables essential for ML tasks. </span><span class="koboSpan" id="kobo.736.2">Label encoding, which assigns unique integers to each category, is straightforward but may inadvertently impose ordinality where none exists. </span><span class="koboSpan" id="kobo.736.3">One-hot encoding transforms each category into a binary feature, maintaining categorical independence but potentially leading to high-dimensional datasets. </span><span class="koboSpan" id="kobo.736.4">Binary encoding condenses categorical values into binary representations, balancing interpretability, and efficiency particularly well for high-cardinality datasets. </span><span class="koboSpan" id="kobo.736.5">Frequency encoding replaces categories with their occurrence frequencies, capturing valuable information about distributional patterns. </span><span class="koboSpan" id="kobo.736.6">Target encoding incorporates target variable statistics into categorical encoding, enhancing predictive power while requiring careful handling to avoid </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">data leakage.</span></span></p>
			<p><span class="koboSpan" id="kobo.738.1">Let’s summarize our learning in the </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">following table:</span></span></p>
			<table id="table002-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.740.1">Encoding </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">Method</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.742.1">High </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">Cardinality</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.744.1">Preserves Ordinal </span><span class="No-Break"><span class="koboSpan" id="kobo.745.1">Information</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.746.1">Collisions</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.747.1">Interpretability</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.748.1">Good for</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.749.1">Not Good </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">for</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.751.1">Label </span><span class="No-Break"><span class="koboSpan" id="kobo.752.1">encoding</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.753.1">Good</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.754.1">Yes</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.755.1">No</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.756.1">Moderate</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.757.1">Tree-based </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">models</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.759.1">Linear </span><span class="No-Break"><span class="koboSpan" id="kobo.760.1">models</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.761.1">One-hot </span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">encoding</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.763.1">Poor</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.764.1">No</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.765.1">No</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.766.1">High</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.767.1">Linear models, </span><strong class="bold"><span class="koboSpan" id="kobo.768.1">neural </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.769.1">networks</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.770.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.771.1">NNs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">)</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.773.1">High-cardinality</span></span>
 <span class="No-Break"><span class="koboSpan" id="kobo.774.1">features</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.775.1">Target </span><span class="No-Break"><span class="koboSpan" id="kobo.776.1">encoding</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.777.1">Good</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.778.1">No</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.779.1">Possible</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.780.1">Low</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.781.1">Most algorithms</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.782.1">Small datasets (risk of </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">overfitting)</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.784.1">Frequency </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">encoding</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.786.1">Good</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.787.1">No</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.788.1">Possible</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.789.1">Moderate</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.790.1">Tree-based </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">models</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.792.1">Linear </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">models</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.794.1">Binary </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">encoding</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.796.1">Good</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.797.1">Partially</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.798.1">Possible</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.799.1">Low</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.800.1">Tree-based </span><span class="No-Break"><span class="koboSpan" id="kobo.801.1">models</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.802.1">Linear </span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">models</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.804.1">Table 10.2 – Comparison of all the encoding techniques</span></p>
			<p><span class="koboSpan" id="kobo.805.1">Each method offers distinct advantages depending on the dataset’s characteristics and the specific requirements of the modeling task. </span><span class="koboSpan" id="kobo.805.2">In the next chapter, we will shift focus to the considerations and methodologies involved in analyzing time series data. </span><span class="koboSpan" id="kobo.805.3">Time series data introduces temporal dependencies that require specialized techniques for feature engineering, as we will expand upon in the </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">next chapter.</span></span></p>
		</div>
	</body></html>