- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Sinks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the world of modern data processing, crucial decisions about data management,
    storage, and processing will determine successful outcomes. In this chapter, we
    will deep dive into three important pillars that underpin effective data processing
    pipelines: selecting the right **data sink**, choosing the optimal file type,
    and mastering partitioning strategies. By discussing these critical elements and
    their real-world applications, this chapter will equip you with the insights and
    strategies needed to architect data solutions that optimize efficiency, scalability,
    and performance within the complicated landscape of data processing technologies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right data sink for your use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right file type for your use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigating partitioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing an online retail data platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, we will need to install the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As always, you can find all the code for this chapter in this book’s GitHub
    repository: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter07](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter07).'
  prefs: []
  type: TYPE_NORMAL
- en: Each section is followed by a script with a similar naming convention, so feel
    free to execute the scripts and/or follow along by reading this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right data sink for your use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A data sink refers to a destination or endpoint where data is directed or stored.
    The term “sink” is used metaphorically to convey the idea of data flowing into
    and being absorbed by a designated location. Data sinks are commonly used as storage
    locations where data can be permanently or temporarily stored. This storage can
    be in the form of databases, files, or other data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers and data scientists often work with a variety of data sinks,
    depending on their specific tasks and use cases. Let’s look at some common data
    sinks, along with code examples, while considering the pros and cons of each type.
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Relational databases are a type of **database management system** (**DBMS**)
    that organizes data into tables with rows and columns, where each row represents
    a record and each column represents a field. The relationships between tables
    are established using keys. The primary key uniquely identifies each record in
    a table, and foreign keys create links between tables.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of relational databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is a quick overview of the key components of relational databases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tables**: Data is organized into tables, where each table represents a specific
    entity or concept. For example, in a database for a library, there might be tables
    for books, authors, and borrowers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rows and columns**: Each table consists of rows and columns. A row represents
    a specific record (e.g., a book), and each column represents a specific attribute
    or field of that record (e.g., title, author, and publication year).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keys**: Keys are used to establish relationships between tables. The primary
    key uniquely identifies each record in a table, and foreign keys in related tables
    create links between them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured Query Language** (**SQL**): Relational databases use SQL for querying
    and manipulating data. SQL allows users to retrieve, insert, update, and delete
    data, as well as define and modify the database’s structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the data field, we usually find relational databases in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured data**: If your data has a well-defined structure with clear relationships
    between entities, a relational database is a suitable choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data integrity requirements**: If maintaining data integrity is critical
    for your application (e.g., in financial systems or healthcare applications),
    a relational database provides mechanisms to enforce integrity constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Atomicity, consistency, isolation, and durability** **(ACID) properties**:
    **Atomicity** ensures a transaction is an all-or-nothing operation: either all
    changes are committed, or none are. For instance, in transferring money between
    accounts, atomicity guarantees both balances are updated together or not at all.
    **Consistency** means transactions move the database from one valid state to another
    while adhering to integrity constraints. If a rule such as unique customer IDs
    is violated, the transaction is rolled back to maintain consistency. **Isolation**
    ensures transactions execute independently, preventing interference and visibility
    of uncommitted changes between concurrent transactions. This avoids issues such
    as dirty reads. Finally, **durability** guarantees that once committed, changes
    persist, even after system failures, ensuring the permanence of updates such as
    contact information in an online application. If your application demands adherence
    to ACID properties, relational databases are designed to meet these requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex queries**: If your application involves complex queries and reporting
    needs, relational databases, with their SQL querying capabilities, are well-suited
    for such scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many different options for building relational databases, as we will
    see in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Different options for relational database management systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are many different **relational database management systems** (**RDBMSs**)
    out there. We’ve summarized the main ones in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Database** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| MySQL | An open source RDBMS known for its speed, reliability, and wide usage
    in web development |'
  prefs: []
  type: TYPE_TB
- en: '| PostgreSQL | An open source RDBMS with advanced features, extensibility,
    and support for complex queries |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle Database | A commercial RDBMS known for its scalability, security,
    and comprehensive set of data management features |'
  prefs: []
  type: TYPE_TB
- en: '| Microsoft SQL Server | A commercial RDBMS by Microsoft that integrates with
    Microsoft technologies and has business intelligence support |'
  prefs: []
  type: TYPE_TB
- en: '| SQLite | A lightweight, embedded, serverless RDBMS suitable for applications
    with low to moderate database requirements |'
  prefs: []
  type: TYPE_TB
- en: '| MariaDB | An open source RDBMS forked from MySQL that aims for compatibility
    while introducing new features |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Summary of RDBMSs
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see an example of how to quickly set up a local relational database,
    connect to it, and write a new table.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a PostgreSQL database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we need to install and set up PostgreSQL. This differs depending on
    the **operating system** (**OS**), but the logic remains the same. The following
    script automates the process of installing and setting up PostgreSQL on macOS
    or Debian-based Linux systems: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/setup_postgres.sh](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/setup_postgres.sh).'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, it detects the OS using the `uname` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If macOS is detected, it uses Homebrew to update package lists, install PostgreSQL,
    and start the PostgreSQL service. If a Debian-based Linux OS is detected, it uses
    `apt-get` to update package lists, install PostgreSQL and its `contrib` package,
    and start the PostgreSQL service. Here’s the code for installing macOS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If your OS isn’t supported by this script, then the following error message
    will be shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, you will need to install PostgreSQL *manually and start the service*.
    Once you’ve done that, you can continue to the second part of the script. The
    script then switches to the default `postgres` user to execute SQL commands that
    create a new database user if it doesn’t already exist, create a new database
    owned by this user, and grant all privileges on the database to the user, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute the preceding code, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you pull the repository to your local laptop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the folder where the repository is located.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a terminal in the repository folder location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute the following commands to navigate to the right folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: maria.zevrou@FVFGR3ANQ05P chapter7 % cd setup
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: maria.zevrou@FVFGR3ANQ05P set up % ls
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: setup_postgres.sh
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make the script executable by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, run the actual script using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After executing the script, you should see a confirmation message that the
    PostgreSQL setup, including the database and user creation, has been completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we’re ready to execute the script so that we can write our incoming data
    to the database we created in the previous step. You can find this script here:
    [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/1.postgressql.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/1.postgressql.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This script connects to the PostgreSQL database that we created previously
    and manages a table within it. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must define several functions, starting with `table_exists`. This
    function checks whether a specified table already exists in the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next function we need is the `create_table` function, which creates a new
    table if it doesn’t already exist within a specific schema. In our case, it will
    have three columns: `id` as the primary key, then `name` and `age`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must define the `insert_data` function, which inserts rows of data
    into the table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we must use the following function to display the retrieved data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, the script will create a mock DataFrame containing sample data
    (names and ages):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It establishes a connection to a PostgreSQL database using the specified connection
    parameters (database name, user, password, host, and port). These are the details
    we used in the previous step when we set up the database, so no change is required
    on your side:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, it checks whether a table named `example_table` exists, creates it
    if necessary, and then inserts the mock data into the table. After committing
    the changes to the database, the script fetches and prints the data from the table
    to confirm the successful insertion before finally closing the database connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To execute the preceding script, just execute the following command in the
    `chapter7` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Remember to always close the connections as it helps you avoid performance issues
    and ensures that new connections can be established when needed. It allows the
    database to free up resources associated with the connection, and it ensures that
    any uncommitted transactions are handled properly. Closing a connection returns
    it to the connection pool, making it available for reuse by other parts of your
    application.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the table that was created in the database, you can open the PSQL process
    in your terminal and connect to the `learn_sql` database by executing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the following command to list all the available tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 7.1 – List tables in the database](img/B19801_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – List tables in the database
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also interact now with the table by executing the following SQL commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Showing all the rows in the table](img/B19801_07_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Showing all the rows in the table
  prefs: []
  type: TYPE_NORMAL
- en: 'If you rerun the same Python script without dropping the existing table first,
    you won’t see a new table being created; instead, new rows will be added to the
    same table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Showing all the rows in the table once the script has been rerun](img/B19801_07_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Showing all the rows in the table once the script has been rerun
  prefs: []
  type: TYPE_NORMAL
- en: Having understood how to set up a relational database and use it as a sink by
    writing new data in it, let’s deep dive into the advantages and disadvantages
    of relational databases.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and disadvantages of relational databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we’ll summarize the advantages and disadvantages of using RDBMSs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: RDBMS systems have ACID properties, providing a robust framework for reliable
    and secure transactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RDBMS technology has been around for decades, resulting in mature and well-established
    systems with extensive documentation and community support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, they also have various disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: The rigid schema of RDBMS can be a limitation when dealing with evolving or
    dynamic data structures as they require schema modifications. Schema changes might
    be required for new data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RDBMSs are primarily designed for structured data and may not be the best choice
    for handling unstructured or semi-structured data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re wondering in what file type the data in the relational databases is
    written, then you’ll find the following subsection fascinating.
  prefs: []
  type: TYPE_NORMAL
- en: Relational database file types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In relational databases, the choice of file types for storing data is typically
    *abstracted* from users and developers, and it is not common to interact with
    the underlying files directly. Relational databases manage data storage and retrieval
    through their internal mechanisms, which often involve *proprietary* *file formats.*
  prefs: []
  type: TYPE_NORMAL
- en: The process of storing and organizing data within relational databases is managed
    by the DBMS, and users interact with the data using SQL or other query languages.
    The DBMS abstracts the physical storage details from users, providing a logical
    layer that allows for data manipulation and retrieval without direct concern for
    the underlying file formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss the key points regarding file types in relational databases:'
  prefs: []
  type: TYPE_NORMAL
- en: Relational database vendors often use proprietary file formats for their data
    storage. Each database management system may have *its own internal structure
    and mechanisms for* *managing data*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational databases typically organize data into *tablespaces*, which are logical
    storage containers. These tablespaces consist of pages or blocks where data is
    stored. The organization and structure of these pages are determined by the specific
    DBMS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational databases prioritize ACID properties to ensure data integrity and
    reliability. The internal file formats are designed to support these transactional
    guarantees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational databases use various indexing and optimization techniques to enhance
    query performance. The internal file structures, including B-trees or other indexing
    structures, are optimized for efficient data retrieval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users interact with relational databases using SQL commands.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While users typically don’t interact directly with the underlying file formats,
    understanding the concepts of tablespaces, pages, and how the DBMS manages data
    storage can be useful for database administrators and developers when they’re
    optimizing performance or troubleshooting issues.
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning from an RDBMS to a **not only SQL** (**NoSQL**) database involves
    a shift in data modeling, schema design, and querying approaches. We’ll explore
    the differences in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NoSQL databases, also known as **not only SQL** or **non-SQL** databases, are
    a class of database systems that provide a flexible and scalable approach to handling
    and storing data. Unlike traditional relational databases, which enforce a structured
    schema with predefined tables, columns, and relationships, NoSQL databases are
    designed to handle various data models, accommodating different ways of structuring
    and organizing data and providing a more dynamic and adaptable approach to data
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of NoSQL databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s a quick overview of the key components of NoSQL databases:'
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL databases often use a schemaless or schema-flexible approach, allowing
    data to be stored without the need for a predefined schema. This flexibility is
    particularly useful in cases where the data structure is evolving or not well-known
    in advance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL databases come in different types, each with its own data model, such
    as document-oriented (such as MongoDB), key-value stores (such as Redis), column-family
    stores (such as Apache Cassandra), and graph databases (such as Neo4j). Data models
    vary to accommodate different types of data and use cases. The document-oriented
    data model stores data as JSON documents, allowing each document to have a different
    structure, which is ideal for semi-structured or unstructured data. The key-value
    data model stores data as key-value pairs, with the value being a simple type
    or a complex structure, offering fast data retrieval but limited query capabilities.
    The column-family data model organizes data into columns instead of rows, enabling
    efficient storage and retrieval of large datasets. Lastly, the graph data model
    represents data as nodes and edges, making it perfect for applications focused
    on relationships, such as social networks and network analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL databases are typically designed for horizontal scalability, meaning they
    can efficiently distribute data across multiple nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL databases often adhere to the **consistency, availability, and partition
    tolerance** (**CAP**) theorem, which states that a distributed system can provide
    – at most – two out of three of the guarantees. NoSQL databases may prioritize
    availability and partition tolerance over strict consistency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the data field, we usually find NoSQL databases as sinks in the following
    cases:'
  prefs: []
  type: TYPE_NORMAL
- en: When we’re dealing with data models that may change frequently or aren’t well-defined
    in advance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an application anticipates or experiences rapid growth. In this case, horizontal
    scalability is essential.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the data can’t be put into tables with fixed relationships. In this case,
    a more flexible storage model is needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When rapid development and iteration are critical. In this case, we need to
    modify the data model on the fly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the specific features and capabilities of a particular type of NoSQL database
    align with the requirements of the application (e.g., document-oriented for content-heavy
    applications, key-value stores for caching, and graph databases for relationship-centric
    data).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see an example of how to connect to a NoSQL database and write a new table.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a MongoDB database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before diving into the code, we’ll take some time to explain MongoDB and some
    important concepts related to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document**: This is the basic unit of data in MongoDB and is represented
    as a **Binary JSON** (**BSON**) object. Documents are like rows in a relational
    database but can have varying structures. Documents are composed of fields (key-value
    pairs). Each field can contain different data types, such as strings, numbers,
    arrays, or nested documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collection**: A grouping of MongoDB documents, analogous to a table in a
    relational database. Collections contain documents and serve as the primary method
    of organizing data. Collections don’t require a predefined schema, allowing documents
    within the same collection to have different structures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database**: A container for collections. MongoDB databases hold collections
    and serve as the highest level of data organization. Each database is isolated
    from others, meaning operations in one database don’t affect others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have a better understanding, let’s go through the code. To run
    this example, set up MongoDB locally by following the documentation for your OS.
    For mac instructions, go here: [https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/](https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/).
    The following code example shows how to create a database and then write data
    in MongoDB using `pymongo`. Note that `pymongo` is the official Python driver
    for MongoDB and provides a Python interface for connecting to MongoDB databases,
    executing queries, and manipulating data using Python scripts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing MongoDB, open your Terminal and start the service. The commands
    presented here are for Mac; follow the commands in the documentation for your
    OS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Validate whether the service is running by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Having finished with the installations, let’s set up a MongoDB database.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In your Terminal, type the following command to go into the MongoDB editor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'best_collection_ever:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, we’re ready to switch to Python and start adding data to this
    collection. You can find the code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/2.pymongo.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/2.pymongo.py).
    Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Every time we connect to a NoSQL database, we need to provide the connection
    details. Update all the values for the parameters in the `mongo_params` dictionary,
    which contains the MongoDB server host, port, username, password, and authentication
    source:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at the different functions we will use in this example to insert
    the documents into the MongoDB database. The first function checks whether a collection
    exists in the database before creating a new one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following function takes a database and collection name as arguments and
    creates a collection with the name we pass (in our case, we provided `collection_name`
    as the name):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will take the collection we created in the previous step, which
    is just a placeholder for now, and insert some data into it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create some data to be inserted into the collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s specify the parameters that are required for the connection and create
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s check whether the collection with the provided name exists. If the
    collection exists, use the existing collection; if not, create a new collection
    with the provided name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, take the collection and insert the provided data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, close the MongoDB connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After executing this script, you should be able to see the records being added
    to the collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This script demonstrates how to interact with MongoDB databases and collections.
    Unlike relational databases, MongoDB does not require *tables or schemas to be
    created upfront*. Instead, you work directly with databases and collections. As
    an exercise to understand more about this flexible data model, try inserting data
    into the collection of a different structure. This contrasts with relational databases,
    where you insert rows into a *table with a fixed schema*. You can find an example
    here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/3.pymongo_expand.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/3.pymongo_expand.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the script uses the `find` method to query and retrieve documents from
    a collection. MongoDB queries are more flexible compared to SQL queries, especially
    for nested data.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t delete the MongoDB database
  prefs: []
  type: TYPE_NORMAL
- en: Please don’t clean the Mongo resources we created as we will use them in the
    streaming sink example. We will clean up all the resources at the end of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss the advantages and disadvantages that NoSQL
    databases offer.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and disadvantages of NoSQL databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s summarize the advantages and disadvantages of using NoSQL systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of NoSQL databases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: NoSQL databases are designed for horizontal scalability, allowing
    them to handle large volumes of data by distributing it across multiple servers.
    This makes them particularly suitable for big data applications and cloud environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Unlike SQL databases, which require a fixed schema, NoSQL
    databases offer flexible schemas. This allows structured, semi-structured, and
    unstructured data to be stored, making it easier to adapt to changing data models
    without significant restructuring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: NoSQL databases can offer superior performance for certain
    types of queries, especially when dealing with large datasets. They’re often optimized
    for high-speed data retrieval and can handle large volumes of transactions per
    second.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-effectiveness**: Many NoSQL databases are open source and can be scaled
    using commodity hardware, which reduces costs compared to the expensive hardware
    often required for scaling SQL databases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Developer agility**: The flexibility in schema and data models allows developers
    to iterate quickly and adapt to new requirements without the need for extensive
    database administration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, they also have their disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lack of standardization**: NoSQL databases don’t have a standardized query
    language such as SQL. This can lead to a steeper learning curve and make it challenging
    to switch between different NoSQL systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited support for complex queries**: NoSQL databases generally lack the
    advanced querying capabilities of SQL databases, such as joins and complex transactions,
    which can limit their use in applications that require complex data relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consistency**: Many NoSQL databases prioritize availability and partition
    tolerance over consistency (as per the CAP theorem). This can lead to eventual
    consistency models, which may not be suitable for applications that require strict
    data integrity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maturity and community support**: NoSQL databases are relatively newer compared
    to SQL databases, which means they may have less mature ecosystems and smaller
    communities. This can make finding support and resources more challenging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex maintenance**: The distributed nature of NoSQL databases can lead
    to complex maintenance tasks, such as data distribution and load balancing, which
    require specialized knowledge'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s discuss the file formats we may encounter when we’re working with
    NoSQL databases.
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL database file types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most prevalent file formats are JSON and BSON. JSON is a lightweight, human-readable
    data interchange format that uses a key-value pair structure and supports nested
    data structures. It’s widely adopted for web-based data exchange due to its simplicity
    and ease of parsing. JSON is language-agnostic, making it suitable for various
    programming languages. JSON’s flexible and schemaless nature aligns well with
    the flexible schema approach of many NoSQL databases and it allows for easy handling
    of evolving data structures. NoSQL databases often deal with semi-structured or
    unstructured data, and JSON’s hierarchical structure accommodates such data well.
    Here’s an example of a JSON data file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'BSON is a binary-encoded serialization for JSON-like documents and is designed
    to be efficient for storage and traversal. It adds additional data types not present
    in JSON, such as `date` and `binary`. BSON files are encoded before they’re stored
    and decoded before they’re displayed. BSON’s binary format is more efficient for
    storage and serialization, making it suitable for scenarios where data needs to
    be represented compactly. BSON is the primary data format that’s used in MongoDB.
    Let’s have a look at the BSON representation of the file presented previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In NoSQL databases, the choice between JSON and BSON often depends on the specific
    requirements of the database and the use case. While JSON is more human-readable
    and easy to work with in many scenarios, BSON’s binary efficiency is beneficial
    in certain contexts, particularly where storage and serialization efficiency are
    critical.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss data warehouses, what challenges they solve,
    and which use cases you should consider implementing when using them.
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transitioning to a data warehouse becomes important when the volume and complexity
    of data, as well as the need for advanced analytics, surpass the capabilities
    of your existing relational or NoSQL databases. If your relational database struggles
    with large data volumes, complex queries, or performance issues during analytical
    processing, a data warehouse can offer optimized storage and query performance
    for such workloads. Similarly, NoSQL databases, while excellent for handling unstructured
    or semi-structured data and scaling horizontally, may lack the sophisticated query
    capabilities and performance needed for in-depth analytics and reporting. Data
    warehouses are designed to integrate data from multiple sources, including both
    relational and NoSQL databases, facilitating comprehensive analysis and reporting.
    They provide robust support for historical data analysis, complex queries, and
    data governance, making them an ideal solution when you need to enhance your data
    integration, analytics, and reporting capabilities beyond what traditional databases
    offer.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of data warehouses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A data warehouse is a specialized database system designed for storing, organizing,
    and retrieving *large volumes of data* that are used for business intelligence
    and analytics efficiently. Unlike transactional databases, which are optimized
    for quick data updates and individual record retrievals, data warehouses are structured
    to support complex queries, aggregations, and reporting on historical and current
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a quick overview of the key components of data warehouses:'
  prefs: []
  type: TYPE_NORMAL
- en: Various data sources contribute to a data warehouse, including transactional
    databases, external files, logs, and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extract, transform, and load** (**ETL**) processes are used to gather data
    from source systems, transform it into a consistent format, and load it into the
    data warehouse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data warehouses employ optimized storage methods, such as columnar storage,
    to store large volumes of data efficiently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indexes and pre-aggregated tables are used to optimize query performance. In
    a data warehouse, indexes play a crucial role in optimizing query performance.
    An index is a data structure that enhances the speed and efficiency of data retrieval
    from a table by creating a separate, organized subset of the data. Indexes are
    typically created on one or more columns to facilitate faster querying. Without
    indexes, the database must scan the entire table to locate relevant rows. Indexes
    help the database quickly find rows that meet query conditions. Common candidates
    for indexing include columns used in `WHERE` clauses, `JOIN` conditions, and `ORDER
    BY` clauses. However, over-indexing can lead to diminishing returns and increased
    maintenance overhead. While indexes improve query performance, they also consume
    additional storage and can slow down write operations due to the need to maintain
    the index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques such as parallel processing and indexing are employed to enhance
    the speed of analytical queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with business intelligence tools allows users to create reports
    and dashboards and perform data analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is organized using multidimensional models, often in the form of star or
    snowflake schemas, to support analytics and reporting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s expand on dimensional modeling. It’s a design technique that’s used in
    data warehousing to structure data so that it supports efficient retrieval for
    analytical queries and reporting. Unlike traditional relational models, dimensional
    models are optimized for query performance and ease of use. In the next section,
    we will present the main schema types in dimensional models.
  prefs: []
  type: TYPE_NORMAL
- en: Schema types in dimensional models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dimensional modeling primarily involves two types of schemas: the star schema
    and the snowflake schema. The **star schema** is the simplest form of dimensional
    modeling and is where a central fact table is directly connected to multiple dimension
    tables, forming a star-like structure. This schema type is highly intuitive and
    easy to navigate, making it ideal for straightforward queries and reporting. Each
    dimension table in a star schema contains a primary key that maps to a foreign
    key in the fact table, providing descriptive context to the quantitative data
    in the fact table. For instance, a sales star schema might include a central sales
    fact table with foreign keys linking to dimension tables for products, customers,
    time, and stores, thereby simplifying complex queries by reducing the number of
    joins:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Star schema](img/B19801_07_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Star schema
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the **snowflake schema** is a more normalized version of
    the star schema and is where dimension tables are further broken down into related
    sub-tables, resembling a snowflake pattern. This structure reduces data redundancy
    and can save storage space, although it introduces more complexity in query design
    due to the additional joins required. For example, a product dimension table in
    a snowflake schema might be normalized into separate tables for product categories
    and brands, creating a multi-layered structure that ensures higher data integrity
    and reduces update anomalies. While the snowflake schema may be slightly more
    complex to query, it offers benefits in terms of data maintenance and scalability,
    especially in environments where data consistency and storage optimization are
    critical:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Snowflake schema](img/B19801_07_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Snowflake schema
  prefs: []
  type: TYPE_NORMAL
- en: Star schemas are often preferred for simpler hierarchies and when query performance
    is a higher priority, while snowflake schemas may be chosen when more efficient
    use of storage and achieving a higher level of normalization is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning from understanding schema types in dimensional modeling, it’s
    essential to explore the diverse options available for implementing and leveraging
    data warehouses in various organizational contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouse solutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are many different data warehouse options out there. We’ve summarized
    the main ones in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data Warehouse** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Databricks SQL | A cloud-based, serverless data warehouse that brings warehouse
    capabilities to data lakes. It’s also known for its scalability, performance,
    and parallel processing, as well as its built-in machine learning capabilities.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon Redshift | A fully managed, scalable data warehouse service in the
    cloud. It’s optimized for high-performance analytics. |'
  prefs: []
  type: TYPE_TB
- en: '| Snowflake | A cloud-based data warehouse with a multi-cluster, shared architecture
    that supports diverse workloads. |'
  prefs: []
  type: TYPE_TB
- en: '| Google BigQuery | A serverless, highly scalable data warehouse with built-in
    machine learning capabilities. |'
  prefs: []
  type: TYPE_TB
- en: '| **Data Warehouse** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Teradata | An on-premises or cloud-based data warehouse known for its scalability,
    performance, and parallel processing. |'
  prefs: []
  type: TYPE_TB
- en: '| Microsoft Azure Synapse Analytics | A cloud-based analytics service that
    offers both on-demand and provisioned resources. |'
  prefs: []
  type: TYPE_TB
- en: Table 7.2 – Data warehousing solutions
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at an example of creating a BigQuery table
    to illustrate the practical application of data warehouses.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a data warehouse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s learn how to create a new table in BigQuery. Google Cloud provides a
    client library for various programming languages, including Python, to interact
    with BigQuery. To get the ready so that you can run the following example, go
    to the BigQuery documentation: [https://cloud.google.com/python/docs/reference/bigquery/latest](https://cloud.google.com/python/docs/reference/bigquery/latest).
    Let’s deep dive into the example. We will follow the same patterns that have been
    presented so far in this chapter. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To run this example, you need to have a **Google Cloud Platform** (**GCP**)
    account and a Google Storage bucket ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, we’ll set up the project ID. Replace `your_project_id` with your actual
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the dataset and table name and update the following fields:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check whether the dataset and the table exist:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the table schema (replace it with your schema if you wish to update
    the data). In this example, we will create a table with two columns (`column1`
    and `column2`). The first column will be of the `STRING` type, while the second
    one will be of the `INTEGER` type. The first column can’t contain missing values,
    whereas the second one can:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s perform the checks for the existence of a table with the same name. If
    the table doesn’t exist, then we create it with the provided name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create some mock data that will be inserted into the table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Construct the data to be inserted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Insert the data and check for errors. If everything works as expected, close
    the connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If any insertion errors occur, print them out:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Close the BigQuery client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What’s the difference between a container and a table?
  prefs: []
  type: TYPE_NORMAL
- en: A container (called different things in different systems, such as database,
    dataset, or schema) is a logical grouping mechanism that’s used to organize and
    manage data objects such as tables, views, and related metadata. Containers provide
    a way to partition and structure data based on specific needs, such as access
    control, data governance, or logical separation of data domains. On the other
    hand, a table is a fundamental data structure that stores the actual data records,
    organized into rows and columns. Tables define the schema (column names and data
    types) and hold the data values.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, let’s transition from understanding the fundamental components
    of a data warehouse environment to discussing the advantages and disadvantages
    that data warehouses offer in managing and analyzing large volumes of data efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and disadvantages of data warehouses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s summarize the advantages and disadvantages of using data warehouses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimized for analytical queries and large-scale data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle massive amounts of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with other data tools and services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are their disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Higher costs for storage and querying compared to traditional databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Might have limitations regarding real-time data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File types in data warehouses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In terms of file formats, it’s accurate to say that many modern data warehouses
    use **proprietary** internal storage formats for writing data. These proprietary
    formats are usually columnar storage formats that are optimized for efficient
    querying and analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the differences we may find in these proprietary formats:'
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses often use columnar storage formats such as Parquet, ORC, or
    Avro. While these formats are open and widely adopted, each data warehouse might
    have its internal optimizations or extensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The actual implementations of these columnar storage formats may have vendor-specific
    optimizations or features. For example, how a specific data warehouse handles
    compression, indexing, and metadata might be specific to the vendor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users interact with data warehouses through standard interfaces, such as SQL
    queries. The choice of storage format often doesn’t affect users, so long as the
    data warehouse supports common data interchange formats for import and export.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, while the internal storage mechanisms may have vendor-specific optimizations,
    the use of well-established, open, and widely adopted columnar storage formats
    ensures a degree of interoperability and flexibility. Users typically interact
    with data warehouses using standard SQL queries or data interchange formats such
    as CSV, JSON, or Avro for data import/export, which adds a layer of standardization
    to the external-facing aspects of these systems.
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning from traditional data warehouses to data lakes represents a strategic
    shift that embraces a more flexible and scalable paradigm. In the next section,
    we will deep dive into data lakes.
  prefs: []
  type: TYPE_NORMAL
- en: Data lakes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transition from traditional data warehouses to data lakes represents a shift
    in how organizations handle and analyze their data. Traditional data warehouses
    are designed to store structured data, which is highly organized and formatted
    according to a predefined schema, such as tables with rows and columns in relational
    databases. Structured data is easy to query and analyze using SQL. However, data
    warehouses struggle with handling unstructured data, which lacks a predefined
    format or organization. Examples of unstructured data include text documents,
    emails, images, videos, and social media posts. Data lakes, on the other hand,
    offer a more flexible and scalable solution by storing both structured and unstructured
    data in its native format. This allows organizations to ingest and store vast
    amounts of data without the need for immediate structuring. This transition addresses
    the limitations of data warehouses, offering a more versatile and future-proof
    approach to data management.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of data lakes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A data lake is a centralized repository that allows organizations to store
    vast amounts of raw and unstructured data in any format needed. They’re designed
    to handle diverse data types, such as structured, semi-structured, and unstructured
    data, and enable data exploration, analytics, and machine learning. Data lakes
    solve several problems: they enable the consolidation of diverse data sources,
    break down silos by providing a unified data storage solution, and support advanced
    analytics by making all types of data easily accessible.'
  prefs: []
  type: TYPE_NORMAL
- en: Remember
  prefs: []
  type: TYPE_NORMAL
- en: Think of data lakes as a filesystem, such as for storing data in a file location
    on your laptop, just on a much larger scale.
  prefs: []
  type: TYPE_NORMAL
- en: Data lake solutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s a summary of the available data lake solutions in the data space:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data Lake** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon S3 | A cloud-based object storage service by **Amazon Web Services**
    (**AWS**). It’s commonly used as a foundation for data lakes. |'
  prefs: []
  type: TYPE_TB
- en: '| Azure Data Lake Storage | A scalable and secure cloud-based storage solution
    by Microsoft Azure that’s designed to support big data analytics and data lakes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Hadoop Distributed File** **System** (**HDFS**) | A distributed filesystem
    that forms the storage layer of Apache Hadoop, an open source big data processing
    framework. |'
  prefs: []
  type: TYPE_TB
- en: '| Google Cloud Storage | Google Cloud’s object storage service, often used
    as part of a data lake architecture in the GCP. |'
  prefs: []
  type: TYPE_TB
- en: Table 7.3 – Data lake solutions
  prefs: []
  type: TYPE_NORMAL
- en: 'The shift from traditional data warehouses to data lakes represents a fundamental
    transformation in how organizations manage and analyze data. This shift was driven
    by several factors, including the need for greater flexibility, scalability, and
    the ability to handle diverse and unstructured data types. The following table
    highlights the key differences between traditional data warehouses and data lakes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Data Warehouses** | **Data Lakes** |'
  prefs: []
  type: TYPE_TB
- en: '| Data variety and flexibility | Traditional data warehouses are designed to
    handle structured data and are less adaptable to handle diverse data types or
    unstructured data. | Data lakes emerged as a response to the increasing volume
    and variety of data. They provide a storage repository for raw, unstructured,
    and diverse data types, allowing organizations to store large volumes of data
    without the need for predefined schemas. |'
  prefs: []
  type: TYPE_TB
- en: '| Scalability | Traditional data warehouses often face scalability challenges
    when dealing with massive amounts of data. Scaling up a data warehouse can be
    costly and may have limitations. | Data lakes, particularly cloud-based solutions,
    offer scalable storage and computing resources. They can efficiently scale horizontally
    to handle growing datasets and processing demands. |'
  prefs: []
  type: TYPE_TB
- en: '|  | **Data Warehouses** | **Data Lakes** |'
  prefs: []
  type: TYPE_TB
- en: '| Cost-efficiency | Traditional data warehouses can be expensive to scale,
    and the cost structure may not be conducive to storing large volumes of raw or
    less structured data. | Cloud-based data lakes often follow a pay-as-you-go model,
    allowing organizations to manage costs more efficiently by paying for the resources
    they use. This is particularly beneficial for storing large amounts of raw data.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Schema-on-dead versus schema-on-write | Follow a schema-on-write approach,
    where data is structured and transformed before being loaded into the warehouse.
    | Follow a schema-on-read approach, allowing for the storage of raw, untransformed
    data. The schema is applied at the time of data analysis, providing more flexibility
    in data exploration. |'
  prefs: []
  type: TYPE_TB
- en: Table 7.4 – Data warehouses versus data lakes
  prefs: []
  type: TYPE_NORMAL
- en: 'The emergence of the Lakehouse architecture further refined the shift away
    from data warehouses by solving some key challenges associated with data lakes
    and by bringing features traditionally associated with data warehouses into the
    data lake environment. Here’s a breakdown of the key aspects of this evolution:'
  prefs: []
  type: TYPE_NORMAL
- en: The Lakehouse integrates ACID transactions into the data lake, providing transactional
    capabilities that were traditionally associated with data warehouses. This ensures
    data consistency and reliability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Lakehouse supports schema evolution, allowing changes to be made to data
    schemas over time without requiring a full transformation of existing data. This
    enhances flexibility and reduces the impact of schema changes on existing processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Lakehouse introduces features for managing data quality, including schema
    enforcement and constraints, ensuring that data stored in the lake meets specified
    standards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Lakehouse aims to provide a unified platform for analytics by combining
    the strengths of data lakes and data warehouses. It allows organizations to perform
    analytics on both structured and semi-structured data in a centralized repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Lakehouse enhances the metadata catalog, providing a comprehensive view
    of data lineage, quality, and transformations. This facilitates better governance
    and understanding of the data stored in the lake.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Lakehouse concept has evolved through discussions in the data and analytics
    community, with various companies contributing to the development and adoption
    of Lakehouse principles.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a data lake
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s explore an example of how to write some Parquet files on S3, AWS’s cloud
    storage. To get everything set up, go to the AWS documentation: [https://docs.aws.amazon.com/code-library/latest/ug/python_3_s3_code_examples.html](https://docs.aws.amazon.com/code-library/latest/ug/python_3_s3_code_examples.html).
    Now, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To run this example, you need to have an AWS account and an S3 bucket ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by importing the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we’re going to create some mock data so that we can write it to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must convert the DataFrame into Parquet format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update your authentication key and the bucket name in which we’re going to
    write the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Connect to the S3 bucket using the connection details from the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload the Parquet file to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As discussed previously, Lakehouses come with their own set of advantages and
    disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Lakehouses provide a unified platform that integrates the strengths of both
    data lakes and data warehouses. This allows organizations to leverage the flexibility
    of data lakes and the transactional capabilities of data warehouses in a single
    environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lakehouses follow a schema-on-read approach, allowing for the storage of raw,
    untransformed data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lakehouses support diverse data types, including structured, semi-structured,
    and unstructured data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lakehouses integrate ACID transactions, providing transactional capabilities
    that ensure data consistency and reliability. This is particularly important for
    use cases where data integrity is critical.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many Lakehouse solutions offer time travel capabilities, allowing users to query
    data at specific points in time. Versioning of data provides historical context
    and supports audit requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lakehouses often implement optimized storage formats (e.g., Delta and Iceberg)
    that contribute to storage efficiency and improved query performance, especially
    for large-scale analytical workloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Users and administrators may need to adapt to a new way of working with data
    while considering both schema-on-read and schema-on-write paradigms. This may
    require training and education.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the implementation and cloud provider, costs associated with storing,
    processing, and managing data in a Lakehouse architecture may vary. Organizations
    need to carefully manage costs to ensure efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we’ve discussed, Lakehouses have the amazing advantage of allowing any data
    type to be ingested and stored, from structured to semi-structured to unstructured.
    This means we can find any file type in the ingestion part of the process, from
    CSVs to Parquets and Avros. While in the ingestion part, we can see any file type,
    on the write part, we can take advantage of the flexibility the Lakehouse offers
    and store the data in optimized open table file formats. Open table format is
    a file format that’s used to store tabular data in a way that’s easily accessible
    and interoperable across various data processing and analytics tools.
  prefs: []
  type: TYPE_NORMAL
- en: File types in data lakes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Lakehouse architecture, we have three prominent formats: Delta, Apache Iceberg,
    and Apache Hudi. These formats provide features such as ACID transactions, schema
    evolution, incremental data processing, and read and write optimizations. Here’s
    a brief overview of these formats:'
  prefs: []
  type: TYPE_NORMAL
- en: Delta Lake is an open source storage layer designed to enhance the reliability
    and performance of data processing in data lakes. It is well-suited for building
    data lakes on infrastructure such as S3 or Azure Storage and has strong support
    for ACID transactions and data versioning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Iceberg is another open source table format that’s optimized for fast
    query performance. It’s a good choice when query efficiency is required and it
    has excellent support for schema evolution and versioning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Hudi (Hadoop Upserts, Deletes, and Incrementals) is another open source
    data lake storage format that provides great support for real-time data processing
    and streaming features. While it may not be as widely known as Delta Lake or Apache
    Iceberg, Hudi is gaining traction, especially in Apache Spark and Hadoop ecosystems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, all these formats were built to solve the same challenges, which
    is why they have a lot of common features. Thus, before choosing the best one
    for your workload, there are a couple of things you should consider to ensure
    you’re going in the right direction:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the compatibility of each technology with your existing data processing
    ecosystem and tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the level of community support, ongoing development, and adoption within
    the data community for each technology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess the performance characteristics of each technology concerning your specific
    use case, especially in terms of read and write operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, the choice between Delta Lake, Apache Iceberg, and Apache Hudi should
    be driven by the specific requirements and priorities of your data lake or lakehouse
    environment. It’s also beneficial to experiment and benchmark each solution with
    your data and workloads to make an informed decision.
  prefs: []
  type: TYPE_NORMAL
- en: The last sink technology we’re going to discuss is streaming data sinks.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data sinks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transition from batch and micro-batch processing to streaming technologies
    marks a significant evolution in data processing and analytics. Batch processing
    involves collecting and processing data in large, discrete chunks at scheduled
    intervals, which can lead to delays in data availability and insights. Micro-batch
    processing improves on this by handling smaller batches at more frequent intervals,
    reducing latency but still not achieving real-time data processing. Streaming
    technologies, however, enable the continuous ingestion and processing of data
    in real-time, allowing organizations to immediately analyze and act on data as
    it arrives. This shift to streaming technologies addresses the growing need for
    real-time analytics and decision-making in today’s fast-paced business environments,
    providing a more dynamic and responsive approach to data management.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of streaming data sinks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Streaming data sinks are components or services that consume and store streaming
    data in real time. They act as the endpoint where streaming data is ingested,
    processed, and persisted for further analysis or retrieval. Here’s an overview
    of streaming data sinks and their main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingestion component**: This is responsible for receiving and accepting incoming
    data streams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processing logic**: This is a bespoke logic that may include components for
    data enrichment, transformation, or aggregation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage component**: This persists streaming data for future analysis or
    retrieval'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connectors**: Their main role is to interact with various data processing
    or storage systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We usually implement streaming sinks in the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: In real-time analytics systems. This allows organizations to gain insights into
    their data as events occur.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In systems monitoring, where streaming data sinks capture and process real-time
    metrics, logs, or events, enabling immediate alerting and responses to issues
    or anomalies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In financial transactions or e-commerce. Here, streaming data sinks can be used
    for real-time fraud detection by analyzing patterns and anomalies in transaction
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the **Internet of Things** (**IoT**) scenarios, streaming data sinks handle
    the continuous flow of data from sensors and devices, supporting real-time monitoring
    and control.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s have a look at the available options we have for streaming sinks.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming sinks solutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many cloud platforms offer managed streaming data services that act as sinks,
    such as Amazon Kinesis, Azure Event Hubs, and Google Cloud Dataflow, as shown
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Streaming** **Data Sink** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon Kinesis | A fully managed service for real-time stream processing
    in AWS. It supports data streams, analytics, and storage. |'
  prefs: []
  type: TYPE_TB
- en: '| Azure Event Hub | A cloud-based real-time analytics service in Azure for
    processing and analyzing streaming data. |'
  prefs: []
  type: TYPE_TB
- en: '| Google Cloud Dataflow | A fully managed stream and batch processing service
    on GCP. |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Kafka | A distributed streaming platform that can act as both a source
    and a sink for streaming data. |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Spark Streaming | A real-time data processing framework that’s part
    of the Apache Spark ecosystem. |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Flink | A stream processing framework that supports event time processing
    and various sink connectors. |'
  prefs: []
  type: TYPE_TB
- en: Table 7.5 – Different streaming data services
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will use one of the most popular streaming sinks, Kafka,
    to get an idea of what writing in a streaming sink looks like.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a streaming data sink
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First things first, let’s get an initial understanding of the main components
    of Apache Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Brokers** are the core servers that make up a Kafka cluster. They handle
    the storage and management of messages. Each broker is identified by a unique
    ID. Brokers are responsible for replicating data across the cluster for fault
    tolerance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topics** are the primary abstractions in Kafka for organizing and categorizing
    messages. They are like tables in a database or folders in a filesystem. Messages
    are published to and read from specific topics. Topics can be partitioned for
    scalability and parallel processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partitions** are the units of parallelism in Kafka. Each topic is divided
    into one or more partitions, which allow for distributed storage and processing
    of data. Messages within a partition are ordered and immutable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Producers** are client applications that publish (write) messages to Kafka
    topics. They can choose which partition to send messages to or use a partitioning
    strategy. Producers are responsible for serializing, compressing, and load balancing
    data among partitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumers** are client applications that subscribe to (read) messages from
    Kafka topics. They can read from one or more partitions of a topic and keep track
    of which messages they have already consumed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ZooKeeper** is used for managing and coordinating Kafka brokers. It maintains
    metadata about the Kafka cluster. Newer versions of Kafka are moving toward removing
    the ZooKeeper dependency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have a better understanding of the main Kafka components, let’s
    start with our step-by-step guide. We will need to install a couple of components
    for this example, so stay with me as we go through the process. To simplify this,
    we will use Docker as it allows you to define the entire environment in a `docker-compose.yml`
    file, making it easy to set up Kafka and Zookeeper with minimal configuration.
    This eliminates the need to manually install and configure each component on your
    local machine. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download Docker by following the public documentation: [https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, set up Kafka with Docker. For this, let’s have a look at the `docker-compose.yml`
    file at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/docker-compose.yml](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/docker-compose.yml).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This Docker Compose configuration sets up a simple Kafka and Zookeeper environment
    using version 3 of the Docker Compose file format. The configuration defines two
    services: `zookeeper` and `kafka`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zookeeper uses the `confluentinc/cp-zookeeper:latest` image. It maps the host
    machine’s port, `2181`, to the container’s port, `2181`, for client connections.
    The `ZOOKEEPER_CLIENT_PORT` environment variable is set to `2181`, which specifies
    the port Zookeeper will listen on for client requests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Kafka uses the `confluentinc/cp-kafka:latest` image. It maps the host machine’s
    port, `9092`, to the container’s port, `9092`, for external client connections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are some key environment variables that configure Kafka:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`KAFKA_BROKER_ID` is set to `1`, identifying this broker uniquely in a Kafka
    cluster'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_ZOOKEEPER_CONNECT` points to the Zookeeper service (`zookeeper:2181`),
    allowing Kafka to connect to Zookeeper for managing cluster metadata'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_ADVERTISED_LISTENERS` advertises two listeners:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PLAINTEXT://kafka:29092` for internal Docker network communication'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PLAINTEXT_HOST://localhost:9092` for connections from outside the Docker network
    (e.g., from the host machine)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_LISTENER_SECURITY_PROTOCOL_MAP` ensures both advertised listeners use
    the `PLAINTEXT` protocol, meaning no encryption or authentication'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_INTER_BROKER_LISTENER_NAME` is set to `PLAINTEXT`, specifying which
    listener Kafka brokers will use to communicate with each other'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR` is set to `1`, meaning the offsets
    topic (used for storing consumer group offsets) will not be replicated across
    multiple brokers, which is typical for a single-broker setup'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This setup is ideal for local development or testing, where you need a simple,
    single-node Kafka environment without the complexities of a multi-node, production-grade
    cluster. Now, we’re ready to run the container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s run the Docker container to start Kafka and Zookeeper. In your terminal,
    enter the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will take the Kafka and Zookeeper images and install them in your environment.
    You should see something similar to the following printed on your terminal:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Kafka producer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let’s go back to our Python IDE and look at how we can push data to a
    Kafka producer. For this, we’re going to read the data written in MongoDB and
    produce it in Kafka. You can find the code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4a.kafka_producer.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4a.kafka_producer.py).
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define the MongoDB connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `MongoClient('mongodb://localhost:27017')` connects to a MongoDB instance
    running on localhost at the default port of `27017`. This creates a client object
    that allows interaction with the database. Then, `db = mongo_client['no_sql_db']`
    selects the `no_sql_db` database from the MongoDB instance. Finally, `collection
    = db['best_collection_ever']` selects the `best_collection_ever` collection from
    the `no_sql_db` database.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s perform the Kafka producer configuration that creates a Kafka producer
    object with the specified configuration. This producer will be used to send messages
    (in this case, MongoDB documents) to a Kafka topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following function is a callback function that will be called when the
    Kafka producer finishes sending a message. It checks whether there was an error
    during the message delivery and prints a message indicating success or failure.
    This function provides feedback on whether messages were successfully sent to
    Kafka, which is useful for debugging and monitoring:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read from MongoDB and produce to Kafka for the document in `collection.find()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code iterates over each document in the `best_collection_ever`
    collection. The `find()` method retrieves all documents from the collection. Then,
    `message = json.dumps(document, default=str)` converts each MongoDB document (a
    Python dictionary) into a JSON string. The `default=str` parameter handles data
    types that aren’t JSON serializable by converting them into strings. Next, `producer.produce('mongodb_topic',
    value=message.encode('utf-8'), callback=delivery_report)` sends the JSON string
    as a message to the `mongodb_topic` Kafka topic. The message is encoded in UTF-8,
    and the `delivery_report` function is set as a callback to handle delivery confirmation.
    Finally, `producer.poll(0)` ensures that the Kafka producer processes delivery
    reports and other events. This is necessary to keep the producer active and responsive.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This ensures that all messages in the producer’s queue are sent to Kafka before
    the script exits. Without this step, there might be unsent messages remaining
    in the queue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running this script, you should see the following print statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: So far, we’ve connected to a MongoDB database, read the documents from a collection,
    and sent these documents as messages to the Kafka topic.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka consumer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, let’s run the consumer so that it can consume the messages from the Kafka
    producer. The full code can be found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4b.kafka_consumer.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4b.kafka_consumer.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must create the Kafka consumer configuration that specifies the Kafka
    broker(s) to connect to. Here, it’s connecting to a Kafka broker running on localhost
    at port `9092`. In this case, `group.id` sets the consumer group ID, which allows
    multiple consumers to coordinate and share the work of processing messages from
    a topic. Messages will be distributed among consumers in the same group. Next,
    `auto.offset.reset` defines the behavior when there is no initial offset in Kafka
    or if the current offset doesn’t exist. Setting this to `earliest` means the consumer
    will start reading from the earliest available message in the topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will instantiate a Kafka consumer with the configuration specified
    earlier. Here, `consumer.subscribe([''mongodb_topic''])` subscribes the consumer
    to the `mongodb_topic` Kafka topic. This means the consumer will receive messages
    from this topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the duration for which the consumer should run (in seconds):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code begins an infinite loop that will run until it’s explicitly
    broken out of. This loop continuously polls Kafka for new messages. Here, `if
    time.time() - start_time > run_duration` checks whether the consumer has been
    running for longer than the specified `run_duration`. If so, it prints a message
    and breaks out of the loop, stopping the consumer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running the preceding code, you should see the following print statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The goal of this example was to show you how data can be continuously read from
    a NoSQL database such as MongoDB and then streamed in real-time to other systems
    using Kafka. Kafka acts as a messaging system that allows data producers (e.g.,
    MongoDB) to be decoupled from data consumers. This example also illustrates how
    data can be processed in stages, allowing for scalable and flexible data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of a real use case scenario, imagine that we are building a ride-sharing
    app. Handling events such as ride requests, cancellations, and driver statuses
    in real-time is crucial for efficiently matching riders with drivers. MongoDB
    stores this event data, such as ride requests and driver locations, while Kafka
    streams the events to various microservices. These microservices then process
    the events to make decisions, such as assigning a driver to a rider. By using
    Kafka, the system becomes highly responsive, scalable, and resilient as it decouples
    event producers (such as ride requests) from consumers (such as the driver assignment
    logic).
  prefs: []
  type: TYPE_NORMAL
- en: To summarize what we have seen so far, in contrast to relational sinks, which
    involve structured data with defined schemas, Kafka can serve as a buffer or intermediary
    for data ingestion, allowing for decoupled and scalable data pipelines. NoSQL
    sinks often handle unstructured or semi-structured data, similar to Kafka’s flexibility
    with message formats. Kafka’s ability to handle high-throughput data streams complements
    NoSQL databases’ scalability and flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'To clean all the resources that have been used so far, execute the cleaning
    script: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/cleanup_script.sh](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/cleanup_script.sh).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will deep dive into the file format seen in streaming
    data sinks.
  prefs: []
  type: TYPE_NORMAL
- en: File types in streaming data sinks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Streaming data sinks primarily deal with messages or events rather than traditional
    file storage. The data that’s transmitted through streaming data sinks is often
    in formats such as JSON, Avro, or binary. These formats are commonly used for
    serializing and encoding data in streaming scenarios. They are efficient and support
    schema evolution. In the *NoSQL databases* section of this chapter, we deep-dived
    in the JSON file format. Here, we’ll look at Avro and binary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Avro is a binary serialization format developed within the Apache Hadoop
    project. It uses a schema to define data structures, allowing for efficient serialization
    and deserialization. Avro is known for its compact binary representation, providing
    fast serialization and efficient storage. In streaming scenarios, minimizing data
    size is crucial for efficient transmission over the network. Avro’s compact binary
    format reduces data size, improving bandwidth utilization. Avro also supports
    schema evolution, allowing for changes in data structures over time without requiring
    all components to be updated simultaneously. Avro’s schema-based approach enables
    interoperability between different systems and languages, making it suitable for
    diverse ecosystems. Let’s see an example of an Avro file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Binary formats use a compact binary representation of data, resulting in efficient
    storage and transmission. Various binary protocols can be employed based on specific
    requirements, such as Google’s **Protocol Buffers** (**protobuf**) or Apache Thrift.
    Binary formats minimize the size of the transmitted data, reducing bandwidth usage
    in streaming scenarios. Binary serialization and deserialization are generally
    faster than text-based formats, which is crucial in high-velocity streaming environments.
    Let’s have a look at a binary file in protobuf:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: In streaming sinks, the choice between JSON, Avro, or binary depends on the
    specific requirements of the streaming use case, including factors such as interoperability,
    schema evolution, and data size considerations.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve discussed the most common data sinks used by data engineers and
    data scientists, as well as the different file types we usually encounter with
    those sinks. In the following sections, we will provide a summary of all the discussed
    data sinks and file types, as well as their pros and cons and when to best use
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Which sink is the best for my use case?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s summarize what we’ve learned regarding the different data sinks and get
    a deeper understanding of when to use which:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Technology** | **Pros** | **Cons** | **When** **to Choose** | **Use Case**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Relational database | ACID properties ensure data consistency.Mature query
    languages (SQL) for complex queries.Support for complex transactions and joins.
    | Limited scalability for read-heavy workloads.Schema changes may be challenging
    and downtime-prone.May not scale well horizontally. | Structured data with a well-defined
    schema.When you’re maintaining relationships between data entities. | Transactional
    applicationsEnterprise applications with structured data. |'
  prefs: []
  type: TYPE_TB
- en: '| NoSQL database | Flexible schema, suitable for semi-structured or unstructured
    data.Scalability – horizontal scaling is often easier.High write throughput for
    certain workloads. | Lack of standardized query language may require learning
    a specific API.May lack ACID compliance in favor of eventual consistency.Limited
    support for complex transactions. | Dynamic or evolving data schema.Rapid development
    and iteration.Handling large volumes of data with varying structures. | Document
    databases for content management.Real-time a pplications with variable schema.JSON
    data storage for web applications |'
  prefs: []
  type: TYPE_TB
- en: '| Data warehouse | Optimized for complex analytics and reporting.Efficient
    data compression and indexing.Scalability for read-heavy analytical workloads.
    | May not be cost-effective for high-volume transactional workloads.May have higher
    latency for real-time queries.May require specialized skills for maintenance and
    optimization. | Analytical processing on large datasets.Aggregating and analyzing
    historical data. | Business intelligence and reporting tools.Running complex queries
    on terabytes of data. |'
  prefs: []
  type: TYPE_TB
- en: '| **Technology** | **Pros** | **Cons** | **When** **to Choose** | **Use Case**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Lakehouse | Unified platform combining data lake and data warehouse features.Offers
    scalable storage and computing resources.It can efficiently scale horizontally
    to handle growing datasets and processing demands.Pay-as-you-go model, allowing
    organizations to manage costs more efficiently by paying for the resources they
    use. This is particularly beneficial for storing large amounts of raw data.Follow
    a schema-on-read approach, allowing for the storage of raw, untransformed data.
    | Complexity in managing both schema-on-read and schema-on-write.Depending on
    the implementation and cloud provider, costs associated with storage, processing,
    and managing data in a Lakehouse architecture may vary. Organizations need to
    carefully manage costs to ensure efficiency. | A balance between flexibility and
    transactional capabilities. | Real-time analytics with long-term storage.Any engineering,
    machine learning, and analytics use case |'
  prefs: []
  type: TYPE_TB
- en: '| Streaming sinks | Enables real-time processing and analysis of streaming
    data.Scales horizontally to handle high volumes of incoming data.Integral to building
    event-driven architectures. | Implementing and managing streaming data sinks can
    be complex.The processing and persistence of streaming data introduces some latency.Depending
    on the chosen solution, infrastructure costs may be a consideration. | Continuous
    ingestion and processing of data in real-time | IoTReal-time analytical use casesSystems
    monitoring |'
  prefs: []
  type: TYPE_TB
- en: Table 7.6 – Summary table of all the data sinks, as well as their pros, cons,
    and use cases
  prefs: []
  type: TYPE_NORMAL
- en: In *Table 7.8*, the **Use Case** column provides more context and practical
    examples of how each data sink technology can be effectively applied in real-world
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Moving from selecting the right data sink technology to choosing the appropriate
    file type is a crucial step in designing an effective data processing pipeline.
    Once you’ve determined where your data will be stored (data sink), you need to
    consider how it will be stored (file type). The choice of file type can impact
    data storage efficiency, query performance, data integrity, and interoperability
    with other systems.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding file types for optimal usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing the right file type when selecting a data sink is crucial for optimizing
    data storage, processing, and retrieval. One of the file types that we haven’t
    discussed so far but is very important since it’s used as an underline format
    for other file formats is the Parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: Parquet is a columnar storage file format designed for efficient data storage
    and processing in big data and analytics environments. It is an open standard
    file format that provides benefits such as high compression ratios, columnar storage,
    and support for complex data structures. Parquet is widely adopted in the Apache
    Hadoop ecosystem and is supported by various data processing frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Parquet stores data in a columnar format, which means values from the same column
    are stored together. This design is advantageous for analytics workloads where
    queries often involve selecting a subset of columns. Parquet also supports different
    compression algorithms, allowing users to choose the one that best suits their
    requirements. This contributes to reduced storage space and improved query performance.
    Parquet files can handle schema evolution as well, making it possible to add or
    remove columns without requiring a complete rewrite of the dataset. This feature
    is essential for scenarios where the data schema evolves. Due to its advantages,
    Parquet has become a widely adopted and standardized file format in the big data
    ecosystem, forming the basis for other optimized formats, such as Delta and Iceberg.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having discussed Parquet files, we can now compare the common file types, along
    with their pros and cons, and provide guidance on when to choose each type for
    different data sinks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **File Type** | **Pros** | **Cons** | **When** **to Choose** |'
  prefs: []
  type: TYPE_TB
- en: '| JSON | Human-readable | Larger file size compared to binary formatsSlower
    serialization/ deserialization | Semi-structured or human-readable data is required
    |'
  prefs: []
  type: TYPE_TB
- en: '| BSON | Compact binary formatSupports richer data types | May not be as human-readable
    as JSON, with limited adoption outside MongoDB | Efficiency in storage and transmission
    is crucial |'
  prefs: []
  type: TYPE_TB
- en: '| Parquet | Columnar storage, which is efficient for analyticsCompression and
    encoding lead to smaller file sizes | Not as human-readable as JSONYou can’t update
    tables – you need to rewrite them | Analytical processing, data warehousing |'
  prefs: []
  type: TYPE_TB
- en: '| Avro | Compact binary serializationSchema-based and supports schema evolutionInteroperable
    across different systems | Slightly less human-readable compared to JSON | Bandwidth-efficient
    streaming and diverse language support |'
  prefs: []
  type: TYPE_TB
- en: '| Delta | ACID transactions for data consistencyEfficient storage format for
    data lakesSchema evolution and time-travel queries | Larger size than Parquet
    | Real-time analytics with long-term storage |'
  prefs: []
  type: TYPE_TB
- en: '| Hudi | Efficient incremental data processingACID transactions for real-time
    data | Larger size than Parquet | Streaming data applications and change data
    capture |'
  prefs: []
  type: TYPE_TB
- en: '| Iceberg | Schema evolution, ACID transactionsOptimized storage formats such
    as Parquet | Larger size than Parquet | Time-travel queries and evolving schemas
    |'
  prefs: []
  type: TYPE_TB
- en: '| **File Type** | **Pros** | **Cons** | **When** **to Choose** |'
  prefs: []
  type: TYPE_TB
- en: '| Binary format | Compact and efficient storageFast serialization and deserialization
    | Not human-readableLimited support for schema evolution | Efficiency is crucial
    in bandwidth usage and processing speed |'
  prefs: []
  type: TYPE_TB
- en: Table 7.7 – A summary table of all the file formats, as well as their pros,
    cons, and use cases
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’re going to discuss **partitioning**, an important concept
    in the context of data storage, especially in distributed storage systems. While
    the concept of partitioning itself is more closely associated with data lakes,
    data warehouses, and distributed filesystems, its relevance extends to the broader
    discussion of data sinks.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data partitioning** is a technique that’s used to divide and organize large
    datasets into smaller, more manageable subsets called partitions. When writing
    data to sinks, such as databases or distributed storage systems, employing appropriate
    data partitioning strategies is crucial for optimizing query performance, data
    retrieval, and storage efficiency. Partitioning in data storage systems, including
    time-based, geographic, and hybrid partitioning, offers several benefits in terms
    of read operations, updates, and writes:'
  prefs: []
  type: TYPE_NORMAL
- en: When querying the data, partitioning allows the system to skip irrelevant data
    quickly. For example, in **time-based partitioning**, if you’re interested in
    data for a specific date, the system can directly access the partition corresponding
    to that date, leading to faster query times. It ensures that only the necessary
    partitions are scanned, reducing the amount of data to process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning can simplify updates, especially when the updates are concentrated
    in specific partitions. For example, if you need to update data for a specific
    date or region, the system can isolate the affected partition, reducing the scope
    of the update operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning can enhance the efficiency of write operations, particularly when
    appending data. New data can be written to the appropriate partition without affecting
    the existing data, leading to a more straightforward and faster write process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning supports parallel processing. Different partitions can be read
    or written concurrently, enabling better utilization of resources and faster overall
    processing times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning provides a logical organization of data. It simplifies data management
    tasks such as archiving old data, deleting obsolete records, or migrating specific
    partitions to different storage tiers based on access patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With partitioning, you can optimize storage based on usage patterns. For example,
    frequently accessed partitions can be stored in high-performance storage, while
    less frequently accessed partitions can be stored in lower-cost storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning supports pruning, where the system can eliminate entire partitions
    from consideration during query execution. This pruning mechanism further accelerates
    query performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s have a closer look at the different partitioning strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal versus vertical partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When discussing partitioning strategies in the context of databases or distributed
    systems, we generally refer to two main types: **horizontal partitioning** and
    **vertical partitioning**. Each approach organizes data differently to improve
    performance, scalability, or manageability. Let’s start with horizontal partitioning.'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal partitioning, or sharding, involves dividing a table’s rows into
    multiple partitions, each containing a subset of the data. This approach is commonly
    used to scale out databases by distributing data across multiple servers, where
    each shard maintains the same schema but holds different rows. For example, a
    user table in a large application could be sharded by user IDs, with IDs 1 to
    10,000 in one partition and IDs 10,001 to 20,000 in another. This strategy enables
    the system to handle larger datasets than a single machine could manage, enhancing
    performance in large-scale applications.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical partitioning, on the other hand, involves splitting a table’s columns
    into different partitions, where each partition contains a subset of columns but
    includes all rows. This strategy is effective when different columns are accessed
    or updated at varying frequencies as it optimizes performance by minimizing the
    amount of data that’s processed during a query. For example, in a user profiles
    table, basic information such as name and email could be stored in one partition,
    while a large binary data column, such as a profile picture, is stored in another.
    This allows queries targeting specific columns to access a smaller, more efficient
    dataset, thereby enhancing performance.
  prefs: []
  type: TYPE_NORMAL
- en: Both strategies can be used in combination to meet the specific needs of a database
    system, depending on the data structure and access patterns. The reality is that
    in the data field, **horizontal partitioning** is more commonly seen and widely
    adopted than vertical partitioning. This is particularly true in large-scale,
    distributed databases and applications that need to handle vast amounts of data,
    high traffic, or geographically dispersed users. In the next section, we will
    see some examples of horizontal partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: Time-based partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time-based partitioning involves organizing data based on timestamps. Each partition
    represents a specific time interval, such as a day, hour, or minute. It allows
    for efficient retrieval of historical data and time-based aggregations. It also
    facilitates data retention and archiving policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, you’ll learn how to create time-based partitioning on your
    local laptop using Parquet files. You can find the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/5.time_based_partitioning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/5.time_based_partitioning.py).
    Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a sample dataset with two columns: `timestamp` and `value`. This dataset
    represents time series data with timestamps and corresponding values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the `timestamp` column into a `datetime` type. This ensures that the
    timestamps are treated as datetime objects for accurate time-based operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the path to store the data. Use an existing path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Iterate through the DataFrame, grouping rows by the `date` component of the
    `timestamp` column. Convert each group into a PyArrow table and write it to the
    corresponding partition path in Parquet format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the directory if it doesn’t exist:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After executing this script, you’ll see two Parquet files being created in
    your base directory – one for each day of the week:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Time-based partitioning output](img/B19801_07_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Time-based partitioning output
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at another common partitioning strategy, known as geographic
    partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: Geographic partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Geographic partitioning** involves dividing data based on geographical attributes
    such as regions, countries, or cities. This strategy is valuable when you’re dealing
    with geospatial data or location-based analytics. It enables fast and targeted
    retrieval of data related to specific geographic areas, thus supporting spatial
    queries and analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how you can create geographic-based partitioning in your
    local laptop using Parquet files. You can find the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/6.geo_partitioning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/6.geo_partitioning.py).
    Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a base directory for storing partitioned data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert each group (region-specific data) into a PyArrow table. Then, write
    the tables to the corresponding paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a directory for each region within the base directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the group into a PyArrow table and write it to the partition path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After executing this script, you will see three Parquet files being created
    in your base directory – one for each geographic location available in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Geographic-based partitioning output](img/B19801_07_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Geographic-based partitioning output
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at the last common partitioning strategy, known as hybrid
    partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Geographic partitioning is a specialized form of category partitioning that
    organizes data based on geographical attributes or spatial criteria. **Category
    partitioning** is a fundamental strategy in data organization that involves grouping
    data based on specific categories or attributes, such as customer demographics,
    product types, or transactional characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Hybrid partitioning** involves combining multiple partitioning strategies
    to optimize data organization for specific use cases. For instance, you might
    partition data first by time and then further partition each time interval by
    a key or geographic location. It offers flexibility for addressing complex querying
    patterns and diverse data access requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how you can create hybrid partitioning on your local laptop
    using Parquet files. You can find the full code here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/7.hybrid_partitioning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/7.hybrid_partitioning.py).
    Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a base directory for storing partitioned data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform hybrid partitioning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a directory for each timestamp and region combination within the base
    directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the group into a PyArrow table and write it to the partition path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After executing this script, you will see three Parquet files being created
    in your base directory – two locations for January 1, 2022, and one for January
    2, 2022:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Hybrid partitioning output](img/B19801_07_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Hybrid partitioning output
  prefs: []
  type: TYPE_NORMAL
- en: Remember
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve explored various types of partitioning, such as time-based and
    geographic. However, remember you can use any column that makes sense in your
    data, your use case, and the query patterns for the table as partitioning column(s).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve discussed different partitioning strategies, it’s time to talk
    about how to choose the column you will partition your data on.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for choosing partitioning strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Choosing the right partitioning strategy for your data involves considering
    various factors to optimize performance, query efficiency, and data management.
    Here are some key considerations for choosing partitioning strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query patterns**: Select partitioning strategies based on the types of queries
    your application or analytics platform will perform most frequently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data distribution**: Ensure partitions are distributed evenly to prevent
    data hotspots and resource contention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data size**: Consider the volume of data that will be stored in each partition.
    Smaller partitions can improve query performance, but too many small partitions
    might impact management overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query complexity**: Some queries might benefit from hybrid partitioning,
    especially if they involve multiple attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Partitioning should allow for future scalability and accommodate
    data growth over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data partitioning is a key architectural decision that can significantly impact
    the efficiency and performance of your data processing pipeline. By employing
    appropriate data partitioning strategies, you can ensure that your data is organized
    in a way that aligns with your querying patterns and maximizes the benefits of
    your chosen data sink technology.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’re going to put everything we’ve learned in this chapter
    into practice by describing a real-world case scenario and going through all the
    logical steps for defining the best strategy associated with data sinks and file
    types.
  prefs: []
  type: TYPE_NORMAL
- en: Designing an online retail data platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An online retailer wants to create an analytics platform to collect and analyze
    all the data generated by their e-commerce website. This platform aims to provide
    capabilities that allow for real-time data processing and analytics to improve
    customer experiences, optimize business operations, and drive strategic decision-making
    for the online retail business.
  prefs: []
  type: TYPE_NORMAL
- en: 'After long discussions with the team, we identified four main requirements
    to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Handle large volumes of transaction data**: The platform needs to efficiently
    ingest and transform large volumes of transaction data. This needs to be done
    by accounting for scalability, high throughput, and cost-effectiveness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provide real-time insights**: Business analysts require immediate access
    to real-time insights derived from transaction data. The platform should support
    real-time data processing and analytics to enable timely decision-making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s a need to combine batch and streaming data ingestion to handle both
    the real-time website data and the batch customer data, which is updated slowly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use AWS as the cloud provider**: The choice of the cloud provider (AWS) comes
    from the fact that the retailer is currently using other AWS services and wants
    to stick with the same provider.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s have a quick look at how we can solve these requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose the right data sink technology:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Thinking process**: A Lakehouse architecture is an ideal solution for the
    data platform requirements due to its ability to handle large volumes of data
    with scalability, high throughput, and cost-effectiveness. It leverages distributed
    storage and compute resources, allowing for efficient data ingestion and transformation.
    Additionally, the architecture supports real-time data processing and analytics,
    enabling business analysts to access immediate insights from transaction data
    for timely decision-making. By combining batch and streaming data ingestion, the
    Lakehouse seamlessly integrates real-time website data with batch-updated customer
    data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choice**: A Lakehouse solution on AWS is selected for its scalability, cost-effectiveness,
    and seamless integration with other AWS services. AWS is compatible with a Lakehouse
    architecture.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evaluate and choose the data file format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data characteristics**: The customer data consists of structured transaction
    records, including customer IDs, product IDs, purchase amounts, timestamps, and
    geolocation. The streaming data includes customer IDs and other web metrics, such
    as what each customer is currently browsing on the website.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choice**: Delta file format is selected for its transactional capabilities
    and ACID compliance. It also supports batch and streaming workloads.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implement data ingestion for batch and streaming data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data ingestion**: ETL processes are designed to transform incoming transaction
    data into Delta files. Real-time transaction data is streamed from AWS Kinesis
    for immediate processing and stored as Delta files while batch data coming from
    different other systems is integrated.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partitioning logic**: Batch and streaming data are being processed and stored
    in Delta files. The streaming data is *partitioned by date when written out*.
    Next, transformations and data consolidation happen before it’s stored as the
    final analytical tables.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Define a partitioning strategy for analytical tables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Query patterns**: Analysts often query data based on certain periods of time
    and some tables based on product categories.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choice**: As we learned in the *Considerations for choosing partitioning
    strategies* section, we need to take into account the way users are querying the
    table. To get the best read performance out of the queries, time-based and category-based
    partitioning must be implemented. Data is partitioned by date and further partitioned
    by product category in analytics tables that the users query often.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monitor and optimize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Performance monitoring**: Regularly monitor query performance, streaming
    throughput, and resource utilization using AWS monitoring and logging services'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**: Continuously optimize both batch and streaming components
    based on observed performance and changing data patterns'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schema evolution**: Ensure that the Delta schema accommodates streaming data
    changes and maintains compatibility with existing batch data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With this architecture, the online retail analytics platform gains the capability
    to process both batch and real-time data in an effective and cost-optimized way.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we focused on the components of designing and optimizing
    data write operations. We discussed how to choose the right data sink technology,
    how file formats significantly impact storage efficiency and query performance,
    and why it matters to choose the right one for your use case. Finally, we discussed
    why data partitioning is crucial for optimizing query performance and resource
    utilization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start transforming the data that’s been written
    on the data sink to better prepare it for downstream analytics by detecting and
    handling outliers and missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Downstream Data Cleaning – Consuming Structured Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part delves into the processes required for cleaning and preparing structured
    data for analysis, focusing on handling common data challenges that occur in more
    refined datasets. It provides practical techniques for managing missing values
    and outliers, ensuring data consistency through normalization and standardization,
    and effectively processing categorical features. Additionally, it introduces specialized
    methods for working with time series data, a common yet complex data type. By
    mastering these downstream cleaning and preparation techniques, readers will be
    well-equipped to turn structured data into actionable insights for advanced analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B19801_08.xhtml#_idTextAnchor195)*, Detecting and Handling Missing
    Values and Outliers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19801_09.xhtml#_idTextAnchor213)*, Normalization and Standardization*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19801_10.xhtml#_idTextAnchor223)*, Handling Categorical Features*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B19801_11.xhtml#_idTextAnchor246)*, Consuming Time Series Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
