<html><head></head><body>
  <div id="_idContainer558" class="Basic-Text-Frame">
    <h1 class="chapterNumber">12</h1>
    <h1 id="_idParaDest-268" class="chapterTitle">Building Blocks of Deep Learning for Time Series</h1>
    <p class="normal">While we laid the foundations of deep learning in the previous chapter, it was very general. Deep learning is a vast field with applications in all possible domains, but in this book, we will focus on the applications in time series forecasting.</p>
    <p class="normal">So in this chapter, let’s strengthen the foundation by looking at a few building blocks of deep learning that are commonly used in time series forecasting. Even though the global machine learning models perform well in time series problems, some deep learning approaches have also shown good promise. They are a good addition to your toolset due to the flexibility they allow when modeling.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Understanding the encoder-decoder paradigm</li>
      <li class="bulletList">Feed-forward networks</li>
      <li class="bulletList">Recurrent neural networks</li>
      <li class="bulletList">Long short-term memory networks</li>
      <li class="bulletList">Gated recurrent unit</li>
      <li class="bulletList">Convolution networks</li>
    </ul>
    <h1 id="_idParaDest-269" class="heading-1">Technical requirements</h1>
    <p class="normal">You will need to set up the <strong class="keyWord">Anaconda</strong> environment, following the instructions in the <em class="italic">Preface</em> of the book, to get a working environment with all the libraries and datasets required for the code in this book. Any additional libraries will be installed while running the notebooks.</p>
    <p class="normal">The associated code for this chapter can be found at <a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter12"><span class="url">https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter12</span></a>.</p>
    <h1 id="_idParaDest-270" class="heading-1">Understanding the encoder-decoder paradigm</h1>
    <p class="normal">In <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>, we saw that machine learning is all about learning <a id="_idIndexMarker940"/>a function that maps our inputs to the desired output:</p>
    <p class="center"><em class="italic">y</em> = <em class="italic">h</em>(<em class="italic">x</em>)</p>
    <p class="normal">where <em class="italic">x</em> is the input and <em class="italic">y</em> is our desired output.</p>
    <p class="normal">Adapting this to time series forecasting (using univariate time series forecasting to keep things simple), we can rewrite it as follows:</p>
    <p class="center"><em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub> = <em class="italic">h</em>(<em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-1</sub>, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-2</sub>, …, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t-N</sub>)</p>
    <p class="normal">Here, <em class="italic">t</em> is the current timestep and <em class="italic">N</em> is the total amount of history available at time <em class="italic">t</em>.</p>
    <p class="normal">Deep learning, like any other machine learning approach, is tasked with learning this function, which maps history to the future. In <em class="chapterRef">Chapter 11</em>, <em class="italic">Introduction to Deep Learning</em>, we saw how deep learning learns good features using representation learning, and then it uses the learned features to carry out the task at hand. This understanding can be further refined to the time series perspective by using the encoder-decoder paradigm.</p>
    <p class="normal">Like everything in research, it is not entirely clear when and who proposed this idea of the encoder-decoder architecture. In 1997, Ramon Neco and Mikel Forcada proposed an architecture for machine translation that had ideas reminiscent of the encoder-decoder paradigm. In 2013, Nal Kalchbrenner and Phil Blunsom proposed an encoder-decoder model for machine translation, although they did not call it that. But it was when Ilya Sutskever et al. (2014) and Cho et al. (2014) proposed two new models for machine translation that worked independently that this idea took off. Cho et al. called it the encoder-decoder architecture, while Sutskever et al. called it the Seq2Seq architecture. The key innovation it drove was the ability to model variable-length inputs and outputs in an end-to-end fashion.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research papers by Ramon Neco et al., Nal Kalchbrenner et al., Cho et al., and Ilya Sutskever et al. are cited in the <em class="italic">References</em> section as <em class="italic">1</em>, <em class="italic">2</em>, <em class="italic">3</em>, and <em class="italic">4</em>, respectively.</p>
    </div>
    <p class="normal">The idea is very<a id="_idIndexMarker941"/> straightforward, but before we get into that, we need to have a high-level understanding of latent spaces and feature/input spaces.</p>
    <p class="normal">The <strong class="keyWord">feature space</strong>, or the <strong class="keyWord">input space</strong>, is <a id="_idIndexMarker942"/>the vector space where your data resides. If the<a id="_idIndexMarker943"/> data has 10 dimensions, then the input space is the 10-dimensional vector space. Latent space is an abstract vector space that encodes a meaningful internal representation of the feature space. To understand this, we can think about how we, as humans, recognize a tiger. We do not remember every minute detail of a tiger; we just have a general idea of what a tiger looks like and its prominent features, such as its stripes. It is a compressed understanding of this concept that helps our brains process and recognize a tiger faster. </p>
    <p class="normal">In the machine learning domain, there are techniques <a id="_idIndexMarker944"/>like <strong class="keyWord">Principal Component Analysis</strong> (<strong class="keyWord">PCA</strong>) that do similar transformations to a latent space, preserving the essential features of the input data. With this intuition, rereading the definition may bring a bit more clarity to the concept.</p>
    <p class="normal">Now that we have an idea about latent spaces, let’s see what an encoder-decoder architecture does.</p>
    <p class="normal">An encoder-decoder architecture has two main parts—an encoder and a decoder:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Encoder</strong>: The<a id="_idIndexMarker945"/> encoder takes in the input vector, <em class="italic">x</em>, and encodes it into a latent space. This encoded representation is called the latent vector, <em class="italic">z</em>.</li>
      <li class="bulletList"><strong class="keyWord">Decoder</strong>: The<a id="_idIndexMarker946"/> decoder takes in the latent vector, <em class="italic">z</em>, and decodes it into the kind of output we need (<img src="../Images/B22389_05_001.png" alt=""/>).</li>
    </ul>
    <p class="normal">The following diagram shows the encoder-decoder setup visually:</p>
    <figure class="mediaobject"><img src="../Images/B22389_12_01.png" alt="Figure 12.1 – The encoder-decoder architecture "/></figure>
    <p class="packt_figref">Figure 12.1: The encoder-decoder architecture</p>
    <p class="normal">In the context of time series forecasting, the encoder consumes the history and retains the information that is required for the decoder to generate the forecast. As we learned previously, time<a id="_idIndexMarker947"/> series forecasting can be written as follows:</p>
    <p class="center"><em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub> = <em class="italic">h</em>(<em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-1</sub>, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-2</sub>, …, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t-N</sub>)</p>
    <p class="normal">Now, using the encoder-decoder paradigm, we can rewrite it as follows:</p>
    <p class="center"><em class="italic">z</em><sub class="subscript-italic" style="font-style: italic;">t</sub> = <em class="italic">h</em>(<em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-1</sub>, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-2</sub>, …, <em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t-N</sub>)</p>
    <p class="center"><em class="italic">y</em><sub class="subscript-italic" style="font-style: italic;">t</sub> = <em class="italic">g</em>(<em class="italic">z</em><sub class="subscript-italic" style="font-style: italic;">t</sub>)</p>
    <p class="normal">Here, <em class="italic">h</em> is the encoder and <em class="italic">g</em> is the decoder.</p>
    <p class="normal">Each encoder and decoder can be some special architecture suited for time series forecasting. Let’s look at a few common components that are used in the encoder-decoder paradigm.</p>
    <h1 id="_idParaDest-271" class="heading-1">Feed-forward networks</h1>
    <p class="normal"><strong class="keyWord">Feed-forward networks</strong> (<strong class="keyWord">FFNs</strong>) or <strong class="keyWord">fully connected networks</strong> are <a id="_idIndexMarker948"/>the most basic architecture a<a id="_idIndexMarker949"/> neural network can take. We discussed perceptrons in <em class="chapterRef">Chapter 11</em>, <em class="italic">Introduction to Deep Learning</em>. If we stack multiple perceptrons (both linear units and non-linear activations) and create a network of such units, we get what we call an FFN. The following diagram will help us understand this:</p>
    <figure class="mediaobject"><img src="../Images/B22389_12_02.png" alt="Figure 12.2 – Feed-forward network "/></figure>
    <p class="packt_figref">Figure 12.2: A Feed Forward Network (FFN)</p>
    <p class="normal">An FFN takes a <a id="_idIndexMarker950"/>fixed-size input vector and passes it through a series of computational layers leading up to the desired output. This architecture is called feed-forward because the information is fed forward through the network. This is also <a id="_idIndexMarker951"/>called a <strong class="keyWord">fully connected network</strong> because every unit in a layer is connected to every unit in the previous layer and every unit in the next layer.</p>
    <p class="normal">The first layer is called the input layer, and <a id="_idIndexMarker952"/>this is equal to the dimension of the input. The last layer is called the output layer, which<a id="_idIndexMarker953"/> is defined as per our desired output. If we need a single output, we will need 1 unit, while if we need 10 outputs, we will need 10 units. All the layers in <a id="_idIndexMarker954"/>between are called <strong class="keyWord">hidden layers</strong>. Two hyperparameters define the structure of the network—the number of hidden layers and the number of units in each layer. For instance, in <em class="italic">Figure 12.2</em>, we have a network with two hidden layers and eight units per layer.</p>
    <p class="normal">In the time series forecasting context, an FFN can be used as an encoder as well as a decoder. As an encoder, we can use an FFN just like we used machine learning models in <em class="chapterRef">Chapter 5</em>, <em class="italic">Time Series Forecasting as Regression</em>. We embed time and convert a time series problem into a regression problem before feeding it into the FFN. As a decoder, we use it on the<a id="_idIndexMarker955"/> latent vector (the output from the encoder) to get to the output (this is the most common usage of an FFN in time series forecasting).</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Additional reading</strong>:</p>
      <p class="normal">We are going to be using PyTorch throughout this book to work with deep learning. If you are not comfortable with PyTorch, don’t worry—I’ll try and explain the concepts when necessary. To get a head start, you can go through the <code class="inlineCode">01-PyTorch_Basics.ipynb</code> notebook in <code class="inlineCode">Chapter12</code>, where we have explored the basic functionalities of tensors and trained a very small neural network from scratch using PyTorch. I also suggest heading over to the <em class="italic">Further reading</em> section at the end of this chapter, where you’ll find a few resources to learn PyTorch.</p>
    </div>
    <p class="normal">Now, let’s put on our practical hats and see some of these in action. PyTorch is an open source deep learning framework developed primarily by the <strong class="keyWord">Facebook AI Research</strong> (<strong class="keyWord">FAIR</strong>) <strong class="keyWord">Lab</strong>. Although<a id="_idIndexMarker956"/> it is a library that can manipulate <strong class="keyWord">tensors</strong> (which are <em class="italic">n</em>-dimensional matrices) and accelerate such manipulations<a id="_idIndexMarker957"/> with a GPU, a large part of the use case for such a library is in building and training deep learning systems. Because of that, PyTorch provides a lot of ready-to-use components that we can use to build a deep learning system. Let’s see how we can use PyTorch for an FFN.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the <code class="inlineCode">02-Building_Blocks.ipynb</code> notebook in the <code class="inlineCode">Chapter12</code> folder and the code in the <code class="inlineCode">src</code> folder.</p>
    </div>
    <p class="normal">As we learned earlier in the section, an FFN is a network of linear and non-linear units arranged in a network. A linear operation consists of multiplying the input vector, <em class="italic">X</em>, with a weight matrix, <em class="italic">W</em>, and adding a bias term, <em class="italic">b</em>. This operation, <em class="italic">WX</em> + <em class="italic">b</em>, is encapsulated in a <code class="inlineCode">Linear</code> class in the <code class="inlineCode">nn</code> module of<a id="_idIndexMarker958"/> the <strong class="keyWord">PyTorch</strong> library. We can import this from the library using <code class="inlineCode">torch.nn import Linear</code>. But usually, we must import the whole<strong class="keyWord"> nn</strong> module because we would be using a lot of components from that module. For non-linearity, let’s use <strong class="keyWord">ReLU</strong> (as introduced in <em class="chapterRef">Chapter 11</em>, <em class="italic">Introduction to Deep Learning</em>), which is also a class in the <strong class="keyWord">nn</strong> module.</p>
    <p class="normal">Before moving on, let’s create a random walk time series whose length is <strong class="keyWord">20</strong>:</p>
    <pre class="programlisting code"><code class="hljs-code">N = <span class="hljs-number">20</span>
df = pd.DataFrame({
    <span class="hljs-string">"date"</span>: pd.date_range(periods=N, start=<span class="hljs-string">"2021-04-12"</span>, freq=<span class="hljs-string">"D"</span>),
    <span class="hljs-string">"ts"</span>: np.random.randn(N)
})
</code></pre>
    <p class="normal">We can use <a id="_idIndexMarker959"/>this tensor directly in the FFN, but usually, we use a sliding window technique to split the tensor and train the networks. We do this for multiple reasons:</p>
    <ul>
      <li class="bulletList">We can see this as a data augmentation technique that creates a greater number of samples as opposed to using the entire sequence just once.</li>
      <li class="bulletList">It helps us reduce and restrict computation by limiting the calculation to a fixed window.</li>
    </ul>
    <p class="normal">Let’s do that now:</p>
    <pre class="programlisting code"><code class="hljs-code">ts = torch.from_numpy(df.ts.values).<span class="hljs-built_in">float</span>()
window = <span class="hljs-number">15</span>
<span class="hljs-comment"># Creating windows of 15 over the dataset</span>
ts_dataset = ts.unfold(<span class="hljs-number">0</span>, size=window, step=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Now, we have a tensor, <code class="inlineCode">ts_dataset</code>, whose size is <em class="italic">6x15</em> (this can create 6 samples of 15 input features each when we move the sliding window across the length of the series). For a standard FFN, the input shape is specified as <em class="italic">batch size x input features</em>. So 6 becomes our batch size and 15 becomes the input feature size.</p>
    <p class="normal">Now, let’s define the layers in the FFN. For this exercise, let’s assume the network’s structure is as follows:</p>
    <figure class="mediaobject"><img src="../Images/B22389_12_03.png" alt="Figure 12.3 – FFNs – a matrix multiplication perspective "/></figure>
    <p class="packt_figref">Figure 12.3: FFNs—a matrix multiplication perspective</p>
    <p class="normal">The input <a id="_idIndexMarker960"/>data (6x15) will be passed through these layers one by one. Here, we can see how the tensor dimensions change as they flow through the network. Each of the linear layers is essentially a matrix multiplication that converts the input into the output of a specified dimension. After each linear transformation, we stack a non-linear activation function in there. These alternative linear and non-linear modules are what give the neural network the expressive power it has. The linear layers are an affine transformation of the vector space (rotation, translation, and so on), and the non-linearity <em class="italic">squashes</em> the vector space. Together, they can morph the input space so that it’s useful for the task at hand. Now, let’s see how we can code this in PyTorch. </p>
    <p class="normal">We are going to use a handy module from PyTorch called <strong class="keyWord">Sequential</strong>, which allows us to stack different sub-components together and use them with ease:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># The FFN we define would have this architecture</span>
<span class="hljs-comment"># window(windowed input) &gt;&gt; 64 (hidden layer 1) &gt;&gt; 32 (hidden layer 2) &gt;&gt; 32 (hidden layer 2) &gt;&gt; 1 (output)</span>
ffn = nn.Sequential(
    nn.Linear(in_features=window,out_features=<span class="hljs-number">64</span>), <span class="hljs-comment"># (batch-size x window) --&gt; (batch-size x 64)</span>
    nn.ReLU(),
    nn.Linear(in_features=<span class="hljs-number">64</span>,out_features=<span class="hljs-number">32</span>), <span class="hljs-comment"># (batch-size x 64) --&gt; (batch-size x 32)</span>
    nn.ReLU(),
    nn.Linear(in_features=<span class="hljs-number">32</span>,out_features=<span class="hljs-number">32</span>), <span class="hljs-comment"># (batch-size x 32) --&gt; (batch-size x 32)</span>
    nn.ReLU(),
    nn.Linear(in_features=<span class="hljs-number">32</span>,out_features=<span class="hljs-number">1</span>), <span class="hljs-comment"># (batch-size x 32) --&gt; (batch-size x 1)</span>
)
</code></pre>
    <p class="normal">Now that we have defined the FFN, let’s see how we can use it:</p>
    <pre class="programlisting code"><code class="hljs-code">ffn(ts_dataset)
<span class="hljs-comment"># or more explicitly</span>
ffn.forward(ts_dataset)
</code></pre>
    <p class="normal">This will return a<a id="_idIndexMarker961"/> tensor whose shape is based on <em class="italic">batch size x output units</em>. We can have any number of output units, not just one. Therefore, when using an encoder, we can have an arbitrary dimension for the latent vector. Then, when we use it as a decoder, we can have the output units equal the number of timesteps we forecast.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Sneak peak</strong>:</p>
      <p class="normal">We have not seen multi-step forecasting until now because it will be covered in more detail in <em class="chapterRef">Chapter 18</em>, <em class="italic">Multi-Step Forecasting</em>. But for now, just understand that there are cases where we will need to forecast multiple timesteps into the future. The classical statistical models do this out of the box. But for machine learning and deep learning, we need to design systems that can do that. Fortunately, there are a few different techniques to do so, which will be covered later in this chapter.</p>
    </div>
    <p class="normal">FFNs are designed for non-temporal data. We can use FFNs by embedding our data temporally and then passing that to the network. Also, the computational cost in an FFN is directly proportional to the memory we use in the embedding (the number of previous timesteps we include as features). We will also not be able to handle variable-length sequences in this setting.</p>
    <p class="normal">Now, let’s look at another common architecture that is specifically designed for temporal data.</p>
    <h1 id="_idParaDest-272" class="heading-1">Recurrent neural networks</h1>
    <p class="normal"><strong class="keyWord">Recurrent neural networks</strong> (<strong class="keyWord">RNNs</strong>) are a<a id="_idIndexMarker962"/> family of neural networks specifically designed to handle sequential data. They were first proposed by <em class="italic">Rumelhart et al</em>. (1986) in their seminal work, <em class="italic">Learning Representations by Back-Propagating Errors</em>. The work borrows ideas such as parameter sharing and recurrence from previous work in statistics and machine learning, resulting in a neural network architecture that helps overcome many of the disadvantages that FFNs have when processing sequential data.</p>
    <h2 id="_idParaDest-273" class="heading-2">RNN architecture</h2>
    <p class="normal"><strong class="keyWord">Parameter sharing</strong> is when<a id="_idIndexMarker963"/> we use the same set <a id="_idIndexMarker964"/>of parameters for different parts of a model. Apart from a regularization effect (restricting the model to using the same set of weights for multiple tasks, which regularizes the model by constraining the search space while optimizing the model), parameter sharing enables us to extend and apply the model to examples of different forms. RNNs can scale to much longer sequences because of this. In an FFN, each timestep (each feature) has a fixed weight, and even if the motif we are looking for shifts by just one timestep, the network may not capture it correctly. In an RNN enabled by parameter sharing, they are captured in a much better way.</p>
    <p class="normal">In a sentence (which is also a sequence), we may want a model to recognize that “<em class="italic">Tomorrow I will go to the bank</em>” and “<em class="italic">I will go to the bank tomorrow</em>” are the same thing. An FFN can’t do this, but an RNN will be able to because it uses the same parameters at all positions and will be able to identify the motif “<em class="italic">I will go to the bank</em>” wherever it occurs. Intuitively, we can think of RNNs as applying the same FFN at each time window but enhanced with some kind of memory to store relevant information for the task at hand.</p>
    <p class="normal">Let’s visualize how an RNN processes inputs:</p>
    <figure class="mediaobject"><img src="../Images/B22389_12_04.png" alt="Figure 12.4 – How an RNN processes input sequences "/></figure>
    <p class="packt_figref">Figure 12.4: How an RNN processes input sequences</p>
    <p class="normal">Let’s assume<a id="_idIndexMarker965"/> we are talking about a sequence with four elements in it, <em class="italic">x</em><sub class="subscript">1</sub> to <em class="italic">x</em><sub class="subscript">4</sub>. Any RNN block (let’s consider it as a black box for now) consumes input and a hidden state (memory), producing an output. In the beginning, there is no memory, so we start with an initial memory (<em class="italic">H</em><sub class="subscript">0</sub>), which is typically an array filled with zeroes. Now, the RNN block takes in the first input (<em class="italic">x</em><sub class="subscript">1</sub>) along with the initial hidden state (<em class="italic">H</em><sub class="subscript">0</sub>), producing an output (<em class="italic">o</em><sub class="subscript">1</sub>) and a hidden state (<em class="italic">H</em><sub class="subscript">1</sub>).</p>
    <p class="normal">To process the second element in the sequence, <em class="italic">the same RNN</em> block takes in the hidden state from the previous timestep (<em class="italic">H</em><sub class="subscript">1</sub>) and the input at the current timestep (<em class="italic">x</em><sub class="subscript">2</sub>), producing the output at the second timestep (<em class="italic">o</em><sub class="subscript">2</sub>) and a new hidden state (<em class="italic">H</em><sub class="subscript">2</sub>). This process continues until we reach the end of the sequence. After processing the entire sequence, we will have all the outputs at each timestep (<em class="italic">o</em><sub class="subscript">1</sub> through <em class="italic">o</em><sub class="subscript">4</sub>) and the final hidden state (<em class="italic">H</em><sub class="subscript">4</sub>).</p>
    <p class="normal">These outputs and the hidden state will have encoded the information contained in the sequence and can be used for further processing, such as to predict the next step using a decoder. The RNN block can also be used as a decoder that takes in the encoded representation and produces the outputs. Because of this flexibility, the RNN blocks can be arranged to suit a wide variety of input and output combinations, such as the following:</p>
    <ul>
      <li class="bulletList">Many-to-one, where we have many inputs and a single output—for instance, single-step forecasting or time series classification</li>
      <li class="bulletList">Many-to-many, where we have many inputs and many outputs—for instance, multi-step forecasting</li>
    </ul>
    <p class="normal">Now, let’s look at what happens inside an RNN.</p>
    <p class="normal">Let the input to<a id="_idIndexMarker966"/> the RNN at time <em class="italic">t</em> be <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub> and the hidden state from the previous timestep be <em class="italic">H</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-1</sub>. The updated equations are as follows:</p>
    <p class="center"><img src="../Images/B22389_12_002.png" alt=""/></p>
    <p class="center"><img src="../Images/B22389_12_003.png" alt=""/></p>
    <p class="center"><img src="../Images/B22389_12_004.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">U</em>, <em class="italic">V</em>, and <em class="italic">W</em> are learnable weight matrices, and <em class="italic">b</em><sub class="subscript">1</sub> and <em class="italic">b</em><sub class="subscript">2</sub> are two learnable bias vectors. <em class="italic">U</em>, <em class="italic">V</em>, and <em class="italic">W</em> can be easily remembered as <em class="italic">input-to-hidden</em>, <em class="italic">hidden-to-output</em>, and <em class="italic">hidden-to-hidden</em> matrices based on the kind of transformation they perform, respectively. Intuitively, we can think of the operation that the RNN does as a kind of learning and forgetting information as it sees fit. The <em class="italic">tanh</em> activation, as we saw in <em class="chapterRef">Chapter 11</em>, <em class="italic">Introduction to Deep Learning</em>, produces a value between -1 and 1, which acts analogous to forgetting and remembering. So the RNN transforms the input into a latent dimension, uses the <em class="italic">tanh</em> activation to decide what information from the current timestep and previous memory to keep and forget, and uses this new memory to generate an output.</p>
    <p class="normal">In standard backpropagation, we backpropagate gradients from one unit to another. But in recurrent nets, we have a special situation where we have to backpropagate the gradients within a single unit, but through time or the different timesteps. A special case of <a id="_idIndexMarker967"/>backpropagation, called <strong class="keyWord">Back Propagation Through Time</strong> (<strong class="keyWord">BPTT</strong>), has been developed for RNNs. </p>
    <p class="normal">Thankfully, all the major deep learning frameworks are capable of doing this without any problems. For a more detailed understanding and the mathematical foundations of BPTT, please refer to the <em class="italic">Further reading</em> section.</p>
    <p class="normal">PyTorch has made RNNs available as ready-to-use modules—all you need to do is import one of the modules from the library and start using it. But before we do that, we need to understand a few more concepts.</p>
    <p class="normal">The first concept we will look at is the possibility of <em class="italic">stacking multiple layers</em> of RNNs on top of each other so that the outputs at each timestep become the input to the RNN in the next layer. Each layer will have a hidden state or memory. This enables hierarchical feature learning, which is one of the bedrocks of successful deep learning today.</p>
    <p class="normal">Another <a id="_idIndexMarker968"/>concept is <em class="italic">bidirectional</em> RNNs, introduced by Schuster and Paliwal in 1997. Bidirectional RNNs are very similar to RNNs. In a<a id="_idIndexMarker969"/> vanilla RNN, we <a id="_idIndexMarker970"/>process the inputs sequentially from start to end (forward). However, a bidirectional RNN uses one set of input-to-hidden <a id="_idIndexMarker971"/>and hidden-to-hidden weights to process the inputs from start to end, and then it uses another set to process the inputs in reverse (end to start) and concatenate the hidden states from both directions. It is on this concatenated hidden state that we apply the output equation.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research papers by Rumelhart. et al and Schuster and Paliwal are cited in the <em class="italic">References</em> section as <em class="italic">5</em> and <em class="italic">6</em>, respectively.</p>
    </div>
    <h2 id="_idParaDest-274" class="heading-2">RNN in PyTorch</h2>
    <p class="normal">Now, let’s<a id="_idIndexMarker972"/> understand the PyTorch implementation of RNN. As with the <strong class="keyWord">Linear</strong> module, the <strong class="keyWord">RNN</strong> module is also available from <code class="inlineCode">torch.nn</code>. Let’s look at the different parameters that the implementation provides while initializing:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">input_size</code>: The number of expected features in the input. If we use just the history of the time series, this is 1. However, when we use history along with some other features, this is &gt;1.</li>
      <li class="bulletList"><code class="inlineCode">hidden_size</code>: The dimension of the hidden state. This defines the size of the input-to-hidden and hidden-to-hidden matrices.</li>
      <li class="bulletList"><code class="inlineCode">num_layers</code>: This is the number of RNNs that will be stacked on top of each other. The default is <strong class="keyWord">1</strong>.</li>
      <li class="bulletList"><code class="inlineCode">nonlinearity</code>: The non-linearity to use. Although tanh is the originally proposed non-linearity, PyTorch also allows us to use ReLU (<code class="inlineCode">relu</code>). The default is <code class="inlineCode">tanh</code>.</li>
      <li class="bulletList"><code class="inlineCode">bias</code>: This parameter decides whether or not to add bias to the update equations we discussed earlier. If the parameter is <strong class="keyWord">False</strong>, there will be no bias. The default is <strong class="keyWord">True</strong>.</li>
      <li class="bulletList"><code class="inlineCode">batch_first</code>: There are two input data configurations that the RNN cell can use—we can have the input as (<em class="italic">batch size, sequence length, number of features</em>) or (<em class="italic">sequence length, batch size, number of features</em>). <code class="inlineCode">batch_first = True</code> selects the former as the expected input dimensions. The default is <strong class="keyWord">False</strong>.</li>
      <li class="bulletList"><code class="inlineCode">dropout</code>: This parameter, if non-zero, uses a dropout layer on the outputs of each RNN layer except the last. Dropout is a popular regularization technique where randomly selected neurons are ignored during training (the <em class="italic">Further reading</em> section contains a link to the paper that proposed this). The dropout probability will be equal to <strong class="keyWord">dropout</strong>. The default is <strong class="keyWord">0</strong>.</li>
      <li class="bulletList"><code class="inlineCode">bidirectional</code>: This parameter enables a bidirectional RNN. If <strong class="keyWord">True</strong>, a bidirectional RNN is used. The default is <strong class="keyWord">False</strong>.</li>
    </ul>
    <p class="normal">To continue applying<a id="_idIndexMarker973"/> the model to the same synthetic data we generated earlier in this chapter, let’s initialize the RNN model, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">rnn = nn.RNN(
    input_size=<span class="hljs-number">1</span>,
    hidden_size=<span class="hljs-number">32</span>,
    num_layers=<span class="hljs-number">1</span>,
    batch_first=<span class="hljs-literal">True</span>,
    dropout=<span class="hljs-number">0</span>,
    bidirectional=<span class="hljs-literal">False</span>,
)
</code></pre>
    <p class="normal">Now, let’s look at the inputs and outputs that are expected from an RNN cell.</p>
    <p class="normal">As opposed to the <strong class="keyWord">Linear</strong> layer we saw earlier, the RNN cell takes in <em class="italic">two inputs</em>—the input sequence and the hidden state vector. The input sequence can be either (<em class="italic">batch size</em>, <em class="italic">sequence length</em>, <em class="italic">number of features</em>) or (<em class="italic">sequence length</em>, <em class="italic">batch size</em>, <em class="italic">number of features</em>), depending on whether we have set <code class="inlineCode">batch_first=True</code>. The hidden state is a tensor whose size is (<em class="italic">D*number of layers</em>, <em class="italic">batch size, hidden size</em>), where <em class="italic">D</em> = 1 for <code class="inlineCode">bidirectional=False</code> and <em class="italic">D</em> = 2 for <code class="inlineCode">bidirectional=True</code>. The hidden state is an optional input and will default to zero tensors if left blank.</p>
    <p class="normal">There are two outputs of the RNN cell: an output and a hidden state. The output can be either (<em class="italic">batch size</em>, <em class="italic">sequence length</em>, <em class="italic">D*hidden size</em>) or (<em class="italic">sequence length</em>, <em class="italic">batch size</em>, <em class="italic">D*hidden size</em>), depending on <code class="inlineCode">batch_first</code>. The hidden state has the dimension of (<em class="italic">D*number of layers</em>, <em class="italic">batch size</em>, <em class="italic">hidden size</em>). Here, <em class="italic">D</em> = 1 or 2 is based on the <strong class="keyWord">bidirectional</strong> parameter.</p>
    <p class="normal">So let’s run <a id="_idIndexMarker974"/>our sequence through an RNN and look at the inputs and outputs (for more detailed steps, refer to the accompanying notebook):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#input dim: torch.Size([6, 15, 1])</span>
<span class="hljs-comment"># batch size = 6, sequence length = 15 and number of features = 1, batch_first = True</span>
output, hidden_states = rnn(rnn_input)
<span class="hljs-comment"># output.shape -&gt; torch.Size([6, 15, 32])</span>
<span class="hljs-comment"># hidden_states.shape -&gt; torch.Size([1, 6, 32]))</span>
</code></pre>
    <div class="note">
      <p class="normal">Although we saw that the RNN cell contains the output as well as the hidden state, we also know that the output is just an affine transformation of the hidden state. Therefore, to provide flexibility to the users, PyTorch only implements the update equations regarding the hidden states in the module. There are cases where we have no use for the outputs at each timestep (such as in a many-to-one scenario) and we can save computation if we do not do the output update at each step. Therefore, <code class="inlineCode">output</code> from the PyTorch RNN is just the hidden states at each timestep, and <code class="inlineCode">hidden_states</code> is the latest hidden state.</p>
    </div>
    <p class="normal">We can verify this by checking whether the hidden-state tensor is equal to the last-output tensor:</p>
    <pre class="programlisting code"><code class="hljs-code">torch.equal(hidden_states[<span class="hljs-number">0</span>], output[:,-<span class="hljs-number">1</span>]) <span class="hljs-comment"># -&gt; True</span>
</code></pre>
    <p class="normal">To make this clearer, let’s look at it visually:</p>
    <figure class="mediaobject"><img src="../Images/B22389_12_05.png" alt="Figure 12.5 – PyTorch implementation of stacked RNNs "/></figure>
    <p class="packt_figref">Figure 12.5: PyTorch implementation of stacked RNNs</p>
    <p class="normal">The hidden<a id="_idIndexMarker975"/> states at each timestep are used as input for the subsequent layer of RNNs and the hidden states of the last layer of RNNs are collected as the output. But each layer has a hidden state (that’s not shared with the others), and the PyTorch RNN collects the last hidden state from each layer and gives us that as well. </p>
    <p class="normal">Now, it is up to us to decide how to use these outputs. For instance, in a one-step-ahead forecast, we can use the output hidden states and stack a few linear layers on top of it to get the next timestep prediction. Alternatively, we can use the hidden states to transfer memory into another RNN as a decoder and generate predictions for multiple timesteps. There are many more ways we can use this output and PyTorch gives us that flexibility.</p>
    <p class="normal">RNNs, while very effective in modeling sequences, have one big flaw. Because of BPTT, the number of units through which you need to backpropagate increases drastically with the length of the sequence to be used for training. When we have to backpropagate through such a long <a id="_idIndexMarker976"/>computational graph, we<a id="_idIndexMarker977"/> will encounter <strong class="keyWord">vanishing</strong> or <strong class="keyWord">exploding gradients</strong>. This is when the gradient, as it is backpropagated through the network, either shrinks to zero or explodes to a very high number. The former makes the network stop learning, while the latter makes the learning unstable.</p>
    <p class="normal">We can think <a id="_idIndexMarker978"/>of what’s happening as akin to what happens when we multiply a scalar number repeatedly by itself. If the number is less than one, with every subsequent multiplication, the number becomes smaller and smaller until it is practically zero. If the number is greater than one, then the number becomes larger and larger at an exponential scale. This was discovered, independently, by Hochreiter in his diploma thesis (1991) and Yoshua Bengio et al. in two papers published in 1993 and 1994, respectively. Over the years, many tweaks to the model and training process have been proposed to tackle this disadvantage. Nowadays, vanilla RNNs are hardly used in practice and have been replaced almost completely by their newer cousins.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The references for Hochreiter (1991) and Bengio et al. (1993, 1994) are cited in the <em class="italic">References</em> section as <em class="italic">7</em>, <em class="italic">8</em>, and <em class="italic">9</em>, respectively.</p>
    </div>
    <p class="normal">Now, let’s look at two key improvements that have been made to the RNN architecture that have shown good performance, gaining popularity in the machine learning community.</p>
    <h1 id="_idParaDest-275" class="heading-1">Long short-term memory (LSTM) networks</h1>
    <p class="normal">Hochreiter <a id="_idIndexMarker979"/>and Schmidhuber proposed a modification of the classical RNNs in 1997—LSTM networks. It aimed to resolve the vanishing and exploding gradients in vanilla RNNs. The design of the LSTM was inspired by the logic gates of a computer. It introduces a new component, called a <strong class="keyWord">memory cell</strong>, which<a id="_idIndexMarker980"/> serves as long-term memory and is used in addition to the hidden-state memory of classical RNNs. In an LSTM, multiple gates are tasked with reading, adding, and forgetting information from these memory cells. This memory cell acts as a <em class="italic">gradient highway</em>, allowing the gateways to pass relatively unhindered through a network. This is the key innovation that avoided vanishing gradients in RNNs.</p>
    <h2 id="_idParaDest-276" class="heading-2">LSTM architecture</h2>
    <p class="normal">Let’s imagine<a id="_idIndexMarker981"/> that the input to the LSTM at time <em class="italic">t</em> is <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, and the hidden state from the previous timestep is <em class="italic">H</em><sub class="subscript-italic" style="font-style: italic;">t</sub><sub class="subscript">-1</sub>. Now, there are three gates that process information. Each gate is nothing but two learnable weight matrices (one for the input and one for the hidden state from the last step) and a bias term that is multiplied/added to the input and hidden state, and finally, it is passed through a sigmoid activation. </p>
    <p class="normal">The output of these gates will be a real number between 0 and 1. Let’s look at each of these gates in detail:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Input gate</strong>: The function of this gate is to decide how much information to read from the current input and previous hidden state. The update equation for this is:</li>
    </ul>
    <p class="center"><img src="../Images/B22389_12_005.png" alt=""/></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Forget gate</strong>: The forget gate decides how much information to forget from long-term memory. The updated equation for this is:</li>
    </ul>
    <p class="center"><img src="../Images/B22389_12_006.png" alt=""/></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Output gate</strong>: The output gate decides how much of the current cell state should be used to create the current hidden state, which is the output of the cell. The update equation for this is:</li>
    </ul>
    <p class="center"><img src="../Images/B22389_12_007.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">W</em><sub class="subscript">xi</sub>, <em class="italic">W</em><sub class="subscript">xf</sub>, <em class="italic">W</em><sub class="subscript">xo</sub>, <em class="italic">W</em><sub class="subscript">hi</sub>, <em class="italic">W</em><sub class="subscript">hf</sub>, and <em class="italic">W</em><sub class="subscript">ho</sub> are learnable weight parameters, and <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">i</sub>, <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">f</sub>, and <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">o</sub> are learnable bias parameters.</p>
    <p class="normal">Now, we can introduce a new long-term memory (cell state), <em class="italic">C</em><sub class="subscript-italic" style="font-style: italic;">t</sub>. The three gates mentioned previously serve to update and forget from this memory. If the cell state from the previous timestep is <em class="italic">C</em><sub class="subscript-italic" style="font-style: italic;">t-1</sub>, then the LSTM cell calculates a candidate cell state, <img src="../Images/B22389_12_008.png" alt=""/>, using another gate, but this time with <code class="inlineCode">tanh</code> activation:</p>
    <p class="center"><img src="../Images/B22389_12_009.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">xc</sub>, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">xh</sub> are learnable weight parameters and <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">c</sub> is the learnable bias parameter.</p>
    <p class="normal">Now, let’s look at the key update equation, which updates the cell state or long-term memory of the cell:</p>
    <p class="center"><img src="../Images/B22389_12_010.png" alt=""/></p>
    <p class="normal">Here, <img src="../Images/B22389_12_011.png" alt=""/> is elementwise multiplication. We use the forget gate to decide how much information from the previous timestep to carry forward, and we use the input gate to decide how much of the current candidate <a id="_idIndexMarker982"/>cell state will be written into long-term memory.</p>
    <p class="normal">Last but not least, we use the newly created current cell state and the output gate to decide how much information to pass on to the predictor through the current hidden state:</p>
    <p class="center"><img src="../Images/B22389_12_012.png" alt=""/></p>
    <p class="normal">A visual representation of this process can be seen in <em class="italic">Figure 12.6</em>.</p>
    <figure class="mediaobject"><img src="../Images/B22389_12_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.6: A gating diagram of LSTM</p>
    <h2 id="_idParaDest-277" class="heading-2">LSTM in PyTorch</h2>
    <p class="normal">Now, let’s <a id="_idIndexMarker983"/>understand the PyTorch implementation of LSTM. It is very similar to the RNN implementation we saw earlier, but it has one key difference: the parameters to initialize the class are pretty much the same. The API for this can be found at <a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM"><span class="url">https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM</span></a>. The key difference here is how the hidden states are used. While the RNN has a single tensor as a hidden state, the LSTM expects a <strong class="keyWord">tuple</strong> of tensors of the same dimensions: (<strong class="keyWord">hidden state</strong>, <strong class="keyWord">cell state</strong>). </p>
    <p class="normal">LSTMs, just like RNNs, have stacked and bidirectional variants, and PyTorch handles them in the same way.</p>
    <p class="normal">Now, let’s initialize some LSTM modules and use the synthetic data we have been using to see them in action:</p>
    <pre class="programlisting code"><code class="hljs-code">lstm = nn.LSTM(
    input_size=<span class="hljs-number">1</span>,
    hidden_size=<span class="hljs-number">32</span>,
    num_layers=<span class="hljs-number">5</span>,
    batch_first=<span class="hljs-literal">True</span>,
    dropout=<span class="hljs-number">0</span>,
    <span class="hljs-comment"># bidirectional=True,</span>
)
output, (hidden_states, cell_states) = lstm(rnn_input)
output.shape <span class="hljs-comment"># -&gt; [6, 15, 32]</span>
hidden_states.shape <span class="hljs-comment"># -&gt; [5, 6, 32]</span>
cell_states.shape <span class="hljs-comment"># -&gt; [5, 6, 32]</span>
</code></pre>
    <p class="normal">Now, let’s look<a id="_idIndexMarker984"/> at another modification that’s been made to vanilla RNNs that has resolved the vanishing and exploding gradient problems.</p>
    <h1 id="_idParaDest-278" class="heading-1">Gated recurrent unit (GRU)</h1>
    <p class="normal">In 2014, Cho et al. proposed <a id="_idIndexMarker985"/>another variant of the RNN that has a much simpler structure than an LSTM, called a GRU. The intuition behind this is similar to when we use a bunch of gates to regulate the information that flows through the cell, but a GRU eliminates the long-term memory component and uses just the hidden state to propagate information. So instead of the memory cell becoming the <em class="italic">gradient highway</em>, the hidden state itself becomes the “gradient highway.” In keeping with the same notation convention we used in the previous section, let’s look at the updated equations for a GRU.</p>
    <h2 id="_idParaDest-279" class="heading-2">GRU architecture</h2>
    <p class="normal">While we had<a id="_idIndexMarker986"/> three gates in an LSTM, we only have two in a GRU:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Reset gate</strong>: This gate decides how much of the previous hidden state will be considered as the candidate’s hidden state of the current timestep. The equation for this is:</li>
    </ul>
    <p class="center"><img src="../Images/B22389_12_013.png" alt=""/></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Update gate</strong>: The update gate decides how much of the previous hidden state should be carried forward and how much of the current candidate’s hidden state will be written into the hidden state. The equation for this is:</li>
    </ul>
    <p class="center"><img src="../Images/B22389_12_014.png" alt=""/></p>
    <p class="normal">Here <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">xr</sub>, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">xu</sub>, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hr</sub>, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hu</sub> are learnable weight parameters, and <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">r</sub> and \<em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">u</sub> are learnable bias parameters.</p>
    <p class="normal">Now, we can calculate the candidate’s hidden state (<img src="../Images/B22389_12_015.png" alt=""/>), as follows:</p>
    <p class="center"><img src="../Images/B22389_12_016.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">xh</sub> and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hh</sub> are learnable weight parameters, and <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">h</sub> is the learnable bias parameter. Here, we use the reset gate to throttle the information flow from the previous hidden<a id="_idIndexMarker987"/> state to the current candidate’s hidden state.</p>
    <p class="normal">Finally, the current hidden state (the output that goes to a predictor) is computed using the following equation:</p>
    <p class="center"><img src="../Images/B22389_12_017.png" alt=""/></p>
    <p class="normal">We use the update gate to decide how much from the previous hidden state and how much from the current candidate will be passed to the next timestep or predictor.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research papers for LSTM and GRUs are cited in the <em class="italic">References</em> section as <em class="italic">10</em> and <em class="italic">11</em>, respectively.</p>
    </div>
    <p class="normal">A visual representation of this process can be found in <em class="italic">Figure 12.7</em>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_12_07.png" alt="Figure 12.6 – A gating diagram of LSTM versus GRU  "/></figure>
    <p class="packt_figref">Figure 12.7: A gating diagram of GRU</p>
    <h2 id="_idParaDest-280" class="heading-2">GRU in PyTorch</h2>
    <p class="normal">Now, let’s<a id="_idIndexMarker988"/> understand the PyTorch implementation of the GRU. The APIs, inputs, and outputs are the same as with an RNN. The API for this can be referenced here: <a href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU"><span class="url">https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU</span></a>. The key difference is the internal workings of the modules, where the GRU update equations are used instead of the standard RNN ones.</p>
    <p class="normal">Now, let’s initialize a GRU module and use the synthetic data we have been using to see it in action:</p>
    <pre class="programlisting code"><code class="hljs-code">Gru = nn.GRU(
    input_size=<span class="hljs-number">1</span>,
    hidden_size=<span class="hljs-number">32</span>,
    num_layers=<span class="hljs-number">5</span>,
    batch_first=<span class="hljs-literal">True</span>,
    dropout=<span class="hljs-number">0</span>,
    <span class="hljs-comment"># bidirectional=True,</span>
)
output, hidden_states = gru(rnn_input)
output.shape <span class="hljs-comment"># -&gt; [6, 15, 32]</span>
hidden_states.shape <span class="hljs-comment"># -&gt; [5, 6, 32]</span>
</code></pre>
    <p class="normal">Now, let’s look at another major component that can be used for sequential data.</p>
    <h1 id="_idParaDest-281" class="heading-1">Convolution networks</h1>
    <p class="normal"><strong class="keyWord">Convolution networks</strong>, also called <strong class="keyWord">convolutional neural networks</strong> (<strong class="keyWord">CNNs</strong>), are like neural networks for <a id="_idIndexMarker989"/>processing data in the form of a grid. This grid can be 2D (such as an image), 1D (such as a time series), 3D (such as data from LIDAR sensors), and so on. Although this book is about time series and, typically, 1D convolutions are used in time series forecasting, it’s easier to understand convolutions in the 2D context (an image and so on) and then move back to a single-dimensional grid for time series.</p>
    <p class="normal">The basic idea behind CNNs is inspired by how human vision works. In 1979, Fukushima proposed Neocognitron (Reference <em class="italic">12</em>). It was a one-of-a-kind architecture that was directly inspired by how human vision works. But CNNs came into existence as we know them today in 1989 when Yann Le Cun used backpropagation to learn such a network, proving it by getting state-of-the-art results in handwritten digit recognition (Reference <em class="italic">13</em>). In 2012, when AlexNet (a CNN architecture for image recognition) won the annual challenge of image recognition called ImageNet, that too<a id="_idIndexMarker990"/> by a large margin between it and competing non-deep learning approaches, the interest and research in CNNs peaked. People soon figured out that, apart from images, CNNs are effective with sequences, such as language and time series data.</p>
    <h2 id="_idParaDest-282" class="heading-2">Convolution</h2>
    <p class="normal">At the<a id="_idIndexMarker991"/> heart of CNNs is a mathematical operation <a id="_idIndexMarker992"/>called <strong class="keyWord">convolution</strong>. The mathematical interpretation of a convolution operation is beyond the scope of this book, but there are a couple of links in the <em class="italic">Further reading</em> section if you want to learn more. For our purposes, we’ll develop an intuitive understanding of the convolution operation. </p>
    <p class="normal">Since CNNs rose to popularity using image data, let’s start by discussing the image domain and then move on to the sequence domain.</p>
    <p class="normal">Any image (for simplicity, let’s assume it’s grayscale) can be considered as a grid of pixel values, each value denoting how bright a point is, with 1 being pure white and 0 being pure black. Before we start discussing convolution, let’s understand <a id="_idIndexMarker993"/>what a <strong class="keyWord">kernel</strong> is. For now, let’s think of a kernel as a 2D matrix with some values in it. Typically, the kernel’s size is smaller than the size of the image we are using. Since the kernel is smaller than the image, we can “fit” the kernel inside the image. Let’s start with the kernel aligned with the top-left edge. With the kernel at the current position, there is a set of values in the image that this kernel is superpositioned over. We can perform elementwise multiplication between this subset of the image and the kernel, and then we add up all the elements into a single scalar. Now, we can repeat this process by “sliding” the kernel into all positions in the image. For instance, the following shows a sample image input whose size is 4x4 and how a convolution operation is carried out on that, using a kernel whose size is 2x2:</p>
    <figure class="mediaobject"><img src="../Images/B22389_12_08.png" alt="Figure 12.7 – A convolution operation on 2D and 1D inputs "/></figure>
    <p class="packt_figref">Figure 12.8: A convolution operation on 2D and 1D inputs</p>
    <p class="normal">So if we <a id="_idIndexMarker994"/>place the 2x2 kernel at the top-left position and perform the element-wise multiplication and the summation, we get the top-left item in the 3x3 output. If we slide the kernel by one position to the right, we get the next element in the top row of the output, and so on. Similarly, if we slide the kernel by one position down, we get the second element in the first column in the output.</p>
    <p class="normal">While this is interesting, we want to understand convolutions from a time series perspective. To do so, let’s shift our paradigm to 1D convolutions—convolutions performed on 1D data such as a sequence. In the preceding diagram, we can also see an example of a 1D convolution where we take the 1D kernel and slide it across the sequence to get an output of 1x3.</p>
    <p class="normal">Although we have set the kernel weights so that they’re convenient to understand and compute, in practice, these weights are learned by the network from data. If we set the kernel size as <em class="italic">n</em> and all the kernel weights as 1/<em class="italic">n</em>, what would such a convolution give us? This is something we covered in <em class="chapterRef">Chapter 6</em>, <em class="italic">Feature Engineering for Time Series Forecasting</em>. Yes, they result in the rolling means with a window of <em class="italic">n</em>. Remember, we learned this as a feature engineering technique for machine learning models. So 1D convolutions can be<a id="_idIndexMarker995"/> thought of as a more powerful feature generator, where the features are learned from data. With different weights on the kernels, we will extract different features. It is this knowledge that we should hold onto while learning about CNNs for time series data.</p>
    <h2 id="_idParaDest-283" class="heading-2">Padding, stride, and dilations</h2>
    <p class="normal">Now that <a id="_idIndexMarker996"/>we understand what a convolution operation is, we need to understand a few more terms, such as <strong class="keyWord">padding</strong>, <strong class="keyWord">stride</strong>, and <strong class="keyWord">dilations</strong>.</p>
    <p class="normal">Before we start discussing<a id="_idIndexMarker997"/> these terms, let’s look at an equation that gives the output dimensions (<em class="italic">O</em>) of a convolutional layer, given the input dimensions (<em class="italic">L</em>), kernel size (<em class="italic">k</em>), padding size (<em class="italic">p</em><sub class="subscript-italic" style="font-style: italic;">l</sub> for left padding and <em class="italic">p</em><sub class="subscript-italic" style="font-style: italic;">r</sub> for right padding), stride (<em class="italic">s</em>), and dilation (<em class="italic">d</em>):</p>
    <p class="center"><img src="../Images/B22389_12_018.png" alt=""/></p>
    <p class="normal">The default values (padding, strides, and dilations are special cases of a convolution process) of these terms are <em class="italic">p</em><sub class="subscript-italic" style="font-style: italic;">r</sub>, <em class="italic">p</em><sub class="subscript-italic" style="font-style: italic;">l</sub> = 0, <em class="italic">s</em> = 1, <em class="italic">d</em> = 1. Don’t worry if you don’t understand the formula or the terms in it—just keep the default values in mind so that when we understand each term, we can negate the others.</p>
    <p class="normal">In <em class="italic">Figure 12.8</em>, we saw that the convolution operation always reduces the size of the input. So in the default case, the formula becomes <em class="italic">O</em> = <em class="italic">L</em> – (<em class="italic">k</em> - 1). This is because the earliest position we can place the kernel in the sequence is from <em class="italic">t</em> = 0 to <em class="italic">t</em> = <em class="italic">k</em>. Then, by convolving through the sequence, we get <em class="italic">L</em> – (<em class="italic">k</em> - 1) terms in the output. Padding is when we add some values to the beginning or the end of the sequence. The value we use for padding is dependent on the problem. Typically, we choose zero as a padding value. So padding a sequence essentially increases the size of the input. Therefore, in the preceding formula, we can think of <em class="italic">L</em> + <em class="italic">p</em><sub class="subscript-italic" style="font-style: italic;">l</sub> + <em class="italic">p</em><sub class="subscript-italic" style="font-style: italic;">r</sub> as the effective length of the sequence after padding.</p>
    <p class="normal">The next two terms (stride and dilation) are closely related to the <strong class="keyWord">receptive field</strong> of the convolutional layer. The receptive field of a convolutional layer is the region in the input space that influences the feature that’s generated by the convolutional layer. In other words, it is the size of the window of input over which we have performed the convolution operation. For a single convolutional layer (with default settings), this is pretty much the kernel size. For multi-layered CNNs, this calculation becomes a bit more complicated because of the hierarchical structure (the <em class="italic">Further reading</em> section contains a link to a paper by Arujo et al. who derived a formula to calculate the receptive field of a CNN). But generally, increasing the receptive field of a CNN is associated with an increase in the accuracy of the CNN. For computer vision, Araujo et al. noted the following:</p>
    <blockquote class="packt_quote">
      <p class="quote">”We observe a logarithmic relationship between classification accuracy and receptive field size, which suggests that large receptive fields are necessary for high-level recognition tasks, but with diminishing rewards.”</p>
    </blockquote>
    <p class="normal">In time series, this is important because if the receptive field of a CNN is smaller than the long-term dependency, such as the seasonality, that we want to capture, then the network will fail to do so. Making the CNN deeper by stacking more convolutional layers on top of the others is one way to increase the receptive field of a network. However, there are a few ways to increase the receptive field of a single convolutional layer. Strides and dilations are two such ways:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Stride</strong>: Earlier, when <a id="_idIndexMarker998"/>we talked<a id="_idIndexMarker999"/> about <em class="italic">sliding</em> the kernel over the sequence, we mentioned that we move the kernel by one position at a time. This is called the stride of the convolutional layer, and there is no necessity that the stride should be 1. If we set the stride to 2, the convolution operation would be performed by skipping a position in between, as shown in <em class="italic">Figure 12.9</em>. This can make each layer in the convolutional network look at a larger slice of history, thereby increasing the receptive field.</li>
      <li class="bulletList"><strong class="keyWord">Dilation</strong>: Another <a id="_idIndexMarker1000"/>way we can <a id="_idIndexMarker1001"/>tweak the basic convolutional layer is by dilating the input connections. In the standard convolutional layer with a kernel size of 3, we apply the kernel to three consecutive elements in the input with a dilation of 1. If we increase the dilation to 2, then the kernel will be dilated spatially and will be applied. Instead of being applied to three consecutive elements, an element in between will be skipped. <em class="italic">Figure 12.8</em> shows how this works. As we can see, this can also increase the receptive field of the network.</li>
    </ul>
    <p class="normal">Both these techniques are similar but different and are compatible with each other. The following diagram shows what happens when we apply strides and dilations together (although this doesn’t happen frequently):</p>
    <figure class="mediaobject"><img src="../Images/B22389_12_09.png" alt="Figure 12.8 – Strides and dilations in convolutions "/></figure>
    <figure class="mediaobject">Figure 12.9: Strides and dilations in convolutions</figure>
    <p class="normal">Now, what if we want to make the output dimensions the same as the input dimensions? By using some basic algebra and rearranging the previous formula, we get the following:</p>
    <p class="center"><em class="italic">P</em><sub class="subscript-italic" style="font-style: italic;">l</sub> + <em class="italic">p</em><sub class="subscript-italic" style="font-style: italic;">r</sub> = <em class="italic">d</em>(<em class="italic">k</em>-1) + <em class="italic">L</em>(<em class="italic">s</em>-1) – (<em class="italic">s</em>-1)</p>
    <p class="normal">And in time series, we typically pad on the left rather than the right because of the strong autocorrelation that is typically present. Padding the latest few entries with zeros or some other values will make the learning of the prediction function very hard, as the latest hidden states are directly influenced by the padded values. The <em class="italic">Further reading</em> section contains a <a id="_idIndexMarker1002"/>link to an article by Kilian Batzner <a id="_idIndexMarker1003"/>about autoregressive convolutions. It is a must-read if you wish to really understand the concepts we have discussed here and also understand a few limitations. The <em class="italic">Further reading</em> section also contains a link to a GitHub repository that contains animations of convolutions for 2D inputs, which will give you a good intuition of what is happening.</p>
    <p class="normal">There is just one more term that you may hear often about convolutions, especially in time series—<strong class="keyWord">causal convolutions</strong>. All <a id="_idIndexMarker1004"/>you have to keep in mind is that causal convolutions are not special types of convolutions. So long as we ensure that we won’t use future timesteps to predict the current timestep while training, we are performing causal operations. This is typically done by offsetting the target and padding the inputs.</p>
    <h2 id="_idParaDest-284" class="heading-2">Convolution in PyTorch</h2>
    <p class="normal">Now, let’s<a id="_idIndexMarker1005"/> understand the PyTorch implementation of the CNN (a one-dimensional CNN, which is typically used for sequences such as time series). Let’s look at the different parameters the implementation provides while initializing. We have just discussed the following terms, so they should be familiar to you by now:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">in_channels</code>: The number of expected features in the input. If we are using just the history of the time series, then this would be 1. But when we use history along with some other features, then this will be &gt;1. For subsequent layers, the <code class="inlineCode">out_channels</code> you used in the previous layer become your <code class="inlineCode">in_channels</code> in the current layer.</li>
      <li class="bulletList"><code class="inlineCode">out_channels</code>: The number of kernels or filters applied to the input. Each kernel/filter produces a convolution operation with its own weights.</li>
      <li class="bulletList"><code class="inlineCode">kernel_size</code>: This is the size of the kernel we use for convolving.</li>
      <li class="bulletList"><code class="inlineCode">stride</code>: The stride of the convolution. The default is <code class="inlineCode">1</code>.</li>
      <li class="bulletList"><code class="inlineCode">padding</code>: This is the padding that is added to <em class="italic">both</em> sides. If we set the value as <code class="inlineCode">2</code>, the sequence that we pass to the layer will have a padded position on both the left and right. We can also give <code class="inlineCode">valid</code> or <code class="inlineCode">same</code> as input. These are easy ways of mentioning the kind of padding we need to add. <code class="inlineCode">padding='valid'</code> is the same as no padding. <code class="inlineCode">padding='same'</code> pads the input so that the output has the shape as the input. However, this mode doesn’t support any stride values other than <code class="inlineCode">1</code>. The default is <code class="inlineCode">0</code>.</li>
      <li class="bulletList"><code class="inlineCode">padding_mode</code>: This defines how the padded positions should be filled with values. The most common and default option is <em class="italic">zeros</em>, where all the padded tokens are filled with zeros. Another useful mode that is relevant for time series is <strong class="keyWord">replicate</strong>, which behaves like forward and backward fill in pandas. The other two options—<strong class="keyWord">reflect</strong> and <strong class="keyWord">circular</strong>—are more esoteric and are only used for <a id="_idIndexMarker1006"/>specific use cases. The default is <strong class="keyWord">zeros</strong>.</li>
      <li class="bulletList"><code class="inlineCode">dilation</code>: The dilation of the convolution. The default is <code class="inlineCode">1</code>.</li>
      <li class="bulletList"><code class="inlineCode">groups</code>: This parameter lets you control the way input channels are connected to output channels. The number specified in <code class="inlineCode">groups</code> specifies how many groups will be formed so that the convolutions happen within a group and not across. For instance, <code class="inlineCode">group=2</code> means that half the input channels will be convolved by one set of kernels and that the other half will be convolved by a separate set of kernels. This is equivalent to running two convolution layers side by side. Check the documentation for more information on this parameter. Again, this is for an esoteric use case. The default is <code class="inlineCode">1</code>.</li>
      <li class="bulletList"><code class="inlineCode">bias</code>: This parameter adds a learnable bias to the convolutions. The default is <code class="inlineCode">True</code>.</li>
    </ul>
    <p class="normal">Let’s apply a CNN model to the same synthetic data we generated earlier in this chapter with a kernel size of 3:</p>
    <pre class="programlisting code"><code class="hljs-code">conv = nn.Conv1d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">1</span>, kernel_size=k)
</code></pre>
    <p class="normal">Now, let’s look at the inputs and outputs that are expected from a CNN.</p>
    <p class="normal"><code class="inlineCode">Conv1d</code> expects the inputs to have three dimensions<code class="inlineCode">—(batch size, number of channels, sequence length)</code>. For the initial input layer, the number of channels is the number of features you feed into the network; for intermediate layers, it is the number of kernels we used in the previous layer. The output from <code class="inlineCode">Conv1d</code> is in the form of <code class="inlineCode">(batch size, number of channels (output), sequence length (output))</code>.</p>
    <p class="normal">So let’s run our sequence through <code class="inlineCode">Conv1d</code> and look at the inputs and outputs (for more detailed steps, refer to the <code class="inlineCode">02-Building_Blocks.ipynb</code> notebook):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#input dim: torch.Size([6, 1, 15])</span>
<span class="hljs-comment"># batch size = 6, number of features = 1 and sequence length = 15</span>
output = conv(cnn_input)
<span class="hljs-comment"># Output should be in_dim - k + 1</span>
<span class="hljs-keyword">assert</span> output.size(-<span class="hljs-number">1</span>)==cnn_input.size(-<span class="hljs-number">1</span>)-k+<span class="hljs-number">1</span>
output.shape <span class="hljs-comment">#-&gt; torch.Size([6, 1, 13])</span>
</code></pre>
    <p class="normal">The notebook <a id="_idIndexMarker1007"/>provides a slightly more detailed analysis of <code class="inlineCode">Conv1d</code>, with tables showing the impact that the hyperparameters have on the shape of the output, what kind of padding is used to make the input and output dimensions the same, and how a convolution with equal weights is just like a rolling mean. I highly suggest that you check it out and play around with the different options to get a feel of what the layer does for you.</p>
    <div class="note">
      <p class="normal">The inbuilt padding in <code class="inlineCode">Conv1d</code> has its roots in image processing, so the padding technique defaults to adding to both sides. However, for sequences, it is preferable to use padding on the left, and because of that, it is also preferable to handle how the input sequences are padded separately and not use the inbuilt mechanism. <code class="inlineCode">torch.nn.functional</code> has a handy method called <code class="inlineCode">pad</code> that can be used to this effect.</p>
    </div>
    <p class="normal">Other building blocks are used in time series forecasting because the architecture of a deep neural network is only limited by creativity. But the point of this chapter was to introduce you to the common ones that appear in many different architectures. We also intentionally left out one of the most popular architectures used nowadays: the transformer. This is because we have devoted another chapter (<em class="chapterRef">Chapter 14</em>, <em class="italic">Attention and Transformers for Time Series</em>) to understanding attention before we look at transformers. Another major block that is slowly gaining popularity is graph neural networks, which can be thought of as specialized CNNs that operate on graph-based data rather than grids. However, these are outside the scope of this book, since they are an area of active research.</p>
    <h1 id="_idParaDest-285" class="heading-1">Summary</h1>
    <p class="normal">After introducing deep learning in the previous chapter, in this chapter, we gained a deeper understanding of the common architectural blocks that are used for time series forecasting. The encoder-decoder paradigm was explained as a fundamental way to structure a deep neural network for forecasting. Then, we learned about FFNs, RNNs (LSTMs and GRUs), and CNNs, exploring how they are used to process time series. We also saw how we could use all these major blocks in PyTorch by using the associated notebook, and then we got our hands dirty with some PyTorch code.</p>
    <p class="normal">In the next chapter, we’ll learn about a few major patterns we can use to arrange these blocks to perform time series forecasting.</p>
    <h1 id="_idParaDest-286" class="heading-1">References</h1>
    <p class="normal">The following references were used in this chapter:</p>
    <ol>
      <li class="numberedList" value="1">Neco, R. P. and Forcada, M. L. (1997), <em class="italic">Asynchronous translations with recurrent neural nets</em>. Neural Networks, 1997., International Conference on (Vol. 4, pp. 2535–2540). IEEE: <a href="https://ieeexplore.ieee.org/document/614693"><span class="url">https://ieeexplore.ieee.org/document/614693</span></a>.</li>
      <li class="numberedList">Kalchbrenner, N. and Blunsom, P. (2013), <em class="italic">Recurrent Continuous Translation Models</em>. EMNLP (Vol. 3, No. 39, p. 413): <a href="https://aclanthology.org/D13-1176/"><span class="url">https://aclanthology.org/D13-1176/</span></a>.</li>
      <li class="numberedList">Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. (2014), <em class="italic">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</em>. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar. Association for Computational Linguistics: <a href="https://aclanthology.org/D14-1179/"><span class="url">https://aclanthology.org/D14-1179/</span></a>.</li>
      <li class="numberedList">Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. (2014), <em class="italic">Sequence to sequence learning with neural networks.</em> Proceedings of the 27th International Conference on Neural Information Processing Systems – Volume 2: <a href="https://dl.acm.org/doi/10.5555/2969033.2969173"><span class="url">https://dl.acm.org/doi/10.5555/2969033.2969173</span></a>.</li>
      <li class="numberedList">Rumelhart, D., Hinton, G., and Williams, R (1986). <em class="italic">Learning representations by back-propagating errors</em>. Nature 323, 533–536: <a href="https://doi.org/10.1038/323533a0"><span class="url">https://doi.org/10.1038/323533a0</span></a>.</li>
      <li class="numberedList">Schuster, M. and Paliwal, K. K. (1997). <em class="italic">Bidirectional recurrent neural networks</em>. IEEE Transactions on Signal Processing, 45(11), 2673–2681: <a href="https://doi.org/10.1109/78.650093"><span class="url">https://doi.org/10.1109/78.650093</span></a>.</li>
      <li class="numberedList">Sepp Hochreiter (1991) <em class="italic">Untersuchungen zu dynamischen neuronalen Netzen</em>. Diploma thesis, TU Munich: <a href="https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf"><span class="url">https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf</span></a>.</li>
      <li class="numberedList">Y. Bengio, P. Frasconi, and P. Simard (1993), <em class="italic">The problem of learning long-term dependencies in recurrent networks</em>. IEEE International Conference on Neural Networks, pp. 1183-1188 vol.3: 10.1109/ICNN.1993.298725.</li>
      <li class="numberedList">Y. Bengio, P. Simard, and P. Frasconi (1994) <em class="italic">Learning long-term dependencies with gradient descent is difficult</em> in IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157–166, March 1994: 10.1109/72.279181.</li>
      <li class="numberedList">Hochreiter, S. and Schmidhuber, J. (1997). <em class="italic">Long Short-Term Memory</em>. Neural Computation, 9(8), 1735–1780: <a href="https://doi.org/10.1162/neco.1997.9.8.1735"><span class="url">https://doi.org/10.1162/neco.1997.9.8.1735</span></a>.</li>
      <li class="numberedList">Cho, K., Merrienboer, B.V., Gülçehre, Ç., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014). <em class="italic">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.</em> EMNLP: <a href="https://www.aclweb.org/anthology/D14-1179.pdf"><span class="url">https://www.aclweb.org/anthology/D14-1179.pdf</span></a>.</li>
      <li class="numberedList">Fukushima, K. <em class="italic">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</em>. Biol. Cybernetics 36, 193–202 (1980): <a href="https://doi.org/10.1007/BF00344251"><span class="url">https://doi.org/10.1007/BF00344251</span></a>.</li>
      <li class="numberedList">Y. Le Cun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D. Jackel, and D. Henderson. 1990. <em class="italic">Handwritten Digit Recognition with a Back-Propagation Network</em>. Advances in neural information processing systems 2. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 396–404: <a href="https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf</span></a>.</li>
    </ol>
    <h1 id="_idParaDest-287" class="heading-1">Further reading</h1>
    <p class="normal">Take a look at the following resources to learn more about the topics that were covered in this chapter:</p>
    <ul>
      <li class="bulletList">Official PyTorch tutorials: <a href="https://pytorch.org/tutorials/beginner/basics/intro.html"><span class="url">https://pytorch.org/tutorials/beginner/basics/intro.html</span></a></li>
      <li class="bulletList"><em class="italic">Essence of linear algebra</em>, by 3Blue1Brown: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"><span class="url">https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab</span></a></li>
      <li class="bulletList"><em class="italic">Neural Networks – A Linear Algebra Perspective</em>, by Manu Joseph: <a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/"><span class="url">https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/</span></a></li>
      <li class="bulletList"><em class="italic">Deep Learning</em>, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville: <a href="https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/"><span class="url">https://deep-and-shallow.com/2022/01/15/neural-networks-a-linear-algebra-perspective/</span></a></li>
      <li class="bulletList"><em class="italic">Understanding LSTMs</em>, by Christopher Olah: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"><span class="url">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</span></a></li>
      <li class="bulletList"><em class="italic">Intuitive Guide to Convolution</em>: <a href="https://betterexplained.com/articles/intuitive-convolution/"><span class="url">https://betterexplained.com/articles/intuitive-convolution/</span></a></li>
      <li class="bulletList"><em class="italic">Computing Receptive Fields of Convolutional Neural Networks</em>, by Andre Araujo, Wade Norris, and Jack Sim: <a href="https://distill.pub/2019/computing-receptive-fields/"><span class="url">https://distill.pub/2019/computing-receptive-fields/</span></a></li>
      <li class="bulletList"><em class="italic">Convolutions in Autoregressive Neural Networks</em>, by Kilian Batzner: <a href="https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/"><span class="url">https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/</span></a></li>
      <li class="bulletList"><em class="italic">Convolution Arithmetic</em>, by Vincent Dumoulin and Francesco Visin: <a href="https://github.com/vdumoulin/conv_arithmetic"><span class="url">https://github.com/vdumoulin/conv_arithmetic</span></a></li>
      <li class="bulletList"><em class="italic">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</em>, by Nitish Srivastava et al.: <a href="https://jmlr.org/papers/v15/srivastava14a.html"><span class="url">https://jmlr.org/papers/v15/srivastava14a.html</span></a></li>
    </ul>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
  </div>
</body></html>