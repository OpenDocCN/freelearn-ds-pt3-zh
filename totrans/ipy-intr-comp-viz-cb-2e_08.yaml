- en: Chapter 8. Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting who will survive on the Titanic with logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to recognize handwritten digits with a K-nearest neighbors classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning from text – Naive Bayes for Natural Language Processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using support vector machines for classification tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a random forest to select important features for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the dimensionality of a dataset with a Principal Component Analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting hidden structures in a dataset with clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we were interested in getting insight into data, understanding
    complex phenomena through partial observations, and making informed decisions
    in the presence of uncertainty. Here, we are still interested in analyzing and
    processing data using statistical tools. However, the goal is not necessarily
    to *understand* the data, but to *learn* from it.
  prefs: []
  type: TYPE_NORMAL
- en: Learning from data is close to what we do as humans. From our experience, we
    intuitively learn general facts and relations about the world, even if we don't
    fully understand their complexity. The increasing computational power of computers
    makes them able to learn from data too. That's the heart of **machine learning**,
    a modern and fascinating branch of artificial intelligence, computer science,
    statistics, and applied mathematics. For more information on machine learning,
    refer to [http://en.wikipedia.org/wiki/Machine_learning](http://en.wikipedia.org/wiki/Machine_learning).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is a hands-on introduction to some of the most basic methods in
    machine learning. These methods are routinely used by data scientists. We will
    use these methods with **scikit-learn**, a popular and user-friendly Python package
    for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: A bit of vocabulary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this introduction, we will explain the fundamental definitions and concepts
    of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Learning from data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In machine learning, most data can be represented as a table of numerical values.
    Every row is called an **observation**, a **sample**, or a **data point**. Every
    column is called a **feature** or a **variable**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's call *N* the number of rows (or the number of points) and *D* the number
    of columns (or number of features). The number *D* is also called the **dimensionality**
    of the data. The reason is that we can view this table as a set *E* of **vectors**
    in a space with *D* dimensions (or **vector space**). Here, a vector **x** contains
    *D* numbers *(x[1], ..., x[D])*, also called **components**. This mathematical
    point of view is very useful and we will use it throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We generally make the distinction between supervised learning and unsupervised
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning** is when we have a **label** *y* associated with a data
    point *x*. The goal is to *learn* the mapping from *x* to *y* from our data. The
    data gives us this mapping for a finite set of points, but what we want is to
    *generalize* this mapping to the full set *E*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning** is when we don''t have any labels. What we want to
    do is discover some form of hidden structure in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Mathematically, supervised learning consists of finding a function *f* that
    maps the set of points *E* to a set of labels *F*, knowing a finite set of associations
    *(x, y)*, which is given by our data. This is what *generalization* is about:
    after observing the pairs *(x[i], y[i])*, given a new *x*, we are able to find
    the corresponding *y* by applying the function *f* to *x*. For more information
    on supervised learning, refer to [http://en.wikipedia.org/wiki/Supervised_learning](http://en.wikipedia.org/wiki/Supervised_learning).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a common practice to split the set of data points into two subsets: the
    **training set** and the **test set**. We learn the function *f* on the training
    set and test it on the test set. This is essential when assessing the predictive
    power of a model. By training and testing a model on the same set, our model might
    not be able to generalize well. This is the fundamental concept of **overfitting**,
    which we will detail later in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: We generally make the distinction between classification and regression, two
    particular instances of supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification** is when our labels *y* can only take a finite set of values
    (categories). Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Handwritten digit recognition**: *x* is an image with a handwritten digit;
    *y* is a digit between 0 and 9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spam filtering**: *x* is an e-mail and *y* is 1 or 0, depending on whether
    that e-mail is spam or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression** is when our labels *y* can take any real (continuous) value.
    Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting stock market data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting sales
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting the age of a person from a picture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A classification task yields a division of our space *E* in different regions
    (also called **partition**), each region being associated to one particular value
    of the label *y*. A regression task yields a mathematical model that associates
    a real number to any point *x* in the space *E*. This difference is illustrated
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Supervised learning](img/4818OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Difference between classification and regression
  prefs: []
  type: TYPE_NORMAL
- en: Classification and regression can be combined. For example, in the **probit
    model**, although the dependent variable is binary (classification), the *probability*
    that this variable belongs to one category can also be modeled (regression). We
    will see an example in the recipe about logistic regression. For more information
    on the probit model, refer to [http://en.wikipedia.org/wiki/Probit_model](http://en.wikipedia.org/wiki/Probit_model).
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Broadly speaking, unsupervised learning helps us discover systemic structures
    in our data. This is harder to grasp than supervised learning, in that there is
    generally no precise question and answer. For more information on unsupervised
    learning, refer to [http://en.wikipedia.org/wiki/Unsupervised_learning](http://en.wikipedia.org/wiki/Unsupervised_learning).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few important terms related to unsupervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering**: Grouping similar points together within **clusters**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Density estimation**: Estimating a probability density function that can
    explain the distribution of the data points'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimension reduction**: Getting a simple representation of high-dimensional
    data points by projecting them onto a lower-dimensional space (notably for **data
    visualization**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manifold learning**: Finding a low-dimensional manifold containing the data
    points (also known as **nonlinear dimension reduction**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection and feature extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a supervised learning context, when our data contains many features, it is
    sometimes necessary to choose a subset of them. The features we want to keep are
    those that are most relevant to our question. This is the problem of **feature
    selection**.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we might want to extract new features by applying complex transformations
    on our original dataset. This is **feature extraction**. For example, in computer
    vision, training a classifier directly on the pixels is not the most efficient
    method in general. We might want to extract the relevant points of interest or
    make appropriate mathematical transformations. These steps depend on our dataset
    and on the questions we want to answer.
  prefs: []
  type: TYPE_NORMAL
- en: For example, it is often necessary to preprocess the data before learning models.
    **Feature scaling** (or **data normalization**) is a common **preprocessing**
    step where features are linearly rescaled to fit in the range *[-1,1]* or *[0,1]*.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction and feature selection involve a balanced combination of domain
    expertise, intuition, and mathematical methods. These early steps are crucial,
    and they might be even more important than the learning steps themselves. The
    reason is that the few dimensions that are relevant to our problem are generally
    hidden in the high dimensionality of our dataset. We need to uncover the low-dimensional
    structure of interest to improve the efficiency of the learning models.
  prefs: []
  type: TYPE_NORMAL
- en: We will see a few feature selection and feature extraction methods in this chapter.
    Methods that are specific to signals, images, or sounds will be covered in [Chapter
    10](ch10.html "Chapter 10. Signal Processing"), *Signal Processing*, and [Chapter
    11](ch11.html "Chapter 11. Image and Audio Processing"), *Image and Audio Processing*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few further references:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection in scikit-learn, documented at [http://scikit-learn.org/stable/modules/feature_selection.html](http://scikit-learn.org/stable/modules/feature_selection.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection on Wikipedia at [http://en.wikipedia.org/wiki/Feature_selection](http://en.wikipedia.org/wiki/Feature_selection)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting, underfitting, and the bias-variance tradeoff
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A central notion in machine learning is the trade-off between **overfitting**
    and **underfitting**. A model may be able to represent our data accurately. However,
    if it is *too* accurate, it might not generalize well to unobserved data. For
    example, in facial recognition, a too-accurate model would be unable to identify
    someone who styled their hair differently that day. The reason is that our model
    might learn irrelevant features in the training data. On the contrary, an insufficiently
    trained model would not generalize well either. For example, it would be unable
    to correctly recognize twins. For more information on overfitting, refer to [http://en.wikipedia.org/wiki/Overfitting](http://en.wikipedia.org/wiki/Overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: A popular solution to reduce overfitting consists of adding *structure* to the
    model, for example, with **regularization**. This method favors simpler models
    during training (Occam's razor). You will find more information at [http://en.wikipedia.org/wiki/Regularization_%28mathematics%29](http://en.wikipedia.org/wiki/Regularization_%28mathematics%29).
  prefs: []
  type: TYPE_NORMAL
- en: The **bias-variance dilemma** is closely related to the issue of overfitting
    and underfitting. The **bias** of a model quantifies how precise it is across
    training sets. The **variance** quantifies how sensitive the model is to small
    changes in the training set. A **robust** model is not overly sensitive to small
    changes. The dilemma involves minimizing both bias and variance; we want a precise
    and robust model. Simpler models tend to be less accurate but more robust. Complex
    models tend to be more accurate but less robust. For more information on the bias-variance
    dilemma, refer to [http://en.wikipedia.org/wiki/Bias-variance_dilemma](http://en.wikipedia.org/wiki/Bias-variance_dilemma).
  prefs: []
  type: TYPE_NORMAL
- en: The importance of this trade-off cannot be overstated. This question pervades
    the entire discipline of machine learning. We will see concrete examples in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we will see in this chapter, there are many supervised and unsupervised algorithms.
    For example, well-known classifiers that we will cover in this chapter include
    logistic regression, nearest-neighbors, Naive Bayes, and support vector machines.
    There are many other algorithms that we can't cover here.
  prefs: []
  type: TYPE_NORMAL
- en: No model performs uniformly better than the others. One model may perform well
    on one dataset and badly on another. This is the question of **model selection**.
  prefs: []
  type: TYPE_NORMAL
- en: We will see systematic methods to assess the quality of a model on a particular
    dataset (notably cross-validation). In practice, machine learning is not an "exact
    science" in that it frequently involves trial and error. We need to try different
    models and empirically choose the one that performs best.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, understanding the details of the learning models allows us
    to gain intuition about which model is best adapted to our current problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few references on this question:'
  prefs: []
  type: TYPE_NORMAL
- en: Model selection on Wikipedia, available at [http://en.wikipedia.org/wiki/Model_selection](http://en.wikipedia.org/wiki/Model_selection)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blog post on how to choose a classifier, available at [http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning references
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are a few excellent, math-heavy textbooks on machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pattern Recognition and Machine Learning*, *Christopher M. Bishop*, *(2006)*,
    *Springer*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning – A Probabilistic Perspective*, *Kevin P. Murphy*, *(2012)*,
    *MIT Press*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Elements of Statistical Learning*, *Trevor Hastie*, *Robert Tibshirani*,
    *Jerome Friedman*, *(2009)*, *Springer*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few books more oriented toward programmers without a strong mathematical
    background:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Machine Learning for Hackers*, *Drew Conway*, *John Myles White*, *(2012)*,
    *O''Reilly Media*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning in Action*, *Peter Harrington*, (2012), *Manning Publications
    Co.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will find many other references online.
  prefs: []
  type: TYPE_NORMAL
- en: Important classes of machine learning methods that we couldn't cover in this
    chapter include neural networks and deep learning. Deep learning is the subject
    of very active research in machine learning. Many state-of-the-art results are
    currently achieved by using deep learning methods. For more information on deep
    learning, refer to [http://en.wikipedia.org/wiki/Deep_learning](http://en.wikipedia.org/wiki/Deep_learning).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we introduce the basics of the machine learning **scikit-learn**
    package ([http://scikit-learn.org](http://scikit-learn.org)). This package is
    the main tool we will use throughout this chapter. Its clean API makes it really
    easy to define, train, and test models. Plus, scikit-learn is specifically designed
    for speed and (relatively) big data.
  prefs: []
  type: TYPE_NORMAL
- en: We will show here a very basic example of linear regression in the context of
    curve fitting. This toy example will allow us to illustrate key concepts such
    as linear models, overfitting, underfitting, regularization, and cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can find all instructions to install scikit-learn in the main documentation.
    For more information, refer to [http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html).
    With anaconda, you can type `conda install scikit-learn` in a terminal.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will generate a one-dimensional dataset with a simple model (including some
    noise), and we will try to fit a function to this data. With this function, we
    can predict values on new data points. This is a **curve fitting regression**
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s make all the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now define a deterministic nonlinear function underlying our generative
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We generate the values along the curve on *[0,2]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s generate data points within *[0,1]*. We use the function *f* and
    we add some Gaussian noise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s plot our data points on *[0,1]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: In the image, the dotted curve represents the generative model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we use scikit-learn to fit a linear model to the data. There are three
    steps. First, we create the model (an instance of the `LinearRegression` class).
    Then, we fit the model to our data. Finally, we predict values from our trained
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to convert `x` and `x_tr` to column vectors, as it is a general convention
    in scikit-learn that observations are rows, while features are columns. Here,
    we have seven observations with one feature.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now plot the result of the trained linear model. We obtain a regression
    line in green here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The linear fit is not well-adapted here, as the data points are generated according
    to a nonlinear model (an exponential curve). Therefore, we are now going to fit
    a nonlinear model. More precisely, we will fit a polynomial function to our data
    points. We can still use linear regression for this, by precomputing the exponents
    of our data points. This is done by generating a **Vandermonde matrix**, using
    the `np.vander` function. We will explain this trick in *How it works…*. In the
    following code, we perform and plot the fit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: We have fitted two polynomial models of degree 2 and 5\. The degree 2 polynomial
    appears to fit the data points less precisely than the degree 5 polynomial. However,
    it seems more robust; the degree 5 polynomial seems really bad at predicting values
    outside the data points (look for example at the *x* ![How to do it...](img/4818OS_08_41.jpg)
    * 1* portion). This is what we call overfitting; by using a too-complex model,
    we obtain a better fit on the trained dataset, but a less robust model outside
    this set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Note the large coefficients of the degree 5 polynomial; this is generally a
    sign of overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will now use a different learning model called **ridge regression**. It works
    like linear regression except that it prevents the polynomial's coefficients from
    becoming too big. This is what happened in the previous example. By adding a **regularization**
    **term** in the **loss function**, ridge regression imposes some structure on
    the underlying model. We will see more details in the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ridge regression model has a meta-parameter, which represents the weight
    of the regularization term. We could try different values with trial and error
    using the `Ridge` class. However, scikit-learn provides another model called `RidgeCV`,
    which includes a parameter search with **cross-validation**. In practice, this
    means that we don't have to tweak this parameter by hand—scikit-learn does it
    for us. As the models of scikit-learn always follow the fit-predict API, all we
    have to do is replace `lm.LinearRegression()` with `lm.RidgeCV()` in the previous
    code. We will give more details in the next section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: This time, the degree 5 polynomial seems more precise than the simpler degree
    2 polynomial (which now causes **underfitting**). Ridge regression mitigates the
    overfitting issue here. Observe how the degree 5 polynomial's coefficients are
    much smaller than in the previous example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explain all the aspects covered in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: scikit-learn implements a clean and coherent API for supervised and unsupervised
    learning. Our data points should be stored in a *(N,D)* matrix *X*, where *N*
    is the number of observations and *D* is the number of features. In other words,
    each row is an observation. The first step in a machine learning task is to define
    what the matrix *X* is exactly.
  prefs: []
  type: TYPE_NORMAL
- en: In a supervised learning setup, we also have a *target*, an *N*-long vector
    *y* with a scalar value for each observation. This value is either continuous
    or discrete, depending on whether we have a regression or classification problem,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, models are implemented in classes that have the `fit()` and
    `predict()` methods. The `fit()` method accepts the data matrix *X* as input,
    and *y* as well for supervised learning models. This method *trains* the model
    on the given data.
  prefs: []
  type: TYPE_NORMAL
- en: The `predict()` method also takes data points as input (as a *(M,D)* matrix).
    It returns the labels or transformed points as predicted by the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinary least squares regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Ordinary least squares regression** is one of the simplest regression methods.
    It consists of approaching the output values *y[i]* with a linear combination
    of *X[ij]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ordinary least squares regression](img/4818OS_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *w = (w[1], ..., w[D])* is the (unknown) **parameter vector**. Also,
    ![Ordinary least squares regression](img/4818OS_08_33.jpg) represents the model''s
    output. We want this vector to match the data points *y* as closely as possible.
    Of course, the exact equality ![Ordinary least squares regression](img/4818OS_08_34.jpg)
    cannot hold in general (there is always some noise and uncertainty—models are
    always idealizations of reality). Therefore, we want to *minimize* the difference
    between these two vectors. The ordinary least squares regression method consists
    of minimizing the following **loss function**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ordinary least squares regression](img/4818OS_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This sum of the components squared is called the **L²** **norm**. It is convenient
    because it leads to *differentiable* loss functions so that gradients can be computed
    and common optimization procedures can be performed.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial interpolation with linear regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ordinary least squares regression fits a linear model to the data. The model
    is linear both in the data points *x[i]* and in the parameters *w[j]*. In our
    example, we obtain a poor fit because the data points were generated according
    to a nonlinear generative model (an exponential function).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we can still use the linear regression method with a model that is
    linear in *w[j]* but nonlinear in *x[i]*. To do this, we need to increase the
    number of dimensions in our dataset by using a basis of polynomial functions.
    In other words, we consider the following data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial interpolation with linear regression](img/4818OS_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *D* is the maximum degree. The input matrix *X* is therefore the **Vandermonde
    matrix** associated to the original data points *x[i]*. For more information on
    the Vandermonde matrix, refer to [http://en.wikipedia.org/wiki/Vandermonde_matrix](http://en.wikipedia.org/wiki/Vandermonde_matrix).
  prefs: []
  type: TYPE_NORMAL
- en: Here, it is easy to see that training a linear model on these new data points
    is equivalent to training a polynomial model on the original data points.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Polynomial interpolation with linear regression can lead to overfitting if the
    degree of the polynomials is too large. By capturing the random fluctuations (noise)
    instead of the general trend of the data, the model loses some of its predictive
    power. This corresponds to a divergence of the polynomial's coefficients *w[j]*.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to this problem is to prevent these coefficients from growing unboundedly.
    With **ridge regression** (also known as **Tikhonov regularization**), this is
    done by adding a *regularization* term to the loss function. For more details
    on Tikhonov regularization, refer to [http://en.wikipedia.org/wiki/Tikhonov_regularization](http://en.wikipedia.org/wiki/Tikhonov_regularization).
  prefs: []
  type: TYPE_NORMAL
- en: '![Ridge regression](img/4818OS_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By minimizing this loss function, we not only minimize the error between the
    model and the data (first term, related to the bias), but also the size of the
    model's coefficients (second term, related to the variance). The bias-variance
    trade-off is quantified by the hyperparameter ![Ridge regression](img/4818OS_08_43.jpg),
    which specifies the relative weight between the two terms in the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Here, ridge regression led to a polynomial with smaller coefficients, and thus
    a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation and grid search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A drawback of the ridge regression model compared to the ordinary least squares
    model is the presence of an extra hyperparameter ![Cross-validation and grid search](img/4818OS_08_43.jpg).
    The quality of the prediction depends on the choice of this parameter. One possibility
    would be to fine-tune this parameter manually, but this procedure can be tedious
    and can also lead to overfitting problems.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, we can use a **grid search**; we loop over many possible
    values for ![Cross-validation and grid search](img/4818OS_08_43.jpg), and we evaluate
    the performance of the model for each possible value. Then, we choose the parameter
    that yields the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: How can we assess the performance of a model with a given ![Cross-validation
    and grid search](img/4818OS_08_43.jpg) value? A common solution is to use **cross-validation**.
    This procedure consists of splitting the dataset into a training set and a test
    set. We fit the model on the train set, and we test its predictive performance
    on the *test set*. By testing the model on a different dataset than the one used
    for training, we reduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to split the initial dataset into two parts like this. One
    possibility is to remove *one* sample to form the train set and to put this one
    sample into the test set. This is called **Leave-One-Out** cross-validation. With
    *N* samples, we obtain *N* sets of train and test sets. The cross-validated performance
    is the average performance on all these set decompositions.
  prefs: []
  type: TYPE_NORMAL
- en: As we will see later, scikit-learn implements several easy-to-use functions
    to do cross-validation and grid search. In this recipe, there exists a special
    estimator called `RidgeCV` that implements a cross-validation and grid search
    procedure that is specific to the ridge regression model. Using this class ensures
    that the best hyperparameter ![Cross-validation and grid search](img/4818OS_08_43.jpg)
    is found automatically for us.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references about least squares:'
  prefs: []
  type: TYPE_NORMAL
- en: Ordinary least squares on Wikipedia, available at [http://en.wikipedia.org/wiki/Ordinary_least_squares](http://en.wikipedia.org/wiki/Ordinary_least_squares)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear least squares on Wikipedia, available at [http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)](http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few references about cross-validation and grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid search in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/grid_search.html](http://scikit-learn.org/stable/modules/grid_search.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation on Wikipedia, available at [http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few references about scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn basic tutorial available at [http://scikit-learn.org/stable/tutorial/basic/tutorial.html](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn tutorial given at the SciPy 2013 conference, available at [https://github.com/jakevdp/sklearn_scipy2013](https://github.com/jakevdp/sklearn_scipy2013)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Using support vector machines for classification tasks* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting who will survive on the Titanic with logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will introduce **logistic regression**, a basic classifier.
    We will also show how to perform a **grid search** with **cross-validation**.
  prefs: []
  type: TYPE_NORMAL
- en: We will apply these techniques on a **Kaggle** dataset where the goal is to
    predict survival on the Titanic based on real data.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kaggle ([www.kaggle.com/competitions](http://www.kaggle.com/competitions)) hosts
    machine learning competitions where anyone can download a dataset, train a model,
    and test the predictions on the website. The author of the best model might even
    win a prize! It is a fun way to get started with machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the *Titanic* dataset from the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has been obtained from [www.kaggle.com/c/titanic-gettingStarted](http://www.kaggle.com/c/titanic-gettingStarted).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import the standard packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the training and test datasets with pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s keep only a few fields for this example, and also convert the `sex`
    field to a binary variable so that it can be handled correctly by NumPy and scikit-learn.
    Finally, we remove the rows that contain `NaN` values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we convert this `DataFrame` object to a NumPy array so that we can pass
    it to scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s have a look at the survival of male and female passengers as a function
    of their age:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Let''s try to train a `LogisticRegression` classifier in order to predict the
    survival of people based on their gender, age, and class. We first need to create
    a train and a test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We train the model and we get the predicted values on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure shows the actual and predicted results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: In this screenshot, the first line shows the survival of several people from
    the test set (white for survival, black otherwise). The second line shows the
    values predicted by the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To get an estimation of the model''s performance, we compute the cross-validation
    score with the `cross_val_score()` function. This function uses a three-fold stratified
    cross-validation procedure by default, but this can be changed with the `cv` keyword
    argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function returns, for each pair of train and test set, a prediction score
    (we give more details in *How it works…*).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `LogisticRegression` class accepts a *C* hyperparameter as an argument.
    This parameter quantifies the regularization strength. To find a good value, we
    can perform a grid search with the generic `GridSearchCV` class. It takes an estimator
    as input and a dictionary of parameter values. This new estimator uses cross-validation
    to select the best parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the performance of the best estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Performance is slightly better after the *C* hyperparameter has been chosen
    with a grid search.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is *not* a regression model, it is a classification model.
    Yet, it is closely related to linear regression. This model predicts the probability
    that a binary variable is 1, by applying a **sigmoid function** (more precisely,
    a logistic function) to a linear combination of the variables. The equation of
    the sigmoid is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure shows a logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A logistic function
  prefs: []
  type: TYPE_NORMAL
- en: If a binary variable has to be obtained, we can round the value to the closest
    integer.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter *w* is obtained with an optimization procedure during the learning
    step.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression on Wikipedia, available at [http://en.wikipedia.org/wiki/Logistic_regression](http://en.wikipedia.org/wiki/Logistic_regression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Getting started with scikit-learn* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Learning to recognize handwritten digits with a K-nearest neighbors classifier*
    recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using support vector machines for classification tasks* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to recognize handwritten digits with a K-nearest neighbors classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to recognize handwritten digits with a **K-nearest
    neighbors** (**K-NN**) classifier. This classifier is a simple but powerful model,
    well-adapted to complex, highly nonlinear datasets such as images. We will explain
    how it works later in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import the modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s load the *digits* dataset, part of the `datasets` module of scikit-learn.
    This dataset contains handwritten digits that have been manually labeled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the matrix `X`, each row contains *8 * 8=64* pixels (in grayscale, values
    between 0 and 16). The row-major ordering is used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s display some of the images along with their labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now, let''s fit a K-nearest neighbors classifier on the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s evaluate the score of the trained classifier on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's see if our classifier can recognize a *handwritten* digit!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Can our model recognize this number? Let''s see:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Good job!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This example illustrates how to deal with images in scikit-learn. An image is
    a 2D *(N, M)* matrix, which has *NM* features. This matrix needs to be flattened
    when composing the data matrix; each row is a full image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of K-nearest neighbors is as follows: given a new point in the feature
    space, find the *K* closest points from the training set and assign the label
    of the majority of those points.'
  prefs: []
  type: TYPE_NORMAL
- en: The distance is generally the Euclidean distance, but other distances can be
    used too.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows the space partition obtained with a 15-nearest-neighbors
    classifier on a toy dataset (with three labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: K-nearest neighbors space partition
  prefs: []
  type: TYPE_NORMAL
- en: The number *K* is a hyperparameter of the model. If it is too small, the model
    will not generalize well (high variance). In particular, it will be highly sensitive
    to outliers. By contrast, the precision of the model will worsen if *K* is too
    large. At the extreme, if *K* is equal to the total number of points, the model
    will always predict the exact same value disregarding the input (high bias). There
    are heuristics to choose this hyperparameter (see the next section).
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that no model is learned by a K-nearest neighbor algorithm;
    the classifier just stores all data points and compares any new target points
    with them. This is an example of **instance-based learning**. It is in contrast
    to other classifiers such as the logistic regression model, which explicitly learns
    a simple mathematical model on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: The K-nearest neighbors method works well on complex classification problems
    that have irregular decision boundaries. However, it might be computationally
    intensive with large training datasets because a large number of distances have
    to be computed for testing. Dedicated tree-based data structures such as **K-D
    trees** or **ball trees** can be used to accelerate the search of nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: The K-nearest neighbors method can be used for classification, like here, and
    also for regression problems. The model assigns the average of the target value
    of the nearest neighbors. In both cases, different weighting strategies can be
    used.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  prefs: []
  type: TYPE_NORMAL
- en: The K-NN algorithm in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/neighbors.html](http://scikit-learn.org/stable/modules/neighbors.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The K-NN algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm](http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blog post about how to choose the K hyperparameter, available at [http://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/](http://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instance-based learning on Wikipedia, available at [http://en.wikipedia.org/wiki/Instance-based_learning](http://en.wikipedia.org/wiki/Instance-based_learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Predicting who will survive on the Titanic with logistic regression* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using support vector machines for classification tasks* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning from text – Naive Bayes for Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we show how to handle text data with scikit-learn. Working with
    text requires careful preprocessing and feature extraction. It is also quite common
    to deal with highly sparse matrices.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn to recognize whether a comment posted during a public discussion
    is considered insulting to one of the participants. We will use a labeled dataset
    from Impermium, released during a Kaggle competition.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the *Troll* dataset from the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data).
  prefs: []
  type: TYPE_NORMAL
- en: This dataset was obtained from Kaggle, at [www.kaggle.com/c/detecting-insults-in-social-commentary](http://www.kaggle.com/c/detecting-insults-in-social-commentary).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import our libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s open the CSV file with pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each row is a comment. We will consider two columns: whether the comment is
    insulting (1) or not (0) and the unicode-encoded contents of the comment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are going to define the feature matrix `X` and the labels `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Obtaining the feature matrix from the text is not trivial. scikit-learn can
    only work with numerical matrices. So how do we convert text into a matrix of
    numbers? A classical solution is to first extract a **vocabulary**, a list of
    words used throughout the corpus. Then, we count, for each sample, the frequency
    of each word. We end up with a **sparse matrix**, a huge matrix containing mostly
    zeros. Here, we do this in two lines. We will give more details in *How it works…*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The general rule here is that whenever one of our features is categorical (that
    is, the presence of a word, a color belonging to a fixed set of *n* colors, and
    so on), we should *vectorize* it by considering one binary feature per item in
    the class. For example, instead of a feature `color` being `red`, `green`, or
    `blue`, we should consider three *binary* features `color_red`, `color_green`,
    and `color_blue`. We give further references in the *There's more…* section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are 3947 comments and 16469 different words. Let''s estimate the sparsity
    of this feature matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are going to train a classifier as usual. We first split the data into
    a train and test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use a **Bernoulli Naive Bayes classifier** with a grid search on the ![How
    to do it...](img/4818OS_08_43.jpg) parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s check the performance of this classifier on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at the words corresponding to the largest coefficients (the
    words we find frequently in insulting comments):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s test our estimator on a few test sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's not bad, but we can probably do better.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: scikit-learn implements several utility functions to obtain a sparse feature
    matrix from text data. A **vectorizer** such as `CountVectorizer()` extracts a
    vocabulary from a corpus (`fit`) and constructs a sparse representation of the
    corpus based on this vocabulary (`transform`). Each sample is represented by the
    vocabulary's word frequencies. The trained instance also contains attributes and
    methods to map feature indices to the corresponding words (`get_feature_names()`)
    and conversely (`vocabulary_`).
  prefs: []
  type: TYPE_NORMAL
- en: N-grams can also be extracted. These are pairs or tuples of words occurring
    successively (the `ngram_range` keyword).
  prefs: []
  type: TYPE_NORMAL
- en: The frequency of the words can be weighted in different ways. Here, we have
    used **tf-idf**, or **term frequency-inverse document frequency**. This quantity
    reflects how important a word is to a corpus. Frequent words in comments have
    a high weight except if they appear in most comments (which means that they are
    common terms, for example, "the" and "and" would be filtered out using this technique).
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes algorithms are Bayesian methods based on the naive assumption of
    independence between the features. This strong assumption drastically simplifies
    the computations and leads to very fast yet decent classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  prefs: []
  type: TYPE_NORMAL
- en: Text feature extraction in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequency-inverse document-frequency on Wikipedia, available at [http://en.wikipedia.org/wiki/tf-idf](http://en.wikipedia.org/wiki/tf-idf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorizer in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes classifier on Wikipedia, at [http://en.wikipedia.org/wiki/Naive_Bayes_classifier](http://en.wikipedia.org/wiki/Naive_Bayes_classifier)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/naive_bayes.html](http://scikit-learn.org/stable/modules/naive_bayes.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impermium Kaggle challenge, at [http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/](http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document classification example in scikit-learn's documentation, at [http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html](http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides scikit-learn, which has good support for text processing, we should
    also mention NLTK (available at [www.nltk.org](http://www.nltk.org)), a Natural
    Language Toolkit in Python.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Predicting who will survive on the Titanic with logistic regression* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Learning to recognize handwritten digits with a K-nearest neighbors classifier*
    recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using support vector machines for classification tasks* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using support vector machines for classification tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we introduce **support vector machines**, or **SVMs**. These
    powerful models can be used for classification and regression. Here, we illustrate
    how to use linear and nonlinear SVMs on a simple classification task.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import the packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We generate 2D points and assign a binary label according to a linear operation
    on the coordinates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now fit a linear **Support Vector Classifier** (**SVC**). This classifier
    tries to separate the two groups of points with a linear boundary (a line here,
    but more generally a hyperplane):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define a function that displays the boundaries and decision function of
    a trained classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at the classification results with the linear SVC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The linear SVC tried to separate the points with a line and it did a pretty
    good job here.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now modify the labels with an `XOR` function. A point''s label is 1 if the
    coordinates have different signs. This classification is not linearly separable.
    Therefore, a linear SVC fails completely:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Fortunately, it is possible to use nonlinear SVCs by using nonlinear **kernels**.
    Kernels specify a nonlinear transformation of the points into a higher dimensional
    space. Transformed points in this space are assumed to be more linearly separable.
    By default, the `SVC` classifier in scikit-learn uses the **Radial Basis Function**
    (**RBF**) kernel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: This time, the nonlinear SVC successfully managed to classify these nonlinearly
    separable points.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A two-class linear SVC tries to find a hyperplane (defined as a linear equation)
    that best separates the two sets of points (grouped according to their labels).
    There is also the constraint that this separating hyperplane needs to be as far
    as possible from the points. This method works best when such a hyperplane exists.
    Otherwise, this method can fail completely, as we saw in the `XOR` example. `XOR`
    is known as being a nonlinearly separable operation.
  prefs: []
  type: TYPE_NORMAL
- en: The SVM classes in scikit-learn have a *C* hyperparameter. This hyperparameter
    trades off misclassification of training examples against simplicity of the decision
    surface. A low *C* value makes the decision surface smooth, while a high *C* value
    aims at classifying all training examples correctly. This is another example where
    a hyperparameter quantifies the bias-variance trade-off. This hyperparameter can
    be chosen with cross-validation and grid search.
  prefs: []
  type: TYPE_NORMAL
- en: The linear SVC can also be extended to multiclass problems. The multiclass SVC
    is directly implemented in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: The nonlinear SVC works by considering a nonlinear transformation ![How it works...](img/4818OS_08_35.jpg)
    from the original space into a higher dimensional space. This nonlinear transformation
    can increase the linear separability of the classes. In practice, all dot products
    are replaced by the ![How it works...](img/4818OS_08_36.jpg) kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Nonlinear SVC
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several widely-used nonlinear kernels. By default, SVC uses Gaussian
    radial basis functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![How it works...](img/4818OS_08_37.jpg) is a hyperparameter of the model
    that can be chosen with grid search and cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: The ![How it works...](img/4818OS_08_42.jpg) function does not need to be computed
    explicitly. This is the **kernel trick**; it suffices to know the kernel *k(x,
    x')*. The existence of a function ![How it works...](img/4818OS_08_42.jpg) corresponding
    to a given kernel *k(x, x')* is guaranteed by a mathematical theorem in functional
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references about support vector machines:'
  prefs: []
  type: TYPE_NORMAL
- en: Exclusive OR on Wikipedia, available at [http://en.wikipedia.org/wiki/Exclusive_or](http://en.wikipedia.org/wiki/Exclusive_or)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines on Wikipedia, available at [http://en.wikipedia.org/wiki/Support_vector_machine](http://en.wikipedia.org/wiki/Support_vector_machine)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVMs in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/svm.html](http://scikit-learn.org/stable/modules/svm.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel trick on Wikipedia, available at [http://en.wikipedia.org/wiki/Kernel_method](http://en.wikipedia.org/wiki/Kernel_method)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notes about the kernel trick available at [www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example with a nonlinear SVM available at [http://scikit-learn.org/0.11/auto_examples/svm/plot_svm_nonlinear.html](http://scikit-learn.org/0.11/auto_examples/svm/plot_svm_nonlinear.html)
    (this example inspired this recipe)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Predicting who will survive on the Titanic with logistic regression* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Learning to recognize handwritten digits with a K-nearest neighbors classifier*
    recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a random forest to select important features for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Decision trees** are frequently used to represent workflows or algorithms.
    They also form a method for nonparametric supervised learning. A tree mapping
    observations to target values is learned on a training set and gives the outcomes
    of new observations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random forests** are ensembles of decision trees. Multiple decision trees
    are trained and aggregated to form a model that is more performant than any of
    the individual trees. This general idea is the purpose of **ensemble learning**.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of ensemble methods. Random forests are an instance of
    **bootstrap aggregating**, also called **bagging**, where models are trained on
    randomly drawn subsets of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests yield information about the importance of each feature for the
    classification or regression task. In this recipe, we will find the most influential
    features of Boston house prices using a classic dataset that contains a range
    of diverse indicators about the houses' neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import the packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the Boston dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The details of this dataset can be found in `data[''DESCR'']`. Here is the
    description of some features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*CRIM*: Per capita crime rate by town'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NOX*: Nitric oxide concentration (parts per 10 million)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*RM*: Average number of rooms per dwelling'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AGE*: Proportion of owner-occupied units built prior to 1940'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DIS*: Weighted distances to five Boston employment centers'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PTRATIO*: Pupil-teacher ratio by town'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LSTAT*: Percentage of lower status of the population'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MEDV*: Median value of owner-occupied homes in $1000s'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The target value is *MEDV*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We create a `RandomForestRegressor` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get the samples and the target values from this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The importance of our features can be found in `reg.feature_importances_`.
    We sort them by decreasing order of importance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we plot a histogram of the features'' importance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'We find that *LSTAT* (proportion of lower status of the population) and *RM*
    (number of rooms per dwelling) are the most important features determining the
    price of a house. As an illustration, here is a scatter plot of the price as a
    function of *LSTAT*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several algorithms can be used to train a decision tree. scikit-learn uses the
    **CART**, or **Classification and Regression Trees**, algorithm. This algorithm
    constructs binary trees using the feature and threshold that yield the largest
    information gain at each node. Terminal nodes give the outcomes of input values.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are simple to understand. They can also be visualized with **pydot**,
    a Python package for drawing graphs and trees. This is useful when we want to
    understand what a tree has learned exactly (**white box model**); the conditions
    that apply on the observations at each node can be expressed easily with Boolean
    logic.
  prefs: []
  type: TYPE_NORMAL
- en: However, decision trees may suffer from overfitting, notably when they are too
    deep, and they might be unstable. Additionally, global convergence toward an optimal
    model is not guaranteed, particularly when greedy algorithms are used for training.
    These problems can be mitigated by using ensembles of decision trees, notably
    random forests.
  prefs: []
  type: TYPE_NORMAL
- en: In a random forest, multiple decision trees are trained on bootstrap samples
    of the training dataset (randomly sampled with replacement). Predictions are made
    with the averages of individual trees' predictions (bootstrap aggregating or bagging).
    Additionally, random subsets of the features are chosen at each node (**random
    subspace method**). These methods lead to an overall better model than the individual
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/ensemble.html](http://scikit-learn.org/stable/modules/ensemble.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API reference of `RandomForestRegressor` available at [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests on Wikipedia, available at [http://en.wikipedia.org/wiki/Random_forest](http://en.wikipedia.org/wiki/Random_forest)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree learning on Wikipedia, available at [http://en.wikipedia.org/wiki/Decision_tree_learning](http://en.wikipedia.org/wiki/Decision_tree_learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bootstrap aggregating on Wikipedia, available at [http://en.wikipedia.org/wiki/Bootstrap_aggregating](http://en.wikipedia.org/wiki/Bootstrap_aggregating)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random subspace method on Wikipedia, available at [http://en.wikipedia.org/wiki/Random_subspace_method](http://en.wikipedia.org/wiki/Random_subspace_method)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble learning on Wikipedia, available at [http://en.wikipedia.org/wiki/Ensemble_learning](http://en.wikipedia.org/wiki/Ensemble_learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Using support vector machines for classification tasks* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the dimensionality of a dataset with a principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipes, we presented *supervised learning* methods; our data
    points came with discrete or continuous labels, and the algorithms were able to
    learn the mapping from the points to the labels.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with this recipe, we will present **unsupervised learning** methods.
    These methods might be helpful prior to running a supervised learning algorithm.
    They can give a first insight into the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that our data consists of points *x[i]* without any labels. The
    goal is to discover some form of hidden structure in this set of points. Frequently,
    data points have intrinsic low dimensionality: a small number of features suffice
    to accurately describe the data. However, these features might be hidden among
    many other features not relevant to the problem. Dimension reduction can help
    us find these structures. This knowledge can considerably improve the performance
    of subsequent supervised learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Another useful application of unsupervised learning is **data visualization**;
    high-dimensional datasets are hard to visualize in 2D or 3D. Projecting the data
    points on a subspace or submanifold yields more interesting visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will illustrate a basic unsupervised linear method, **principal
    component analysis** (**PCA**). This algorithm lets us project data points linearly
    on a low-dimensional subspace. Along the **principal components**, which are vectors
    forming a basis of this low-dimensional subspace, the variance of the data points
    is maximum.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the classic *Iris flower* dataset as an example. This dataset contains
    the width and length of the petal and sepal of 150 iris flowers. These flowers
    belong to one of three categories: *Iris setosa*, *Iris virginica*, and *Iris
    versicolor*. We have access to the category in this dataset (labeled data). However,
    because we are interested in illustrating an unsupervised learning method, we
    will only use the data matrix *without* the labels.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import NumPy, matplotlib, and scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The *Iris flower* dataset is available in the `datasets` module of scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each row contains four parameters related to the morphology of the flower.
    Let''s display the first two dimensions. The color reflects the iris variety of
    the flower (the label, between 0 and 2):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: If you're reading the printed version of this book, you might not be able to
    distinguish the colors. You will find the colored images on the book's website.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now apply PCA on the dataset to get the transformed matrix. This operation
    can be done in a single line with scikit-learn: we instantiate a `PCA` model and
    call the `fit_transform()` method. This function computes the principal components
    and projects the data on them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now display the same dataset, but in a new coordinate system (or equivalently,
    a linearly transformed version of the initial dataset):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Points belonging to the same classes are now grouped together, even though the
    `PCA` estimator did *not* use the labels. The PCA was able to find a projection
    maximizing the variance, which corresponds here to a projection where the classes
    are well separated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `scikit.decomposition` module contains several variants of the classic
    `PCA` estimator: `ProbabilisticPCA`, `SparsePCA`, `RandomizedPCA`, `KernelPCA`,
    and others. As an example, let''s take a look at `KernelPCA`, a nonlinear version
    of PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at the mathematical ideas behind PCA. This method is based on a
    matrix decomposition called **Singular Value Decomposition** (**SVD**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *X* is the *(N,D)* data matrix, *U* and *V* are orthogonal matrices, and
    ![How it works...](img/4818OS_08_38.jpg) is a *(N,D)* diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA transforms *X* into *X''* defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The diagonal elements of ![How it works...](img/4818OS_08_38.jpg) are the **singular
    values** of *X*. By convention, they are generally sorted in descending order.
    The columns of *U* are orthonormal vectors called the **left singular vectors**
    of *X*. Therefore, the columns of *X'* are the left singular vectors multiplied
    by the singular values.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, PCA converts the initial set of observations, which are made of
    possibly correlated variables, into vectors of linearly uncorrelated variables
    called **principal components**.
  prefs: []
  type: TYPE_NORMAL
- en: The first new feature (or first component) is a transformation of all original
    features such that the dispersion (variance) of the data points is the highest
    in that direction. In the subsequent principal components, the variance is decreasing.
    In other words, PCA gives us an alternative representation of our data where the
    new features are sorted according to how much they account for the variability
    of the points.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few further references:'
  prefs: []
  type: TYPE_NORMAL
- en: Iris flower dataset on Wikipedia, available at [http://en.wikipedia.org/wiki/Iris_flower_data_set](http://en.wikipedia.org/wiki/Iris_flower_data_set)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA on Wikipedia, available at [http://en.wikipedia.org/wiki/Principal_component_analysis](http://en.wikipedia.org/wiki/Principal_component_analysis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD decomposition on Wikipedia, available at [http://en.wikipedia.org/wiki/Singular_value_decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris dataset example available at [http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decompositions in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/decomposition.html](http://scikit-learn.org/stable/modules/decomposition.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning tutorial with scikit-learn available at [http://scikit-learn.org/dev/tutorial/statistical_inference/unsupervised_learning.html](http://scikit-learn.org/dev/tutorial/statistical_inference/unsupervised_learning.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Detecting hidden structures in a dataset with clustering* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting hidden structures in a dataset with clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A large part of unsupervised learning is devoted to the **clustering** problem.
    The goal is to group similar points together in a totally unsupervised way. Clustering
    is a hard problem, as the very definition of **clusters** (or **groups**) is not
    necessarily well posed. In most datasets, stating that two points should belong
    to the same cluster may be context-dependent or even subjective.
  prefs: []
  type: TYPE_NORMAL
- en: There are many clustering algorithms. We will see a few of them in this recipe,
    applied to a toy example.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s generate a random dataset with three clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need a couple of functions to relabel and display the results of the clustering
    algorithms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we cluster the dataset with the **K-means** algorithm, a classic and simple
    clustering algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: If you're reading the printed version of this book, you might not be able to
    distinguish the colors. You will find the colored images on the book's website.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This algorithm needs to know the number of clusters at initialization time.
    In general, however, we do not necessarily know the number of clusters in the
    dataset. Here, let''s try with `n_clusters=3` (that''s cheating, because we happen
    to know that there are 3 clusters!):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Let''s try a few other clustering algorithms implemented in scikit-learn. The
    simplicity of the API makes it really easy to try different methods; it is just
    a matter of changing the name of the class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/4818OS_08_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The first two algorithms required the number of clusters as input. The next
    two did not, but they were able to find the right number: 3\. The last one failed
    at finding the correct number of clusters (this is *overclustering*—too many clusters
    have been found).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The K-means clustering algorithm consists of partitioning the data points *x[j]*
    into *K* clusters *S[i]* so as to minimize the within-cluster sum of squares:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![How it works...](img/4818OS_08_39.jpg) is the center of the cluster
    *i* (average of all points in *S[i]*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it is very hard to solve this problem exactly, approximation algorithms
    exist. A popular one is **Lloyd''s** **algorithm**. It consists of starting from
    an initial set of *K* means ![How it works...](img/4818OS_08_39.jpg) and alternating
    between two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the *assignment* step, the points are assigned to the cluster associated
    to the closest mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the *update* step, the means are recomputed from the last assignments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm converges to a solution that is not guaranteed to be optimal.
  prefs: []
  type: TYPE_NORMAL
- en: The **expectation-maximization algorithm** can be seen as a probabilistic version
    of the K-means algorithm. It is implemented in the `mixture` module of scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: The other clustering algorithms used in this recipe are explained in the scikit-learn
    documentation. There is no clustering algorithm that works uniformly better than
    all the others, and every algorithm has its strengths and weaknesses. You will
    find more details in the references in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  prefs: []
  type: TYPE_NORMAL
- en: The K-means clustering algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expectation-maximization algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/Expectation-maximization_algorithm](http://en.wikipedia.org/wiki/Expectation-maximization_algorithm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Reducing the dimensionality of a dataset with principal component analysis*
    recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
