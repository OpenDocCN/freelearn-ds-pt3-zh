- en: Chapter 8. Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with scikit-learn
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting who will survive on the Titanic with logistic regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to recognize handwritten digits with a K-nearest neighbors classifier
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning from text – Naive Bayes for Natural Language Processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using support vector machines for classification tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a random forest to select important features for regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the dimensionality of a dataset with a Principal Component Analysis
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting hidden structures in a dataset with clustering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we were interested in getting insight into data, understanding
    complex phenomena through partial observations, and making informed decisions
    in the presence of uncertainty. Here, we are still interested in analyzing and
    processing data using statistical tools. However, the goal is not necessarily
    to *understand* the data, but to *learn* from it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Learning from data is close to what we do as humans. From our experience, we
    intuitively learn general facts and relations about the world, even if we don't
    fully understand their complexity. The increasing computational power of computers
    makes them able to learn from data too. That's the heart of **machine learning**,
    a modern and fascinating branch of artificial intelligence, computer science,
    statistics, and applied mathematics. For more information on machine learning,
    refer to [http://en.wikipedia.org/wiki/Machine_learning](http://en.wikipedia.org/wiki/Machine_learning).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is a hands-on introduction to some of the most basic methods in
    machine learning. These methods are routinely used by data scientists. We will
    use these methods with **scikit-learn**, a popular and user-friendly Python package
    for machine learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: A bit of vocabulary
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this introduction, we will explain the fundamental definitions and concepts
    of machine learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Learning from data
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In machine learning, most data can be represented as a table of numerical values.
    Every row is called an **observation**, a **sample**, or a **data point**. Every
    column is called a **feature** or a **variable**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Let's call *N* the number of rows (or the number of points) and *D* the number
    of columns (or number of features). The number *D* is also called the **dimensionality**
    of the data. The reason is that we can view this table as a set *E* of **vectors**
    in a space with *D* dimensions (or **vector space**). Here, a vector **x** contains
    *D* numbers *(x[1], ..., x[D])*, also called **components**. This mathematical
    point of view is very useful and we will use it throughout this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'We generally make the distinction between supervised learning and unsupervised
    learning:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning** is when we have a **label** *y* associated with a data
    point *x*. The goal is to *learn* the mapping from *x* to *y* from our data. The
    data gives us this mapping for a finite set of points, but what we want is to
    *generalize* this mapping to the full set *E*.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**是指我们有一个与数据点*x*相关联的**标签***y*。目标是从我们的数据中*学习*从*x*到*y*的映射。数据为我们提供了一个有限点集的映射，但我们希望将这个映射*泛化*到整个集合*E*。'
- en: '**Unsupervised learning** is when we don''t have any labels. What we want to
    do is discover some form of hidden structure in the data.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**是指我们没有任何标签。我们希望做的是发现数据中的某种隐藏结构。'
- en: Supervised learning
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'Mathematically, supervised learning consists of finding a function *f* that
    maps the set of points *E* to a set of labels *F*, knowing a finite set of associations
    *(x, y)*, which is given by our data. This is what *generalization* is about:
    after observing the pairs *(x[i], y[i])*, given a new *x*, we are able to find
    the corresponding *y* by applying the function *f* to *x*. For more information
    on supervised learning, refer to [http://en.wikipedia.org/wiki/Supervised_learning](http://en.wikipedia.org/wiki/Supervised_learning).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，监督学习包括找到一个函数*f*，将点集*E*映射到标签集*F*，并且已知一个有限的关联集*(x, y)*，这些数据来自我们的数据集。这就是*泛化*的意义所在：在观察了对*(x[i],
    y[i])*后，给定一个新的*x*，我们能够通过将函数*f*应用到*x*上来找到相应的*y*。关于监督学习的更多信息，请参阅[http://en.wikipedia.org/wiki/Supervised_learning](http://en.wikipedia.org/wiki/Supervised_learning)。
- en: 'It is a common practice to split the set of data points into two subsets: the
    **training set** and the **test set**. We learn the function *f* on the training
    set and test it on the test set. This is essential when assessing the predictive
    power of a model. By training and testing a model on the same set, our model might
    not be able to generalize well. This is the fundamental concept of **overfitting**,
    which we will detail later in this chapter.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通常做法是将数据集分为两个子集：**训练集**和**测试集**。我们在训练集上学习函数*f*，并在测试集上进行测试。这对于评估模型的预测能力至关重要。如果在同一数据集上训练和测试模型，我们的模型可能无法很好地泛化。这就是**过拟合**的基本概念，我们将在本章后面详细讲解。
- en: We generally make the distinction between classification and regression, two
    particular instances of supervised learning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常区分分类和回归，它们是监督学习的两种特定实例。
- en: '**Classification** is when our labels *y* can only take a finite set of values
    (categories). Examples include:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类**是指我们的标签*y*只能取有限的值（类别）。例如：'
- en: '**Handwritten digit recognition**: *x* is an image with a handwritten digit;
    *y* is a digit between 0 and 9'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手写数字识别**：*x*是一个包含手写数字的图像；*y*是一个0到9之间的数字'
- en: '**Spam filtering**: *x* is an e-mail and *y* is 1 or 0, depending on whether
    that e-mail is spam or not'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**垃圾邮件过滤**：*x*是电子邮件，*y*是1或0，取决于该电子邮件是否是垃圾邮件'
- en: '**Regression** is when our labels *y* can take any real (continuous) value.
    Examples include:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**是指我们的标签*y*可以取任何实数（连续值）。例如：'
- en: Predicting stock market data
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股票市场数据预测
- en: Predicting sales
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 销售预测
- en: Detecting the age of a person from a picture
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图片中检测一个人的年龄
- en: 'A classification task yields a division of our space *E* in different regions
    (also called **partition**), each region being associated to one particular value
    of the label *y*. A regression task yields a mathematical model that associates
    a real number to any point *x* in the space *E*. This difference is illustrated
    in the following figure:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务将我们的空间*E*划分为不同的区域（也称为**划分**），每个区域与标签*y*的某个特定值相关联。回归任务则产生一个数学模型，将一个实数与空间*E*中的任何点*x*关联。这一差异可以通过下图来说明：
- en: '![Supervised learning](img/4818OS_08_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/4818OS_08_01.jpg)'
- en: Difference between classification and regression
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 分类与回归的区别
- en: Classification and regression can be combined. For example, in the **probit
    model**, although the dependent variable is binary (classification), the *probability*
    that this variable belongs to one category can also be modeled (regression). We
    will see an example in the recipe about logistic regression. For more information
    on the probit model, refer to [http://en.wikipedia.org/wiki/Probit_model](http://en.wikipedia.org/wiki/Probit_model).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 分类与回归可以结合使用。例如，在**probit模型**中，尽管因变量是二元的（分类），但该变量属于某一类别的*概率*也可以通过回归来建模。我们将在关于逻辑回归的示例中看到这一点。关于probit模型的更多信息，请参阅[http://en.wikipedia.org/wiki/Probit_model](http://en.wikipedia.org/wiki/Probit_model)。
- en: Unsupervised learning
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Broadly speaking, unsupervised learning helps us discover systemic structures
    in our data. This is harder to grasp than supervised learning, in that there is
    generally no precise question and answer. For more information on unsupervised
    learning, refer to [http://en.wikipedia.org/wiki/Unsupervised_learning](http://en.wikipedia.org/wiki/Unsupervised_learning).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 广义而言，非监督学习帮助我们发现数据中的系统性结构。这比监督学习更难理解，因为通常没有明确的问题和答案。欲了解更多非监督学习的信息，请参考[http://en.wikipedia.org/wiki/Unsupervised_learning](http://en.wikipedia.org/wiki/Unsupervised_learning)。
- en: 'Here are a few important terms related to unsupervised learning:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些与非监督学习相关的重要术语：
- en: '**Clustering**: Grouping similar points together within **clusters**'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：将相似的点聚集在**簇**内'
- en: '**Density estimation**: Estimating a probability density function that can
    explain the distribution of the data points'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密度估计**：估计一个概率密度函数，用以解释数据点的分布'
- en: '**Dimension reduction**: Getting a simple representation of high-dimensional
    data points by projecting them onto a lower-dimensional space (notably for **data
    visualization**)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：通过将数据点投影到低维空间，获取高维数据点的简单表示（特别是用于**数据可视化**）'
- en: '**Manifold learning**: Finding a low-dimensional manifold containing the data
    points (also known as **nonlinear dimension reduction**)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流形学习**：找到包含数据点的低维流形（也称为**非线性降维**）'
- en: Feature selection and feature extraction
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征选择和特征提取
- en: In a supervised learning context, when our data contains many features, it is
    sometimes necessary to choose a subset of them. The features we want to keep are
    those that are most relevant to our question. This is the problem of **feature
    selection**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习的背景下，当我们的数据包含许多特征时，有时需要选择其中的一个子集。我们想保留的特征是那些与我们问题最相关的特征。这就是**特征选择**的问题。
- en: Additionally, we might want to extract new features by applying complex transformations
    on our original dataset. This is **feature extraction**. For example, in computer
    vision, training a classifier directly on the pixels is not the most efficient
    method in general. We might want to extract the relevant points of interest or
    make appropriate mathematical transformations. These steps depend on our dataset
    and on the questions we want to answer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可能希望通过对原始数据集应用复杂的变换来提取新特征。这就是**特征提取**。例如，在计算机视觉中，直接在像素上训练分类器通常不是最有效的方法。我们可能希望提取相关的兴趣点或进行适当的数学变换。这些步骤取决于我们的数据集和我们希望回答的问题。
- en: For example, it is often necessary to preprocess the data before learning models.
    **Feature scaling** (or **data normalization**) is a common **preprocessing**
    step where features are linearly rescaled to fit in the range *[-1,1]* or *[0,1]*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在使用学习模型之前，通常需要对数据进行预处理。**特征缩放**（或**数据归一化**）是一个常见的**预处理**步骤，其中特征会线性地重新缩放，以适应范围*[-1,1]*或*[0,1]*。
- en: Feature extraction and feature selection involve a balanced combination of domain
    expertise, intuition, and mathematical methods. These early steps are crucial,
    and they might be even more important than the learning steps themselves. The
    reason is that the few dimensions that are relevant to our problem are generally
    hidden in the high dimensionality of our dataset. We need to uncover the low-dimensional
    structure of interest to improve the efficiency of the learning models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取和特征选择涉及领域专业知识、直觉和数学方法的平衡组合。这些早期步骤至关重要，可能比学习步骤本身还要重要。原因是与我们问题相关的少数维度通常隐藏在数据集的高维度中。我们需要揭示感兴趣的低维结构，以提高学习模型的效率。
- en: We will see a few feature selection and feature extraction methods in this chapter.
    Methods that are specific to signals, images, or sounds will be covered in [Chapter
    10](ch10.html "Chapter 10. Signal Processing"), *Signal Processing*, and [Chapter
    11](ch11.html "Chapter 11. Image and Audio Processing"), *Image and Audio Processing*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到一些特征选择和特征提取方法。特定于信号、图像或声音的方法将在[第10章](ch10.html "第10章. 信号处理")，*信号处理*，和[第11章](ch11.html
    "第11章. 图像和音频处理")，*图像和音频处理*中介绍。
- en: 'Here are a few further references:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些进一步的参考资料：
- en: Feature selection in scikit-learn, documented at [http://scikit-learn.org/stable/modules/feature_selection.html](http://scikit-learn.org/stable/modules/feature_selection.html)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在scikit-learn中的特征选择，文档说明请参见[http://scikit-learn.org/stable/modules/feature_selection.html](http://scikit-learn.org/stable/modules/feature_selection.html)
- en: Feature selection on Wikipedia at [http://en.wikipedia.org/wiki/Feature_selection](http://en.wikipedia.org/wiki/Feature_selection)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的特征选择 [http://en.wikipedia.org/wiki/Feature_selection](http://en.wikipedia.org/wiki/Feature_selection)
- en: Overfitting, underfitting, and the bias-variance tradeoff
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合、欠拟合与偏差-方差权衡
- en: A central notion in machine learning is the trade-off between **overfitting**
    and **underfitting**. A model may be able to represent our data accurately. However,
    if it is *too* accurate, it might not generalize well to unobserved data. For
    example, in facial recognition, a too-accurate model would be unable to identify
    someone who styled their hair differently that day. The reason is that our model
    might learn irrelevant features in the training data. On the contrary, an insufficiently
    trained model would not generalize well either. For example, it would be unable
    to correctly recognize twins. For more information on overfitting, refer to [http://en.wikipedia.org/wiki/Overfitting](http://en.wikipedia.org/wiki/Overfitting).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的一个核心概念是**过拟合**与**欠拟合**之间的权衡。一个模型可能能够准确地表示我们的数据。然而，如果它*过于*准确，它可能无法很好地推广到未见过的数据。例如，在面部识别中，过于精确的模型可能无法识别当天发型不同的人。原因是我们的模型可能会在训练数据中学习到无关的特征。相反，一个训练不足的模型也无法很好地推广。例如，它可能无法正确识别双胞胎。有关过拟合的更多信息，请参考
    [http://en.wikipedia.org/wiki/Overfitting](http://en.wikipedia.org/wiki/Overfitting)。
- en: A popular solution to reduce overfitting consists of adding *structure* to the
    model, for example, with **regularization**. This method favors simpler models
    during training (Occam's razor). You will find more information at [http://en.wikipedia.org/wiki/Regularization_%28mathematics%29](http://en.wikipedia.org/wiki/Regularization_%28mathematics%29).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个减少过拟合的常见方法是向模型中添加*结构*，例如，通过**正则化**。这种方法在训练过程中倾向于选择更简单的模型（奥卡姆剃刀原则）。你可以在 [http://en.wikipedia.org/wiki/Regularization_%28mathematics%29](http://en.wikipedia.org/wiki/Regularization_%28mathematics%29)
    找到更多信息。
- en: The **bias-variance dilemma** is closely related to the issue of overfitting
    and underfitting. The **bias** of a model quantifies how precise it is across
    training sets. The **variance** quantifies how sensitive the model is to small
    changes in the training set. A **robust** model is not overly sensitive to small
    changes. The dilemma involves minimizing both bias and variance; we want a precise
    and robust model. Simpler models tend to be less accurate but more robust. Complex
    models tend to be more accurate but less robust. For more information on the bias-variance
    dilemma, refer to [http://en.wikipedia.org/wiki/Bias-variance_dilemma](http://en.wikipedia.org/wiki/Bias-variance_dilemma).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差-方差困境**与过拟合和欠拟合问题密切相关。一个模型的**偏差**量化了它在多个训练集上的准确性。**方差**量化了模型对训练集小变化的敏感性。一个**鲁棒**的模型不会对小的变化过于敏感。这个困境涉及到同时最小化偏差和方差；我们希望模型既精确又鲁棒。简单的模型通常不太准确，但更鲁棒；复杂的模型往往更准确，但鲁棒性差。有关偏差-方差困境的更多信息，请参考
    [http://en.wikipedia.org/wiki/Bias-variance_dilemma](http://en.wikipedia.org/wiki/Bias-variance_dilemma)。'
- en: The importance of this trade-off cannot be overstated. This question pervades
    the entire discipline of machine learning. We will see concrete examples in this
    chapter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个权衡的重要性无法被过分强调。这个问题贯穿了整个机器学习领域。我们将在本章中看到具体的例子。
- en: Model selection
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型选择
- en: As we will see in this chapter, there are many supervised and unsupervised algorithms.
    For example, well-known classifiers that we will cover in this chapter include
    logistic regression, nearest-neighbors, Naive Bayes, and support vector machines.
    There are many other algorithms that we can't cover here.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中所看到的，存在许多监督学习和无监督学习的算法。例如，本章将讨论一些著名的分类器，包括逻辑回归、最近邻、朴素贝叶斯和支持向量机。还有许多其他算法我们无法在这里讨论。
- en: No model performs uniformly better than the others. One model may perform well
    on one dataset and badly on another. This is the question of **model selection**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何模型能在所有情况下都表现得比其他模型更好。一个模型可能在某个数据集上表现良好，而在另一个数据集上表现差。这就是**模型选择**的问题。
- en: We will see systematic methods to assess the quality of a model on a particular
    dataset (notably cross-validation). In practice, machine learning is not an "exact
    science" in that it frequently involves trial and error. We need to try different
    models and empirically choose the one that performs best.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到一些系统化的方法来评估模型在特定数据集上的质量（特别是交叉验证）。实际上，机器学习并不是一门“精确的科学”，因为它通常涉及试错过程。我们需要尝试不同的模型，并通过经验选择表现最好的那个。
- en: That being said, understanding the details of the learning models allows us
    to gain intuition about which model is best adapted to our current problem.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，理解学习模型的细节使我们能够直观地了解哪个模型最适合当前问题。
- en: 'Here are a few references on this question:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于这个问题的参考资料：
- en: Model selection on Wikipedia, available at [http://en.wikipedia.org/wiki/Model_selection](http://en.wikipedia.org/wiki/Model_selection)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科上的模型选择，详见 [http://en.wikipedia.org/wiki/Model_selection](http://en.wikipedia.org/wiki/Model_selection)
- en: Model evaluation in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 文档中的模型评估，详见 [http://scikit-learn.org/stable/modules/model_evaluation.html](http://scikit-learn.org/stable/modules/model_evaluation.html)
- en: Blog post on how to choose a classifier, available at [http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于如何选择分类器的博客文章，详见 [http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/](http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/)
- en: Machine learning references
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习参考资料
- en: 'Here are a few excellent, math-heavy textbooks on machine learning:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些优秀的、数学内容较多的机器学习教科书：
- en: '*Pattern Recognition and Machine Learning*, *Christopher M. Bishop*, *(2006)*,
    *Springer*'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模式识别与机器学习*，*Christopher M. Bishop*，*(2006)*，*Springer*'
- en: '*Machine Learning – A Probabilistic Perspective*, *Kevin P. Murphy*, *(2012)*,
    *MIT Press*'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习——一种概率视角*，*Kevin P. Murphy*，*(2012)*，*MIT Press*'
- en: '*The Elements of Statistical Learning*, *Trevor Hastie*, *Robert Tibshirani*,
    *Jerome Friedman*, *(2009)*, *Springer*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*统计学习的元素*，*Trevor Hastie*，*Robert Tibshirani*，*Jerome Friedman*，*(2009)*，*Springer*'
- en: 'Here are a few books more oriented toward programmers without a strong mathematical
    background:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些更适合没有强数学背景的程序员的书籍：
- en: '*Machine Learning for Hackers*, *Drew Conway*, *John Myles White*, *(2012)*,
    *O''Reilly Media*'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*黑客的机器学习*，*Drew Conway*，*John Myles White*，*(2012)*，*O''Reilly Media*'
- en: '*Machine Learning in Action*, *Peter Harrington*, (2012), *Manning Publications
    Co.*'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习实战*，*Peter Harrington*，(2012)，*Manning Publications Co.*'
- en: You will find many other references online.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在线找到更多的参考资料。
- en: Important classes of machine learning methods that we couldn't cover in this
    chapter include neural networks and deep learning. Deep learning is the subject
    of very active research in machine learning. Many state-of-the-art results are
    currently achieved by using deep learning methods. For more information on deep
    learning, refer to [http://en.wikipedia.org/wiki/Deep_learning](http://en.wikipedia.org/wiki/Deep_learning).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们未能涵盖的机器学习方法的重要类别包括神经网络和深度学习。深度学习是机器学习中一个非常活跃的研究领域。许多最先进的成果目前都是通过使用深度学习方法实现的。有关深度学习的更多信息，请参见
    [http://en.wikipedia.org/wiki/Deep_learning](http://en.wikipedia.org/wiki/Deep_learning)。
- en: Getting started with scikit-learn
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 scikit-learn
- en: In this recipe, we introduce the basics of the machine learning **scikit-learn**
    package ([http://scikit-learn.org](http://scikit-learn.org)). This package is
    the main tool we will use throughout this chapter. Its clean API makes it really
    easy to define, train, and test models. Plus, scikit-learn is specifically designed
    for speed and (relatively) big data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们介绍了机器学习 **scikit-learn** 包的基础知识 ([http://scikit-learn.org](http://scikit-learn.org))。这个包是我们在本章中将要使用的主要工具。它简洁的
    API 使得定义、训练和测试模型变得非常容易。而且，scikit-learn 专为速度和（相对）大数据而设计。
- en: We will show here a very basic example of linear regression in the context of
    curve fitting. This toy example will allow us to illustrate key concepts such
    as linear models, overfitting, underfitting, regularization, and cross-validation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里展示一个非常基础的线性回归示例，应用于曲线拟合的背景下。这个玩具示例将帮助我们说明关键概念，如线性模型、过拟合、欠拟合、正则化和交叉验证。
- en: Getting ready
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You can find all instructions to install scikit-learn in the main documentation.
    For more information, refer to [http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html).
    With anaconda, you can type `conda install scikit-learn` in a terminal.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在主文档中找到安装 scikit-learn 的所有指令。更多信息，请参考 [http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html)。使用
    anaconda 时，你可以在终端中输入 `conda install scikit-learn`。
- en: How to do it...
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We will generate a one-dimensional dataset with a simple model (including some
    noise), and we will try to fit a function to this data. With this function, we
    can predict values on new data points. This is a **curve fitting regression**
    problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s make all the necessary imports:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We now define a deterministic nonlinear function underlying our generative
    model:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We generate the values along the curve on *[0,2]*:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s generate data points within *[0,1]*. We use the function *f* and
    we add some Gaussian noise:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s plot our data points on *[0,1]*:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![How to do it...](img/4818OS_08_02.jpg)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: In the image, the dotted curve represents the generative model.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we use scikit-learn to fit a linear model to the data. There are three
    steps. First, we create the model (an instance of the `LinearRegression` class).
    Then, we fit the model to our data. Finally, we predict values from our trained
    model.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to convert `x` and `x_tr` to column vectors, as it is a general convention
    in scikit-learn that observations are rows, while features are columns. Here,
    we have seven observations with one feature.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now plot the result of the trained linear model. We obtain a regression
    line in green here:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![How to do it...](img/4818OS_08_03.jpg)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The linear fit is not well-adapted here, as the data points are generated according
    to a nonlinear model (an exponential curve). Therefore, we are now going to fit
    a nonlinear model. More precisely, we will fit a polynomial function to our data
    points. We can still use linear regression for this, by precomputing the exponents
    of our data points. This is done by generating a **Vandermonde matrix**, using
    the `np.vander` function. We will explain this trick in *How it works…*. In the
    following code, we perform and plot the fit:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![How to do it...](img/4818OS_08_04.jpg)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: We have fitted two polynomial models of degree 2 and 5\. The degree 2 polynomial
    appears to fit the data points less precisely than the degree 5 polynomial. However,
    it seems more robust; the degree 5 polynomial seems really bad at predicting values
    outside the data points (look for example at the *x* ![How to do it...](img/4818OS_08_41.jpg)
    * 1* portion). This is what we call overfitting; by using a too-complex model,
    we obtain a better fit on the trained dataset, but a less robust model outside
    this set.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Note the large coefficients of the degree 5 polynomial; this is generally a
    sign of overfitting.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will now use a different learning model called **ridge regression**. It works
    like linear regression except that it prevents the polynomial's coefficients from
    becoming too big. This is what happened in the previous example. By adding a **regularization**
    **term** in the **loss function**, ridge regression imposes some structure on
    the underlying model. We will see more details in the next section.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ridge regression model has a meta-parameter, which represents the weight
    of the regularization term. We could try different values with trial and error
    using the `Ridge` class. However, scikit-learn provides another model called `RidgeCV`,
    which includes a parameter search with **cross-validation**. In practice, this
    means that we don't have to tweak this parameter by hand—scikit-learn does it
    for us. As the models of scikit-learn always follow the fit-predict API, all we
    have to do is replace `lm.LinearRegression()` with `lm.RidgeCV()` in the previous
    code. We will give more details in the next section.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 岭回归模型有一个超参数，表示正则化项的权重。我们可以使用 `Ridge` 类通过反复试验尝试不同的值。然而，scikit-learn 提供了另一个叫做
    `RidgeCV` 的模型，它包括 **交叉验证**的参数搜索。实际上，这意味着我们无需手动调整该参数——scikit-learn 会为我们完成。由于 scikit-learn
    的模型始终遵循 fit-predict API，我们只需在之前的代码中将 `lm.LinearRegression()` 替换为 `lm.RidgeCV()`。我们将在下一节中提供更多细节。
- en: '[PRE8]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![How to do it...](img/4818OS_08_05.jpg)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何实现...](img/4818OS_08_05.jpg)'
- en: This time, the degree 5 polynomial seems more precise than the simpler degree
    2 polynomial (which now causes **underfitting**). Ridge regression mitigates the
    overfitting issue here. Observe how the degree 5 polynomial's coefficients are
    much smaller than in the previous example.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这次，5 次多项式似乎比简单的 2 次多项式更加精确（后者现在导致 **欠拟合**）。岭回归在此处缓解了过拟合问题。观察 5 次多项式的系数比前一个例子中要小得多。
- en: How it works...
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this section, we explain all the aspects covered in this recipe.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将解释本食谱中涉及的所有方面。
- en: The scikit-learn API
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: scikit-learn API
- en: scikit-learn implements a clean and coherent API for supervised and unsupervised
    learning. Our data points should be stored in a *(N,D)* matrix *X*, where *N*
    is the number of observations and *D* is the number of features. In other words,
    each row is an observation. The first step in a machine learning task is to define
    what the matrix *X* is exactly.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 为监督学习和无监督学习实现了一个简洁且一致的 API。我们的数据点应该存储在一个 *(N,D)* 矩阵 *X* 中，其中 *N*
    是观测值的数量，*D* 是特征的数量。换句话说，每一行都是一个观测值。机器学习任务的第一步是明确矩阵 *X* 的确切含义。
- en: In a supervised learning setup, we also have a *target*, an *N*-long vector
    *y* with a scalar value for each observation. This value is either continuous
    or discrete, depending on whether we have a regression or classification problem,
    respectively.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们还需要一个 *目标*，一个长度为 *N* 的向量 *y*，每个观测值对应一个标量值。这个值是连续的或离散的，具体取决于我们是回归问题还是分类问题。
- en: In scikit-learn, models are implemented in classes that have the `fit()` and
    `predict()` methods. The `fit()` method accepts the data matrix *X* as input,
    and *y* as well for supervised learning models. This method *trains* the model
    on the given data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，模型通过包含 `fit()` 和 `predict()` 方法的类来实现。`fit()` 方法接受数据矩阵 *X* 作为输入，对于监督学习模型，还接受
    *y*。此方法用于在给定数据上*训练*模型。
- en: The `predict()` method also takes data points as input (as a *(M,D)* matrix).
    It returns the labels or transformed points as predicted by the trained model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()` 方法也接受数据点作为输入（作为 *(M,D)* 矩阵）。它返回训练模型预测的标签或转换后的点。'
- en: Ordinary least squares regression
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 普通最小二乘回归
- en: '**Ordinary least squares regression** is one of the simplest regression methods.
    It consists of approaching the output values *y[i]* with a linear combination
    of *X[ij]*:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**普通最小二乘回归** 是最简单的回归方法之一。它通过 *X[ij]* 的线性组合来逼近输出值 *y[i]*：'
- en: '![Ordinary least squares regression](img/4818OS_08_06.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![普通最小二乘回归](img/4818OS_08_06.jpg)'
- en: 'Here, *w = (w[1], ..., w[D])* is the (unknown) **parameter vector**. Also,
    ![Ordinary least squares regression](img/4818OS_08_33.jpg) represents the model''s
    output. We want this vector to match the data points *y* as closely as possible.
    Of course, the exact equality ![Ordinary least squares regression](img/4818OS_08_34.jpg)
    cannot hold in general (there is always some noise and uncertainty—models are
    always idealizations of reality). Therefore, we want to *minimize* the difference
    between these two vectors. The ordinary least squares regression method consists
    of minimizing the following **loss function**:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w = (w[1], ..., w[D])* 是（未知的）**参数向量**。另外，![普通最小二乘回归](img/4818OS_08_33.jpg)
    代表模型的输出。我们希望这个向量与数据点 *y* 尽可能匹配。当然，精确的相等式 ![普通最小二乘回归](img/4818OS_08_34.jpg) 通常是不可能成立的（总会有一些噪声和不确定性——模型始终是对现实的理想化）。因此，我们希望*最小化*这两个向量之间的差异。普通最小二乘回归方法的核心是最小化以下
    **损失函数**：
- en: '![Ordinary least squares regression](img/4818OS_08_07.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![普通最小二乘回归](img/4818OS_08_07.jpg)'
- en: This sum of the components squared is called the **L²** **norm**. It is convenient
    because it leads to *differentiable* loss functions so that gradients can be computed
    and common optimization procedures can be performed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件的平方和称为**L²** **范数**。它之所以方便，是因为它导致了*可微分*的损失函数，从而可以计算梯度，并进行常见的优化过程。
- en: Polynomial interpolation with linear regression
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用线性回归的多项式插值
- en: Ordinary least squares regression fits a linear model to the data. The model
    is linear both in the data points *x[i]* and in the parameters *w[j]*. In our
    example, we obtain a poor fit because the data points were generated according
    to a nonlinear generative model (an exponential function).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 普通最小二乘回归将线性模型拟合到数据上。该模型在数据点*x[i]*和参数*w[j]*中都是线性的。在我们的例子中，由于数据点是根据非线性生成模型（一个指数函数）生成的，因此我们获得了较差的拟合。
- en: 'However, we can still use the linear regression method with a model that is
    linear in *w[j]* but nonlinear in *x[i]*. To do this, we need to increase the
    number of dimensions in our dataset by using a basis of polynomial functions.
    In other words, we consider the following data points:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然可以使用线性回归方法，模型在*w[j]*上是线性的，但在*x[i]*上是非线性的。为此，我们需要通过使用多项式函数的基来增加数据集的维度。换句话说，我们考虑以下数据点：
- en: '![Polynomial interpolation with linear regression](img/4818OS_08_08.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![使用线性回归的多项式插值](img/4818OS_08_08.jpg)'
- en: Here, *D* is the maximum degree. The input matrix *X* is therefore the **Vandermonde
    matrix** associated to the original data points *x[i]*. For more information on
    the Vandermonde matrix, refer to [http://en.wikipedia.org/wiki/Vandermonde_matrix](http://en.wikipedia.org/wiki/Vandermonde_matrix).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*D*是最大阶数。因此，输入矩阵*X*是与原始数据点*x[i]*相关的**范德蒙德矩阵**。有关范德蒙德矩阵的更多信息，请参见[http://en.wikipedia.org/wiki/Vandermonde_matrix](http://en.wikipedia.org/wiki/Vandermonde_matrix)。
- en: Here, it is easy to see that training a linear model on these new data points
    is equivalent to training a polynomial model on the original data points.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，很容易看出，在这些新数据点上训练线性模型等同于在原始数据点上训练多项式模型。
- en: Ridge regression
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 岭回归
- en: Polynomial interpolation with linear regression can lead to overfitting if the
    degree of the polynomials is too large. By capturing the random fluctuations (noise)
    instead of the general trend of the data, the model loses some of its predictive
    power. This corresponds to a divergence of the polynomial's coefficients *w[j]*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性回归的多项式插值如果多项式的阶数过大，可能导致过拟合。通过捕捉随机波动（噪声）而不是数据的一般趋势，模型失去了部分预测能力。这对应于多项式系数*w[j]*的发散。
- en: A solution to this problem is to prevent these coefficients from growing unboundedly.
    With **ridge regression** (also known as **Tikhonov regularization**), this is
    done by adding a *regularization* term to the loss function. For more details
    on Tikhonov regularization, refer to [http://en.wikipedia.org/wiki/Tikhonov_regularization](http://en.wikipedia.org/wiki/Tikhonov_regularization).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是防止这些系数无限增大。通过**岭回归**（也称为**Tikhonov正则化**），这是通过向损失函数中添加*正则化*项来实现的。有关Tikhonov正则化的更多信息，请参见[http://en.wikipedia.org/wiki/Tikhonov_regularization](http://en.wikipedia.org/wiki/Tikhonov_regularization)。
- en: '![Ridge regression](img/4818OS_08_09.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![岭回归](img/4818OS_08_09.jpg)'
- en: By minimizing this loss function, we not only minimize the error between the
    model and the data (first term, related to the bias), but also the size of the
    model's coefficients (second term, related to the variance). The bias-variance
    trade-off is quantified by the hyperparameter ![Ridge regression](img/4818OS_08_43.jpg),
    which specifies the relative weight between the two terms in the loss function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化这个损失函数，我们不仅最小化了模型与数据之间的误差（第一项，与偏差相关），还最小化了模型系数的大小（第二项，与方差相关）。偏差-方差权衡通过超参数![岭回归](img/4818OS_08_43.jpg)量化，它指定了损失函数中两项之间的相对权重。
- en: Here, ridge regression led to a polynomial with smaller coefficients, and thus
    a better fit.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，岭回归导致了具有较小系数的多项式，因此得到了更好的拟合。
- en: Cross-validation and grid search
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉验证和网格搜索
- en: A drawback of the ridge regression model compared to the ordinary least squares
    model is the presence of an extra hyperparameter ![Cross-validation and grid search](img/4818OS_08_43.jpg).
    The quality of the prediction depends on the choice of this parameter. One possibility
    would be to fine-tune this parameter manually, but this procedure can be tedious
    and can also lead to overfitting problems.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, we can use a **grid search**; we loop over many possible
    values for ![Cross-validation and grid search](img/4818OS_08_43.jpg), and we evaluate
    the performance of the model for each possible value. Then, we choose the parameter
    that yields the best performance.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: How can we assess the performance of a model with a given ![Cross-validation
    and grid search](img/4818OS_08_43.jpg) value? A common solution is to use **cross-validation**.
    This procedure consists of splitting the dataset into a training set and a test
    set. We fit the model on the train set, and we test its predictive performance
    on the *test set*. By testing the model on a different dataset than the one used
    for training, we reduce overfitting.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to split the initial dataset into two parts like this. One
    possibility is to remove *one* sample to form the train set and to put this one
    sample into the test set. This is called **Leave-One-Out** cross-validation. With
    *N* samples, we obtain *N* sets of train and test sets. The cross-validated performance
    is the average performance on all these set decompositions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: As we will see later, scikit-learn implements several easy-to-use functions
    to do cross-validation and grid search. In this recipe, there exists a special
    estimator called `RidgeCV` that implements a cross-validation and grid search
    procedure that is specific to the ridge regression model. Using this class ensures
    that the best hyperparameter ![Cross-validation and grid search](img/4818OS_08_43.jpg)
    is found automatically for us.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references about least squares:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Ordinary least squares on Wikipedia, available at [http://en.wikipedia.org/wiki/Ordinary_least_squares](http://en.wikipedia.org/wiki/Ordinary_least_squares)
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear least squares on Wikipedia, available at [http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)](http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics))
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few references about cross-validation and grid search:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid search in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/grid_search.html](http://scikit-learn.org/stable/modules/grid_search.html)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation on Wikipedia, available at [http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29)
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few references about scikit-learn:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn basic tutorial available at [http://scikit-learn.org/stable/tutorial/basic/tutorial.html](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn tutorial given at the SciPy 2013 conference, available at [https://github.com/jakevdp/sklearn_scipy2013](https://github.com/jakevdp/sklearn_scipy2013)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Using support vector machines for classification tasks* recipe
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting who will survive on the Titanic with logistic regression
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will introduce **logistic regression**, a basic classifier.
    We will also show how to perform a **grid search** with **cross-validation**.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: We will apply these techniques on a **Kaggle** dataset where the goal is to
    predict survival on the Titanic based on real data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kaggle ([www.kaggle.com/competitions](http://www.kaggle.com/competitions)) hosts
    machine learning competitions where anyone can download a dataset, train a model,
    and test the predictions on the website. The author of the best model might even
    win a prize! It is a fun way to get started with machine learning.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the *Titanic* dataset from the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has been obtained from [www.kaggle.com/c/titanic-gettingStarted](http://www.kaggle.com/c/titanic-gettingStarted).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import the standard packages:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We load the training and test datasets with pandas:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s keep only a few fields for this example, and also convert the `sex`
    field to a binary variable so that it can be handled correctly by NumPy and scikit-learn.
    Finally, we remove the rows that contain `NaN` values:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we convert this `DataFrame` object to a NumPy array so that we can pass
    it to scikit-learn:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s have a look at the survival of male and female passengers as a function
    of their age:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![How to do it...](img/4818OS_08_10.jpg)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Let''s try to train a `LogisticRegression` classifier in order to predict the
    survival of people based on their gender, age, and class. We first need to create
    a train and a test dataset:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We train the model and we get the predicted values on the test set:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following figure shows the actual and predicted results:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![How to do it...](img/4818OS_08_11.jpg)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: In this screenshot, the first line shows the survival of several people from
    the test set (white for survival, black otherwise). The second line shows the
    values predicted by the model.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To get an estimation of the model''s performance, we compute the cross-validation
    score with the `cross_val_score()` function. This function uses a three-fold stratified
    cross-validation procedure by default, but this can be changed with the `cv` keyword
    argument:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This function returns, for each pair of train and test set, a prediction score
    (we give more details in *How it works…*).
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `LogisticRegression` class accepts a *C* hyperparameter as an argument.
    This parameter quantifies the regularization strength. To find a good value, we
    can perform a grid search with the generic `GridSearchCV` class. It takes an estimator
    as input and a dictionary of parameter values. This new estimator uses cross-validation
    to select the best parameter:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is the performance of the best estimator:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Performance is slightly better after the *C* hyperparameter has been chosen
    with a grid search.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is *not* a regression model, it is a classification model.
    Yet, it is closely related to linear regression. This model predicts the probability
    that a binary variable is 1, by applying a **sigmoid function** (more precisely,
    a logistic function) to a linear combination of the variables. The equation of
    the sigmoid is:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_12.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'The following figure shows a logistic function:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_13.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: A logistic function
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: If a binary variable has to be obtained, we can round the value to the closest
    integer.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The parameter *w* is obtained with an optimization procedure during the learning
    step.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression on Wikipedia, available at [http://en.wikipedia.org/wiki/Logistic_regression](http://en.wikipedia.org/wiki/Logistic_regression)
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Getting started with scikit-learn* recipe
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Learning to recognize handwritten digits with a K-nearest neighbors classifier*
    recipe
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using support vector machines for classification tasks* recipe
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to recognize handwritten digits with a K-nearest neighbors classifier
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to recognize handwritten digits with a **K-nearest
    neighbors** (**K-NN**) classifier. This classifier is a simple but powerful model,
    well-adapted to complex, highly nonlinear datasets such as images. We will explain
    how it works later in this recipe.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import the modules:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s load the *digits* dataset, part of the `datasets` module of scikit-learn.
    This dataset contains handwritten digits that have been manually labeled:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the matrix `X`, each row contains *8 * 8=64* pixels (in grayscale, values
    between 0 and 16). The row-major ordering is used.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s display some of the images along with their labels:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![How to do it...](img/4818OS_08_14.jpg)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now, let''s fit a K-nearest neighbors classifier on the data:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s evaluate the score of the trained classifier on the test dataset:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now, let's see if our classifier can recognize a *handwritten* digit!
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![How to do it...](img/4818OS_08_15.jpg)'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Can our model recognize this number? Let''s see:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Good job!
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This example illustrates how to deal with images in scikit-learn. An image is
    a 2D *(N, M)* matrix, which has *NM* features. This matrix needs to be flattened
    when composing the data matrix; each row is a full image.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of K-nearest neighbors is as follows: given a new point in the feature
    space, find the *K* closest points from the training set and assign the label
    of the majority of those points.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The distance is generally the Euclidean distance, but other distances can be
    used too.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows the space partition obtained with a 15-nearest-neighbors
    classifier on a toy dataset (with three labels):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_16.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: K-nearest neighbors space partition
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: The number *K* is a hyperparameter of the model. If it is too small, the model
    will not generalize well (high variance). In particular, it will be highly sensitive
    to outliers. By contrast, the precision of the model will worsen if *K* is too
    large. At the extreme, if *K* is equal to the total number of points, the model
    will always predict the exact same value disregarding the input (high bias). There
    are heuristics to choose this hyperparameter (see the next section).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that no model is learned by a K-nearest neighbor algorithm;
    the classifier just stores all data points and compares any new target points
    with them. This is an example of **instance-based learning**. It is in contrast
    to other classifiers such as the logistic regression model, which explicitly learns
    a simple mathematical model on the training data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: The K-nearest neighbors method works well on complex classification problems
    that have irregular decision boundaries. However, it might be computationally
    intensive with large training datasets because a large number of distances have
    to be computed for testing. Dedicated tree-based data structures such as **K-D
    trees** or **ball trees** can be used to accelerate the search of nearest neighbors.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The K-nearest neighbors method can be used for classification, like here, and
    also for regression problems. The model assigns the average of the target value
    of the nearest neighbors. In both cases, different weighting strategies can be
    used.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The K-NN algorithm in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/neighbors.html](http://scikit-learn.org/stable/modules/neighbors.html)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The K-NN algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm](http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blog post about how to choose the K hyperparameter, available at [http://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/](http://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instance-based learning on Wikipedia, available at [http://en.wikipedia.org/wiki/Instance-based_learning](http://en.wikipedia.org/wiki/Instance-based_learning)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Predicting who will survive on the Titanic with logistic regression* recipe
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using support vector machines for classification tasks* recipe
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning from text – Naive Bayes for Natural Language Processing
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we show how to handle text data with scikit-learn. Working with
    text requires careful preprocessing and feature extraction. It is also quite common
    to deal with highly sparse matrices.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: We will learn to recognize whether a comment posted during a public discussion
    is considered insulting to one of the participants. We will use a labeled dataset
    from Impermium, released during a Kaggle competition.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the *Troll* dataset from the book's GitHub repository at [https://github.com/ipython-books/cookbook-data](https://github.com/ipython-books/cookbook-data).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: This dataset was obtained from Kaggle, at [www.kaggle.com/c/detecting-insults-in-social-commentary](http://www.kaggle.com/c/detecting-insults-in-social-commentary).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import our libraries:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s open the CSV file with pandas:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Each row is a comment. We will consider two columns: whether the comment is
    insulting (1) or not (0) and the unicode-encoded contents of the comment:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we are going to define the feature matrix `X` and the labels `y`:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Obtaining the feature matrix from the text is not trivial. scikit-learn can
    only work with numerical matrices. So how do we convert text into a matrix of
    numbers? A classical solution is to first extract a **vocabulary**, a list of
    words used throughout the corpus. Then, we count, for each sample, the frequency
    of each word. We end up with a **sparse matrix**, a huge matrix containing mostly
    zeros. Here, we do this in two lines. We will give more details in *How it works…*.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The general rule here is that whenever one of our features is categorical (that
    is, the presence of a word, a color belonging to a fixed set of *n* colors, and
    so on), we should *vectorize* it by considering one binary feature per item in
    the class. For example, instead of a feature `color` being `red`, `green`, or
    `blue`, we should consider three *binary* features `color_red`, `color_green`,
    and `color_blue`. We give further references in the *There's more…* section.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'There are 3947 comments and 16469 different words. Let''s estimate the sparsity
    of this feature matrix:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we are going to train a classifier as usual. We first split the data into
    a train and test set:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We use a **Bernoulli Naive Bayes classifier** with a grid search on the ![How
    to do it...](img/4818OS_08_43.jpg) parameter:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s check the performance of this classifier on the test dataset:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s take a look at the words corresponding to the largest coefficients (the
    words we find frequently in insulting comments):'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, let''s test our estimator on a few test sentences:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: That's not bad, but we can probably do better.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: scikit-learn implements several utility functions to obtain a sparse feature
    matrix from text data. A **vectorizer** such as `CountVectorizer()` extracts a
    vocabulary from a corpus (`fit`) and constructs a sparse representation of the
    corpus based on this vocabulary (`transform`). Each sample is represented by the
    vocabulary's word frequencies. The trained instance also contains attributes and
    methods to map feature indices to the corresponding words (`get_feature_names()`)
    and conversely (`vocabulary_`).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: N-grams can also be extracted. These are pairs or tuples of words occurring
    successively (the `ngram_range` keyword).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The frequency of the words can be weighted in different ways. Here, we have
    used **tf-idf**, or **term frequency-inverse document frequency**. This quantity
    reflects how important a word is to a corpus. Frequent words in comments have
    a high weight except if they appear in most comments (which means that they are
    common terms, for example, "the" and "and" would be filtered out using this technique).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes algorithms are Bayesian methods based on the naive assumption of
    independence between the features. This strong assumption drastically simplifies
    the computations and leads to very fast yet decent classifiers.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Text feature extraction in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequency-inverse document-frequency on Wikipedia, available at [http://en.wikipedia.org/wiki/tf-idf](http://en.wikipedia.org/wiki/tf-idf)
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorizer in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes classifier on Wikipedia, at [http://en.wikipedia.org/wiki/Naive_Bayes_classifier](http://en.wikipedia.org/wiki/Naive_Bayes_classifier)
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/naive_bayes.html](http://scikit-learn.org/stable/modules/naive_bayes.html)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impermium Kaggle challenge, at [http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/](http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/)
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document classification example in scikit-learn's documentation, at [http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html](http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html)
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides scikit-learn, which has good support for text processing, we should
    also mention NLTK (available at [www.nltk.org](http://www.nltk.org)), a Natural
    Language Toolkit in Python.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Predicting who will survive on the Titanic with logistic regression* recipe
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Learning to recognize handwritten digits with a K-nearest neighbors classifier*
    recipe
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using support vector machines for classification tasks* recipe
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using support vector machines for classification tasks
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we introduce **support vector machines**, or **SVMs**. These
    powerful models can be used for classification and regression. Here, we illustrate
    how to use linear and nonlinear SVMs on a simple classification task.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import the packages:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We generate 2D points and assign a binary label according to a linear operation
    on the coordinates:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We now fit a linear **Support Vector Classifier** (**SVC**). This classifier
    tries to separate the two groups of points with a linear boundary (a line here,
    but more generally a hyperplane):'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We define a function that displays the boundaries and decision function of
    a trained classifier:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let''s take a look at the classification results with the linear SVC:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![How to do it...](img/4818OS_08_17.jpg)'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The linear SVC tried to separate the points with a line and it did a pretty
    good job here.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now modify the labels with an `XOR` function. A point''s label is 1 if the
    coordinates have different signs. This classification is not linearly separable.
    Therefore, a linear SVC fails completely:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![How to do it...](img/4818OS_08_18.jpg)'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Fortunately, it is possible to use nonlinear SVCs by using nonlinear **kernels**.
    Kernels specify a nonlinear transformation of the points into a higher dimensional
    space. Transformed points in this space are assumed to be more linearly separable.
    By default, the `SVC` classifier in scikit-learn uses the **Radial Basis Function**
    (**RBF**) kernel:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![How to do it...](img/4818OS_08_19.jpg)'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: This time, the nonlinear SVC successfully managed to classify these nonlinearly
    separable points.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A two-class linear SVC tries to find a hyperplane (defined as a linear equation)
    that best separates the two sets of points (grouped according to their labels).
    There is also the constraint that this separating hyperplane needs to be as far
    as possible from the points. This method works best when such a hyperplane exists.
    Otherwise, this method can fail completely, as we saw in the `XOR` example. `XOR`
    is known as being a nonlinearly separable operation.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: The SVM classes in scikit-learn have a *C* hyperparameter. This hyperparameter
    trades off misclassification of training examples against simplicity of the decision
    surface. A low *C* value makes the decision surface smooth, while a high *C* value
    aims at classifying all training examples correctly. This is another example where
    a hyperparameter quantifies the bias-variance trade-off. This hyperparameter can
    be chosen with cross-validation and grid search.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: The linear SVC can also be extended to multiclass problems. The multiclass SVC
    is directly implemented in scikit-learn.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: The nonlinear SVC works by considering a nonlinear transformation ![How it works...](img/4818OS_08_35.jpg)
    from the original space into a higher dimensional space. This nonlinear transformation
    can increase the linear separability of the classes. In practice, all dot products
    are replaced by the ![How it works...](img/4818OS_08_36.jpg) kernel.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_20.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: Nonlinear SVC
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several widely-used nonlinear kernels. By default, SVC uses Gaussian
    radial basis functions:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_21.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: Here, ![How it works...](img/4818OS_08_37.jpg) is a hyperparameter of the model
    that can be chosen with grid search and cross-validation.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: The ![How it works...](img/4818OS_08_42.jpg) function does not need to be computed
    explicitly. This is the **kernel trick**; it suffices to know the kernel *k(x,
    x')*. The existence of a function ![How it works...](img/4818OS_08_42.jpg) corresponding
    to a given kernel *k(x, x')* is guaranteed by a mathematical theorem in functional
    analysis.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references about support vector machines:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Exclusive OR on Wikipedia, available at [http://en.wikipedia.org/wiki/Exclusive_or](http://en.wikipedia.org/wiki/Exclusive_or)
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines on Wikipedia, available at [http://en.wikipedia.org/wiki/Support_vector_machine](http://en.wikipedia.org/wiki/Support_vector_machine)
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVMs in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/svm.html](http://scikit-learn.org/stable/modules/svm.html)
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel trick on Wikipedia, available at [http://en.wikipedia.org/wiki/Kernel_method](http://en.wikipedia.org/wiki/Kernel_method)
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notes about the kernel trick available at [www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example with a nonlinear SVM available at [http://scikit-learn.org/0.11/auto_examples/svm/plot_svm_nonlinear.html](http://scikit-learn.org/0.11/auto_examples/svm/plot_svm_nonlinear.html)
    (this example inspired this recipe)
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Predicting who will survive on the Titanic with logistic regression* recipe
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Learning to recognize handwritten digits with a K-nearest neighbors classifier*
    recipe
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a random forest to select important features for regression
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Decision trees** are frequently used to represent workflows or algorithms.
    They also form a method for nonparametric supervised learning. A tree mapping
    observations to target values is learned on a training set and gives the outcomes
    of new observations.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '**Random forests** are ensembles of decision trees. Multiple decision trees
    are trained and aggregated to form a model that is more performant than any of
    the individual trees. This general idea is the purpose of **ensemble learning**.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of ensemble methods. Random forests are an instance of
    **bootstrap aggregating**, also called **bagging**, where models are trained on
    randomly drawn subsets of the training set.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Random forests yield information about the importance of each feature for the
    classification or regression task. In this recipe, we will find the most influential
    features of Boston house prices using a classic dataset that contains a range
    of diverse indicators about the houses' neighborhood.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import the packages:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We load the Boston dataset:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The details of this dataset can be found in `data[''DESCR'']`. Here is the
    description of some features:'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*CRIM*: Per capita crime rate by town'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NOX*: Nitric oxide concentration (parts per 10 million)'
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*RM*: Average number of rooms per dwelling'
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AGE*: Proportion of owner-occupied units built prior to 1940'
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DIS*: Weighted distances to five Boston employment centers'
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PTRATIO*: Pupil-teacher ratio by town'
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LSTAT*: Percentage of lower status of the population'
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MEDV*: Median value of owner-occupied homes in $1000s'
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The target value is *MEDV*.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We create a `RandomForestRegressor` model:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We get the samples and the target values from this dataset:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s fit the model:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The importance of our features can be found in `reg.feature_importances_`.
    We sort them by decreasing order of importance:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, we plot a histogram of the features'' importance:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![How to do it...](img/4818OS_08_22.jpg)'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'We find that *LSTAT* (proportion of lower status of the population) and *RM*
    (number of rooms per dwelling) are the most important features determining the
    price of a house. As an illustration, here is a scatter plot of the price as a
    function of *LSTAT*:'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![How to do it...](img/4818OS_08_23.jpg)'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several algorithms can be used to train a decision tree. scikit-learn uses the
    **CART**, or **Classification and Regression Trees**, algorithm. This algorithm
    constructs binary trees using the feature and threshold that yield the largest
    information gain at each node. Terminal nodes give the outcomes of input values.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are simple to understand. They can also be visualized with **pydot**,
    a Python package for drawing graphs and trees. This is useful when we want to
    understand what a tree has learned exactly (**white box model**); the conditions
    that apply on the observations at each node can be expressed easily with Boolean
    logic.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: However, decision trees may suffer from overfitting, notably when they are too
    deep, and they might be unstable. Additionally, global convergence toward an optimal
    model is not guaranteed, particularly when greedy algorithms are used for training.
    These problems can be mitigated by using ensembles of decision trees, notably
    random forests.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: In a random forest, multiple decision trees are trained on bootstrap samples
    of the training dataset (randomly sampled with replacement). Predictions are made
    with the averages of individual trees' predictions (bootstrap aggregating or bagging).
    Additionally, random subsets of the features are chosen at each node (**random
    subspace method**). These methods lead to an overall better model than the individual
    trees.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/ensemble.html](http://scikit-learn.org/stable/modules/ensemble.html)
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API reference of `RandomForestRegressor` available at [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests on Wikipedia, available at [http://en.wikipedia.org/wiki/Random_forest](http://en.wikipedia.org/wiki/Random_forest)
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree learning on Wikipedia, available at [http://en.wikipedia.org/wiki/Decision_tree_learning](http://en.wikipedia.org/wiki/Decision_tree_learning)
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bootstrap aggregating on Wikipedia, available at [http://en.wikipedia.org/wiki/Bootstrap_aggregating](http://en.wikipedia.org/wiki/Bootstrap_aggregating)
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random subspace method on Wikipedia, available at [http://en.wikipedia.org/wiki/Random_subspace_method](http://en.wikipedia.org/wiki/Random_subspace_method)
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble learning on Wikipedia, available at [http://en.wikipedia.org/wiki/Ensemble_learning](http://en.wikipedia.org/wiki/Ensemble_learning)
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Using support vector machines for classification tasks* recipe
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the dimensionality of a dataset with a principal component analysis
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipes, we presented *supervised learning* methods; our data
    points came with discrete or continuous labels, and the algorithms were able to
    learn the mapping from the points to the labels.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Starting with this recipe, we will present **unsupervised learning** methods.
    These methods might be helpful prior to running a supervised learning algorithm.
    They can give a first insight into the data.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that our data consists of points *x[i]* without any labels. The
    goal is to discover some form of hidden structure in this set of points. Frequently,
    data points have intrinsic low dimensionality: a small number of features suffice
    to accurately describe the data. However, these features might be hidden among
    many other features not relevant to the problem. Dimension reduction can help
    us find these structures. This knowledge can considerably improve the performance
    of subsequent supervised learning algorithms.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Another useful application of unsupervised learning is **data visualization**;
    high-dimensional datasets are hard to visualize in 2D or 3D. Projecting the data
    points on a subspace or submanifold yields more interesting visualizations.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will illustrate a basic unsupervised linear method, **principal
    component analysis** (**PCA**). This algorithm lets us project data points linearly
    on a low-dimensional subspace. Along the **principal components**, which are vectors
    forming a basis of this low-dimensional subspace, the variance of the data points
    is maximum.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the classic *Iris flower* dataset as an example. This dataset contains
    the width and length of the petal and sepal of 150 iris flowers. These flowers
    belong to one of three categories: *Iris setosa*, *Iris virginica*, and *Iris
    versicolor*. We have access to the category in this dataset (labeled data). However,
    because we are interested in illustrating an unsupervised learning method, we
    will only use the data matrix *without* the labels.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import NumPy, matplotlib, and scikit-learn:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The *Iris flower* dataset is available in the `datasets` module of scikit-learn:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Each row contains four parameters related to the morphology of the flower.
    Let''s display the first two dimensions. The color reflects the iris variety of
    the flower (the label, between 0 and 2):'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![How to do it...](img/4818OS_08_24.jpg)'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Tip
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: If you're reading the printed version of this book, you might not be able to
    distinguish the colors. You will find the colored images on the book's website.
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now apply PCA on the dataset to get the transformed matrix. This operation
    can be done in a single line with scikit-learn: we instantiate a `PCA` model and
    call the `fit_transform()` method. This function computes the principal components
    and projects the data on them:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We now display the same dataset, but in a new coordinate system (or equivalently,
    a linearly transformed version of the initial dataset):'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![How to do it...](img/4818OS_08_25.jpg)'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Points belonging to the same classes are now grouped together, even though the
    `PCA` estimator did *not* use the labels. The PCA was able to find a projection
    maximizing the variance, which corresponds here to a projection where the classes
    are well separated.
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `scikit.decomposition` module contains several variants of the classic
    `PCA` estimator: `ProbabilisticPCA`, `SparsePCA`, `RandomizedPCA`, `KernelPCA`,
    and others. As an example, let''s take a look at `KernelPCA`, a nonlinear version
    of PCA:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![How to do it...](img/4818OS_08_26.jpg)'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at the mathematical ideas behind PCA. This method is based on a
    matrix decomposition called **Singular Value Decomposition** (**SVD**):'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_27.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
- en: Here, *X* is the *(N,D)* data matrix, *U* and *V* are orthogonal matrices, and
    ![How it works...](img/4818OS_08_38.jpg) is a *(N,D)* diagonal matrix.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA transforms *X* into *X''* defined by:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_28.jpg)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
- en: The diagonal elements of ![How it works...](img/4818OS_08_38.jpg) are the **singular
    values** of *X*. By convention, they are generally sorted in descending order.
    The columns of *U* are orthonormal vectors called the **left singular vectors**
    of *X*. Therefore, the columns of *X'* are the left singular vectors multiplied
    by the singular values.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: In the end, PCA converts the initial set of observations, which are made of
    possibly correlated variables, into vectors of linearly uncorrelated variables
    called **principal components**.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: The first new feature (or first component) is a transformation of all original
    features such that the dispersion (variance) of the data points is the highest
    in that direction. In the subsequent principal components, the variance is decreasing.
    In other words, PCA gives us an alternative representation of our data where the
    new features are sorted according to how much they account for the variability
    of the points.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few further references:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: Iris flower dataset on Wikipedia, available at [http://en.wikipedia.org/wiki/Iris_flower_data_set](http://en.wikipedia.org/wiki/Iris_flower_data_set)
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA on Wikipedia, available at [http://en.wikipedia.org/wiki/Principal_component_analysis](http://en.wikipedia.org/wiki/Principal_component_analysis)
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD decomposition on Wikipedia, available at [http://en.wikipedia.org/wiki/Singular_value_decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition)
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris dataset example available at [http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decompositions in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/decomposition.html](http://scikit-learn.org/stable/modules/decomposition.html)
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning tutorial with scikit-learn available at [http://scikit-learn.org/dev/tutorial/statistical_inference/unsupervised_learning.html](http://scikit-learn.org/dev/tutorial/statistical_inference/unsupervised_learning.html)
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Detecting hidden structures in a dataset with clustering* recipe
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting hidden structures in a dataset with clustering
  id: totrans-442
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A large part of unsupervised learning is devoted to the **clustering** problem.
    The goal is to group similar points together in a totally unsupervised way. Clustering
    is a hard problem, as the very definition of **clusters** (or **groups**) is not
    necessarily well posed. In most datasets, stating that two points should belong
    to the same cluster may be context-dependent or even subjective.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: There are many clustering algorithms. We will see a few of them in this recipe,
    applied to a toy example.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-445
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import the libraries:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let''s generate a random dataset with three clusters:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We need a couple of functions to relabel and display the results of the clustering
    algorithms:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now, we cluster the dataset with the **K-means** algorithm, a classic and simple
    clustering algorithm:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![How to do it...](img/4818OS_08_29.jpg)'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Tip
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: If you're reading the printed version of this book, you might not be able to
    distinguish the colors. You will find the colored images on the book's website.
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This algorithm needs to know the number of clusters at initialization time.
    In general, however, we do not necessarily know the number of clusters in the
    dataset. Here, let''s try with `n_clusters=3` (that''s cheating, because we happen
    to know that there are 3 clusters!):'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '![How to do it...](img/4818OS_08_30.jpg)'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Let''s try a few other clustering algorithms implemented in scikit-learn. The
    simplicity of the API makes it really easy to try different methods; it is just
    a matter of changing the name of the class:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![How to do it...](img/4818OS_08_31.jpg)'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The first two algorithms required the number of clusters as input. The next
    two did not, but they were able to find the right number: 3\. The last one failed
    at finding the correct number of clusters (this is *overclustering*—too many clusters
    have been found).'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The K-means clustering algorithm consists of partitioning the data points *x[j]*
    into *K* clusters *S[i]* so as to minimize the within-cluster sum of squares:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/4818OS_08_32.jpg)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
- en: Here, ![How it works...](img/4818OS_08_39.jpg) is the center of the cluster
    *i* (average of all points in *S[i]*).
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it is very hard to solve this problem exactly, approximation algorithms
    exist. A popular one is **Lloyd''s** **algorithm**. It consists of starting from
    an initial set of *K* means ![How it works...](img/4818OS_08_39.jpg) and alternating
    between two steps:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: In the *assignment* step, the points are assigned to the cluster associated
    to the closest mean
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the *update* step, the means are recomputed from the last assignments
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm converges to a solution that is not guaranteed to be optimal.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: The **expectation-maximization algorithm** can be seen as a probabilistic version
    of the K-means algorithm. It is implemented in the `mixture` module of scikit-learn.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: The other clustering algorithms used in this recipe are explained in the scikit-learn
    documentation. There is no clustering algorithm that works uniformly better than
    all the others, and every algorithm has its strengths and weaknesses. You will
    find more details in the references in the next section.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: The K-means clustering algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering)
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expectation-maximization algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/Expectation-maximization_algorithm](http://en.wikipedia.org/wiki/Expectation-maximization_algorithm)
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering in scikit-learn's documentation, available at [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-479
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Reducing the dimensionality of a dataset with principal component analysis*
    recipe
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
