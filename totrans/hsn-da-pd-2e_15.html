<html><head></head><body>
		<div id="_idContainer478">
			<h1 id="_idParaDest-238"><em class="italic"><a id="_idTextAnchor237"/>Chapter 11</em>: Machine Learning Anomaly Detection</h1>
			<p>For our final application chapter, we will be revisiting <strong class="bold">anomaly detection</strong> on login attempts. Let's imagine we work for a company that launched its web application at the beginning of 2018. This web application has been collecting log events for all login attempts since it launched. We know the IP address that the attempt was made from, the result of the attempt, when it was made, and which username was entered. What we don't know is whether the attempt was made by one of our valid users or a nefarious party.</p>
			<p>Our company has been expanding and, since data breaches seem to be in the news every day, has created an information security department to monitor the traffic. The CEO saw our rule-based approach to identifying hackers from <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>, and was intrigued by our initiative, but wants us to move beyond using rules and thresholds for such a vital task. We have been tasked with developing a machine learning model for anomaly detection of the login attempts on the web application.</p>
			<p>Since this will require a good amount of data, we have been given access to all the logs from January 1, 2018 through December 31, 2018. In addition, the newly formed <strong class="bold">security operations center</strong> (<strong class="bold">SOC</strong>) will be auditing all this traffic now and will indicate which time frames contain nefarious users based on their investigations. Since the SOC members are subject matter experts, this data will be exceptionally valuable to us. We will be able to use the labeled data they provide to build a supervised learning model for future use; however, it will take them some time to sift through all the traffic, so we should get started with some unsupervised learning until they have that ready for us.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Exploring the simulated login attempts data</li>
				<li>Utilizing unsupervised methods of anomaly detection</li>
				<li>Implementing supervised anomaly detection</li>
				<li>Incorporating a feedback loop with online learning</li>
			</ul>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor238"/>Chapter materials</h1>
			<p>The materials for this chapter can be found at <a href="https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_11">https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_11</a>. In this chapter, we will be revisiting attempted login data; however, the <strong class="source-inline">simulate.py</strong> script has been updated to allow additional command-line arguments. We won't be running the simulation this time, but be sure to take a look at the script and check out the process that was followed to generate the data files and create the database for this chapter in the <strong class="source-inline">0-simulating_the_data.ipynb</strong> notebook. The <strong class="source-inline">user_data/</strong> directory contains the files used for this simulation, but we won't be using them directly in this chapter.</p>
			<p>The simulated log data we will be using for this chapter can be found in the <strong class="source-inline">logs/</strong> directory. The <strong class="source-inline">logs_2018.csv</strong> and <strong class="source-inline">hackers_2018.csv</strong> files are logs of login attempts and a record of hacker activity from all 2018 simulations, respectively. Files with the <strong class="source-inline">hackers</strong> prefix are treated as the labeled data we will receive from the SOC, so we will pretend we don't have them initially. The files with <strong class="source-inline">2019</strong> instead of <strong class="source-inline">2018</strong> in the name are the data from simulating the first quarter of 2019, rather than the full year. In addition, the CSV files have been written to the <strong class="source-inline">logs.db</strong> SQLite database. The <strong class="source-inline">logs</strong> table contains the data from <strong class="source-inline">logs_2018.csv</strong> and <strong class="source-inline">logs_2019.csv</strong>; the <strong class="source-inline">attacks</strong> table contains the data from <strong class="source-inline">hackers_2018.csv</strong> and <strong class="source-inline">hackers_2019.csv</strong>.</p>
			<p>The parameters of the simulation vary per month, and in most months, the hackers are varying their IP addresses for each username they attempt to log in with. This will make our method from <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>, useless because we were looking for IP addresses with many attempts and high failure rates. If the hackers now vary their IP addresses, we won't have many attempts associated with them. Therefore, we won't be able to flag them with that strategy, so we will have to find another way around this:</p>
			<div>
				<div id="_idContainer450" class="IMG---Figure">
					<img src="image/Figure_11.1_B16834.jpg" alt="Figure 11.1 – Simulation parameters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Simulation parameters</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The <strong class="source-inline">merge_logs.py</strong> file contains the Python code to merge the logs from each of the individual simulations, and <strong class="source-inline">run_simulations.sh</strong> contains a Bash script for running the entire process. These are provided for completeness, but we don't need to use them (or worry about Bash).</p>
			<p>Our workflow for this chapter has been split across several notebooks, which are all preceded by a number indicating their order. Before we have the labeled data, we will conduct some EDA in the <strong class="source-inline">1-EDA_unlabeled_data.ipynb</strong> notebook, and then move on to the <strong class="source-inline">2-unsupervised_anomaly_detection.ipynb</strong> notebook to try out some unsupervised anomaly detection methods. Once we have the labeled data, we will perform some additional EDA in the <strong class="source-inline">3-EDA_labeled_data.ipynb</strong> notebook, and then move on to the <strong class="source-inline">4-supervised_anomaly_detection.ipynb</strong> notebook for supervised methods. Finally, we will use the <strong class="source-inline">5-online_learning.ipynb</strong> notebook for our discussion of online learning. As usual, the text will indicate when it is time to switch between notebooks.</p>
			<h1 id="_idParaDest-240"><a id="_idTextAnchor239"/>Exploring the simulated login attempts data</h1>
			<p>We don't <a id="_idIndexMarker1504"/>have labeled data yet, but we can still examine the data to see whether there is something that stands out. This data is different from the data in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>. The hackers are smarter in this simulation—they don't always try as many users or stick with the same IP address every time. Let's see whether we can come up with some features that will help with anomaly detection by performing some EDA in the <strong class="source-inline">1-EDA_unlabeled_data.ipynb</strong> notebook.</p>
			<p>As usual, we begin with our imports. These will be the same for all notebooks, so it will be reproduced in this section only:</p>
			<p class="source-code">&gt;&gt;&gt; %matplotlib inline</p>
			<p class="source-code">&gt;&gt;&gt; import matplotlib.pyplot as plt</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; import seaborn as sns</p>
			<p>Next, we <a id="_idIndexMarker1505"/>read in the 2018 logs from the <strong class="source-inline">logs</strong> table in the SQLite database:</p>
			<p class="source-code">&gt;&gt;&gt; import sqlite3</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">with sqlite3.connect('logs/logs.db') as conn:</strong></p>
			<p class="source-code">...     <strong class="bold">logs_2018 = pd.read_sql(</strong></p>
			<p class="source-code">...         <strong class="bold">"""</strong></p>
			<p class="source-code">...         <strong class="bold">SELECT *</strong> </p>
			<p class="source-code">...         <strong class="bold">FROM logs</strong> </p>
			<p class="source-code">...<strong class="bold">         WHERE</strong></p>
			<p class="source-code">...<strong class="bold">             datetime BETWEEN "2018-01-01" AND "2019-01-01";</strong></p>
			<p class="source-code">...<strong class="bold">         """, </strong></p>
			<p class="source-code">...<strong class="bold">         conn, parse_dates=['datetime'],</strong></p>
			<p class="source-code">...<strong class="bold">         index_col='datetime'</strong></p>
			<p class="source-code">...     <strong class="bold">)</strong></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If the SQLAlchemy package (<a href="https://www.sqlalchemy.org/">https://www.sqlalchemy.org/</a>) is installed in the environment <a id="_idIndexMarker1506"/>we are working with (as is the case for us), we have the option of providing the database <strong class="bold">uniform resource identifier</strong> (<strong class="bold">URI</strong>) for the connection when calling <strong class="source-inline">pd.read_sql()</strong>, eliminating the need for the <strong class="source-inline">with</strong> statement. In our case, this would be <strong class="source-inline">sqlite:///logs/logs.db</strong>, where <strong class="source-inline">sqlite</strong> is the dialect and <strong class="source-inline">logs/logs.db</strong> is the path to the file. Note that there are three <strong class="source-inline">/</strong> characters in a row.</p>
			<p>Our data looks like this:</p>
			<div>
				<div id="_idContainer451" class="IMG---Figure">
					<img src="image/Figure_11.2_B16834.jpg" alt="Figure 11.2 – Login attempt logs for 2018&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Login attempt logs for 2018</p>
			<p>Our data types will be the same as in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>, with the exception of the <strong class="source-inline">success</strong> column. SQLite doesn't support Boolean values, so this column <a id="_idIndexMarker1507"/>was converted to the binary representation of its original form (stored as an integer) upon writing the data to the database:</p>
			<p class="source-code">&gt;&gt;&gt; logs_2018.dtypes</p>
			<p class="source-code">source_ip         object</p>
			<p class="source-code">username          object</p>
			<p class="source-code">success            int64</p>
			<p class="source-code">failure_reason    object</p>
			<p class="source-code">dtype: object</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We are using a SQLite database here because the Python standard library provides the means to make the connection already (<strong class="source-inline">sqlite3</strong>). If we want to use another type of database, such as MySQL or PostgreSQL, we will need to install SQLAlchemy (and possibly <a id="_idIndexMarker1508"/>additional packages, depending on the database dialect). More information can be found at <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sql-queries">https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sql-queries</a>. Check the <em class="italic">Further reading</em> section at the end of this chapter for a SQLAlchemy tutorial.</p>
			<p>Using the <strong class="source-inline">info()</strong> method, we see that <strong class="source-inline">failure_reason</strong> is the only column with nulls. It is null when the attempt is successful. When looking to build a model, we should also pay <a id="_idIndexMarker1509"/>attention to the memory usage of our data. Some models will require increasing the dimensionality of our data, which can quickly get too large to hold in memory:</p>
			<p class="source-code">&gt;&gt;&gt; logs_2018.info()</p>
			<p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p>
			<p class="source-code">DatetimeIndex: 38700 entries, </p>
			<p class="source-code">2018-01-01 00:05:32.988414 to 2018-12-31 23:29:42.482166</p>
			<p class="source-code">Data columns (total 4 columns):</p>
			<p class="source-code"> #   Column          Non-Null Count  Dtype </p>
			<p class="source-code">---  ------          --------------  ----- </p>
			<p class="source-code"> 0   source_ip       38700 non-null  object</p>
			<p class="source-code"> 1   username        38700 non-null  object</p>
			<p class="source-code"> 2   success         38700 non-null  int64 </p>
			<p class="source-code"> <strong class="bold">3   failure_reason  11368 non-null  object</strong></p>
			<p class="source-code">dtypes: int64(1), object(3)</p>
			<p class="source-code"><strong class="bold">memory usage: 1.5+ MB</strong></p>
			<p>Running the <strong class="source-inline">describe()</strong> method tells us that the most common reason for failure is providing the wrong password. We can also see that the number of unique usernames tried (1,797) is well over the number of users in our user base (133), indicating some suspicious activity. The most frequent IP address made 314 attempts, but since that isn't even one per day (remember we are looking at the full year of 2018), we can't make any assumptions:</p>
			<p class="source-code">&gt;&gt;&gt; logs_2018.describe(include='all')</p>
			<p class="source-code"><strong class="bold">           source_ip username  </strong>    success <strong class="bold">      failure_reason</strong></p>
			<p class="source-code"><strong class="bold">count          38700    38700 </strong>38700.000000    <strong class="bold">            11368</strong></p>
			<p class="source-code"><strong class="bold">unique          4956     1797      </strong>    NaN<strong class="bold">                    3</strong></p>
			<p class="source-code"><strong class="bold">top   168.123.156.81   wlopez      </strong>    NaN<strong class="bold"> error_wrong_password</strong></p>
			<p class="source-code"><strong class="bold">freq             314      387      </strong>    NaN<strong class="bold">                 6646</strong></p>
			<p class="source-code">mean             NaN      NaN     0.706253                  NaN</p>
			<p class="source-code">std              NaN      NaN     0.455483                  NaN</p>
			<p class="source-code">min              NaN      NaN     0.000000                  NaN</p>
			<p class="source-code">25%              NaN      NaN     0.000000                  NaN</p>
			<p class="source-code">50%              NaN      NaN     1.000000                  NaN</p>
			<p class="source-code">75%              NaN      NaN     1.000000                  NaN</p>
			<p class="source-code">max              NaN      NaN     1.000000                  NaN</p>
			<p>We can <a id="_idIndexMarker1510"/>look at the unique usernames with attempted logins per IP address, as in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>, which shows us that most of the IP addresses have a few usernames, but there is at least one with many:</p>
			<p class="source-code">&gt;&gt;&gt; logs_2018.groupby('source_ip')\</p>
			<p class="source-code">...     .agg(dict(username='nunique'))\</p>
			<p class="source-code">...     .username.describe()</p>
			<p class="source-code">count    4956.000000</p>
			<p class="source-code">mean        1.146287</p>
			<p class="source-code">std         1.916782</p>
			<p class="source-code">min         1.000000</p>
			<p class="source-code">25%         1.000000</p>
			<p class="source-code">50%         1.000000</p>
			<p class="source-code">75%         1.000000</p>
			<p class="source-code"><strong class="bold">max       129.000000</strong></p>
			<p class="source-code">Name: username, dtype: float64</p>
			<p>Let's <a id="_idIndexMarker1511"/>calculate the metrics per IP address:</p>
			<p class="source-code">&gt;&gt;&gt; pivot = logs_2018.pivot_table(</p>
			<p class="source-code">...     values='success', index='source_ip', </p>
			<p class="source-code">...     columns=logs_2018.failure_reason.fillna('success'), </p>
			<p class="source-code">...     aggfunc='count', fill_value=0</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; pivot.insert(0, 'attempts', pivot.sum(axis=1))</p>
			<p class="source-code">&gt;&gt;&gt; pivot = pivot\</p>
			<p class="source-code">...     .sort_values('attempts', ascending=False)\</p>
			<p class="source-code">...     .assign(</p>
			<p class="source-code">...         success_rate=lambda x: x.success / x.attempts,</p>
			<p class="source-code">...         error_rate=lambda x: 1 - x.success_rate</p>
			<p class="source-code">...     )</p>
			<p class="source-code">&gt;&gt;&gt; pivot.head()</p>
			<p>The top five IP addresses with the most attempts appear to be valid users since they have relatively high success rates: </p>
			<div>
				<div id="_idContainer452" class="IMG---Figure">
					<img src="image/Figure_11.3_B16834.jpg" alt="Figure 11.3 – Metrics per IP address&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – Metrics per IP address</p>
			<p>Let's use <a id="_idIndexMarker1512"/>this dataframe to plot successes versus attempts per IP address to see whether there is a pattern we can exploit to separate valid activity from malicious activity:</p>
			<p class="source-code">&gt;&gt;&gt; pivot.plot(</p>
			<p class="source-code">...     kind='scatter', x='attempts', y='success', </p>
			<p class="source-code">...     title='successes vs. attempts by IP address',</p>
			<p class="source-code">...     alpha=0.25</p>
			<p class="source-code">... )</p>
			<p>There appear to be a few points at the bottom that don't belong, but notice the scales on the axes don't perfectly line up. The majority of the points are along a line that is slightly less than a 1:1 relationship of attempts to successes. Recall that this chapter's simulation is more realistic than the one we used in <a href="B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Anomaly Detection</em>; as such, if we compare <em class="italic">Figure 8.11</em> to this plot, we can observe that it is much more difficult to separate valid from malicious activity here:</p>
			<div>
				<div id="_idContainer453" class="IMG---Figure">
					<img src="image/Figure_11.4_B16834.jpg" alt="Figure 11.4 – Scatter plot of successes versus attempts per IP address&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – Scatter plot of successes versus attempts per IP address</p>
			<p>Remember, this is a binary classification problem where we want to find a way to distinguish between valid user and attacker login activity. We want to build a model that will learn <a id="_idIndexMarker1513"/>some decision boundary that separates valid users from attackers. Since valid users have a higher probability of entering their password correctly, the relationship between attempts and successes will be closer to 1:1 compared to the attackers. Therefore, we may imagine the separation boundary looking something like this:</p>
			<div>
				<div id="_idContainer454" class="IMG---Figure">
					<img src="image/Figure_11.5_B16834.jpg" alt="Figure 11.5 – A possible decision boundary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – A possible decision boundary</p>
			<p>Now, the question is, which of those two groups is the attackers? Well, if more of the IP addresses are the attackers (since they use different IP addresses for each username they attempt), then the valid users would be considered outliers, and the attackers would be considered "inliers" with a box plot. Let's create one to see if that is what is happening:</p>
			<p class="source-code">&gt;&gt;&gt; pivot[['attempts', 'success']].plot(</p>
			<p class="source-code">...     kind='box', subplots=True, figsize=(10, 3), </p>
			<p class="source-code">...     title='stats per IP address'</p>
			<p class="source-code">... )</p>
			<p>Indeed, this <a id="_idIndexMarker1514"/>appears to be what is happening. Our valid users have more successes than the attackers because they only use 1-3 different IP addresses:</p>
			<div>
				<div id="_idContainer455" class="IMG---Figure">
					<img src="image/Figure_11.6_B16834.jpg" alt="Figure 11.6 – Looking for outliers using metrics per IP address&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – Looking for outliers using metrics per IP address</p>
			<p>Clearly, looking at the data like this isn't helping too much, so let's see whether a smaller granularity can help us. Let's visualize the distributions of attempts, the number of usernames, and the number of failures per IP address on a minute-by-minute resolution for January 2018:</p>
			<p class="source-code">&gt;&gt;&gt; from matplotlib.ticker import MultipleLocator</p>
			<p class="source-code">&gt;&gt;&gt; ax = logs_2018.loc['2018-01'].assign(</p>
			<p class="source-code">...     <strong class="bold">failures=lambda x: 1 - x.success</strong></p>
			<p class="source-code">... ).<strong class="bold">groupby('source_ip').resample('1min')</strong>.agg({</p>
			<p class="source-code">...     'username': 'nunique', </p>
			<p class="source-code">...     'success': 'sum', </p>
			<p class="source-code">...     'failures': 'sum'</p>
			<p class="source-code">... }).assign(</p>
			<p class="source-code">...     attempts=lambda x: x.success + x.failures</p>
			<p class="source-code">... ).dropna().query('attempts &gt; 0').reset_index().plot(</p>
			<p class="source-code">...     y=['attempts', 'username', 'failures'], kind='hist',</p>
			<p class="source-code">...     subplots=True, layout=(1, 3), figsize=(20, 3),</p>
			<p class="source-code">...     title='January 2018 distributions of minutely stats'</p>
			<p class="source-code">...           'by IP address'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; for axes in ax.flatten():</p>
			<p class="source-code">...     axes.xaxis.set_major_locator(MultipleLocator(1))</p>
			<p>It looks <a id="_idIndexMarker1515"/>like most of the IP addresses have just a single username associated with them; however, some IP addresses also have multiple failures for their attempts:</p>
			<div>
				<div id="_idContainer456" class="IMG---Figure">
					<img src="image/Figure_11.7_B16834.jpg" alt="Figure 11.7 – Distribution of metrics per minute per IP address&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – Distribution of metrics per minute per IP address</p>
			<p>Perhaps a combination of unique usernames and failures will give us something that doesn't rely on the IP address being constant. Let's visualize the number of usernames with failures per minute over 2018:</p>
			<p class="source-code">&gt;&gt;&gt; logs_2018.loc['2018'].assign(</p>
			<p class="source-code">...     <strong class="bold">failures=lambda x: 1 - x.success</strong></p>
			<p class="source-code">... ).<strong class="bold">query('failures &gt; 0')</strong>.resample('1min').agg(</p>
			<p class="source-code">...     {'username': 'nunique', 'failures': 'sum'}</p>
			<p class="source-code">... ).dropna().rename(</p>
			<p class="source-code">...     columns={'username': 'usernames_with_failures'}</p>
			<p class="source-code">... ).usernames_with_failures.plot(</p>
			<p class="source-code">...     title='usernames with failures per minute in 2018', </p>
			<p class="source-code">...     figsize=(15, 3)</p>
			<p class="source-code">... ).set_ylabel('usernames with failures')</p>
			<p>This looks <a id="_idIndexMarker1516"/>promising; we should definitely be looking into spikes in usernames with failures. It could be an issue with our website, or something malicious:</p>
			<div>
				<div id="_idContainer457" class="IMG---Figure">
					<img src="image/Figure_11.8_B16834.jpg" alt="Figure 11.8 – Usernames with failures over time&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – Usernames with failures over time</p>
			<p>After a thorough exploration of the data we will be working with, we have an idea of what <a id="_idIndexMarker1517"/>features we could use when building machine learning models. Since we don't yet have the labeled data, let's try out some unsupervised models next.</p>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor240"/>Utilizing unsupervised methods of anomaly detection</h1>
			<p>If the hackers are conspicuous and distinct from our valid users, unsupervised methods may prove <a id="_idIndexMarker1518"/>pretty effective. This is a good place to start before we have labeled data, or if the labeled data is difficult to gather or not guaranteed to be representative of the full spectrum we are looking to flag. Note that, in most cases, we won't have labeled data, so it is crucial that we are familiar with some unsupervised methods.</p>
			<p>In our initial EDA, we identified the number of usernames with a failed login attempt in a given minute as a feature for anomaly detection. We will now test out some unsupervised anomaly detection algorithms, using this feature as the jumping-off point. Scikit-learn provides a few such algorithms. In this section, we will look at <a id="_idIndexMarker1519"/>isolation forest and local outlier factor; a third method, using a one-class <strong class="bold">support vector machine</strong> (<strong class="bold">SVM</strong>), is in the <em class="italic">Exercises</em> section.</p>
			<p>Before we can try out these methods, we need to prepare our training data. Since the SOC will be sending over the labeled data for January 2018 first, we will use just the January 2018 minute-by-minute data for our unsupervised models. Our features will be the day of the week (one-hot encoded), the hour of the day (one-hot encoded), and the number of usernames with failures. See the <em class="italic">Encoding data</em> section in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>, for a refresher on one-hot encoding, if needed.</p>
			<p>Let's turn to the <strong class="source-inline">2-unsupervised_anomaly_detection.ipynb</strong> notebook and write a utility function to grab this data easily:</p>
			<p class="source-code">&gt;&gt;&gt; def get_X(log, day):</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     Get data we can use for the X</p>
			<p class="source-code">...</p>
			<p class="source-code">...     Parameters:</p>
			<p class="source-code">...         - log: The logs dataframe</p>
			<p class="source-code">...         - day: A day or single value we can use as a</p>
			<p class="source-code">...                datetime index slice</p>
			<p class="source-code">...</p>
			<p class="source-code">...     Returns: </p>
			<p class="source-code">...         A `pandas.DataFrame` object</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     return <strong class="bold">pd.get_dummies</strong>(</p>
			<p class="source-code">...         log.loc[day].assign(</p>
			<p class="source-code">...             failures=lambda x: 1 - x.success</p>
			<p class="source-code">...         ).query('failures &gt; 0').resample('1min').agg(</p>
			<p class="source-code">...             {'username': 'nunique', 'failures': 'sum'}</p>
			<p class="source-code">...         ).dropna().rename(</p>
			<p class="source-code">...             columns={'username': 'usernames_with_failures'}</p>
			<p class="source-code">...         ).assign(</p>
			<p class="source-code">...             day_of_week=lambda x: x.index.dayofweek, </p>
			<p class="source-code">...             hour=lambda x: x.index.hour</p>
			<p class="source-code">...         ).drop(columns=['failures']),</p>
			<p class="source-code">...         <strong class="bold">columns=['day_of_week', 'hour']</strong></p>
			<p class="source-code">...     )</p>
			<p>Now, we <a id="_idIndexMarker1520"/>can grab January and store it in <strong class="source-inline">X</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">X = get_X(logs_2018, '2018-01')</strong></p>
			<p class="source-code">&gt;&gt;&gt; X.columns</p>
			<p class="source-code">Index(['usernames_with_failures', 'day_of_week_0',</p>
			<p class="source-code">       'day_of_week_1', 'day_of_week_2', 'day_of_week_3',</p>
			<p class="source-code">       'day_of_week_4', 'day_of_week_5', 'day_of_week_6',</p>
			<p class="source-code">       'hour_0', 'hour_1', ..., 'hour_22', 'hour_23'],</p>
			<p class="source-code">      dtype='object')</p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor241"/>Isolation forest</h2>
			<p>The <strong class="bold">isolation forest</strong> algorithm uses splitting techniques to isolate outliers from the rest of the data; therefore, it can be used for anomaly detection. Under the hood, it is a random forest <a id="_idIndexMarker1521"/>where the splits are made on randomly chosen features. A random value of that feature between its maximum and its minimum is selected <a id="_idIndexMarker1522"/>to split on. Note that this range is from the range of the feature at that node in the tree, not the starting data. </p>
			<p>A single tree in the forest will look something like the following:</p>
			<div>
				<div id="_idContainer458" class="IMG---Figure">
					<img src="image/Figure_11.9_B16834.jpg" alt="Figure 11.9 – Example of a single tree in an isolation forest&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.9 – Example of a single tree in an isolation forest</p>
			<p>The average length of the path that must be traveled from the top of each tree in the forest to the leaf containing a given point is used to score a point as an outlier or inlier. The outliers <a id="_idIndexMarker1523"/>have much shorter paths, since they will be one of the few <a id="_idIndexMarker1524"/>on a given side of a split and have less in common with other points. Conversely, points with many dimensions in common will take more splits to separate.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">More <a id="_idIndexMarker1525"/>information on this algorithm can be found at <a href="https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest">https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest</a>.</p>
			<p>Let's implement an isolation forest with a pipeline that first standardizes our data:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.ensemble import IsolationForest</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; iso_forest_pipeline = Pipeline([</p>
			<p class="source-code">...     ('scale', StandardScaler()),</p>
			<p class="source-code">...     ('iforest', <strong class="bold">IsolationForest(</strong></p>
			<p class="source-code">...         <strong class="bold">random_state=0, contamination=0.05</strong></p>
			<p class="source-code">...     <strong class="bold">)</strong>)</p>
			<p class="source-code">... ]).fit(X)</p>
			<p>We had to specify how much of the data was expected to be outliers (<strong class="source-inline">contamination</strong>), which we estimated to be 5%; this will be difficult to choose since we don't have labeled data. There is an <strong class="source-inline">auto</strong> option that will determine a value for us but, in this case, it gives us no outliers, so it's clear that that value isn't the one we want. In practice, we could perform a statistical analysis on the data to determine an initial value or consult domain experts.</p>
			<p>The <strong class="source-inline">predict()</strong> method can be used to check whether each data point is an outlier. Anomaly detection algorithms implemented in <strong class="source-inline">scikit-learn</strong> typically return <strong class="source-inline">1</strong> or <strong class="source-inline">-1</strong> if the point is an inlier or outlier, respectively: </p>
			<p class="source-code">&gt;&gt;&gt; isolation_forest_preds = iso_forest_pipeline.predict(X)</p>
			<p class="source-code">&gt;&gt;&gt; pd.Series(<strong class="bold">np.where(</strong></p>
			<p class="source-code">...     <strong class="bold">isolation_forest_preds == -1, 'outlier', 'inlier'</strong></p>
			<p class="source-code">... <strong class="bold">)</strong>).value_counts()</p>
			<p class="source-code">inlier     42556</p>
			<p class="source-code">outlier     2001</p>
			<p class="source-code">dtype: int64</p>
			<p>Since we <a id="_idIndexMarker1526"/>don't have the labeled <a id="_idIndexMarker1527"/>data yet, we will come back to evaluate this later; for now, let's take a look at the second unsupervised algorithm that we will discuss in this chapter.</p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor242"/>Local outlier factor</h2>
			<p>While inliers are typically located in denser regions of the dataset (32-dimensional here), outliers <a id="_idIndexMarker1528"/>tend to be located in sparser, more isolated regions with few neighboring points. The <strong class="bold">local outlier factor</strong> (<strong class="bold">LOF</strong>) algorithm <a id="_idIndexMarker1529"/>looks for these sparsely populated regions to identify outliers. It scores all points based on the ratio of the density around each point to that of its nearest neighbors. Points that are considered normal will have similar densities to their neighbors; those with few others nearby will be considered abnormal.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">More <a id="_idIndexMarker1530"/>information on this algorithm can be found at <a href="https://scikit-learn.org/stable/modules/outlier_detection.html#local-outlier-factor">https://scikit-learn.org/stable/modules/outlier_detection.html#local-outlier-factor</a>.</p>
			<p>Let's build another pipeline, but swap out the isolation forest for LOF. Note that we have to guess the best value for the <strong class="source-inline">n_neighbors</strong> parameter, because <strong class="source-inline">GridSearchCV</strong> has nothing to score models on if we don't have labeled data. We are using the default value for this parameter, which is <strong class="source-inline">20</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.neighbors import LocalOutlierFactor</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; lof_pipeline = Pipeline([</p>
			<p class="source-code">...     ('scale', StandardScaler()),</p>
			<p class="source-code">...     ('lof', <strong class="bold">LocalOutlierFactor()</strong>)</p>
			<p class="source-code">... ]).fit(X)</p>
			<p>Now, let's <a id="_idIndexMarker1531"/>see how many outliers we have <a id="_idIndexMarker1532"/>this time. LOF doesn't have a <strong class="source-inline">predict()</strong> method, so we have to check the <strong class="source-inline">negative_outlier_factor_</strong> attribute of the LOF object to see the scores of each of the data points we fit it with:</p>
			<p class="source-code">&gt;&gt;&gt; lof_preds = lof_pipeline.named_steps['lof']\</p>
			<p class="source-code">...     .negative_outlier_factor_ </p>
			<p class="source-code">&gt;&gt;&gt; lof_preds</p>
			<p class="source-code">array([<strong class="bold">-1.33898756e+10</strong>, -1.00000000e+00, -1.00000000e+00, ...,</p>
			<p class="source-code">       -1.00000000e+00, -1.00000000e+00, <strong class="bold">-1.11582297e+10</strong>])</p>
			<p>There is another difference between LOF and isolation forests: the values for the <strong class="source-inline">negative_outlier_factor_</strong> attribute aren't strictly <strong class="source-inline">-1</strong> or <strong class="source-inline">1</strong>. In fact, they can be any number—take a look at the first and last values in the previous result, and you'll see that they are way less than <strong class="source-inline">-1</strong>. This means we can't use the method we used with the isolation forest to count the inliers and outliers. Instead, we need to compare the <strong class="source-inline">negative_outlier_factor_</strong> attribute to the <strong class="source-inline">offset_</strong> attribute of the LOF model, which <a id="_idIndexMarker1533"/>tells us the cutoff value as determined by the LOF model during training (using the <strong class="source-inline">contamination</strong> parameter):</p>
			<p class="source-code">&gt;&gt;&gt; pd.Series(np.where(</p>
			<p class="source-code">...     <strong class="bold">lof_preds &lt; lof_pipeline.named_steps['lof'].offset_</strong>, </p>
			<p class="source-code">...     'outlier', 'inlier'</p>
			<p class="source-code">... )).value_counts()</p>
			<p class="source-code">inlier     44248</p>
			<p class="source-code">outlier      309</p>
			<p class="source-code">dtype: int64</p>
			<p>Now <a id="_idIndexMarker1534"/>that we have two unsupervised models, we need to compare them to see which one would be more beneficial to our stakeholders.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor243"/>Comparing models</h2>
			<p>LOF <a id="_idIndexMarker1535"/>indicates fewer outliers than the isolation forest, but perhaps they don't even agree with each other. As we learned in <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>, we can use the <strong class="source-inline">cohen_kappa_score()</strong> function from <strong class="source-inline">sklearn.metrics</strong> to check their level of agreement:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import cohen_kappa_score</p>
			<p class="source-code">&gt;&gt;&gt; is_lof_outlier = np.where(</p>
			<p class="source-code">...     lof_preds &lt; lof_pipeline.named_steps['lof'].offset_, </p>
			<p class="source-code">...     'outlier', 'inlier'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; is_iso_outlier = np.where(</p>
			<p class="source-code">...     isolation_forest_preds == -1, 'outlier', 'inlier'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">cohen_kappa_score(is_lof_outlier, is_iso_outlier)</strong></p>
			<p class="source-code">0.25862517997335677</p>
			<p>They have a low level of agreement, indicating that it's not so obvious which data points are anomalies. Without labeled data, however, it really is impossible for us to tell which one is better. We would have to work with the consumers of the results to determine which <a id="_idIndexMarker1536"/>model gives them the most useful data. Thankfully, the SOC has just sent over the January 2018 labeled data, so we can determine which of our models is better and let them start using it until we get a supervised model ready.</p>
			<p>First, we will read in the labeled data they wrote to the database in the <strong class="source-inline">attacks</strong> table and add some columns indicating the minute the attack started, the duration, and when it ended:</p>
			<p class="source-code">&gt;&gt;&gt; with sqlite3.connect('logs/logs.db') as conn:</p>
			<p class="source-code">...     hackers_jan_2018 = pd.read_sql(</p>
			<p class="source-code">...         """</p>
			<p class="source-code">...         SELECT * </p>
			<p class="source-code">...         <strong class="bold">FROM attacks</strong> </p>
			<p class="source-code">...         WHERE start BETWEEN "2018-01-01" AND "2018-02-01";</p>
			<p class="source-code">...         """, conn, parse_dates=['start', 'end']</p>
			<p class="source-code">...     ).<strong class="bold">assign(</strong></p>
			<p class="source-code">...         <strong class="bold">duration=lambda x: x.end - x.start,</strong></p>
			<p class="source-code">...         <strong class="bold">start_floor=lambda x: x.start.dt.floor('min'),</strong></p>
			<p class="source-code">...         <strong class="bold">end_ceil=lambda x: x.end.dt.ceil('min')</strong></p>
			<p class="source-code">...     <strong class="bold">)</strong></p>
			<p class="source-code">&gt;&gt;&gt; hackers_jan_2018.shape</p>
			<p class="source-code">(7, 6)</p>
			<p>Note that the SOC only has a single IP address for the ones involved in each attack, so it's a good thing we aren't relying on that anymore. Instead, the SOC wants us to tell them <a id="_idIndexMarker1537"/>at which minute there was suspicious activity so that they can investigate further. Also note that while the attacks are quick in duration, our minute-by-minute data means we will trigger many alerts per attack:</p>
			<div>
				<div id="_idContainer459" class="IMG---Figure">
					<img src="image/Figure_11.10_B16834.jpg" alt="Figure 11.10 – Labeled data for evaluating our models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.10 – Labeled data for evaluating our models</p>
			<p>Using the <strong class="source-inline">start_floor</strong> and <strong class="source-inline">end_ceil</strong> columns, we can create a range of datetimes and can check whether the data we marked as outliers falls within that range. For this, we will use the following function:</p>
			<p class="source-code">&gt;&gt;&gt; def get_y(datetimes, hackers, resolution='1min'):</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     Get data we can use for the y (whether or not a</p>
			<p class="source-code">...     hacker attempted a log in during that time).</p>
			<p class="source-code">...</p>
			<p class="source-code">...     Parameters:</p>
			<p class="source-code">...         - datetimes: The datetimes to check for hackers</p>
			<p class="source-code">...         - hackers: The dataframe indicating when the </p>
			<p class="source-code">...                    attacks started and stopped</p>
			<p class="source-code">...         - resolution: The granularity of the datetime. </p>
			<p class="source-code">...                       Default is 1 minute.</p>
			<p class="source-code">...</p>
			<p class="source-code">...     Returns: `pandas.Series` of Booleans.</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     <strong class="bold">date_ranges = hackers.apply(</strong></p>
			<p class="source-code">...         <strong class="bold">lambda x: pd.date_range(</strong></p>
			<p class="source-code">...             <strong class="bold">x.start_floor, x.end_ceil, freq=resolution</strong></p>
			<p class="source-code">...         <strong class="bold">),</strong> </p>
			<p class="source-code">...         <strong class="bold">axis=1</strong></p>
			<p class="source-code">...     <strong class="bold">)</strong></p>
			<p class="source-code">...     dates = pd.Series(dtype='object')</p>
			<p class="source-code">...     for date_range in date_ranges:</p>
			<p class="source-code">...         dates = pd.concat([dates, date_range.to_series()])</p>
			<p class="source-code">...     return <strong class="bold">datetimes.isin(dates)</strong></p>
			<p>Now, let's <a id="_idIndexMarker1538"/>find the datetimes in our <strong class="source-inline">X</strong> data that had hacker activity:</p>
			<p class="source-code">&gt;&gt;&gt; is_hacker = \</p>
			<p class="source-code">...     get_y(X.reset_index().datetime, hackers_jan_2018)</p>
			<p>We now have everything we need to make a classification report and a confusion matrix. Since we will be passing in the <strong class="source-inline">is_hacker</strong> series a lot, we will make some partials to reduce our typing a bit:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from functools import partial</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.classification import confusion_matrix_visual</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">report = partial(classification_report, is_hacker)</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">conf_matrix = partial(</strong></p>
			<p class="source-code">...     <strong class="bold">confusion_matrix_visual, is_hacker,</strong> </p>
			<p class="source-code">...     <strong class="bold">class_labels=[False, True]</strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p>Let's <a id="_idIndexMarker1539"/>start with the classification reports, which indicate that the isolation forest is much better in terms of recall:</p>
			<p class="source-code">&gt;&gt;&gt; iso_forest_predicts_hacker = isolation_forest_preds == - 1</p>
			<p class="source-code">&gt;&gt;&gt; print(report(iso_forest_predicts_hacker)) <strong class="bold"># iso. forest</strong></p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">       False       1.00      0.96      0.98     44519</p>
			<p class="source-code">        <strong class="bold">True       0.02      0.82      0.03        38</strong></p>
			<p class="source-code">    accuracy                           0.96     44557</p>
			<p class="source-code">   <strong class="bold">macro avg       0.51      0.89      0.50     44557</strong></p>
			<p class="source-code">weighted avg       1.00      0.96      0.98     44557</p>
			<p class="source-code">&gt;&gt;&gt; lof_predicts_hacker = \</p>
			<p class="source-code">...     lof_preds &lt; lof_pipeline.named_steps['lof'].offset_ </p>
			<p class="source-code">&gt;&gt;&gt; print(report(lof_predicts_hacker)) <strong class="bold"># LOF</strong></p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">       False       1.00      0.99      1.00     44519</p>
			<p class="source-code">        <strong class="bold">True       0.03      0.26      0.06        38</strong></p>
			<p class="source-code">    accuracy                           0.99     44557</p>
			<p class="source-code">   <strong class="bold">macro avg       0.52      0.63      0.53     44557</strong></p>
			<p class="source-code">weighted avg       1.00      0.99      1.00     44557</p>
			<p>To <a id="_idIndexMarker1540"/>better understand the results in the classification report, let's create confusion matrices for our unsupervised methods and place them side-by-side for comparison:</p>
			<p class="source-code">&gt;&gt;&gt; fig, axes = plt.subplots(1, 2, figsize=(15, 5))</p>
			<p class="source-code">&gt;&gt;&gt; conf_matrix(</p>
			<p class="source-code">...     iso_forest_predicts_hacker, </p>
			<p class="source-code">...     ax=axes[0], title='Isolation Forest'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; conf_matrix(</p>
			<p class="source-code">...     lof_predicts_hacker, </p>
			<p class="source-code">...     ax=axes[1], title='Local Outlier Factor'</p>
			<p class="source-code">... )</p>
			<p>The isolation forest has more true positives and a greater number of false positives compared to LOF, but it has fewer false negatives:</p>
			<div>
				<div id="_idContainer460" class="IMG---Figure">
					<img src="image/Figure_11.11_B16834.jpg" alt="Figure 11.11 – Confusion matrices for our unsupervised models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.11 – Confusion matrices for our unsupervised models</p>
			<p>The SOC has informed us that false negatives are much more costly than false positives. However, they would like us to keep false positives in check to avoid bogging down the team with <a id="_idIndexMarker1541"/>an excessive number <a id="_idIndexMarker1542"/>of false alarms. This tells us that recall (the <strong class="bold">true positive rate</strong> (<strong class="bold">TPR</strong>)) is more valuable than precision as a performance metric. The SOC wants us to target a <em class="italic">recall of at least 70%</em>.</p>
			<p>Since we <a id="_idIndexMarker1543"/>have a very large class imbalance, the <strong class="bold">false positive rate</strong> (<strong class="bold">FPR</strong>) won't be too informative for us. Remember, the FPR is the ratio of false positives to the sum of false positives and true negatives (everything belonging to the negative class). Due to the nature of the attacks being rare, we will have a very large number of true negatives and, therefore, our FPR will remain very low. Consequently, the secondary metric determined by the SOC is to attain a <em class="italic">precision of 85% or greater</em>.</p>
			<p>The isolation forest model exceeds our target recall, but the precision is too low. Since we were able to obtain some labeled data, we can now use supervised learning to find the minutes with suspicious activity (note that this won't always be the case). Let's see whether we can use this extra information to find the minutes of interest more precisely.</p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor244"/>Implementing supervised anomaly detection</h1>
			<p>The SOC has finished up labeling the 2018 data, so we should revisit our EDA to make sure our <a id="_idIndexMarker1544"/>plan of looking at the number of usernames with failures on a minute resolution does separate the data. This EDA is in the <strong class="source-inline">3-EDA_labeled_data.ipynb</strong> notebook. After some data wrangling, we are able to create the following scatter plot, which shows that this strategy does indeed appear to separate the suspicious activity:</p>
			<div>
				<div id="_idContainer461" class="IMG---Figure">
					<img src="image/Figure_11.12_B16834.jpg" alt="Figure 11.12 – Confirming that our features can help form a decision boundary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.12 – Confirming that our features can help form a decision boundary</p>
			<p>In the <strong class="source-inline">4-supervised_anomaly_detection.ipynb</strong> notebook, we will create some supervised models. This time we need to read in all the labeled data for 2018. Note that the code for reading in the logs is omitted since it is the same as in the previous section:</p>
			<p class="source-code">&gt;&gt;&gt; with sqlite3.connect('logs/logs.db') as conn:</p>
			<p class="source-code">...     hackers_2018 = pd.read_sql(</p>
			<p class="source-code">...         """</p>
			<p class="source-code">...         SELECT * </p>
			<p class="source-code">...         FROM attacks </p>
			<p class="source-code">...         WHERE start BETWEEN "2018-01-01" AND "2019-01-01";</p>
			<p class="source-code">...         """, conn, parse_dates=['start', 'end']</p>
			<p class="source-code">...     ).assign(</p>
			<p class="source-code">...         duration=lambda x: x.end - x.start,</p>
			<p class="source-code">...         start_floor=lambda x: x.start.dt.floor('min'),</p>
			<p class="source-code">...         end_ceil=lambda x: x.end.dt.ceil('min')</p>
			<p class="source-code">...     )</p>
			<p>Before we build our models, however, let's create a new function that will create both <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> at <a id="_idIndexMarker1545"/>the same time. The <strong class="source-inline">get_X_y()</strong> function will use the <strong class="source-inline">get_X()</strong> and <strong class="source-inline">get_y()</strong> functions we made earlier, returning both <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; def get_X_y(log, day, hackers):</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     Get the X, y data to build a model with.</p>
			<p class="source-code">...</p>
			<p class="source-code">...     Parameters:</p>
			<p class="source-code">...         - log: The logs dataframe</p>
			<p class="source-code">...         - day: A day or single value we can use as a </p>
			<p class="source-code">...                datetime index slice</p>
			<p class="source-code">...         - hackers: The dataframe indicating when the </p>
			<p class="source-code">...                    attacks started and stopped</p>
			<p class="source-code">...</p>
			<p class="source-code">...     Returns:</p>
			<p class="source-code">...         X, y tuple where X is a `pandas.DataFrame` object</p>
			<p class="source-code">...         and y is a `pandas.Series` object</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     <strong class="bold">X = get_X(log, day)</strong></p>
			<p class="source-code">...     <strong class="bold">y = get_y(X.reset_index().datetime, hackers)</strong></p>
			<p class="source-code">...     <strong class="bold">return X, y</strong></p>
			<p>Now, let's <a id="_idIndexMarker1546"/>make a training set with January 2018 data and a testing set with February 2018 data, using our new function:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">X_train, y_train</strong> = \</p>
			<p class="source-code">...     get_X_y(logs_2018, <strong class="bold">'2018-01'</strong>, hackers_2018)</p>
			<p class="source-code">&gt;&gt;&gt; X_test, y_test = \</p>
			<p class="source-code">...     get_X_y(logs_2018, '2018-02', hackers_2018)</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">While we have a very large class imbalance, we don't jump right to balancing the training sets. It's crucial to try out the model without premature optimization. If we build our model and see that it is being affected by the class imbalance, then we can try those techniques. Remember to be very cautious with over-/under-sampling techniques, as some make assumptions of the data that aren't always applicable or realistic. Think about SMOTE—would we really expect all future attackers to be similar to the ones we have in the data?</p>
			<p>Let's use this data to build some supervised anomaly detection models. Remember that the SOC has given us the performance requirements in terms of recall (at least 70%) and precision (85% or greater), so we will use those metrics to evaluate our models.</p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor245"/>Baselining</h2>
			<p>Our first <a id="_idIndexMarker1547"/>step will be to build <a id="_idIndexMarker1548"/>some baseline models, so we know that our machine learning algorithms are performing better than some simpler models and have predictive value. We will build two such models:</p>
			<ul>
				<li>A dummy classifier that will predict labels based on the stratification in the data.</li>
				<li>A Naive Bayes model that will predict the labels leveraging Bayes' theorem.</li>
			</ul>
			<h3>Dummy classifier</h3>
			<p>A dummy classifier <a id="_idIndexMarker1549"/>will give us a model that is equivalent to the baseline we have been drawing on our ROC curves. The results will be poor on purpose. We will never use this classifier to actually make predictions; rather, we can use <a id="_idIndexMarker1550"/>it to see whether the models we are building are better than random guessing strategies. In the <strong class="source-inline">dummy</strong> module, <strong class="source-inline">scikit-learn</strong> provides the <strong class="source-inline">DummyClassifier</strong> class precisely for this purpose.</p>
			<p>Using the <strong class="source-inline">strategy</strong> parameter, we can specify how the dummy classifier will make its predictions. Some <a id="_idIndexMarker1551"/>interesting options are as follows:</p>
			<ul>
				<li><strong class="source-inline">uniform</strong>: The classifier will guess each time whether or not the observation belongs to a hacking attempt.</li>
				<li><strong class="source-inline">most_frequent</strong>: The classifier will always predict the most frequent label, which, in our case, will result in never marking anything as nefarious. This will achieve high accuracy, but be useless since the minority class is the class of interest.</li>
				<li><strong class="source-inline">stratified</strong>: The classifier will use the class distribution from the training data and maintain that ratio with its guesses.</li>
			</ul>
			<p>Let's build a dummy classifier with the <strong class="source-inline">stratified</strong> strategy:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.dummy import DummyClassifier</p>
			<p class="source-code">&gt;&gt;&gt; dummy_model = <strong class="bold">DummyClassifier(</strong></p>
			<p class="source-code">...     <strong class="bold">strategy='stratified'</strong>, random_state=0</p>
			<p class="source-code">... ).fit(X_train, y_train)</p>
			<p class="source-code">&gt;&gt;&gt; dummy_preds = dummy_model.predict(X_test)</p>
			<p>Now that we have our first baseline model, let's measure its performance for comparisons. We will be using both the ROC curve and the precision-recall curve to show how the class imbalance can make the ROC curve optimistic of performance. To reduce typing, we will once again make some partials:</p>
			<p class="source-code">&gt;&gt;&gt; from functools import partial</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.classification import (</p>
			<p class="source-code">...     confusion_matrix_visual, plot_pr_curve, plot_roc</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; report = partial(classification_report, y_test)</p>
			<p class="source-code">&gt;&gt;&gt; roc = partial(plot_roc, y_test)</p>
			<p class="source-code">&gt;&gt;&gt; pr_curve = partial(plot_pr_curve, y_test)</p>
			<p class="source-code">&gt;&gt;&gt; conf_matrix = partial(</p>
			<p class="source-code">...     confusion_matrix_visual, y_test, </p>
			<p class="source-code">...     class_labels=[False, True]</p>
			<p class="source-code">... )</p>
			<p>Recall from <a id="_idIndexMarker1552"/>our initial discussion of ROC curves in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>, that the diagonal line was random guessing of a dummy model. If our performance isn't better than this line, our model has no predictive value. The dummy model we just created is equivalent to this line. Let's visualize the baseline ROC curve, precision-recall curve, and confusion matrix using subplots:</p>
			<p class="source-code">&gt;&gt;&gt; fig, axes = plt.subplots(1, 3, figsize=(20, 5))</p>
			<p class="source-code">&gt;&gt;&gt; roc(dummy_model.predict_proba(X_test)[:,1], ax=axes[0])</p>
			<p class="source-code">&gt;&gt;&gt; conf_matrix(dummy_preds, ax=axes[1])</p>
			<p class="source-code">&gt;&gt;&gt; pr_curve(</p>
			<p class="source-code">...     dummy_model.predict_proba(X_test)[:,1], ax=axes[2]</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; plt.suptitle('Dummy Classifier with Stratified Strategy')</p>
			<p>The dummy classifier wasn't able to flag any of the attackers. The ROC curve (TPR versus FPR) indicates <a id="_idIndexMarker1553"/>that the dummy model has no predictive value, with an <strong class="bold">area under the curve</strong> (<strong class="bold">AUC</strong>) of 0.5. Note that the area under the precision-recall curve is nearly zero:</p>
			<div>
				<div id="_idContainer462" class="IMG---Figure">
					<img src="image/Figure_11.13_B16834.jpg" alt="Figure 11.13 – Baselining with a dummy classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.13 – Baselining with a dummy classifier</p>
			<p>Since we <a id="_idIndexMarker1554"/>have a very large class imbalance, the stratified random guessing strategy should perform horrendously on the minority class and very well on the majority class. We can observe this by examining the classification report:</p>
			<p class="source-code">&gt;&gt;&gt; print(report(dummy_preds))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">       False       1.00      1.00      1.00     39958</p>
			<p class="source-code">        <strong class="bold">True       0.00      0.00      0.00         5</strong></p>
			<p class="source-code">    accuracy                           1.00     39963</p>
			<p class="source-code">   <strong class="bold">macro avg       0.50      0.50      0.50     39963</strong></p>
			<p class="source-code">weighted avg       1.00      1.00      1.00     39963</p>
			<h3>Naive Bayes</h3>
			<p>Our last baseline <a id="_idIndexMarker1555"/>model will be a Naive Bayes classifier. Before we discuss this model, we need to <a id="_idIndexMarker1556"/>review a few concepts of probability. The first is conditional probability. When dealing with two events, <em class="italic">A</em> and <em class="italic">B</em>, the probability of event <em class="italic">A</em> happening <em class="italic">given</em> that event <em class="italic">B</em> happened is the <strong class="bold">conditional probability</strong> and is <a id="_idIndexMarker1557"/>written as <em class="italic">P(A|B)</em>. When events <em class="italic">A</em> and <em class="italic">B</em> are independent, meaning <em class="italic">B</em> happening doesn't tell us anything about <em class="italic">A</em> happening and vice versa, <em class="italic">P(A|B)</em> is <em class="italic">P(A)</em>.</p>
			<p>The conditional <a id="_idIndexMarker1558"/>probability is defined as the <strong class="bold">joint probability</strong> of both <em class="italic">A</em> and <em class="italic">B</em> occurring (which is the intersection of these events), written as <em class="italic">P(A </em><em class="italic">∩</em><em class="italic"> B)</em>, divided by the probability of <em class="italic">B</em> occurring (provided this is not zero):</p>
			<div>
				<div id="_idContainer463" class="IMG---Figure">
					<img src="image/Formula_11_001.jpg" alt=""/>
				</div>
			</div>
			<p>This equation can be rearranged as follows: </p>
			<div>
				<div id="_idContainer464">
					<img src="image/Formula_11_002.png" alt=""/>
				</div>
			</div>
			<p>The joint probability of <em class="italic">A </em><em class="italic">∩ </em><em class="italic">B</em> is equivalent to <em class="italic">B </em><em class="italic">∩</em><em class="italic"> A</em>; therefore, we get the following equation:</p>
			<div>
				<div id="_idContainer465" class="IMG---Figure">
					<img src="image/Formula_11_003.jpg" alt=""/>
				</div>
			</div>
			<p>It then follows that we can change the first equation to use conditional probabilities instead of the joint probability. This gives us <strong class="bold">Bayes' theorem</strong>:</p>
			<div>
				<div id="_idContainer466" class="IMG---Figure">
					<img src="image/Formula_11_004.jpg" alt=""/>
				</div>
			</div>
			<p>When working <a id="_idIndexMarker1559"/>with the previous equation, <em class="italic">P(A)</em> is referred to as the <strong class="bold">prior probability</strong>, or initial degree of belief that event <em class="italic">A</em> will happen. After accounting for event <em class="italic">B</em> occurring, this initial belief gets updated; this is represented as <em class="italic">P(A|B)</em> and is <a id="_idIndexMarker1560"/>called the <strong class="bold">posterior probability</strong>. The <strong class="bold">likelihood</strong> of event <em class="italic">B</em> given event <em class="italic">A</em> is <em class="italic">P(B|A)</em>. The support that event <em class="italic">B</em> occurring gives to our belief of observing event <em class="italic">A</em> is the following:</p>
			<div>
				<div id="_idContainer467" class="IMG---Figure">
					<img src="image/Formula_11_005.jpg" alt=""/>
				</div>
			</div>
			<p>Let's take a look at an example—say we are building a spam filter, and we find that 10% of emails are spam. This 10% is our prior, or <em class="italic">P(spam)</em>. We want to know the probability an email we <a id="_idIndexMarker1561"/>just received is spam given that it contains the word <em class="italic">free</em>—we want to find <em class="italic">P(spam|free)</em>. In order to find this, we need the probability that the word <em class="italic">free</em> is in <a id="_idIndexMarker1562"/>an email given that it is spam, or <em class="italic">P(free|spam)</em>, and the probability of the word <em class="italic">free</em> being in an email, or <em class="italic">P(free)</em>.</p>
			<p>Suppose we learned that 12% of emails contained the word <em class="italic">free</em> and 20% of the emails that were determined to be spam contained the word <em class="italic">free</em>. Plugging all this into the equation from before, we see that once we know an email contains the word <em class="italic">free</em>, our belief that it is spam increases from 10% to 16.7%, which is our posterior probability:</p>
			<div>
				<div id="_idContainer468" class="IMG---Figure">
					<img src="image/Formula_11_006.jpg" alt=""/>
				</div>
			</div>
			<p>Bayes' theorem can be leveraged in a type of classifier called <strong class="bold">Naive Bayes</strong>. Depending on the assumptions we make of the data, we get a different member of the Naive Bayes family of classifiers. These models are very fast to train because they make a simplifying assumption of conditional independence of each pair of the <strong class="source-inline">X</strong> features, given the <strong class="source-inline">y</strong> variable (meaning <em class="italic">P(x</em><span class="subscript">i</span><em class="italic">|y,x</em><span class="subscript">1</span><em class="italic">...x</em><span class="subscript">n</span><em class="italic">)</em> is equivalent to <em class="italic">P(x</em><span class="subscript">i</span><em class="italic">|y)</em>). They are called <em class="italic">naive</em> because this assumption is often incorrect; however, these classifiers have traditionally worked well in building spam filters.</p>
			<p>Let's say we also find multiple dollar signs in the email and the word <em class="italic">prescription</em>, and we want to know the probability of it being spam. While some of these features may depend on each other, the Naive Bayes model will treat them as conditionally independent. This means our equation for the posterior probability is now the following:</p>
			<div>
				<div id="_idContainer469" class="IMG---Figure">
					<img src="image/Formula_11_007_New.jpg" alt=""/>
				</div>
			</div>
			<p>Suppose we <a id="_idIndexMarker1563"/>find out that 5% of spam emails contain multiple dollar signs, 55% of spam emails contain the word <em class="italic">prescription</em>, 25% of emails contain multiple <a id="_idIndexMarker1564"/>dollar signs, and the word <em class="italic">prescription</em> is found in 2% of emails overall. This means that our belief of the email being spam, given that it has the words <em class="italic">free</em> and <em class="italic">prescription</em> and multiple dollar signs, increases from 10% to 91.7%:</p>
			<div>
				<div id="_idContainer470" class="IMG---Figure">
					<img src="image/Formula_11_008.jpg" alt=""/>
				</div>
			</div>
			<p>Now that we understand the basics of the algorithm, let's build a Naive Bayes classifier. Note that <strong class="source-inline">scikit-learn</strong> provides various Naive Bayes classifiers that differ by the assumed distributions of the likelihoods of the features, which we defined as <em class="italic">P(x</em><span class="subscript">i</span><em class="italic">|y,x</em><span class="subscript">1</span><em class="italic">...x</em><span class="subscript">n</span><em class="italic">)</em>. We will use the version that assumes they are normally distributed, <strong class="source-inline">GaussianNB</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.naive_bayes import GaussianNB</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; nb_pipeline = Pipeline([</p>
			<p class="source-code">...     ('scale', StandardScaler()),</p>
			<p class="source-code">...     <strong class="bold">('nb', GaussianNB())</strong></p>
			<p class="source-code">... ]).fit(X_train, y_train)</p>
			<p class="source-code">&gt;&gt;&gt; nb_preds = nb_pipeline.predict(X_test)</p>
			<p>We can retrieve the class priors from the model, which, in this case, tells us that the prior for a minute containing normal activity is 99.91% versus 0.09% for abnormal activity:</p>
			<p class="source-code">&gt;&gt;&gt; nb_pipeline.named_steps['nb'].<strong class="bold">class_prior_</strong></p>
			<p class="source-code">array([9.99147160e-01, 8.52840182e-04])</p>
			<p>Naive Bayes <a id="_idIndexMarker1565"/>makes a nice baseline model because we don't have to <a id="_idIndexMarker1566"/>tune any hyperparameters, and it is quick to train. Let's see how it performs on the test data (February 2018):</p>
			<p class="source-code">&gt;&gt;&gt; fig, axes = plt.subplots(1, 3, figsize=(20, 5))</p>
			<p class="source-code">&gt;&gt;&gt; roc(nb_pipeline.predict_proba(X_test)[:,1], ax=axes[0])</p>
			<p class="source-code">&gt;&gt;&gt; conf_matrix(nb_preds, ax=axes[1])</p>
			<p class="source-code">&gt;&gt;&gt; pr_curve(</p>
			<p class="source-code">...     nb_pipeline.predict_proba(X_test)[:,1], ax=axes[2]</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; plt.suptitle('Naive Bayes Classifier')</p>
			<p>The Naive Bayes classifier finds all five attackers and is above the baseline (the dashed line) in both the ROC curve and precision-recall curve, meaning this model has some predictive value:</p>
			<div>
				<div id="_idContainer471" class="IMG---Figure">
					<img src="image/Figure_11.14_B16834.jpg" alt="Figure 11.14 – Performance of the Naive Bayes classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.14 – Performance of the Naive Bayes classifier</p>
			<p>Unfortunately, we are triggering an enormous quantity of false positives (8,218). For the month of February, roughly 1 out of every 1,644 attack classifications was indeed an attack. This has the effect of desensitizing the users of these classifications. They may choose to always ignore our classifications because they are too noisy and, consequently, miss a real issue. This trade-off can be captured in the metrics of the classification report:</p>
			<p class="source-code">&gt;&gt;&gt; print(report(nb_preds))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">       False       1.00      0.79      0.89     39958</p>
			<p class="source-code">        <strong class="bold">True       0.00      1.00      0.00         5</strong></p>
			<p class="source-code">    accuracy                           0.79     39963</p>
			<p class="source-code">   <strong class="bold">macro avg       0.50      0.90      0.44     39963</strong></p>
			<p class="source-code">weighted avg       1.00      0.79      0.89     39963</p>
			<p>While the Naive Bayes <a id="_idIndexMarker1567"/>classifier outperforms the dummy classifier, it does not meet the requirements of our stakeholders. Precision rounds <a id="_idIndexMarker1568"/>to zero for the target class because we have lots of false positives. Recall is higher than precision because the model is better with false negatives than false positives (since it isn't very discerning). This leaves the F<span class="subscript">1</span> score at zero. Now, let's try to beat these baseline models.</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor246"/>Logistic regression</h2>
			<p>Since logistic <a id="_idIndexMarker1569"/>regression is another <a id="_idIndexMarker1570"/>simple model, let's try it out next. We used logistic regression in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>, for classification problems, so we already know how it works. As we learned in <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>, we will use a grid search to find a good value for the regularization hyperparameter in our desired search space, using <strong class="source-inline">recall_macro</strong> for scoring. Remember there is a large cost associated with false negatives, so we are focusing on recall. The <strong class="source-inline">_macro</strong> suffix indicates that we want to average the recall between the positive and negative classes, instead of looking at it overall (due to the class imbalance).</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If we know exactly how much more valuable recall is to us over precision, we can replace this with a custom scorer made using the <strong class="source-inline">make_scorer()</strong> function in <strong class="source-inline">sklearn.metrics</strong>. The notebook we are working in has an example.</p>
			<p>When using <a id="_idIndexMarker1571"/>grid search, warnings from <strong class="source-inline">scikit-learn</strong> may be printed at each iteration. Therefore, to avoid having to <a id="_idIndexMarker1572"/>scroll through all that, we will use the <strong class="source-inline">%%capture</strong> magic command to capture everything that would have been printed, keeping our notebook clean:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">%%capture</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.pipeline import Pipeline</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; lr_pipeline = Pipeline([</p>
			<p class="source-code">...     ('scale', StandardScaler()),</p>
			<p class="source-code">...     ('lr', LogisticRegression(random_state=0))</p>
			<p class="source-code">... ])</p>
			<p class="source-code">&gt;&gt;&gt; search_space = {'lr__C': [0.1, 0.5, 1, 2]}</p>
			<p class="source-code">&gt;&gt;&gt; lr_grid = GridSearchCV(</p>
			<p class="source-code">...     lr_pipeline, search_space, scoring='recall_macro', cv=5</p>
			<p class="source-code">... ).fit(X_train, y_train)</p>
			<p class="source-code">&gt;&gt;&gt; lr_preds = lr_grid.predict(X_test) </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">With <strong class="source-inline">%%capture</strong>, all errors and output will be captured by default. We have the option of writing <strong class="source-inline">--no-stderr</strong> to hide errors only and <strong class="source-inline">--no-stdout</strong> to hide output only. These go after <strong class="source-inline">%%capture</strong>; for example, <strong class="source-inline">%%capture --no-stderr</strong>. </p>
			<p class="callout">If we want to hide specific errors, we can use the <strong class="source-inline">warnings</strong> module, instead. For example, after importing <strong class="source-inline">filterwarnings</strong> from the <strong class="source-inline">warnings</strong> module, we can run the following to ignore warnings of future deprecations: <strong class="source-inline">filterwarnings('ignore',               category=DeprecationWarning)</strong></p>
			<p>Now that <a id="_idIndexMarker1573"/>we have our logistic regression <a id="_idIndexMarker1574"/>model trained, let's check on the performance:</p>
			<p class="source-code">&gt;&gt;&gt; fig, axes = plt.subplots(1, 3, figsize=(20, 5))</p>
			<p class="source-code">&gt;&gt;&gt; roc(lr_grid.predict_proba(X_test)[:,1], ax=axes[0])</p>
			<p class="source-code">&gt;&gt;&gt; conf_matrix(lr_preds, ax=axes[1])</p>
			<p class="source-code">&gt;&gt;&gt; pr_curve(lr_grid.predict_proba(X_test)[:,1], ax=axes[2])</p>
			<p class="source-code">&gt;&gt;&gt; plt.suptitle('Logistic Regression Classifier')</p>
			<p>This model has no false positives and is much better than the baselines. The ROC curve is significantly closer to the top-left corner, as is the precision-recall curve to the top-right corner. Notice that the ROC curve is a bit more optimistic about the performance:</p>
			<div>
				<div id="_idContainer472" class="IMG---Figure">
					<img src="image/Figure_11.15_B16834.jpg" alt="Figure 11.15 – Performance using logistic regression&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.15 – Performance using logistic regression</p>
			<p>This model <a id="_idIndexMarker1575"/>meets the requirements <a id="_idIndexMarker1576"/>of the SOC. Our recall is at least 70% and our precision is at least 85%:</p>
			<p class="source-code">&gt;&gt;&gt; print(report(lr_preds))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">       False       1.00      1.00      1.00     39958</p>
			<p class="source-code">        <strong class="bold">True       1.00      0.80      0.89         5</strong></p>
			<p class="source-code">    accuracy                           1.00     39963</p>
			<p class="source-code">   <strong class="bold">macro avg       1.00      0.90      0.94     39963</strong></p>
			<p class="source-code">weighted avg       1.00      1.00      1.00     39963</p>
			<p>The SOC has given us data for January and February 2019, and they want us to update our model. Unfortunately, our model has already been trained, so we have the choice of rebuilding from scratch or ignoring this new data. Ideally, we would build a model with a feedback loop to incorporate this (and future) new data. In the next section, we will discuss how to do this.</p>
			<h1 id="_idParaDest-248"><a id="_idTextAnchor247"/>Incorporating a feedback loop with online learning</h1>
			<p>There are some big issues with the models we have built so far. Unlike the data we worked with in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>, and <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>, we wouldn't expect the attacker behavior to be static over time. There is also a limit to how much data we can hold in <a id="_idIndexMarker1577"/>memory, which limits how <a id="_idIndexMarker1578"/>much data we can train our model on. Therefore, we will now build an online learning model to flag anomalies <a id="_idIndexMarker1579"/>in usernames with failures per minute. An <strong class="bold">online learning</strong> model is constantly getting updated (in near real time via streaming, or in batches). This allows us to learn from new data as it comes and then get rid of it (to keep space in memory).</p>
			<p>In addition, the model can evolve over time and adapt to changes in the underlying distribution of the data. We will also be providing our model with feedback as it learns so that we <a id="_idIndexMarker1580"/>are able to make sure it stays robust to changes in the hacker behavior over time. This is called <strong class="bold">active learning</strong>. Not all models in <strong class="source-inline">scikit-learn</strong> support this kind of behavior; so, we are limited to the models that offer a <strong class="source-inline">partial_fit()</strong> method (models without this need to be trained from scratch with new data).</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Scikit-learn <a id="_idIndexMarker1581"/>refers to models implementing the <strong class="source-inline">partial_fit()</strong> method as <strong class="bold">incremental learners</strong>. More information, including which models support this, can be found at <a href="https://scikit-learn.org/stable/computing/scaling_strategies.html#incremental-learning">https://scikit-learn.org/stable/computing/scaling_strategies.html#incremental-learning</a>.</p>
			<p>Our data is currently being rolled up to the minute and then passed to the model, so this will be batch learning, not streaming; however, note that if we were to put this into production, we could update our model each minute, if desired.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor248"/>Creating the PartialFitPipeline subclass</h2>
			<p>We saw in <em class="italic">Chapter 9</em>, <em class="italic">Getting Started with Machine Learning in Python</em>, that the <strong class="source-inline">Pipeline</strong> class <a id="_idIndexMarker1582"/>made streamlining our machine learning processes a cinch, but unfortunately, we can't use it with the <strong class="source-inline">partial_fit()</strong> method. To get around this, we can create our own <strong class="source-inline">PartialFitPipeline</strong> class, which is a subclass of the <strong class="source-inline">Pipeline</strong> class but supports calling <strong class="source-inline">partial_fit()</strong>. The <strong class="source-inline">PartialFitPipeline</strong> class is located in the <strong class="source-inline">ml_utils.partial_fit_pipeline</strong> module.</p>
			<p>We simply <a id="_idIndexMarker1583"/>inherit from <strong class="source-inline">sklearn.pipeline.Pipeline</strong> and define a single new method—<strong class="source-inline">partial_fit()</strong>—which will call <strong class="source-inline">fit_transform()</strong> on all the steps except the last one, and <strong class="source-inline">partial_fit()</strong> on the last step:</p>
			<p class="source-code">from sklearn.pipeline import Pipeline</p>
			<p class="source-code">class PartialFitPipeline(Pipeline):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Subclass of sklearn.pipeline.Pipeline that supports the </p>
			<p class="source-code">    `partial_fit()` method.</p>
			<p class="source-code">    """</p>
			<p class="source-code">    def partial_fit(self, X, y):</p>
			<p class="source-code">        """</p>
			<p class="source-code">        Run `partial_fit()` for online learning estimators </p>
			<p class="source-code">        when used in a pipeline.</p>
			<p class="source-code">        """</p>
			<p class="source-code">        # for all but last step</p>
			<p class="source-code">        for _, step in self.steps[:-1]: # (name, object) tuples</p>
			<p class="source-code">            X = step.fit_transform(X)</p>
			<p class="source-code">        # grab object from tuple position 1 for partial_fit()</p>
			<p class="source-code">        self.steps[-1][1].partial_fit(X, y)</p>
			<p class="source-code">        return self</p>
			<p>Now that <a id="_idIndexMarker1584"/>we have the <strong class="source-inline">PartialFitPipeline</strong> class, the last piece that remains is to select a model capable of online learning.</p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor249"/>Stochastic gradient descent classifier</h2>
			<p>Our logistic regression model performed well—it met the requirements for recall and precision. However, the <strong class="source-inline">LogisticRegression</strong> class does not support online learning <a id="_idIndexMarker1585"/>because the method it uses to calculate the coefficients is a closed-form solution. We have the option of using an optimization algorithm, such as gradient descent, to determine the coefficients instead; this will be capable of online learning.</p>
			<p>Rather than use a different incremental learner, we can train a new logistic regression model with the <strong class="source-inline">SGDClassifier</strong> class. It uses <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) to optimize the loss function of our choice. For this example, we will be using log loss, which gives us a logistic regression where the coefficients are found using SGD.</p>
			<p>Whereas standard gradient descent optimization looks at all the samples or batches to estimate the gradient, SGD reduces the computational cost by selecting samples at random (stochastically). How much the model learns from each sample is determined by the <strong class="bold">learning rate</strong>, with earlier updates having more of an effect than later ones. A single iteration of SGD is carried out as follows:</p>
			<ol>
				<li>Shuffle the training data.</li>
				<li>For each sample in the training data, estimate the gradient and update the model with decreasing strength as determined by the learning rate.</li>
				<li>Repeat <em class="italic">step 2</em> until all samples have been used.</li>
			</ol>
			<p>In machine learning, we use <strong class="bold">epochs</strong> to refer to the number of times the full training set is used. The process <a id="_idIndexMarker1586"/>of SGD we just outlined is for a single epoch. When we train for multiple epochs, we repeat the preceding steps for the desired number of epochs, continuing each time from where we left off.</p>
			<p>Now that <a id="_idIndexMarker1587"/>we understand how SGD works, we are ready to build our model. Here's an overview of the process we will follow before presenting it to the SOC:</p>
			<div>
				<div id="_idContainer473" class="IMG---Figure">
					<img src="image/Figure_11.16_B16834.jpg" alt="Figure 11.16 – Process for preparing our online learning model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.16 – Process for preparing our online learning model</p>
			<p>Let's now turn to the <strong class="source-inline">5-online_learning.ipynb</strong> notebook to build our online learning model.</p>
			<h3>Building our initial model</h3>
			<p>First, we will use the <strong class="source-inline">get_X_y()</strong> function to get our <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> training data using the full year of 2018:</p>
			<p class="source-code">&gt;&gt;&gt; X_2018, y_2018 = get_X_y(logs_2018, <strong class="bold">'2018'</strong>, hackers_2018)</p>
			<p>Since we <a id="_idIndexMarker1588"/>will be updating this model in batches, our test set will always be the data we are using for our current predictions. After we do so, it will become the training set and be used to update the model. Let's build our initial model trained on the 2018 labeled data. Note that the <strong class="source-inline">PartialFitPipeline</strong> object is created in the same way we create a <strong class="source-inline">Pipeline</strong> object:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from sklearn.linear_model import SGDClassifier</strong></p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.partial_fit_pipeline import \</p>
			<p class="source-code">...     PartialFitPipeline</p>
			<p class="source-code">&gt;&gt;&gt; model = <strong class="bold">PartialFitPipeline</strong>([</p>
			<p class="source-code">...     ('scale', StandardScaler()), </p>
			<p class="source-code">...     ('sgd', <strong class="bold">SGDClassifier(</strong></p>
			<p class="source-code">...<strong class="bold">         random_state=10, max_iter=1000, </strong></p>
			<p class="source-code">...<strong class="bold">         tol=1e-3, loss='log', average=1000,</strong></p>
			<p class="source-code">...<strong class="bold">         learning_rate='adaptive', eta0=0.01</strong></p>
			<p class="source-code">...     <strong class="bold">)</strong>)</p>
			<p class="source-code">... ]).fit(X_2018, y_2018)</p>
			<p>Our pipeline will first standardize the data, and then pass it to the model. We start building our model using the <strong class="source-inline">fit()</strong> method so that we have a good starting point for our updates with <strong class="source-inline">partial_fit()</strong> later. The <strong class="source-inline">max_iter</strong> parameter defines the number <a id="_idIndexMarker1589"/>of epochs for the training. The <strong class="source-inline">tol</strong> parameter (tolerance) specifies when to stop iterating, which occurs when the loss from the current iteration is greater than the previous loss minus the tolerance (or we have reached <strong class="source-inline">max_iter</strong> iterations). We specified <strong class="source-inline">loss='log'</strong> to use logistic regression; however, there are many other options for the loss functions, including the default value of <strong class="source-inline">'hinge'</strong> for a linear SVM.</p>
			<p>Here, we also passed in a value for the <strong class="source-inline">average</strong> parameter, telling the <strong class="source-inline">SGDClassifier</strong> object to store the coefficients as averages of the results once 1,000 samples have been seen; note that this parameter is optional and, by default, this won't be calculated. Examining these coefficients can be achieved as follows:</p>
			<p class="source-code">&gt;&gt;&gt; [(col, coef) for col, coef in </p>
			<p class="source-code">...  zip(X_2018.columns, <strong class="bold">model.named_steps['sgd'].coef_[0]</strong>)]</p>
			<p class="source-code">[('usernames_with_failures', 0.9415581997027198),</p>
			<p class="source-code"> ('day_of_week_0', 0.05040751530926895),</p>
			<p class="source-code"> ...,</p>
			<p class="source-code"> ('hour_23', -0.02176726532333003)]</p>
			<p>Lastly, we passed in <strong class="source-inline">eta0=0.01</strong> for our starting learning rate and specified to only adjust the learning rate when we have failed to improve our loss by the tolerance defined <a id="_idIndexMarker1590"/>for a given number of consecutive epochs (<strong class="source-inline">learning_rate='adaptive'</strong>). This number of epochs is defined by the <strong class="source-inline">n_iter_no_change</strong> parameter, which will be <strong class="source-inline">5</strong> (the default), since we aren't setting it explicitly.</p>
			<h3>Evaluating the model</h3>
			<p>Since we now have labeled data for January and February 2019, we can evaluate how the model performs each month. First, we read in the 2019 data from the database:</p>
			<p class="source-code">&gt;&gt;&gt; with sqlite3.connect('logs/logs.db') as conn:</p>
			<p class="source-code">...     logs_2019 = pd.read_sql(</p>
			<p class="source-code">...         """</p>
			<p class="source-code">...         <strong class="bold">SELECT *</strong> </p>
			<p class="source-code">...         <strong class="bold">FROM logs</strong> </p>
			<p class="source-code">...<strong class="bold">         WHERE</strong></p>
			<p class="source-code">...<strong class="bold">             datetime BETWEEN "2019-01-01" AND "2020-01-01";</strong></p>
			<p class="source-code">...         """, </p>
			<p class="source-code">...         conn, parse_dates=['datetime'],</p>
			<p class="source-code">...         index_col='datetime'</p>
			<p class="source-code">...     )</p>
			<p class="source-code">...     hackers_2019 = pd.read_sql(</p>
			<p class="source-code">...         """</p>
			<p class="source-code">...         <strong class="bold">SELECT *</strong> </p>
			<p class="source-code">...         <strong class="bold">FROM attacks</strong> </p>
			<p class="source-code">...         <strong class="bold">WHERE start BETWEEN "2019-01-01" AND "2020-01-01";</strong></p>
			<p class="source-code">...         """, </p>
			<p class="source-code">...         conn, parse_dates=['start', 'end']</p>
			<p class="source-code">...     ).assign(</p>
			<p class="source-code">...         start_floor=lambda x: x.start.dt.floor('min'),</p>
			<p class="source-code">...         end_ceil=lambda x: x.end.dt.ceil('min')</p>
			<p class="source-code">...     )</p>
			<p>Next, we isolate the January 2019 data:</p>
			<p class="source-code">&gt;&gt;&gt; X_jan, y_jan = get_X_y(logs_2019, '2019-01', hackers_2019)</p>
			<p>The classification <a id="_idIndexMarker1591"/>report indicates that this model performs pretty well, but our recall for the positive class is lower than our target:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(y_jan, model.predict(X_jan)))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">       False       1.00      1.00      1.00     44559</p>
			<p class="source-code">        <strong class="bold">True       1.00      0.64      0.78        44</strong></p>
			<p class="source-code">    accuracy                           1.00     44603</p>
			<p class="source-code">   <strong class="bold">macro avg       1.00      0.82      0.89     44603</strong></p>
			<p class="source-code">weighted avg       1.00      1.00      1.00     44603</p>
			<p>Remember, our stakeholders have specified we must achieve a recall (TPR) of at least 70% and a precision of at least 85%. Let's write a function that will show us the ROC curve, confusion matrix, and precision-recall curve and indicate the region we need to be in as well as where we currently are:</p>
			<p class="source-code">&gt;&gt;&gt; from ml_utils.classification import (</p>
			<p class="source-code">...     confusion_matrix_visual, plot_pr_curve, plot_roc</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; def plot_performance(model, X, y, threshold=None, </p>
			<p class="source-code">...                      title=None, show_target=True):</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     Plot ROC, confusion matrix, and precision-recall curve.</p>
			<p class="source-code">...     </p>
			<p class="source-code">...     Parameters:</p>
			<p class="source-code">...         - model: The model object to use for prediction.</p>
			<p class="source-code">...         - X: The features to pass in for prediction.</p>
			<p class="source-code">...         - y: The actuals to evaluate the prediction.</p>
			<p class="source-code">...         - threshold: Value to use as when predicting </p>
			<p class="source-code">...                      probabilities.</p>
			<p class="source-code">...         - title: A title for the subplots.</p>
			<p class="source-code">...         - show_target: Whether to show the target regions.</p>
			<p class="source-code">...         </p>
			<p class="source-code">...     Returns: </p>
			<p class="source-code">...         Matplotlib `Axes` object.</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     fig, axes = plt.subplots(1, 3, figsize=(20, 5))</p>
			<p class="source-code">...     # plot each visualization</p>
			<p class="source-code">...     plot_roc(y, model.predict_proba(X)[:,1], ax=axes[0])</p>
			<p class="source-code">...     confusion_matrix_visual(</p>
			<p class="source-code">...         y, </p>
			<p class="source-code">...         model.predict_proba(X)[:,1] &gt;= (threshold or 0.5), </p>
			<p class="source-code">...         class_labels=[False, True], ax=axes[1]</p>
			<p class="source-code">...     )</p>
			<p class="source-code">...     plot_pr_curve(</p>
			<p class="source-code">...         y, model.predict_proba(X)[:,1], ax=axes[2]</p>
			<p class="source-code">...     )</p>
			<p class="source-code">...</p>
			<p class="source-code">...     # show the target regions if desired</p>
			<p class="source-code">...     if show_target:</p>
			<p class="source-code">...         <strong class="bold">axes[0]\</strong></p>
			<p class="source-code">...             <strong class="bold">.axvspan(0, 0.1, color='lightgreen', alpha=0.5)</strong></p>
			<p class="source-code">...         <strong class="bold">axes[0]\</strong></p>
			<p class="source-code">...             <strong class="bold">.axhspan(0.7, 1, color='lightgreen', alpha=0.5)</strong></p>
			<p class="source-code">...         <strong class="bold">axes[0].annotate(</strong></p>
			<p class="source-code">...             <strong class="bold">'region with acceptable\nFPR and TPR',</strong> </p>
			<p class="source-code">...             <strong class="bold">xy=(0.1, 0.7), xytext=(0.17, 0.65),</strong> </p>
			<p class="source-code">...             <strong class="bold">arrowprops=dict(arrowstyle='-&gt;')</strong></p>
			<p class="source-code">...         <strong class="bold">)</strong></p>
			<p class="source-code">...</p>
			<p class="source-code">...         <strong class="bold">axes[2]\</strong></p>
			<p class="source-code">...             <strong class="bold">.axvspan(0.7, 1, color='lightgreen', alpha=0.5)</strong></p>
			<p class="source-code">...<strong class="bold">         axes[2].axhspan(</strong></p>
			<p class="source-code">...<strong class="bold">             0.85, 1, color='lightgreen', alpha=0.5</strong></p>
			<p class="source-code">...<strong class="bold">         )</strong></p>
			<p class="source-code">...         <strong class="bold">axes[2].annotate(</strong></p>
			<p class="source-code">...             <strong class="bold">'region with acceptable\nprecision and recall',</strong> </p>
			<p class="source-code">...             <strong class="bold">xy=(0.7, 0.85), xytext=(0.3, 0.6),</strong> </p>
			<p class="source-code">...             <strong class="bold">arrowprops=dict(arrowstyle='-&gt;')</strong></p>
			<p class="source-code">...         <strong class="bold">)</strong></p>
			<p class="source-code">...</p>
			<p class="source-code">...         # mark the current performance</p>
			<p class="source-code">...         <strong class="bold">tn, fn, fp, tp = \</strong></p>
			<p class="source-code">...             <strong class="bold">[int(x.get_text()) for x in axes[1].texts]</strong></p>
			<p class="source-code">...         <strong class="bold">precision, recall = tp / (tp + fp), tp / (tp + fn)</strong></p>
			<p class="source-code">...         <strong class="bold">fpr = fp / (fp + tn)</strong></p>
			<p class="source-code">...</p>
			<p class="source-code">...         <strong class="bold">prefix = 'current performance' if not threshold \</strong></p>
			<p class="source-code">...                  <strong class="bold">else f'chosen threshold: {threshold:.2%}'</strong></p>
			<p class="source-code">...         <strong class="bold">axes[0].annotate(</strong></p>
			<p class="source-code">...<strong class="bold">             f'{prefix}\n- FPR={fpr:.2%}'</strong></p>
			<p class="source-code">...<strong class="bold">             f'\n- TPR={recall:.2%}',</strong></p>
			<p class="source-code">...             <strong class="bold">xy=(fpr, recall), xytext=(0.05, 0.45),</strong> </p>
			<p class="source-code">...             <strong class="bold">arrowprops=dict(arrowstyle='-&gt;')</strong></p>
			<p class="source-code">...         <strong class="bold">)</strong></p>
			<p class="source-code">...         <strong class="bold">axes[2].annotate(</strong></p>
			<p class="source-code">...             <strong class="bold">f'{prefix}\n- precision={precision:.2%}'</strong></p>
			<p class="source-code">...             <strong class="bold">f'\n- recall={recall:.2%}',</strong> </p>
			<p class="source-code">...             <strong class="bold">xy=(recall, precision), xytext=(0.2, 0.85),</strong> </p>
			<p class="source-code">...             <strong class="bold">arrowprops=dict(arrowstyle='-&gt;')</strong></p>
			<p class="source-code">...         <strong class="bold">)</strong></p>
			<p class="source-code">...</p>
			<p class="source-code">...     if title: # show the title if specified</p>
			<p class="source-code">...         plt.suptitle(title)</p>
			<p class="source-code">... </p>
			<p class="source-code">...     return axes</p>
			<p>Now, let's <a id="_idIndexMarker1592"/>call the function to see how we are doing:</p>
			<p class="source-code">&gt;&gt;&gt; axes = plot_performance(</p>
			<p class="source-code">...     model, X_jan, y_jan, </p>
			<p class="source-code">...     title='Stochastic Gradient Descent Classifier '</p>
			<p class="source-code">...           '(Tested on January 2019 Data)'</p>
			<p class="source-code">... )</p>
			<p>Notice we are not currently meeting the specifications of our stakeholders; our performance is not in the target region:</p>
			<div>
				<div id="_idContainer474" class="IMG---Figure">
					<img src="image/Figure_11.17_B16834.jpg" alt="Figure 11.17 – Model performance with a default threshold&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.17 – Model performance with a default threshold</p>
			<p>Our resulting recall (TPR) is 63.64%, which doesn't meet the goal of 70% or better. By default, when we use the <strong class="source-inline">predict()</strong> method, our probability threshold is 50%. If we are targeting a specific precision/recall or TPR/FPR region, we may have to change the threshold and use <strong class="source-inline">predict_proba()</strong> to get the desired performance.</p>
			<p>The <strong class="source-inline">ml_utils.classification</strong> module contains the <strong class="source-inline">find_threshold_roc()</strong> and <strong class="source-inline">find_threshold_pr()</strong> functions, which will help us pick a threshold along the ROC curve or precision-recall curve, respectively. Since we are targeting a specific precision/recall region, we will use the latter. This function uses the <strong class="source-inline">precision_recall_curve()</strong> function from <strong class="source-inline">scikit-learn</strong> also, but instead of <a id="_idIndexMarker1593"/>plotting the resulting precision and recall data, we use it to select the thresholds that meet our criteria:</p>
			<p class="source-code">from sklearn.metrics import precision_recall_curve</p>
			<p class="source-code">def find_threshold_pr(y_test, y_preds, *, min_precision,  </p>
			<p class="source-code">                      min_recall):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Find the threshold to use with `predict_proba()` for </p>
			<p class="source-code">    classification based on the minimum acceptable precision </p>
			<p class="source-code">    and the minimum acceptable recall.</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - y_test: The actual labels.</p>
			<p class="source-code">        - y_preds: The predicted labels.</p>
			<p class="source-code">        - min_precision: The minimum acceptable precision.</p>
			<p class="source-code">        - min_recall: The minimum acceptable recall.</p>
			<p class="source-code">    Returns: The thresholds that meet the criteria.</p>
			<p class="source-code">    """</p>
			<p class="source-code">    precision, recall, thresholds = \</p>
			<p class="source-code">        precision_recall_curve(y_test, y_preds)</p>
			<p class="source-code">    # precision and recall have one extra value at the end </p>
			<p class="source-code">    # for plotting -- needs to be removed to make a mask </p>
			<p class="source-code">    return <strong class="bold">thresholds[</strong></p>
			<p class="source-code">        <strong class="bold">(precision[:-1] &gt;= min_precision) &amp;</strong> </p>
			<p class="source-code">        <strong class="bold">(recall[:-1] &gt;= min_recall)</strong></p>
			<p class="source-code">    <strong class="bold">]</strong></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The notebook also shows an example of finding a threshold for a TPR/FPR goal. Our current target precision/recall happens to give the same threshold as targeting a TPR (recall) of at least 70% and an FPR of at most 10%.</p>
			<p>Let's use <a id="_idIndexMarker1594"/>this function to find a threshold that meets our stakeholders' specifications. We take the max of the probabilities that fall in the desired region to pick the least sensitive of the candidate thresholds:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from ml_utils.classification import find_threshold_pr</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">threshold = find_threshold_pr(</strong></p>
			<p class="source-code">...     <strong class="bold">y_jan, model.predict_proba(X_jan)[:,1],</strong> </p>
			<p class="source-code">...     <strong class="bold">min_precision=0.85, min_recall=0.7</strong></p>
			<p class="source-code">... <strong class="bold">).max()</strong></p>
			<p class="source-code">&gt;&gt;&gt; threshold</p>
			<p class="source-code">0.0051533333839830974</p>
			<p>This result tells us that we can reach the desired precision and recall if we flag results that have a 0.52% chance of being in the positive class. No doubt this seems like a very low probability, or that the model isn't sure of itself, but we can think about it this way: if the model thinks there is even a slight chance that the login activity is suspicious, we want to know. Let's see <a id="_idIndexMarker1595"/>how our performance looks using this threshold:</p>
			<p class="source-code">&gt;&gt;&gt; axes = plot_performance(</p>
			<p class="source-code">...     model, X_jan, y_jan, <strong class="bold">threshold=threshold</strong>, </p>
			<p class="source-code">...     title='Stochastic Gradient Descent Classifier '</p>
			<p class="source-code">...           '(Tested on January 2019 Data)'</p>
			<p class="source-code">... )</p>
			<p>This threshold gives us a recall of 70.45%, satisfying our stakeholders. Our precision is in the acceptable range as well:</p>
			<div>
				<div id="_idContainer475" class="IMG---Figure">
					<img src="image/Figure_11.18_B16834.jpg" alt="Figure 11.18 – Model performance using a custom threshold&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.18 – Model performance using a custom threshold</p>
			<p>Using the custom threshold, we have correctly identified another three cases, reducing our false negatives, which are very costly for the SOC. Here, this improvement didn't come at the cost of additional false positives, but remember, there is often a trade-off between <a id="_idIndexMarker1596"/>reducing false negatives (<strong class="bold">type II error</strong>) and reducing false positives (<strong class="bold">type I error</strong>). In some cases, we have a very low tolerance for type I errors (the FPR must be very small), whereas in others, we are more concerned with finding all the positive cases (the TPR must be high). In information security, we have a low tolerance for false negatives because they are very costly; therefore, we will move forward with the custom threshold.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Sometimes, the requirements of a model's performance aren't feasible. It's important to maintain an open line of communication with stakeholders to explain the issues and discuss relaxing criteria when necessary.</p>
			<h3>Updating the model</h3>
			<p>Continuous updating will help the model adapt to changes in hacker behavior over time. Now that <a id="_idIndexMarker1597"/>we have evaluated our January predictions, we can use them to update the model. To do so, we use the <strong class="source-inline">partial_fit()</strong> method and the labeled data for January, which will run a single epoch on the January data:</p>
			<p class="source-code">&gt;&gt;&gt; model.partial_fit(X_jan, y_jan)</p>
			<p>Our model has now been updated, so we can test its performance on the February data now. Let's grab the February data first:</p>
			<p class="source-code">&gt;&gt;&gt; X_feb, y_feb = get_X_y(logs_2019, <strong class="bold">'2019-02'</strong>, hackers_2019)</p>
			<p>February had fewer attacks, but we caught a higher percentage of them (80%):</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(</p>
			<p class="source-code">...     y_feb, model.predict_proba(X_feb)[:,1] &gt;= threshold</p>
			<p class="source-code">... ))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">       False       1.00      1.00      1.00     40248</p>
			<p class="source-code">        <strong class="bold">True       1.00      0.80      0.89        10</strong></p>
			<p class="source-code">    accuracy                           1.00     40258</p>
			<p class="source-code">   <strong class="bold">macro avg       1.00      0.90      0.94     40258</strong></p>
			<p class="source-code">weighted avg       1.00      1.00      1.00     40258</p>
			<p>Let's look <a id="_idIndexMarker1598"/>at the performance plots for February to see how they changed:</p>
			<p class="source-code">&gt;&gt;&gt; axes = plot_performance(</p>
			<p class="source-code">...     model, X_feb, y_feb, threshold=threshold,</p>
			<p class="source-code">...     title='Stochastic Gradient Descent Classifier '</p>
			<p class="source-code">...           '(Tested on February 2019 Data)'</p>
			<p class="source-code">... )</p>
			<p>Notice the area under the precision-recall curve has increased and more of the curve is in the target region:</p>
			<div>
				<div id="_idContainer476" class="IMG---Figure">
					<img src="image/Figure_11.19_B16834.jpg" alt="Figure 11.19 – Model performance after one update&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.19 – Model performance after one update</p>
			<h3>Presenting our results</h3>
			<p>The SOC has finished up the March data. They want us to implement into our model the feedback <a id="_idIndexMarker1599"/>they gave on our February predictions, and then make predictions for the March data for them to review. They will be evaluating our performance on each minute in March, using the classification report, ROC curve, confusion matrix, and precision-recall curve. It's time to put our model to the test.</p>
			<p>First, we need to update our model for the February data:</p>
			<p class="source-code">&gt;&gt;&gt; model.partial_fit(X_feb, y_feb)</p>
			<p>Next, we grab the March data and make our predictions, using a threshold of 0.52%:</p>
			<p class="source-code">&gt;&gt;&gt; X_march, y_march = \</p>
			<p class="source-code">...     get_X_y(logs_2019, '2019-03', hackers_2019)</p>
			<p class="source-code">&gt;&gt;&gt; march_2019_preds = \</p>
			<p class="source-code">...     model.predict_proba(X_march)[:,1] &gt;= threshold</p>
			<p>Our classification report looks good. We have a recall of 76%, a precision of 88%, and a solid F<span class="subscript">1</span> score:</p>
			<p class="source-code">&gt;&gt;&gt; from sklearn.metrics import classification_report</p>
			<p class="source-code">&gt;&gt;&gt; print(classification_report(y_march, march_2019_preds))</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">       False       1.00      1.00      1.00     44154</p>
			<p class="source-code">        <strong class="bold">True       0.88      0.76      0.81        29</strong></p>
			<p class="source-code">    accuracy                           1.00     44183</p>
			<p class="source-code">   <strong class="bold">macro avg       0.94      0.88      0.91     44183</strong></p>
			<p class="source-code">weighted avg       1.00      1.00      1.00     44183</p>
			<p>Now, let's see how the plots look:</p>
			<p class="source-code">&gt;&gt;&gt; axes = plot_performance(</p>
			<p class="source-code">...     model, X_march, y_march, threshold=threshold,</p>
			<p class="source-code">...     title='Stochastic Gradient Descent Classifier '</p>
			<p class="source-code">...           '(Tested on March 2019 Data)'</p>
			<p class="source-code">... )</p>
			<p>Our <a id="_idIndexMarker1600"/>AUC for the ROC curve is slightly higher now, while it dropped for the precision-recall curve:</p>
			<div>
				<div id="_idContainer477" class="IMG---Figure">
					<img src="image/Figure_11.20_B16834.jpg" alt="Figure 11.20 – Model performance after two updates&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.20 – Model performance after two updates</p>
			<h3>Further improvements</h3>
			<p>The SOC is pleased with our results and now wants us to provide predictions each minute. They <a id="_idIndexMarker1601"/>have also promised to provide feedback within an hour. We won't implement this request here, but we will briefly discuss how we could go about this.</p>
			<p>We have been using batch processing to update the model each month; however, in order to provide our stakeholders with what they want, we will need to shorten our feedback loop by performing the following actions:</p>
			<ul>
				<li>Running <strong class="source-inline">predict_proba()</strong> on our model every single minute and having the predictions sent to our stakeholders. This will require setting up a process to pass the logs one minute at a time to a preprocessing function, and then to the model itself.</li>
				<li>Delivering the results to our stakeholders via an agreed-upon medium. </li>
				<li>Updating the model with <strong class="source-inline">partial_fit()</strong> every hour using the feedback we receive from the stakeholders (once we have determined how to have them share this information with us).</li>
			</ul>
			<p>After the <a id="_idIndexMarker1602"/>aforementioned actions are implemented, all that remains is for us to put the model into production and determine the update and prediction frequencies everyone will be accountable for meeting.</p>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor250"/>Summary</h1>
			<p>In practice, detecting attackers isn't easy. Real-life hackers are much savvier than the ones in this simulation. Attacks are also much less frequent, creating a huge class imbalance. Building machine learning models that will catch everything just isn't possible. That is why it is so vital that we work with those who have domain knowledge; they can help us squeeze some extra performance out of our models by really understanding the data and its peculiarities. No matter how experienced we become with machine learning, we should never turn down help from someone who often works with the data in question.</p>
			<p>Our initial attempts at anomaly detection were unsupervised while we waited for the labeled data from our subject matter experts. We tried LOF and isolation forest using <strong class="source-inline">scikit-learn</strong>. Once we received the labeled data and performance requirements from our stakeholders, we determined that the isolation forest model was better for our data.</p>
			<p>However, we didn't stop there. Since we had just been given the labeled data, we tried our hand at supervised methods. We learned how to build baseline models using dummy classifiers and Naive Bayes. Then, we revisited logistic regression to see whether it could help us. Our logistic regression model performed well; however, since it used a closed-form solution to find the coefficients, we were unable to incorporate a feedback loop without retraining the model from scratch.</p>
			<p>This limitation led us to build an online learning model, which is constantly updated. First, we had to make a subclass to allow pipelines to use the <strong class="source-inline">partial_fit()</strong> method. Then, we tried SGD classification with log loss. We were able to train on an entire year of data at once, and then update our model when we received new labeled data. This allows the model to adjust to changes in the distributions of the features over time.</p>
			<p>In the next chapter, we will recap what we have learned throughout the book and introduce additional resources for finding data, as well as working with it in Python.</p>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor251"/>Exercises</h1>
			<p>Complete the following exercises for some practice with the machine learning workflow and exposure to some additional anomaly detection strategies:</p>
			<ol>
				<li value="1">A one-class SVM is another model that can be used for unsupervised outlier detection. Build a one-class SVM with the default parameters, using a pipeline with a <strong class="source-inline">StandardScaler</strong> object followed by a <strong class="source-inline">OneClassSVM</strong> object. Train the model on the January 2018 data, just as we did for the isolation forest. Make predictions on that same data. Count the number of inliers and outliers this model identifies.</li>
				<li>Using the 2018 minutely data, build a k-means model with two clusters after standardizing the data with a <strong class="source-inline">StandardScaler</strong> object. With the labeled data in the <strong class="source-inline">attacks</strong> table in the SQLite database (<strong class="source-inline">logs/logs.db</strong>), see whether this model gets a good Fowlkes-Mallows score (use the <strong class="source-inline">fowlkes_mallows_score()</strong> function in <strong class="source-inline">sklearn.metrics</strong>).</li>
				<li>Evaluate the performance of a random forest classifier for supervised anomaly detection. Set <strong class="source-inline">n_estimators</strong> to <strong class="source-inline">100</strong> and use the remaining defaults, including the prediction threshold. Train on January 2018 and test on February 2018.</li>
				<li>The <strong class="source-inline">partial_fit()</strong> method isn't available with the <strong class="source-inline">GridSearchCV</strong> class. Instead, we can use its <strong class="source-inline">fit()</strong> method with a model that has a <strong class="source-inline">partial_fit()</strong> method (or a <strong class="source-inline">PartialFitPipeline</strong> object) to find the best hyperparameters in our search space. Then, we can grab the best model from the grid search (<strong class="source-inline">best_estimator_</strong>) and use <strong class="source-inline">partial_fit()</strong> on it. Try this with the <strong class="source-inline">PassiveAggressiveClassifier</strong> class from the <strong class="source-inline">sklearn.linear_model</strong> module and a <strong class="source-inline">PartialFitPipeline</strong> object. This online learning classifier is passive when it makes a correct prediction, but aggressive in correcting itself when it makes an incorrect prediction. Don't worry about selecting a custom threshold. Be sure to follow these steps:<p>a) Run a grid search using the January 2018 data for the initial training.</p><p>b) Grab the tuned model with the <strong class="source-inline">best_estimator_</strong> attribute.</p><p>c) Evaluate the best estimator with the February 2018 data.</p><p>d) Make updates with the February 2018 data.</p><p>e) Evaluate the final model on March through June 2018 data.</p></li>
			</ol>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor252"/>Further reading</h1>
			<p>Check out the following resources for more information on the topics covered in this chapter:</p>
			<ul>
				<li><em class="italic">Deploying scikit-learn Models at Scale</em>: <a href="https://towardsdatascience.com/deploying-scikit-learn-models-at-scale-f632f86477b8">https://towardsdatascience.com/deploying-scikit-learn-models-at-scale-f632f86477b8</a></li>
				<li><em class="italic">Local Outlier Factor for Anomaly Detection</em>: <a href="https://towardsdatascience.com/local-outlier-factor-for-anomaly-detection-cc0c770d2ebe">https://towardsdatascience.com/local-outlier-factor-for-anomaly-detection-cc0c770d2ebe</a></li>
				<li><em class="italic">Model Persistence</em> (<em class="italic">from the scikit-learn user guide</em>): <a href="https://scikit-learn.org/stable/modules/model_persistence.html">https://scikit-learn.org/stable/modules/model_persistence.html</a></li>
				<li><em class="italic">Novelty and Outlier Detection</em> (<em class="italic">from the </em><em class="italic">scikit-learn user guide</em>): <a href="https://scikit-learn.org/stable/modules/outlier_detection.html">https://scikit-learn.org/stable/modules/outlier_detection.html</a></li>
				<li><em class="italic">Naive Bayes</em> (<em class="italic">from the scikit-learn user guide</em>): <a href="https://scikit-learn.org/stable/modules/naive_bayes.html">https://scikit-learn.org/stable/modules/naive_bayes.html</a></li>
				<li><em class="italic">Outlier Detection with Isolation Forest</em>: <a href="https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e">https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e</a></li>
				<li><em class="italic">Passive Aggressive Algorithm</em> (<em class="italic">video explanation</em>): <a href="https://www.youtube.com/watch?v=uxGDwyPWNkU">https://www.youtube.com/watch?v=uxGDwyPWNkU</a></li>
				<li><em class="italic">Python Context Managers and the "with" Statement</em>: <a href="https://blog.ramosly.com/python-context-managers-and-the-with-statement-8f53d4d9f87">https://blog.ramosly.com/python-context-managers-and-the-with-statement-8f53d4d9f87</a></li>
				<li><em class="italic">Seeing Theory – </em><a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a><em class="italic">, Bayesian Inference</em>: <a href="https://seeing-theory.brown.edu/index.html#secondPage/chapter5">https://seeing-theory.brown.edu/index.html#secondPage/chapter5</a></li>
				<li><em class="italic">SQLAlchemy — Python Tutorial</em>: <a href="https://towardsdatascience.com/sqlalchemy-python-tutorial-79a577141a91">https://towardsdatascience.com/sqlalchemy-python-tutorial-79a577141a91</a></li>
				<li><em class="italic">Stochastic Gradient Descent</em> (<em class="italic">from the scikit-learn user guide</em>): <a href="https://scikit-learn.org/stable/modules/sgd.html">https://scikit-learn.org/stable/modules/sgd.html</a></li>
				<li><em class="italic">Strategies to scale computationally: bigger data</em> (<em class="italic">from the scikit-learn user guide</em>): <a href="https://scikit-learn.org/stable/computing/scaling_strategies.html">https://scikit-learn.org/stable/computing/scaling_strategies.html</a></li>
				<li><em class="italic">Unfair Coin Bayesian Simulation</em>: <a href="https://github.com/xofbd/unfair-coin-bayes">https://github.com/xofbd/unfair-coin-bayes</a></li>
			</ul>
		</div>
	</body></html>