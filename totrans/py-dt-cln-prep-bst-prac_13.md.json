["```py\npip install torchvision\npip install keras==3.4.1\npip install tensorflow==2.17.0\npip install opencv-python==4.10.0.84\npip install opencv-python==4.10.0.84\npip install paddleocr==2.8.1\npip install paddlepaddle==2.6.1\n```", "```py\n    from PIL import Image\n    import numpy as np\n    import cv2\n    import requests\n    from io import BytesIO\n    import matplotlib.pyplot as plt\n    import tensorflow as tf\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    ```", "```py\n    def load_image_from_url(url):\n        response = requests.get(url)\n        img = Image.open(BytesIO(response.content))\n        return img\n    ```", "```py\n    def show_image(image, title=\"Image\"):\n        plt.imshow(image)\n        plt.title(title)\n        plt.axis('off')\n        plt.show()\n    ```", "```py\n    image_url = \"https://images.unsplash.com/photo-1593642532871-8b12e02d091c\"\n    image = load_image_from_url(image_url)\n    ```", "```py\n    show_image(image, \"Original Image\")\n    ```", "```py\ndef resize_and_crop(image, target_size):\n    image = image.resize((target_size, target_size),\n    Image.LANCZOS)\n    return image\ntarget_size = 256\nprocessed_image = resize_and_crop(image, target_size)\n```", "```py\nshow_image(processed_image, \"Resized and Cropped Image\")\n```", "```py\ndef normalize(image):\n    image_array = np.array(image)\n    normalized_array = image_array / 255.0\n    return normalized_array\nnormalized_image = normalize(processed_image)\n```", "```py\ndef standardize(image):\n    image_array = np.array(image)\n    mean = np.mean(image_array, axis=(0, 1), keepdims=True)\n    std = np.std(image_array, axis=(0, 1), keepdims=True)\n    standardized_array = (image_array - mean) / std\n    return standardized_array\nstandardized_image = standardize(processed_image)\n```", "```py\nMean after standardization: 0.0\nStandard deviation after standardization: 1.000000000000416\n```", "```py\n    datagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    ```", "```py\n    def augment_image(image):\n        image = np.expand_dims(image, axis=0) # Add batch dimension\n        augmented_iter = datagen.flow(image, batch_size=1)\n        augmented_image = next(augmented_iter)[0]\n        return augmented_image\n    augmented_image = augment_image(normalized_image)\n    show_image(augmented_image, \"Augmented Image\")\n    ```", "```py\ndef gaussian_blur(image):\n    blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n    return blurred_image\n```", "```py\nblurred_image = gaussian_blur(noisy_image)\nshow_image(blurred_image, \"Gaussian Blur\")\n```", "```py\ndef bilateral_filter(image):\n    image_uint8 = (image * 255).astype(np.uint8)\n    filtered_image = cv2.bilateralFilter(\n        image_uint8, 9, 75, 75)\n    filtered_image = filtered_image / 255.0\n    return filtered_image\n```", "```py\nbilateral_filtered_image = bilateral_filter(noisy_image)\nshow_image(bilateral_filtered_image, \"Bilateral Filter\")\n```", "```py\ndef remove_noise(image):\n    image_uint8 = (image * 255).astype(np.uint8)\n    denoised_image = cv2.fastNlMeansDenoisingColored(\n        image_uint8, None, h=10, templateWindowSize=7,\n        searchWindowSize=21)\n    denoised_image = denoised_image / 255.0\n    return denoised_image\ndenoised_image = remove_noise(noisy_image)\nshow_image(denoised_image, \"Non-Local Means Denoising\")\n```", "```py\ndef perform_median_blur(image):\n    image_uint8 = (image * 255).astype(np.uint8)\n    #The parameter below specifies the size of the kernel (5x5).\n    blurred_image = cv2.medianBlur(image_uint8, 5)\n    blurred_image = blurred_image / 255.0\n    return blurred_image\n```", "```py\nmedian_blurred_image = median_blur(noisy_image)\nshow_image(median_blurred_image, \"Median Blur\")\n```", "```py\ndef add_salt_and_pepper_noise(image, salt_prob=0.02, pepper_prob=0.02):\n    noisy_image = np.copy(image)\n    num_salt = np.ceil(salt_prob * image.size)\n    coords = [np.random.randint(0, i - 1, int(num_salt)) for i in image.shape]\n     noisy_image[coords[0], coords[1], :] = 1\n     num_pepper = np.ceil(pepper_prob * image.size)\n    coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in image.shape]\n     noisy_image[coords[0], coords[1], :] = 0\n    return noisy_image\n```", "```py\nuse_salt_and_pepper_noise = True\nif use_salt_and_pepper_noise:\nnoisy_image = add_salt_and_pepper_noise(tensor_to_image(tensor_image))\nshow_image(noisy_image, \"Salt-and-Pepper Noisy Image\")\n```", "```py\n    ocr = PaddleOCR(use_angle_cls=True, lang='en')\n    ```", "```py\n    folder_path = 'chapter13/images'\n    ```", "```py\n    supported_extensions = ('.png', '.jpg', '.jpeg')\n    ```", "```py\n    image_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.lower().endswith(supported_extensions)]\n    ```", "```py\n    df = pd.DataFrame(columns=['Image Path', 'Extracted Text'])\n    ```", "```py\n    if not image_paths:\n        print(\"No images found in the specified folder.\")\n    else:\n        for image_path in image_paths:\n            process_image function. This function processes images and extracts text. For each thumbnail image, the function will extract any visible text, such as titles, keywords, or promotional phrases:\n\n    ```", "```py\n\n    The `process_image` function performs OCR on an image specified by `image_path`. It starts by invoking the `ocr` method from the `PaddleOCR` library, which processes the image and returns the recognized text along with other details. The function initializes an empty string, `extracted_text`, to accumulate the recognized text. It then iterates through each line of text detected by the OCR process, appending each line to `extracted_text` along with a space for separation. After processing the entire image, it prints the accumulated text along with the filename of the image. Finally, the function adds a new entry to a DataFrame called `df`, storing `image_path` and the corresponding `extracted_text` in a new row, thus updating the DataFrame with the latest OCR results.\n    ```", "```py\n    df.to_csv('extracted_texts.csv', index=False)\n    ```", "```py\n    df = pd.read_csv('extracted_texts.csv')\n    ```", "```py\n    model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\n    ```", "```py\n    api_token = PromptTemplate class, which helps create a prompt for the model:\n\n    ```", "```py\n\n    ```", "```py\n    huggingface_llm = HuggingFaceHub(repo_id=model_name, huggingfacehub_api_token=api_token, model_kwargs={\"task\": \"text-generation\"})\n    ```", "```py\n    llm_chain = LLMChain(prompt=prompt_template, llm=huggingface_llm)\n    ```", "```py\n    response = llm_chain.invoke(text)\n    ```", "```py\n    df['Corrected Text'] = df['Extracted Text'].apply(correct_text)\n    ```", "```py\nOriginal: HOW TO MITIGATE SaCURITY RISKS IN AI AND ML SYSTEM VECTOR LAB\nCorrected: how to mitigate security risks in ai and ml system vector lab\nOriginal: BUILDING DBRX-CLASS CUSTOM LLMS WITH MOSAIC A1 TRAINING VECTOR LAB\nCorrected: building dbrx-class custom llms with mosaic a1 training vector lab\nOriginal: MPROVING TeXT2SO L PeRFORMANCe WITH EASE ON DATABRICKS 7 VECTOR LAB\nCorrected: improving text2so l means text2sql, which is challenging for the model to fix unless it is fine-tuned on this type of correction data. Another approach you could try is to include these very technical cases that the model seems to miss in the few-shot examples in the prompt to “teach” the model how to interpret these words.\n\t\t\tIn the code file on the GitHub repository, you’ll see that we have added error handling and parsing for the JSON output. This is necessary because we are asking the model to return the output in a specific format, but LLMs do not always follow these instructions precisely. There is currently ongoing work on enforcing the output of LLMs in a specific format, but at this point, it is experimental. You can find more information here: [https://python.langchain.com/v0.1/docs/integrations/llms/lmformatenforcer_experimental/](https://python.langchain.com/v0.1/docs/integrations/llms/lmformatenforcer_experimental/).\n\t\t\tAs of now, we have seen how we can use OCR to extract text from images and then fix the extracted text by passing it to an LLM. In the case of a thumbnail image, this extracted text can also be used as an image caption, as the video’s title is usually depicted on the image. However, there are cases where the image doesn’t contain any text, and we need to ask the model to infer the caption based on what it has seen and understood from the image. This will be the point of discussion for the next part.\n\t\t\tCreating image captions\n\t\t\tCreating accurate and meaningful captions for images involves not only recognizing and interpreting visual content but also understanding context and generating descriptive text that accurately reflects the image’s content. The complexity arises from the need for models to process various elements in an image, such as objects, scenes, and activities, and then translate these elements into coherent and relevant language. This challenge is further compounded by the diverse and often subtle nature of visual information, which can include different lighting conditions, angles, and contexts.\n\t\t\tTo showcase the difference between the technique we demonstrated earlier, and the captioning based on image understanding, we will use the same images from the thumbnails and attempt to create captions for them based on the image understanding process instead of the text extraction process. You can find the code for this part here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/4.image_captioning.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/4.image_captioning.py).\n\t\t\tNow, let’s dive into the code:\n\n\t\t\t\t1.  Let’s start by importing the libraries we’ll need for this example:\n\n    ```", "```py\n\n    \t\t\t\t2.  We then define the folder containing the images:\n\n    ```", "```py\n\n    \t\t\t\t3.  Next, we make a list of supported image extensions:\n\n    ```", "```py\n\n    \t\t\t\t4.  We then get all image paths for each image in the folder:\n\n    ```", "```py\n\n    \t\t\t\t5.  We create an empty DataFrame to store the results:\n\n    ```", "```py\n\n    \t\t\t\t6.  Then, we initialize the BLIP model and processor for image captioning:\n\n    ```", "```py\n\n    The `BlipForConditionalGeneration` model is a pretrained model designed for image captioning. It generates descriptive text (captions) for given images by understanding the visual content and producing coherent and relevant descriptions. The model is based on the BLIP architecture, which is optimized for linking visual and textual information. `BlipProcessor` is responsible for preparing images and text inputs in a format suitable for the BLIP model. It handles the preprocessing of images (such as resizing and normalization) and any required text formatting to ensure that the data fed into the model is in the correct format.\n\n    \t\t\t\t7.  Now, we initialize the LLM for text refinement. Once we create the caption with the BLIP model, we will then pass it to an LLM again to clean and optimize the caption:\n\n    ```", "```py\n\n    This piece of code specifies the name of the pretrained model to be used. Here, `\"google/flan-t5-small\"` refers to a specific version of the T5 model, called `FLAN-T5 Small`, developed by Google. The `AutoTokenizer` class from Hugging Face’s Transformers library is used to load the tokenizer associated with the specified model. As we learned in [*Chapter 12*](B19801_12.xhtml#_idTextAnchor277), *Text Preprocessing in the Era of LLMs*, tokenizers are responsible for converting raw text into token IDs that the model can understand. They handle tasks such as tokenizing (splitting text into manageable units), adding special tokens, and encoding the text in a format suitable for model input. Finally, it loads the `google/flan-t5-small sequence-to-sequence` language model, which is suitable for tasks such as translation, summarization, or any task where the model needs to generate text based on some input text. The model has been pretrained on a large dataset, enabling it to understand and generate human-like text, and it is perfect for our use case of caption generation.\n\n    \t\t\t\t8.  Next, we need to chain all our steps together and we will use functionality from LangChain to do so:\n\n    ```", "```py\n\n    The `PromptTemplate` object, which is used to define how prompts (input requests) are structured for the language model is created here. Here, we need a much simpler prompt than the one in the previous example as the task is simpler to explain to the model. This instructs the model to refine and correct the provided caption. The `{text}` placeholder will be replaced with the actual text that needs refinement. Then, an instance of `HuggingFaceHub` is created and finally, we create the LLMChain to connect the prompt with the language model.\n\n    \t\t\t\t9.  We create a `refine_caption` function that accepts a generated caption as input and creates a prompt by formatting `prompt_template` with the input caption. It then uses `llm_chain` to run the prompt through the LLM, generating a refined caption, and it returns the refined caption:\n\n    ```", "```py\n\n    \t\t\t\t10.  We then create the `generate_caption` function, which accepts an image path as input:\n\n    ```", "```py\n\n    This function performs the following:\n\n    *   The function opens the image file and converts it to RGB format.\n    *   It then processes the image using `blip_processor`, returning a tensor suitable for the BLIP model.\n    *   The function generates a caption by passing the processed image to the BLIP model. It finally decodes the model’s output into a human-readable caption, skipping special tokens, and returns the caption.\t\t\t\t11.  Finally, we process each image in the folder, generate an image caption, refine it, and append the final result to a DataFrame:\n\n    ```", "```py\n\n\t\t\tLet’s have a look at the captions generated by this process:\n\t\t\t![Figure 13.17 – Image caption creation](img/B19801_13_18.jpg)\n\n\t\t\tFigure 13.17 – Image caption creation\n\t\t\tAs we can see, this caption is poor compared to the previous method we demonstrated. The model attempts to understand what is happening in the image and grasp the context, but since the context is derived from a thumbnail, it ends up being quite inadequate.  We need to understand that thumbnails often provide limited context about the video content; while they are designed to attract clicks, they may not convey enough information for the model to generate informative captions. The lack of context in combination with the fact that thumbnails are frequently visually cluttered with various images, graphics, and text elements makes it challenging for the model to discern the main subject or context. This complexity can lead to captions that are less coherent or relevant than we have experienced. So, in the case of dealing with thumbnails, the OCR process is best.\n\t\t\tHowever, in cases where images do not contain text, unlike thumbnails that are often filled with written elements, the image understanding process becomes the primary method for generating captions. Since these images lack textual information, relying on the model’s visual understanding is essential for creating accurate and meaningful descriptions. Here is some homework for you: Pass through the BLIP process an image that has no text and see what you get!\n\t\t\tBut what about videos?\n\t\t\tTo handle videos, the process involves reading the video file and capturing frames at specified intervals, allowing us to analyze each frame, so, *each image*, independently. Once we have the frames, we can apply techniques like those used for images, such as OCR for text extraction, or image understanding models, such as BLIP, for caption generation.\n\t\t\tNext, we will move from image to audio data and discuss how we can simplify the audio processing.\n\t\t\tHandling audio data\n\t\t\tA lot of work is happening in the audio processing space with the most significant advancements happening in **automatic speech recognition** (**ASR**) models. These models transform spoken language into written text, allowing the seamless integration of voice inputs into text-based workflows, thereby making it easier to analyze, search, and interact with. For instance, voice assistants, such as Siri and Google Assistant, rely on ASR to understand and respond to user commands, while transcription services convert meeting recordings into searchable text documents.\n\t\t\tThis conversion allows the passing of text input to LLMs to unlock powerful capabilities, such as sentiment analysis, topic modeling, automated summarization, and even supporting chat applications. For example, customer service call centers can use ASR to transcribe conversations, which can then be analyzed for customer sentiment or common issues, improving service quality and efficiency.\n\t\t\tHandling audio data as text not only enhances accessibility and usability but also facilitates more efficient data storage and retrieval. Text data takes up less space than audio files and is easier to index and search. Moreover, it bridges the gap between spoken and written communication, enabling more natural and intuitive user interactions across various platforms and devices. For instance, integrating ASR in educational apps can help students with disabilities access spoken content in a text format, making learning more inclusive.\n\t\t\tAs ASR technologies continue to improve, the ability to accurately and efficiently convert audio to text will become increasingly important, driving innovation and expanding the potential of AI-driven solutions. Enhanced ASR models will further benefit areas such as real-time translation services, automated note-taking in professional settings, and accessibility tools for individuals with hearing impairments, showcasing the broad and transformative impact of this technology.\n\t\t\tIn the next section, we will discuss the Whisper model, which is effective for transforming audio into text and performing a range of audio processing tasks.\n\t\t\tUsing Whisper for audio-to-text conversion\n\t\t\tThe **Whisper model** from OpenAI is a powerful tool for transforming audio to text and serves as a base for many modern AI and ML applications. The applications range from real-time transcription and customer service to healthcare and education, showcasing its versatility and importance in the evolving landscape of audio processing technology:\n\n\t\t\t\t*   Whisper can be integrated into voice assistant systems, such as Siri, Google Assistant, and Alexa, to accurately transcribe user commands and queries.\n\t\t\t\t*   Call centers can use Whisper to transcribe customer interactions, allowing for sentiment analysis, quality assurance, and topic detection, thereby enhancing service quality.\n\t\t\t\t*   Platforms such as YouTube and podcast services can use Whisper to generate subtitles and transcriptions, improving accessibility and content indexing.\n\t\t\t\t*   Whisper can be used in real-time transcription services for meetings, lectures, and live events. This helps create accurate text records that are easy to search and analyze later.\n\t\t\t\t*   In telemedicine, Whisper can transcribe doctor-patient conversations accurately, facilitating better record-keeping and analysis. Moreover, it can assist in creating automated medical notes from audio recordings.\n\t\t\t\t*   Educational platforms can use Whisper to transcribe lectures and tutorials, providing students with written records of spoken content, enhancing learning and accessibility.\n\t\t\t\t*   Security systems use direct audio processing to verify identity based on unique vocal characteristics, offering a more secure and non-intrusive method of authentication.\n\n\t\t\tAs a pretrained model, Whisper can be used out of the box for many tasks, reducing the need for extensive fine-tuning and allowing for quick integration into various applications. The model supports multiple languages, making it versatile for global applications and diverse user bases. While Whisper primarily focuses on transforming audio to text, it also benefits from advancements in handling audio signals, potentially capturing nuances, such as tone and emotion. Although direct audio processing (such as emotion detection or music analysis) might require additional specialized models, Whisper’s robust transcription capability is foundational for many applications.\n\t\t\tUsing some audio from the `@VectorLab`) videos, we will parse the audio through Whisper to get the extracted text.\n\t\t\tExtracting text from audio\n\t\t\tThe following code demonstrates how to use the Whisper model from Hugging Face to transcribe audio files into text. It covers loading necessary libraries, processing an audio file, generating a transcription using the model, and finally decoding and printing the transcribed text. Let’s have a look at the code, which you can also find here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/5.whisper.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/5.whisper.py).\n\t\t\tLet’s begin:\n\n\t\t\t\t1.  We’ll start by importing the required libraries:\n\n    ```", "```py\n\n    \t\t\t\t2.  We start by loading the Whisper processor and model from Hugging Face:\n\n    ```", "```py\n\n    \t\t\t\t3.  Next, we define the path to your audio file:\n\n    ```", "```py\n\n    You can replace this file with any other audio you want.\n\n    \t\t\t\t4.  Then, we load the audio file:\n\n    ```", "```py\n\n    5.  `rate` is the sampling rate of the audio file. The `sr=16000` argument resamples the audio to a sampling rate of 16 kHz, which is the required input sampling rate for the Whisper mode.\t\t\t\t6.  Now, we preprocess the audio file for the Whisper model:\n\n    ```", "```py\n\n    \t\t\t\t7.  We then generate the transcription:\n\n    ```", "```py\n\n    This line passes the preprocessed audio features to the model to generate transcription IDs. The model produces token IDs that correspond to the transcribed text.\n\n    \t\t\t\t8.  Now, we decode the generated transcription:\n\n    ```", "```py\n\n    This line decodes the predicted token IDs back into readable text. The `[0]` value at the end extracts the first (and only) transcription from the resulting list.\n\n    \t\t\t\t9.  Finally, we print the transcribed text:\n\n    ```", "```py\n\n\t\t\tIs the transcription slow?\n\t\t\tDepending on the model size and your hardware capabilities, the transcription process might take some time.\n\t\t\tAs we can see, the transcription is excellent. Now, in the use case we are dealing with, after transcribing the YouTube video, there are several valuable actions you can take on this project. First, you can create captions or subtitles to improve accessibility for viewers who are deaf or hard of hearing. Additionally, writing a summary or extracting key points can help viewers grasp the main ideas without watching the entire video. The transcription can also be transformed into a blog post or article, providing more context on the topic discussed. Extracting quotes or highlights from the transcription allows you to create engaging social media posts that promote the video. Utilizing the transcription for SEO purposes can improve the video’s search engine ranking by including relevant keywords in the description. You can also develop FAQs or discussion questions based on the video to encourage viewer engagement. Additionally, the transcription can serve as a reference for research, and you might consider adapting it into a script for an audiobook or podcast. Incorporating the transcription into educational materials, such as lesson plans, is another effective way to utilize the content. Lastly, you can create visual summaries or infographics based on the key points to present the main ideas visually. How cool is that?\n\t\t\tIn the following section, we will expand the use case and do some emotion detection from the transcribed text.\n\t\t\tDetecting emotions\n\t\t\tEmotion detection from text, often referred to as sentiment analysis or emotion recognition, is a subfield of **natural language processing** (**NLP**) that focuses on identifying and classifying emotions conveyed in written content. This area of study has gained significant traction due to the growing amount of textual data generated across social media, customer feedback, and other platforms.\n\t\t\tIn our case, we will use the `j-hartmann/emotion-english-distilroberta-base` model, built upon the DistilRoBERTa architecture. The DistilRoBERTa model is a smaller and faster variant of the RoBERTa model, which itself is based on the Transformer architecture. This model is specifically fine-tuned for emotion detection tasks. It has been trained on a dataset designed to recognize various emotions expressed in text, making it adept at identifying and classifying emotions from written content. It is designed to detect the following emotions from text:\n\n\t\t\t\t*   **Joy**: This represents happiness and positivity\n\t\t\t\t*   **Sadness**: This reflects feelings of sorrow and unhappiness\n\t\t\t\t*   **Anger**: This indicates feelings of frustration, annoyance, or rage\n\t\t\t\t*   **Fear**: This conveys feelings of anxiety or apprehension\n\t\t\t\t*   **Surprise**: This represents astonishment or unexpectedness\n\t\t\t\t*   **Disgust**: This reflects feelings of aversion or distaste\n\t\t\t\t*   **Neutral**: This indicates a lack of strong emotion or feeling\n\n\t\t\tThese emotions are typically derived from various datasets that categorize text based on emotional expressions, allowing the model to classify input text into these predefined categories.\n\t\t\tLet’s have a look at the code, which is also available here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/6.emotion_detection.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/6.emotion_detection.py).\n\t\t\tMemory check\n\t\t\tThe following code is memory intensive, so you may need to allocate more memory if working on virtual machines or Google Collab. The code was tested on Mac M1, 16 GB memory.\n\t\t\tLet’s start coding:\n\n\t\t\t\t1.  We first import the libraries required for this example:\n\n    ```", "```py\n\n    \t\t\t\t2.  We then load the Whisper processor and model from Hugging Face:\n\n    ```", "```py\n\n    \t\t\t\t3.  Then, we load the emotion detection processor and model from Hugging Face:\n\n    ```", "```py\n\n    \t\t\t\t4.  We define the path to your audio file:\n\n    ```", "```py\n\n    \t\t\t\t5.  Once the path is defined, we load the audio file:\n\n    ```", "```py\n\n    \t\t\t\t6.  We create a function called `split_audio` to split audio into chunks:\n\n    ```", "```py\n\n    \t\t\t\t7.  We also create a function to transcribe audio using Whisper:\n\n    ```", "```py\n\n    The function preprocesses the audio file for the Whisper model and generates the transcription. Once it’s generated, the function decodes the generated transcription.\n\n    \t\t\t\t8.  We then create a function to detect emotions from text using the emotion detection model:\n\n    ```", "```py\n\n    This function begins by tokenizing the input text with `emotion_tokenizer`, converting it into PyTorch tensors while handling padding, truncation, and maximum length constraints. The tokenized input is then fed into `emotion_model`, which generates raw prediction scores (logits) for various emotion classes. The function identifies the emotion with the highest score using `torch.argmax` to determine the class ID. This ID is then mapped to the corresponding emotion label through the `id2label` dictionary provided by the model’s configuration. Finally, the function returns the detected emotion as a readable label!\n\n    \t\t\t\t9.  Then, we split the audio into chunks:\n\n    ```", "```py\n\n    \t\t\t\t10.  We also create a DataFrame to store the results:\n\n    ```", "```py\n\n    \t\t\t\t11.  Finally, we process each audio chunk:\n\n    ```", "```py\n\n\t\t\tThe output emotions are shown for each chunk of transcribed text, and in our case, all are neutral, as the video is just a teaching concept video:\n\n```", "```py\n\n\t\t\tNow, we will expand our use case a bit further to demonstrate how you can take the transcribed text and pass it through an LLM to create highlights for the YouTube video.\n\t\t\tAutomatically creating video highlights\n\t\t\tIn the era of digital content consumption, viewers often seek concise and engaging summaries of longer videos. Automatically creating video highlights involves analyzing video content and extracting key moments that capture the essence of the material. This process saves time and improves content accessibility, making it a valuable tool for educators, marketers, and entertainment providers alike.\n\t\t\tLet’s have a look at the code. You can find it at the following link: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/7.write_highlights.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter13/7.write_highlights.py).\n\t\t\tIn this code, we will expand the Whisper example. We will transcribe the text, then join all the transcribed chunks together, and finally, we will pass all the transcriptions to the LLM to create the highlights for the entire video. Let’s continue the previous example:\n\n\t\t\t\t1.  We start by initializing the Hugging Face model:\n\n    ```", "```py\n\n    \t\t\t\t2.  Then, we add your Hugging Face API token:\n\n    ```", "```py\n\n    \t\t\t\t3.  Here’s the LangChain setup that we’ll be using in this use case. Notice the new prompt that we added:\n\n    ```", "```py\n\n    \t\t\t\t4.  Next, we generate the transcription:\n\n    ```", "```py\n\n    \t\t\t\t5.  Then, we create a function to generate the key highlights from text using the LLM:\n\n    ```", "```py\n\n    \t\t\t\t6.  Next, we split the audio into chunks:\n\n    ```", "```py\n\n    \t\t\t\t7.  We then transcribe each audio chunk:\n\n    ```", "```py\n\n    \t\t\t\t8.  Then, we join all transcriptions into a single text:\n\n    ```", "```py\n\n    \t\t\t\t9.  Finally, we generate highlights from the full transcription:\n\n    ```", "```py\n\n\t\t\tLet’s see the automatically created highlights:\n\n```", "```py\n\n\t\t\tAs we can see, there are some minor mistakes, mainly coming from the Whisper process, but other than that, it is actually pretty good.\n\t\t\tIn the next part, we will quickly review the research happening in the audio space, as it is a rapidly evolving field.\n\t\t\tFuture research in audio preprocessing\n\t\t\tThere is a growing trend toward the development of multimodal LLMs capable of processing various types of data, including audio. Currently, many language models are primarily text-based, but we anticipate the emergence of models that can handle text, images, and audio simultaneously. These multimodal LLMs have diverse applications, such as generating image captions and providing medical diagnoses based on patient reports. Research is underway to extend LLMs to support direct speech inputs. As noted, “Several studies have attempted to extend LLMs to support direct speech inputs with a connection module” ([https://arxiv.org/html/2406.07914v2](https://arxiv.org/html/2406.07914v2)), indicating ongoing efforts to incorporate audio processing capabilities into LLMs. Although not only relevant to audio, LLMs face several challenges with other data types, including the following:\n\n\t\t\t\t*   High computational resources required for processing\n\t\t\t\t*   Data privacy and security concerns\n\n\t\t\tResearchers are actively exploring various strategies to overcome these challenges. To address the high computational demands, there is a focus on developing more efficient algorithms and architectures, such as transformer models with reduced parameter sizes and optimized training techniques. Techniques such as model compression, quantization, and distillation are being employed to make these models more resource-efficient without sacrificing performance ([https://arxiv.org/abs/2401.13601](https://arxiv.org/abs/2401.13601), [https://arxiv.org/html/2408.04275v1](https://arxiv.org/html/2408.04275v1), [https://arxiv.org/html/2408.01319v1](https://arxiv.org/html/2408.01319v1)). In terms of data privacy and security, researchers are investigating privacy-preserving ML techniques, including federated learning and differential privacy. These approaches aim to protect sensitive data by allowing models to learn from decentralized data sources without exposing individual data points. Additionally, advancements in encryption and secure multi-party computation are being integrated to ensure that data remains confidential throughout the processing pipeline. These efforts are crucial for enabling the widespread adoption of multimodal LLMs across various domains while ensuring they remain efficient and secure ([https://towardsdatascience.com/differential-privacy-and-federated-learning-for-medical-data-0f2437d6ece9](https://towardsdatascience.com/differential-privacy-and-federated-learning-for-medical-data-0f2437d6ece9), [https://arxiv.org/pdf/2403.05156](https://arxiv.org/pdf/2403.05156), [https://pair.withgoogle.com/explorables/federated-learning/](https://pair.withgoogle.com/explorables/federated-learning/)).\n\t\t\tLet’s now summarize the learnings from this chapter.\n\t\t\tSummary\n\t\t\tIn this chapter, we covered various image processing techniques, such as loading, resizing, normalizing, and standardizing images to prepare them for ML applications. We implemented augmentation to generate diverse variations for improved model generalization and applied noise removal to enhance image quality. We also examined the use of OCR for text extraction from images, particularly addressing the challenges presented by thumbnails. Additionally, we explored the BLIP model’s capability to generate captions based on visual content. Furthermore, we discussed video processing techniques involving frame extraction and key moment analysis.\n\t\t\tFinally, we introduced the Whisper model, highlighting its effectiveness in converting audio to text and its automatic speech recognition capabilities across multiple languages.\n\t\t\tThis concludes the book! You did it!\n\t\t\tI want to express my sincere gratitude for your dedication to finishing this book. I’ve aimed to share the insights from my experience, with a focus on ML and AI, as these fields have been central to my career. I find them incredibly fascinating and transformative, though I might be a bit biased.\n\t\t\tAs you’ve seen in the later chapters, I believe LLMs are poised to revolutionize the field and the way we process data. That’s why I dedicated the last chapters to building a foundation and showcasing how effortlessly different types of data can be transformed and manipulated using LLMs.\n\t\t\tPlease take my advice and spend some time diving into the code examples provided. Implement these techniques in your daily tasks and projects. If there’s anything you find yourself doing manually or redoing frequently, *code it up* to streamline your process. This hands-on practice will help reinforce your learning. Experiment with the techniques and concepts from this book on your own projects, as real growth occurs when you adapt and innovate with these tools in practical scenarios.\n\t\t\tI’m traveling the world speaking at conferences and running workshops. If you see me at one of these events, don’t hesitate to say hello and talk to me! Who knows, our paths might cross at one of these events! In any case, I’d also love to hear about your progress and see what you’ve learned and built. Feel free to share your experiences with me—your insights and developments are always exciting to see. So, let’s stay connected! Connect with me on LinkedIn ([https://www.linkedin.com/in/maria-zervou-533222107/](https://www.linkedin.com/in/maria-zervou-533222107/)) and you can subscribe to my YouTube channel ([https://www.youtube.com/channel/UCY2Z8Sc2L0wQnTOQPlLzUQw](https://www.youtube.com/channel/UCY2Z8Sc2L0wQnTOQPlLzUQw)) for ongoing tutorials and content to keep you informed about new developments and techniques in ML and AI.\n\t\t\tThank you once again for your time and effort. Remember, learning is just the beginning; real growth comes from practicing and applying your knowledge. I’m excited to see where your journey leads you and hope our paths cross again in the future.\n\n```"]