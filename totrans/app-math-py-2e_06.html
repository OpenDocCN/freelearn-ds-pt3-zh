<html><head></head><body>
		<div id="_idContainer799">
			<h1 class="chapter-number" id="_idParaDest-227"><a id="_idTextAnchor226"/>6</h1>
			<h1 id="_idParaDest-228"><a id="_idTextAnchor227"/>Working with Data and Statistics</h1>
			<p>One of the most attractive features of Python for people who need to analyze data is the huge ecosystem of data manipulation and analysis packages, as well as the active community of data scientists working with Python. Python is easy to use, while also offering very powerful, fast libraries, which enables even relatively novice programmers to quickly and easily process vast sets of data. At the heart of many data science packages and tools is the pandas library. pandas provides two data container types that build on top of NumPy arrays and have good support for labels (other than simple integers). These data containers make working with large sets of data <span class="No-Break">extremely easy.</span></p>
			<p>Data and statistics are part of everything in the modern world. Python is leading the charge in trying to make sense of the vast quantity of data produced every day, and usually, this all starts with pandas – Python’s basic library for working with data. First, we’ll see some basic techniques for working with data using pandas. Then, we’ll discuss the basics of statistics, which will provide us with a systematic approach to understanding a whole population by looking at a <span class="No-Break">small sample.</span></p>
			<p>In this chapter, we will learn how to leverage Python and pandas to work with large sets of data and perform <span class="No-Break">statistical tests.</span></p>
			<p>This chapter covers the <span class="No-Break">following recipes:</span></p>
			<ul>
				<li>Creating Series and <span class="No-Break">DataFrame objects</span></li>
				<li>Loading and storing data from <span class="No-Break">a DataFrame</span></li>
				<li>Manipulating data <span class="No-Break">in DataFrames</span></li>
				<li>Plotting data from <span class="No-Break">a DataFrame</span></li>
				<li>Getting descriptive statistics from <span class="No-Break">a DataFrame</span></li>
				<li>Understanding a population <span class="No-Break">using sampling</span></li>
				<li>Performing operations on grouped data in <span class="No-Break">a DataFrame</span></li>
				<li>Testing hypotheses <span class="No-Break">using t-tests</span></li>
				<li>Testing hypotheses <span class="No-Break">using ANOVA</span></li>
				<li>Testing hypotheses for <span class="No-Break">non-parametric data</span></li>
				<li>Creating interactive plots <span class="No-Break">with Bokeh</span></li>
			</ul>
			<h1 id="_idParaDest-229"><a id="_idTextAnchor228"/>What is statistics?</h1>
			<p>Statistics<a id="_idIndexMarker574"/> is the systematic study of data using mathematical – specifically, probability – theory. There are two major aspects to statistics. The first aspect of statistics <a id="_idIndexMarker575"/>is <strong class="bold">summarizing data</strong>. This is where we find numerical values that describe a set of data, including characteristics such as the center (mean or median) and spread (standard deviation or variance) of the data. These values are <a id="_idIndexMarker576"/>called <strong class="bold">descriptive statistics</strong>. What we’re doing here is fitting a probability distribution that describes the likelihood of a particular characteristic appearing in a population. Here, a <em class="italic">population</em> simply <a id="_idIndexMarker577"/>means a complete set of measurements of a particular characteristic – for example, the height of every person currently alive <span class="No-Break">on Earth.</span></p>
			<p>The second – and arguably more important – aspect of statistics <a id="_idIndexMarker578"/>is <strong class="bold">inference</strong>. Here, we try to estimate the distribution of data describing a population by computing numerical values on a relatively small sample of that population. Not only do we try to estimate the distribution of the population, but we also try to quantify how good our approximation is. This usually takes the form of a confidence interval. A confidence interval is a range of values where we are confident the true value lies given the data we have observed. We usually give 95% or 99% confidence intervals for <span class="No-Break">estimated values.</span></p>
			<p>Inference<a id="_idIndexMarker579"/> also includes tests for whether two or more sets of sampled data come from the same population. This is the <a id="_idIndexMarker580"/>area of <strong class="bold">hypothesis testing</strong>. Here, we compare the likely distributions of both sets of data to determine whether they are likely to be the same. Many hypothesis tests require that the data is a normal distribution or, more likely, that we can apply<a id="_idIndexMarker581"/> the <em class="italic">central limit theorem</em>. These tests are sometimes described as parametric tests and include t-tests<a id="_idIndexMarker582"/> and <a id="_idIndexMarker583"/>ANOVA. However, if your data is not sufficiently nice that the central limit theorem can help, then some tests do not require the assumption of normality. These are<a id="_idIndexMarker584"/> called <span class="No-Break">non-parametric tests.</span></p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor229"/>Technical requirements</h1>
			<p>For this chapter, we will mostly make use of the pandas library for data manipulation, which provides R-like data structures, such as <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> objects, for storing, organizing, and manipulating data. We will also use the Bokeh data visualization library in the final recipe of this chapter. These libraries can be installed using your favorite package manager, such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
python3.10 -m pip install pandas bokeh</pre>
			<p>We will also make use of the NumPy and <span class="No-Break">SciPy packages.</span></p>
			<p>The code for this chapter can be found in the <span class="No-Break"><strong class="source-inline">Chapter 06</strong></span> folder of this book’s GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2006"><span class="No-Break">https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2006</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor230"/>Creating Series and DataFrame objects</h1>
			<p>Most data handling in Python is done using the pandas library, which builds on NumPy to provide R-like structures for holding data. These structures allow the easy indexing of rows and columns, using strings or other Python objects besides just integers. Once data is loaded into a <a id="_idIndexMarker585"/>pandas <strong class="source-inline">DataFrame</strong> or <strong class="source-inline">Series</strong>, it can be easily manipulated, just as if it were in a spreadsheet. This makes Python, when combined with pandas, a powerful tool for<a id="_idIndexMarker586"/> processing and <span class="No-Break">analyzing data.</span></p>
			<p>In this recipe, we will see how to create new pandas <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> objects and access items <span class="No-Break">from them.</span></p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor231"/>Getting ready</h2>
			<p>For this recipe, we <a id="_idIndexMarker587"/>will <a id="_idIndexMarker588"/>import the pandas library as <strong class="source-inline">pd</strong> using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
import pandas as pd</pre>
			<p>The NumPy<a id="_idIndexMarker589"/> package is <strong class="source-inline">np</strong>. We must <a id="_idIndexMarker590"/>also create a (seeded) random number generator from NumPy, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor232"/>How to do it...</h2>
			<p>The following steps outline how to create <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> objects that <span class="No-Break">hold data:</span></p>
			<ol>
				<li value="1">First, create the random data that we will store in the <strong class="source-inline">Series</strong> and <span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break"> objects:</span><pre class="console">
diff_data = rng.normal(0, 1, size=100)</pre><pre class="console">
cumulative = diff_data.cumsum()</pre></li>
				<li>Next, create a <strong class="source-inline">Series</strong> object that holds <strong class="source-inline">diff_data</strong>. We’ll print <strong class="source-inline">Series</strong> to produce a view of <span class="No-Break">the data:</span><pre class="console">
data_series = pd.Series(diff_data)</pre><pre class="console">
print(data_series)</pre></li>
				<li>Now, create a <strong class="source-inline">DataFrame</strong> object with <span class="No-Break">two columns:</span><pre class="console">
data_frame = pd.DataFrame({</pre><pre class="console">
    "diffs": data_series,</pre><pre class="console">
    "cumulative": cumulative</pre><pre class="console">
})</pre></li>
				<li>Print the <strong class="source-inline">DataFrame</strong> object to produce a view of the data <span class="No-Break">it holds:</span><pre class="console">
print(data_frame)</pre></li>
			</ol>
			<p>The printed objects are as follows; the <strong class="source-inline">Series</strong> object is on the left and the <strong class="source-inline">DataFrame</strong> object is on <span class="No-Break">the right:</span></p>
			<pre class="source-code">
                                     diffs  cumulative
0    -1.423825                0  -1.423825   -1.423825
1     1.263728                1   1.263728   -0.160097
2    -0.870662                2  -0.870662   -1.030758
3    -0.259173                3  -0.259173   -1.289932
4    -0.075343                4  -0.075343   -1.365275
        ...                  ..       ...         ...
95   -0.061905               95 -0.061905   -1.107210
96   -0.359480               96 -0.359480   -1.466690
97   -0.748644               97 -0.748644   -2.215334
98   -0.965479               98 -0.965479   -3.180813
99    0.360035               99  0.360035   -2.820778
Length: 100, dtype: float64  [100 rows x 2 columns]</pre>
			<p>As <a id="_idIndexMarker591"/>expected, both <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> contain 100 rows. Since the data in the series is of a <a id="_idIndexMarker592"/>single type – guaranteed by the fact that it is just a NumPy array – the data type is shown as <strong class="source-inline">float64</strong>. <strong class="source-inline">DataFrame</strong> has two columns, which may have different data types in general (although here, they both <span class="No-Break">have </span><span class="No-Break"><strong class="source-inline">float64</strong></span><span class="No-Break">).</span></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor233"/>How it works...</h2>
			<p>The pandas package provides the <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> classes, which mirror the function and capabilities of their R counterparts. <strong class="source-inline">Series</strong> is used to store one-dimensional data, such as time series data, and <strong class="source-inline">DataFrame</strong> is used to store multidimensional data; you can think of a <strong class="source-inline">DataFrame</strong> object as <span class="No-Break">a “spreadsheet.”</span></p>
			<p>What separates <strong class="source-inline">Series</strong> from a simple NumPy <strong class="source-inline">ndarray</strong> is the way that <strong class="source-inline">Series</strong> indexes its items. A NumPy array is indexed by integers, which is also the default for a <strong class="source-inline">Series</strong> object. However, <strong class="source-inline">Series</strong> can be indexed by any hashable Python object, including strings and <strong class="source-inline">datetime</strong> objects. This makes <strong class="source-inline">Series</strong> useful for storing time series data. A <strong class="source-inline">Series</strong> can be created in several ways. In this recipe, we used a NumPy array, but any Python iterable, such as a list, can be <span class="No-Break">used instead.</span></p>
			<p>Each column in a <strong class="source-inline">DataFrame</strong> object is a series containing rows, just as in a traditional database or spreadsheet. In this recipe, the columns are given labels when the <strong class="source-inline">DataFrame</strong> object is constructed via the keys of <span class="No-Break">the dictionary.</span></p>
			<p>The <strong class="source-inline">DataFrame</strong> and <strong class="source-inline">Series</strong> objects create a summary of the data they contain when printed. This <a id="_idIndexMarker593"/>includes column names, the number of rows and columns, and the first and last five rows <a id="_idIndexMarker594"/>of the frame (series). This is useful for quickly obtaining an overview of the object and the spread of data <span class="No-Break">it contains.</span></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor234"/>There’s more...</h2>
			<p>The individual rows (records) of a <strong class="source-inline">Series</strong> object can<a id="_idIndexMarker595"/> be accessed using the usual index notation by providing the corresponding index. We can also access the rows by their numerical position using the special <strong class="source-inline">iloc</strong> property object. This allows us to access the rows by their numerical (integer) index, such as with Python lists or <span class="No-Break">NumPy arrays.</span></p>
			<p>The columns in a <strong class="source-inline">DataFrame</strong> object<a id="_idIndexMarker596"/> can be accessed using the usual index notation, providing the name of the column. The result of this is a <strong class="source-inline">Series</strong> object that contains the data from the selected column. <strong class="source-inline">DataFrame</strong> also provides two properties that can be used to access data. The <strong class="source-inline">loc</strong> attribute provides access to individual rows by their index, whatever this object may be. The <strong class="source-inline">iloc</strong> attribute provides access to the rows by numerical index, just as for the <span class="No-Break"><strong class="source-inline">Series</strong></span><span class="No-Break"> object.</span></p>
			<p>You can provide selection criteria to <strong class="source-inline">loc</strong> (or just use index notation for the object) to select data. This includes a single label, a list of labels, a slice of labels, or a Boolean array (of an appropriate size). The <strong class="source-inline">iloc</strong> selection method accepts <span class="No-Break">similar criteria.</span></p>
			<p>There are other ways to select data from a <strong class="source-inline">Series</strong> or <strong class="source-inline">DataFrame</strong> object beyond the simple methods we describe here. For example, we can use the <strong class="source-inline">at</strong> attribute to access a single value at a specified row (and column) in <span class="No-Break">the object.</span></p>
			<p>Sometimes, a <a id="_idIndexMarker597"/>pandas <strong class="source-inline">Series</strong> or <strong class="source-inline">DataFrame</strong> is not sufficiently rich to describe the data<a id="_idIndexMarker598"/> because they are inherently low-dimensional. The <strong class="source-inline">xarray</strong> package builds upon the pandas interface and provides support for labeled multidimensional arrays (that is, NumPy arrays). We’ll learn about <strong class="source-inline">xarray</strong> in the <em class="italic">Loading and storing data from NetCDF files</em> recipe in <a href="B19085_10.xhtml#_idTextAnchor395"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>. More information about <strong class="source-inline">xarray</strong> can be found<a id="_idIndexMarker599"/> in the <span class="No-Break">documentation: </span><a href="https://docs.xarray.dev/en/stable/index.html"><span class="No-Break">https://docs.xarray.dev/en/stable/index.html</span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor235"/>See also</h2>
			<p>The pandas documentation contains a detailed description of the different ways to create and index a <strong class="source-inline">DataFrame</strong> or <strong class="source-inline">Series</strong> <span class="No-Break">object: </span><a href="https://pandas.pydata.org/docs/user_guide/indexing.html"><span class="No-Break">https://pandas.pydata.org/docs/user_guide/indexing.html</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-237"><a id="_idTextAnchor236"/>Loading and storing data from a DataFrame</h1>
			<p>It is fairly <a id="_idIndexMarker600"/>unusual to create a <strong class="source-inline">DataFrame</strong> object from the<a id="_idIndexMarker601"/> raw data in a Python session. In practice, the data will often come from an external source, such as an existing spreadsheet or CSV file, database, or API endpoint. For this reason, pandas provides numerous utilities for loading and storing data to file. Out of the box, pandas supports loading and storing data from CSV, Excel (<strong class="source-inline">xls</strong> or <strong class="source-inline">xlsx</strong>), JSON, SQL, Parquet, and Google BigQuery. This makes it very easy to import your data into pandas and then manipulate and analyze this data <span class="No-Break">using Python.</span></p>
			<p>In this recipe, we will learn how to load and store data in a CSV file. The instructions will be similar for loading and storing data in other <span class="No-Break">file formats.</span></p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor237"/>Getting ready</h2>
			<p>For this recipe, we will need to import the pandas package under the <strong class="source-inline">pd</strong> alias and the NumPy library as <strong class="source-inline">np</strong>. We must also create a default random number generator from NumPy using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345) # seed for example</pre>
			<p>Let’s learn how to store and then load data from <span class="No-Break">a </span><span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor238"/>How to do it...</h2>
			<p>Follow these<a id="_idIndexMarker602"/> steps to store data in a file and then load the data back<a id="_idIndexMarker603"/> <span class="No-Break">into Python:</span></p>
			<ol>
				<li value="1">First, we’ll create a sample <strong class="source-inline">DataFrame</strong> object using random data. Then, we will print this <strong class="source-inline">DataFrame</strong> object so that we can compare it to the data that we will <span class="No-Break">read later:</span><pre class="console">
diffs = rng.normal(0, 1, size=100)</pre><pre class="console">
cumulative = diffs.cumsum()</pre><pre class="console">
data_frame = pd.DataFrame({</pre><pre class="console">
    "diffs": diffs, </pre><pre class="console">
    "cumulative": cumulative</pre><pre class="console">
})</pre><pre class="console">
print(data_frame)</pre></li>
				<li>We will store the data in this <strong class="source-inline">DataFrame</strong> object in the <strong class="source-inline">sample.csv</strong> file by using the <strong class="source-inline">to_csv</strong> method on the <strong class="source-inline">DataFrame</strong> object. We will use the <strong class="source-inline">index=False</strong> keyword argument so that the index is not stored in the <span class="No-Break">CSV file:</span><pre class="console">
data_frame.to_csv("sample.csv", index=False)</pre></li>
				<li>Now, we can use the <strong class="source-inline">read_csv</strong> routine from pandas to read the <strong class="source-inline">sample.csv</strong> file into a new <strong class="source-inline">DataFrame</strong> object. We will print this object to show <span class="No-Break">the result:</span><pre class="console">
df = pd.read_csv("sample.csv", index_col=False)</pre><pre class="console">
print(df)</pre></li>
			</ol>
			<p>The two printed <a id="_idIndexMarker604"/>DataFrames are shown side by side. The <strong class="source-inline">DataFrame</strong> object <a id="_idIndexMarker605"/>from <em class="italic">step 1</em> is on the left and the <strong class="source-inline">DataFrame</strong> object from <em class="italic">step 3</em> is on <span class="No-Break">the right:</span></p>
			<pre class="source-code">
    diffs      cumulative          diffs       cumulative
0  -1.423825   -1.423825        0  -1.423825   -1.423825
1   1.263728   -0.160097        1   1.263728   -0.160097
2  -0.870662   -1.030758        2  -0.870662   -1.030758
3  -0.259173   -1.289932        3  -0.259173   -1.289932
4  -0.075343   -1.365275        4  -0.075343   -1.365275
..         ...            ...        ..         ...            ...
95 -0.061905   -1.107210        95 -0.061905   -1.107210
96 -0.359480   -1.466690        96 -0.359480   -1.466690
97 -0.748644   -2.215334        97 -0.748644   -2.215334
98 -0.965479   -3.180813        98 -0.965479   -3.180813
99  0.360035   -2.820778        99  0.360035   -2.820778
[100 rows x 2 columns]           [100 rows x 2 columns]</pre>
			<p>As we can see from the rows, these two DataFrames <span class="No-Break">are identical.</span></p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor239"/>How it works...</h2>
			<p>The core of this <a id="_idIndexMarker606"/>recipe is the <strong class="source-inline">read_csv</strong> routine in pandas. This<a id="_idIndexMarker607"/> routine takes path- or file-like objects as an argument and reads the contents of the file as CSV data. We can customize the delimiter using the <strong class="source-inline">sep</strong> keyword argument, which is a comma (<strong class="source-inline">,</strong>) by default. There are also options to customize the column headers and customize the type of <span class="No-Break">each column.</span></p>
			<p>The <strong class="source-inline">to_csv</strong> method in a <strong class="source-inline">DataFrame</strong> or <strong class="source-inline">Series</strong> stores the contents in a CSV file. We used the <strong class="source-inline">index</strong> keyword argument here so that the indices are not printed into the file. This means that pandas will infer the index from the row number in the CSV file. This behavior is desirable if the data is indexed by integers, but this might not be the case if the data is<a id="_idIndexMarker608"/> indexed by times or dates, for example. We can also<a id="_idIndexMarker609"/> use this keyword argument to specify which column in the CSV file is the <span class="No-Break">indexing column.</span></p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor240"/>See also</h2>
			<p>See the pandas documentation for a list of supported file <span class="No-Break">formats: </span><a href="https://pandas.pydata.org/docs/reference/io.html"><span class="No-Break">https://pandas.pydata.org/docs/reference/io.html</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor241"/>Manipulating data in DataFrames</h1>
			<p>Once we have <a id="_idIndexMarker610"/>data in a <strong class="source-inline">DataFrame</strong>, we often need to apply some simple transformations or filters to the data before we can perform any analysis. This could include, for example, filtering the rows that are missing data or applying a function to <span class="No-Break">individual columns.</span></p>
			<p>In this recipe, we will learn how to perform some basic manipulation of <strong class="source-inline">DataFrame</strong> objects to prepare the data <span class="No-Break">for analysis.</span></p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor242"/>Getting ready</h2>
			<p>For this recipe, we will need the pandas package imported under the <strong class="source-inline">pd</strong> alias, the NumPy package imported under the <strong class="source-inline">np</strong> alias, and a default random number generator object from NumPy to be created using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<p>Let’s learn how to perform some simple manipulations on data in <span class="No-Break">a </span><span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor243"/>How to do it...</h2>
			<p>The following steps<a id="_idIndexMarker611"/> illustrate how to perform some basic filtering and manipulations on a <span class="No-Break">pandas </span><span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break">:</span></p>
			<ol>
				<li value="1">First, we will create a sample <strong class="source-inline">DataFrame</strong> using <span class="No-Break">random data:</span><pre class="console">
three = rng.uniform(-0.2, 1.0, size=100)</pre><pre class="console">
three[three &lt; 0] = np.nan</pre><pre class="console">
data_frame = pd.DataFrame({</pre><pre class="console">
    "one": rng.random(size=100),</pre><pre class="console">
    "two": rng.normal(0, 1, size=100).cumsum(),</pre><pre class="console">
    "three": three</pre><pre class="console">
})</pre></li>
				<li>Next, we will <a id="_idIndexMarker612"/>generate a new column from an existing column. This new column will hold <strong class="source-inline">True</strong> if the corresponding entry of the <strong class="source-inline">"one"</strong> column is greater than <strong class="source-inline">0.5</strong>, and <span class="No-Break"><strong class="source-inline">False</strong></span><span class="No-Break"> otherwise:</span><pre class="console">
data_frame["four"] = data_frame["one"] &gt; 0.5</pre></li>
				<li>Now, let’s create a new function that we will apply to our <strong class="source-inline">DataFrame</strong>. This function multiplies the row <strong class="source-inline">"two"</strong> value by the maximum of row <strong class="source-inline">"one"</strong> and <strong class="source-inline">0.5</strong> (there are more concise ways to write <span class="No-Break">this function):</span><pre class="console">
def transform_function(row):</pre><pre class="console">
    if row["four"]:</pre><pre class="console">
        return 0.5*row["two"]</pre><pre class="console">
        return row["one"]*row["two"]</pre></li>
				<li>Now, we will apply the previously defined function to each row in the DataFrame to generate a new column. We will also print the updated DataFrame for <span class="No-Break">comparison later:</span><pre class="console">
data_frame["five"] = data_frame.apply(</pre><pre class="console">
    transform_function, axis=1)</pre><pre class="console">
print(data_frame)</pre></li>
				<li>Finally, we have to filter out <a id="_idIndexMarker613"/>the rows in the DataFrame that contain a <strong class="bold">Not a Number</strong> (<strong class="bold">NaN</strong>) value. We will print the <span class="No-Break">resulting DataFrame:</span><pre class="console">
df = data_frame.dropna()</pre><pre class="console">
print(df)</pre></li>
			</ol>
			<p>The output <a id="_idIndexMarker614"/>from the <strong class="source-inline">print</strong> command in <em class="italic">step 4</em> is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
         one       two     three   four      five
0   0.168629  1.215005  0.072803  False  0.204885
1   0.240144  1.431064  0.180110  False  0.343662
2   0.780008  0.466240  0.756839   True  0.233120
3   0.203768 -0.090367  0.611506  False -0.018414
4   0.552051 -2.388755  0.269331   True -1.194377
..         ...         ...         ...     ...         ...
95  0.437305  2.262327  0.254499  False  0.989326
96  0.143115  1.624691  0.131136  False  0.232517
97  0.422742  2.769187  0.959325  False  1.170652
98  0.764412  1.128285         NaN   True  0.564142
99  0.413983 -0.185719  0.290481  False -0.076885
[100 rows x 5 columns]</pre>
			<p>There is a single NaN value visible in row 98. As expected, we have 100 rows total and 5 columns of data. Now, we can compare this to the output of the <strong class="source-inline">print</strong> command in <span class="No-Break"><em class="italic">step 6</em></span><span class="No-Break">:</span></p>
			<pre class="source-code">
         one       two     three   four      five
0   0.168629  1.215005  0.072803  False  0.204885
1   0.240144  1.431064  0.180110  False  0.343662
2   0.780008  0.466240  0.756839   True  0.233120
3   0.203768 -0.090367  0.611506  False -0.018414
4   0.552051 -2.388755  0.269331   True -1.194377
..         ...         ...         ...     ...         ...
94  0.475131  3.065343  0.330151  False  1.456440
95  0.437305  2.262327  0.254499  False  0.989326
96  0.143115  1.624691  0.131136  False  0.232517
97  0.422742  2.769187  0.959325  False  1.170652
99  0.413983 -0.185719  0.290481  False -0.076885
[88 rows x 5 columns]</pre>
			<p>As expected, the <a id="_idIndexMarker615"/>number of rows has dropped by 12, since we have removed all the rows that contain a NaN value. (Notice that row 98 no longer contains NaN in <span class="No-Break">column 3.)</span></p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor244"/>How it works...</h2>
			<p>New<a id="_idIndexMarker616"/> columns can be added to an existing <strong class="source-inline">DataFrame</strong> by simply assigning them to the new column index. However, some care needs to be taken here. In some situations, pandas will create a “view” of a <strong class="source-inline">DataFrame</strong> object rather than copying, and in this case, assigning it to a new column might not have the desired effect. This is discussed in the pandas <span class="No-Break">documentation (</span><a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"><span class="No-Break">https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy</span></a><span class="No-Break">).</span></p>
			<p>pandas <strong class="source-inline">Series</strong> objects (columns in a <strong class="source-inline">DataFrame</strong>) support rich comparison operators, such as equality and less than or greater than (in this recipe, we used the greater than operator). These comparison operators return a <strong class="source-inline">Series</strong> containing Boolean values corresponding to the positions at which the comparison was true and false. This can, in turn, be used to index the original <strong class="source-inline">Series</strong> and get just the rows where the comparison was true. In this recipe, we simply added this <strong class="source-inline">Series</strong> of Boolean values to the <span class="No-Break">original </span><span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break">.</span></p>
			<p>The <strong class="source-inline">apply</strong> method takes a function (or other callable function) and applies it to each column in the <strong class="source-inline">DataFrame</strong> object. In this recipe, we instead wanted to apply the function to each row, so we used the <strong class="source-inline">axis=1</strong> keyword argument to apply the function to each row in the <strong class="source-inline">DataFrame</strong> object. In either case, the function is provided with a <strong class="source-inline">Series</strong> object indexed by the rows (columns). We also applied a function to each row, which returned a value computed using the data from each row. In practice, this application would be quite slow if the <strong class="source-inline">DataFrame</strong> object contains a large number of rows. If possible, you should operate on the columns as a whole, using functions designed to operate on NumPy arrays, for better efficiency. This is especially true for performing simple arithmetic on values in columns of a <strong class="source-inline">DataFrame</strong>. Just like NumPy arrays, <strong class="source-inline">Series</strong> objects implement standard arithmetic operations, which can greatly improve the operation time for <span class="No-Break">large DataFrames.</span></p>
			<p>In the final step of this recipe, we used the <strong class="source-inline">dropna</strong> method to quickly select only the rows from the DataFrames that do not contain a NaN value. pandas uses NaN to represent missing data in a <strong class="source-inline">DataFrame</strong>, so this method selects the rows that don’t contain a missing value. This method returns a view to the original <strong class="source-inline">DataFrame</strong> object, but it can also modify the original <strong class="source-inline">DataFrame</strong> by passing the <strong class="source-inline">inplace=True</strong> keyword argument. As in this recipe, this is roughly equivalent to using the <a id="_idIndexMarker617"/>indexing notation to select rows using an indexing array containing <span class="No-Break">Boolean values.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should always be cautious when modifying original data directly since it might not be possible to return to this data to repeat your analysis later. If you do need to modify the data directly, you should make sure that it is either backed up or that the modifications do not remove data that you might <span class="No-Break">need later.</span></p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor245"/>There’s more...</h2>
			<p>Most pandas routines deal with missing data (NaN) sensibly. However, if you do need to remove or replace missing data<a id="_idIndexMarker618"/> in a <strong class="source-inline">DataFrame</strong>, then there are several ways to do this. In this recipe, we used the <strong class="source-inline">dropna</strong> method<a id="_idIndexMarker619"/> to simply drop the rows from the DataFrames that are missing data. Instead, we could fill all the missing values with a specific value using<a id="_idIndexMarker620"/> the <strong class="source-inline">fillna</strong> method, or interpolate missing values <a id="_idIndexMarker621"/>using the surrounding values using the <span class="No-Break"><strong class="source-inline">interpolate</strong></span><span class="No-Break"> method.</span></p>
			<p>More generally, we can use the <strong class="source-inline">replace</strong> method<a id="_idIndexMarker622"/> to replace specific (non-NaN) values with other values. This method can work with both numeric values and string values, including pattern-matching <span class="No-Break">with regex.</span></p>
			<p>The <strong class="source-inline">DataFrame</strong> class has many useful methods. We’ve only covered the very basic methods here, but there are two other methods that we should also mention. These are the <strong class="source-inline">agg</strong> method and<a id="_idIndexMarker623"/> the <span class="No-Break"><strong class="source-inline">merge</strong></span><span class="No-Break"> method.</span></p>
			<p>The <strong class="source-inline">agg</strong> method<a id="_idIndexMarker624"/> aggregates the results of one or more operations over a given axis of the <strong class="source-inline">DataFrame</strong> object. This allows us to quickly produce summary information for each column (or row) by applying an aggregating function. The output is a <strong class="source-inline">DataFrame</strong> that contains the names of the functions applied as the rows, and the labels for the chosen axis (column labels, for instance) for <span class="No-Break">the columns.</span></p>
			<p>The <strong class="source-inline">merge</strong> method performs a SQL-like join over two DataFrames. This will produce a new <strong class="source-inline">DataFrame</strong> that contains the result of the join. Various parameters can be passed to the <strong class="source-inline">how</strong> keyword argument to specify the type of merge to be performed, with the default being <strong class="source-inline">inner</strong>. The name of the column or index over which to perform the join should be passed to either the <strong class="source-inline">on</strong> keyword argument – if both <strong class="source-inline">DataFrame</strong> objects contain the same key – or to <strong class="source-inline">left_on</strong> and <strong class="source-inline">right_on</strong>. Here is a very simple example of a merge <span class="No-Break">on DataFrames:</span></p>
			<pre class="source-code">
rng = default_rng(12345)
df1 = pd.DataFrame({
    "label": rng.choice(["A", "B", "C"], size=5),
    "data1": rng.standard_normal(size=5)
})
df2 = pd.DataFrame({
    "label": rng.choice(["A", "B", "C", "D"], size=4),
    "data2": rng.standard_normal(size=4)
})
df3 = df1.merge(df2, how="inner", on="label")</pre>
			<p>This will <a id="_idIndexMarker625"/>produce a <strong class="source-inline">DataFrame</strong> that contains rows with <strong class="source-inline">label</strong>, <strong class="source-inline">data1</strong>, and <strong class="source-inline">data2</strong> corresponding to the rows from <strong class="source-inline">df1</strong> and <strong class="source-inline">df2</strong> that share the same label. Let’s print the three DataFrames to see <span class="No-Break">the result:</span></p>
			<pre class="source-code">
&gt;&gt;&gt; print(df1)                          &gt;&gt;&gt; print(df2)
  label      data1                        label      data2
0      C -0.259173                     0      D  2.347410
1      A -0.075343                     1      A  0.968497
2      C -0.740885                     2      C -0.759387
3      A -1.367793                     3      C  0.902198
4      A  0.648893
&gt;&gt;&gt; df3
  label      data1      data2
0      C -0.259173 -0.759387
1      C -0.259173  0.902198
2      C -0.740885 -0.759387
3      C -0.740885  0.902198
4      A -0.075343  0.968497
5      A -1.367793  0.968497
6      A  0.648893  0.968497</pre>
			<p>Here, you can see that each combination of <strong class="source-inline">data1</strong> and <strong class="source-inline">data2</strong> values from <strong class="source-inline">df1</strong> and <strong class="source-inline">df2</strong>, respectively, with matching labels, have a row in <strong class="source-inline">df3</strong>. Moreover, the row with label <strong class="source-inline">D</strong> from <strong class="source-inline">df2</strong> is not used since there is no row with label <strong class="source-inline">D</strong> in <strong class="source-inline">df1</strong>. </p>
			<h1 id="_idParaDest-247"><a id="_idTextAnchor246"/>Plotting data from a DataFrame</h1>
			<p>As with<a id="_idIndexMarker626"/> many mathematical problems, one of the first steps to finding some way to visualize the problem and all the information is to formulate a strategy. For data-based problems, this usually means producing a plot of the data and visually inspecting it for trends, patterns, and the underlying structure. Since this is such a common operation, pandas provides a quick and simple interface for plotting data in various forms, using Matplotlib under the hood by default, directly from a <strong class="source-inline">Series</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break">.</span></p>
			<p>In this recipe, we will learn how to plot data directly from a <strong class="source-inline">DataFrame</strong> or <strong class="source-inline">Series</strong> to understand the underlying trends <span class="No-Break">and structure.</span></p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor247"/>Getting ready</h2>
			<p>For this<a id="_idIndexMarker627"/> recipe, we will need the pandas library imported as <strong class="source-inline">pd</strong>, the NumPy library imported as <strong class="source-inline">np</strong>, the Matplotlib <strong class="source-inline">pyplot</strong> module imported as <strong class="source-inline">plt</strong>, and a default random number generator instance created using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor248"/>How to do it...</h2>
			<p>Follow these steps to create a simple <strong class="source-inline">DataFrame</strong> using random data and produce plots of the data <span class="No-Break">it contains:</span></p>
			<ol>
				<li value="1">Create a sample <strong class="source-inline">DataFrame</strong> using <span class="No-Break">random data:</span><pre class="console">
diffs = rng.standard_normal(size=100)</pre><pre class="console">
walk = diffs.cumsum()</pre><pre class="console">
df = pd.DataFrame({</pre><pre class="console">
    "diffs": diffs,</pre><pre class="console">
    "walk": walk</pre><pre class="console">
})</pre></li>
				<li>Next, we have to create a blank figure with two subplots ready <span class="No-Break">for plotting:</span><pre class="console">
fig, (ax1, ax2) = plt.subplots(1, 2, tight_layout=True)</pre></li>
				<li>We have to plot the <strong class="source-inline">walk</strong> column as a standard line graph. This can be done by using the <strong class="source-inline">plot</strong> method on the <strong class="source-inline">Series</strong> (column) object without additional arguments. We will force the plotting on <strong class="source-inline">ax1</strong> by passing the <strong class="source-inline">ax=ax1</strong> keyword argument: <pre class="console">
df["walk"].plot(ax=ax1, title="Random walk", color="k")</pre><pre class="console">
ax1.set_xlabel("Index")</pre><pre class="console">
ax1.set_ylabel("Value")</pre></li>
				<li>Now, we have to plot a histogram of the <strong class="source-inline">diffs</strong> column by passing the <strong class="source-inline">kind="hist"</strong> keyword argument to the <span class="No-Break"><strong class="source-inline">plot</strong></span><span class="No-Break"> method:</span><pre class="console">
df["diffs"].plot(kind="hist", ax=ax2, </pre><pre class="console">
    title="Histogram of diffs", color="k", alpha=0.6)</pre><pre class="console">
ax2.set_xlabel("Difference")</pre></li>
			</ol>
			<p>The resulting<a id="_idIndexMarker628"/> plots are <span class="No-Break">shown here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer744">
					<img alt="Figure 6.1 – Plot of the walk value and a histogram of differences from a DataFrame&#13;&#10;" src="image/6.1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Plot of the walk value and a histogram of differences from a DataFrame</p>
			<p>Here, we can see<a id="_idIndexMarker629"/> that the histogram of differences approximates a standard normal distribution (mean 0 and variance 1). The random walk plot shows the cumulative sum of the differences and oscillates (fairly symmetrically) above and <span class="No-Break">below 0.</span></p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor249"/>How it works...</h2>
			<p>The <strong class="source-inline">plot</strong> method on a <strong class="source-inline">Series</strong> (or a <strong class="source-inline">DataFrame</strong>) is a quick way to plot the data it contains against the row index. The <strong class="source-inline">kind</strong> keyword argument is used to control the type of plot that is produced, with a line plot being the default. There are lots of options for the plotting type, including <strong class="source-inline">bar</strong> for a vertical bar chart, <strong class="source-inline">barh</strong> for a horizontal bar chart, <strong class="source-inline">hist</strong> for a histogram (also seen in this recipe), <strong class="source-inline">box</strong> for a box plot, and <strong class="source-inline">scatter</strong> for a scatter plot. There are several other keyword arguments to customize the plot that it produces. In this recipe, we also provided the <strong class="source-inline">title</strong> keyword argument to add a title to <span class="No-Break">each subplot.</span></p>
			<p>Since we wanted<a id="_idIndexMarker630"/> to put both plots on the same figure side by side using subplots that we had already created, we used the <strong class="source-inline">ax</strong> keyword argument to pass in the respective axes handles to the plotting routine. Even if you let the <strong class="source-inline">plot</strong> method construct a figure, you may still need to use the <strong class="source-inline">plt.show</strong> routine to display the figure with <span class="No-Break">certain settings.</span></p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor250"/>There’s more...</h2>
			<p>We can produce several common types of plots using the pandas interface. This includes, in addition to those mentioned in this recipe, scatter plots, bar plots (horizontal bars and vertical bars), area plots, pie charts, and box plots. The <strong class="source-inline">plot</strong> method also accepts various keyword arguments to customize the appearance of the plot. </p>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor251"/>Getting descriptive statistics from a DataFrame</h1>
			<p>Descriptive statistics, or <a id="_idIndexMarker631"/>summary statistics, are simple values associated with a set of data, such <a id="_idIndexMarker632"/>as the mean, median, standard deviation, minimum, maximum, and quartile values. These values describe the location and spread of a dataset in various ways. The mean and median are measures of the center (location) of the data, and the other values measure the spread of the data from the mean and median. These statistics are vital for understanding a dataset and form the basis for many techniques <span class="No-Break">for analysis.</span></p>
			<p>In this recipe, we will learn how to <a id="_idIndexMarker633"/>generate descriptive statistics for each column in <span class="No-Break">a </span><span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor252"/>Getting ready</h2>
			<p>For this recipe, we need the pandas package imported as <strong class="source-inline">pd</strong>, the NumPy package imported as <strong class="source-inline">np</strong>, the Matplotlib <strong class="source-inline">pyplot</strong> module imported as <strong class="source-inline">plt</strong>, and a default random number generator created using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor253"/>How to do it...</h2>
			<p>The following steps<a id="_idIndexMarker634"/> show how to generate descriptive statistics for each column in <span class="No-Break">a </span><span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break">:</span></p>
			<ol>
				<li value="1">First, we will create some sample data that we <span class="No-Break">can analyze:</span><pre class="console">
uniform = rng.uniform(1, 5, size=100)</pre><pre class="console">
normal = rng.normal(1, 2.5, size=100)</pre><pre class="console">
bimodal = np.concatenate([rng.normal(0, 1, size=50), </pre><pre class="console">
    rng.normal(6, 1, size=50)])</pre><pre class="console">
df = pd.DataFrame({</pre><pre class="console">
    "uniform": uniform, </pre><pre class="console">
    "normal": normal, </pre><pre class="console">
    "bimodal": bimodal</pre><pre class="console">
})</pre></li>
				<li>Next, we will plot histograms of the data so that we can understand the distribution of the data in the <span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break"> object:</span><pre class="console">
fig, (ax1, ax2, ax3) = plt.subplots(1, 3,</pre><pre class="console">
                                    tight_layout=True)</pre><pre class="console">
df["uniform"].plot(kind="hist",</pre><pre class="console">
    title="Uniform", ax=ax1, color="k", alpha=0.6)</pre><pre class="console">
df["normal"].plot(kind="hist",</pre><pre class="console">
    title="Normal", ax=ax2, color="k", alpha=0.6)</pre></li>
				<li>To get a proper <a id="_idIndexMarker635"/>view of the distribution for the <strong class="source-inline">bimodal</strong> data, we will change the number of bins in the histogram <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">20</strong></span><span class="No-Break">:</span><pre class="console">
df["bimodal"].plot(kind="hist", title="Bimodal",</pre><pre class="console">
    ax=ax3, bins=20, color="k", alpha=0.6)</pre></li>
				<li>pandas <strong class="source-inline">DataFrame</strong> objects have a method for getting several common descriptive statistics for each column. The <strong class="source-inline">describe</strong> method creates a new <strong class="source-inline">DataFrame</strong>, where the column headers are the same as from the original object and each row contains a different <span class="No-Break">descriptive statistic:</span><pre class="console">
descriptive = df.describe()</pre></li>
				<li>We must<a id="_idIndexMarker636"/> also compute the <em class="italic">kurtosis</em> and add this to the new <strong class="source-inline">DataFrame</strong> object we just obtained. We must also print the descriptive statistics to the console to see what the <span class="No-Break">values are:</span><pre class="console">
descriptive.loc["kurtosis"] = df.kurtosis()</pre><pre class="console">
print(descriptive)</pre><pre class="console">
#             uniform      normal     bimodal</pre><pre class="console">
# count     100.000000 100.000000 100.000000</pre><pre class="console">
# mean         2.813878   1.087146   2.977682</pre><pre class="console">
# std           1.093795   2.435806   3.102760</pre><pre class="console">
# min           1.020089  -5.806040  -2.298388</pre><pre class="console">
# 25%           1.966120  -0.498995   0.069838</pre><pre class="console">
# 50%           2.599687   1.162897   3.100215</pre><pre class="console">
# 75%           3.674468   2.904759   5.877905</pre><pre class="console">
# max           4.891319   6.375775   8.471313</pre><pre class="console">
# kurtosis  -1.055983   0.061679  -1.604305</pre></li>
				<li>Finally, we <a id="_idIndexMarker637"/>must add vertical lines to the histograms to illustrate the value of the mean in <span class="No-Break">each case:</span><pre class="console">
uniform_mean = descriptive.loc["mean", "uniform"]</pre><pre class="console">
normal_mean = descriptive.loc["mean", "normal"]</pre><pre class="console">
bimodal_mean = descriptive.loc["mean", "bimodal"]</pre><pre class="console">
ax1.vlines(uniform_mean, 0, 20, "k")</pre><pre class="console">
ax2.vlines(normal_mean, 0, 25, "k")</pre><pre class="console">
ax3.vlines(bimodal_mean, 0, 20, "k")</pre></li>
			</ol>
			<p>The resulting histograms are <span class="No-Break">shown here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer745">
					<img alt="Figure 6.2 – Histograms of three sets of data with their mean values indicated&#13;&#10;" src="image/6.2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Histograms of three sets of data with their mean values indicated</p>
			<p>Here, we can see that the mean is central to the normally distributed data (middle), but for the uniformly distributed data (left), the “mass” of the distribution is slightly more biased toward the lower values to the left of the mean. With the bimodal day (right), the mean line lies exactly between the two components of mass. </p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor254"/>How it works...</h2>
			<p>The <strong class="source-inline">describe</strong> method<a id="_idIndexMarker638"/> returns a <strong class="source-inline">DataFrame</strong> with rows for the following descriptive statistics of the data: the count, mean, standard deviation, minimum value, 25% quartile, median (50% quartile), 75% quartile, and maximum value. The count is fairly self-explanatory, as are the minimum and maximum values. The mean and the median are two different <em class="italic">averages</em> of the data, which roughly represent the central value of the data. The definition of the mean should be familiar, as the sum of all values divided by the number of values. We can express this quantity using the <span class="No-Break">following formula:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_06_001.png"/></p>
			<p>Here, the <img alt="" src="image/Formula_06_002.png"/> values represent the data values and <img alt="" src="image/Formula_06_003.png"/> is the number (count) of values. Here, we also adopt the common notation of the bar to represent the mean value. The median is the “middle value” when all the data is sorted (taking an average of the two middle values if there is an odd number of values). The quartile values at 25% and 75% are similarly defined, but taking the value at 25% or 75% of the way through the ordered values. You might also think <a id="_idIndexMarker639"/>of the minimum as the 0% quartile and the maximum as the <span class="No-Break">100% quartile.</span></p>
			<p><strong class="bold">Standard deviation</strong> is a <a id="_idIndexMarker640"/>measure of the spread of the data from the mean and is related to another quantity that is frequently mentioned in statistics: the <strong class="bold">variance</strong>. The variance<a id="_idIndexMarker641"/> is the square of the standard deviation and is defined <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_06_004.png"/></p>
			<p>You might also see <img alt="" src="image/Formula_06_005.png"/> appear in the fraction here, which is a correction for <strong class="bold">bias</strong> when <a id="_idIndexMarker642"/>estimating population parameters from a sample. We will discuss population parameters and their estimation in the next recipe. The standard deviation, variance, quartiles, and maximum and minimum values describe the spread of the data. For example, if the maximum value is 5, the minimum value is 0, the 25% quartile is 2, and the 75% quartile is 4, then this indicates that most (at least 50% of the values, in fact) of the data is concentrated between 2 <span class="No-Break">and 4.</span></p>
			<p>The <em class="italic">kurtosis</em> is a measure of how much the data is concentrated in the “tails” of the distribution (far from the mean). This is not as common as the other quantities we have discussed in this recipe, but it does appear in some analyses. We have included it here mostly as a demonstration of how to compute summary statistic values that do not appear in the <strong class="source-inline">DataFrame</strong> object returned from the <strong class="source-inline">describe</strong> method using the appropriately named method – here, <strong class="source-inline">kurtosis</strong>. There are, of course, separate methods for computing the mean (<strong class="source-inline">mean</strong>), standard deviation (<strong class="source-inline">std</strong>), and the other quantities <a id="_idIndexMarker643"/>from the <span class="No-Break"><strong class="source-inline">describe</strong></span><span class="No-Break"> method.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">When pandas computes the quantities described in this recipe, it will automatically ignore any “missing values” represented by NaN. This will also be reflected in the count reported in the <span class="No-Break">descriptive statistics.</span></p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor255"/>There’s more...</h2>
			<p>The third dataset that we included in our statistics illustrates the importance of looking at the data to make sure the values we have calculated make sense. Indeed, we compute the mean as approximately <strong class="source-inline">2.9</strong>, but looking at the histogram, it is clear that most of the data is relatively far from this value. We should always check whether the summary statistics that we calculate give an accurate summary of the data in our sample. Simply quoting the mean might give an inaccurate representation of the sample. </p>
			<h1 id="_idParaDest-257"><a id="_idTextAnchor256"/>Understanding a population using sampling</h1>
			<p>One of the central problems in statistics is to make estimations – and quantify how good these estimations are – of the distribution of an entire population given only a small (random) sample. A classic example is to estimate the average height of all the people in a country when measuring the height of a randomly selected sample of people. These kinds of problems are particularly interesting when the true population distribution, by which we usually mean the mean of the whole population, cannot feasibly be measured. In this case, we must rely on our knowledge of statistics and a (usually much smaller) randomly selected sample to estimate the true population mean and standard deviation, and also quantify how good our estimations are. It is the latter that is the source of confusion, misunderstanding, and misrepresentation of statistics in the wider world. </p>
			<p>In this recipe, we will learn how to estimate the population mean and give a <strong class="bold">confidence interval</strong> for <span class="No-Break">these </span><span class="No-Break"><a id="_idIndexMarker644"/></span><span class="No-Break">estimates.</span></p>
			<h2 id="_idParaDest-258"><a id="_idTextAnchor257"/>Getting ready</h2>
			<p>For this recipe, we need <a id="_idIndexMarker645"/>the pandas package imported as <strong class="source-inline">pd</strong>, the <strong class="source-inline">math</strong> module from the Python standard library, and the SciPy <strong class="source-inline">stats</strong> module, imported using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
from scipy import stats</pre>
			<p>Let’s learn how to construct confidence intervals using the statistical routines <span class="No-Break">from SciPy.</span></p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor258"/>How to do it...</h2>
			<p>In the following steps, we<a id="_idIndexMarker646"/> will give an estimation of the mean height of males in the United Kingdom, based on a randomly selected sample of <span class="No-Break">20 people:</span></p>
			<ol>
				<li value="1">First, we must load our sample data into a <span class="No-Break">pandas </span><span class="No-Break"><strong class="source-inline">Series</strong></span><span class="No-Break">:</span><pre class="console">
sample_data = pd.Series(</pre><pre class="console">
    [172.3, 171.3, 164.7, 162.9, 172.5, 176.3, 174.8,</pre><pre class="console">
    171.9,176.8, 167.8, 164.5, 179.7, 157.8, 170.6,</pre><pre class="console">
    189.9, 185., 172.7, 165.5, 174.5, 171.5]</pre><pre class="console">
)</pre></li>
				<li>Next, we must compute the sample mean and <span class="No-Break">standard deviation:</span><pre class="console">
sample_mean = sample_data.mean()</pre><pre class="console">
sample_std = sample_data.std()</pre><pre class="console">
print(f"Mean {sample_mean}, st. dev {sample_std}")</pre><pre class="console">
# Mean 172.15, st. dev 7.473778724383846</pre></li>
				<li>Then, we must<a id="_idIndexMarker647"/> compute the <strong class="bold">standard error</strong>, <span class="No-Break">as follows:</span><pre class="console">
N = sample_data.count()</pre><pre class="console">
std_err = sample_std/math.sqrt(N)</pre></li>
				<li>We must <a id="_idIndexMarker648"/>compute <a id="_idIndexMarker649"/>the <strong class="bold">critical values</strong> for the confidence values we desire from the student <span class="No-Break">t distribution:</span><pre class="console">
cv_95, cv_99 = stats.t.ppf([0.975, 0.995], df=N-1)</pre></li>
				<li>Now, we can compute the 95% and 99% confidence intervals for the true population mean using the <span class="No-Break">following code:</span><pre class="console">
pm_95 = cv_95*std_err</pre><pre class="console">
conf_interval_95 = [sample_mean - pm_95,</pre><pre class="console">
    sample_mean + pm_95]</pre><pre class="console">
pm_99 = cv_99*std_err</pre><pre class="console">
conf_interval_99 = [sample_mean - pm_99,</pre><pre class="console">
    sample_mean + pm_99]</pre><pre class="console">
print("95% confidence", conf_interval_95)</pre><pre class="console">
# 95% confidence [168.65216388659374, 175.64783611340627]</pre><pre class="console">
print("99% confidence", conf_interval_99)</pre><pre class="console">
# 99% confidence [167.36884119608774, 176.93115880391227]</pre></li>
			</ol>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor259"/>How it works...</h2>
			<p>The key to parameter estimation is normal distribution, which we discussed in <a href="B19085_04.xhtml#_idTextAnchor138"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Working with Randomness and Probability</em>. If we find the critical value of <img alt="" src="image/Formula_06_006.png"/> for which the probability that a standard, normally distributed random number lies below this value <img alt="" src="image/Formula_06_007.png"/> is 97.5%, then the probability that such a number lies between the values of <img alt="" src="image/Formula_06_008.png"/> and <img alt="" src="image/Formula_06_009.png"/> is 95% (2.5% in each tail). This critical value of <img alt="" src="image/Formula_06_010.png"/> turns out to be 1.96, rounded to 2 decimal places. That is, we can be 95% sure that the value of a standard normally distributed random number lies between <img alt="" src="image/Formula_06_011.png"/> and <img alt="" src="image/Formula_06_012.png"/>. Similarly, the critical value of 99% confidence is 2.58 (rounded to 2 <span class="No-Break">decimal places).</span></p>
			<p>If our<a id="_idIndexMarker650"/> sample is “large,” we could invoke the <strong class="bold">central limit theorem</strong>, which tells<a id="_idIndexMarker651"/> us that even if the population is not normally distributed the means of random samples drawn from this population will be normally distributed with the same mean as the whole population. However, this is only valid if our samples are large. In this recipe, the sample is not large – it only has 20 values, which is certainly not large compared to the male population of the UK. This means that, rather than the normal distribution, we have to use a student t distribution with <img alt="" src="image/Formula_06_013.png"/> degrees of freedom to find our critical values, where <img alt="" src="image/Formula_06_014.png"/> is the size of our sample. For this, we must use the <strong class="source-inline">stats.t.ppf</strong> routine from the SciPy <span class="No-Break"><strong class="source-inline">stats</strong></span><span class="No-Break"> module.</span></p>
			<p>The student t distribution is related to the normal distribution but has a parameter – the degree of freedom – that changes the shape of the distribution. As the number of degrees of freedom increases, the student t distribution will look more and more like a normal distribution. The point at which you consider the distributions to be sufficiently similar depends on your application and your data. A general rule of thumb says that a sample size of 30 is sufficient to invoke the central limit theorem and simply use the normal distribution, but it is by no means a good rule. You should be very careful when making deductions based on a sample, especially if the sample is very small compared to the <span class="No-Break">total population.</span></p>
			<p>Once we have the critical values, the confidence interval for the true population mean can be computed by multiplying the critical value by the standard error of the sample and adding and subtracting this from the sample mean. The standard error is an approximation of the spread of the distribution of sample means of a given sample size from the true population mean. This is why we use the standard error to give the confidence interval for our estimation of the population mean. When we multiply the standard error by the critical value taken from the student t distribution (in this case), we obtain an estimate of the maximum difference between the observed sample mean and the true population mean at the given <span class="No-Break">confidence level.</span></p>
			<p>In this recipe, that means that we are 95% certain that the mean height of UK males lies between 168.7 cm and 175.6 cm, and we are 99% certain that the mean height of UK males lies between 167.4 cm and 176.9 cm. Our sample was drawn from a population with a mean of 175.3 cm and a<a id="_idIndexMarker652"/> standard deviation of 7.2 cm. This true mean (175.3 cm) does indeed lie within both of our confidence intervals, but <span class="No-Break">only just.</span></p>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor260"/>See also</h2>
			<p>There is a useful package called <strong class="source-inline">uncertainties</strong> for doing computations involving values with some uncertainty attached. See the <em class="italic">Accounting for uncertainty in calculations</em> recipe in <a href="B19085_10.xhtml#_idTextAnchor395"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Improving Your Productivity</em>, for <span class="No-Break">more information.</span></p>
			<h1 id="_idParaDest-262"><a id="_idTextAnchor261"/>Performing operations on grouped data in a DataFrame</h1>
			<p>One of the great <a id="_idIndexMarker653"/>features of pandas DataFrames is the ability to group the data by the values in particular columns. For example, we might group assembly line data by the line ID and the shift ID. The ability to operate on this grouped data ergonomically is very important since data is often aggregated for analysis but needs to be grouped <span class="No-Break">for preprocessing.</span></p>
			<p>In this recipe, we will learn how to perform operations on grouped data in a <strong class="source-inline">DataFrame</strong>. We’ll also take the opportunity to show how to operate on rolling windows of (<span class="No-Break">grouped) data.</span></p>
			<h2 id="_idParaDest-263"><a id="_idTextAnchor262"/>Getting ready</h2>
			<p>For this recipe, we will need the NumPy library imported as <strong class="source-inline">np</strong>, the Matplotlib <strong class="source-inline">pyplot</strong> interface imported as <strong class="source-inline">plt</strong>, and the pandas library imported as <strong class="source-inline">pd</strong>. We’ll also need an instance of the default random number generator created <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
rng = np.random.default_rng(12345)</pre>
			<p>Before we start, we also need to set up the Matplotlib plotting settings to change the plotting style in this recipe. We’re going to change the mechanism that cycles through the plotting style when multiple plots are produced on the same axes, which usually results in different colors. To do this, we’re going to change this to produce black lines with different <span class="No-Break">line styles:</span></p>
			<pre class="source-code">
from matplotlib.rcsetup import cycler
plt.rc("axes", prop_cycle=cycler(
    c=["k"]*3, ls=["-", "--", "-."]))</pre>
			<p>Now, let’s learn how to use the grouping features of <span class="No-Break">pandas DataFrames.</span></p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor263"/>How to do it...</h2>
			<p>Follow these <a id="_idIndexMarker654"/>steps to learn how to perform operations on grouped data inside a <span class="No-Break">pandas </span><span class="No-Break"><strong class="source-inline">DataFrame</strong></span><span class="No-Break">:</span></p>
			<ol>
				<li value="1">First, we need to generate some sample data in a <strong class="source-inline">DataFrame</strong>. For this example, we’re going to generate two label columns and one column of <span class="No-Break">numerical data:</span><pre class="console">
labels1 = rng.choice(["A", "B", "C"], size=50)</pre><pre class="console">
labels2 = rng.choice([1, 2], size=50)</pre><pre class="console">
data = rng.normal(0.0, 2.0, size=50)</pre><pre class="console">
df = pd.DataFrame({"label1": labels1, "label2": labels2, "data": data})</pre></li>
				<li>Next, let’s add a new column that consists of the cumulative sum of the <strong class="source-inline">"data"</strong> column, grouped by the first <span class="No-Break">label, </span><span class="No-Break"><strong class="source-inline">"label1"</strong></span><span class="No-Break">:</span><pre class="console">
df[“first_group”] = df.groupby(“label1”)[“data”].cumsum()</pre><pre class="console">
print(df.head())</pre></li>
			</ol>
			<p>The first five rows of <strong class="source-inline">df</strong> are now <span class="No-Break">as follows:</span></p>
			<pre class="console">
  label1  label2      data  first_group
0      C       2  0.867309     0.867309
1      A       2  0.554967     0.554967
2      C       1  1.060505     1.927814
3      A       1  1.073442     1.628409
4      A       1  1.236700     2.865109</pre>
			<p>Here, we<a id="_idIndexMarker655"/> can see that the <strong class="source-inline">"first_group"</strong> column contains the cumulative sum for each of the labels in the <strong class="source-inline">"label1"</strong> column. For instance, the row 0 and row 1 sums are just the value from the <strong class="source-inline">"data"</strong> column. The new entry in row 2 is the sum of the data in row 0 and row 2 since these are the first two rows with the <span class="No-Break">label “</span><span class="No-Break"><strong class="source-inline">C</strong></span><span class="No-Break">”.</span></p>
			<ol>
				<li value="3">Now, let’s perform a grouping on both the <strong class="source-inline">"label1"</strong> and <strong class="source-inline">"label2"</strong> <span class="No-Break">columns simultaneously:</span><pre class="console">
grouped = df.groupby(["label1", "label2"])</pre></li>
				<li>Now, we can compute the rolling mean over consecutive entries within each group using the <strong class="source-inline">transform</strong> and  <strong class="source-inline">rolling</strong> methods on the <span class="No-Break">grouped data:</span><pre class="console">
df["second_group"] = grouped["data"].transform(lambda d:</pre><pre class="console">
    d.rolling(2, min_periods=1).mean())</pre><pre class="console">
print(df.head())</pre><pre class="console">
print(df[df["label1"]=="C"].head())</pre></li>
			</ol>
			<p>The first five printed rows are <span class="No-Break">as follows:</span></p>
			<pre class="console">
  label1  label2      data  first_group  second_group
0      C       2  0.867309     0.867309      0.867309
1      A       2  0.554967     0.554967      0.554967
2      C       1  1.060505     1.927814      1.060505
3      A       1  1.073442     1.628409      1.073442
4      A       1  1.236700     2.865109      1.155071</pre>
			<p>As before, the first few rows all represent different groups, so the values in the <strong class="source-inline">"second_group"</strong> column are the same as the corresponding values in the <strong class="source-inline">"data"</strong> column. The value in row 4 is the mean of the data values in rows 3 and 4. The next five<a id="_idIndexMarker656"/> printed rows are those with the <span class="No-Break">label </span><span class="No-Break"><strong class="source-inline">C</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
  label1  label2      data  first_group  second_group
0      C       2  0.867309     0.867309      0.867309
2      C       1  1.060505     1.927814      1.060505
5      C       1 -1.590035     0.337779     -0.264765
7      C       1 -3.205403    -2.867624     -2.397719
8      C       1  0.533598    -2.334027     -1.335903</pre>
			<p>Here, we can see the rolling average and cumulative sums more clearly. All but the first row have the <span class="No-Break">same labels.</span></p>
			<ol>
				<li value="5">Finally, let’s plot the<a id="_idIndexMarker657"/> values of the <strong class="source-inline">"first_group"</strong> column grouped by the <strong class="source-inline">"</strong><span class="No-Break"><strong class="source-inline">label1"</strong></span><span class="No-Break"> column:</span><pre class="console">
fig, ax = plt.subplots()</pre><pre class="console">
df.groupby("label1")["first_group"].plot(ax=ax)</pre><pre class="console">
ax.set(title="Grouped data cumulative sums",     xlabel="Index", ylabel="value")</pre><pre class="console">
ax.legend()</pre></li>
			</ol>
			<p>The resulting plot is shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer760">
					<img alt="Figure 6.3 – Plot of cumulative sums by the label1 group&#13;&#10;" src="image/6.3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Plot of cumulative sums by the label1 group</p>
			<p>Here, we can see<a id="_idIndexMarker658"/> that each of the groups has produced a distinct line on the plot. This is a quick and easy way to produce plots of grouped data from a <strong class="source-inline">DataFrame</strong>. (Remember that we changed the default style cycle in the <em class="italic">Getting ready</em> section to make the plot style more distinctive on <span class="No-Break">the page.)</span></p>
			<h2 id="_idParaDest-265"><a id="_idTextAnchor264"/>How it works...</h2>
			<p>The <strong class="source-inline">groupby</strong> method creates a proxy for the DataFrame with an index generated from the requested columns. We can then perform operations on this proxy object. In this case, we used the <strong class="source-inline">cumsum</strong> method to generate the cumulative sum of the numerical values in the <strong class="source-inline">"data"</strong> column within each of the groups. We can use this approach to generate summary statistics of the grouped data in the same way. This is very useful for data exploration. </p>
			<p>In the second part of this recipe, we grouped by two different label columns and computed a rolling average (with window length 2) on each group. Notice that we “wrap” this computation using the <strong class="source-inline">transform</strong> method rather than calling <strong class="source-inline">rolling</strong> directly on the grouped <strong class="source-inline">DataFrame</strong>. This is so that the result has the correct indexing to be put back into <strong class="source-inline">df</strong>. Otherwise, the output of <strong class="source-inline">mean</strong> will inherit the grouped index, and we will not be able to put the result into <strong class="source-inline">df</strong>. We used the <strong class="source-inline">min_periods</strong> optional argument on <strong class="source-inline">rolling</strong> to make sure that all rows had a value. Otherwise, the rows that appeared before the window size would be <span class="No-Break">assigned NaN.</span></p>
			<p>The final part <a id="_idIndexMarker659"/>of this recipe used the <strong class="source-inline">plot</strong> routine on the data grouped by <strong class="source-inline">"label1"</strong>. This is a fast and easy way to plot multiple streams of data from within the same  <strong class="source-inline">DataFrame</strong> object. Unfortunately, it is a little difficult to customize the plotting in this case, although it can be done using the <strong class="source-inline">rcparams</strong> settings <span class="No-Break">in Matplotlib.</span></p>
			<h1 id="_idParaDest-266"><a id="_idTextAnchor265"/>Testing hypotheses using t-tests</h1>
			<p>One of the most common tasks in statistics is to test the validity of a hypothesis about the mean of a normally distributed population, given that you have collected sample data from that population. For example, in quality control, we might wish to test that the thickness of a sheet produced at a mill is 2 mm. To test this, we can randomly select sample sheets and measure the thickness to obtain our sample data. Then, we can <a id="_idIndexMarker660"/>use a <strong class="bold">t-test</strong> to test our null hypothesis, <img alt="" src="image/Formula_06_015.png"/>, that the mean paper thickness is 2 mm, against the alternative hypothesis, <img alt="" src="image/Formula_06_016.png"/>, that the mean paper thickness is not 2 mm. We can use the SciPy <strong class="source-inline">stats</strong> module to compute a t statistic and a <img alt="" src="image/Formula_06_017.png"/> value. If the <img alt="" src="image/Formula_06_018.png"/> value is below 0.05, then we accept the null hypothesis with 5% significance (95% confidence). If the <img alt="" src="image/Formula_06_019.png"/> value is larger than 0.05, then we must reject the null hypothesis in favor of our <span class="No-Break">alternative hypothesis.</span></p>
			<p>In this recipe, we will <a id="_idIndexMarker661"/>learn how to use a t-test to test whether the assumed <a id="_idIndexMarker662"/>population mean is valid given a sample. </p>
			<h2 id="_idParaDest-267"><a id="_idTextAnchor266"/>Getting ready</h2>
			<p>For this recipe we will need the pandas package imported as <strong class="source-inline">pd</strong> and the SciPy <strong class="source-inline">stats</strong> module imported using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
from scipy import stats</pre>
			<p>Let’s learn how to perform t-tests using the SciPy <span class="No-Break"><strong class="source-inline">stats</strong></span><span class="No-Break"> module.</span></p>
			<h2 id="_idParaDest-268"><a id="_idTextAnchor267"/>How to do it...</h2>
			<p>Follow these<a id="_idIndexMarker663"/> steps to use a t-test to test the validity of a proposed <a id="_idIndexMarker664"/>population mean given some <span class="No-Break">sample data:</span></p>
			<ol>
				<li value="1">First, we must load the data into a <span class="No-Break">pandas </span><span class="No-Break"><strong class="source-inline">Series</strong></span><span class="No-Break">:</span><pre class="console">
sample = pd.Series([</pre><pre class="console">
    2.4, 2.4, 2.9, 2.6, 1.8, 2.7, 2.6, 2.4, 2.8, </pre><pre class="console">
    2.4, 2.4, 2.4, 2.7, 2.7, 2.3, 2.4, 2.4, 3.2, </pre><pre class="console">
    2.9, 2.5, 2.5, 3.2, 2. , 2.3, 3. , 1.5, 3.1,</pre><pre class="console">
    2.5, 2.2, 2.5, 2.1,1.8, 3.1, 2.4, 3. , 2.5,</pre><pre class="console">
    2.7, 2.1, 2.3, 2.2, 2.5, 2.6, 2.5, 2.8, 2.5,</pre><pre class="console">
    2.9, 2.1, 2.8, 2.1, 2.3</pre><pre class="console">
])</pre></li>
				<li>Now, let’s set the hypothesized population mean and the significance level that we will be <span class="No-Break">testing at:</span><pre class="console">
mu0 = 2.0</pre><pre class="console">
significance = 0.05</pre></li>
				<li>Next, we will use the <strong class="source-inline">ttest_1samp</strong> routine from the SciPy <strong class="source-inline">stats</strong> module to generate the t statistic and the <img alt="" src="image/Formula_06_020.png"/> <span class="No-Break">value:</span><pre class="console">
t_statistic, p_value = stats.ttest_1samp(sample, mu0)</pre><pre class="console">
print(f"t stat: {t_statistic}, p value: {p_value}")</pre><pre class="console">
# t stat: 9.752368720068665, p value: 4.596949515944238e-13</pre></li>
				<li>Finally, let’s test <a id="_idIndexMarker665"/>whether the <img alt="" src="image/Formula_06_020.png"/> value is smaller than the <a id="_idIndexMarker666"/>significance level <span class="No-Break">we chose:</span><pre class="console">
if p_value &lt;= significance:</pre><pre class="console">
    print("Reject H0 in favour of H1: mu != 2.0")</pre><pre class="console">
else:</pre><pre class="console">
    print("Accept H0: mu = 2.0")</pre><pre class="console">
# Reject H0 in favour of H1: mu != 2.0</pre></li>
			</ol>
			<p>We can conclude with 95% confidence that the mean of the population from which the data was sampled is not equal to 2. (Given that most of the numbers shown in the sample are greater than 2, this isn’t much of a surprise.) We can be very confident that this is the case given how small the <img alt="" src="image/Formula_06_020.png"/> value <span class="No-Break">is here.</span></p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor268"/>How it works...</h2>
			<p>The t statistic is computed using the <span class="No-Break">following formula:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_06_023.png"/></p>
			<p>Here, <img alt="" src="image/Formula_06_024.png"/> is the hypothesized mean (from the null hypothesis), <img alt="" src="image/Formula_06_025.png"/> is the sample mean, <img alt="" src="image/Formula_06_026.png"/> is the sample standard deviation, and <img alt="" src="image/Formula_06_027.png"/> is the size of the sample. The t statistic is an estimation of the difference between the observed sample mean and the hypothesized population mean, <img alt="" src="image/Formula_06_028.png"/>, normalized by the standard error. Assuming the population is normally distributed, the t statistic will follow a t distribution with <img alt="" src="image/Formula_06_029.png"/> degrees of freedom. Looking at where the t statistic lies within the corresponding student t distribution gives us an idea of how likely it is that the sample mean we observed came from the population with the hypothesized mean. This is given in the form of a <img alt="" src="image/Formula_06_030.png"/> <span class="No-Break">value.</span></p>
			<p>The <img alt="" src="image/Formula_06_020.png"/> value is the <a id="_idIndexMarker667"/>probability of observing a more extreme value than<a id="_idIndexMarker668"/> the sample mean we have observed, given the assumption that the population mean is equal to <img alt="" src="image/Formula_06_032.png"/>. If the <img alt="" src="image/Formula_06_033.png"/> value is smaller than the significance value we have chosen, then we cannot expect the true population mean to be the value, <img alt="" src="image/Formula_06_034.png"/>, that we assumed. In this case, we accept the alternative hypothesis that the true population norm is not equal <span class="No-Break">to <img alt="" src="image/Formula_06_035.png"/>.</span></p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor269"/>There’s more...</h2>
			<p>The test that we demonstrated in this recipe is the most basic use of a t-test. Here, we compared the sample mean to a hypothesized population mean to decide whether it was reasonable that the mean of the whole population is this hypothesized value. More generally, we can use t-tests to compare two independent populations given samples taken from each using a <strong class="bold">two-sample t-test</strong>, or <a id="_idIndexMarker669"/>compare the populations where data is paired (in some way) using<a id="_idIndexMarker670"/> a <strong class="bold">paired t-test</strong>. This makes the t-test an important tool for <span class="No-Break">a statistician.</span></p>
			<p>Significance<a id="_idIndexMarker671"/> and confidence<a id="_idIndexMarker672"/> are two concepts that occur frequently in statistics. A statistically significant result has a high probability of being correct. In many contexts, we consider any result that has a probability of being wrong below a certain threshold (usually either 5% or 1%) to be statistically significant. Confidence is a quantification of how certain we are about a result. The confidence of a result is 1 minus <span class="No-Break">the significance.</span></p>
			<p>Unfortunately, the significance of a result is something that is often misused or misunderstood. To say that a result is statistically significant at 5% is to say that there is a 5% chance that we have wrongly accepted the null hypothesis. That is, if we repeated the same test on 20 other samples from the population, we would expect at least one of them to give the opposite result. That, however, is not to say that one of them is guaranteed to <span class="No-Break">do so.</span></p>
			<p>High significance indicates that we are more sure that the conclusion we have reached is correct, but it is certainly not a guarantee that this is indeed the case. The results found in this recipe are evidence for this; the sample that we used was drawn from a population with a mean of <strong class="source-inline">2.5</strong> and a standard deviation of <strong class="source-inline">0.35</strong>. (Some rounding was applied to the sample after creation, which will have altered the distribution slightly.) This is not to say that our analysis is wrong, or that the conclusion we reached from our sample is not the <span class="No-Break">right one.</span></p>
			<p>It is important to remember that t-tests are only valid when the underlying populations follow a normal distribution, or at least approximately do so. If this is not the case, then you might need to use a non-parametric test instead. We will discuss this in the <em class="italic">Testing hypotheses for non-parametric </em><span class="No-Break"><em class="italic">data</em></span><span class="No-Break"> recipe.</span></p>
			<h1 id="_idParaDest-271"><a id="_idTextAnchor270"/>Testing hypotheses using ANOVA</h1>
			<p>Suppose we have designed an experiment that tests two new processes against the current process and we want to test whether the results of these new processes are different from the current process. In this case, we can use <strong class="bold">Analysis of Variance</strong> (<strong class="bold">ANOVA</strong>) to <a id="_idIndexMarker673"/>help us determine whether there are any differences between the mean values of the three sets of results (for this, we need to assume that each sample is drawn from a normal distribution with a <span class="No-Break">common variance).</span></p>
			<p>In this recipe, we will <a id="_idIndexMarker674"/>learn how to use ANOVA <a id="_idIndexMarker675"/>to compare multiple samples with <span class="No-Break">one another.</span></p>
			<h2 id="_idParaDest-272"><a id="_idTextAnchor271"/>Getting ready</h2>
			<p>For this recipe, we need the SciPy <strong class="source-inline">stats</strong> module. We will also need to create a default random number generator instance using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<h2 id="_idParaDest-273"><a id="_idTextAnchor272"/>How to do it...</h2>
			<p>Follow these steps to perform a (one-way) ANOVA test to test for differences between three <span class="No-Break">different processes:</span></p>
			<ol>
				<li value="1">First, we will create some sample data, which we <span class="No-Break">will analyze:</span><pre class="console">
current = rng.normal(4.0, 2.0, size=40)</pre><pre class="console">
process_a = rng.normal(6.2, 2.0, size=25)</pre><pre class="console">
process_b = rng.normal(4.5, 2.0, size=64)</pre></li>
				<li>Next, we will set the significance level for <span class="No-Break">our test:</span><pre class="console">
significance = 0.05</pre></li>
				<li>Then, we will use the <strong class="source-inline">f_oneway</strong> routine from the SciPy <strong class="source-inline">stats</strong> module to generate the F-statistic and the <img alt="" src="image/Formula_06_036.png"/> <span class="No-Break">value:</span><pre class="console">
F_stat, p_value = stats.f_oneway(</pre><pre class="console">
    current, process_a, process_b)</pre><pre class="console">
print(f"F stat: {F_stat}, p value: {p_value}")</pre><pre class="console">
# F stat: 9.949052026027028, p value: 9.732322721019206e-05</pre></li>
				<li>Now, we must test whether the <img alt="" src="image/Formula_06_020.png"/> value is sufficiently small to see whether we should accept or reject our null hypothesis that all mean values <span class="No-Break">are equal:</span><pre class="console">
if p_value &lt;= significance:</pre><pre class="console">
    print("Reject H0: there is a difference between means")</pre><pre class="console">
else:</pre><pre class="console">
    print("Accept H0: all means equal")</pre><pre class="console">
# Reject H0: there is a difference between means</pre></li>
			</ol>
			<p>Here, the <img alt="" src="image/Formula_06_038.png"/> value <a id="_idIndexMarker676"/>is so small (of order <img alt="" src="image/Formula_06_039.png"/>) that<a id="_idIndexMarker677"/> the difference is significant not only at 95% confidence (that is, <img alt="" src="image/Formula_06_040.png"/>) but also at 99% <span class="No-Break">confidence (<img alt="" src="image/Formula_06_041.png"/>).</span></p>
			<h2 id="_idParaDest-274"><a id="_idTextAnchor273"/>How it works...</h2>
			<p>ANOVA is a<a id="_idIndexMarker678"/> powerful technique for comparing multiple samples against one another simultaneously. It works by comparing the variation in the samples relative to the overall variation. ANOVA is especially powerful when comparing three or more samples since no cumulative error is incurred from running multiple tests. Unfortunately, if ANOVA detects that not all the mean values are equal, then there is no way from the test information to determine which sample(s) are significantly different from the others. For this, you would need to use an extra test to find <span class="No-Break">the differences.</span></p>
			<p>The <strong class="source-inline">f_oneway</strong> SciPy <strong class="source-inline">stats</strong> module routine performs a one-way ANOVA test – the test statistic generated in ANOVA follows an F-distribution. Again, the <img alt="" src="image/Formula_06_018.png"/> value is the crucial piece of information coming from the test. We accept the null hypothesis if the <img alt="" src="image/Formula_06_030.png"/> value is less<a id="_idIndexMarker679"/> than our predefined<a id="_idIndexMarker680"/> significance level (in this recipe, 5%) and reject the null <span class="No-Break">hypothesis otherwise.</span></p>
			<h2 id="_idParaDest-275"><a id="_idTextAnchor274"/>There’s more...</h2>
			<p>The <a id="_idIndexMarker681"/>ANOVA method is very flexible. The one-way ANOVA test<a id="_idIndexMarker682"/> that we presented here is the most simple case as there is only a single factor to test. A two-way ANOVA test<a id="_idIndexMarker683"/> can be used to test for differences between two different factors. This is useful in clinical trials of medicines, for example, where we test against a control measure but also measure the effects of gender (for instance) on the outcomes. Unfortunately, SciPy does not have a routine for performing two-way ANOVA in the <strong class="source-inline">stats</strong> module. You will need to use an alternative package, such as the <span class="No-Break"><strong class="source-inline">statsmodels</strong></span><span class="No-Break"> package.</span></p>
			<p>As mentioned previously, ANOVA can only detect whether there are differences. It cannot detect where these differences occur if there are significant differences. For example, we can use Durnett’s test to test whether the other sample’s mean values differ from a control sample, or Tukey’s range test to test each group’s mean against every other <span class="No-Break">group’s mean.</span></p>
			<h1 id="_idParaDest-276"><a id="_idTextAnchor275"/>Testing hypotheses for non-parametric data</h1>
			<p>Both t-tests<a id="_idIndexMarker684"/> and ANOVA have a major drawback: the<a id="_idIndexMarker685"/> population that is being sampled must follow a normal distribution. In many applications, this is not too restrictive because many real-world population values follow a normal distribution, or some rules, such as the central limit theorem, allow us to analyze some related data. However, it is simply not true that all possible population values follow a normal distribution in any reasonable way. For these (thankfully, rare) cases, we need some alternative test statistics to use as replacements for t-tests <span class="No-Break">and ANOVA.</span></p>
			<p>In this recipe, we will use a Wilcoxon rank-sum test<a id="_idIndexMarker686"/> and the Kruskal-Wallis test<a id="_idIndexMarker687"/> to test for differences between two (or more, in the latter <span class="No-Break">case) populations.</span></p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor276"/>Getting ready</h2>
			<p>For this recipe, we<a id="_idIndexMarker688"/> will need the pandas package<a id="_idIndexMarker689"/> imported as <strong class="source-inline">pd</strong>, the SciPy <strong class="source-inline">stats</strong> module, and a default random number generator instance created using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<p>Let’s learn how to use the non-parametric hypothesis testing tools in <span class="No-Break">SciPy </span><span class="No-Break"><strong class="source-inline">stats</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-278"><a id="_idTextAnchor277"/>How to do it...</h2>
			<p>Follow these steps to compare the populations of two or more populations that are not <span class="No-Break">normally distributed:</span></p>
			<ol>
				<li value="1">First, we will generate some sample data to use in <span class="No-Break">our analysis:</span><pre class="console">
sample_A = rng.uniform(2.5, 3.5, size=25)</pre><pre class="console">
sample_B = rng.uniform(3.0, 4.4, size=25)</pre><pre class="console">
sample_C = rng.uniform(3.1, 4.5, size=25)</pre></li>
				<li>Next, we will set the significance level that we will use in <span class="No-Break">this analysis:</span><pre class="console">
significance = 0.05</pre></li>
				<li>Now, we will use the <strong class="source-inline">stats.kruskal</strong> routine to generate the test statistic and the <img alt="" src="image/Formula_06_038.png"/> value for the null hypothesis that the populations have the same <span class="No-Break">median value:</span><pre class="console">
statistic, p_value = stats.kruskal(sample_A, sample_B,</pre><pre class="console">
    sample_C)</pre><pre class="console">
print(f"Statistic: {statistic}, p value: {p_value}")</pre><pre class="console">
# Statistic: 40.22214736842102, p value: 1.8444703308682906e-09</pre></li>
				<li>We will use a conditional statement to print a statement about the outcome of <span class="No-Break">the test:</span><pre class="console">
if p_value &lt;= significance:</pre><pre class="console">
    print("There are differences between population  medians")</pre><pre class="console">
else:</pre><pre class="console">
    print("Accept H0: all medians equal")</pre><pre class="console">
# There are differences between population medians</pre></li>
				<li>Now, we will use <a id="_idIndexMarker690"/>Wilcoxon rank-sum tests to obtain the <img alt="" src="image/Formula_06_033.png"/> values for the comparisons between each pair of samples. The null hypothesis for these tests is that they are drawn from the <span class="No-Break">same distribution:</span><pre class="console">
_, p_A_B = stats.ranksums(sample_A, sample_B)</pre><pre class="console">
_, p_A_C = stats.ranksums(sample_A, sample_C)</pre><pre class="console">
_, p_B_C = stats.ranksums(sample_B, sample_C)</pre></li>
				<li>Next, we will<a id="_idIndexMarker691"/> use conditional <a id="_idIndexMarker692"/>statements to print out messages for those comparisons that indicate a <span class="No-Break">significant difference:</span><pre class="console">
if p_A_B &lt;= significance:</pre><pre class="console">
    print("Significant differences between A and B,</pre><pre class="console">
        p value", p_A_B)</pre><pre class="console">
# Significant differences between A and B, p value</pre><pre class="console">
1.0035366080480683e-07</pre><pre class="console">
if p_A_C &lt;= significance:</pre><pre class="console">
    print("Significant differences between A and C,</pre><pre class="console">
        p value", p_A_C)</pre><pre class="console">
# Significant differences between A and C, p value</pre><pre class="console">
2.428534673701913e-08</pre><pre class="console">
if p_B_C &lt;= significance:</pre><pre class="console">
    print("Significant differences between B and C,</pre><pre class="console">
        p value", p_B_C)</pre><pre class="console">
else:</pre><pre class="console">
    print("No significant differences between B and C,</pre><pre class="console">
        p value", p_B_C)</pre><pre class="console">
# No significant differences between B and C, p value</pre><pre class="console">
0.3271631660572756</pre></li>
			</ol>
			<p>These printed lines<a id="_idIndexMarker693"/> show that our tests have detected <a id="_idIndexMarker694"/>significant differences between populations A and B and populations A and C, but not between populations B and C. </p>
			<h2 id="_idParaDest-279"><a id="_idTextAnchor278"/>How it works...</h2>
			<p>We say that data is non-parametric if the population from which the data was sampled does not follow a distribution that can be described by a small number of parameters. This usually means that the population is not normally distributed but is broader than this. In this recipe, we sampled from uniform distributions, but this is still a more structured example than we would generally have when non-parametric tests are necessary. Non-parametric tests can and should be used in any situation where we are not sure about the underlying distribution. The cost of doing this is that the tests are slightly <span class="No-Break">less powerful.</span></p>
			<p>The first step of any (real) analysis should be to plot a histogram of the data and inspect the distribution visually. If you draw a random sample from a normally distributed population, you might also expect the sample to be normally distributed (we have seen this several times in this book). If your sample shows the characteristic bell curve of a normal distribution, then it is fairly likely that the population is itself normally distributed. You might also use a <strong class="bold">kernel density estimation</strong> plot<a id="_idIndexMarker695"/> to help determine the distribution. This is available on the pandas plotting interface as <strong class="source-inline">kind="kde"</strong>. If you still aren’t sure whether the population is normal, you can apply a statistical test, such as D’Agostino’s K-squared test or Pearson’s Chi-squared test for normality. These two tests are combined into a single routine to test for normality called <strong class="source-inline">normaltest</strong> in the SciPy <strong class="source-inline">stats</strong> module, along with several other tests <span class="No-Break">for normality.</span></p>
			<p>The Wilcoxon rank-sum test<a id="_idIndexMarker696"/> is a non-parametric replacement for a two-sample t-test. Unlike the t-test, the rank-sum test does not compare the sample mean values to quantify whether the populations have different distributions. Instead, it combines the data of the samples and ranks them in order of size. The test statistic is generated from the sum of the ranks from the sample with the fewest elements. From here, as usual, we generate a <img alt="" src="image/Formula_06_033.png"/> value for the null hypothesis that the two populations have the <span class="No-Break">same distribution.</span></p>
			<p>The Kruskal-Wallis test<a id="_idIndexMarker697"/> is a non-parametric replacement for a one-way ANOVA test. Like the rank-sum test, it uses the ranking of the sample data to generate a test statistic and <img alt="" src="image/Formula_06_030.png"/> values for the null hypothesis that all the populations have the same median value. As with one-way ANOVA, we can only detect whether all the populations have the same median, not where the differences lie. For this, we would have to use <span class="No-Break">additional tests.</span></p>
			<p>In this recipe, we <a id="_idIndexMarker698"/>used the Kruskal-Wallis test to determine<a id="_idIndexMarker699"/> whether there were any significant differences between the populations corresponding to our three samples. A difference was detected with a <img alt="" src="image/Formula_06_018.png"/> value with a very small <img alt="" src="image/Formula_06_020.png"/> value. We then used rank-sum tests to determine where significant differences occur between the populations. Here, we found that sample A is significantly different from samples B and C, but B is not significantly different from sample C. This is hardly surprising given the way that these samples <span class="No-Break">were generated.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Unfortunately, since we have used multiple tests in this recipe, our overall confidence in our conclusions is not as high as we might expect it to be. We performed four tests with 95% confidence, which means our overall confidence in our conclusion is only approximately 81%. This is because errors aggregate over multiple tests, reducing the overall confidence. To correct this, we would have to adjust our significance threshold for each test, using the Bonferroni correction (<span class="No-Break">or similar).</span></p>
			<h1 id="_idParaDest-280"><a id="_idTextAnchor279"/>Creating interactive plots with Bokeh</h1>
			<p>Test statistics<a id="_idIndexMarker700"/> and numerical reasoning<a id="_idIndexMarker701"/> are good for systematically analyzing sets of data. However, they don’t give us a good picture of the whole set of data like a plot would. Numerical values are definitive but can be difficult to understand, especially in statistics, whereas a plot instantly illustrates differences between sets of data and trends. For this reason, there is a large number of libraries for plotting data in even more creative ways. One particularly interesting package for producing plots of data is Bokeh, which allows us to create interactive plots in the browser by leveraging <span class="No-Break">JavaScript libraries.</span></p>
			<p>In this recipe, we will learn how to use Bokeh to create an interactive plot that can be displayed in <span class="No-Break">the browser.</span></p>
			<h2 id="_idParaDest-281"><a id="_idTextAnchor280"/>Getting ready</h2>
			<p>For this <a id="_idIndexMarker702"/>recipe, we will need the pandas package<a id="_idIndexMarker703"/> imported as <strong class="source-inline">pd</strong>, the NumPy package imported as <strong class="source-inline">np</strong>, an instance of the default random number generator constructed with the following code, and the <strong class="source-inline">plotting</strong> module from Bokeh, which we have imported under the <span class="No-Break"><strong class="source-inline">bk</strong></span><span class="No-Break"> alias:</span></p>
			<pre class="source-code">
from bokeh import plotting as bk
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<h2 id="_idParaDest-282"><a id="_idTextAnchor281"/>How to do it...</h2>
			<p>These steps show how to create an interactive plot in the browser <span class="No-Break">using Bokeh:</span></p>
			<ol>
				<li value="1">First, we need to create some sample data <span class="No-Break">to plot:</span><pre class="console">
date_range = pd.date_range("2020-01-01", periods=50)</pre><pre class="console">
data = rng.normal(0, 3, size=50).cumsum()</pre><pre class="console">
series = pd.Series(data, index=date_range)</pre></li>
				<li>Next, we must specify the output file where the HTML code for the plot will be stored by using the <span class="No-Break"><strong class="source-inline">output_file</strong></span><span class="No-Break"> routine:</span><pre class="console">
bk.output_file("sample.html")</pre></li>
				<li>Now, we will create a new figure and set the title and axes labels, and set the <img alt="" src="image/Formula_06_050.png"/>-axis type to <strong class="source-inline">datetime</strong> so that our date index will be <span class="No-Break">correctly displayed:</span><pre class="console">
fig = bk.figure(title="Time series data", </pre><pre class="console">
                       x_axis_label="date",</pre><pre class="console">
                       x_axis_type="datetime",</pre><pre class="console">
                       y_axis_label="value")</pre></li>
				<li>We will add the data to the figure as <span class="No-Break">a line:</span><pre class="console">
fig.line(date_range, series)</pre></li>
				<li>Finally, we can use either the <strong class="source-inline">show</strong> routine or the <strong class="source-inline">save</strong> routine to save or update the HTML in the specified output file. We are using <strong class="source-inline">show</strong> here to cause the plot to open in <span class="No-Break">the browser:</span><pre class="console">
bk.show(fig)</pre></li>
			</ol>
			<p>Bokeh plots <a id="_idIndexMarker704"/>are not static objects and are supposed to be<a id="_idIndexMarker705"/> interactive via the browser. The data as it will appear in the Bokeh plot has been recreated here, using <strong class="source-inline">matplotlib</strong> <span class="No-Break">for comparison:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer797">
					<img alt="Figure 6.4 – Plot of time series data created using Matplotlib&#13;&#10;" src="image/6.4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Plot of time series data created using Matplotlib</p>
			<p>The real power of Bokeh is its ability to insert dynamic, interactive plots into web pages and documents (for example, Jupyter notebooks) so that the reader can look into the detail of the data that <span class="No-Break">is plotted.</span></p>
			<h2 id="_idParaDest-283"><a id="_idTextAnchor282"/>How it works...</h2>
			<p>Bokeh <a id="_idIndexMarker706"/>uses a JavaScript library to render a plot in a<a id="_idIndexMarker707"/> browser, using data provided by the Python backend. The advantage of this is that it can generate plots that a user can inspect for themselves. For instance, we can zoom in to see detail in the plot that might otherwise be hidden, or pan through the data naturally. The example given in this recipe is just a taster of what is possible <span class="No-Break">using Bokeh.</span></p>
			<p>The <strong class="source-inline">figure</strong> routine<a id="_idIndexMarker708"/> creates an object representing the plot, which we add elements to – such as a line through the data points – in the same way that we would add plots to a Matplotlib <strong class="source-inline">Axes</strong> object. In this recipe, we created a simple HTML file that contains JavaScript code to render the data. This HTML code is dumped to the specified file whenever we save or, as is in the recipe, call the <strong class="source-inline">show</strong> routine. In practice, the smaller the <img alt="" src="image/Formula_06_033.png"/> value, the more confident we can be that the hypothesized population mean <span class="No-Break">is correct.</span></p>
			<h2 id="_idParaDest-284"><a id="_idTextAnchor283"/>There’s more...</h2>
			<p>The capabilities of Bokeh go far beyond what is described here. Bokeh plots can be embedded in files such as Jupyter notebooks, which are also rendered in the browser, or into existing websites. If you are using a Jupyter notebook, you should use the <strong class="source-inline">output_notebook</strong> routine instead of the <strong class="source-inline">output_file</strong> routine to print the plot directly into the notebook. It has a wide array of different plotting styles, supports sharing data between plots (data can be selected in one plot and highlighted in the other(s), for example), and supports <span class="No-Break">streaming data.</span></p>
			<h1 id="_idParaDest-285"><a id="_idTextAnchor284"/>Further reading</h1>
			<p>There are a large number of textbooks on statistics and statistical theory. The following books are good references for the statistics covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Mendenhall, W., Beaver, R., and Beaver, B. (2006), <em class="italic">Introduction To Probability And Statistics</em>. 12th ed., (Belmont, Calif.: <span class="No-Break">Thomson Brooks/Cole).</span></li>
				<li><span class="selectable">Freedman, D., Pisani, R., and Purves, R. (2007</span>),<span class="selectable"> </span><em class="italic">Statistics</em><span class="selectable">. New York: </span><span class="No-Break"><span class="selectable">W.W. Norton.</span></span></li>
			</ul>
			<p>The pandas documentation (<a href="https://pandas.pydata.org/docs/index.html">https://pandas.pydata.org/docs/index.html</a>) and the following pandas book serve as good references for working <span class="No-Break">with pandas:</span></p>
			<ul>
				<li>McKinney, W., (2017),<em class="italic"> Python for Data Analysis</em>. 2nd ed., (Sebastopol: O’Reilly Media, <span class="No-Break">Inc, US).</span></li>
			</ul>
			<p>The SciPy documentation (<a href="https://docs.scipy.org/doc/scipy/tutorial/stats.html">https://docs.scipy.org/doc/scipy/tutorial/stats.html</a>) also contains detailed information about the statistics module that was used several times in <span class="No-Break">this chapter.</span></p>
		</div>
	</body></html>