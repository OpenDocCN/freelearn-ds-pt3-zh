<html><head></head><body>
		<div id="_idContainer061">
			<h1 id="_idParaDest-138" class="chapter-number"><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.2.1">Data Sinks</span></h1>
			<p><span class="koboSpan" id="kobo.3.1">In the world of modern data processing, crucial decisions about data management, storage, and processing will determine successful outcomes. </span><span class="koboSpan" id="kobo.3.2">In this chapter, we will deep dive into three important pillars that underpin effective data processing pipelines: selecting the</span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.4.1"> right </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">data sink</span></strong><span class="koboSpan" id="kobo.6.1">, choosing the optimal file type, and mastering partitioning strategies. </span><span class="koboSpan" id="kobo.6.2">By discussing these critical elements and their real-world applications, this chapter will equip you with the insights and strategies needed to architect data solutions that optimize efficiency, scalability, and performance within the complicated landscape of data </span><span class="No-Break"><span class="koboSpan" id="kobo.7.1">processing technologies.</span></span></p>
			<p><span class="koboSpan" id="kobo.8.1">In this chapter, we will discuss the </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">following topics:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.10.1">Choosing the right data sink for your </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">use case</span></span></li>
				<li><span class="koboSpan" id="kobo.12.1">Choosing the right file type for your </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">use case</span></span></li>
				<li><span class="No-Break"><span class="koboSpan" id="kobo.14.1">Navigating partitioning</span></span></li>
				<li><span class="koboSpan" id="kobo.15.1">Designing an online retail </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">data platform</span></span></li>
			</ul>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.17.1">Technical requirements</span></h1>
			<p><span class="koboSpan" id="kobo.18.1">For this chapter, we will need to install the </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">following libraries:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.20.1">
pip install pymongo==4.8.0
pip install pyarrow
pip install confluent_kafka
pip install psycopg2-binary==2.9.9</span></pre>			<p><span class="koboSpan" id="kobo.21.1">As always, you can find all the code for this chapter in this book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter07"><span class="No-Break"><span class="koboSpan" id="kobo.23.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter07</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.24.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.25.1">Each section is followed by a script with a similar naming convention, so feel free to execute the scripts and/or follow along by reading </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">this chapter.</span></span></p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.27.1">Choosing the right data sink for your use case</span></h1>
			<p><span class="koboSpan" id="kobo.28.1">A data sink</span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.29.1"> refers to a destination or endpoint where data is directed or stored. </span><span class="koboSpan" id="kobo.29.2">The term “sink” is used metaphorically to convey the idea of data flowing into and being absorbed by a designated location. </span><span class="koboSpan" id="kobo.29.3">Data sinks are commonly used as storage locations where data can be permanently or temporarily stored. </span><span class="koboSpan" id="kobo.29.4">This storage can be in the form of databases, files, or other </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">data structures.</span></span></p>
			<p><span class="koboSpan" id="kobo.31.1">Data engineers and data scientists often work with a variety of data sinks, depending on their specific tasks and use cases. </span><span class="koboSpan" id="kobo.31.2">Let’s look at some common data sinks, along with code examples, while considering the pros and cons of </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">each type.</span></span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.33.1">Relational databases</span></h2>
			<p><span class="koboSpan" id="kobo.34.1">Relational databases</span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.35.1"> are a type of </span><strong class="bold"><span class="koboSpan" id="kobo.36.1">database management system</span></strong><span class="koboSpan" id="kobo.37.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.38.1">DBMS</span></strong><span class="koboSpan" id="kobo.39.1">) that</span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.40.1"> organizes data into </span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.41.1">tables with rows and columns, where each row represents a record and each column represents a field. </span><span class="koboSpan" id="kobo.41.2">The relationships between tables are established using keys. </span><span class="koboSpan" id="kobo.41.3">The primary key uniquely identifies each record in a table, and foreign keys create links </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">between tables.</span></span></p>
			<h3><span class="koboSpan" id="kobo.43.1">Overview of relational databases</span></h3>
			<p><span class="koboSpan" id="kobo.44.1">The following is a </span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.45.1">quick overview of the key components of </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">relational databases:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.47.1">Tables</span></strong><span class="koboSpan" id="kobo.48.1">: Data </span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.49.1">is organized into tables, where each table represents a specific entity or concept. </span><span class="koboSpan" id="kobo.49.2">For example, in a database for a library, there might be tables for books, authors, </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">and borrowers.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.51.1">Rows and columns</span></strong><span class="koboSpan" id="kobo.52.1">: Each </span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.53.1">table consists of rows and columns. </span><span class="koboSpan" id="kobo.53.2">A row represents a specific record (e.g., a book), and each column represents a specific attribute or field of that record (e.g., title, author, and </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">publication year).</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.55.1">Keys</span></strong><span class="koboSpan" id="kobo.56.1">: Keys are </span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.57.1">used to establish relationships between tables. </span><span class="koboSpan" id="kobo.57.2">The primary key uniquely identifies each record in a table, and foreign keys in related tables create links </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">between them.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.59.1">Structured Query Language</span></strong><span class="koboSpan" id="kobo.60.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.61.1">SQL</span></strong><span class="koboSpan" id="kobo.62.1">): Relational databases use SQL for querying</span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.63.1"> and manipulating data. </span><span class="koboSpan" id="kobo.63.2">SQL allows users to retrieve, insert, update, and delete data, as well as define and modify the </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">database’s structure.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.65.1">In the data field, we usually </span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.66.1">find relational databases in the </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">following scenarios:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.68.1">Structured data</span></strong><span class="koboSpan" id="kobo.69.1">: If your data has a well-defined structure with clear relationships between entities, a relational database is a </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">suitable choice.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.71.1">Data integrity requirements</span></strong><span class="koboSpan" id="kobo.72.1">: If maintaining data integrity is critical for your application (e.g., in financial systems or healthcare applications), a relational database provides mechanisms to enforce </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">integrity constraints.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.74.1">Atomicity, consistency, isolation, and durability</span></strong><strong class="bold"><span class="koboSpan" id="kobo.75.1"> (ACID) properties</span></strong><span class="koboSpan" id="kobo.76.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.77.1">Atomicity</span></strong><span class="koboSpan" id="kobo.78.1"> ensures a</span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.79.1"> transaction is an all-or-nothing operation: either all changes are committed, or none are. </span><span class="koboSpan" id="kobo.79.2">For instance, in transferring money between accounts, atomicity guarantees both balances are updated together or not at all. </span><strong class="bold"><span class="koboSpan" id="kobo.80.1">Consistency</span></strong><span class="koboSpan" id="kobo.81.1"> means transactions move the database from one valid state to another while adhering to integrity constraints. </span><span class="koboSpan" id="kobo.81.2">If a rule such as unique customer IDs is violated, the transaction is rolled back to maintain consistency. </span><strong class="bold"><span class="koboSpan" id="kobo.82.1">Isolation</span></strong><span class="koboSpan" id="kobo.83.1"> ensures transactions execute independently, preventing interference and visibility of uncommitted changes between concurrent transactions. </span><span class="koboSpan" id="kobo.83.2">This avoids issues such as dirty reads. </span><span class="koboSpan" id="kobo.83.3">Finally, </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">durability</span></strong><span class="koboSpan" id="kobo.85.1"> guarantees that once committed, changes persist, even after system failures, ensuring the permanence of updates such as contact information in an online application. </span><span class="koboSpan" id="kobo.85.2">If your application demands adherence to ACID properties, relational databases are designed to meet </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">these requirements.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.87.1">Complex queries</span></strong><span class="koboSpan" id="kobo.88.1">: If your application involves complex queries and reporting needs, relational </span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.89.1">databases, with their SQL querying capabilities, are well-suited for </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">such scenarios.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.91.1">There are many different options for building relational databases, as we will see in the </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">following section.</span></span></p>
			<h3><span class="koboSpan" id="kobo.93.1">Different options for relational database management systems</span></h3>
			<p><span class="koboSpan" id="kobo.94.1">There are many </span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.95.1">different </span><strong class="bold"><span class="koboSpan" id="kobo.96.1">relational database management systems</span></strong><span class="koboSpan" id="kobo.97.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.98.1">RDBMSs</span></strong><span class="koboSpan" id="kobo.99.1">) out there. </span><span class="koboSpan" id="kobo.99.2">We’ve summarized the main ones in the </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">following table:</span></span></p>
			<table id="table001-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.101.1">Database</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.102.1">Description</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.103.1">MySQL</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.104.1">An open source RDBMS known for its speed, reliability, and wide usage in </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">web development</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.106.1">PostgreSQL</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.107.1">An open source RDBMS with advanced features, extensibility, and support for </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">complex queries</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.109.1">Oracle Database</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.110.1">A commercial RDBMS known for its scalability, security, and comprehensive set of data </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">management features</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.112.1">Microsoft </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">SQL Server</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.114.1">A commercial RDBMS by Microsoft that integrates with Microsoft technologies and has business </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">intelligence support</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.116.1">SQLite</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.117.1">A lightweight, embedded, serverless RDBMS suitable for applications with low to moderate </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">database requirements</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.119.1">MariaDB</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.120.1">An open source RDBMS forked from MySQL that aims for compatibility while introducing </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">new features</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.122.1">Table 7.1 – Summary of RDBMSs</span></p>
			<p><span class="koboSpan" id="kobo.123.1">Now, let’s see an example of how to quickly set up a local relational database, connect to it, and write a </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">new table.</span></span></p>
			<h3><span class="koboSpan" id="kobo.125.1">An example of a PostgreSQL database</span></h3>
			<p><span class="koboSpan" id="kobo.126.1">First, we need to</span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.127.1"> install and set up PostgreSQL. </span><span class="koboSpan" id="kobo.127.2">This differs depending on the </span><strong class="bold"><span class="koboSpan" id="kobo.128.1">operating system</span></strong><span class="koboSpan" id="kobo.129.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.130.1">OS</span></strong><span class="koboSpan" id="kobo.131.1">), but the logic remains the same. </span><span class="koboSpan" id="kobo.131.2">The following script automates the process of installing and setting up PostgreSQL on macOS or Debian-based Linux </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">systems: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/setup_postgres.sh"><span class="No-Break"><span class="koboSpan" id="kobo.133.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/setup_postgres.sh</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.134.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.135.1">First, it detects the OS using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.136.1">uname</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.137.1"> command:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.138.1">
OS=$(uname)</span></pre>			<p><span class="koboSpan" id="kobo.139.1">If macOS is detected, it uses Homebrew to update package lists, install PostgreSQL, and start the PostgreSQL service. </span><span class="koboSpan" id="kobo.139.2">If a Debian-based Linux OS is detected, it uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.140.1">apt-get</span></strong><span class="koboSpan" id="kobo.141.1"> to update package lists, install PostgreSQL and its </span><strong class="source-inline"><span class="koboSpan" id="kobo.142.1">contrib</span></strong><span class="koboSpan" id="kobo.143.1"> package, and start the PostgreSQL service. </span><span class="koboSpan" id="kobo.143.2">Here’s the code for </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">installing macOS:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.145.1">
if [ "$OS" == "Darwin" ]; then
    echo "Detected macOS. </span><span class="koboSpan" id="kobo.145.2">Installing PostgreSQL via Homebrew..."
</span><span class="koboSpan" id="kobo.145.3">    brew update
    brew install postgresql
    brew services start postgresql</span></pre>			<p><span class="koboSpan" id="kobo.146.1">If your OS isn’t supported by this script, then the following error message will </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">be shown:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.148.1">
Unsupported OS. </span><span class="koboSpan" id="kobo.148.2">Please install PostgreSQL manually.</span></pre>			<p><span class="koboSpan" id="kobo.149.1">In this case, you will need to install PostgreSQL </span><em class="italic"><span class="koboSpan" id="kobo.150.1">manually and start the service</span></em><span class="koboSpan" id="kobo.151.1">. </span><span class="koboSpan" id="kobo.151.2">Once you’ve done that, you can continue to the second part of the script. </span><span class="koboSpan" id="kobo.151.3">The script then switches to the default </span><strong class="source-inline"><span class="koboSpan" id="kobo.152.1">postgres</span></strong><span class="koboSpan" id="kobo.153.1"> user to execute SQL commands that create a new database user if it doesn’t already exist, create a new database owned by this user, and grant all privileges on the database to the user, as </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">shown here:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.155.1">
psql postgres &lt;&lt; EOF
DO \$\$
BEGIN
    IF NOT EXISTS (
        SELECT FROM pg_catalog.pg_user
        WHERE usename = 'the_great_coder'
    ) THEN
        CREATE USER the_great_coder
        WITH PASSWORD 'the_great_coder_again';
    END IF;
END
\$\$;
EOF
psql postgres &lt;&lt; EOF
CREATE DATABASE learn_sql2 OWNER the_great_coder;
EOF
psql postgres &lt;&lt; EOF
-- Grant privileges to the user on the database
GRANT ALL PRIVILEGES ON DATABASE learn_sql2 TO the_great_coder;
EOF</span></pre>			<p><span class="koboSpan" id="kobo.156.1">To execute the</span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.157.1"> preceding code, do </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">the following:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.159.1">Make sure you pull the repository to your </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">local laptop.</span></span></li>
				<li><span class="koboSpan" id="kobo.161.1">Go to the folder where the repository </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">is located.</span></span></li>
				<li><span class="koboSpan" id="kobo.163.1">Open a terminal in the repository </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">folder location.</span></span></li>
				<li><span class="koboSpan" id="kobo.165.1">Execute the following commands to navigate to the </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">right folder:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.167.1">cd chapter7</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.168.1">cd setup</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.169.1">Validate that you’re in the right location and that you can see the </span><strong class="source-inline"><span class="koboSpan" id="kobo.170.1">setup_postgres.sh</span></strong><span class="koboSpan" id="kobo.171.1"> script, as </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">shown here:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.173.1">maria.zevrou@FVFGR3ANQ05P chapter7 %</span></strong><span class="koboSpan" id="kobo.174.1"> cd setup
</span><strong class="bold"><span class="koboSpan" id="kobo.175.1">maria.zevrou@FVFGR3ANQ05P set up %</span></strong><span class="koboSpan" id="kobo.176.1"> ls
</span><strong class="bold"><span class="koboSpan" id="kobo.177.1">setup_postgres.sh</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.178.1">Make the script executable by running the </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">following command:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.180.1">chmod +x setup_postgres.sh</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.181.1">Finally, run the actual script using the </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">following command:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.183.1">./setup_postgres.sh</span></strong></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.184.1">After executing</span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.185.1"> the script, you should see a confirmation message that the PostgreSQL setup, including the database and user creation, has </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">been completed:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.187.1">
PostgreSQL setup completed. </span><span class="koboSpan" id="kobo.187.2">Database and user created.</span></pre>			<p><span class="koboSpan" id="kobo.188.1">Now, we’re ready to execute the script so that we can write our incoming data to the database we created in the previous step. </span><span class="koboSpan" id="kobo.188.2">You can find this script </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/1.postgressql.py"><span class="No-Break"><span class="koboSpan" id="kobo.190.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/1.postgressql.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.191.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.192.1">This script connects to the PostgreSQL database that we created previously and manages a table within it. </span><span class="koboSpan" id="kobo.192.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.194.1">Let’s start by importing the </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">necessary libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.196.1">
import pandas as pd
import psycopg2
from psycopg2 import sql</span></pre></li>				<li><span class="koboSpan" id="kobo.197.1">Then, we must define several functions, starting with </span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">table_exists</span></strong><span class="koboSpan" id="kobo.199.1">. </span><span class="koboSpan" id="kobo.199.2">This function checks whether a specified table already exists in </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">the database:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.201.1">
def table_exists(cursor, table_name):
    cursor.execute(
        sql.SQL("SELECT EXISTS ( \
                SELECT 1 FROM information_schema.tables \
                WHERE table_name = %s)"),
        [table_name]
    )
    return cursor.fetchone()[0]</span></pre></li>				<li><span class="koboSpan" id="kobo.202.1">The next </span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.203.1">function we need is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.204.1">create_table</span></strong><span class="koboSpan" id="kobo.205.1"> function, which creates a new table if it doesn’t already exist within a specific schema. </span><span class="koboSpan" id="kobo.205.2">In our case, it will have three columns: </span><strong class="source-inline"><span class="koboSpan" id="kobo.206.1">id</span></strong><span class="koboSpan" id="kobo.207.1"> as the primary key, then </span><strong class="source-inline"><span class="koboSpan" id="kobo.208.1">name</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.209.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.210.1">age</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.212.1">
def create_table(cursor, table_name):
    cursor.execute(
        sql.SQL("""
            CREATE TABLE {} (
                id SERIAL PRIMARY KEY,
                name VARCHAR(255),
                age INT
            )
        """).format(sql.Identifier(table_name))
    )</span></pre></li>				<li><span class="koboSpan" id="kobo.213.1">Then, we must define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.214.1">insert_data</span></strong><span class="koboSpan" id="kobo.215.1"> function, which inserts rows of data into </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">the table:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.217.1">
def insert_data(cursor, table_name, data):
    cursor.executemany(
        sql.SQL("INSERT INTO {} (name, age) \
                VALUES (%s, %s)"
        ).format(sql.Identifier(table_name)),
        data
    )</span></pre></li>				<li><span class="koboSpan" id="kobo.218.1">Finally, we must use the following function to display the </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">retrieved data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.220.1">
def print_table_data(cursor, table_name):
    cursor.execute(
        sql.SQL(
            "SELECT * FROM {}"
        ).format(sql.Identifier(table_name))
    )
    rows = cursor.fetchall()
    for row in rows:
        print(row)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.221.1">At this point, the </span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.222.1">script will create a mock DataFrame containing sample data (names </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">and ages):</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.224.1">data = {
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 22]
}
df = pd.DataFrame(data)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.225.1">It establishes a connection to a PostgreSQL database using the specified connection parameters (database name, user, password, host, and port). </span><span class="koboSpan" id="kobo.225.2">These are the details we used in the previous step when we set up the database, so no change is required on </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">your side:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.227.1">db_params = {
    'dbname': 'learn_sql',
    'user': 'the_great_coder',
    'password': 'the_great_coder_again',
    'host': 'localhost',
    'port': '5432'
}
conn = psycopg2.connect(**</span><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">db_params)</span></strong><span class="koboSpan" id="kobo.229.1">
cursor = conn.cursor()</span></pre></li>				<li><span class="koboSpan" id="kobo.230.1">Finally, it </span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.231.1">checks whether a table named </span><strong class="source-inline"><span class="koboSpan" id="kobo.232.1">example_table</span></strong><span class="koboSpan" id="kobo.233.1"> exists, creates it if necessary, and then inserts the mock data into the table. </span><span class="koboSpan" id="kobo.233.2">After committing the changes to the database, the script fetches and prints the data from the table to confirm the successful insertion before finally closing the </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">database connection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.235.1">
table_name = 'example_table'
if not table_exists(cursor, table_name):
    create_table(cursor, table_name)
insert_data(cursor, table_name, df.values.tolist())
conn.commit()
print_table_data(cursor, table_name)
cursor.close()
conn.close()</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.236.1">To execute the preceding script, just execute the following command in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">chapter7</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.238.1"> folder:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.239.1">
python 1.postgressql.py</span></pre>			<p class="callout-heading"><span class="koboSpan" id="kobo.240.1">Important Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.241.1">Remember to always close the connections as it helps you avoid performance issues and ensures that new connections can be established when needed. </span><span class="koboSpan" id="kobo.241.2">It allows the database to free up resources associated with the connection, and it ensures that any uncommitted transactions are handled properly. </span><span class="koboSpan" id="kobo.241.3">Closing a connection returns it to the connection pool, making it available for reuse by other parts of </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">your application.</span></span></p>
			<p><span class="koboSpan" id="kobo.243.1">To see the</span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.244.1"> table that was created in the database, you can open the PSQL process in your terminal and connect to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">learn_sql</span></strong><span class="koboSpan" id="kobo.246.1"> database by executing the </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">following command:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.248.1">
psql -h localhost -U the_great_coder -d learn_sql</span></pre>			<p><span class="koboSpan" id="kobo.249.1">Then, run the following command to list all the </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">available tables:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.251.1">
\dt</span></pre>			<p><span class="koboSpan" id="kobo.252.1">You should see something similar to </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">the following:</span></span></p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<span class="koboSpan" id="kobo.254.1"><img src="image/B19801_07_1.jpg" alt=" Figure 7.1 – List tables in the database"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.255.1"> Figure 7.1 – List tables in the database</span></p>
			<p><span class="koboSpan" id="kobo.256.1">You can also interact now with the table by executing the following </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">SQL commands:</span></span></p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<span class="koboSpan" id="kobo.258.1"><img src="image/B19801_07_2.jpg" alt="Figure 7.2 – Showing all the rows in the table"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.259.1">Figure 7.2 – Showing all the rows in the table</span></p>
			<p><span class="koboSpan" id="kobo.260.1">If you rerun the same Python script without dropping the existing table first, you won’t see a new table being created; instead, new rows will be added to the </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">same table:</span></span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<span class="koboSpan" id="kobo.262.1"><img src="image/B19801_07_3.jpg" alt="Figure 7.3 – Showing all the rows in the table once the script has been rerun"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.263.1">Figure 7.3 – Showing all the rows in the table once the script has been rerun</span></p>
			<p><span class="koboSpan" id="kobo.264.1">Having understood </span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.265.1">how to set up a relational database and use it as a sink by writing new data in it, let’s deep dive into the advantages and disadvantages of </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">relational databases.</span></span></p>
			<h3><span class="koboSpan" id="kobo.267.1">Advantages and disadvantages of relational databases</span></h3>
			<p><span class="koboSpan" id="kobo.268.1">In this section, we’ll summarize the advantages and disadvantages of </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">using RDBMSs.</span></span></p>
			<p><span class="koboSpan" id="kobo.270.1">The advantages</span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.271.1"> are </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">as follows:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.273.1">RDBMS systems have ACID properties, providing a robust framework for reliable and </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">secure transactions</span></span></li>
				<li><span class="koboSpan" id="kobo.275.1">RDBMS technology has been around for decades, resulting in mature and well-established systems with extensive documentation and </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">community support</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.277.1">However, they also </span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.278.1">have </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">various disadvantages:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.280.1">The rigid schema of RDBMS can be a limitation when dealing with evolving or dynamic data structures as they require schema modifications. </span><span class="koboSpan" id="kobo.280.2">Schema changes might be required for </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">new data.</span></span></li>
				<li><span class="koboSpan" id="kobo.282.1">RDBMSs are primarily designed for structured data and may not be the best choice for handling </span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.283.1">unstructured or </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">semi-structured data.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.285.1">If you’re wondering in what file type the data in the relational databases is written, then you’ll find the following </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">subsection fascinating.</span></span></p>
			<h3><span class="koboSpan" id="kobo.287.1">Relational database file types</span></h3>
			<p><span class="koboSpan" id="kobo.288.1">In </span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.289.1">relational databases, the choice of file types for storing data is typically </span><em class="italic"><span class="koboSpan" id="kobo.290.1">abstracted</span></em><span class="koboSpan" id="kobo.291.1"> from users and developers, and it is not common to interact with the underlying files directly. </span><span class="koboSpan" id="kobo.291.2">Relational databases manage data storage and retrieval through their internal mechanisms, which often involve </span><em class="italic"><span class="koboSpan" id="kobo.292.1">proprietary </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.293.1">file formats.</span></em></span></p>
			<p><span class="koboSpan" id="kobo.294.1">The process of storing and organizing data within relational databases is managed by the DBMS, and users interact with the data using SQL or other query languages. </span><span class="koboSpan" id="kobo.294.2">The DBMS abstracts the physical storage details from users, providing a logical layer that allows for data manipulation and retrieval without direct concern for the underlying </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">file formats.</span></span></p>
			<p><span class="koboSpan" id="kobo.296.1">Let’s discuss the key points regarding file types in </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">relational databases:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.298.1">Relational database vendors often use proprietary file formats for their data storage. </span><span class="koboSpan" id="kobo.298.2">Each database management system may have </span><em class="italic"><span class="koboSpan" id="kobo.299.1">its own internal structure and mechanisms for </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.300.1">managing data</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">.</span></span></li>
				<li><span class="koboSpan" id="kobo.302.1">Relational databases typically organize data into </span><em class="italic"><span class="koboSpan" id="kobo.303.1">tablespaces</span></em><span class="koboSpan" id="kobo.304.1">, which are logical storage containers. </span><span class="koboSpan" id="kobo.304.2">These tablespaces consist of pages or blocks where data is stored. </span><span class="koboSpan" id="kobo.304.3">The organization and structure of these pages are determined by the </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">specific DBMS.</span></span></li>
				<li><span class="koboSpan" id="kobo.306.1">Relational databases prioritize ACID properties to ensure data integrity and reliability. </span><span class="koboSpan" id="kobo.306.2">The internal file formats are designed to support these </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">transactional guarantees.</span></span></li>
				<li><span class="koboSpan" id="kobo.308.1">Relational databases use various indexing and optimization techniques to enhance query performance. </span><span class="koboSpan" id="kobo.308.2">The internal file structures, including B-trees or other indexing structures, are optimized for efficient </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">data retrieval.</span></span></li>
				<li><span class="koboSpan" id="kobo.310.1">Users interact with relational databases using </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">SQL commands.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.312.1">While users </span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.313.1">typically don’t interact directly with the underlying file formats, understanding the concepts of tablespaces, pages, and how the DBMS manages data storage can be useful for database administrators and developers when they’re optimizing performance or </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">troubleshooting issues.</span></span></p>
			<p><span class="koboSpan" id="kobo.315.1">Transitioning from an RDBMS to a </span><strong class="bold"><span class="koboSpan" id="kobo.316.1">not only SQL</span></strong><span class="koboSpan" id="kobo.317.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.318.1">NoSQL</span></strong><span class="koboSpan" id="kobo.319.1">) database involves a shift in data modeling, schema design, and querying approaches. </span><span class="koboSpan" id="kobo.319.2">We’ll explore the differences in the </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">following section.</span></span></p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor180"/><span class="koboSpan" id="kobo.321.1">NoSQL databases</span></h2>
			<p><span class="koboSpan" id="kobo.322.1">NoSQL </span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.323.1">databases, also known as </span><strong class="bold"><span class="koboSpan" id="kobo.324.1">not only SQL</span></strong><span class="koboSpan" id="kobo.325.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.326.1">non-SQL</span></strong><span class="koboSpan" id="kobo.327.1"> databases, are a class of database systems</span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.328.1"> that provide a flexible and scalable approach to handling and storing data. </span><span class="koboSpan" id="kobo.328.2">Unlike traditional relational databases, which enforce a structured schema with predefined tables, columns, and relationships, NoSQL databases are designed to handle various data models, accommodating different ways of structuring and organizing data and providing a more dynamic and adaptable approach to </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">data modeling.</span></span></p>
			<h3><span class="koboSpan" id="kobo.330.1">Overview of NoSQL databases</span></h3>
			<p><span class="koboSpan" id="kobo.331.1">Here’s a </span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.332.1">quick overview of the key components of </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">NoSQL databases:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.334.1">NoSQL databases often use a schemaless or schema-flexible approach, allowing data to be stored without the need for a predefined schema. </span><span class="koboSpan" id="kobo.334.2">This flexibility is particularly useful in cases where the data structure is evolving or not well-known </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">in advance.</span></span></li>
				<li><span class="koboSpan" id="kobo.336.1">NoSQL databases come in different types, each with its own data model, such as document-oriented (such as MongoDB), key-value stores (such as Redis), column-family stores (such as Apache Cassandra), and graph databases (such as Neo4j). </span><span class="koboSpan" id="kobo.336.2">Data models vary to accommodate different types of data and use cases. </span><span class="koboSpan" id="kobo.336.3">The document-oriented data model stores data as JSON documents, allowing each document to have a different structure, which is ideal for semi-structured or unstructured data. </span><span class="koboSpan" id="kobo.336.4">The key-value data model stores data as key-value pairs, with the value being a simple type or a complex structure, offering fast data retrieval but limited query capabilities. </span><span class="koboSpan" id="kobo.336.5">The column-family data model organizes data into columns instead of rows, enabling efficient storage and retrieval of </span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.337.1">large datasets. </span><span class="koboSpan" id="kobo.337.2">Lastly, the graph data model represents data as nodes and edges, making it perfect for applications focused on relationships, such as social networks and </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">network analysis.</span></span></li>
				<li><span class="koboSpan" id="kobo.339.1">NoSQL databases are typically designed for horizontal scalability, meaning they can efficiently distribute data across </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">multiple nodes.</span></span></li>
				<li><span class="koboSpan" id="kobo.341.1">NoSQL databases often adhere to the </span><strong class="bold"><span class="koboSpan" id="kobo.342.1">consistency, availability, and partition tolerance</span></strong><span class="koboSpan" id="kobo.343.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.344.1">CAP</span></strong><span class="koboSpan" id="kobo.345.1">) theorem, which</span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.346.1"> states that a distributed system can provide – at most – two out of three of the guarantees. </span><span class="koboSpan" id="kobo.346.2">NoSQL databases may prioritize availability and partition tolerance over </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">strict consistency.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.348.1">In the data field, we usually find NoSQL databases as sinks in the </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">following cases:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.350.1">When we’re dealing with data models that may change frequently or aren’t well-defined </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">in advance.</span></span></li>
				<li><span class="koboSpan" id="kobo.352.1">When an application anticipates or experiences rapid growth. </span><span class="koboSpan" id="kobo.352.2">In this case, horizontal scalability </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">is essential.</span></span></li>
				<li><span class="koboSpan" id="kobo.354.1">When the data can’t be put into tables with fixed relationships. </span><span class="koboSpan" id="kobo.354.2">In this case, a more flexible storage model </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">is needed.</span></span></li>
				<li><span class="koboSpan" id="kobo.356.1">When rapid development and iteration are critical. </span><span class="koboSpan" id="kobo.356.2">In this case, we need to modify the data model on </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">the fly.</span></span></li>
				<li><span class="koboSpan" id="kobo.358.1">When the specific features and capabilities of a particular type of NoSQL database align with the requirements of the application (e.g., document-oriented for content-heavy applications, key-value stores for caching, and graph databases for </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">relationship-centric data).</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.360.1">Let’s see an example of how to connect to a NoSQL database and write a </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">new table.</span></span></p>
			<h3><span class="koboSpan" id="kobo.362.1">An example of a MongoDB database</span></h3>
			<p><span class="koboSpan" id="kobo.363.1">Before diving into the code, we’ll take some time to explain MongoDB and some important </span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.364.1">concepts related </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">to it:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.366.1">Document</span></strong><span class="koboSpan" id="kobo.367.1">: This is the basic unit </span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.368.1">of data in MongoDB and is represented as a </span><strong class="bold"><span class="koboSpan" id="kobo.369.1">Binary JSON</span></strong><span class="koboSpan" id="kobo.370.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.371.1">BSON</span></strong><span class="koboSpan" id="kobo.372.1">) object. </span><span class="koboSpan" id="kobo.372.2">Documents are like rows in a relational database but can have varying structures. </span><span class="koboSpan" id="kobo.372.3">Documents are composed of fields (key-value pairs). </span><span class="koboSpan" id="kobo.372.4">Each field can contain different data types, such as strings, numbers, arrays, or </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">nested documents.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.374.1">Collection</span></strong><span class="koboSpan" id="kobo.375.1">: A grouping of MongoDB documents, analogous to a table in a relational database. </span><span class="koboSpan" id="kobo.375.2">Collections contain documents and serve as the primary method of organizing data. </span><span class="koboSpan" id="kobo.375.3">Collections don’t require a predefined schema, allowing documents within the same collection to have </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">different structures.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.377.1">Database</span></strong><span class="koboSpan" id="kobo.378.1">: A container for collections. </span><span class="koboSpan" id="kobo.378.2">MongoDB databases hold collections and serve as the highest level of data organization. </span><span class="koboSpan" id="kobo.378.3">Each database is isolated from others, meaning operations in one database don’t </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">affect others.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.380.1">Now that we have a better understanding, let’s go through the code. </span><span class="koboSpan" id="kobo.380.2">To run this example, set up MongoDB locally by following the documentation for your OS. </span><span class="koboSpan" id="kobo.380.3">For mac instructions, go here: </span><a href="https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/"><span class="koboSpan" id="kobo.381.1">https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/</span></a><span class="koboSpan" id="kobo.382.1">. </span><span class="koboSpan" id="kobo.382.2">The following code example shows how to create a database and then write data in MongoDB using </span><strong class="source-inline"><span class="koboSpan" id="kobo.383.1">pymongo</span></strong><span class="koboSpan" id="kobo.384.1">. </span><span class="koboSpan" id="kobo.384.2">Note that </span><strong class="source-inline"><span class="koboSpan" id="kobo.385.1">pymongo</span></strong><span class="koboSpan" id="kobo.386.1"> is the official Python driver for MongoDB and provides a Python interface for connecting to MongoDB databases, executing queries, and manipulating data using </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">Python scripts.</span></span></p>
			<p><span class="koboSpan" id="kobo.388.1">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.390.1">After installing MongoDB, open your Terminal and start the service. </span><span class="koboSpan" id="kobo.390.2">The commands presented here are for Mac; follow the commands in the documentation for </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">your OS:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.392.1">brew services start mongodb-community@7.0</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.393.1">Validate whether the service is running by executing the </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">following command:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.395.1">brew services list</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.396.1">You should see something similar to </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">the following:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.398.1">mongodb-community@7.0 started</span></strong><span class="koboSpan" id="kobo.399.1"> maria.zervou ~/Library/LaunchAgents/h</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.400.1">Having finished </span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.401.1">with the installations, let’s set up a </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">MongoDB database.</span></span></p></li>				<li><span class="koboSpan" id="kobo.403.1">In your Terminal, type the following command to go into the </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">MongoDB editor:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.405.1">Mongosh</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.406.1">Then, create a database </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">called </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">no_sql_db</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.410.1">use no_sql_d</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.411.1">Next, create a collection </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">called </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">best_collection_ever</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.415.1">db.createCollection("best_collection_ever")</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.416.1">You should see a response similar to </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">the following:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.418.1">{ ok: 1 }</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.419.1">At this point, we’re ready to switch to Python and start adding data to this collection. </span><span class="koboSpan" id="kobo.419.2">You can find the code here: </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/2.pymongo.py"><span class="koboSpan" id="kobo.420.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/2.pymongo.py</span></a><span class="koboSpan" id="kobo.421.1">. </span><span class="koboSpan" id="kobo.421.2">Follow </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">these steps:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.423.1">First, let’s import the </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">required libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.425.1">
from pymongo import MongoClient</span></pre></li>				<li><span class="koboSpan" id="kobo.426.1">Every time we connect to a NoSQL database, we need to provide the connection details. </span><span class="koboSpan" id="kobo.426.2">Update all the values for the parameters in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.427.1">mongo_params</span></strong><span class="koboSpan" id="kobo.428.1"> dictionary, which contains the MongoDB server host, port, username, password, and </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">authentication source:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.430.1">
mongo_params = {
    'host': 'localhost',
    'port': 27017,
    'username': 'your_mongo_username',
    'password': 'your_mongo_password',
    'authSource': 'your_auth_database'
}</span></pre></li>				<li><span class="koboSpan" id="kobo.431.1">Let’s look </span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.432.1">at the different functions we will use in this example to insert the documents into the MongoDB database. </span><span class="koboSpan" id="kobo.432.2">The first function checks whether a collection exists in the database before creating a </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">new one:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.434.1">
def collection_exists(db, collection_name):
    return collection_name in db.list_collection_names()</span></pre></li>				<li><span class="koboSpan" id="kobo.435.1">The following function takes a database and collection name as arguments and creates a collection with the name we pass (in our case, we provided </span><strong class="source-inline"><span class="koboSpan" id="kobo.436.1">collection_name</span></strong><span class="koboSpan" id="kobo.437.1"> as </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">the name):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.439.1">
def create_collection(db, collection_name):
    db.create_collection(collection_name)</span></pre></li>				<li><span class="koboSpan" id="kobo.440.1">Finally, we will take the collection we created in the previous step, which is just a placeholder for now, and insert some data </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">into it:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.442.1">
def insert_data(collection, data):
    collection.insert_many(data)</span></pre></li>				<li><span class="koboSpan" id="kobo.443.1">Let’s create some data to be inserted into </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">the collection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.445.1">
documents = [
    {'name': 'Alice', 'age': 25},
    {'name': 'Bob', 'age': 30},
    {'name': 'Charlie', 'age': 22}
]</span></pre></li>				<li><span class="koboSpan" id="kobo.446.1">Let’s specify the parameters that are required for the connection and </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">create it:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.448.1">
db_name = ' no_sql_db'
collection_name = 'best_collection_ever'
client = MongoClient(**mongo_params)
db = client[db_name]</span></pre></li>				<li><span class="koboSpan" id="kobo.449.1">Now, let’s check</span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.450.1"> whether the collection with the provided name exists. </span><span class="koboSpan" id="kobo.450.2">If the collection exists, use the existing collection; if not, create a new collection with the </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">provided name:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.452.1">
if not collection_exists(db, collection_name):
    create_collection(db, collection_name)</span></pre></li>				<li><span class="koboSpan" id="kobo.453.1">Then, take the collection and insert the </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">provided data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.455.1">
collection = db[collection_name]
insert_data(collection, documents)</span></pre></li>				<li><span class="koboSpan" id="kobo.456.1">Finally, close the </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">MongoDB connection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.458.1">
client.close()</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.459.1">After executing this script, you should be able to see the records being added to </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">the collection:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.461.1">
{'_id': ObjectId('66d833ec27bc08e40e0537b4'), 'name': 'Alice', 'age': 25}
{'_id': ObjectId('66d833ec27bc08e40e0537b5'), 'name': 'Bob', 'age': 30}
{'_id': ObjectId('66d833ec27bc08e40e0537b6'), 'name': 'Charlie', 'age': 22}</span></pre>			<p><span class="koboSpan" id="kobo.462.1">This script demonstrates how to interact with MongoDB databases and collections. </span><span class="koboSpan" id="kobo.462.2">Unlike relational databases, MongoDB does not require </span><em class="italic"><span class="koboSpan" id="kobo.463.1">tables or schemas to be created upfront</span></em><span class="koboSpan" id="kobo.464.1">. </span><span class="koboSpan" id="kobo.464.2">Instead, you work directly with databases and collections. </span><span class="koboSpan" id="kobo.464.3">As an exercise to understand more about this flexible data model, try inserting data into the collection of a different structure. </span><span class="koboSpan" id="kobo.464.4">This contrasts with relational databases, where you insert rows into a </span><em class="italic"><span class="koboSpan" id="kobo.465.1">table with a fixed schema</span></em><span class="koboSpan" id="kobo.466.1">. </span><span class="koboSpan" id="kobo.466.2">You can find an example </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">here: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/3.pymongo_expand.py"><span class="No-Break"><span class="koboSpan" id="kobo.468.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/3.pymongo_expand.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.469.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.470.1">Finally, the script uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.471.1">find</span></strong><span class="koboSpan" id="kobo.472.1"> method to query and retrieve documents from a collection. </span><span class="koboSpan" id="kobo.472.2">MongoDB</span><a id="_idIndexMarker462"/><span class="koboSpan" id="kobo.473.1"> queries are more flexible compared to SQL queries, especially for </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">nested data.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.475.1">Don’t delete the MongoDB database</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.476.1">Please don’t clean the Mongo resources we created as we will use them in the streaming sink example. </span><span class="koboSpan" id="kobo.476.2">We will clean up all the resources at the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">this chapter.</span></span></p>
			<p><span class="koboSpan" id="kobo.478.1">In the next section, we will discuss the advantages and disadvantages that NoSQL </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">databases offer.</span></span></p>
			<h3><span class="koboSpan" id="kobo.480.1">Advantages and disadvantages of NoSQL databases</span></h3>
			<p><span class="koboSpan" id="kobo.481.1">Let’s summarize</span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.482.1"> the advantages and disadvantages of using </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">NoSQL systems.</span></span></p>
			<p><span class="koboSpan" id="kobo.484.1">The advantages of NoSQL databases are </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">as follows:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.486.1">Scalability</span></strong><span class="koboSpan" id="kobo.487.1">: NoSQL databases are designed for horizontal scalability, allowing them to handle large volumes of data by distributing it across multiple servers. </span><span class="koboSpan" id="kobo.487.2">This makes them particularly suitable for big data applications and </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">cloud environments.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.489.1">Flexibility</span></strong><span class="koboSpan" id="kobo.490.1">: Unlike SQL databases, which require a fixed schema, NoSQL databases offer flexible schemas. </span><span class="koboSpan" id="kobo.490.2">This allows structured, semi-structured, and unstructured data to be stored, making it easier to adapt to changing data models without </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">significant restructuring.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.492.1">Performance</span></strong><span class="koboSpan" id="kobo.493.1">: NoSQL databases can offer superior performance for certain types of queries, especially when dealing with large datasets. </span><span class="koboSpan" id="kobo.493.2">They’re often optimized for high-speed data retrieval and can handle large volumes of transactions </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">per second.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.495.1">Cost-effectiveness</span></strong><span class="koboSpan" id="kobo.496.1">: Many NoSQL databases are open source and can be scaled using commodity hardware, which reduces costs compared to the expensive hardware often required for scaling </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">SQL databases.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.498.1">Developer agility</span></strong><span class="koboSpan" id="kobo.499.1">: The flexibility in schema and data models allows developers to iterate quickly and adapt to new requirements without the need for extensive </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">database</span></span><span class="No-Break"><a id="_idIndexMarker464"/></span><span class="No-Break"><span class="koboSpan" id="kobo.501.1"> administration</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.502.1">However, they also have </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">their disadvantages:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.504.1">Lack of standardization</span></strong><span class="koboSpan" id="kobo.505.1">: NoSQL databases don’t have a standardized query language such as SQL. </span><span class="koboSpan" id="kobo.505.2">This </span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.506.1">can lead to a steeper learning curve and make it challenging to switch between different </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">NoSQL systems.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.508.1">Limited support for complex queries</span></strong><span class="koboSpan" id="kobo.509.1">: NoSQL databases generally lack the advanced querying capabilities of SQL databases, such as joins and complex transactions, which can limit their use in applications that require complex </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">data relationships.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.511.1">Data consistency</span></strong><span class="koboSpan" id="kobo.512.1">: Many NoSQL databases prioritize availability and partition tolerance over consistency (as per the CAP theorem). </span><span class="koboSpan" id="kobo.512.2">This can lead to eventual consistency models, which may not be suitable for applications that require strict </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">data integrity.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.514.1">Maturity and community support</span></strong><span class="koboSpan" id="kobo.515.1">: NoSQL databases are relatively newer compared to SQL databases, which means they may have less mature ecosystems and smaller communities. </span><span class="koboSpan" id="kobo.515.2">This can make finding support and resources </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">more challenging.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.517.1">Complex maintenance</span></strong><span class="koboSpan" id="kobo.518.1">: The distributed nature of NoSQL databases can lead to complex maintenance tasks, such as data distribution and load balancing, which require </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">specialized knowledge</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.520.1">Now, let’s discuss the file formats we may encounter when we’re working with </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">NoSQL databases.</span></span></p>
			<h3><span class="koboSpan" id="kobo.522.1">NoSQL database file types</span></h3>
			<p><span class="koboSpan" id="kobo.523.1">The most </span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.524.1">prevalent file formats are JSON and BSON. </span><span class="koboSpan" id="kobo.524.2">JSON is a lightweight, human-readable data interchange format that uses a key-value pair structure and supports nested data structures. </span><span class="koboSpan" id="kobo.524.3">It’s widely adopted for web-based data exchange due to its simplicity and ease of parsing. </span><span class="koboSpan" id="kobo.524.4">JSON is language-agnostic, making it suitable for various programming languages. </span><span class="koboSpan" id="kobo.524.5">JSON’s flexible and schemaless nature aligns well with the flexible schema approach of many NoSQL databases and it allows for easy handling of evolving data structures. </span><span class="koboSpan" id="kobo.524.6">NoSQL databases often deal with semi-structured or unstructured data, and JSON’s hierarchical structure accommodates such data well. </span><span class="koboSpan" id="kobo.524.7">Here’s an example of a JSON </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">data file:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.526.1">
{
    "person": {
        "name": "John Doe",
        "age": 30,
        "address": {
            "city": "New York",
            "country": "USA"
        },
        "email": ["john.doe@email.com", "john@example.com"]
    }
}</span></pre>			<p><span class="koboSpan" id="kobo.527.1">BSON is a binary-encoded serialization for JSON-like documents and is designed to be efficient for storage and traversal. </span><span class="koboSpan" id="kobo.527.2">It adds additional data types not present in JSON, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">date</span></strong><span class="koboSpan" id="kobo.529.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.530.1">binary</span></strong><span class="koboSpan" id="kobo.531.1">. </span><span class="koboSpan" id="kobo.531.2">BSON files are encoded before they’re stored and decoded before they’re displayed. </span><span class="koboSpan" id="kobo.531.3">BSON’s binary format is more efficient for storage and serialization, making it suitable for scenarios where data needs to be represented compactly. </span><span class="koboSpan" id="kobo.531.4">BSON is the primary data format that’s used in MongoDB. </span><span class="koboSpan" id="kobo.531.5">Let’s have a look at the BSON representation of the file </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">presented previously:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.533.1">
\x16\x00\x00\x00
{
    "person": {
        "name": "John Doe",
        "age": 30,
        "address": {
            "city": "New York",
            "country": "USA"
        },
        "email": ["john.doe@email.com", "john@example.com"]
    }
}\x00</span></pre>			<p><span class="koboSpan" id="kobo.534.1">In</span><a id="_idIndexMarker467"/><span class="koboSpan" id="kobo.535.1"> NoSQL databases, the choice between JSON and BSON often depends on the specific requirements of the database and the use case. </span><span class="koboSpan" id="kobo.535.2">While JSON is more human-readable and easy to work with in many scenarios, BSON’s binary efficiency is beneficial in certain contexts, particularly where storage and serialization efficiency </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">are critical.</span></span></p>
			<p><span class="koboSpan" id="kobo.537.1">In the next section, we will discuss data warehouses, what challenges they solve, and which use cases you should consider implementing when </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">using them.</span></span></p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.539.1">Data warehouses</span></h2>
			<p><span class="koboSpan" id="kobo.540.1">Transitioning to a data warehouse becomes important when the volume and</span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.541.1"> complexity of data, as well as the need for advanced analytics, surpass the capabilities of your existing relational or NoSQL databases. </span><span class="koboSpan" id="kobo.541.2">If your relational database struggles</span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.542.1"> with large data volumes, complex queries, or performance issues during analytical processing, a data warehouse can offer optimized storage and query performance for such workloads. </span><span class="koboSpan" id="kobo.542.2">Similarly, NoSQL databases, while excellent for handling unstructured or semi-structured data and scaling horizontally, may lack the sophisticated query capabilities and performance needed for in-depth analytics and reporting. </span><span class="koboSpan" id="kobo.542.3">Data warehouses are designed to integrate data from multiple sources, including both relational and NoSQL databases, facilitating comprehensive analysis and reporting. </span><span class="koboSpan" id="kobo.542.4">They provide robust support for historical data analysis, complex queries, and data governance, making them an ideal solution when you need to enhance your data integration, analytics, and reporting capabilities beyond what traditional </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">databases offer.</span></span></p>
			<h3><span class="koboSpan" id="kobo.544.1">Overview of data warehouses</span></h3>
			<p><span class="koboSpan" id="kobo.545.1">A data warehouse </span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.546.1">is a specialized database system designed for storing, organizing, and retrieving </span><em class="italic"><span class="koboSpan" id="kobo.547.1">large volumes of data</span></em><span class="koboSpan" id="kobo.548.1"> that are used for business intelligence and analytics efficiently. </span><span class="koboSpan" id="kobo.548.2">Unlike transactional databases, which are optimized for quick data updates and individual record retrievals, data warehouses are structured to support complex queries, aggregations, and reporting on historical and </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">current data.</span></span></p>
			<p><span class="koboSpan" id="kobo.550.1">Here’s a quick overview of </span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.551.1">the key components of </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">data warehouses:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.553.1">Various data sources contribute to a data warehouse, including transactional databases, external files, logs, </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">and more.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.555.1">Extract, transform, and load</span></strong><span class="koboSpan" id="kobo.556.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.557.1">ETL</span></strong><span class="koboSpan" id="kobo.558.1">) processes </span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.559.1">are used to gather data from source systems, transform it into a consistent format, and load it into the </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">data warehouse.</span></span></li>
				<li><span class="koboSpan" id="kobo.561.1">Data warehouses employ optimized storage methods, such as columnar storage, to store large volumes of </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">data efficiently.</span></span></li>
				<li><span class="koboSpan" id="kobo.563.1">Indexes and pre-aggregated tables are used to optimize query performance. </span><span class="koboSpan" id="kobo.563.2">In a data warehouse, indexes play a crucial role in optimizing query performance. </span><span class="koboSpan" id="kobo.563.3">An index is a data structure that enhances the speed and efficiency of data retrieval from a table by creating a separate, organized subset of the data. </span><span class="koboSpan" id="kobo.563.4">Indexes are typically created on one or more columns to facilitate faster querying. </span><span class="koboSpan" id="kobo.563.5">Without indexes, the database must scan the entire table to locate relevant rows. </span><span class="koboSpan" id="kobo.563.6">Indexes help the database quickly find rows that meet query conditions. </span><span class="koboSpan" id="kobo.563.7">Common candidates for indexing include columns used in </span><strong class="source-inline"><span class="koboSpan" id="kobo.564.1">WHERE</span></strong><span class="koboSpan" id="kobo.565.1"> clauses, </span><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">JOIN</span></strong><span class="koboSpan" id="kobo.567.1"> conditions, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.568.1">ORDER BY</span></strong><span class="koboSpan" id="kobo.569.1"> clauses. </span><span class="koboSpan" id="kobo.569.2">However, over-indexing can lead to diminishing returns and increased maintenance overhead. </span><span class="koboSpan" id="kobo.569.3">While indexes improve query performance, they also consume additional storage and can slow down write operations due to the need to maintain </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">the index.</span></span></li>
				<li><span class="koboSpan" id="kobo.571.1">Techniques such as parallel processing and indexing are employed to enhance the speed of </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">analytical queries.</span></span></li>
				<li><span class="koboSpan" id="kobo.573.1">Integration with business intelligence tools allows users to create reports and dashboards and perform </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">data analysis.</span></span></li>
				<li><span class="koboSpan" id="kobo.575.1">Data is organized using multidimensional models, often in the form of star or snowflake schemas, to support analytics </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">and reporting.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.577.1">Let’s expand on</span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.578.1"> dimensional modeling. </span><span class="koboSpan" id="kobo.578.2">It’s a design technique that’s used in data warehousing to structure data so that it supports efficient retrieval for analytical queries and reporting. </span><span class="koboSpan" id="kobo.578.3">Unlike traditional relational models, dimensional models are optimized for query performance and ease of use. </span><span class="koboSpan" id="kobo.578.4">In the next section, we will present the main schema types in </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">dimensional models.</span></span></p>
			<h3><span class="koboSpan" id="kobo.580.1">Schema types in dimensional models</span></h3>
			<p><span class="koboSpan" id="kobo.581.1">Dimensional modeling </span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.582.1">primarily involves two types of schemas: the star schema and the snowflake schema. </span><span class="koboSpan" id="kobo.582.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.583.1">star schema</span></strong><span class="koboSpan" id="kobo.584.1"> is the simplest form of dimensional modeling and is where a central fact </span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.585.1">table is directly connected to multiple dimension tables, forming a star-like structure. </span><span class="koboSpan" id="kobo.585.2">This schema type is highly intuitive and easy to navigate, making it ideal for straightforward queries and reporting. </span><span class="koboSpan" id="kobo.585.3">Each dimension table in a star schema contains a primary key that maps to a foreign key in the fact table, providing descriptive context to the quantitative data in the fact table. </span><span class="koboSpan" id="kobo.585.4">For instance, a sales star schema might include a central sales fact table with foreign keys linking to dimension tables for products, customers, time, and stores, thereby simplifying complex queries by reducing the number </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">of joins:</span></span></p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<span class="koboSpan" id="kobo.587.1"><img src="image/B19801_07_4.jpg" alt="Figure 7.4 – Star schema"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.588.1">Figure 7.4 – Star schema</span></p>
			<p><span class="koboSpan" id="kobo.589.1">On the other hand, the </span><strong class="bold"><span class="koboSpan" id="kobo.590.1">snowflake schema</span></strong><span class="koboSpan" id="kobo.591.1"> is a more normalized version of the star schema and is </span><a id="_idIndexMarker476"/><span class="koboSpan" id="kobo.592.1">where dimension tables are further broken down into related sub-tables, resembling a snowflake pattern. </span><span class="koboSpan" id="kobo.592.2">This structure reduces data redundancy and can save storage space, although it introduces more complexity in query design due to the additional joins required. </span><span class="koboSpan" id="kobo.592.3">For example, a product dimension table in a snowflake schema might be normalized into separate tables for product categories and brands, creating a multi-layered structure that ensures higher data integrity and reduces update anomalies. </span><span class="koboSpan" id="kobo.592.4">While the snowflake schema may be slightly more complex to query, it offers benefits in terms of data maintenance and scalability, especially in environments where data consistency and storage optimization </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">are critical:</span></span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<span class="koboSpan" id="kobo.594.1"><img src="image/B19801_07_5.jpg" alt="Figure 7.5 – Snowflake schema"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.595.1">Figure 7.5 – Snowflake schema</span></p>
			<p><span class="koboSpan" id="kobo.596.1">Star schemas</span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.597.1"> are often preferred for simpler hierarchies and when query performance is a higher priority, while snowflake schemas may be chosen when more efficient use of storage and achieving a higher level of normalization </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">is essential.</span></span></p>
			<p><span class="koboSpan" id="kobo.599.1">Transitioning from understanding schema types in dimensional modeling, it’s essential to explore the diverse options available for implementing and leveraging data warehouses in various </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">organizational contexts.</span></span></p>
			<h3><span class="koboSpan" id="kobo.601.1">Data warehouse solutions</span></h3>
			<p><span class="koboSpan" id="kobo.602.1">There are</span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.603.1"> many different data warehouse options out there. </span><span class="koboSpan" id="kobo.603.2">We’ve summarized the main ones in the </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">following table:</span></span></p>
			<table id="table002-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.605.1">Data Warehouse</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.606.1">Description</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.607.1">Databricks SQL</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.608.1">A cloud-based, serverless data warehouse that brings warehouse capabilities to data lakes. </span><span class="koboSpan" id="kobo.608.2">It’s also known for its scalability, performance, and parallel processing, as well as its built-in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">learning capabilities.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.610.1">Amazon Redshift</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.611.1">A fully managed, scalable data warehouse service in the cloud. </span><span class="koboSpan" id="kobo.611.2">It’s optimized for </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">high-performance analytics.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.613.1">Snowflake</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.614.1">A cloud-based data warehouse with a multi-cluster, shared architecture that supports </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">diverse workloads.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.616.1">Google BigQuery</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.617.1">A serverless, highly scalable data warehouse with built-in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">learning capabilities.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.619.1">Data Warehouse</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.620.1">Description</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.621.1">Teradata</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.622.1">An on-premises or cloud-based data warehouse known for its scalability, performance, and </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">parallel processing.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.624.1">Microsoft Azure </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">Synapse Analytics</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.626.1">A cloud-based analytics service that offers both on-demand and </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">provisioned resources.</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.628.1">Table 7.2 – Data warehousing solutions</span></p>
			<p><span class="koboSpan" id="kobo.629.1">In the next section, we will </span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.630.1">look at an example of creating a BigQuery table to illustrate the practical application of </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">data warehouses.</span></span></p>
			<h3><span class="koboSpan" id="kobo.632.1">An example of a data warehouse</span></h3>
			<p><span class="koboSpan" id="kobo.633.1">Let’s learn </span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.634.1">how to create a new table in BigQuery. </span><span class="koboSpan" id="kobo.634.2">Google Cloud provides a client library for various programming languages, including Python, to interact with BigQuery. </span><span class="koboSpan" id="kobo.634.3">To get the ready so that you can run the following example, go to the BigQuery documentation: </span><a href="https://cloud.google.com/python/docs/reference/bigquery/latest"><span class="koboSpan" id="kobo.635.1">https://cloud.google.com/python/docs/reference/bigquery/latest</span></a><span class="koboSpan" id="kobo.636.1">. </span><span class="koboSpan" id="kobo.636.2">Let’s deep dive into the example. </span><span class="koboSpan" id="kobo.636.3">We will follow the same patterns that have been presented so far in this chapter. </span><span class="koboSpan" id="kobo.636.4">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">get started:</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.638.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.639.1">To run this example, you need to </span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.640.1">have a </span><strong class="bold"><span class="koboSpan" id="kobo.641.1">Google Cloud Platform</span></strong><span class="koboSpan" id="kobo.642.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.643.1">GCP</span></strong><span class="koboSpan" id="kobo.644.1">) account and a Google Storage </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">bucket ready.</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.646.1">Import the </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">required libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.648.1">
from google.cloud import bigquery
from google.cloud.bigquery import SchemaField</span></pre></li>				<li><span class="koboSpan" id="kobo.649.1">First, we’ll set </span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.650.1">up the project ID. </span><span class="koboSpan" id="kobo.650.2">Replace </span><strong class="source-inline"><span class="koboSpan" id="kobo.651.1">your_project_id</span></strong><span class="koboSpan" id="kobo.652.1"> with your </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">actual values:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.654.1">
client = bigquery.Client(project='your_project_id')</span></pre></li>				<li><span class="koboSpan" id="kobo.655.1">Define the dataset and table name and update the </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">following fields:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.657.1">
dataset_name = 'your_dataset'
table_name = 'your_table'</span></pre></li>				<li><span class="koboSpan" id="kobo.658.1">Check whether the dataset and the </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">table exist:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.660.1">
dataset_ref = client.dataset(dataset_name)
table_ref = dataset_ref.table(table_name)
table_exists = client.get_table(
    table_ref, retry=3, timeout=30, max_results=None
) is not None</span></pre></li>				<li><span class="koboSpan" id="kobo.661.1">Define the table schema (replace it with your schema if you wish to update the data). </span><span class="koboSpan" id="kobo.661.2">In this example, we will create a table with two columns (</span><strong class="source-inline"><span class="koboSpan" id="kobo.662.1">column1</span></strong><span class="koboSpan" id="kobo.663.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.664.1">column2</span></strong><span class="koboSpan" id="kobo.665.1">). </span><span class="koboSpan" id="kobo.665.2">The first column will be of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.666.1">STRING</span></strong><span class="koboSpan" id="kobo.667.1"> type, while the second one will be of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.668.1">INTEGER</span></strong><span class="koboSpan" id="kobo.669.1"> type. </span><span class="koboSpan" id="kobo.669.2">The first column can’t contain missing values, whereas the second </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">one can:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.671.1">
schema = [
    SchemaField('column1', 'STRING', mode='REQUIRED'),
    SchemaField('column2', 'INTEGER', mode='NULLABLE'),
    # Add more fields as needed
]</span></pre></li>				<li><span class="koboSpan" id="kobo.672.1">Let’s perform the checks for the existence of a table with the same name. </span><span class="koboSpan" id="kobo.672.2">If the table doesn’t exist, then we create it with the </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">provided name:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.674.1">
if not table_exists:
    table = bigquery.Table(table_ref, schema=schema)
    client.create_table(table)</span></pre></li>				<li><span class="koboSpan" id="kobo.675.1">Let’s create </span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.676.1">some mock data that will be inserted into </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">the table:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.678.1">
rows_to_insert = [
    ('value1', 1),
    ('value2', 2),
    ('value3', 3)
]</span></pre></li>				<li><span class="koboSpan" id="kobo.679.1">Construct the data to </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">be inserted:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.681.1">
data_to_insert = [dict(zip([field.name for field in schema], row)) for row in rows_to_insert]</span></pre></li>				<li><span class="koboSpan" id="kobo.682.1">Insert the data and check for errors. </span><span class="koboSpan" id="kobo.682.2">If everything works as expected, close </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">the connection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.684.1">
errors = client.insert_rows(table, data_to_insert)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.685.1">If any insertion errors occur, print </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">them out:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.687.1">print(f"Errors occurred during data insertion: {errors}")</span></pre></li>				<li><span class="koboSpan" id="kobo.688.1">Close the </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">BigQuery client:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.690.1">
client.close()</span></pre></li>			</ol>
			<p class="callout-heading"><span class="koboSpan" id="kobo.691.1">What’s the difference between a container and a table?</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.692.1">A container (called different things in different systems, such as database, dataset, or schema) is a logical grouping mechanism that’s used to organize and manage data objects such as tables, views, and related metadata. </span><span class="koboSpan" id="kobo.692.2">Containers provide a way to partition and structure data based on specific needs, such as access control, data governance, or logical separation of data domains. </span><span class="koboSpan" id="kobo.692.3">On the other hand, a table is a fundamental data structure that stores the actual data records, organized into rows and columns. </span><span class="koboSpan" id="kobo.692.4">Tables define the schema (column names and data types) and hold the </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">data values.</span></span></p>
			<p><span class="koboSpan" id="kobo.694.1">At this point, let’s </span><a id="_idIndexMarker484"/><span class="koboSpan" id="kobo.695.1">transition from understanding the fundamental components of a data warehouse environment to discussing the advantages and disadvantages that data warehouses offer in managing and analyzing large volumes of </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">data efficiently.</span></span></p>
			<h3><span class="koboSpan" id="kobo.697.1">Advantages and disadvantages of data warehouses</span></h3>
			<p><span class="koboSpan" id="kobo.698.1">Let’s summarize the advantages and disadvantages of using </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">data warehouses.</span></span></p>
			<p><span class="koboSpan" id="kobo.700.1">The advantages </span><a id="_idIndexMarker485"/><span class="koboSpan" id="kobo.701.1">are </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">as follows:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.703.1">Optimized for analytical queries and large-scale </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">data processing</span></span></li>
				<li><span class="koboSpan" id="kobo.705.1">Can handle massive amounts </span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">of data</span></span></li>
				<li><span class="koboSpan" id="kobo.707.1">Integration with other data tools </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">and services</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.709.1">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">their</span></span><span class="No-Break"><a id="_idIndexMarker486"/></span><span class="No-Break"><span class="koboSpan" id="kobo.711.1"> disadvantages:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.712.1">Higher costs for storage and querying compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">traditional databases</span></span></li>
				<li><span class="koboSpan" id="kobo.714.1">Might have limitations regarding real-time </span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">data processing</span></span></li>
			</ul>
			<h3><span class="koboSpan" id="kobo.716.1">File types in data warehouses</span></h3>
			<p><span class="koboSpan" id="kobo.717.1">In terms </span><a id="_idIndexMarker487"/><span class="koboSpan" id="kobo.718.1">of file formats, it’s accurate to say that many modern data warehouses </span><a id="_idIndexMarker488"/><span class="koboSpan" id="kobo.719.1">use </span><strong class="bold"><span class="koboSpan" id="kobo.720.1">proprietary</span></strong><span class="koboSpan" id="kobo.721.1"> internal storage formats for writing data. </span><span class="koboSpan" id="kobo.721.2">These proprietary formats are usually columnar storage formats that are optimized for efficient querying </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">and analytics.</span></span></p>
			<p><span class="koboSpan" id="kobo.723.1">Let’s have a look at the differences we may find in these </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">proprietary formats:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.725.1">Data warehouses often use columnar storage formats such as Parquet, ORC, or Avro. </span><span class="koboSpan" id="kobo.725.2">While these formats are open and widely adopted, each data warehouse might have its internal optimizations </span><span class="No-Break"><span class="koboSpan" id="kobo.726.1">or extensions.</span></span></li>
				<li><span class="koboSpan" id="kobo.727.1">The actual implementations of these columnar storage formats may have vendor-specific optimizations or features. </span><span class="koboSpan" id="kobo.727.2">For example, how a specific data warehouse handles compression, indexing, and metadata might be specific to </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">the vendor.</span></span></li>
				<li><span class="koboSpan" id="kobo.729.1">Users interact with data warehouses through standard interfaces, such as SQL queries. </span><span class="koboSpan" id="kobo.729.2">The choice of storage format often doesn’t affect users, so long as the data warehouse supports common data interchange formats for import </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">and export.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.731.1">So, while the</span><a id="_idIndexMarker489"/><span class="koboSpan" id="kobo.732.1"> internal storage mechanisms may have vendor-specific optimizations, the use of well-established, open, and widely adopted columnar storage formats ensures a degree of interoperability and flexibility. </span><span class="koboSpan" id="kobo.732.2">Users typically interact with data warehouses using standard SQL queries or data interchange formats such as CSV, JSON, or Avro for data import/export, which adds a layer of standardization to the external-facing aspects of </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">these systems.</span></span></p>
			<p><span class="koboSpan" id="kobo.734.1">Transitioning from traditional data warehouses to data lakes represents a strategic shift that embraces a more flexible and scalable paradigm. </span><span class="koboSpan" id="kobo.734.2">In the next section, we will deep dive into </span><span class="No-Break"><span class="koboSpan" id="kobo.735.1">data lakes.</span></span></p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.736.1">Data lakes</span></h2>
			<p><span class="koboSpan" id="kobo.737.1">The transition</span><a id="_idIndexMarker490"/><span class="koboSpan" id="kobo.738.1"> from traditional data warehouses to data lakes represents a shift in how organizations handle and analyze their data. </span><span class="koboSpan" id="kobo.738.2">Traditional data warehouses are designed to store structured data, which is highly organized and formatted according to a predefined schema, such as tables with rows and columns in relational databases. </span><span class="koboSpan" id="kobo.738.3">Structured data is easy to query and analyze using SQL. </span><span class="koboSpan" id="kobo.738.4">However, data warehouses struggle with handling unstructured data, which lacks a predefined format or organization. </span><span class="koboSpan" id="kobo.738.5">Examples of unstructured data include text documents, emails, images, videos, and social media posts. </span><span class="koboSpan" id="kobo.738.6">Data lakes, on the other hand, offer a more flexible and scalable solution by storing both structured and unstructured data in its native format. </span><span class="koboSpan" id="kobo.738.7">This allows organizations to ingest and store vast amounts of data without the need for immediate structuring. </span><span class="koboSpan" id="kobo.738.8">This transition addresses the limitations of data warehouses, offering a more versatile and future-proof approach to </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">data management.</span></span></p>
			<h3><span class="koboSpan" id="kobo.740.1">Overview of data lakes</span></h3>
			<p><span class="koboSpan" id="kobo.741.1">A data lake </span><a id="_idIndexMarker491"/><span class="koboSpan" id="kobo.742.1">is a centralized repository that allows organizations to store vast amounts of raw and unstructured data in any format needed. </span><span class="koboSpan" id="kobo.742.2">They’re designed to handle diverse data types, such as structured, semi-structured, and unstructured data, and enable data exploration, analytics, and machine learning. </span><span class="koboSpan" id="kobo.742.3">Data lakes solve several problems: they enable the consolidation of diverse data sources, break down silos by providing a unified data storage solution, and support advanced analytics by making all types of data </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">easily accessible.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.744.1">Remember</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.745.1">Think of data lakes as a filesystem, such as for storing data in a file location on your laptop, just on a much </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">larger scale.</span></span></p>
			<h3><span class="koboSpan" id="kobo.747.1">Data lake solutions</span></h3>
			<p><span class="koboSpan" id="kobo.748.1">Here’s a </span><a id="_idIndexMarker492"/><span class="koboSpan" id="kobo.749.1">summary of the available data lake solutions in the </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">data space:</span></span></p>
			<table id="table003-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.751.1">Data Lake</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.752.1">Description</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.753.1">Amazon S3</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.754.1">A cloud-based object storage service by </span><strong class="bold"><span class="koboSpan" id="kobo.755.1">Amazon Web Services</span></strong><span class="koboSpan" id="kobo.756.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.757.1">AWS</span></strong><span class="koboSpan" id="kobo.758.1">). </span><span class="koboSpan" id="kobo.758.2">It’s commonly used as a foundation for </span><span class="No-Break"><span class="koboSpan" id="kobo.759.1">data lakes.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.760.1">Azure Data </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">Lake Storage</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.762.1">A scalable and secure cloud-based storage solution by Microsoft Azure that’s designed to support big data analytics and </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">data lakes.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.764.1">Hadoop Distributed File </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.765.1">System</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.766.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.767.1">HDFS</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.768.1">)</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.769.1">A distributed filesystem that forms the storage layer of Apache Hadoop, an open source big data </span><span class="No-Break"><span class="koboSpan" id="kobo.770.1">processing framework.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.771.1">Google </span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">Cloud Storage</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.773.1">Google Cloud’s object storage service, often used as part of a data lake architecture in </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">the GCP.</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.775.1">Table 7.3 – Data lake solutions</span></p>
			<p><span class="koboSpan" id="kobo.776.1">The shift from traditional </span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.777.1">data warehouses to data lakes represents a fundamental transformation in how organizations manage and analyze data. </span><span class="koboSpan" id="kobo.777.2">This shift was driven by several factors, including the need for greater flexibility, scalability, and the ability to handle diverse and unstructured data types. </span><span class="koboSpan" id="kobo.777.3">The following table highlights the key differences between traditional data warehouses and </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">data lakes:</span></span></p>
			<table id="table004-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.779.1">Data Warehouses</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.780.1">Data Lakes</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.781.1">Data variety </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">and flexibility</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.783.1">Traditional data warehouses are designed to handle structured data and are less adaptable to handle diverse data types or </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">unstructured data.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.785.1">Data lakes emerged as a response to the increasing volume and variety of data. </span><span class="koboSpan" id="kobo.785.2">They provide a storage repository for raw, unstructured, and diverse data types, allowing organizations to store large volumes of data without the need for </span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">predefined schemas.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.787.1">Scalability</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.788.1">Traditional data warehouses often face scalability challenges when dealing with massive amounts of data. </span><span class="koboSpan" id="kobo.788.2">Scaling up a data warehouse can be costly and may </span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">have limitations.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.790.1">Data lakes, particularly cloud-based solutions, offer scalable storage and computing resources. </span><span class="koboSpan" id="kobo.790.2">They can efficiently scale horizontally to handle growing datasets and </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">processing demands.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.792.1">Data Warehouses</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.793.1">Data Lakes</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.794.1">Cost-efficiency</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.795.1">Traditional data warehouses can be expensive to scale, and the cost structure may not be conducive to storing large volumes of raw or less </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">structured data.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.797.1">Cloud-based data lakes often follow a pay-as-you-go model, allowing organizations to manage costs more efficiently by paying for the resources they use. </span><span class="koboSpan" id="kobo.797.2">This is particularly beneficial for storing large amounts of </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">raw data.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.799.1">Schema-on-dead </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">versus schema-on-write</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.801.1">Follow a schema-on-write approach, where data is structured and transformed before being loaded into </span><span class="No-Break"><span class="koboSpan" id="kobo.802.1">the warehouse.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.803.1">Follow a schema-on-read approach, allowing for the storage of raw, untransformed data. </span><span class="koboSpan" id="kobo.803.2">The schema is applied at the time of data analysis, providing more flexibility in </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">data exploration.</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.805.1">Table 7.4 – Data warehouses versus data lakes</span></p>
			<p><span class="koboSpan" id="kobo.806.1">The </span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.807.1">emergence of the Lakehouse architecture further </span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.808.1">refined the shift away from data warehouses by solving some key challenges associated with data lakes and by bringing features traditionally associated with data warehouses into the data lake environment. </span><span class="koboSpan" id="kobo.808.2">Here’s a breakdown of the key aspects of </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">this evolution:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.810.1">The Lakehouse integrates ACID transactions into the data lake, providing transactional capabilities that were traditionally associated with data warehouses. </span><span class="koboSpan" id="kobo.810.2">This ensures data consistency </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">and reliability.</span></span></li>
				<li><span class="koboSpan" id="kobo.812.1">The Lakehouse supports schema evolution, allowing changes to be made to data schemas over time without requiring a full transformation of existing data. </span><span class="koboSpan" id="kobo.812.2">This enhances flexibility and reduces the impact of schema changes on </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">existing processes.</span></span></li>
				<li><span class="koboSpan" id="kobo.814.1">The Lakehouse introduces features for managing data quality, including schema enforcement and constraints, ensuring that data stored in the lake meets </span><span class="No-Break"><span class="koboSpan" id="kobo.815.1">specified standards.</span></span></li>
				<li><span class="koboSpan" id="kobo.816.1">The Lakehouse aims to provide a unified platform for analytics by combining the strengths of data lakes and data warehouses. </span><span class="koboSpan" id="kobo.816.2">It allows organizations to perform analytics on both structured and semi-structured data in a </span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">centralized repository.</span></span></li>
				<li><span class="koboSpan" id="kobo.818.1">The Lakehouse enhances the metadata catalog, providing a comprehensive view of data lineage, quality, and transformations. </span><span class="koboSpan" id="kobo.818.2">This facilitates better governance and understanding of the data stored in </span><span class="No-Break"><span class="koboSpan" id="kobo.819.1">the lake.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.820.1">The Lakehouse</span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.821.1"> concept has evolved through discussions in the data and analytics community, with various companies contributing to the development and adoption of </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">Lakehouse principles.</span></span></p>
			<h3><span class="koboSpan" id="kobo.823.1">An example of a data lake</span></h3>
			<p><span class="koboSpan" id="kobo.824.1">Let’s explore an </span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.825.1">example of how to write some Parquet files on S3, AWS’s cloud storage. </span><span class="koboSpan" id="kobo.825.2">To get everything set up, go to the AWS documentation: </span><a href="https://docs.aws.amazon.com/code-library/latest/ug/python_3_s3_code_examples.html"><span class="koboSpan" id="kobo.826.1">https://docs.aws.amazon.com/code-library/latest/ug/python_3_s3_code_examples.html</span></a><span class="koboSpan" id="kobo.827.1">. </span><span class="koboSpan" id="kobo.827.2">Now, follow </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">these steps:</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.829.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.830.1">To run this example, you need to have an AWS account and an S3 </span><span class="No-Break"><span class="koboSpan" id="kobo.831.1">bucket ready.</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.832.1">We’ll start by importing the </span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">required libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.834.1">
import pandas as pd
import pyarrow.parquet as pq
import boto3
from io import BytesIO</span></pre></li>				<li><span class="koboSpan" id="kobo.835.1">Now, we’re going to create some mock data so that we can write it </span><span class="No-Break"><span class="koboSpan" id="kobo.836.1">to S3:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.837.1">
data = {'Name': ['Alice', 'Bob', 'Charlie'],
        'Age': [25, 30, 22],
        'City': ['New York', 'San Francisco', 'Los Angeles']}
df = pd.DataFrame(data)</span></pre></li>				<li><span class="koboSpan" id="kobo.838.1">Next, we must convert the DataFrame into </span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">Parquet format:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.840.1">
parquet_buffer = BytesIO()
pq.write_table(pq.Table.from_pandas(df), parquet_buffer)</span></pre></li>				<li><span class="koboSpan" id="kobo.841.1">Update</span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.842.1"> your authentication key and the bucket name in which we’re going to write </span><span class="No-Break"><span class="koboSpan" id="kobo.843.1">the data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.844.1">
aws_access_key_id = 'YOUR_ACCESS_KEY_ID'
aws_secret_access_key = 'YOUR_SECRET_ACCESS_KEY'
bucket_name = 'your-s3-bucket'
file_key = 'example_data.parquet'  # The key (path) of the file in S3</span></pre></li>				<li><span class="koboSpan" id="kobo.845.1">Connect to the S3 bucket using the connection details from the </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">previous step:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.847.1">
s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)</span></pre></li>				<li><span class="koboSpan" id="kobo.848.1">Upload the Parquet file </span><span class="No-Break"><span class="koboSpan" id="kobo.849.1">to S3:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.850.1">
s3.put_object(Body=parquet_buffer.getvalue(), Bucket=bucket_name, Key=file_key)</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.851.1">As discussed previously, Lakehouses come with their own set of advantages </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">and disadvantages.</span></span></p>
			<p><span class="koboSpan" id="kobo.853.1">The advantages are </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">as follows:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.855.1">Lakehouses provide a </span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.856.1">unified platform that integrates the strengths of both data lakes and data warehouses. </span><span class="koboSpan" id="kobo.856.2">This allows organizations to leverage the flexibility of data lakes and the transactional capabilities of data warehouses in a </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">single environment.</span></span></li>
				<li><span class="koboSpan" id="kobo.858.1">Lakehouses follow a schema-on-read approach, allowing for the storage of raw, </span><span class="No-Break"><span class="koboSpan" id="kobo.859.1">untransformed data.</span></span></li>
				<li><span class="koboSpan" id="kobo.860.1">Lakehouses support diverse data types, including structured, semi-structured, and </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">unstructured data.</span></span></li>
				<li><span class="koboSpan" id="kobo.862.1">Lakehouses integrate ACID transactions, providing transactional capabilities that ensure data consistency and reliability. </span><span class="koboSpan" id="kobo.862.2">This is particularly important for use cases where data integrity </span><span class="No-Break"><span class="koboSpan" id="kobo.863.1">is critical.</span></span></li>
				<li><span class="koboSpan" id="kobo.864.1">Many Lakehouse </span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.865.1">solutions offer time travel capabilities, allowing users to query data at specific points in time. </span><span class="koboSpan" id="kobo.865.2">Versioning of data provides historical context and supports </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">audit requirements.</span></span></li>
				<li><span class="koboSpan" id="kobo.867.1">Lakehouses often implement optimized storage formats (e.g., Delta and Iceberg) that contribute to storage efficiency and improved query performance, especially for large-scale </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">analytical workloads.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.869.1">The disadvantages are </span><span class="No-Break"><span class="koboSpan" id="kobo.870.1">as follows:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.871.1">Users and</span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.872.1"> administrators may need to adapt to a new way of working with data while considering both schema-on-read and schema-on-write paradigms. </span><span class="koboSpan" id="kobo.872.2">This may require training </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">and education.</span></span></li>
				<li><span class="koboSpan" id="kobo.874.1">Depending on the implementation and cloud provider, costs associated with storing, processing, and managing data in a Lakehouse architecture may vary. </span><span class="koboSpan" id="kobo.874.2">Organizations need to carefully manage costs to </span><span class="No-Break"><span class="koboSpan" id="kobo.875.1">ensure efficiency.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.876.1">As we’ve discussed, Lakehouses have the amazing advantage of allowing any data type to be ingested and stored, from structured to semi-structured to unstructured. </span><span class="koboSpan" id="kobo.876.2">This means we can find any file type in the ingestion part of the process, from CSVs to Parquets and Avros. </span><span class="koboSpan" id="kobo.876.3">While in the ingestion part, we can see any file type, on the write part, we can take advantage of the flexibility the Lakehouse offers and store the data in optimized open table file formats. </span><span class="koboSpan" id="kobo.876.4">Open table format is a file format that’s used to store tabular data in a way that’s easily accessible and interoperable across various data processing and </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">analytics tools.</span></span></p>
			<h3><span class="koboSpan" id="kobo.878.1">File types in data lakes</span></h3>
			<p><span class="koboSpan" id="kobo.879.1">In Lakehouse</span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.880.1"> architecture, we have three prominent formats: Delta, Apache Iceberg, and Apache Hudi. </span><span class="koboSpan" id="kobo.880.2">These formats provide features such as ACID transactions, schema evolution, incremental data processing, and read and write optimizations. </span><span class="koboSpan" id="kobo.880.3">Here’s a brief overview of </span><span class="No-Break"><span class="koboSpan" id="kobo.881.1">these formats:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.882.1">Delta Lake</span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.883.1"> is an open source storage layer designed to enhance the reliability and performance of data processing in data lakes. </span><span class="koboSpan" id="kobo.883.2">It is well-suited for building data lakes on infrastructure such as S3 or Azure Storage and has strong support for ACID transactions and </span><span class="No-Break"><span class="koboSpan" id="kobo.884.1">data versioning.</span></span></li>
				<li><span class="koboSpan" id="kobo.885.1">Apache Iceberg </span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.886.1">is another open source table format that’s</span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.887.1"> optimized for fast query performance. </span><span class="koboSpan" id="kobo.887.2">It’s a good choice when query efficiency is required and it has excellent support for schema evolution </span><span class="No-Break"><span class="koboSpan" id="kobo.888.1">and versioning.</span></span></li>
				<li><span class="koboSpan" id="kobo.889.1">Apache Hudi (Hadoop Upserts, Deletes, and Incrementals) is another open source data</span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.890.1"> lake storage format that provides great support for real-time data processing and streaming features. </span><span class="koboSpan" id="kobo.890.2">While it may not be as widely known as Delta Lake or Apache Iceberg, Hudi is gaining traction, especially in Apache Spark and </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">Hadoop ecosystems.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.892.1">In general, all these formats were built to solve the same challenges, which is why they have a lot of common features. </span><span class="koboSpan" id="kobo.892.2">Thus, before choosing the best one for your workload, there are a couple of things you should consider to ensure you’re going in the </span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">right direction:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.894.1">Consider the compatibility of each technology with your existing data processing ecosystem </span><span class="No-Break"><span class="koboSpan" id="kobo.895.1">and tools</span></span></li>
				<li><span class="koboSpan" id="kobo.896.1">Evaluate the level of community support, ongoing development, and adoption within the data community for </span><span class="No-Break"><span class="koboSpan" id="kobo.897.1">each technology</span></span></li>
				<li><span class="koboSpan" id="kobo.898.1">Assess the performance characteristics of each technology concerning your specific use case, especially in terms of read and </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">write operations</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.900.1">Ultimately, the choice between Delta Lake, Apache Iceberg, and Apache Hudi should be driven by the specific requirements and priorities of your data lake or lakehouse environment. </span><span class="koboSpan" id="kobo.900.2">It’s also beneficial to experiment and benchmark each solution with your data and workloads to make an </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">informed decision.</span></span></p>
			<p><span class="koboSpan" id="kobo.902.1">The last sink technology we’re going to discuss is streaming </span><span class="No-Break"><span class="koboSpan" id="kobo.903.1">data sinks.</span></span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.904.1">Streaming data sinks</span></h2>
			<p><span class="koboSpan" id="kobo.905.1">The</span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.906.1"> transition from batch and micro-batch processing to streaming technologies marks a significant evolution in data processing and analytics. </span><span class="koboSpan" id="kobo.906.2">Batch processing involves collecting and processing data in large, discrete chunks at scheduled intervals, which can lead to delays in data availability and insights. </span><span class="koboSpan" id="kobo.906.3">Micro-batch processing improves on this by handling smaller batches at more frequent intervals, reducing latency but still not achieving real-time data processing. </span><span class="koboSpan" id="kobo.906.4">Streaming technologies, however, enable the continuous ingestion and processing of data in real-time, allowing organizations to immediately analyze and act on data as it arrives. </span><span class="koboSpan" id="kobo.906.5">This shift to streaming technologies addresses the growing need for real-time analytics and decision-making in today’s fast-paced business environments, providing a more dynamic and responsive approach to </span><span class="No-Break"><span class="koboSpan" id="kobo.907.1">data management.</span></span></p>
			<h3><span class="koboSpan" id="kobo.908.1">Overview of streaming data sinks</span></h3>
			<p><span class="koboSpan" id="kobo.909.1">Streaming </span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.910.1">data sinks are components or services that consume and store streaming data in real time. </span><span class="koboSpan" id="kobo.910.2">They act as the endpoint where streaming data is ingested, processed, and persisted for further analysis or retrieval. </span><span class="koboSpan" id="kobo.910.3">Here’s an overview of streaming data sinks and their </span><span class="No-Break"><span class="koboSpan" id="kobo.911.1">main components:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.912.1">Ingestion component</span></strong><span class="koboSpan" id="kobo.913.1">: This is responsible for receiving and accepting incoming </span><span class="No-Break"><span class="koboSpan" id="kobo.914.1">data streams</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.915.1">Processing logic</span></strong><span class="koboSpan" id="kobo.916.1">: This is a bespoke logic that may include components for data enrichment, transformation, </span><span class="No-Break"><span class="koboSpan" id="kobo.917.1">or aggregation</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.918.1">Storage component</span></strong><span class="koboSpan" id="kobo.919.1">: This persists streaming data for future analysis </span><span class="No-Break"><span class="koboSpan" id="kobo.920.1">or retrieval</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.921.1">Connectors</span></strong><span class="koboSpan" id="kobo.922.1">: Their main role is to interact with various data processing or </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">storage systems</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.924.1">We usually implement</span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.925.1"> streaming sinks in the </span><span class="No-Break"><span class="koboSpan" id="kobo.926.1">following areas:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.927.1">In real-time analytics systems. </span><span class="koboSpan" id="kobo.927.2">This allows organizations to gain insights into their data as </span><span class="No-Break"><span class="koboSpan" id="kobo.928.1">events occur.</span></span></li>
				<li><span class="koboSpan" id="kobo.929.1">In systems monitoring, where streaming data sinks capture and process real-time metrics, logs, or events, enabling immediate alerting and responses to issues </span><span class="No-Break"><span class="koboSpan" id="kobo.930.1">or anomalies.</span></span></li>
				<li><span class="koboSpan" id="kobo.931.1">In financial </span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.932.1">transactions or e-commerce. </span><span class="koboSpan" id="kobo.932.2">Here, streaming data sinks can be used for real-time fraud detection by analyzing patterns and anomalies in </span><span class="No-Break"><span class="koboSpan" id="kobo.933.1">transaction data.</span></span></li>
				<li><span class="koboSpan" id="kobo.934.1">In the </span><strong class="bold"><span class="koboSpan" id="kobo.935.1">Internet of Things</span></strong><span class="koboSpan" id="kobo.936.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.937.1">IoT</span></strong><span class="koboSpan" id="kobo.938.1">) scenarios, streaming data sinks </span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.939.1">handle the continuous flow of data from sensors and devices, supporting real-time monitoring </span><span class="No-Break"><span class="koboSpan" id="kobo.940.1">and control.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.941.1">Now, let’s have a look at the available options we have for </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">streaming sinks.</span></span></p>
			<h3><span class="koboSpan" id="kobo.943.1">Streaming sinks solutions</span></h3>
			<p><span class="koboSpan" id="kobo.944.1">Many cloud platforms </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.945.1">offer managed streaming data services that act as sinks, such as Amazon Kinesis, Azure Event Hubs, and Google Cloud Dataflow, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">following table:</span></span></p>
			<table id="table005" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.947.1">Streaming </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.948.1">Data Sink</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.949.1">Description</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.950.1">Amazon Kinesis</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.951.1">A fully managed service for real-time stream processing in AWS. </span><span class="koboSpan" id="kobo.951.2">It supports data streams, analytics, </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">and storage.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.953.1">Azure </span><span class="No-Break"><span class="koboSpan" id="kobo.954.1">Event Hub</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.955.1">A cloud-based real-time analytics service in Azure for processing and analyzing </span><span class="No-Break"><span class="koboSpan" id="kobo.956.1">streaming data.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.957.1">Google </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">Cloud Dataflow</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.959.1">A fully managed stream and batch processing service </span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">on GCP.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.961.1">Apache Kafka</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.962.1">A distributed streaming platform that can act as both a source and a sink for </span><span class="No-Break"><span class="koboSpan" id="kobo.963.1">streaming data.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.964.1">Apache </span><span class="No-Break"><span class="koboSpan" id="kobo.965.1">Spark Streaming</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.966.1">A real-time data processing framework that’s part of the Apache </span><span class="No-Break"><span class="koboSpan" id="kobo.967.1">Spark ecosystem.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.968.1">Apache Flink</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.969.1">A stream processing framework that supports event time processing and various </span><span class="No-Break"><span class="koboSpan" id="kobo.970.1">sink connectors.</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.971.1">Table 7.5 – Different streaming data services</span></p>
			<p><span class="koboSpan" id="kobo.972.1">In the next section, we</span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.973.1"> will use one of the most popular streaming sinks, Kafka, to get an idea of what writing in a streaming sink </span><span class="No-Break"><span class="koboSpan" id="kobo.974.1">looks like.</span></span></p>
			<h3><span class="koboSpan" id="kobo.975.1">An example of a streaming data sink</span></h3>
			<p><span class="koboSpan" id="kobo.976.1">First things first, let’s </span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.977.1">get an initial understanding of the main</span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.978.1"> components of </span><span class="No-Break"><span class="koboSpan" id="kobo.979.1">Apache Kafka:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.980.1">Brokers</span></strong><span class="koboSpan" id="kobo.981.1"> are the core servers that make up a Kafka cluster. </span><span class="koboSpan" id="kobo.981.2">They handle the storage and management of messages. </span><span class="koboSpan" id="kobo.981.3">Each broker is identified by a unique ID. </span><span class="koboSpan" id="kobo.981.4">Brokers are responsible for replicating data across the cluster for </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">fault tolerance.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.983.1">Topics</span></strong><span class="koboSpan" id="kobo.984.1"> are the primary abstractions in Kafka for organizing and categorizing messages. </span><span class="koboSpan" id="kobo.984.2">They are like tables in a database or folders in a filesystem. </span><span class="koboSpan" id="kobo.984.3">Messages are published to and read from specific topics. </span><span class="koboSpan" id="kobo.984.4">Topics can be partitioned for scalability and </span><span class="No-Break"><span class="koboSpan" id="kobo.985.1">parallel processing.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.986.1">Partitions</span></strong><span class="koboSpan" id="kobo.987.1"> are the units of parallelism in Kafka. </span><span class="koboSpan" id="kobo.987.2">Each topic is divided into one or more partitions, which allow for distributed storage and processing of data. </span><span class="koboSpan" id="kobo.987.3">Messages within a partition are ordered </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">and immutable.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.989.1">Producers</span></strong><span class="koboSpan" id="kobo.990.1"> are client applications that publish (write) messages to Kafka topics. </span><span class="koboSpan" id="kobo.990.2">They can choose which partition to send messages to or use a partitioning strategy. </span><span class="koboSpan" id="kobo.990.3">Producers are responsible for serializing, compressing, and load balancing data </span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">among partitions.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.992.1">Consumers</span></strong><span class="koboSpan" id="kobo.993.1"> are client applications that subscribe to (read) messages from Kafka topics. </span><span class="koboSpan" id="kobo.993.2">They can read from one or more partitions of a topic and keep track of which messages they have </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">already consumed.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.995.1">ZooKeeper</span></strong><span class="koboSpan" id="kobo.996.1"> is used </span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.997.1">for managing and coordinating Kafka brokers. </span><span class="koboSpan" id="kobo.997.2">It maintains metadata about </span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.998.1">the Kafka cluster. </span><span class="koboSpan" id="kobo.998.2">Newer versions of Kafka are moving toward removing the </span><span class="No-Break"><span class="koboSpan" id="kobo.999.1">ZooKeeper dependency.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1000.1">Now that we have a </span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.1001.1">better understanding of the main Kafka components, let’s start with our step-by-step guide. </span><span class="koboSpan" id="kobo.1001.2">We will need to install a couple of components for this example, so stay with me as we go through the process. </span><span class="koboSpan" id="kobo.1001.3">To simplify this, we will use Docker as it allows you to define the entire environment in a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1002.1">docker-compose.yml</span></strong><span class="koboSpan" id="kobo.1003.1"> file, making it easy to set up Kafka and Zookeeper with minimal configuration. </span><span class="koboSpan" id="kobo.1003.2">This eliminates the need to manually install and configure each component on your local machine. </span><span class="koboSpan" id="kobo.1003.3">Follow </span><span class="No-Break"><span class="koboSpan" id="kobo.1004.1">these steps:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1005.1">Download Docker by following the public </span><span class="No-Break"><span class="koboSpan" id="kobo.1006.1">documentation: </span></span><a href="https://docs.docker.com/desktop/install/mac-install/"><span class="No-Break"><span class="koboSpan" id="kobo.1007.1">https://docs.docker.com/desktop/install/mac-install/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">.</span></span></li>
				<li><span class="koboSpan" id="kobo.1009.1">Next, set up Kafka with Docker. </span><span class="koboSpan" id="kobo.1009.2">For this, let’s have a look at the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1010.1">docker-compose.yml</span></strong><span class="koboSpan" id="kobo.1011.1"> file </span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/docker-compose.yml"><span class="No-Break"><span class="koboSpan" id="kobo.1013.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/docker-compose.yml</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">.</span></span></li>
				<li><span class="koboSpan" id="kobo.1015.1">This Docker Compose configuration sets up a simple Kafka and Zookeeper environment using version 3 of the Docker Compose file format. </span><span class="koboSpan" id="kobo.1015.2">The configuration defines two services: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1016.1">zookeeper</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1017.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1018.1">kafka</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1019.1">.</span></span></li>
				<li><span class="koboSpan" id="kobo.1020.1">Zookeeper uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1021.1">confluentinc/cp-zookeeper:latest</span></strong><span class="koboSpan" id="kobo.1022.1"> image. </span><span class="koboSpan" id="kobo.1022.2">It maps the host machine’s port, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1023.1">2181</span></strong><span class="koboSpan" id="kobo.1024.1">, to the container’s port, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1025.1">2181</span></strong><span class="koboSpan" id="kobo.1026.1">, for client connections. </span><span class="koboSpan" id="kobo.1026.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1027.1">ZOOKEEPER_CLIENT_PORT</span></strong><span class="koboSpan" id="kobo.1028.1"> environment variable is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1029.1">2181</span></strong><span class="koboSpan" id="kobo.1030.1">, which specifies the port Zookeeper will listen on for </span><span class="No-Break"><span class="koboSpan" id="kobo.1031.1">client requests:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1032.1">
version: '3'
services:
    zookeeper:
        image: confluentinc/cp-zookeeper:latest
        ports:
            - "2181:2181"
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181</span></pre></li>				<li><span class="koboSpan" id="kobo.1033.1">Kafka uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1034.1">confluentinc/cp-kafka:latest</span></strong><span class="koboSpan" id="kobo.1035.1"> image. </span><span class="koboSpan" id="kobo.1035.2">It maps the host machine’s port, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1036.1">9092</span></strong><span class="koboSpan" id="kobo.1037.1">, to the </span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.1038.1">container’s port, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1039.1">9092</span></strong><span class="koboSpan" id="kobo.1040.1">, for external </span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">client connections:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1042.1">
kafka:
    image: confluentinc/cp-kafka:latest
    ports:
        - "9092:9092"
    environment:
        KAFKA_BROKER_ID: 1
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1043.1">Here are some key environment variables that </span><span class="No-Break"><span class="koboSpan" id="kobo.1044.1">configure Kafka:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.1045.1">KAFKA_BROKER_ID</span></strong><span class="koboSpan" id="kobo.1046.1"> is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1047.1">1</span></strong><span class="koboSpan" id="kobo.1048.1">, identifying this broker uniquely in a </span><span class="No-Break"><span class="koboSpan" id="kobo.1049.1">Kafka cluster</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1050.1">KAFKA_ZOOKEEPER_CONNECT</span></strong><span class="koboSpan" id="kobo.1051.1"> points to the Zookeeper service (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1052.1">zookeeper:2181</span></strong><span class="koboSpan" id="kobo.1053.1">), allowing Kafka to connect to Zookeeper for managing </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">cluster metadata</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1055.1">KAFKA_ADVERTISED_LISTENERS</span></strong><span class="koboSpan" id="kobo.1056.1"> advertises </span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">two listeners:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.1058.1">PLAINTEXT://kafka:29092</span></strong><span class="koboSpan" id="kobo.1059.1"> for internal Docker </span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1">network communication</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1061.1">PLAINTEXT_HOST://localhost:9092</span></strong><span class="koboSpan" id="kobo.1062.1"> for connections from outside the Docker network (e.g., from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1063.1">host machine)</span></span></li></ul></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1064.1">KAFKA_LISTENER_SECURITY_PROTOCOL_MAP</span></strong><span class="koboSpan" id="kobo.1065.1"> ensures both advertised listeners use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1066.1">PLAINTEXT</span></strong><span class="koboSpan" id="kobo.1067.1"> protocol, meaning no encryption </span><span class="No-Break"><span class="koboSpan" id="kobo.1068.1">or authentication</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1069.1">KAFKA_INTER_BROKER_LISTENER_NAME</span></strong><span class="koboSpan" id="kobo.1070.1"> is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1071.1">PLAINTEXT</span></strong><span class="koboSpan" id="kobo.1072.1">, specifying which listener Kafka brokers will use to communicate with </span><span class="No-Break"><span class="koboSpan" id="kobo.1073.1">each other</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1074.1">KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR</span></strong><span class="koboSpan" id="kobo.1075.1"> is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1076.1">1</span></strong><span class="koboSpan" id="kobo.1077.1">, meaning the offsets topic (used for storing consumer group offsets) will not be replicated across multiple brokers, which is typical for a </span><span class="No-Break"><span class="koboSpan" id="kobo.1078.1">single-broker setup</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.1079.1">This setup is ideal </span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.1080.1">for local development or testing, where you need a simple, single-node Kafka environment without the complexities of a multi-node, production-grade cluster. </span><span class="koboSpan" id="kobo.1080.2">Now, we’re ready to run </span><span class="No-Break"><span class="koboSpan" id="kobo.1081.1">the container.</span></span></p></li>				<li><span class="koboSpan" id="kobo.1082.1">Let’s run the Docker container to start Kafka and Zookeeper. </span><span class="koboSpan" id="kobo.1082.2">In your terminal, enter the </span><span class="No-Break"><span class="koboSpan" id="kobo.1083.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1084.1">
docker-compose up –d</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1085.1">This will take the Kafka and Zookeeper images and install them in your environment. </span><span class="koboSpan" id="kobo.1085.2">You should see something similar to the following printed on </span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">your terminal:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.1087.1">[+] Running 3/3</span></strong>
<strong class="bold"> </strong><strong class="bold"><span class="koboSpan" id="kobo.1088.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.1089.1"> Network setup_default        Created         0.0s</span></strong>
<strong class="bold"> </strong><strong class="bold"><span class="koboSpan" id="kobo.1090.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.1091.1"> Container setup-kafka-1      Started         0.7s</span></strong>
<strong class="bold"> </strong><strong class="bold"><span class="koboSpan" id="kobo.1092.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.1093.1"> Container setup-zookeeper-1</span></strong><strong class="bold"><span class="koboSpan" id="kobo.1094.1">  Started         0.6s</span></strong></pre></li>			</ol>
			<h4><span class="koboSpan" id="kobo.1095.1">Kafka producer</span></h4>
			<p><span class="koboSpan" id="kobo.1096.1">Now, let’s go</span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.1097.1"> back to our Python IDE and look at how we can push data to a Kafka producer. </span><span class="koboSpan" id="kobo.1097.2">For this, we’re going to read the data written in MongoDB and produce it in Kafka. </span><span class="koboSpan" id="kobo.1097.3">You can find the code here: </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4a.kafka_producer.py"><span class="koboSpan" id="kobo.1098.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4a.kafka_producer.py</span></a><span class="koboSpan" id="kobo.1099.1">. </span><span class="koboSpan" id="kobo.1099.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">get started:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1101.1">First, let’s import the </span><span class="No-Break"><span class="koboSpan" id="kobo.1102.1">necessary libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1103.1">
from pymongo import MongoClient
from confluent_kafka import Producer
import json</span></pre></li>				<li><span class="koboSpan" id="kobo.1104.1">Next, define the </span><span class="No-Break"><span class="koboSpan" id="kobo.1105.1">MongoDB connection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1106.1">
mongo_client = MongoClient('mongodb://localhost:27017')
db = mongo_client['no_sql_db']
collection = db['best_collection_ever']</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1107.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1108.1">MongoClient('mongodb://localhost:27017')</span></strong><span class="koboSpan" id="kobo.1109.1"> connects to a MongoDB instance running on localhost at the default port of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1110.1">27017</span></strong><span class="koboSpan" id="kobo.1111.1">. </span><span class="koboSpan" id="kobo.1111.2">This creates a client object that allows interaction with the database. </span><span class="koboSpan" id="kobo.1111.3">Then, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1112.1">db = mongo_client['no_sql_db']</span></strong><span class="koboSpan" id="kobo.1113.1"> selects the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1114.1">no_sql_db</span></strong><span class="koboSpan" id="kobo.1115.1"> database from the MongoDB instance. </span><span class="koboSpan" id="kobo.1115.2">Finally, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1116.1">collection = db['best_collection_ever']</span></strong><span class="koboSpan" id="kobo.1117.1"> selects the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1118.1">best_collection_ever</span></strong><span class="koboSpan" id="kobo.1119.1"> collection from the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1120.1">no_sql_db</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1121.1"> database.</span></span></p></li>				<li><span class="koboSpan" id="kobo.1122.1">Let’s perform the Kafka producer configuration that creates a Kafka producer object with the specified configuration. </span><span class="koboSpan" id="kobo.1122.2">This producer will be used to send messages (in this case, MongoDB documents) to a </span><span class="No-Break"><span class="koboSpan" id="kobo.1123.1">Kafka topic:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1124.1">
kafka_config = {
    'bootstrap.servers': 'localhost:9092'
}
producer = Producer(kafka_config)</span></pre></li>				<li><span class="koboSpan" id="kobo.1125.1">The following function is a callback function that will be called when the Kafka producer finishes </span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.1126.1">sending a message. </span><span class="koboSpan" id="kobo.1126.2">It checks whether there was an error during the message delivery and prints a message indicating success or failure. </span><span class="koboSpan" id="kobo.1126.3">This function provides feedback on whether messages were successfully sent to Kafka, which is useful for debugging </span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">and monitoring:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1128.1">
def delivery_report(err, msg):
    if err is not None:
        print(f'Message delivery failed: {err}')
    else:
        print(f'Message delivered to {msg.topic()} [{msg.partition()}]')</span></pre></li>				<li><span class="koboSpan" id="kobo.1129.1">Read from MongoDB and produce to Kafka for the document </span><span class="No-Break"><span class="koboSpan" id="kobo.1130.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1131.1">collection.find()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1132.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1133.1">
    message = json.dumps(document, default=str)
    producer.produce('mongodb_topic',
                     alue=message.encode('utf-8'),
                     callback=delivery_report)
    producer.poll(0)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1134.1">The preceding code iterates over each document in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1135.1">best_collection_ever</span></strong><span class="koboSpan" id="kobo.1136.1"> collection. </span><span class="koboSpan" id="kobo.1136.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1137.1">find()</span></strong><span class="koboSpan" id="kobo.1138.1"> method retrieves all documents from the collection. </span><span class="koboSpan" id="kobo.1138.2">Then, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1139.1">message = json.dumps(document, default=str)</span></strong><span class="koboSpan" id="kobo.1140.1"> converts each MongoDB document (a Python dictionary) into a JSON string. </span><span class="koboSpan" id="kobo.1140.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1141.1">default=str</span></strong><span class="koboSpan" id="kobo.1142.1"> parameter handles data types that aren’t JSON serializable by converting them into strings. </span><span class="koboSpan" id="kobo.1142.2">Next, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1143.1">producer.produce('mongodb_topic', value=message.encode('utf-8'), callback=delivery_report)</span></strong><span class="koboSpan" id="kobo.1144.1"> sends the JSON string as a message to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1145.1">mongodb_topic</span></strong><span class="koboSpan" id="kobo.1146.1"> Kafka topic. </span><span class="koboSpan" id="kobo.1146.2">The message is encoded in UTF-8, and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1147.1">delivery_report</span></strong><span class="koboSpan" id="kobo.1148.1"> function is set as a callback to handle delivery confirmation. </span><span class="koboSpan" id="kobo.1148.2">Finally, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1149.1">producer.poll(0)</span></strong><span class="koboSpan" id="kobo.1150.1"> ensures that the Kafka producer processes delivery reports and other events. </span><span class="koboSpan" id="kobo.1150.2">This is necessary to keep the producer active </span><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">and responsive.</span></span></p></li>				<li><span class="koboSpan" id="kobo.1152.1">This</span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.1153.1"> ensures that all messages in the producer’s queue are sent to Kafka before the script exits. </span><span class="koboSpan" id="kobo.1153.2">Without this step, there might be unsent messages remaining in </span><span class="No-Break"><span class="koboSpan" id="kobo.1154.1">the queue:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1155.1">
producer.flush()</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1156.1">After running this script, you should see the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1157.1">print statements:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1158.1">
Message delivered to mongodb_topic [0]
Message delivered to mongodb_topic [0]
Message delivered to mongodb_topic [0]
Message delivered to mongodb_topic [0]
Message delivered to mongodb_topic [0]
Message delivered to mongodb_topic [0]
Message delivered to mongodb_topic [0]
Message delivered to mongodb_topic [0]</span></pre>			<p><span class="koboSpan" id="kobo.1159.1">So far, we’ve connected to a MongoDB database, read the documents from a collection, and sent these documents as messages to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1160.1">Kafka topic.</span></span></p>
			<h4><span class="koboSpan" id="kobo.1161.1">Kafka consumer</span></h4>
			<p><span class="koboSpan" id="kobo.1162.1">Next, let’s run the</span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.1163.1"> consumer so that it can consume the messages from the Kafka producer. </span><span class="koboSpan" id="kobo.1163.2">The full code can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.1164.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4b.kafka_consumer.py"><span class="No-Break"><span class="koboSpan" id="kobo.1165.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/4b.kafka_consumer.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1166.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1167.1">Let’s start by importing </span><span class="No-Break"><span class="koboSpan" id="kobo.1168.1">the libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1169.1">
from confluent_kafka import Consumer, KafkaError
import json
import time</span></pre></li>				<li><span class="koboSpan" id="kobo.1170.1">Next, we must create the Kafka consumer configuration that specifies the Kafka broker(s) to connect to. </span><span class="koboSpan" id="kobo.1170.2">Here, it’s connecting to a Kafka broker running on localhost at port </span><strong class="source-inline"><span class="koboSpan" id="kobo.1171.1">9092</span></strong><span class="koboSpan" id="kobo.1172.1">. </span><span class="koboSpan" id="kobo.1172.2">In this case, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1173.1">group.id</span></strong><span class="koboSpan" id="kobo.1174.1"> sets the consumer group ID, which </span><a id="_idIndexMarker525"/><span class="koboSpan" id="kobo.1175.1">allows multiple consumers to coordinate and share the work of processing messages from a topic. </span><span class="koboSpan" id="kobo.1175.2">Messages will be distributed among consumers in the same group. </span><span class="koboSpan" id="kobo.1175.3">Next, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1176.1">auto.offset.reset</span></strong><span class="koboSpan" id="kobo.1177.1"> defines the behavior when there is no initial offset in Kafka or if the current offset doesn’t exist. </span><span class="koboSpan" id="kobo.1177.2">Setting this to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1178.1">earliest</span></strong><span class="koboSpan" id="kobo.1179.1"> means the consumer will start reading from the earliest available message in </span><span class="No-Break"><span class="koboSpan" id="kobo.1180.1">the topic:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1181.1">
consumer_config = {
    'bootstrap.servers': 'localhost:9092',
    'group.id': 'mongodb_consumer_group',
    'auto.offset.reset': 'earliest'
}</span></pre></li>				<li><span class="koboSpan" id="kobo.1182.1">Now, we will instantiate a Kafka consumer with the configuration specified earlier. </span><span class="koboSpan" id="kobo.1182.2">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1183.1">consumer.subscribe(['mongodb_topic'])</span></strong><span class="koboSpan" id="kobo.1184.1"> subscribes the consumer to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1185.1">mongodb_topic</span></strong><span class="koboSpan" id="kobo.1186.1"> Kafka topic. </span><span class="koboSpan" id="kobo.1186.2">This means the consumer will receive messages from </span><span class="No-Break"><span class="koboSpan" id="kobo.1187.1">this topic:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1188.1">
consumer = Consumer(consumer_config)
consumer.subscribe(['mongodb_topic'])</span></pre></li>				<li><span class="koboSpan" id="kobo.1189.1">Set the duration for which the consumer should run (</span><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">in seconds):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1191.1">
run_duration = 10 # For example, 10 seconds
start_time = time.time()
print("Starting consumer...")</span></pre></li>				<li><span class="koboSpan" id="kobo.1192.1">The following code begins an infinite loop that will run until it’s explicitly broken out of. </span><span class="koboSpan" id="kobo.1192.2">This loop continuously polls Kafka for new messages. </span><span class="koboSpan" id="kobo.1192.3">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1193.1">if time.time() - start_time &gt; run_duration</span></strong><span class="koboSpan" id="kobo.1194.1"> checks whether the consumer has been running for longer than the specified </span><strong class="source-inline"><span class="koboSpan" id="kobo.1195.1">run_duration</span></strong><span class="koboSpan" id="kobo.1196.1">. </span><span class="koboSpan" id="kobo.1196.2">If so, it prints a</span><a id="_idIndexMarker526"/><span class="koboSpan" id="kobo.1197.1"> message and breaks out of the loop, stopping </span><span class="No-Break"><span class="koboSpan" id="kobo.1198.1">the consumer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1199.1">
while True:
    if time.time() - start_time &gt; run_duration:
        print("Time limit reached, shutting down consumer.")
        break
    msg = consumer.poll(1.0)
    if msg is None:
        continue
    if msg.error():
        if msg.error().code() == KafkaError._PARTITION_EOF:
            print('Reached end of partition')
        else:
            print(f'Error: {msg.error()}')
    else:
        document = json.loads(msg.value().decode('utf-8'))
        print(f'Received document: {document}')
consumer.close()
print("Consumer closed.")</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1200.1">After running the preceding code, you should see the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1201.1">print statements:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1202.1">
Starting consumer...
</span><span class="koboSpan" id="kobo.1202.2">Received document: {'_id': '66d833ec27bc08e40e0537b4', 'name': 'Alice', 'age': 25}
Received document: {'_id': '66d833ec27bc08e40e0537b5', 'name': 'Bob', 'age': 30}
Received document: {'_id': '66d833ec27bc08e40e0537b6', 'name': 'Charlie', 'age': 22}
Received document: {'_id': '66d835aa1798a2275cecaba8', 'name': 'Alice', 'age': 25, 'email': 'alice@example.com'}
Received document: {'_id': '66d835aa1798a2275cecaba9', 'name': 'Bob', 'age': 30, 'address': '123 Main St'}
Received document: {'_id': '66d835aa1798a2275cecabaa', 'name': 'Charlie', 'age': 22, 'hobbies': ['reading', 'gaming']}
Received document: {'_id': '66d835aa1798a2275cecabab', 'name': 'David', 'age': 40, 'email': 'david@example.com', 'address': '456 Elm St', 'active': True}
Received document: {'_id': '66d835aa1798a2275cecabac', 'name': 'Eve', 'age': 35, 'email': 'eve@example.com', 'phone': '555-1234'}</span></pre>			<p><span class="koboSpan" id="kobo.1203.1">The goal of this</span><a id="_idIndexMarker527"/><span class="koboSpan" id="kobo.1204.1"> example was to show you how data can be continuously read from a NoSQL database such as MongoDB and then streamed in real-time to other systems using Kafka. </span><span class="koboSpan" id="kobo.1204.2">Kafka acts as a messaging system that allows data producers (e.g., MongoDB) to be decoupled from data consumers. </span><span class="koboSpan" id="kobo.1204.3">This example also illustrates how data can be processed in stages, allowing for scalable and flexible </span><span class="No-Break"><span class="koboSpan" id="kobo.1205.1">data pipelines.</span></span></p>
			<p><span class="koboSpan" id="kobo.1206.1">In terms of a real use case scenario, imagine that we are building a ride-sharing app. </span><span class="koboSpan" id="kobo.1206.2">Handling events such as ride requests, cancellations, and driver statuses in real-time is crucial for efficiently matching riders with drivers. </span><span class="koboSpan" id="kobo.1206.3">MongoDB stores this event data, such as ride requests and driver locations, while Kafka streams the events to various microservices. </span><span class="koboSpan" id="kobo.1206.4">These microservices then process the events to make decisions, such as assigning a driver to a rider. </span><span class="koboSpan" id="kobo.1206.5">By using Kafka, the system becomes highly responsive, scalable, and resilient as it decouples event producers (such as ride requests) from consumers (such as the driver </span><span class="No-Break"><span class="koboSpan" id="kobo.1207.1">assignment logic).</span></span></p>
			<p><span class="koboSpan" id="kobo.1208.1">To summarize what we have seen so far, in contrast to relational sinks, which involve structured data with defined schemas, Kafka can serve as a buffer or intermediary for data ingestion, allowing for decoupled and scalable data pipelines. </span><span class="koboSpan" id="kobo.1208.2">NoSQL sinks often handle unstructured </span><a id="_idIndexMarker528"/><span class="koboSpan" id="kobo.1209.1">or semi-structured data, similar to Kafka’s flexibility with message formats. </span><span class="koboSpan" id="kobo.1209.2">Kafka’s ability to handle high-throughput data streams complements NoSQL databases’ scalability </span><span class="No-Break"><span class="koboSpan" id="kobo.1210.1">and flexibility.</span></span></p>
			<p><span class="koboSpan" id="kobo.1211.1">To clean all the resources that have been used so far, execute the cleaning </span><span class="No-Break"><span class="koboSpan" id="kobo.1212.1">script: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/cleanup_script.sh"><span class="No-Break"><span class="koboSpan" id="kobo.1213.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/setup/cleanup_script.sh</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1214.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.1215.1">In the next section, we will deep dive into the file format seen in streaming </span><span class="No-Break"><span class="koboSpan" id="kobo.1216.1">data sinks.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1217.1">File types in streaming data sinks</span></h3>
			<p><span class="koboSpan" id="kobo.1218.1">Streaming data sinks </span><a id="_idIndexMarker529"/><span class="koboSpan" id="kobo.1219.1">primarily deal with messages or events rather than traditional file storage. </span><span class="koboSpan" id="kobo.1219.2">The data that’s transmitted through streaming data sinks is often in formats such as JSON, Avro, or binary. </span><span class="koboSpan" id="kobo.1219.3">These formats are commonly used for serializing and encoding data in streaming scenarios. </span><span class="koboSpan" id="kobo.1219.4">They are efficient and support schema evolution. </span><span class="koboSpan" id="kobo.1219.5">In the </span><em class="italic"><span class="koboSpan" id="kobo.1220.1">NoSQL databases</span></em><span class="koboSpan" id="kobo.1221.1"> section of this chapter, we deep-dived in the JSON file format. </span><span class="koboSpan" id="kobo.1221.2">Here, we’ll look at Avro </span><span class="No-Break"><span class="koboSpan" id="kobo.1222.1">and binary.</span></span></p>
			<p><span class="koboSpan" id="kobo.1223.1">Apache Avro is a binary serialization format developed within the Apache Hadoop project. </span><span class="koboSpan" id="kobo.1223.2">It uses a schema to define data structures, allowing for efficient serialization and deserialization. </span><span class="koboSpan" id="kobo.1223.3">Avro is known for its compact binary representation, providing fast serialization and efficient storage. </span><span class="koboSpan" id="kobo.1223.4">In streaming scenarios, minimizing data size is crucial for efficient transmission over the network. </span><span class="koboSpan" id="kobo.1223.5">Avro’s compact binary format reduces data size, improving bandwidth utilization. </span><span class="koboSpan" id="kobo.1223.6">Avro also supports schema evolution, allowing for changes in data structures over time without requiring all components to be updated simultaneously. </span><span class="koboSpan" id="kobo.1223.7">Avro’s schema-based approach enables interoperability between different systems and languages, making it suitable for diverse ecosystems. </span><span class="koboSpan" id="kobo.1223.8">Let’s see an example of an </span><span class="No-Break"><span class="koboSpan" id="kobo.1224.1">Avro file:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1225.1">
{
  "type": "record",
  "name": "SensorData",
  "fields": [
    {"name": "sensor_id", "type": "int"},
    {"name": "timestamp", "type": "long"},
    {"name": "value", "type": "float"},
    {"name": "status", "type": "string"}
  ]
}</span></pre>			<p><span class="koboSpan" id="kobo.1226.1">Binary formats </span><a id="_idIndexMarker530"/><span class="koboSpan" id="kobo.1227.1">use a compact binary representation of data, resulting in efficient storage and transmission. </span><span class="koboSpan" id="kobo.1227.2">Various binary protocols can be employed based on specific requirements, such as Google’s </span><strong class="bold"><span class="koboSpan" id="kobo.1228.1">Protocol Buffers</span></strong><span class="koboSpan" id="kobo.1229.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1230.1">protobuf</span></strong><span class="koboSpan" id="kobo.1231.1">) or </span><a id="_idIndexMarker531"/><span class="koboSpan" id="kobo.1232.1">Apache Thrift. </span><span class="koboSpan" id="kobo.1232.2">Binary formats minimize the size of the transmitted data, reducing bandwidth usage in streaming scenarios. </span><span class="koboSpan" id="kobo.1232.3">Binary serialization and deserialization are generally faster than text-based formats, which is crucial in high-velocity streaming environments. </span><span class="koboSpan" id="kobo.1232.4">Let’s have a look at a binary file </span><span class="No-Break"><span class="koboSpan" id="kobo.1233.1">in protobuf:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1234.1">
syntax = "proto3";
message SensorData {
  int32 sensor_id = 1;
  int64 timestamp = 2;
  float value = 3;
  string status = 4;
}</span></pre>			<p><span class="koboSpan" id="kobo.1235.1">In streaming sinks, the choice between JSON, Avro, or binary depends on the specific requirements of the streaming use case, including factors such as interoperability, schema evolution, and data </span><span class="No-Break"><span class="koboSpan" id="kobo.1236.1">size considerations.</span></span></p>
			<p><span class="koboSpan" id="kobo.1237.1">So far, we’ve discussed the most common data sinks used by data engineers and data scientists, as well as the different file types we usually encounter with those sinks. </span><span class="koboSpan" id="kobo.1237.2">In the following sections, we will provide a summary of all the discussed data sinks and file types, as well as their pros and cons and when to best </span><span class="No-Break"><span class="koboSpan" id="kobo.1238.1">use them.</span></span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.1239.1">Which sink is the best for my use case?</span></h2>
			<p><span class="koboSpan" id="kobo.1240.1">Let’s summarize what we’ve</span><a id="_idIndexMarker532"/><span class="koboSpan" id="kobo.1241.1"> learned regarding the different data sinks and get a deeper understanding of when to </span><span class="No-Break"><span class="koboSpan" id="kobo.1242.1">use which:</span></span></p>
			<table id="table006" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1243.1">Technology</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1244.1">Pros</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1245.1">Cons</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1246.1">When </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1247.1">to Choose</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1248.1">Use Case</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1249.1">Relational </span><span class="No-Break"><span class="koboSpan" id="kobo.1250.1">database</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1251.1">ACID properties ensure </span><span class="No-Break"><span class="koboSpan" id="kobo.1252.1">data consistency.</span></span></p>
							<p><span class="koboSpan" id="kobo.1253.1">Mature query languages (SQL) for </span><span class="No-Break"><span class="koboSpan" id="kobo.1254.1">complex queries.</span></span></p>
							<p><span class="koboSpan" id="kobo.1255.1">Support for complex transactions </span><span class="No-Break"><span class="koboSpan" id="kobo.1256.1">and joins.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1257.1">Limited scalability for </span><span class="No-Break"><span class="koboSpan" id="kobo.1258.1">read-heavy workloads.</span></span></p>
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1259.1">Schema changes may be challenging </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1260.1">and downtime-prone.</span></span></p>
							<p><span class="koboSpan" id="kobo.1261.1">May not scale </span><span class="No-Break"><span class="koboSpan" id="kobo.1262.1">well horizontally.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1263.1">Structured data with a well-defined </span><span class="No-Break"><span class="koboSpan" id="kobo.1264.1">schema.</span></span></p>
							<p><span class="koboSpan" id="kobo.1265.1">When you’re maintaining </span><span class="No-Break"><span class="koboSpan" id="kobo.1266.1">relationships</span></span><span class="koboSpan" id="kobo.1267.1">
between data </span><span class="No-Break"><span class="koboSpan" id="kobo.1268.1">entities.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1269.1">Transactional </span><span class="No-Break"><span class="koboSpan" id="kobo.1270.1">applications</span></span></p>
							<p><span class="koboSpan" id="kobo.1271.1">Enterprise applications with structured </span><span class="No-Break"><span class="koboSpan" id="kobo.1272.1">data.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1273.1">NoSQL </span><span class="No-Break"><span class="koboSpan" id="kobo.1274.1">database</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1275.1">Flexible schema, suitable for semi-structured or </span><span class="No-Break"><span class="koboSpan" id="kobo.1276.1">unstructured data.</span></span></p>
							<p><span class="koboSpan" id="kobo.1277.1">Scalability – horizontal scaling is </span><span class="No-Break"><span class="koboSpan" id="kobo.1278.1">often easier.</span></span></p>
							<p><span class="koboSpan" id="kobo.1279.1">High write throughput for </span><span class="No-Break"><span class="koboSpan" id="kobo.1280.1">certain workloads.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1281.1">Lack of standardized query language may require learning a </span><span class="No-Break"><span class="koboSpan" id="kobo.1282.1">specific API.</span></span></p>
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1283.1">May lack ACID compliance in favor of </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1284.1">eventual consistency.</span></span></p>
							<p><span class="koboSpan" id="kobo.1285.1">Limited support for </span><span class="No-Break"><span class="koboSpan" id="kobo.1286.1">complex transactions.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1287.1">Dynamic or evolving data </span><span class="No-Break"><span class="koboSpan" id="kobo.1288.1">schema.</span></span></p>
							<p><span class="koboSpan" id="kobo.1289.1">Rapid development </span><span class="No-Break"><span class="koboSpan" id="kobo.1290.1">and iteration.</span></span></p>
							<p><span class="koboSpan" id="kobo.1291.1">Handling large volumes of data with varying </span><span class="No-Break"><span class="koboSpan" id="kobo.1292.1">structures.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1293.1">Document databases for content </span><span class="No-Break"><span class="koboSpan" id="kobo.1294.1">management.</span></span></p>
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1295.1">Real-time a</span></span><span class="koboSpan" id="kobo.1296.1">
pplications with variable </span><span class="No-Break"><span class="koboSpan" id="kobo.1297.1">schema.</span></span></p>
							<p><span class="koboSpan" id="kobo.1298.1">JSON data storage for web </span><span class="No-Break"><span class="koboSpan" id="kobo.1299.1">applications</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1300.1">Data </span><span class="No-Break"><span class="koboSpan" id="kobo.1301.1">warehouse</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1302.1">Optimized for complex analytics </span><span class="No-Break"><span class="koboSpan" id="kobo.1303.1">and reporting.</span></span></p>
							<p><span class="koboSpan" id="kobo.1304.1">Efficient data compression </span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1">and indexing.</span></span></p>
							<p><span class="koboSpan" id="kobo.1306.1">Scalability for read-heavy analytical </span><span class="No-Break"><span class="koboSpan" id="kobo.1307.1">workloads.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1308.1">May not be cost-effective for high-volume </span><span class="No-Break"><span class="koboSpan" id="kobo.1309.1">transactional workloads.</span></span></p>
							<p><span class="koboSpan" id="kobo.1310.1">May have higher latency for </span><span class="No-Break"><span class="koboSpan" id="kobo.1311.1">real-time queries.</span></span></p>
							<p><span class="koboSpan" id="kobo.1312.1">May require specialized skills</span><a id="_idIndexMarker533"/><span class="koboSpan" id="kobo.1313.1"> for maintenance </span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">and optimization.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1315.1">Analytical processing on </span><span class="No-Break"><span class="koboSpan" id="kobo.1316.1">large datasets.</span></span></p>
							<p><span class="koboSpan" id="kobo.1317.1">Aggregating and analyzing </span><span class="No-Break"><span class="koboSpan" id="kobo.1318.1">historical data.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1319.1">Business intelligence and reporting </span><span class="No-Break"><span class="koboSpan" id="kobo.1320.1">tools.</span></span></p>
							<p><span class="koboSpan" id="kobo.1321.1">Running complex queries on terabytes </span><span class="No-Break"><span class="koboSpan" id="kobo.1322.1">of data.</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1323.1">Technology</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1324.1">Pros</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1325.1">Cons</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1326.1">When </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1327.1">to Choose</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1328.1">Use Case</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1329.1">Lakehouse</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1330.1">Unified platform combining data lake and data </span><span class="No-Break"><span class="koboSpan" id="kobo.1331.1">warehouse features.</span></span></p>
							<p><span class="koboSpan" id="kobo.1332.1">Offers scalable storage and </span><span class="No-Break"><span class="koboSpan" id="kobo.1333.1">computing resources.</span></span></p>
							<p><span class="koboSpan" id="kobo.1334.1">It can efficiently scale horizontally to handle growing datasets and </span><span class="No-Break"><span class="koboSpan" id="kobo.1335.1">processing demands.</span></span></p>
							<p><span class="koboSpan" id="kobo.1336.1">Pay-as-you-go model, allowing organizations to manage costs more efficiently by paying for the resources they use. </span><span class="koboSpan" id="kobo.1336.2">This is particularly beneficial for storing large amounts of </span><span class="No-Break"><span class="koboSpan" id="kobo.1337.1">raw data.</span></span></p>
							<p><span class="koboSpan" id="kobo.1338.1">Follow a schema-on-read approach, allowing for the storage of raw, </span><span class="No-Break"><span class="koboSpan" id="kobo.1339.1">untransformed data.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1340.1">Complexity in managing both schema-on-read </span><span class="No-Break"><span class="koboSpan" id="kobo.1341.1">and schema-on-write.</span></span></p>
							<p><span class="koboSpan" id="kobo.1342.1">Depending on the implementation and cloud provider, costs associated with storage, processing, and managing data in a Lakehouse architecture may</span><a id="_idIndexMarker534"/><span class="koboSpan" id="kobo.1343.1"> vary. </span><span class="koboSpan" id="kobo.1343.2">Organizations need to carefully manage costs to </span><span class="No-Break"><span class="koboSpan" id="kobo.1344.1">ensure efficiency.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1345.1">A balance </span><span class="No-Break"><span class="koboSpan" id="kobo.1346.1">between</span></span><span class="koboSpan" id="kobo.1347.1">
 flexibility and transactional </span><span class="No-Break"><span class="koboSpan" id="kobo.1348.1">capabilities.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1349.1">Real-time analytics with long-term </span><span class="No-Break"><span class="koboSpan" id="kobo.1350.1">storage.</span></span></p>
							<p><span class="koboSpan" id="kobo.1351.1">Any engineering, machine learning, and analytics use </span><span class="No-Break"><span class="koboSpan" id="kobo.1352.1">case</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1353.1">Streaming </span><span class="No-Break"><span class="koboSpan" id="kobo.1354.1">sinks</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1355.1">Enables real-time processing and analysis of </span><span class="No-Break"><span class="koboSpan" id="kobo.1356.1">streaming data.</span></span></p>
							<p><span class="koboSpan" id="kobo.1357.1">Scales horizontally to handle high volumes of </span><span class="No-Break"><span class="koboSpan" id="kobo.1358.1">incoming data.</span></span></p>
							<p><span class="koboSpan" id="kobo.1359.1">Integral to building event-driven </span><span class="No-Break"><span class="koboSpan" id="kobo.1360.1">architectures.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1361.1">Implementing </span><a id="_idIndexMarker535"/><span class="koboSpan" id="kobo.1362.1">and managing streaming data sinks can </span><span class="No-Break"><span class="koboSpan" id="kobo.1363.1">be complex.</span></span></p>
							<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1364.1">The processing and persistence of streaming data introduces </span></span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1365.1">some latency.</span></span></p>
							<p><span class="koboSpan" id="kobo.1366.1">Depending on the chosen solution, infrastructure costs may be </span><span class="No-Break"><span class="koboSpan" id="kobo.1367.1">a consideration.</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1368.1">Continuous ingestion and </span><span class="No-Break"><span class="koboSpan" id="kobo.1369.1">processing of</span></span><span class="koboSpan" id="kobo.1370.1">
data in </span><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">real-time</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1372.1">IoT</span></span></p>
							<p><span class="koboSpan" id="kobo.1373.1">Real-time analytical </span><span class="No-Break"><span class="koboSpan" id="kobo.1374.1">use cases</span></span></p>
							<p><span class="koboSpan" id="kobo.1375.1">Systems </span><span class="No-Break"><span class="koboSpan" id="kobo.1376.1">monitoring</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1377.1">Table 7.6 – Summary table of all the data sinks, as well as their pros, cons, and use cases</span></p>
			<p><span class="koboSpan" id="kobo.1378.1">In </span><em class="italic"><span class="koboSpan" id="kobo.1379.1">Table 7.8</span></em><span class="koboSpan" id="kobo.1380.1">, the </span><strong class="bold"><span class="koboSpan" id="kobo.1381.1">Use Case</span></strong><span class="koboSpan" id="kobo.1382.1"> column </span><a id="_idIndexMarker536"/><span class="koboSpan" id="kobo.1383.1">provides more context and practical examples of how each data sink technology can be effectively applied in </span><span class="No-Break"><span class="koboSpan" id="kobo.1384.1">real-world scenarios.</span></span></p>
			<p><span class="koboSpan" id="kobo.1385.1">Moving from selecting the right data sink technology to choosing the appropriate file type is a crucial step in designing an effective data processing pipeline. </span><span class="koboSpan" id="kobo.1385.2">Once you’ve determined where your data will be stored (data sink), you need to consider how it will be stored (file type). </span><span class="koboSpan" id="kobo.1385.3">The choice of file type can impact data storage efficiency, query performance, data integrity, and interoperability with </span><span class="No-Break"><span class="koboSpan" id="kobo.1386.1">other systems.</span></span></p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.1387.1">Decoding file types for optimal usage</span></h1>
			<p><span class="koboSpan" id="kobo.1388.1">Choosing the</span><a id="_idIndexMarker537"/><span class="koboSpan" id="kobo.1389.1"> right file type when selecting a data sink is crucial for optimizing data storage, processing, and retrieval. </span><span class="koboSpan" id="kobo.1389.2">One of the file types that we haven’t discussed so far but is very important since it’s used as an underline format for other file formats is the </span><span class="No-Break"><span class="koboSpan" id="kobo.1390.1">Parquet file.</span></span></p>
			<p><span class="koboSpan" id="kobo.1391.1">Parquet is a columnar storage file format designed for efficient data storage and processing in big data and analytics environments. </span><span class="koboSpan" id="kobo.1391.2">It is an open standard file format that provides benefits such as high compression ratios, columnar storage, and support for complex data structures. </span><span class="koboSpan" id="kobo.1391.3">Parquet is widely adopted in the Apache Hadoop ecosystem and is supported by various data </span><span class="No-Break"><span class="koboSpan" id="kobo.1392.1">processing frameworks.</span></span></p>
			<p><span class="koboSpan" id="kobo.1393.1">Parquet stores data in a columnar format, which means values from the same column are stored together. </span><span class="koboSpan" id="kobo.1393.2">This design is advantageous for analytics workloads where queries often involve selecting a subset of columns. </span><span class="koboSpan" id="kobo.1393.3">Parquet also supports different compression algorithms, allowing users to choose the one that best suits their requirements. </span><span class="koboSpan" id="kobo.1393.4">This contributes to reduced storage space and improved query performance. </span><span class="koboSpan" id="kobo.1393.5">Parquet files can handle schema evolution as well, making it possible to add or remove columns without requiring a complete rewrite of the dataset. </span><span class="koboSpan" id="kobo.1393.6">This feature is essential for scenarios where the data schema evolves. </span><span class="koboSpan" id="kobo.1393.7">Due to its advantages, Parquet has become a widely adopted and standardized file format in the big data ecosystem, forming the basis for other optimized formats, such as Delta </span><span class="No-Break"><span class="koboSpan" id="kobo.1394.1">and Iceberg.</span></span></p>
			<p><span class="koboSpan" id="kobo.1395.1">Having</span><a id="_idIndexMarker538"/><span class="koboSpan" id="kobo.1396.1"> discussed Parquet files, we can now compare the common file types, along with their pros and cons, and provide guidance on when to choose each type for different </span><span class="No-Break"><span class="koboSpan" id="kobo.1397.1">data sinks:</span></span></p>
			<table id="table007" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1398.1">File Type</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1399.1">Pros</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1400.1">Cons</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1401.1">When </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1402.1">to Choose</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1403.1">JSON</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1404.1">Human-readable</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1405.1">Larger file size compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.1406.1">binary formats</span></span></p>
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1407.1">Slower serialization/</span></span>
<span class="No-Break"><span class="koboSpan" id="kobo.1408.1">deserialization</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1409.1">Semi-structured or human-readable data </span><span class="No-Break"><span class="koboSpan" id="kobo.1410.1">is required</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1411.1">BSON</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1412.1">Compact </span><span class="No-Break"><span class="koboSpan" id="kobo.1413.1">binary format</span></span></p>
							<p><span class="koboSpan" id="kobo.1414.1">Supports richer </span><span class="No-Break"><span class="koboSpan" id="kobo.1415.1">data types</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1416.1">May not be as human-readable as JSON, with limited adoption </span><span class="No-Break"><span class="koboSpan" id="kobo.1417.1">outside MongoDB</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1418.1">Efficiency in storage and transmission </span><span class="No-Break"><span class="koboSpan" id="kobo.1419.1">is crucial</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1420.1">Parquet</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1421.1">Columnar storage, which is efficient </span><span class="No-Break"><span class="koboSpan" id="kobo.1422.1">for analytics</span></span></p>
							<p><span class="koboSpan" id="kobo.1423.1">Compression and encoding lead to smaller </span><span class="No-Break"><span class="koboSpan" id="kobo.1424.1">file sizes</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1425.1">Not as human-readable </span><span class="No-Break"><span class="koboSpan" id="kobo.1426.1">as JSON</span></span></p>
							<p><span class="koboSpan" id="kobo.1427.1">You can’t update tables – you need to </span><span class="No-Break"><span class="koboSpan" id="kobo.1428.1">rewrite them</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1429.1">Analytical processing, </span><span class="No-Break"><span class="koboSpan" id="kobo.1430.1">data warehousing</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1431.1">Avro</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1432.1">Compact </span><span class="No-Break"><span class="koboSpan" id="kobo.1433.1">binary serialization</span></span></p>
							<p><span class="koboSpan" id="kobo.1434.1">Schema-based and supports </span><span class="No-Break"><span class="koboSpan" id="kobo.1435.1">schema evolution</span></span></p>
							<p><span class="koboSpan" id="kobo.1436.1">Interoperable across </span><span class="No-Break"><span class="koboSpan" id="kobo.1437.1">different systems</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1438.1">Slightly less human-readable compared </span><span class="No-Break"><span class="koboSpan" id="kobo.1439.1">to JSON</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1440.1">Bandwidth-efficient streaming and diverse </span><span class="No-Break"><span class="koboSpan" id="kobo.1441.1">language support</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1442.1">Delta</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1443.1">ACID transactions for </span><span class="No-Break"><span class="koboSpan" id="kobo.1444.1">data consistency</span></span></p>
							<p><span class="koboSpan" id="kobo.1445.1">Efficient storage format for </span><span class="No-Break"><span class="koboSpan" id="kobo.1446.1">data lakes</span></span></p>
							<p><span class="koboSpan" id="kobo.1447.1">Schema evolution and </span><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">time-travel queries</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1449.1">Larger size </span><span class="No-Break"><span class="koboSpan" id="kobo.1450.1">than Parquet</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1451.1">Real-time analytics with </span><span class="No-Break"><span class="koboSpan" id="kobo.1452.1">long-term storage</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1453.1">Hudi</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1454.1">Efficient incremental </span><span class="No-Break"><span class="koboSpan" id="kobo.1455.1">data processing</span></span></p>
							<p><span class="koboSpan" id="kobo.1456.1">ACID</span><a id="_idIndexMarker539"/><span class="koboSpan" id="kobo.1457.1"> transactions for </span><span class="No-Break"><span class="koboSpan" id="kobo.1458.1">real-time data</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1459.1">Larger size </span><span class="No-Break"><span class="koboSpan" id="kobo.1460.1">than Parquet</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1461.1">Streaming data applications and change </span><span class="No-Break"><span class="koboSpan" id="kobo.1462.1">data capture</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1463.1">Iceberg</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1464.1">Schema evolution, </span><span class="No-Break"><span class="koboSpan" id="kobo.1465.1">ACID transactions</span></span></p>
							<p><span class="koboSpan" id="kobo.1466.1">Optimized storage formats such </span><span class="No-Break"><span class="koboSpan" id="kobo.1467.1">as Parquet</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1468.1">Larger size </span><span class="No-Break"><span class="koboSpan" id="kobo.1469.1">than Parquet</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1470.1">Time-travel queries and </span><span class="No-Break"><span class="koboSpan" id="kobo.1471.1">evolving schemas</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1472.1">File Type</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1473.1">Pros</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1474.1">Cons</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1475.1">When </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1476.1">to Choose</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1477.1">Binary </span><span class="No-Break"><span class="koboSpan" id="kobo.1478.1">format</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1479.1">Compact and </span><span class="No-Break"><span class="koboSpan" id="kobo.1480.1">efficient storage</span></span></p>
							<p><span class="koboSpan" id="kobo.1481.1">Fast serialization </span><span class="No-Break"><span class="koboSpan" id="kobo.1482.1">and deserialization</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1483.1">Not human-readable</span></span></p>
							<p><span class="koboSpan" id="kobo.1484.1">Limited support for </span><span class="No-Break"><span class="koboSpan" id="kobo.1485.1">schema evolution</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1486.1">Efficiency is crucial in bandwidth usage and </span><span class="No-Break"><span class="koboSpan" id="kobo.1487.1">processing speed</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1488.1">Table 7.7 – A summary table of all the file formats, as well as their pros, cons, and use cases</span></p>
			<p><span class="koboSpan" id="kobo.1489.1">In the next section, we’re going to discuss </span><strong class="bold"><span class="koboSpan" id="kobo.1490.1">partitioning</span></strong><span class="koboSpan" id="kobo.1491.1">, an important concept in the context of data storage, especially in distributed storage systems. </span><span class="koboSpan" id="kobo.1491.2">While the concept of partitioning itself is more closely associated with data lakes, data warehouses, and distributed filesystems, its relevance extends to the broader discussion of </span><span class="No-Break"><span class="koboSpan" id="kobo.1492.1">data sinks.</span></span></p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.1493.1">Navigating partitioning</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.1494.1">Data partitioning</span></strong><span class="koboSpan" id="kobo.1495.1"> is a </span><a id="_idIndexMarker540"/><span class="koboSpan" id="kobo.1496.1">technique</span><a id="_idIndexMarker541"/><span class="koboSpan" id="kobo.1497.1"> that’s used to divide and organize large datasets into smaller, more manageable subsets called partitions. </span><span class="koboSpan" id="kobo.1497.2">When writing data to sinks, such as databases or distributed storage systems, employing appropriate data partitioning strategies is crucial for optimizing query performance, data retrieval, and storage efficiency. </span><span class="koboSpan" id="kobo.1497.3">Partitioning in data storage systems, including time-based, geographic, and hybrid partitioning, offers several benefits in terms of </span><a id="_idIndexMarker542"/><span class="koboSpan" id="kobo.1498.1">read operations, updates, </span><span class="No-Break"><span class="koboSpan" id="kobo.1499.1">and writes:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.1500.1">When querying the data, partitioning allows the system to skip irrelevant data quickly. </span><span class="koboSpan" id="kobo.1500.2">For example, in </span><strong class="bold"><span class="koboSpan" id="kobo.1501.1">time-based partitioning</span></strong><span class="koboSpan" id="kobo.1502.1">, if you’re interested in data for a specific date, the</span><a id="_idIndexMarker543"/><span class="koboSpan" id="kobo.1503.1"> system can directly access the partition corresponding to that date, leading to faster query times. </span><span class="koboSpan" id="kobo.1503.2">It ensures that only the necessary partitions are scanned, reducing the amount of data </span><span class="No-Break"><span class="koboSpan" id="kobo.1504.1">to process.</span></span></li>
				<li><span class="koboSpan" id="kobo.1505.1">Partitioning can simplify updates, especially when the updates are concentrated in specific partitions. </span><span class="koboSpan" id="kobo.1505.2">For example, if you need to update data for a specific date or region, the system can isolate the affected partition, reducing the scope of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1506.1">update operation.</span></span></li>
				<li><span class="koboSpan" id="kobo.1507.1">Partitioning can enhance the efficiency of write operations, particularly when appending data. </span><span class="koboSpan" id="kobo.1507.2">New data can be written to the appropriate partition without affecting the existing data, leading to a more straightforward and faster </span><span class="No-Break"><span class="koboSpan" id="kobo.1508.1">write process.</span></span></li>
				<li><span class="koboSpan" id="kobo.1509.1">Partitioning supports parallel processing. </span><span class="koboSpan" id="kobo.1509.2">Different partitions can be read or written concurrently, enabling better utilization of resources and faster overall </span><span class="No-Break"><span class="koboSpan" id="kobo.1510.1">processing times.</span></span></li>
				<li><span class="koboSpan" id="kobo.1511.1">Partitioning provides a logical organization of data. </span><span class="koboSpan" id="kobo.1511.2">It simplifies data management tasks such as archiving old data, deleting obsolete records, or migrating specific partitions to different storage tiers based on </span><span class="No-Break"><span class="koboSpan" id="kobo.1512.1">access patterns.</span></span></li>
				<li><span class="koboSpan" id="kobo.1513.1">With partitioning, you</span><a id="_idIndexMarker544"/><span class="koboSpan" id="kobo.1514.1"> can optimize storage based on usage patterns. </span><span class="koboSpan" id="kobo.1514.2">For example, frequently accessed partitions can be stored in high-performance storage, while less frequently accessed partitions can be stored in </span><span class="No-Break"><span class="koboSpan" id="kobo.1515.1">lower-cost storage.</span></span></li>
				<li><span class="koboSpan" id="kobo.1516.1">Partitioning</span><a id="_idIndexMarker545"/><span class="koboSpan" id="kobo.1517.1"> supports pruning, where the system can eliminate entire partitions from consideration during query execution. </span><span class="koboSpan" id="kobo.1517.2">This pruning mechanism further accelerates </span><span class="No-Break"><span class="koboSpan" id="kobo.1518.1">query performance.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1519.1">Let’s have a closer look at the different </span><span class="No-Break"><span class="koboSpan" id="kobo.1520.1">partitioning strategies.</span></span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.1521.1">Horizontal versus vertical partitioning</span></h2>
			<p><span class="koboSpan" id="kobo.1522.1">When discussing partitioning strategies in the context of databases or distributed systems, we generally refer to two main types: </span><strong class="bold"><span class="koboSpan" id="kobo.1523.1">horizontal partitioning</span></strong><span class="koboSpan" id="kobo.1524.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1525.1">vertical partitioning</span></strong><span class="koboSpan" id="kobo.1526.1">. </span><span class="koboSpan" id="kobo.1526.2">Each</span><a id="_idIndexMarker546"/><span class="koboSpan" id="kobo.1527.1"> approach organizes data differently to improve performance, scalability, or manageability. </span><span class="koboSpan" id="kobo.1527.2">Let’s start with </span><span class="No-Break"><span class="koboSpan" id="kobo.1528.1">horizontal partitioning.</span></span></p>
			<p><span class="koboSpan" id="kobo.1529.1">Horizontal partitioning, or</span><a id="_idIndexMarker547"/><span class="koboSpan" id="kobo.1530.1"> sharding, involves</span><a id="_idIndexMarker548"/><span class="koboSpan" id="kobo.1531.1"> dividing a table’s rows into multiple partitions, each containing a subset of the data. </span><span class="koboSpan" id="kobo.1531.2">This approach is commonly used to scale out databases by distributing data across multiple servers, where each shard maintains the same schema but holds different rows. </span><span class="koboSpan" id="kobo.1531.3">For example, a user table in a large application could be sharded by user IDs, with IDs 1 to 10,000 in one partition and IDs 10,001 to 20,000 in another. </span><span class="koboSpan" id="kobo.1531.4">This strategy enables the system to handle larger datasets than a single machine could manage, enhancing performance in </span><span class="No-Break"><span class="koboSpan" id="kobo.1532.1">large-scale applications.</span></span></p>
			<p><span class="koboSpan" id="kobo.1533.1">Vertical partitioning, on</span><a id="_idIndexMarker549"/><span class="koboSpan" id="kobo.1534.1"> the other hand, involves splitting a </span><a id="_idIndexMarker550"/><span class="koboSpan" id="kobo.1535.1">table’s columns into different partitions, where each partition contains a subset of columns but includes all rows. </span><span class="koboSpan" id="kobo.1535.2">This strategy is effective when different columns are accessed or updated at varying frequencies as it optimizes performance by minimizing the amount of data that’s processed during a query. </span><span class="koboSpan" id="kobo.1535.3">For example, in a user profiles table, basic information such as name and email could be stored in one partition, while a large binary data column, such as a profile picture, is stored in another. </span><span class="koboSpan" id="kobo.1535.4">This allows queries targeting specific columns to access a smaller, more efficient dataset, thereby </span><span class="No-Break"><span class="koboSpan" id="kobo.1536.1">enhancing performance.</span></span></p>
			<p><span class="koboSpan" id="kobo.1537.1">Both strategies can </span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.1538.1">be used in combination to meet the </span><a id="_idIndexMarker552"/><span class="koboSpan" id="kobo.1539.1">specific needs of a database system, depending on the data structure and access patterns. </span><span class="koboSpan" id="kobo.1539.2">The </span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.1540.1">reality is that </span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.1541.1">in the data field, </span><strong class="bold"><span class="koboSpan" id="kobo.1542.1">horizontal partitioning</span></strong><span class="koboSpan" id="kobo.1543.1"> is more commonly seen and widely adopted than vertical partitioning. </span><span class="koboSpan" id="kobo.1543.2">This is particularly true in large-scale, distributed databases and applications that need to handle vast amounts of data, high traffic, or geographically dispersed users. </span><span class="koboSpan" id="kobo.1543.3">In the next section, we will see some examples of </span><span class="No-Break"><span class="koboSpan" id="kobo.1544.1">horizontal partitioning.</span></span></p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.1545.1">Time-based partitioning</span></h2>
			<p><span class="koboSpan" id="kobo.1546.1">Time-based partitioning </span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.1547.1">involves organizing data based </span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.1548.1">on timestamps. </span><span class="koboSpan" id="kobo.1548.2">Each partition represents a specific time interval, such as a day, hour, or minute. </span><span class="koboSpan" id="kobo.1548.3">It allows for efficient retrieval of historical data and time-based aggregations. </span><span class="koboSpan" id="kobo.1548.4">It also facilitates data retention and </span><span class="No-Break"><span class="koboSpan" id="kobo.1549.1">archiving policies.</span></span></p>
			<p><span class="koboSpan" id="kobo.1550.1">In this example, you’ll learn </span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.1551.1">how to create time-based partitioning on your local laptop using Parquet files. </span><span class="koboSpan" id="kobo.1551.2">You can find the full code here: </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/5.time_based_partitioning.py"><span class="koboSpan" id="kobo.1552.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/5.time_based_partitioning.py</span></a><span class="koboSpan" id="kobo.1553.1">. </span><span class="koboSpan" id="kobo.1553.2">Follow </span><span class="No-Break"><span class="koboSpan" id="kobo.1554.1">these steps:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1555.1">Import the </span><span class="No-Break"><span class="koboSpan" id="kobo.1556.1">required libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1557.1">
import os
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from datetime import datetime</span></pre></li>				<li><span class="koboSpan" id="kobo.1558.1">Define a sample dataset with two columns: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1559.1">timestamp</span></strong><span class="koboSpan" id="kobo.1560.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1561.1">value</span></strong><span class="koboSpan" id="kobo.1562.1">. </span><span class="koboSpan" id="kobo.1562.2">This dataset represents time series data with timestamps and </span><span class="No-Break"><span class="koboSpan" id="kobo.1563.1">corresponding values:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1564.1">
data = {
    "timestamp": ["2022-01-01", "2022-01-01", "2022-01-02"],
    "value": [10, 15, 12]
}</span></pre></li>				<li><span class="koboSpan" id="kobo.1565.1">Create a </span><span class="No-Break"><span class="koboSpan" id="kobo.1566.1">pandas DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1567.1">
df = pd.DataFrame(data)</span></pre></li>				<li><span class="koboSpan" id="kobo.1568.1">Convert the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1569.1">timestamp</span></strong><span class="koboSpan" id="kobo.1570.1"> column into a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1571.1">datetime</span></strong><span class="koboSpan" id="kobo.1572.1"> type. </span><span class="koboSpan" id="kobo.1572.2">This ensures that the timestamps </span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.1573.1">are treated as datetime objects for accurate </span><span class="No-Break"><span class="koboSpan" id="kobo.1574.1">time-based operations:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1575.1">
df["timestamp"] = pd.to_datetime(df["timestamp"])</span></pre></li>				<li><span class="koboSpan" id="kobo.1576.1">Update </span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.1577.1">the path to store the data. </span><span class="koboSpan" id="kobo.1577.2">Use an </span><span class="No-Break"><span class="koboSpan" id="kobo.1578.1">existing path:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1579.1">
base_path = " path_to_write_data"</span></pre></li>				<li><span class="koboSpan" id="kobo.1580.1">Iterate through the DataFrame, grouping rows by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1581.1">date</span></strong><span class="koboSpan" id="kobo.1582.1"> component of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1583.1">timestamp</span></strong><span class="koboSpan" id="kobo.1584.1"> column. </span><span class="koboSpan" id="kobo.1584.2">Convert each group into a PyArrow table and write it to the corresponding partition path in </span><span class="No-Break"><span class="koboSpan" id="kobo.1585.1">Parquet format:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1586.1">
for timestamp, group in df.groupby(df["timestamp"].dt.date):</span></pre></li>				<li><span class="koboSpan" id="kobo.1587.1">Create the directory if it </span><span class="No-Break"><span class="koboSpan" id="kobo.1588.1">doesn’t exist:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1589.1">
os.makedirs(base_path, exist_ok=True)
partition_path = os.path.join(base_path, str(timestamp))
table = pa.Table.from_pandas(group)
pq.write_table(table, partition_path)</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1590.1">After executing this script, you’ll see two Parquet files being created in your base directory – one for each day of </span><span class="No-Break"><span class="koboSpan" id="kobo.1591.1">the week:</span></span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1592.1"><img src="image/B19801_07_6.jpg" alt="Figure 7.6 – Time-based partitioning output"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1593.1">Figure 7.6 – Time-based partitioning output</span></p>
			<p><span class="koboSpan" id="kobo.1594.1">Let’s have a look at </span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.1595.1">another common partitioning strategy, known as </span><span class="No-Break"><span class="koboSpan" id="kobo.1596.1">geographic partitioning.</span></span></p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.1597.1">Geographic partitioning</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.1598.1">Geographic partitioning</span></strong><span class="koboSpan" id="kobo.1599.1"> involves </span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.1600.1">dividing data based on geographical </span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.1601.1">attributes such as regions, countries, or cities. </span><span class="koboSpan" id="kobo.1601.2">This strategy is valuable when you’re dealing with geospatial data or location-based analytics. </span><span class="koboSpan" id="kobo.1601.3">It enables fast and targeted retrieval of data related to specific geographic areas, thus supporting spatial queries </span><span class="No-Break"><span class="koboSpan" id="kobo.1602.1">and analysis.</span></span></p>
			<p><span class="koboSpan" id="kobo.1603.1">Here’s an example</span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.1604.1"> of how you can create geographic-based partitioning in your local laptop using Parquet files. </span><span class="koboSpan" id="kobo.1604.2">You can find the full code here: </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/6.geo_partitioning.py"><span class="koboSpan" id="kobo.1605.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/6.geo_partitioning.py</span></a><span class="koboSpan" id="kobo.1606.1">. </span><span class="koboSpan" id="kobo.1606.2">Follow </span><span class="No-Break"><span class="koboSpan" id="kobo.1607.1">these steps:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1608.1">Create a base directory for storing </span><span class="No-Break"><span class="koboSpan" id="kobo.1609.1">partitioned data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1610.1">
base_directory = "/geo_data"
os.makedirs(base_directory, exist_ok=True)</span></pre></li>				<li><span class="koboSpan" id="kobo.1611.1">Convert each group (region-specific data) into a PyArrow table. </span><span class="koboSpan" id="kobo.1611.2">Then, write the tables to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1612.1">corresponding paths:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1613.1">
geo_data = {"region": ["North", "South", "East"],
            "value": [10, 15, 12]}
geo_df = pd.DataFrame(geo_data)
for region, group in geo_df.groupby("region"):</span></pre></li>				<li><span class="koboSpan" id="kobo.1614.1">Create a directory for each region within the </span><span class="No-Break"><span class="koboSpan" id="kobo.1615.1">base directory:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1616.1">
region_path = os.path.join(base_directory, region)</span></pre></li>				<li><span class="koboSpan" id="kobo.1617.1">Convert the</span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.1618.1"> group into a PyArrow table and write it to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1619.1">partition path:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1620.1">
table = pa.Table.from_pandas(group)
pq.write_table(table, region_path)</span></pre></li>				<li><span class="koboSpan" id="kobo.1621.1">After </span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.1622.1">executing this script, you will see three Parquet files being created in your base directory – one for each geographic location available in </span><span class="No-Break"><span class="koboSpan" id="kobo.1623.1">the data:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1624.1"><img src="image/B19801_07_7.jpg" alt="Figure 7.7 – Geographic-based partitioning output"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1625.1">Figure 7.7 – Geographic-based partitioning output</span></p>
			<p><span class="koboSpan" id="kobo.1626.1">Let’s have a look at the last common partitioning strategy, known as </span><span class="No-Break"><span class="koboSpan" id="kobo.1627.1">hybrid partitioning.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1628.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1629.1">Geographic partitioning</span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.1630.1"> is a specialized form of category partitioning that organizes data based on geographical attributes or spatial criteria. </span><strong class="bold"><span class="koboSpan" id="kobo.1631.1">Category partitioning</span></strong><span class="koboSpan" id="kobo.1632.1"> is a fundamental strategy </span><a id="_idIndexMarker567"/><span class="koboSpan" id="kobo.1633.1">in data organization that involves grouping data based on specific categories or attributes, such as customer demographics, product types, or </span><span class="No-Break"><span class="koboSpan" id="kobo.1634.1">transactional characteristics.</span></span></p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.1635.1">Hybrid partitioning</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.1636.1">Hybrid partitioning</span></strong><span class="koboSpan" id="kobo.1637.1"> involves</span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.1638.1"> combining multiple </span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.1639.1">partitioning strategies to optimize data organization for specific use cases. </span><span class="koboSpan" id="kobo.1639.2">For instance, you might partition data first by time and then further partition each time interval by a key or geographic location. </span><span class="koboSpan" id="kobo.1639.3">It offers flexibility for addressing complex querying patterns and diverse data </span><span class="No-Break"><span class="koboSpan" id="kobo.1640.1">access requirements.</span></span></p>
			<p><span class="koboSpan" id="kobo.1641.1">Here’s an example </span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.1642.1">of how you can create hybrid partitioning on your local laptop using Parquet files. </span><span class="koboSpan" id="kobo.1642.2">You can find the full code here: </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/7.hybrid_partitioning.py"><span class="koboSpan" id="kobo.1643.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter07/7.hybrid_partitioning.py</span></a><span class="koboSpan" id="kobo.1644.1">. </span><span class="koboSpan" id="kobo.1644.2">Follow </span><span class="No-Break"><span class="koboSpan" id="kobo.1645.1">these steps:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1646.1">Create a base directory for storing </span><span class="No-Break"><span class="koboSpan" id="kobo.1647.1">partitioned data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1648.1">
base_directory = "/hybrid_data"</span></pre></li>				<li><span class="koboSpan" id="kobo.1649.1">Perform </span><span class="No-Break"><span class="koboSpan" id="kobo.1650.1">hybrid partitioning:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1651.1">
hybrid_data = {
    "timestamp": ["2022-01-01", "2022-01-01", "2022-01-02"],
    "region": ["North", "South", "East"],
    "value": [10, 15, 12]}
hybrid_df = pd.DataFrame(hybrid_data)
for (timestamp, region), group in hybrid_df.groupby(
    ["timestamp", "region"]):</span></pre></li>				<li><span class="koboSpan" id="kobo.1652.1">Create a directory for each timestamp and region combination within the </span><span class="No-Break"><span class="koboSpan" id="kobo.1653.1">base directory:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1654.1">
timestamp_path = os.path.join(base_directory, str(timestamp))
os.makedirs(timestamp_path, exist_ok=True)
timestamp_region_path = os.path.join(
    base_directory, str(timestamp), str(region))</span></pre></li>				<li><span class="koboSpan" id="kobo.1655.1">Convert the</span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.1656.1"> group into a PyArrow table and write it to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1657.1">partition path:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1658.1">
table = pa.Table.from_pandas(group)
pq.write_table(table, timestamp_region_path)</span></pre></li>				<li><span class="koboSpan" id="kobo.1659.1">After </span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.1660.1">executing this script, you will see three Parquet files being created in your base directory – two locations for January 1, 2022, and one for January </span><span class="No-Break"><span class="koboSpan" id="kobo.1661.1">2, 2022:</span></span></li>
			</ol>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1662.1"><img src="image/B19801_07_8.jpg" alt="Figure 7.8 – Hybrid partitioning output"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1663.1">Figure 7.8 – Hybrid partitioning output</span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1664.1">Remember</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1665.1">So far, we’ve explored various types of partitioning, such as time-based and geographic. </span><span class="koboSpan" id="kobo.1665.2">However, remember you can use any column that makes sense in your data, your use case, and the query patterns for the table as </span><span class="No-Break"><span class="koboSpan" id="kobo.1666.1">partitioning column(s).</span></span></p>
			<p><span class="koboSpan" id="kobo.1667.1">Now that we’ve discussed different partitioning strategies, it’s time to talk about how to choose the column you will partition your </span><span class="No-Break"><span class="koboSpan" id="kobo.1668.1">data on.</span></span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.1669.1">Considerations for choosing partitioning strategies</span></h2>
			<p><span class="koboSpan" id="kobo.1670.1">Choosing the</span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.1671.1"> right partitioning strategy for your data involves considering various factors to optimize performance, query efficiency, and data management. </span><span class="koboSpan" id="kobo.1671.2">Here are some key considerations for choosing </span><span class="No-Break"><span class="koboSpan" id="kobo.1672.1">partitioning strategies:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1673.1">Query patterns</span></strong><span class="koboSpan" id="kobo.1674.1">: Select partitioning strategies based on the types of queries your application or analytics platform will perform </span><span class="No-Break"><span class="koboSpan" id="kobo.1675.1">most frequently.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1676.1">Data distribution</span></strong><span class="koboSpan" id="kobo.1677.1">: Ensure partitions are distributed evenly to prevent data hotspots and </span><span class="No-Break"><span class="koboSpan" id="kobo.1678.1">resource contention.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1679.1">Data size</span></strong><span class="koboSpan" id="kobo.1680.1">: Consider the volume of data that will be stored in each partition. </span><span class="koboSpan" id="kobo.1680.2">Smaller partitions can improve query performance, but too many small partitions might impact </span><span class="No-Break"><span class="koboSpan" id="kobo.1681.1">management overhead.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1682.1">Query complexity</span></strong><span class="koboSpan" id="kobo.1683.1">: Some queries might benefit from hybrid partitioning, especially if they involve </span><span class="No-Break"><span class="koboSpan" id="kobo.1684.1">multiple attributes.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1685.1">Scalability</span></strong><span class="koboSpan" id="kobo.1686.1">: Partitioning should allow for future scalability and accommodate data growth </span><span class="No-Break"><span class="koboSpan" id="kobo.1687.1">over time.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1688.1">Data partitioning is </span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.1689.1">a key architectural decision that can significantly impact the efficiency and performance of your data processing pipeline. </span><span class="koboSpan" id="kobo.1689.2">By employing appropriate data partitioning strategies, you can ensure that your data is organized in a way that aligns with your querying patterns and maximizes the benefits of your chosen data </span><span class="No-Break"><span class="koboSpan" id="kobo.1690.1">sink technology.</span></span></p>
			<p><span class="koboSpan" id="kobo.1691.1">In the next section, we’re going to put everything we’ve learned in this chapter into practice by describing a real-world case scenario and going through all the logical steps for defining the best strategy associated with data sinks and </span><span class="No-Break"><span class="koboSpan" id="kobo.1692.1">file types.</span></span></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor192"/><span class="koboSpan" id="kobo.1693.1">Designing an online retail data platform</span></h1>
			<p><span class="koboSpan" id="kobo.1694.1">An online retailer </span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.1695.1">wants to create an analytics platform to collect and analyze all the data generated by their e-commerce website. </span><span class="koboSpan" id="kobo.1695.2">This platform aims to provide capabilities that allow for real-time data processing and analytics to improve customer experiences, optimize business operations, and drive strategic decision-making for the online </span><span class="No-Break"><span class="koboSpan" id="kobo.1696.1">retail business.</span></span></p>
			<p><span class="koboSpan" id="kobo.1697.1">After long discussions with the team, we identified four main requirements </span><span class="No-Break"><span class="koboSpan" id="kobo.1698.1">to consider:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1699.1">Handle large volumes of transaction data</span></strong><span class="koboSpan" id="kobo.1700.1">: The platform needs to efficiently ingest and transform large volumes of transaction data. </span><span class="koboSpan" id="kobo.1700.2">This needs to be done by accounting for scalability, high throughput, </span><span class="No-Break"><span class="koboSpan" id="kobo.1701.1">and cost-effectiveness.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1702.1">Provide real-time insights</span></strong><span class="koboSpan" id="kobo.1703.1">: Business analysts require immediate access to real-time insights derived from transaction data. </span><span class="koboSpan" id="kobo.1703.2">The platform should support real-time data</span><a id="_idIndexMarker576"/><span class="koboSpan" id="kobo.1704.1"> processing and analytics to enable </span><span class="No-Break"><span class="koboSpan" id="kobo.1705.1">timely decision-making.</span></span></li>
				<li><span class="koboSpan" id="kobo.1706.1">There’s a need to combine batch and streaming data ingestion to handle both the real-time website data and the batch customer data, which is </span><span class="No-Break"><span class="koboSpan" id="kobo.1707.1">updated slowly.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.1708.1">Use AWS as the cloud provider</span></strong><span class="koboSpan" id="kobo.1709.1">: The choice of the cloud provider (AWS) comes from the fact that the retailer is currently using other AWS services and wants to stick with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1710.1">same provider.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1711.1">Let’s have a quick look at how we can solve </span><span class="No-Break"><span class="koboSpan" id="kobo.1712.1">these requirements:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1713.1">Choose the right data </span><span class="No-Break"><span class="koboSpan" id="kobo.1714.1">sink technology:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1715.1">Thinking process</span></strong><span class="koboSpan" id="kobo.1716.1">: A Lakehouse architecture is an ideal solution for the data platform requirements due to its ability to handle large volumes of data with scalability, high throughput, and cost-effectiveness. </span><span class="koboSpan" id="kobo.1716.2">It leverages distributed storage and compute resources, allowing for efficient data ingestion and transformation. </span><span class="koboSpan" id="kobo.1716.3">Additionally, the architecture supports real-time data processing and analytics, enabling business analysts to access immediate insights from transaction data for timely decision-making. </span><span class="koboSpan" id="kobo.1716.4">By combining batch and streaming data ingestion, the Lakehouse seamlessly integrates real-time website data with batch-updated </span><span class="No-Break"><span class="koboSpan" id="kobo.1717.1">customer data.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1718.1">Choice</span></strong><span class="koboSpan" id="kobo.1719.1">: A Lakehouse solution on AWS is selected for its scalability, cost-effectiveness, and seamless integration with other AWS services. </span><span class="koboSpan" id="kobo.1719.2">AWS is compatible with a </span><span class="No-Break"><span class="koboSpan" id="kobo.1720.1">Lakehouse architecture.</span></span></li></ul></li>
				<li><span class="koboSpan" id="kobo.1721.1">Evaluate and choose the data </span><span class="No-Break"><span class="koboSpan" id="kobo.1722.1">file format:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1723.1">Data characteristics</span></strong><span class="koboSpan" id="kobo.1724.1">: The customer data consists of structured transaction records, including customer IDs, product IDs, purchase amounts, timestamps, and geolocation. </span><span class="koboSpan" id="kobo.1724.2">The streaming data includes customer IDs and other web metrics, such as what each customer is currently browsing on </span><span class="No-Break"><span class="koboSpan" id="kobo.1725.1">the website.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1726.1">Choice</span></strong><span class="koboSpan" id="kobo.1727.1">: Delta file format is selected for its transactional capabilities and ACID compliance. </span><span class="koboSpan" id="kobo.1727.2">It also supports batch and </span><span class="No-Break"><span class="koboSpan" id="kobo.1728.1">streaming workloads.</span></span></li></ul></li>
				<li><span class="koboSpan" id="kobo.1729.1">Implement data</span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.1730.1"> ingestion for batch and </span><span class="No-Break"><span class="koboSpan" id="kobo.1731.1">streaming data:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1732.1">Data ingestion</span></strong><span class="koboSpan" id="kobo.1733.1">: ETL processes are designed to transform incoming transaction data into Delta files. </span><span class="koboSpan" id="kobo.1733.2">Real-time transaction data is streamed from AWS Kinesis for immediate processing and stored as Delta files while batch data coming from different other systems </span><span class="No-Break"><span class="koboSpan" id="kobo.1734.1">is integrated.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1735.1">Partitioning logic</span></strong><span class="koboSpan" id="kobo.1736.1">: Batch and streaming data are being processed and stored in Delta files. </span><span class="koboSpan" id="kobo.1736.2">The streaming data is </span><em class="italic"><span class="koboSpan" id="kobo.1737.1">partitioned by date when written out</span></em><span class="koboSpan" id="kobo.1738.1">. </span><span class="koboSpan" id="kobo.1738.2">Next, transformations and data consolidation happen before it’s stored as the final </span><span class="No-Break"><span class="koboSpan" id="kobo.1739.1">analytical tables.</span></span></li></ul></li>
				<li><span class="koboSpan" id="kobo.1740.1">Define a partitioning strategy for </span><span class="No-Break"><span class="koboSpan" id="kobo.1741.1">analytical tables:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1742.1">Query patterns</span></strong><span class="koboSpan" id="kobo.1743.1">: Analysts often query data based on certain periods of time and some tables based on </span><span class="No-Break"><span class="koboSpan" id="kobo.1744.1">product categories.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1745.1">Choice</span></strong><span class="koboSpan" id="kobo.1746.1">: As we learned in the </span><em class="italic"><span class="koboSpan" id="kobo.1747.1">Considerations for choosing partitioning strategies</span></em><span class="koboSpan" id="kobo.1748.1"> section, we need to take into account the way users are querying the table. </span><span class="koboSpan" id="kobo.1748.2">To get the best read performance out of the queries, time-based and category-based partitioning must be implemented. </span><span class="koboSpan" id="kobo.1748.3">Data is partitioned by date and further partitioned by product category in analytics tables that the users </span><span class="No-Break"><span class="koboSpan" id="kobo.1749.1">query often.</span></span></li></ul></li>
				<li><span class="koboSpan" id="kobo.1750.1">Monitor </span><span class="No-Break"><span class="koboSpan" id="kobo.1751.1">and optimize:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1752.1">Performance monitoring</span></strong><span class="koboSpan" id="kobo.1753.1">: Regularly monitor query performance, streaming throughput, and resource utilization using AWS monitoring and </span><span class="No-Break"><span class="koboSpan" id="kobo.1754.1">logging services</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1755.1">Optimization</span></strong><span class="koboSpan" id="kobo.1756.1">: Continuously optimize both batch and streaming components based on observed performance and changing </span><span class="No-Break"><span class="koboSpan" id="kobo.1757.1">data patterns</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1758.1">Schema evolution</span></strong><span class="koboSpan" id="kobo.1759.1">: Ensure that the Delta schema accommodates streaming data changes and maintains compatibility with existing </span><span class="No-Break"><span class="koboSpan" id="kobo.1760.1">batch data</span></span></li></ul></li>
			</ol>
			<p><span class="koboSpan" id="kobo.1761.1">With this</span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.1762.1"> architecture, the online retail analytics platform gains the capability to process both batch and real-time data in an effective and </span><span class="No-Break"><span class="koboSpan" id="kobo.1763.1">cost-optimized way.</span></span></p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor193"/><span class="koboSpan" id="kobo.1764.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.1765.1">Throughout this chapter, we focused on the components of designing and optimizing data write operations. </span><span class="koboSpan" id="kobo.1765.2">We discussed how to choose the right data sink technology, how file formats significantly impact storage efficiency and query performance, and why it matters to choose the right one for your use case. </span><span class="koboSpan" id="kobo.1765.3">Finally, we discussed why data partitioning is crucial for optimizing query performance and </span><span class="No-Break"><span class="koboSpan" id="kobo.1766.1">resource utilization.</span></span></p>
			<p><span class="koboSpan" id="kobo.1767.1">In the next chapter, we will start transforming the data that’s been written on the data sink to better prepare it for downstream analytics by detecting and handling outliers and </span><span class="No-Break"><span class="koboSpan" id="kobo.1768.1">missing values.</span></span></p>
		</div>
	

		<div id="_idContainer062" class="Content">
			<h1 id="_idParaDest-157" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor194"/><span class="koboSpan" id="kobo.1.1">Part 2: Downstream Data Cleaning –  Consuming Structured Data</span></h1>
			<p><span class="koboSpan" id="kobo.2.1">This part delves into the processes required for cleaning and preparing structured data for analysis, focusing on handling common data challenges that occur in more refined datasets. </span><span class="koboSpan" id="kobo.2.2">It provides practical techniques for managing missing values and outliers, ensuring data consistency through normalization and standardization, and effectively processing categorical features. </span><span class="koboSpan" id="kobo.2.3">Additionally, it introduces specialized methods for working with time series data, a common yet complex data type. </span><span class="koboSpan" id="kobo.2.4">By mastering these downstream cleaning and preparation techniques, readers will be well-equipped to turn structured data into actionable insights for </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">advanced analytics.</span></span></p>
			<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
			<ul>
				<li><a href="B19801_08.xhtml#_idTextAnchor195"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 8</span></em></a><em class="italic"><span class="koboSpan" id="kobo.7.1">, Detecting and Handling Missing Values and Outliers</span></em></li>
				<li><a href="B19801_09.xhtml#_idTextAnchor213"><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 9</span></em></a><em class="italic"><span class="koboSpan" id="kobo.9.1">, Normalization and Standardization</span></em></li>
				<li><a href="B19801_10.xhtml#_idTextAnchor223"><em class="italic"><span class="koboSpan" id="kobo.10.1">Chapter 10</span></em></a><em class="italic"><span class="koboSpan" id="kobo.11.1">, Handling Categorical Features</span></em></li>
				<li><a href="B19801_11.xhtml#_idTextAnchor246"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 11</span></em></a><em class="italic"><span class="koboSpan" id="kobo.13.1">, Consuming Time Series Data</span></em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer063" class="Basic-Graphics-Frame">
			</div>
		</div>
	</body></html>