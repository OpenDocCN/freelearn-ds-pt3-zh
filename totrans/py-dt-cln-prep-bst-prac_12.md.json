["```py\npip install transformers==4.42.4\npip install beautifulsoup4==4.12.3\npip install langchain-text-splitters==0.2.2\npip install tiktoken==0.7.0\npip install langchain==0.2.10\npip install langchain-experimental==0.0.62\npip install langchain-huggingface==0.0.3\npip install presidio_analyzer==2.2.355\npip install presidio_anonymizer==2.2.355\npip install rapidfuzz-3.9.4 thefuzz-0.22.1\npip install stanza==1.8.2\npip install tf-keras-2.17.0\n```", "```py\n    from bs4 import BeautifulSoup\n    from transformers import BertTokenizer\n    ```", "```py\n    reviews = [\n      \"<html>This product is <b>amazing!</b></html>\",\n      \"The product is good, but it could be better!!!\",\n      \"I've never seen such a terrible product. 0/10\",\n      \"The product is AWESOME!!! Highly recommended!\",\n    ]\n    ```", "```py\n    def clean_html_tags(text):\n        soup = BeautifulSoup(text, \"html.parser\")\n        return soup.get_text()\n    ```", "```py\n    def preprocess_text(text):\n        text = clean_html_tags(text)\n        return text\n    preprocessed_reviews = [preprocess_text(review) for review in reviews]\n    ```", "```py\n    - This product is amazing!\n    - The product is good, but it could be better!!!\n    - I've never seen such a terrible product. 0/10\n    - The product is AWESOME!!! Highly recommended!\n    ```", "```py\n    - This product is amazing!\n    - The product is good, but it could be better!!!\n    - I've never seen such a terrible product. 0/10\n    - The product is AWESOME!!! Highly recommended!\n    ```", "```py\n    def standardize_case(text):\n        return text.lower()\n    ```", "```py\n    def preprocess_text(text):\n        text = clean_html_tags(text)\n        text = standardize_case(text)\n        return text\n    ```", "```py\n    for preprocessed_review in preprocessed_reviews:\n        print(f\"- {preprocessed_review}\")\n    ```", "```py\n    - this product is amazing!\n    - the product is good, but it could be better!!!\n    - i've never seen such a terrible product. 0/10\n    - the product is awesome!!! highly recommended!\n    ```", "```py\ndef standardize_case(text):\n    return text.upper()\n```", "```py\n- THIS PRODUCT IS AMAZING!\n- THE PRODUCT IS GOOD, BUT IT COULD BE BETTER!!!\n- I'VE NEVER SEEN SUCH A TERRIBLE PRODUCT. 0/10\n- THE PRODUCT IS AWESOME!!! HIGHLY RECOMMENDED!\n```", "```py\n    - This product is amazing!\n    - The product is good, but it could be better!!!\n    - I've never seen such a terrible product. 0/10\n    - The product is AWESOME!!! Highly recommended!\n    ```", "```py\n    def remove_numbers_and_symbols(text):\n        return ''.join(e for e in text if e.isalpha() or e.isspace())\n    ```", "```py\n    def preprocess_text(text):\n        text = clean_html_tags(text)\n        text = standardize_case(text)\n        text = remove_numbers_and_symbols(text)\n        return text\n    ```", "```py\n    - this product is amazing\n    - the product is good but it could be better\n    - ive never seen such a terrible product\n    - the product is awesome highly recommended\n    ```", "```py\n    text = \"I love this product!!! It's amazing!!!\"\n    ```", "```py\n    replaced_text = text.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n    print(\"Replaced Text:\", replaced_text)\n    ```", "```py\n    I love this product    It s amazing\n    ```", "```py\n    removed_text = \"\".join(char for char in text if char.isalnum() or char.isspace())\n    print(\"Removed Text:\", removed_text)\n    ```", "```py\n    I love this product Its amazing\n    ```", "```py\n    def remove_extra_whitespace(text):\n        return ' '.join(text.split())\n    ```", "```py\n    def preprocess_text(text):\n        text = clean_html_tags(text)\n        text = standardize_case(text)\n        text = remove_numbers_and_symbols(text)\n        text = remove_extra_whitespace(text)\n        return text\n    ```", "```py\n- this productis amazing\n- the product is good but it could be better\n- ive never seen such a terribleproduct\n- the product is awesome highly recommended\n```", "```py\n- this product is amazing\n- the product is good but it could be better\n- ive never seen such a terrible product\n- the product is awesome highly recommended\n```", "```py\n    import pandas as pd\n    from presidio_analyzer import AnalyzerEngine\n    from presidio_anonymizer import AnonymizerEngine\n    from presidio_anonymizer.entities import OperatorConfig\n    ```", "```py\n    data = {\n        'text': [\n            \"Hello, my name is John Doe. My email is john.doe@example.com\",\n            \"Contact Jane Smith at jane.smith@work.com\",\n            \"Call her at 987-654-3210.\",\n            \"This is a test message without PII.\"\n        ]\n    }\n    df = pd.DataFrame(data)\n    ```", "```py\n    analyzer = AnalyzerEngine()\n    anonymizer = AnonymizerEngine()\n    ```", "```py\n    def anonymize_text(text):\n        analyzer_results = analyzer.analyze(text=text, entities=[\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\"], language=\"en\")\n        operators = {\n            \"PERSON\": OperatorConfig(\"mask\", {\"masking_char\": \"*\", \"chars_to_mask\": 4, \"from_end\": True}),\n            \"EMAIL_ADDRESS\": OperatorConfig(\"mask\", {\"masking_char\": \"*\", \"chars_to_mask\": 5, \"from_end\": True}),\n            \"PHONE_NUMBER\": OperatorConfig(\"mask\", {\"masking_char\": \"*\", \"chars_to_mask\": 6, \"from_end\": True})\n        }\n        anonymized_result = anonymizer.anonymize(\n            text=text, analyzer_results=analyzer_results,\n            operators=operators)\n        return anonymized_result.text\n    ```", "```py\n    df['anonymized_text'] = df['text'].apply(anonymize_text)\n    ```", "```py\n    0    Hello, my name is John. My email is john.d...\n    1            Contact Jane S at jane.smith@wor*\n    2                            Call her at 987-65.\n    3                  This is a test message without PII.\n    ```", "```py\n    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n    ```", "```py\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n    ```", "```py\n    text = \"The quokka, a rare marsupial,\"\n    ```", "```py\n    indexed_tokens = tokenizer.encode(text, return_tensors='pt')\n    ```", "```py\n    output_text = model.generate(indexed_tokens, max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n    ```", "```py\n    output_text_decoded = tokenizer.decode(output_text[0], skip_special_tokens=True)\n    ```", "```py\n    The quokka, a rare marsupial, is one of the world's most endangered species.\n    ```", "```py\n    def fix_spelling(text):\n        spell_check = pipeline(\"text2text-generation\", model=\"oliverguhr/spelling-correction-english-base\")\n    ```", "```py\n        corrected = spell_check(text, max_length=2048)[0]['generated_text']\n        return corrected\n    ```", "```py\n    sample_text = \"My name si from Grece.\"\n    corrected_text = fix_spelling(sample_text)\n    Corrected text: My name is from Greece.\n    ```", "```py\n    pip install thefuzz==0.22.1\n    ```", "```py\n    from transformers import pipeline\n    from thefuzz import process, fuzz\n    ```", "```py\n    def fix_spelling(text, threshold=80):\n        spell_check = pipeline(\"text2text-generation\", model=\"oliverguhr/spelling-correction-english-base\")\n    ```", "```py\n        corrected = spell_check(text, max_length=2048)[0]['generated_text']\n    ```", "```py\n        original_words = text.split()\n        corrected_words = corrected.split()\n    ```", "```py\n        common_words = set(['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at'])\n    ```", "```py\n        final_words = []\n        for orig, corr in zip(original_words, corrected_words):\n            if orig.lower() in common_words:\n                final_words.append(orig)\n            else:\n                matches = process.extractOne(orig, [corr], scorer=fuzz.ratio)\n                if matches[1] >= threshold:\n                    final_words.append(matches[0])\n                else:\n                    final_words.append(orig)\n        return ' '.join(final_words)\n    ```", "```py\n    sample_text = \"Lets do a copmarsion of speling mistaks in this sentense.\"\n    corrected_text = fix_spelling(sample_text)\n    ```", "```py\n    Original text: Lets do a copmarsion of speling mistaks in this sentense.\n    Corrected text: Let's do a comparison of speling mistaks in this sentence.\n    ```", "```py\n    reviews = [\n        \"This smartphone has an excellent camera. The photos are sharp and the colors are vibrant. Overall, very satisfied with my purchase.\",\n        \"I was disappointed with the laptop's performance. It frequently lags and the battery life is shorter than expected.\",\n        \"The blender works great for making smoothies. It's powerful and easy to clean. Definitely worth the price.\",\n        \"Customer support was unresponsive. I had to wait a long time for a reply, and my issue was not resolved satisfactorily.\",\n        \"The book is a fascinating read. The storyline is engaging and the characters are well-developed. Highly recommend to all readers.\"\n    ]\n    ```", "```py\n    from langchain_text_splitters import TokenTextSplitter\n    ```", "```py\n    text_splitter = TokenTextSplitter(chunk_size=50, chunk_overlap=0)\n    ```", "```py\n    text_block = \" \".join(reviews)\n    ```", "```py\n    chunks = text_splitter.split_text(text_block)\n    ```", "```py\n    Chunk 1:\n    This smartphone has an excellent camera. The photos are sharp and the colors are vibrant. Overall, very satisfied with my purchase. I was disappointed with the laptop's performance. It frequently lags and the battery life is shorter than expected. The blender works\n    Chunk 2:\n    great for making smoothies. It's powerful and easy to clean. Definitely worth the price. Customer support was unresponsive. I had to wait a long time for a reply, and my issue was not resolved satisfactorily. The book is a\n    Chunk 3:\n    fascinating read. The storyline is engaging and the characters are well-developed. Highly recommend to all readers.\n    ```", "```py\nchunk_sizes = [20, 70, 150]\nfor size in chunk_sizes:\n    print(f\"Chunk Size: {size}\")\n    text_splitter = TokenTextSplitter(chunk_size=size, chunk_overlap=0)\n    chunks = text_splitter.split_text(text_block)\n    for i, chunk in enumerate(chunks):\n        print(f\"Chunk {i + 1}:\")\n        print(chunk)\n        print(\"\\n\")\n```", "```py\nOriginal text:\nOne of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n```", "```py\nChunk 1: \"One of the most important\"\nChunk 2: \"important things I didn't understand\"\nChunk 3: \"understand about the world when\"\nChunk 4: \"when I was a child\"\nChunk 5: \"child is the degree to\"\nChunk 6: \"to which the returns for\"\nChunk 7: \"for performance are superlinear.\"\n```", "```py\n    text_splitter = RecursiveCharacterTextSplitter(\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n        chunk_size=200,\n        chunk_overlap=0,\n        length_function=len\n        )\n    ```", "```py\n    chunks = text_splitter.split_text(text_block)\n    ```", "```py\n    Chunk 1:\n    This smartphone has an excellent camera. The photos are sharp and the colors are vibrant. Overall, very satisfied with my purchase. I was disappointed with the laptop's performance. It frequently lags\n    ```", "```py\n    Chunk 2:\n    and the battery life is shorter than expected. The blender works great for making smoothies. It's powerful and easy to clean. Definitely worth the price. Customer support was unresponsive. I had to\n    ```", "```py\n    Chunk 3:\n    wait a long time for a reply, and my issue was not resolved satisfactorily. The book is a fascinating read. The storyline is engaging and the characters are well-developed. Highly recommend to all\n    ```", "```py\n    Chunk 4:\n    Readers.\n    ```", "```py\n    text_splitter = SemanticChunker(HuggingFaceEmbeddings())\n    ```", "```py\n    docs = text_splitter.create_documents([text_block])\n    ```", "```py\n    Chunk 1:\n    This smartphone has an excellent camera. The photos are sharp and the colors are vibrant. Overall, very satisfied with my purchase. I was disappointed with the laptop's performance. It frequently lags and the battery life is shorter than expected. The blender works great for making smoothies. It's powerful and easy to clean.\n    Chunk 2:\n    Definitely worth the price. Customer support was unresponsive. I had to wait a long time for a reply, and my issue was not resolved satisfactorily. The book is a fascinating read. The storyline is engaging and the characters are well-developed. Highly recommend to all readers.\n    ```", "```py\ntext_splitter = SemanticChunker(\n    embeddings=embedding_model,\n    buffer_size=200,\n    add_start_index=True,\n    breakpoint_threshold_type='percentile',\n    breakpoint_threshold_amount=0.9,\n    number_of_chunks=4,\n    sentence_split_regex=r'\\.|\\n|\\s'\n)\n```", "```py\n    nltk.download('punkt')\n    ```", "```py\n    text = \"The quick brown fox jumps over the lazy dog. It's unaffordable!\"\n    ```", "```py\n    word_tokens = word_tokenize(text)\n    ```", "```py\n    Tokens:\n    ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'unaffordable', '!']\n    ```", "```py\n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n    ```", "```py\n    text = \"Tokenization in medical texts can include words like hyperlipidemia.\"\n    ```", "```py\n    tokens = tokenizer.tokenize(text)\n    ```", "```py\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n    ```", "```py\n    Tokens: ['Token', 'ization', 'Ġin', 'Ġmedical', 'Ġtexts', 'Ġcan', 'Ġinclude', 'Ġwords', 'Ġlike', 'Ġhyper', 'lip', 'idem', 'ia', '.']\n    Input IDs: [21920, 3666, 287, 1400, 1562, 649, 4551, 3545, 588, 20424, 3182, 1069, 257, 13]\n    ```", "```py\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    ```", "```py\n    text = \"Tokenization in medical texts can include words like hyperlipidemia.\"\n    ```", "```py\n    tokens = tokenizer.tokenize(text)\n    ```", "```py\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n    Tokens:\n    ['token', '##ization', 'in', 'medical', 'texts', 'can', 'include', 'words', 'like', 'hyper', '##lip', '##idem', '##ia']\n    Input IDs:\n    [19204, 10859, 1999, 2966, 4524, 2064, 2421, 2540, 2066, 15088, 17750, 28285, 3676]\n    ```", "```py\n    stanza.download('en', package='mimic', processors='tokenize')\n    nlp = stanza.Pipeline('en', package='mimic', processors='tokenize')\n    ```", "```py\n    standard_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    ```", "```py\n    standard_tokenizer.pad_token = standard_tokenizer.eos_token\n    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n    ```", "```py\n    model.config.pad_token_id = model.config.eos_token_id\n    ```", "```py\n    corpus = [\n      \"The patient suffered a myocardial infarction.\",\n      \"Early detection of heart attack is crucial.\",\n      \"Treatment for myocardial infarction includes medication.\",\n      \"Patients with heart conditions require regular check-ups.\",\n      \"Myocardial infarction can lead to severe complications.\"\n    ]\n    ```", "```py\n    def stanza_tokenize(text):\n        doc = nlp(text)\n        tokens = [word.text for sent in doc.sentences for word in sent.words]\n        return tokens\n    ```", "```py\n    def calculate_oov_and_compression(corpus, tokenizer):\n        oov_count = 0\n        total_tokens = 0\n        all_tokens = []\n        for sentence in corpus:\n            tokens = tokenizer.tokenize(sentence) if hasattr(tokenizer, 'tokenize') else stanza_tokenize(sentence)\n            all_tokens.extend(tokens)\n            total_tokens += len(tokens)\n            oov_count += tokens.count(tokenizer.oov_token) if hasattr(tokenizer, 'oov_token') else 0\n        oov_rate = (oov_count / total_tokens) * 100 if total_tokens > 0 else 0\n        avg_tokens_per_sentence = total_tokens / len(corpus)\n    return oov_rate, avg_tokens_per_sentence, all_tokens\n    ```", "```py\n    def analyze_token_utilization(tokens):\n        token_counts = Counter(tokens)\n        total_tokens = len(tokens)\n        utilization = {token: count / total_tokens for token, count in token_counts.items()}\n        return utilization\n    ```", "```py\n    def calculate_perplexity(tokenizer, model, text):\n        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n        with torch.no_grad():\n            outputs = model(inputs, labels=inputs[\"input_ids\"])\n        return torch.exp(outputs.loss).item()\n    ```", "```py\n    for tokenizer_name, tokenizer in [(\"Standard GPT-2\", standard_tokenizer), (\"Stanza Medical\", stanza_tokenize)]:\n        oov_rate, avg_tokens, all_tokens = calculate_oov_and_compression(corpus, tokenizer)\n        utilization = analyze_token_utilization(all_tokens)\n        print(f\"\\n{tokenizer_name} Tokenizer:\")\n        print(f\"OOV Rate: {oov_rate:.2f}%\")\n        print(f\"Average Tokens per Sentence: {avg_tokens:.2f}\")\n        print(\"Top 5 Most Used Tokens:\")\n        for token, freq in sorted(utilization.items(), key=lambda x: x[1], reverse=True)[:5]:\n            print(f\" {token}: {freq:.2%}\")\n    ```", "```py\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    model = BertModel.from_pretrained(\"bert-base-uncased\")\n    ```", "```py\n    input_text = \"BERT embeddings capture contextual information.\"\n    inputs= tokenizer(input_text, return_tensors=\"pt\")\n    ```", "```py\n    with torch.no_grad():\n        outputs = model(inputs)\n    ```", "```py\n    print(\"Shape of the embeddings tensor:\", last_hidden_states.shape)\n    Shape of the embeddings tensor: torch.Size([1, 14, 768])\n    ```", "```py\nCLS token embedding: [ 0.23148441 -0.32737488 ...  0.02315655]\n```", "```py\nFirst word embedding: [ 0.00773875  0.24699381 ... -0.09120814]\n```", "```py\n    model_name = \"BAAI/bge-small-en\"\n    model_kwargs = {\"device\": \"cpu\"}\n    encode_kwargs = {\"normalize_embeddings\": True}\n    ```", "```py\n    bge_embeddings = HuggingFaceBgeEmbeddings(\n        model_name=model_name,\n        model_kwargs=model_kwargs,\n        encode_kwargs=encode_kwargs\n    )\n    ```", "```py\n    sentences = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"I love machine learning and natural language processing.\"\n    ]\n    ```", "```py\n    embeddings = [bge_embeddings.embed_query(sentence) for sentence in sentences]\n    ```", "```py\n    [-0.07455343008041382, -0.004580824635922909, 0.021685084328055382, 0.06458176672458649, 0.020278634503483772]...\n    Length of embedding: 384\n    Embedding for sentence 2: [-0.025911744683980942, 0.0050039878115057945, -0.011821565218269825, -0.020849423483014107, 0.06114110350608826]...\n    ```", "```py\n    model = SentenceTransformer('thenlper/gte-base')\n    ```", "```py\n    texts = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"I love machine learning and natural language processing.\",\n        \"Embeddings are useful for many NLP tasks.\"\n    ]\n    ```", "```py\n    embeddings = model.encode(texts\n    ```", "```py\n    print(f\"Shape of embeddings: {embeddings.shape}\")\n    Shape of embeddings: (3, 768)\n    ```", "```py\n    [-0.02376037 -0.04635307  0.02570779  0.01606994  0.05594607]\n    ```"]