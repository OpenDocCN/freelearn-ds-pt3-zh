- en: A Brief Tour of Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will take you on a whirlwind tour of machine learning, focusing
    on using the `pandas` library as a tool to preprocess the data used by machine
    learning programs. It will also introduce you to the `scikit-learn` library, which
    is the most popular machine learning toolkit in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will illustrate machine learning techniques by applying
    them to a well-known problem about classifying which passengers survived the Titanic
    disaster at the turn of the last century. The various topics addressed in this
    chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The role of pandas in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing `scikit-learn`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to machine learning concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying machine learning—Kaggle Titanic competition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis and preprocessing using pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A naïve approach to the Titanic problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `scikit-learn` ML classifier interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of pandas in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The library we will be considering for machine learning is called `scikit-learn`.
    The `scikit-learn` Python library is an extensive library of machine learning
    algorithms that can be used to create adaptive programs that learn from data inputs.
  prefs: []
  type: TYPE_NORMAL
- en: However, before this data can be used by `scikit-learn`, it must undergo some
    preprocessing. This is where pandas comes in. pandas can be used to preprocess
    and filter data before passing it to the algorithm implemented in `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming sections, we will see how `scikit-learn` can be used for machine
    learning. So, as the first step, we will learn how to install it on our machines.
  prefs: []
  type: TYPE_NORMAL
- en: Installation of scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As was mentioned in Chapter 2, *Installation of Python and pandas from Third-Party
    Vendors*, the easiest way to install pandas and its accompanying libraries is
    to use a third-party distribution such as Anaconda and be done with it. Installing
    `scikit-learn` should be no different. I will briefly highlight the steps for
    installation on various platforms and third-party distributions, starting with
    Anaconda. The `scikit-learn` library requires the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 2.6.x or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy 1.6.1 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SciPy 0.9 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming that you have already installed pandas as described in [Chapter 2](34a6977b-f807-4ee6-9da8-034d9216bb49.xhtml),
    *Installation of pandas and Supporting Software*, these dependencies should already
    be in place. The various options to install `scikit-learn` on different platforms
    are discussed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Installing via Anaconda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can install `scikit-learn` on Anaconda by running the `conda` Python package
    manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Installing on Unix (Linux/macOS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For Unix, it is best to install from the source (C compiler is required). Assuming
    that pandas and NumPy are already installed and the required dependent libraries
    are already in place, you can install `scikit-learn` via Git by running the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The pandas library can also be installed on Unix by using `pip` from `PyPi`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Installing on Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install on Windows, you can open a console and run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For more in-depth information on installation, you can take a look at the official
    `scikit-learn` documentation at [http://scikit-learn.org/stable/install.html](http://scikit-learn.org/stable/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: You can also take a look at the README file for the `scikit-learn` Git repository
    at [https://github.com/scikit-learn/scikit-learn/blob/master/README.rst](https://github.com/scikit-learn/scikit-learn/blob/master/README.rst).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is the art of creating software programs that learn from data.
    More formally, it can be defined as the practice of building adaptive programs
    that use tunable parameters to improve predictive performance. It is a sub-field
    of artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: We can separate machine learning programs based on the type of problems they
    are trying to solve. These problems are appropriately called learning problems.
    The two categories of these problems, broadly speaking, are referred to as supervised
    and unsupervised learning problems. Furthermore, there are some hybrid problems
    that have aspects that involve both categories—supervised and unsupervised.
  prefs: []
  type: TYPE_NORMAL
- en: The input to a learning problem consists of a dataset of *n* rows. Each row
    represents a sample and may involve one or more fields referred to as attributes
    or features. A dataset can be canonically described as consisting of *n* samples,
    each consisting of *n* features.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised versus unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For supervised learning problems, the input to a learning problem is a dataset
    consisting of *labeled* data. By this, we mean that we have outputs whose values
    are known. The learning program is fed with input samples and their corresponding
    outputs and its goal is to decipher the relationship between them. Such input
    is known as labeled data. Supervised learning problems include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: The learned attribute is categorical (nominal) or discrete'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: The learned attribute is numeric/continuous'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In unsupervised learning or data mining, the learning program is fed with inputs
    but does without the corresponding outputs. This input data is referred to as
    unlabeled data. The goal of machine learning in such cases is to learn or decipher
    the hidden label. Such problems include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Illustration using document classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common usage of machine learning techniques is in the area of document classification.
    The two main categories of machine learning can be applied to this problem—supervised
    and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each document in the input collection is assigned to a category; that is, a
    label. The learning program/algorithm uses the input collection of documents to
    learn how to make predictions for another set of documents with no labels. This
    method is known as **classification**.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The documents in the input collection are not assigned to categories; hence,
    they are unlabeled. The learning program takes this as input and tries to *cluster*
    or discover groups of related or similar documents. This method is known as **clustering**.
  prefs: []
  type: TYPE_NORMAL
- en: How machine learning systems learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning systems utilize what is known as a classifier to learn from
    data. A *classifier* is an interface that takes a matrix of what is known as *feature
    values* and produces an output vector, also known as the class. These feature
    values may be discrete or continuously valued. There are three core components
    of classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Representation**: What type of classifier is it?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation**: How good is the classifier?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**: How can you search among the alternatives?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application of machine learning – Kaggle Titanic competition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate how we can use pandas to assist us at the start of our machine
    learning journey, we will apply it to a classic problem, which is hosted on the
    Kaggle website ([http://www.kaggle.com](http://www.kaggle.com)). **Kaggle** is
    a competition platform for machine learning problems. The idea behind Kaggle is
    to enable companies that are interested in solving predictive analytics problems
    with their data to post their data on Kaggle and invite data scientists to come
    up with proposed solutions to their problems. A competition can be ongoing over
    a period of time, and the rankings of the competitors are posted on a leaderboard.
    At the close of the competition, the top-ranked competitors receive cash prizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classic problem that we will study to illustrate the use of pandas for
    machine learning with `scikit-learn` is the *Titanic: Machine Learning from Disaster*
    problem hosted on Kaggle as their classic introductory machine learning problem.
    The dataset involved in the problem is a raw dataset. Hence, pandas is very useful
    in the preprocessing and cleansing of the data before it is submitted as input
    to the machine learning algorithm implemented in `scikit-learn`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Titanic: Machine Learning from Disaster problem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset for the Titanic consists of the passenger manifest for the doomed
    trip, along with various features and an indicator variable telling whether the
    passenger survived the sinking of the ship or not. The essence of the problem
    is to be able to predict, given a passenger and his/her associated features, whether
    this passenger survived the sinking of the Titanic or not. The features are as
    follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data consists of two datasets: one training dataset and one test dataset.
    The training dataset consists of 891 passenger cases, and the test dataset consists
    of 491 passenger cases.'
  prefs: []
  type: TYPE_NORMAL
- en: The training dataset also consists of 11 variables, of which 10 are features
    and 1 dependent/indicator variable, `Survived`, which indicated whether the passenger
    survived the disaster or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature variables are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: PassengerID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cabin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pclass (passenger class)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parch (number of parents and children)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Age
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sibsp (number of siblings)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embarked
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can make use of pandas to help us to preprocess data in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning and the categorization of some variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exclusion of unnecessary features that obviously have no bearing on the
    survivability of the passenger; for example, name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are various algorithms that we can use to tackle this problem. They are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problem of overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Overfitting** is a well-known problem in machine learning, whereby the program
    memorizes the specific data that it is fed as input, leading to perfect results
    on the training data and abysmal results on the test data.'
  prefs: []
  type: TYPE_NORMAL
- en: To prevent overfitting, the tenfold cross-validation technique can be used to
    introduce variability in the data during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and preprocessing using pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will utilize pandas to do some analysis and preprocessing
    of the data before submitting it as input to `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start our preprocessing of the data, let's read in the training dataset and
    examine what it looks like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we read the training dataset into a pandas DataFrame and display the
    first rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0f654ad-9bf5-41ce-a31f-ef012a4e523f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, we can see the various features: PassengerId, Survived, PClass, Name,
    Sex, Age, Sibsp, Parch, Ticket, Fare, Cabin, and Embarked. One question that springs
    to mind immediately is this: which of the features are likely to influence whether
    a passenger survived or not?'
  prefs: []
  type: TYPE_NORMAL
- en: It should seem obvious that PassengerID, Ticket Code, and Name should not be
    influencers on survivability since they're *identifier* variables. We will skip
    these in our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One issue that we have to deal with in datasets for machine learning is how
    to handle missing values in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Let's visually identify where we have missing values in our feature set.
  prefs: []
  type: TYPE_NORMAL
- en: 'For that, we can make use of an equivalent of the `missmap` function in R,
    written by Tom Augspurger. The next screenshot shows how much data is missing
    for the various features in an intuitively appealing manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f9de889-cbba-4253-b9bc-50fad33d7ff9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For more information and the code used to generate this data, see the following:
    [http://tomaugspurger.github.io/blog/2014/02/22/Visualizing%20Missing%20Data/](http://tomaugspurger.github.io/blog/2014/02/22/Visualizing%20Missing%20Data/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also calculate how much data is missing for each of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Hence, we can see that most of the `Cabin` data is missing (77%), while around
    20% of the `Age` data is missing. We then decide to drop the `Cabin` data from
    our learning feature set as the data is too sparse to be of much use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do a further breakdown of the various features that we would like to
    examine. In the case of categorical/discrete features, we use bar plots; for continuous
    valued features, we use histograms. The code to generate the charts is as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f90ae22d-a5f1-448a-85d8-0979f1254429.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the data and illustration in the preceding screenshot, we can observe
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: About twice as many passengers perished than survived (62% versus 38%).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There were about twice as many male passengers as female passengers (65% versus
    35%).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There were about 20% more passengers in the third class versus the first and
    second together (55% versus 45%).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most passengers were solo; that is, had no children, parents, siblings, or spouse
    on board.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These observations might lead us to dig deeper and investigate whether there
    is some correlation between the chances of survival and gender and fare class,
    particularly if we take into account the fact that the Titanic had a women-and-children-first
    policy and the fact that the Titanic was carrying fewer lifeboats (20) than it
    was designed to (32).
  prefs: []
  type: TYPE_NORMAL
- en: 'In light of this, let''s further examine the relationships between survival
    and some of these features. We''ll start with gender:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender** | **Survived** | **Perished** | **Survival Rate** |'
  prefs: []
  type: TYPE_TB
- en: '| Men | 109 | 468 | 18.89 |'
  prefs: []
  type: TYPE_TB
- en: '| Women | 233 | 81 | 74.2 |'
  prefs: []
  type: TYPE_TB
- en: 'We''ll now illustrate this data in a bar chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following bar graph diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23a1457e-6572-4ee0-9e3f-276d3a8b03ad.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding diagram, we can see that the majority of the women survived
    (74%), while most of the men perished (only 19% survived).
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to the conclusion that the gender of the passenger may be a contributing
    factor to whether a passenger survived or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at passenger class. First, we generate the survived and perished
    data for each of the three passenger classes, as well as survival rates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we show them in a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Survived** | **Perished** | **Survival Rate** |'
  prefs: []
  type: TYPE_TB
- en: '| First Class | 136 | 80 | 62.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Second Class | 87 | 97 | 47.28 |'
  prefs: []
  type: TYPE_TB
- en: '| Third Class | 119 | 372 | 24.24 |'
  prefs: []
  type: TYPE_TB
- en: 'We can then plot the data by using `matplotlib` in a similar manner to that
    for the survivor count by gender described earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following bar plot diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2771a28-dae4-414b-bfe0-e48e2e22cb91.png)'
  prefs: []
  type: TYPE_IMG
- en: It seems clear from the preceding data and diagram that the higher the passenger
    fare class, the greater the passenger's chances of survival.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that both gender and fare class seem to influence the chances of a passenger''s
    survival, let''s see what happens when we combine these two features and plot
    a combination of both. For this, we will use the `crosstab` function in pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now display this data using `matplotlib`. First, let''s do some re-labeling
    for display purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we plot the passenger data by using the `plot` function of a pandas `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2f1447b0-5947-4214-af44-5a4c924e9e20.png)'
  prefs: []
  type: TYPE_IMG
- en: A naive approach to the Titanic problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our first attempt at classifying the Titanic data is to use a naive, yet very
    intuitive, approach. This approach involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a set of features, *S*, that influence whether a person survived or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each possible combination of features, use the training data to indicate
    whether the majority of cases survived or not. This can be evaluated in what is
    known as a survival matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each test example that we wish to predict survival, look up the combination
    of features that corresponds to the values of its features and assign its predicted
    value to the survival value in the survival table. This approach is a naive K-nearest
    neighbor approach.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Based on what we have seen earlier in our analysis, three features seem to
    have the most influence on the survival rate:'
  prefs: []
  type: TYPE_NORMAL
- en: Passenger class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gender
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passenger fare (bucketed)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We include passenger fare as it is related to passenger class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The survival table looks something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To see how we use this table, let''s take a look at a snippet of our test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For passenger `892`, we see that he is male, his ticket price was 7.8292, and
    he traveled in the third class. Hence, the key for the survival table lookup for
    this passenger is *{Sex='male', Pclass=3, PriceBucket=0 (since 7.8292 falls in
    bucket 0)}*. If we look up the survival value corresponding to this key in our
    survival table (row 17), we see that the value is `0`, that is, perished; this
    is the value that we will predict.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for passenger `893`, we have *key={Sex='female', Pclass=3, PriceBucket=0}*.
    This corresponds to row 16, and hence, we will predict `1`, that is, survived,
    and her predicted survival is `1`, that is, survived.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, our results look like the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The source of this information is at [http://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/](http://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/).
  prefs: []
  type: TYPE_NORMAL
- en: Using the survival table approach outlined earlier, we can achieve an accuracy
    of 0.77990 on Kaggle ([http://www.kaggle.com](http://www.kaggle.com)). The survival
    table approach, while intuitive, is a very basic approach that represents only
    the tip of the iceberg of possibilities in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will take a whirlwind tour of various machine
    learning algorithms that will help you, the reader, to get a feel for what is
    available in the machine learning universe.
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn ML/classifier interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll be diving into the basic principles of machine learning and demonstrate
    the use of these principles via the `scikit-learn` basic API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scikit-learn` library has an estimator interface. We illustrate it by
    using a linear regression model. For example, consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The estimator interface is instantiated to create a model, which is a linear
    regression model in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we specify `normalize=True`, indicating that the *x*-values will be normalized
    before regression. **Hyperparameters** (estimator parameters) are passed on as
    arguments in the model creation. This is an example of creating a model with tunable
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The estimated parameters are obtained from the data when the data is fitted
    with an estimator. Let''s first create some sample training data that is normally
    distributed about `y = x/2`. We first generate our `x` and `y` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`sklearn` takes a 2D array of `num_samples × num_features` as input, so we
    convert our `x` data into a 2D array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have 500 samples and 1 feature, `x`. We now train/fit the
    model and display the slope (coefficient) and the intercept of the regression
    line, which is the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/533274a2-3d1e-4c27-8033-cfaf94cf0390.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To summarize the basic use of the estimator interface, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define your model: `LinearRegression`, `SupportVectorMachine`, `DecisionTrees`,
    and so on. You can specify the required hyperparameters in this step; for example,
    `normalize=True`, as specified earlier.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model has been defined, you can train your model on your data by calling
    the `fit(..)` method on the model defined in the previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have fit the model, we can call the `predict(..)` method on test data
    to make predictions or estimations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of a supervised learning problem, the `predict(X)` method is given
    unlabeled observations, `X`, and returns predicted labels, `y`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For extra information, please see the following: [http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will take a brief tour of some well-known supervised learning algorithms
    and see how we can apply them to the Titanic survival prediction problem described
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a model using Patsy for scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start our tour of the machine learning algorithms, we need to know
    a little bit about the `Patsy` library. We will make use of `Patsy` to design
    features that will be used in conjunction with `scikit-learn`. `Patsy` is a package
    for creating what is known as design matrices. These design matrices are transformations
    of the features in our input data. The transformations are specified by expressions
    known as formulas, which correspond to a specification of what features we wish
    the machine learning program to utilize in learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple example of this is as follows: suppose that we wish to linearly regress
    a variable, y, against some other variables—`x`, `a`, and `b`—and the interaction
    between `a` and `b`; then, we can specify the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding line of code, the formula is specified by the following expression:
    `y ~ x + a + b + a:b`.'
  prefs: []
  type: TYPE_NORMAL
- en: For further information, look at [http://patsy.readthedocs.org/en/latest/overview.html](http://patsy.readthedocs.org/en/latest/overview.html).
  prefs: []
  type: TYPE_NORMAL
- en: General boilerplate code explanation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce boilerplate code for the implementation of
    the following algorithms by using `Patsy` and `scikit-learn`. The reason for doing
    this is that most of the code for the following algorithms is repeatable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, the workings of the algorithms will be described
    together with the code specific to each algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s make sure that we''re in the correct folder by using the following
    command line. Assuming that the working directory is located at `~/devel/Titanic`,
    we have the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we import the required packages and read in our training and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we specify the formulas we would like to submit to `Patsy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define a function that helps us to handle missing values. The following
    function finds the cells within the DataFrame that have null values, obtains the
    set of similar passengers, and sets the null value to the mean value of that feature
    for the set of similar passengers. Similar passengers are defined as those having
    the same gender and passenger class as the passenger with the null feature value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create filled versions of our training and test DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our test DataFrame is what the fitted `scikit-learn` model will generate predictions
    on to produce output that will be submitted to Kaggle for evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here is the actual implementation of the call to `scikit-learn` to learn from
    the training data by fitting a model and then generate predictions on the test
    dataset. Note that even though this is boilerplate code, for the purpose of illustration,
    an actual call is made to a specific algorithm—in this case, `DecisionTreeClassifier`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output data is written to files with descriptive names, for example, `csv/dt_PClass_Sex_Age_Sibsp_Parch_1.csv`
    and `csv/dt_PClass_Sex_Fare_1.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code follows a standard recipe, and the summary is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Read in the training and test datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill in any missing values for the features we wish to consider in both datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define formulas for the various feature combinations we wish to generate machine
    learning models for in `Patsy`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each formula, perform the following set of steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Call `Patsy` to create design matrices for our training feature set and
    training label set (designated by `X_train` and `y_train`).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Instantiate the appropriate `scikit-learn` classifier. In this case, we
    use `DecisionTreeClassifier`.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Fit the model by calling the `fit(..)` method.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Make a call to `Patsy` to create a design matrix (`X_test`) for our predicted
    output via a call to `patsy.dmatrix(..)`.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Predict on the `X_test` design matrix, and save the results in the variable
    predicted.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Write our predictions to an output file, which will be submitted to Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will consider the following supervised learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In logistic regression, we attempt to predict the outcome of a categorical,
    that is, discrete-valued dependent variable based on one or more input predictor
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression can be thought of as the equivalent of applying linear
    regression but to discrete or categorical variables. However, in the case of binary
    logistic regression (which applies to the Titanic problem), the function to which
    we''re trying to fit is not a linear one as we''re trying to predict an outcome
    that can take only two values—0 and 1\. Using a linear function for our regression
    doesn''t make sense as the output cannot take values between 0 and 1\. Ideally,
    what we need to model for the regression of a binary valued output is some sort
    of step function for values 0 and 1\. However, such a function is not well-defined
    and not differentiable, so an approximation with nicer properties was defined:
    the logistic function. The logistic function takes values between 0 and 1 but
    is skewed toward the extreme values of 0 and 1 and can be used as a good approximation
    for the regression of categorical variables. The formal definition of the logistic
    regression function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*          f(x) = 1/((1+e^(-ax))*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is a good illustration as to why the logistic function
    is suitable for binary logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6c93ebc-7c4d-4a2c-991a-8dc9f77c1a32.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that as we increase the value of our parameter, *a*, we get closer
    to taking on the 0 to 1 values and to the step function we wish to model. A simple
    application of the preceding function would be to set the output value to 0, if
    *f(x) <0.5*, and 1 if not.
  prefs: []
  type: TYPE_NORMAL
- en: The code for plotting the function is included in `plot_logistic.py`.
  prefs: []
  type: TYPE_NORMAL
- en: A more detailed examination of logistic regression may be found at [http://en.wikipedia.org/wiki/Logit](http://en.wikipedia.org/wiki/Logit)
    and [http://logisticregressionanalysis.com/86-what-is-logistic-regression](http://logisticregressionanalysis.com/86-what-is-logistic-regression).
  prefs: []
  type: TYPE_NORMAL
- en: In applying logistic regression to the Titanic problem, we wish to predict a
    binary outcome, that is, whether a passenger survived or not.
  prefs: []
  type: TYPE_NORMAL
- en: We adapted the boilerplate code to use the `sklearn.linear_model.LogisticRegression`
    class of `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon submitting our data to Kaggle, the following results were obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Formula** | **Kaggle Score** |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Fare | 0.76077 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) | 0.76555 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Sex) | 0.76555 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Age + SibSp + Parch | 0.74641 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked) | 0.75598 |'
  prefs: []
  type: TYPE_TB
- en: The code implementing logistic regression can be found in the `run_logistic_regression_titanic.py`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Support Vector Machine** (**SVM**) is a powerful supervised learning algorithm
    used for classification and regression. It is a discriminative classifier—it draws
    a boundary between clusters or classifications of data, so new points can be classified
    based on the cluster that they fall into.
  prefs: []
  type: TYPE_NORMAL
- en: SVMs do not just find a boundary line; they also try to determine margins for
    the boundary on either side. The SVM algorithm tries to find the boundary with
    the largest possible margin around it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Support vectors are points that define the largest margin around the boundary—remove
    these points, and possibly, a larger margin can be found. Hence the name, support,
    as they support the margin around the boundary line. The support vectors matter.
    This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/303f105c-1f90-43c1-8d09-accdbeec3d20.png)'
  prefs: []
  type: TYPE_IMG
- en: For more information on this, refer to [http://winfwiki.wi-fom.de/images/c/cf/Support_vector_2.png](http://winfwiki.wi-fom.de/images/c/cf/Support_vector_2.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the SVM algorithm for classification, we specify one of the following
    three kernels: linear, poly, and **rbf** (also known as **radial basis functions**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we import the **Support Vector Classifier** (**SVC**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We then instantiate an SVM classifier, fit the model, and predict the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon submitting our data to Kaggle, the following results were obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Formula** | **Kernel Type** | **Kaggle Score** |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Fare | poly | 0.71292 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) | poly | 0.76555 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Sex) | poly | 0.76555 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Age + SibSp + Parch | poly | 0.75598 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Age + Parch + C(Embarked) | poly | 0.77512 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(embarked) | poly | 0.79426 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked) | rbf | 0.7512 |'
  prefs: []
  type: TYPE_TB
- en: 'The code can be seen in its entirety in the following file: `run_svm_titanic.py`.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we see that the SVM with a kernel type of poly (polynomial) and the combination
    of the `Pclass`, `Sex`, `Age`, `Sibsp`, and `Parch` features produces the best
    results when submitted to Kaggle. Surprisingly, it seems as if the embarkation
    point (**Embarked**) and whether the passenger traveled alone or with family members
    (**Sibsp + Parch**) do have a material effect on a passenger's chances of survival.
  prefs: []
  type: TYPE_NORMAL
- en: The latter effect was probably due to the women-and-children-first policy on
    the Titanic.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic idea behind decision trees is to use the training dataset to create
    a tree of decisions to make a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: It recursively splits the training dataset into subsets based on the value of
    a single feature. Each split corresponds to a node in the decision tree. The splitting
    process is continued until every subset is pure; that is, all elements belong
    to a single class. This always works except in cases where there are duplicate
    training examples that fall into different classes. In this case, the majority
    class wins.
  prefs: []
  type: TYPE_NORMAL
- en: The end result is a ruleset for making predictions on the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees encode a sequence of binary choices in a process that mimics
    how a human might classify things, but decide which question is most useful at
    each step by using the information criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this would be if you wished to determine whether animal `x` is
    a mammal, fish, or reptile; in this case, we would ask the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates a decision tree that looks similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e758343-82d7-41e7-89ab-fb37b10cd099.png)'
  prefs: []
  type: TYPE_IMG
- en: Refer to the following link for more information: [https://labs.opendns.com/wp-content/uploads/2013/09/animals.gif](https://labs.opendns.com/wp-content/uploads/2013/09/animals.gif).
  prefs: []
  type: TYPE_NORMAL
- en: The binary splitting of questions at each node is the essence of a decision
    tree algorithm. A major drawback of decision trees is that they can *overfit*
    the data. They are so flexible that, given a large depth, they can memorize the
    inputs, and this results in poor results when they are used to classify unseen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The way to fix this is to use multiple decision trees, and this is known as
    using an ensemble estimator. An example of an ensemble estimator is the random
    forest algorithm, which we will address next.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a decision tree in `scikit-learn`, we import the `tree` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We then instantiate an SVM classifier, fit the model, and predict the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon submitting our data to Kaggle, the following results are obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Formula** | **Kaggle Score** |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Fare | 0.77033 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) | 0.76555 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Sex) | 0.76555 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Age + SibSp + Parch | 0.76555 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Age + Parch +C(Embarked) | 0.78947 |'
  prefs: []
  type: TYPE_TB
- en: '| C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked) | 0.79426 |'
  prefs: []
  type: TYPE_TB
- en: Random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forest is an example of a non-parametric model, as are decision trees.
    Random forests are based on decision trees. The decision boundary is learned from
    the data itself. It doesn't have to be a line or a polynomial or radial basis
    function. The random forest model builds upon the decision tree concept by producing
    a large number of, or a forest of, decision trees. It takes a random sample of
    the data and identifies a set of features to grow each decision tree. The error
    rate of the model is compared across sets of decision trees to find the set of
    features that produce the strongest classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a random forest in `scikit-learn`, we import the `RandomForestClassifier`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We then instantiate a random forest classifier, fit the model, and predict
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon submitting our data to Kaggle (using the formula: *C(Pclass) + C(Sex)
    + Age + Sibsp + Parch + C(Embarked)*), the following results are obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Formula** | **Kaggle Score** |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.74163 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 0.76077 |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 0.76077 |'
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 0.77990 |'
  prefs: []
  type: TYPE_TB
- en: '| 100000 | 0.77990 |'
  prefs: []
  type: TYPE_TB
- en: Unsupervised learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two tasks that we are mostly concerned with in unsupervised learning:
    dimensionality reduction and clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality reduction is used to help to visualize higher-dimensional data
    systematically. This is useful because the human brain can visualize only three
    spatial dimensions (and possibly, a temporal one), but most datasets involve much
    higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The typical technique used in dimensionality reduction is **Principal Component
    Analysis** (**PCA**). PCA involves using linear algebra techniques to project
    higher-dimensional data onto a lower-dimensional space. This inevitably involves
    the loss of information, but often, by projecting along the correct set and number
    of dimensions, the information loss can be minimized. A common dimensionality
    reduction technique is to find the combination of variables that explain the most
    variance (proxy for information) in our data and project along those dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of unsupervised learning problems, we do not have the set of labels
    (`Y`), and so, we only call `fit()` on the input data, `X`, itself, and for PCA,
    we call `transform()` instead of `predict()` as we're trying to transform the
    data into a new representation.
  prefs: []
  type: TYPE_NORMAL
- en: One of the datasets that we will be using to demonstrate USL is the iris dataset,
    possibly the most famous dataset in all of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: The `scikit-learn` library provides a set of pre-packaged datasets, which are
    available via the `sklearn.datasets` modules. The iris dataset is one of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The iris dataset consists of 150 samples of data from three different species
    of iris flowers—versicolor, setosa, and virginica—with 50 samples of each type.
    The dataset consists of four features/dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: Petal length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The length and width values are in centimeters. It can be loaded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In our examination of unsupervised learning, we will be focusing on how to visualize
    and cluster this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before discussing unsupervised learning, let''s examine the iris data a bit.
    The `load_iris()` command returns what is known as a bunch object, which is essentially
    a dictionary with keys in addition to the key containing the data. Hence, we have
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Further, the data itself looks similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This corresponds to 150 samples of four features. These four features are shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also take a peek at the actual data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Our target names (what we''re trying to predict) look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'As noted earlier, the iris feature set corresponds to five-dimensional data
    and we cannot visualize this on a color plot. One thing that we can do is pick
    two features and plot them against each other while using color to differentiate
    between the species features. We do this next for all possible combinations of
    features, selecting two at a time for a set of six different possibilities. These
    combinations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal width versus sepal length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width versus petal width
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width versus petal length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal length versus petal width
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal length versus petal length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width versus petal length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/df7ca771-5f28-4cb2-91c3-c4558134174a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The code for this can be found in the following file: `display_iris_dimensions.py`.
    From the preceding plots, we can observe that the setosa points tend to be clustered
    by themselves, while there is a bit of overlap between the virginica and the versicolor
    points. This may lead us to conclude that the latter two species are more closely
    related to one another than to the setosa species.'
  prefs: []
  type: TYPE_NORMAL
- en: These are, however, two-dimensional slices of data. What if we wanted a somewhat
    more holistic view of the data, with some representation of all four sepal and
    petal dimensions? What if there were some hitherto undiscovered connection between
    the dimensions that our two-dimensional plot wasn't showing? Is there a means
    of visualizing this? Enter dimensionality reduction. We will use dimensionality
    reduction to extract two combinations of sepal and petal dimensions to help to
    visualize it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply dimensionality reduction to do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Hence, we see that the reduced dataset is now in two dimensions. Let''s display
    the data visually in two dimensions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2b93c4f6-41e2-4b88-b741-428741ef005d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can examine the makeup of the two PCA-reduced dimensions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Hence, we can see that the two reduced dimensions are a linear combination of
    all four sepal and petal dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The source of this information is at [https://github.com/jakevdp/sklearn_pycon2014](https://github.com/jakevdp/sklearn_pycon2014).
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea behind clustering is to group together similar points in a dataset
    based on a given criterion, hence finding clusters in the data.
  prefs: []
  type: TYPE_NORMAL
- en: The K-means algorithm aims to partition a set of data points into *K* clusters
    such that each data point belongs to the cluster with the nearest mean point or
    centroid.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate K-means clustering, we can apply it to the set of reduced iris
    data that we obtained via PCA, but in this case, we do not pass the actual labels
    to the `fit(..)` method as we do for supervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We now display the clustered data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/356cef4d-149b-4986-9f9d-adf0907aa1d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that our K-means algorithm clusters do not exactly correspond to the dimensions
    obtained via PCA. The source code is available at [https://github.com/jakevdp/sklearn_pycon2014](https://github.com/jakevdp/sklearn_pycon2014).
  prefs: []
  type: TYPE_NORMAL
- en: More information on K-means clustering in `scikit-learn` and, in general can
    be found at [http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html)
    and [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering).
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**XGBoost** is an ensemble algorithm popular for its outstanding performance.
    An ensemble algorithm involves multiple models instead of just one. Ensemble algorithms
    are of two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging**: Here, a result from the algorithm is the average of results from
    individual models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting**: Here, we start from a base learner model. Each successive model
    gets created with better-trained parameters. The learning of new parameters happens
    through optimization algorithms such as gradient descent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will look at the application of XGBoost on a dataset to predict the
    testing time of a newly manufactured car.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a step-by-step guide and you can just follow along:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the working directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the train and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare for removing columns with zero variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Input the function for removing columns with zero variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Get a list of zero variance columns in the train and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove zero variance columns in the train data from test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Find `Unique, Total Count and NAs` and write it to a CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Find a list of categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Create dummy variables from the categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete the categorical variables from `train` and `test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot a scree plot to get the number of components that will explain the 90%
    variance in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform PCA on the train and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Separate the `x` and `y` variables to be passed to `xgboost`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `xgboost` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict from the `xgboost` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the root mean square error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Entropy is a measure of the homogeneity (or heterogeneity) of data. The more
    homogeneous the data, the more entropy it has. Please keep in mind that, to make
    a better classification decision, heterogeneous data is better.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a dataset where 1,000 people were surveyed about whether
    they smoke or not. In the first case, let's say that 500 people said yes and 500
    said no. In the second case, let's assume that 800 people said yes and 200 said
    no. In which case would the entropy be more?
  prefs: []
  type: TYPE_NORMAL
- en: Yes, you guessed right. It is the first one because it is more homogeneous or,
    in other words, the decisions are equally distributed. If a person had to guess
    whether a survey participant answered yes or no, without knowing the actual answer,
    then the chances of them getting the right answer are less in the first case.
    Hence, we say that the data is messier in terms of classification information
    and hence has more entropy.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of any classification problem, especially decision tree (and hence
    random forest and XGBoost), is to decrease this entropy and gain information.
    Next, let's see how we can quantify this seemingly qualitative term.
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation for calculating entropy for the overall dataset is mathematically
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cca91546-a2b0-4241-a59a-3c5f7af90e04.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *p[i]* is the proportion of the dataset with the *i*^(th) class.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the first case we suggested earlier, *p[yes]* would be 500/1,000
    and *p[no]* would be 500/1,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the variation of entropy (the *y* variable) as
    the *p[i]* (the *x* variable) changes from 0 to 1\. Please notice that the *p[i]s*
    have been multiplied by 100 for plotting purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a39a2df8-a350-4bf7-8c0b-19503b03a95b.png)'
  prefs: []
  type: TYPE_IMG
- en: Plot of entropy versus fraction / proportion (0 to 1)
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe the following about the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: It's almost symmetric, about *p=0.5*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It maximizes at *p=0.5*, which makes sense as the messiness is maximal when
    the two classes are equally distributed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code used to generate this plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s see how we can write a function using pandas to calculate the
    entropy of a dataset on its own and of a column from a dataset. For this purpose,
    we can first create dummy data with two columns: `purchase` (the *y* variable)
    and `se_status` (the predictor variables).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the unique values of the categorical variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we write a function to calculate the initial entropy of a dataset given
    the dataset and the name of the *y* variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the initial entropy, the next goal is to find the entropy assuming
    that one of the predictor variables was used for classification. To calculate
    entropy for such a case, we follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Subset the data based on categories in the particular predictor column—one dataset
    for one category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the entropy for each of these datasets so that you have one entropy
    value for each category of the variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a weighted average of these entropies. Weights are given by the proportion
    of that category in that dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mathematically, it can be represented by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b052203-f238-4dd5-a011-69f464bbc0b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation for calculating entropy for a column
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *f[j]*represents the proportion of the *i^([th])* category in the dataset,
    and *p[ij]* represents the proportion of the *j[th]* category of the *y* variable
    in the dataset for the *i*^(th) category of the predictor column. Let''s see how
    a function can be written to calculate this entropy for a given dataset, the *y*
    variable, and a predictor variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Information gain is defined as the reduction in entropy when we move from making
    a classification decision based on only the `y` variable distribution to making
    this decision based on a column. This can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: For the dataset that I have in this instance, I got an information gain of around
    0.08\. While making a decision tree, this information gain is calculated for every
    column. The column with the highest information gain is selected as the next branching
    node in the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embarked on a whirlwind tour of machine learning, examining
    the role of pandas in feature extraction, selection, and engineering as well as
    learning about key concepts in machine learning such as supervised versus unsupervised
    learning. We also had a brief introduction to a few key algorithms in both methods
    of machine learning, and we used the `scikit-learn` package to utilize these algorithms
    to learn and make predictions on data. This chapter was not intended to be a comprehensive
    treatment of machine learning, but rather to illustrate how pandas can be used
    to assist users in the machine learning space.
  prefs: []
  type: TYPE_NORMAL
