<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">A Tour of Statistics with pandas and NumPy</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll take a brief tour of classical statistics (also called the frequentist approach) and show you how we can use pandas together with the <kbd>numpy</kbd> and <kbd>stats</kbd> packages, such as <kbd>scipy.stats</kbd> and <kbd>statsmodels</kbd>, to conduct statistical analysis. We will also learn how to write the calculations behind these statistics from scratch in Python. This chapter and the following ones are not intended to be primers on statistics; they just serve as an illustration of using pandas along with the <kbd>stats</kbd> and <kbd>numpy</kbd> packages. In the next chapter, we will examine an alternative approach to the classical view<span>—that is, </span><strong>Bayesian statistics</strong>.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Descriptive statistics versus inferential statistics</li>
<li>Measures of central tendency and variability</li>
<li>Hypothesis testing <span>– the null and alternative hypotheses</span></li>
<li>The z-test</li>
<li>The t-test</li>
<li>The chi-square test</li>
<li>Analysis of variance (ANOVA) test</li>
<li>Confidence intervals</li>
<li>Correlation and linear regression</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Descriptive statistics versus inferential statistics</h1>
                </header>
            
            <article>
                
<p>In descriptive or summary statistics, we attempt to describe the features of a collection of data in a quantitative way. This is different from inferential or inductive statistics because its aim is to summarize a sample rather than use the data to infer or draw conclusions about the population from which the sample is drawn.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measures of central tendency and variability</h1>
                </header>
            
            <article>
                
<p>Some of the measures that are used in descriptive statistics include the measures of central tendency and measures of variability.</p>
<p>A measure of central tendency is a single value that attempts to describe a dataset by specifying a central position within the data. The three most common measures of central tendency are the <strong>mean</strong>, <strong>median</strong>, and <strong>mode</strong>.</p>
<p>A measure of variability is used to describe the variability in a dataset. Measures of variability include variance and standard deviation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measures of central tendency</h1>
                </header>
            
            <article>
                
<p>Let's take a look at the measures of central tendency, along with illustrations of them, in the following subsections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The mean</h1>
                </header>
            
            <article>
                
<p class="mce-root">The mean or sample is the most popular measure of central tendency. It is equal to the sum of all the values in the dataset, divided by the number of values in the dataset. Thus, in a dataset of <em>n</em> values, the mean is calculated as follows:</p>
<div style="padding-left: 150px"><img src="assets/d09d2a35-9c6b-4ddc-985f-eb213e2bb47f.png" style="width:25.00em;height:5.08em;"/></div>
<p>We use <img class="fm-editor-equation" src="assets/7d4b5155-fd91-4540-9c10-1e0efbfc1e52.png" style="width:0.92em;height:1.17em;"/> if the data values are from a sample and <strong>µ</strong> if the data values are from a population.</p>
<p>The sample mean and population mean are different. The sample mean is what is known as an unbiased estimator of the true population mean. By repeatedly randomly sampling the population to calculate the sample mean, we can obtain a mean of sample means. We can then invoke the law of large numbers and the <strong>central limit theorem</strong> (<strong>CLT</strong>) and denote the mean of the sample means as an estimate of the true population mean.</p>
<p>The population mean is also referred to as the expected value of the population.</p>
<p>The mean, as a calculated value, is often not one of the values observed in the dataset. The main drawback of using the mean is that it is very susceptible to outlier values, or if the dataset is very skewed. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The median</h1>
                </header>
            
            <article>
                
<p>The median is the data value that divides the set of sorted data values into two halves. It has exactly half of the population to its left and the other half to its right. In the case when the number of values in the dataset is even, the median is the average of the two middle values. It is less affected by outliers and skewed data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The mode</h1>
                </header>
            
            <article>
                
<p>The mode is the most frequently occurring value in the dataset. It is more commonly used for categorical data so that we can find out which category is the most common. One downside to using the mode is that it is not unique. A distribution with two modes is described as bimodal, while one with many modes is described as multimodal. The following code is an illustration of a bimodal distribution with modes at two and seven, since they both occur four times in the dataset:</p>
<pre>    In [4]: import matplotlib.pyplot as plt
               %matplotlib inline  
    In [5]: plt.hist([7,0,1,2,3,7,1,2,3,4,2,7,6,5,2,1,6,8,9,7])
               plt.xlabel('x')
               plt.ylabel('Count')
               plt.title('Bimodal distribution')
               plt.show()<strong><br/></strong></pre>
<p>The generated bimodal distribution appears as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1758 image-border" src="assets/2c81a26c-5b65-4cdd-b97d-8a49dc914a8a.png" style="width:29.17em;height:21.17em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computing the measures of central tendency of a dataset in Python</h1>
                </header>
            
            <article>
                
<p>To illustrate this, let's consider the following dataset, which consists of marks that were obtained by 15 pupils for a test scored out of 20:</p>
<pre>    In [18]: grades = [10, 10, 14, 18, 18, 5, 10, 8, 1, 12, 14, 12, 13, 1, 18]  </pre>
<p>The mean, median, and mode can be obtained as follows:</p>
<pre>    In [29]: %precision 3  # Set output precision to 3 decimal places
    Out[29]:u'%.3f'
    
    In [30]: import numpy as np
             np.mean(grades)
    Out[30]: 10.933
    
    In [35]: %precision
             np.median(grades)
    Out[35]: 12.0
    
    In [24]: from scipy import stats
             stats.mode(grades)
    Out[24]: (array([ 10.]), array([ 3.]))
    In [39]: import matplotlib.pyplot as plt
    In [40]: plt.hist(grades)
             plt.title('Histogram of grades')
             plt.xlabel('Grade')
             plt.ylabel('Frequency')
             plt.show()  </pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1759 image-border" src="assets/b5abb54e-0726-47c0-aaa6-0c26479aa388.png" style="width:29.50em;height:23.75em;"/></div>
<p>To illustrate how the skewness of data or an outlier value can drastically affect the usefulness of the mean as a measure of central tendency, consider the following dataset, which shows the wages (in thousands of dollars) of the staff at a factory:</p>
<pre>    In [45]: %precision 2
             salaries = [17, 23, 14, 16, 19, 22, 15, 18, 18, 93, 95]
    
    In [46]: np.mean(salaries)
    Out[46]: 31.82</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Based on the mean value, we may make the assumption that the data is centered around the mean value of <kbd>31.82</kbd>. However, we would be wrong. To explain why, let's display an empirical distribution of the data using a bar plot:</p>
<pre>    In [59]: fig = plt.figure()
             ax = fig.add_subplot(111)
             ind = np.arange(len(salaries))
             width = 0.2
             plt.hist(salaries, bins=xrange(min(salaries),
             max(salaries)).__len__())
             ax.set_xlabel('Salary')
             ax.set_ylabel('# of employees')
             ax.set_title('Bar chart of salaries')
             plt.show()  </pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1760 image-border" src="assets/53ef5bca-b623-464b-9de9-bf30315fd731.png" style="width:28.08em;height:20.17em;"/></div>
<p>From the preceding bar plot, we can see that most of the salaries are far below 30K and that no one is close to the mean of 32K. Now, if we take a look at the median, we will see that it is a better measure of central tendency in this case:</p>
<pre>    In [47]: np.median(salaries)
    Out[47]: 18.00</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>We can also take a look at a histogram of the data:</p>
<pre>    In [56]: plt.hist(salaries, bins=len(salaries))
             plt.title('Histogram of salaries')
             plt.xlabel('Salary')
             plt.ylabel('Frequency')
             plt.show()</pre>
<p>The following is the output of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1761 image-border" src="assets/34f8f7b2-ba9e-4d66-8183-6a037dbf9a70.png" style="width:32.00em;height:26.17em;"/></div>
<div class="packt_tip">The histogram is actually a better representation of the data as bar plots are generally used to represent categorical data, while histograms are preferred for quantitative data, which is the case for the salaries data. For more information on when to use histograms versus bar plots, refer to <a href="http://onforb.es/1Dru2gv"><span class="URLPACKT">http://onforb.es/1Dru2gv</span></a>.</div>
<p>If the distribution is symmetrical and unimodal (that is, has only one mode), the three measures—mean, median, and mode<span>—</span>will be equal. This is not the case if the distribution is skewed. In that case, the mean and median will differ from each other. With a negatively skewed distribution, the mean will be lower than the median and vice versa for a positively skewed distribution:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/e7c9061d-d758-492c-94a9-b4ac0e186e3b.jpg" style="width:40.83em;height:19.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Diagram sourced from <span class="URLPACKT">http://www.southalabama.edu/coe/bset/johnson/lectures/lec15_files/iage014.jpg</span>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measures of variability, dispersion, or spread</h1>
                </header>
            
            <article>
                
<p>Another characteristic of distribution that we measure in descriptive statistics is variability.</p>
<p>Variability specifies how much the data points are different from each other or dispersed. Measures of variability are important because they provide an insight into the nature of the data that is not provided by the measures of central tendency.</p>
<p>As an example, suppose we conduct a study to examine how effective a pre-K education program is in lifting test scores of economically disadvantaged children. We can measure the effectiveness not only in terms of the average value of the test scores of the entire sample, but also in terms of the dispersion of the scores. Is it useful for some students and not so much for others? The variability of the data may help us identify some steps to be taken to improve the usefulness of the program.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Range</h1>
                </header>
            
            <article>
                
<p>The simplest measure of dispersion is the range. The range is the difference between the lowest and highest scores in a dataset. This is the simplest measure of spread, and can be calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>Range = highest value - lowest value</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quartile</h1>
                </header>
            
            <article>
                
<p>A more significant measure of dispersion is the quartile and related interquartile ranges. It also stands for <em>quarterly percentile</em>, which means that it is the value on the measurement scale below which 25, 50, 75, and 100 percent of the scores in the sorted dataset fall. The quartiles are three points that split the dataset into four groups, with each one containing one-fourth of the data. To illustrate this, suppose we have a dataset of 20 test scores that are ranked, as follows:</p>
<pre>    In [27]: import random
             random.seed(100)
             testScores = [random.randint(0,100) for p in 
                           xrange(0,20)]
             testScores
    Out[27]: [14, 45, 77, 71, 73, 43, 80, 53, 8, 46, 4, 94, 95, 33, 31, 77, 20, 18, 19, 35]
    
    In [28]: #data needs to be sorted for quartiles<br/>          sortedScores = np.sort(testScores) 
    In [30]: rankedScores = {i+1: sortedScores[i] for i in 
                             xrange(len(sortedScores))}
    
    In [31]: rankedScores
    Out[31]:
    {1: 4,
     2: 8,
     3: 14,
     4: 18,
     5: 19,
     6: 20,
     7: 31,
    8: 33,
     9: 35,
     10: 43,
     11: 45,
     12: 46,
     13: 53,
     14: 71,
     15: 73,
     16: 77,
     17: 77,
     18: 80,
     19: 94,
     20: 95}
  </pre>
<p>The first quartile (Q1) lies between the fifth and sixth score, the second quartile (Q2) lies between the tenth and eleventh score, and the third quartile (Q3) lies between the fifteenth and sixteenth score. Thus, we get the following results by using linear interpolation and calculating the midpoint:</p>
<pre>Q1 = (19+20)/2 = 19.5
Q2 = (43 + 45)/2 = 44
Q3 = (73 + 77)/2 = 75  </pre>
<p>To see this in IPython, we can use the <kbd>scipy.stats</kbd> or <kbd>numpy.percentile</kbd> packages:</p>
<pre>    In [38]: from scipy.stats.mstats import mquantiles
             mquantiles(sortedScores)
    Out[38]: array([ 19.45,  44.  ,  75.2 ])
    
    In [40]: [np.percentile(sortedScores, perc) for perc in [25,50,75]]
    Out[40]: [19.75, 44.0, 74.0]
  </pre>
<p>The reason why the values don't match exactly with our previous calculations is due to the different interpolation methods. The interquartile range is the first quartile subtracted from the third quartile (<em>Q3 - Q1</em>). It represents the middle 50 in a dataset.</p>
<div class="packt_infobox">For more information on statistical measures, refer to <a href="https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php"><span class="URLPACKT">https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php</span></a>.<br/>
For more details on the <kbd>scipy.stats</kbd> and <kbd>numpy.percentile</kbd> functions, see the documents at <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.mquantiles.html"><span class="URLPACKT">http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.mquantiles.html</span></a> and <a href="http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html"><span class="URLPACKT">http://docs.scipy.org/doc/nu</span></a><a href="http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html"><span class="URLPACKT">mpy-dev/reference/generated/numpy.percentile.html</span></a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deviation and variance</h1>
                </header>
            
            <article>
                
<p>A fundamental idea in the discussion of variability is the concept of deviation. Simply put, a deviation measure tells us how far away a given value is from the mean of the distribution—that is, <img class="fm-editor-equation" src="assets/54f39392-c434-4f81-993e-69f75ec09d5b.png" style="width:3.67em;height:1.25em;"/>.</p>
<p class="mce-root"/>
<p>To find the deviation of a set of values, we define the variance as the sum of the squared deviations and normalize it by dividing it by the size of the dataset. This is referred to as the variance. We need to use the sum of the squared deviations. By taking the sum of the deviations around the mean results in 0, since the negative and positive deviations cancel each other out. The sum of the squared deviations is defined as follows:</p>
<div style="padding-left: 180px"><img class="alignnone size-full wp-image-1762 image-border" src="assets/52004cdb-f669-458f-aad4-7e365575b42e.png" style="width:9.08em;height:4.33em;"/></div>
<p>The preceding expression is equivalent to the following:</p>
<div style="padding-left: 180px"><img class="alignnone size-full wp-image-1763 image-border" src="assets/4159a0da-e491-4843-b0b4-cc1db7c9f9d5.png" style="width:13.83em;height:3.67em;"/></div>
<p>Formally, the variance is defined as follows:</p>
<ul>
<li>For sample variance, use the following formula:</li>
</ul>
<p style="padding-left: 180px"><img style="font-size: 1em;width:15.33em;height:3.58em;" src="assets/727d8bb9-6145-4127-a593-786249844eca.png"/></p>
<ul>
<li>For population variance, use the following formula:</li>
</ul>
<div style="padding-left: 180px"><img src="assets/8c80298c-4ecc-408d-bc81-ebe9251b0b15.png" style="width:13.67em;height:4.08em;"/></div>
<p>The reason why the denominator is <em>N-1</em><strong> </strong>for the sample variance instead of <em>N</em> is that, for sample variance, we want to use an unbiased estimator. For more details on this, take a look at <a href="http://en.wikipedia.org/wiki/Bias_of_an_estimator"><span class="URLPACKT">http://en.wikipedia.org/wiki/Bias_of_an_estimator</span></a>.</p>
<p>The values of this measure are in squared units. This emphasizes the fact that what we have calculated as the variance is the squared deviation. Therefore, to obtain the deviation in the same units as the original points of the dataset, we must take the square root, and this gives us what we call the standard deviation. Thus, the standard deviation of a sample is given by using the following formula:</p>
<div style="padding-left: 150px"><img src="assets/ec150e4f-1fc1-44a3-9569-f97a2a76185e.png" style="width:12.00em;height:3.58em;"/></div>
<p>However, for a population, the standard deviation is given by the following formula:</p>
<div style="padding-left: 150px"><img src="assets/2c1cc04a-1347-4a1b-81f7-d0c51febc169.png" style="width:10.33em;height:3.50em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hypothesis testing – the null and alternative hypotheses</h1>
                </header>
            
            <article>
                
<p>In the preceding section, we had a brief discussion of what is referred to as descriptive statistics. In this section, we will discuss what is known as inferential statistics, whereby we try to use characteristics of the sample dataset to draw conclusions about the wider population as a whole.</p>
<p>One of the most important methods in inferential statistics is hypothesis testing. In hypothesis testing, we try to determine whether a certain hypothesis or research question is true to a certain degree. One example of a hypothesis would be this: eating spinach improves long-term memory.</p>
<p>In order to investigate this statement using hypothesis testing, we can select a group of people as subjects for our study and divide them into two groups, or samples. The first group will be the experimental group, and it will eat spinach over a predefined period of time. The second group, which does not receive spinach, will be the control group. Over selected periods of time, the memory of The individuals in the two groups will be measured and tallied.</p>
<p>Our goal at the end of our experiment will be to be able to make a statement such as "Eating spinach results in an improvement in long-term memory, which is not due to chance". This is also known as significance.</p>
<p>In the preceding scenario, the collection of subjects in the study is referred to as the sample, and the general set of people about whom we would like to draw conclusions is the population.</p>
<p>The ultimate goal of our study will be to determine whether any effects that we observed in the sample can be generalized to the population as a whole. In order to carry out hypothesis testing, we need to come up with what's known as the null and alternative hypotheses.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The null and alternative hypotheses</h1>
                </header>
            
            <article>
                
<p>By referring to the preceding spinach example, the null hypothesis would be "Eating spinach has no effect on long-term memory performance".</p>
<p>The null hypothesis is just that—it nullifies what we're trying to <em>prove</em> by running our experiment. It does so by asserting that a statistical metric (to be explained later) is zero.</p>
<p>The alternative hypothesis is what we hope to support. It is the opposite of the null hypothesis and we assume it to be true until the data provides sufficient evidence that indicates otherwise. Thus, our alternative hypothesis, in this case, is "Eating spinach results in an improvement in long-term memory".</p>
<p>Symbolically, the null hypothesis is referred to as <strong><em>H0</em></strong> and the alternative hypothesis is referred to as <strong><em>H1</em></strong>. You may wish to restate the preceding null and alternative hypotheses as something more concrete and measurable for our study. For example, we could recast <strong><em>H0</em></strong> as follows:</p>
<p>"The mean memory score for a sample of 1,000 subjects who ate 40 grams of spinach daily for a period of 90 days would not differ from the control group of 1,000 subjects who consume no spinach within the same time period."</p>
<p>In conducting our experiment/study, we focus on trying to prove or disprove the null hypothesis. This is because we can calculate the probability that our results are due to chance. However, there is no easy way to calculate the probability of the alternative hypothesis since any improvement in long-term memory could be due to factors other than just eating spinach.</p>
<p>We test out the null hypothesis by assuming that it is true and calculate the probability of the results we gather arising by chance alone. We set a threshold level—alpha (<em>α)</em>—for which we can reject the null hypothesis if the calculated probability is smaller or accept it if it is greater. Rejecting the null hypothesis is tantamount to accepting the alternative hypothesis and vice versa.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The alpha and p-values</h1>
                </header>
            
            <article>
                
<p>In order to conduct an experiment to support or disprove our null hypothesis, we need to come up with an approach that will allow us to make the decision in a concrete and measurable way. To do this test of significance, we have to consider two numbers—the p-value of the test statistic and the threshold level of significance, which is also known as <strong>alpha</strong>.</p>
<p class="mce-root"/>
<p>The p-value is the probability that the result we observe by assuming that the null hypothesis is true occurred by chance alone.</p>
<p>The p-value can also be thought of as the probability of obtaining a test statistic as extreme as or more extreme than the obtained test statistic, given that the null hypothesis is true.</p>
<p>The alpha value is the threshold value against which we compare p-values. This gives us a cut-off point at which we can accept or reject the null hypothesis. It is a measure of how extreme the results we observe must be in order to reject the null hypothesis of our experiment. The most commonly used values of alpha are 0.05 or 0.01.</p>
<p>In general, the rule is as follows:</p>
<ul>
<li>If the p-value is less than or equal to alpha (p&lt; .05), then we reject the null hypothesis and state that the result is statistically significant.</li>
<li>If the p-value is greater than alpha (p &gt; .05), then we have failed to reject the null hypothesis, and we say that the result is not statistically significant.</li>
</ul>
<p>In other words, the rule is as follows:</p>
<ul>
<li>If the test statistic value is greater than or smaller than the two critical test statistic values (for two-tailed tests), then we reject the null hypothesis and state that the (alternative) result is statistically significant.</li>
<li>If the test statistic value lies within the two critical test statistic values, then we have failed to reject the null hypothesis, and we say that the (alternative) result is not statistically significant.</li>
</ul>
<p>The seemingly arbitrary values of alpha in usage is one of the shortcomings of the frequentist methodology, and there are many questions concerning this approach. An article in the <em>Nature</em> journal highlights some of these problems; you can find it at: <a href="http://www.nature.com/news/scientific-method-statistical-errors-1.14700"><span class="URLPACKT">http://www.nature.com/news/scientific-method-statistical-errors-1.14700</span></a>.</p>
<div class="packt_infobox">For more details on this topic, please refer to the following links:<br/>
<br/>
<span class="URLPACKT"><a href="http://statistics.about.com/od/Inferential-Statistics/a/What-Is-The-Difference-Between-Alpha-And-P-Values.htm">http://statistics.about.com/od/Inferential-Statistics/a/What-Is-The-Difference-Between-Alpha-And-P-Values.htm<br/></a></span><a href="http://courses.washington.edu/p209s07/lecturenotes/Week%205_Monday%20overheads.pdf"/></div>
<div class="packt_infobox"><a href="http://courses.washington.edu/p209s07/lecturenotes/Week%205_Monday%20overheads.pdf"><span class="URLPACKT">http://courses.washington.edu/p209s07/lecturenotes/Week%205_Monday%20overheads.pdf</span></a></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Type I and Type II errors</h1>
                </header>
            
            <article>
                
<p>There are two types of errors:</p>
<ul>
<li><strong>Type I Error</strong>: In this type of error, we reject <em>H0</em> when, in fact, <em>H0</em> is true. An example of this would be a jury convicting an innocent person for a crime that the person did not commit.</li>
<li><strong>Type II Error</strong>: In this type of error, we fail to reject <em>H0</em> when, in fact, <em>H1</em> is true. This is equivalent to a guilty person escaping conviction.</li>
</ul>
<p><span>Here's a table showing null hypothesis conditions leading to an error:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1764 image-border" src="assets/f1b2f5f3-7529-438a-8e35-21cba3a272ea.png" style="width:29.67em;height:14.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Statistical hypothesis tests</h1>
                </header>
            
            <article>
                
<p>A statistical hypothesis test is a method that we use to make a decision. We do this using data from a statistical study or experiment. In statistics, a result is termed statistically significant if it is unlikely to have occurred only by chance based on a predetermined threshold probability or significance level. There are two classes of statistical tests: 1-tailed and 2-tailed tests.</p>
<p>In a 2-tailed test, we allot half of our alpha to testing the statistical significance in one direction and the other half to testing the statistical significance in the other direction.</p>
<p>In a 1-tailed test, the test is performed in one direction only.</p>
<p class="mce-root"/>
<div class="packt_infobox">For more details on this topic, please refer to <a href="http://www.ats.ucla.edu/stat/mult_pkg/faq/general/tail_tests.htm"><span class="URLPACKT">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/tail_tests.htm</span></a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Background</h1>
                </header>
            
            <article>
                
<p>To apply statistical inference, it is important to understand the concept of what is known as a sampling distribution. A sampling distribution is the set of all possible values of a statistic, along with their probabilities, assuming that we sample at random from a population where the null hypothesis holds true.</p>
<p>A more simplistic definition is this: a sampling distribution is the set of values that the statistic can assume (distribution) if we were to repeatedly draw samples from the population, along with their associated probabilities.</p>
<p>The value of a statistic is a random sample from the statistic's sampling distribution. The sampling distribution of the mean is calculated by obtaining many samples of various sizes and taking their mean. </p>
<p>The central limit theorem states that the sampling distribution is normally distributed if the original or raw-score population is normally distributed, or if the sample size is large enough. Conventionally, statisticians define large enough sample sizes as N ≥ 30—that is, a sample size of 30 or more. This is still a topic of debate, though.</p>
<div class="packt_infobox">For more details on this topic, refer to <a href="http://stattrek.com/sampling/sampling-distribution.aspx"><span class="URLPACKT">http://stattrek.com/sampling/sampling-distribution.aspx</span></a>.</div>
<p>The standard deviation of the sampling distribution is often referred to as the standard error of the mean, or just the standard error.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The z-test</h1>
                </header>
            
            <article>
                
<p>The z-test is appropriate under the following conditions:</p>
<ul>
<li>The study involves a single sample mean and the parameters <em>µ</em> and<em> <img class="fm-editor-equation" src="assets/4a9cadd5-c79b-48e1-a397-f5e519b8e881.png" style="width:0.92em;height:0.92em;"/></em> of the null hypothesis population are known</li>
<li>The sampling distribution of the mean is normally distributed</li>
<li>The size of the sample is <em>N ≥ 30</em></li>
</ul>
<p class="mce-root"/>
<p>We use the z-test when the mean of the population is <em>known</em>. In the z-test, we ask the question of whether the population mean, µ, is different from a hypothesized value. The null hypothesis in the case of the z-test is as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7b143bcc-02bd-4cf4-a015-53840a2beb17.png" style="width:6.25em;height:1.25em;"/></p>
<p>Here, µ is the population mean and <img class="fm-editor-equation" src="assets/5350818f-46d3-4c74-a8ff-72260e49754c.png" style="width:1.67em;height:1.25em;"/> is the hypothesized value.</p>
<p>The alternative hypothesis, <img class="fm-editor-equation" src="assets/6aea0dc2-32a7-42d7-8ab2-decdefd929ae.png" style="width:2.17em;height:1.50em;"/>, can be one of the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f3494f6c-5151-4f4e-8334-59f22b1af759.png" style="width:6.00em;height:1.17em;"/></div>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7caa782d-7852-47a6-9721-5ad3775c6c4e.png" style="width:6.83em;height:1.33em;"/></div>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2328f214-aee4-45d3-b96a-adf6a2fd4740.png" style="width:6.83em;height:1.42em;"/></div>
<p>The first two are 1-tailed tests, while the last one is a 2-tailed test. In concrete terms, to test µ, we calculate the test statistic:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6919e0ad-b75c-4f18-84fa-abfe3d45b082.png" style="width:6.50em;height:2.92em;"/> </div>
<p>Here, <img class="fm-editor-equation" src="assets/701bb134-02a3-4ed1-b105-5c645eb8d6fc.png" style="width:1.67em;height:1.17em;"/> is the true standard deviation of the sampling distribution of <img class="fm-editor-equation" src="assets/f8293b68-5991-4916-a9b7-144b1c0c43ec.png" style="width:1.08em;height:1.25em;"/>. If <img class="fm-editor-equation" src="assets/4d1c4354-6b7c-4726-873b-0269a20bdc78.png" style="width:2.00em;height:1.50em;"/>is true, the z-test statistics will have the standard normal distribution.</p>
<p>Let's go through a quick illustration of the z-test.</p>
<p>Suppose we have a fictional company, Intelligenza, that claims that they have come up with a radical new method for improved memory retention and study. They claim that their technique can improve grades over traditional study techniques. Suppose the improvement in grades is 40 percent with a standard deviation of 10 percent compared with results obtained using traditional study techniques.</p>
<p>A random test was run on 100 students using the Intelligenza method, and this resulted in a mean improvement of 44 percent. Does Intelligenza's claim hold true?</p>
<p>The null hypothesis for this study states that there is no improvement in grades using Intelligenza's method over traditional study techniques. The alternative hypothesis is that there is an improvement in using Intelligenza's method over traditional study techniques.</p>
<p class="mce-root"/>
<p>The null hypothesis is given by the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7b143bcc-02bd-4cf4-a015-53840a2beb17.png" style="width:6.67em;height:1.33em;"/></p>
<p>The alternative hypothesis is given by the following equation:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7caa782d-7852-47a6-9721-5ad3775c6c4e.png" style="width:6.83em;height:1.33em;"/></div>
<p class="CDPAlignCenter CDPAlign">std error = 10/sqrt(100) = 1</p>
<p class="CDPAlignCenter CDPAlign">z = (43.75-40)/(10/10) = 3.75 std errors</p>
<p>Remember that if the null hypothesis is true, then the test statistic, z, will have a standard normal distribution that would look like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/596ca817-4bc9-48f5-b765-cbd24e45744f.png" style="width:36.08em;height:18.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Diagram sourced from <span class="URLPACKT">http://mathisfun.com/data/images/normal-distrubution-large.gif</span>.</div>
<p>This value of z would be a random sample from the standard normal distribution, which is the distribution of z if the null hypothesis is true.</p>
<p>The observed value of <em>z=43.75</em> corresponds to an extreme outlier p-value on the standard normal distribution curve, which is much less than 0.1 percent.</p>
<p>The p-value is the area under the curve to the right of the value of <em>3.75</em> on the preceding normal distribution curve.</p>
<p>This suggests that it would be highly unlikely for us to obtain the observed value of the test statistic if we were sampling from a standard normal distribution.</p>
<p>We can look up the actual p-value using Python by using the <kbd>scipy.stats</kbd> package, as follows:</p>
<pre><strong>In [104]: 1 - stats.norm.cdf(3.75)</strong>
<strong>Out[104]: 8.841728520081471e-05</strong>  </pre>
<p>Therefore, <em>P(z ≥ 3.75 = 8.8e-05)</em>—that is, if the test statistic was normally distributed, then the probability of obtaining the observed value is <em>8.8e-05</em>, which is close to zero. So it would be almost impossible to obtain the value that we observed if the null hypothesis was actually true.</p>
<p>In more formal terms, we would normally define a threshold or alpha value and reject the null hypothesis if the p-value was ≤ α or fail to reject it otherwise.</p>
<p>The typical values for α are 0.05 or 0.01. The following list explains the different values of alpha:</p>
<ul>
<li><em>p-value &lt;0.01</em>: There is strong evidence against <em>H0</em></li>
<li><em>0.01 &lt; p-value &lt; 0.05</em>: There is strong evidence against <em>H0</em></li>
<li><em>0.05 &lt; p-value &lt; 0.1</em>: There is weak evidence against <em>H0</em></li>
<li><em>p-value &gt; 0.1</em>: There is little or no evidence against <em>H0</em></li>
</ul>
<p>Therefore, in this case, we would reject the null hypothesis and give credence to Intelligenza's claim and state that their claim is highly significant. The evidence against the null hypothesis, in this case, is significant. There are two methods that we use to determine whether to reject the null hypothesis:</p>
<ul>
<li>The p-value approach</li>
<li>The rejection region approach</li>
</ul>
<p>The approach that we used in the preceding example was the latter one.</p>
<p>The smaller the p-value, the less likely it is that the null hypothesis is true. In the rejection region approach, we have the following rule:</p>
<p>If <img class="fm-editor-equation" src="assets/8e4fae50-e9dc-455d-a787-1a4a29c74196.png" style="width:9.25em;height:1.83em;"/>, reject the null hypothesis; otherwise, retain it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The t-test</h1>
                </header>
            
            <article>
                
<p>The z-test is useful when the standard deviation of the population is known. However, in most real-world cases, this is an unknown quantity. For these cases, we turn to the t-test of significance.</p>
<p>For the t-test, given that the standard deviation of the population is unknown, we replace it with the standard deviation, <em>s</em>, of the sample. The standard error of the mean is now as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/40588631-f2e0-4c27-b03a-527547a2f027.png" style="width:5.50em;height:2.92em;"/></div>
<p>The standard deviation of the sample, <em>s</em>, is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d0f83fd9-a389-485b-91c7-e1cc6c79f509.png" style="width:9.58em;height:3.75em;"/></p>
<p>The denominator is <em>N-1</em> and not <em>N</em>. This value is known as the number of degrees of freedom. We will now state (without an explanation) that, by the CLT, the t-distribution approximates the normal, Guassian, or z-distribution as <em>N,</em> and so <em>N-1</em> increases—that is, with increasing <strong>degrees of freedom</strong> (<strong>df</strong>). When <em>df</em> <em>=</em> ∞, the t-distribution is identical to the normal or z-distribution. This is intuitive since, as <em>df</em> increases, the sample size increases and <em>s</em> approaches <img class="fm-editor-equation" src="assets/03309792-82d0-44d4-9393-cf4ba0feac4c.png" style="width:0.92em;height:0.92em;"/>, which is the true standard deviation of the population. There are an infinite number of t-distributions, each corresponding to a different value of <em>df</em>.</p>
<p>This can be seen in the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/b2e48643-784e-4df8-a029-c1a924ed347e.png" style="width:23.33em;height:23.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Diagram sourced from <span class="URLPACKT"><span class="URLPACKT">http://zoonek2.free.fr/UNIX/48_R/g593.png</span></span> <a href="http://zoonek2.free.fr/UNIX/48_R/g593.png."/></div>
<div class="packt_infobox">A more detailed technical explanation on the relationship between t-distribution, z-distribution, and the degrees of freedom can be found at <a href="http://en.wikipedia.org/wiki/Student's_t-distribution"><span class="URLPACKT">http://en.wikipedia.org/wiki/Student's_t-distribution</span></a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of t-tests</h1>
                </header>
            
            <article>
                
<p>There are various types of t-tests. The following are the most common one. They typically formulate a null hypothesis that makes a claim about the mean of a distribution:</p>
<ul>
<li><strong>One-sample independent t-test</strong>: This is used to compare the mean of a sample with that of a known population mean or known value. Let's assume that we're health researchers in Australia who are concerned with the health of the aboriginal population and wish to ascertain whether babies born to low-income aboriginal mothers have lower birth weight than normal.</li>
</ul>
<p style="padding-left: 60px">An example of a null hypothesis test for a one-sample t-test would be this: the mean birth weight for our sample of 150 deliveries of full-term, live babies from low-income aboriginal mothers is no different from the mean birth weight of babies in the general Australian population—that is, 3,367 grams.</p>
<p style="padding-left: 60px">The reference for this information can be found at <a href="http://www.healthinfonet.ecu.edu.au/health-facts/overviews/births-and-pregnancy-outcome"><span class="URLPACKT">http://www.healthinfonet.ecu.edu.au/health-facts/overviews/births-and-pregnancy-outcome</span></a>.</p>
<ul>
<li><strong>Independent samples t-tests</strong>: This is used to compare means from independent samples with each other. An example of an independent sample t-test would be a comparison of the fuel economy of automatic transmission versus manual transmission vehicles. This is what our real-world example will focus on.</li>
</ul>
<p style="padding-left: 60px">The null hypothesis for the t-test would be this: there is no difference between the average fuel efficiency of cars with manual and automatic transmissions in terms of their average combined city/highway mileage.</p>
<ul>
<li><strong>Paired samples t-test</strong>: In a paired/dependent samples t-test, we take each data point in one sample and pair it with a data point in the other sample in a meaningful way. One way to do this would be to measure against the same sample at different points in time. An example of this would be to examine the efficacy of a slimming diet by comparing the weight of a sample of participants before and after the diet.</li>
</ul>
<p style="padding-left: 60px">The null hypothesis, in this case, would be this: there is no difference between the mean weights of participants before and after going on the slimming diet, or more succinctly, the mean difference between paired observations is zero.</p>
<p style="padding-left: 60px">This information can be found at <a href="http://en.wikiversity.org/wiki/T-test"><span class="URLPACKT">http://en.wikiversity.org/wiki/T-test</span></a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A t-test example</h1>
                </header>
            
            <article>
                
<p>In simplified terms, to do <strong>Null Hypothesis Significance Testing</strong> (<strong>NHST</strong>), we need to do the following:</p>
<ol>
<li>Formulate our null hypothesis. The null hypothesis is our model of the system, assuming that the effect we wish to verify was actually due to chance.</li>
<li>Calculate our p-value.</li>
<li>Compare the calculated p-value with that of our alpha, or threshold value, and decide whether to reject or accept the null hypothesis. If the p-value is low enough (lower than the alpha), we will draw the conclusion that the null hypothesis is likely to be false.</li>
</ol>
<p>For our real-world illustration, we want to investigate whether manual transmission vehicles are more fuel efficient than automatic transmission vehicles. In order to do this, we will make use of the Fuel Economy data that was published by the US government for 2014 at <a href="http://www.fueleconomy.gov"><span class="URLPACKT">http://www.fueleconomy.gov</span></a><span class="URLPACKT">:</span></p>
<pre>In [53]: import pandas as pd  <br/>import numpy as np  <br/>feRawData = pd.read_csv('2014_FEGuide.csv') <br/><br/>In [54]: feRawData.columns[:20]<br/>Out[54]: Index([u'Model Year', u'Mfr Name', u'Division', u'Carline', u'Verify Mfr Cd', u'Index (Model Type Index)', u'Eng Displ', u'# Cyl', u'Trans as listed in FE Guide (derived from col AA thru AF)', u'City FE (Guide) - Conventional Fuel', u'Hwy FE (Guide) - Conventional Fuel', u'Comb FE (Guide) - Conventional Fuel', u'City Unadj FE - Conventional Fuel', u'Hwy Unadj FE - Conventional Fuel', u'Comb Unadj FE - Conventional Fuel', u'City Unrd Adj FE - Conventional Fuel', u'Hwy Unrd Adj FE - Conventional Fuel', u'Comb Unrd Adj FE - Conventional Fuel', u'Guzzler? ', u'Air Aspir Method'], dtype='object')
    
In [51]: feRawData = feRawData.rename(columns={'Trans as listed in FE Guide (derived from col AA thru AF)' :'TransmissionType', 'Comb FE (Guide) - Conventional Fuel' : 'CombinedFuelEcon'})
    
    In [57]: transType=feRawData['TransmissionType']
             transType.head()
    Out[57]: 0      Auto(AM7)
             1     Manual(M6)
             2      Auto(AM7)
             3     Manual(M6)
             4    Auto(AM-S7)
             Name: TransmissionType, dtype: object
  </pre>
<p>Now, we wish to modify the preceding series so that the values just contain the <kbd>Auto</kbd> and <kbd>Manual</kbd> strings. We can do this as follows:</p>
<pre>    In [58]: transTypeSeries = transType.str.split('(').str.get(0)
             transTypeSeries.head()
    Out[58]: 0      Auto
             1    Manual
             2      Auto
             3    Manual
             4      Auto
             Name: TransmissionType, dtype: object
  </pre>
<p>Now, let's create a final modified DataFrame from Series that consists of the transmission type and the combined fuel economy figures:</p>
<pre>    In [61]: feData=pd.DataFrame([transTypeSeries,feRawData['CombinedFuelEcon']]).T
             feData.head()
    Out[61]:    TransmissionType    CombinedFuelEcon
             0  Auto                16
             1  Manual              15
             2  Auto                16
             3  Manual              15
             4  Auto                17
             5 rows × 2 columns
  </pre>
<p>We can now separate the data for vehicles with automatic transmission from those with manual transmission, as follows:</p>
<pre>    In [62]: feData_auto=feData[feData['TransmissionType']=='Auto']
             feData_manual=feData[feData['TransmissionType']=='Manual']
    In [63]: feData_auto.head()
    Out[63]:   TransmissionType     CombinedFuelEcon
            0  Auto                 16
            2  Auto                 16
            4  Auto                 17
            6  Auto                 16
            8  Auto                 17
            5 rows × 2 columns</pre>
<p class="mce-root"/>
<p>This shows that there were 987 vehicles with automatic transmission versus 211 with manual transmission:</p>
<pre>    In [64]: len(feData_auto)
    Out[64]: 987
    
    In [65]: len(feData_manual)
    Out[65]: 211
    
    In [87]: np.mean(feData_auto['CombinedFuelEcon'])
    Out[87]: 22.173252279635257
    
    In [88]: np.mean(feData_manual['CombinedFuelEcon'])
    Out[88]: 25.061611374407583
    
    In [84]: import scipy.stats as stats
             stats.ttest_ind(feData_auto['CombinedFuelEcon'].tolist(), 
                             feData_manual['CombinedFuelEcon'].tolist())
    Out[84]: (array(-6.5520663209014325), 8.4124843426100211e-11)
    
    In [86]: stats.ttest_ind(feData_auto['CombinedFuelEcon'].tolist(), 
                             feData_manual['CombinedFuelEcon'].tolist(), 
                             equal_var=False)
    Out[86]: (array(-6.949372262516113), 1.9954143680382091e-11)
    
  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">chi-square test</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to implement a chi-square test from scratch in Python and run it on an example dataset.</p>
<p class="mce-root">A chi-square test is conducted to determine the statistical significance of a causal relationship of two categorical variables with each other.</p>
<p class="mce-root">For example, in the following dataset, a chi-square test can be used to determine whether color preferences affect a personality type (introvert and extrovert) or not, and vice versa:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6c3fc09e-f64b-4e58-90dc-5a5a283d1b40.png" style="width:19.92em;height:3.83em;"/></p>
<p class="mce-root"> The two hypotheses for chi-square tests are as follows:</p>
<ul>
<li class="mce-root"><strong>H0</strong>: Color preferences are not associated with a personality type</li>
<li class="mce-root"><strong>Ha</strong>: Color preferences are associated with a personality type</li>
</ul>
<p class="mce-root">To calculate the chi-square statistic, we assume that the null hypothesis is true. If there is no relationship between the two variables, we could just take the contribution (proportion) of that column as the total and multiply that with the row total for that cell; that would give us the expected cell. In other words, the absence of a specific relationship implies a simple proportional relationship and distribution. Therefore, we calculate the expected number in each subcategory (assuming the null hypothesis is true) as follows:</p>
<p class="mce-root"><em>Expected Frequency = (Row Total X Column Total )/ Total:</em></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6904394e-51d6-40bc-b8e4-057b5c1b2f24.png" style="width:22.83em;height:4.42em;"/></p>
<p class="mce-root">Once the expected frequency has been calculated, the ratio of the square of the difference between the expected and observed frequency, divided by the expected frequency, is calculated:</p>
<p class="mce-root"><em>Chi_Square_Stat =Sum( (Expected Frequency-Observed Frequency)**2/Expected Frequency)</em></p>
<p class="mce-root">These statistics follow a chi-square distribution with a parameter called the <strong>degree of freedom (DOF)</strong>. The degree of freedom is given by the following equation:</p>
<p class="mce-root"><em>DOF = (Number of Rows -1)*(Number of Column-1)</em></p>
<p class="mce-root">There is a different distribution for each degree of freedom. This is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1765 image-border" src="assets/076c226d-e9b8-4387-9db7-d60c01556359.png" style="width:26.42em;height:17.58em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"> Chi-square distributions at different degrees of freedoms</div>
<p class="mce-root">Like any other test we've looked at, we need to decide a significance level and find the p-value associated with the chi-square statistics for that degree of freedom.</p>
<p class="mce-root">If the p-value is less than the alpha value, the null hypothesis can be rejected.</p>
<p class="mce-root">This whole calculation can be done by writing some Python code. The following two functions calculate the chi-square statistic and the degrees of freedom:</p>
<pre style="padding-left: 30px" class="mce-root"> #Function to calculate the chi-square statistic <br/>def chi_sq_stat(data_ob):              <br/>    col_tot=data_ob.sum(axis=0) <br/>    row_tot=data_ob.sum(axis=1) <br/>    tot=col_tot.sum(axis=0) <br/>    row_tot.shape=(2,1) <br/>    data_ex=(col_tot/tot)*row_tot <br/>    num,den=(data_ob-data_ex)**2,data_ex <br/>    chi=num/den <br/>    return chi.sum() <br/><br/>#Function to calculate the degrees of freedom <br/>def degree_of_freedom(data_ob): <br/>    dof=(data_ob.shape[0]-1)*(data_ex.shape[1]-1) <br/>    return dof <br/><br/># Calculting these for the observed data <br/>data_ob=np.array([(20,6,30,44),(180,34,50,36)]) <br/>chi_sq_stat(data_ob) <br/>degree_of_freedom(data_ob) </pre>
<p class="mce-root">The chi-square statistic is 71.99, while the degrees of freedom is 3. The p-values can be calculated using the table found here: <a href="https://people.smp.uq.edu.au/YoniNazarathy/stat_models_B_course_spring_07/distributions/chisqtab.pdf">https://people.smp.uq.edu.au/YoniNazarathy/stat_models_B_course_spring_07/distributions/chisqtab.pdf</a>.<a href="https://people.smp.uq.edu.au/YoniNazarathy/stat_models_B_course_spring_07/distributions/chisqtab.pdf"/></p>
<p class="mce-root">From the tables, the p-value for 71.99 is very close to 0. Even if we choose alpha to be a small number, such as 0.01, the p-value is still smaller. With this, we can say that the null hypothesis can be rejected with a good degree of statistical confidence.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ANOVA test</h1>
                </header>
            
            <article>
                
<p>Now let's talk about another popular hypothesis test, called ANOVA. It is used to test whether similar data points coming from different groups or under a different set of experiments are statistically similar to each other or different—for example, the average height of different sections of a class in a school or the peptide length of a certain protein found in humans across different ethnicities.</p>
<p>ANOVA calculates two metrics to conduct this test:</p>
<ul>
<li>Variance among different groups</li>
<li>Variance within each group</li>
</ul>
<p>Based on these metrics, a statistic is calculated with variance among different groups as a numerator. If it is a statistically large enough number, it means that the variance among different groups is larger than the variance within the group, implying that the data points coming from the different groups are different.</p>
<p>Let's look at how variance among different groups and variance within each group can be calculated. Suppose we have <em>k</em> groups that data points are coming from:</p>
<p>The data points from group 1 are <em>X<sub>11</sub></em>, <em>X<sub>12</sub></em>, ......., <em>X</em><sub><em>1n</em>.</sub></p>
<p>The data points from group 2 are <em>X<sub>21</sub></em>, <em>X<sub>22</sub></em>, ......., <em>X</em><sub><em>2n</em>.</sub></p>
<p>This means that the data points from group <em>k</em> are <em>X<sub>k1</sub></em>, <em>X<sub>k2</sub></em>, ......., <em>X</em><sub><em>kn</em>.</sub></p>
<p>Let's use the following abbreviations and symbols to describe some features of this data:</p>
<ul>
<li>Variance among different groups is represented by SSAG</li>
<li>Variance within each group is represented by SSWG</li>
<li>Number of elements in group k is represented by <em>n<sub>k</sub></em></li>
<li>Mean of data points in a group is represented by <em>µ<sub>k</sub></em></li>
<li>Mean of all data points across groups is represented by <em>µ</em></li>
<li>Number of groups is represented by <em>k</em></li>
</ul>
<p>Let's define the two hypotheses for our statistical test:</p>
<ul>
<li><strong>Ho</strong>: µ<sub>1</sub> = µ<sub>2</sub> = .....= µ<sub>k</sub></li>
<li><strong>Ho</strong>: µ<sub>1</sub> <span>!= µ</span><sub>2</sub> <span>!= .....= µ</span><sub>k</sub></li>
</ul>
<p>In other words, the null hypothesis states that the mean of the data points across all the groups is the same, while the alternative hypothesis says that the mean of at least one group is different from the others.</p>
<p class="mce-root"/>
<p>This leads to the following equations:</p>
<p class="CDPAlignCenter CDPAlign">SSAG = <em>(∑ n<sub>k</sub> * (X<sub>k</sub> - µ)**2) / k-1</em></p>
<p class="CDPAlignCenter CDPAlign">SSWG = <em>(∑∑(X<sub>ki</sub>-µ<sub>k</sub>)**2) / n*k-k-1</em></p>
<p>In SSAG, the summation is over all the groups.</p>
<p>In SSWG, the first summation is across the data points from a particular group and the second summation is across the groups.</p>
<p>The denominators in both cases denote the degree of freedom. For SSAG, we are dealing with k groups and the last value can be derived from the other <em>k-1</em> values. Therefore, DOF is <em>k-1</em>. For SSWG, there are <em>n*k</em> data points, but the k-1 mean values are restricted (or fixed) by those choices, and so the DOF is <em>n*k-k-1</em>.</p>
<p>Once these numbers have been calculated, the test statistic is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>Test Statistic = SSAG/SSWG</em></p>
<p>This ratio of SSAG and SSWG follows a new distribution called F-distribution, and so the statistic is called an F statistic. It is defined by the two different degrees of freedom, and there is a separate distribution for each combination, as shown in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/72aaa7fc-4367-45ce-9a20-d118c6523055.png" style="width:20.83em;height:15.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">F-distribution based on the two DOFs, d1 =k-1 and d2=n*k-k-1</div>
<p>Like any of the other tests we've looked at, we need to decide a significance level and need to find the p-value associated with the F statistics for those degrees of freedom. If the p-value is less than the alpha value, the null hypothesis can be rejected. This whole calculation can be done by writing some Python code.</p>
<p>Let's have a look at some example data and see how ANOVA can be applied:</p>
<pre>    import pandas as pd
    data=pd.read_csv('ANOVA.csv')
    data.head()</pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4fd41cb4-54c5-4fc8-8239-0d42555201ef.png" style="width:8.75em;height:9.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Head of the OD data by Lot and Run</div>
<p>We are interested in finding out whether the mean OD is the same for different lots and runs. We will apply ANOVA for that purpose, but before that, we can draw a boxplot to get an intuitive sense of the differences in the distributions for different lots and runs:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1766 image-border" src="assets/650a5a5f-161c-4b1d-a879-439bb8edb592.png" style="width:36.17em;height:25.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Boxplot of OD by Lot</div>
<p>Similarly, a boxplot of OD grouped by Run can also be plotted:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1767 image-border" src="assets/89d1e6a6-7c1f-48be-9a6c-2c12249df343.png" style="width:53.75em;height:38.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Boxplot of OD by Run</div>
<p>Now, let's write the Python code to perform the calculations:</p>
<pre># Calculating SSAG<br/>group_mean=data.groupby('Lot').mean()<br/>group_mean=np.array(group_mean['OD'])<br/>tot_mean=np.array(data['OD'].mean())<br/>group_count=data.groupby('Lot').count()<br/>group_count=np.array(group_count['OD'])<br/>fac1=(group_mean-tot_mean)**2<br/>fac2=fac1*group_count<br/>DF1=(data['Lot'].unique()).size-1<br/>SSAG=(fac2.sum())/DF1<br/>SSAG<br/><br/>#Calculating SSWG<br/>group_var=[]<br/>for i in range((data['Lot'].unique()).size):<br/>    lot_data=np.array(data[data['Lot']==i+1]['OD'])<br/>    lot_data_mean=lot_data.mean()<br/>    group_var_int=((lot_data-lot_data_mean)**2).sum()<br/>    group_var.append(group_var_int)<br/>group_var_sum=(np.array(group_var)).sum()<br/>DF2=data.shape[0]-(data['Lot'].unique()).size-1<br/>SSAW=group_var_sum/DF2<br/>SSAW<br/><br/>F=SSAG/SSAW<br/>F</pre>
<p>The value of the F statistic comes out to be 3.84, while the degrees of freedom are 4 and 69, respectively.</p>
<p>At a significance level—that is, alpha—of 0.05, the critical value of the F statistic lies between 2.44 and 2.52 (from the F-distribution table found at: <a href="http://socr.ucla.edu/Applets.dir/F_Table.html">http://socr.ucla.edu/Applets.dir/F_Table.html</a>).</p>
<p>Since the value of the F statistic (3.84) is larger than the critical value of 2.52, the F statistic lies in the rejection region, and so the null hypothesis can be rejected. Therefore, it can be concluded that the mean OD values are different for different lot groups. At a significance level of 0.001, the F statistic becomes smaller than the critical value, and so the null hypothesis can't be rejected. We would have to accept that the OD means from the different groups are statistically the same. The same test can be performed for different run groups. This has been left as an exercise for you to practice with.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confidence intervals</h1>
                </header>
            
            <article>
                
<p>In this section, we will address the issue of confidence intervals. A confidence interval allows us to make a probabilistic estimate of the value of the mean of a population's given sample data.</p>
<p>This estimate, called an interval estimate, consists of a range of values (intervals) that act as good estimates of the unknown population parameter.</p>
<p>The confidence interval is bounded by confidence limits. A 95 percent confidence interval is defined as an interval in which the interval contains the population mean with a 95 percent probability. So how do we construct a confidence interval?</p>
<p class="mce-root"/>
<p>Suppose we have a 2-tailed t-test and we want to construct a 95 percent confidence interval. In this case, we want the sample t-value, <img src="assets/bea7daa2-f67c-4259-bfde-cf790f51841e.png"/>, corresponding to the mean to satisfy the following inequality:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e98d2c60-01e2-458c-952f-a3950a64daaf.png" style="width:11.92em;height:1.25em;"/></div>
<p>Given that <img class="fm-editor-equation" src="assets/a7d87347-ae59-479c-a894-add67b396748.png" style="width:10.00em;height:2.92em;"/> , we can substitute this in the preceding inequality relation to obtain the following equation:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b70a50cf-7705-4a25-8a69-f3b000639f74.png" style="width:20.25em;height:1.25em;"/></div>
<p>The <img class="fm-editor-equation" src="assets/ce778062-d045-4f19-9187-985877ed0186.png" style="width:22.75em;height:1.50em;"/> interval is our 95 percent confidence interval.</p>
<p>Generalizing any confidence interval for any percentage, <em>y</em>, can be expressed as <img class="fm-editor-equation" src="assets/a4721c72-a296-4dbb-8350-430f4ef60c70.png" style="width:20.92em;height:1.50em;"/>, where <img class="fm-editor-equation" src="assets/736ba19b-2281-4848-ad52-13b923adb857.png" style="width:1.75em;height:1.17em;"/> is the t-tailed value of <em>t—t</em>hat is, <img class="fm-editor-equation" src="assets/1640980b-71b8-4e3e-bef0-b50584e5bb79.png" style="width:1.83em;height:1.33em;"/> correlation to the desired confidence interval for <em>y</em>.</p>
<p>We will now take the opportunity to illustrate how we can calculate the confidence interval using a dataset from the popular statistical environment known as R. The <kbd>stats</kbd> models' module provides access to the datasets that are available in the core datasets package of R through the <kbd>get_rdataset</kbd> function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An illustrative example</h1>
                </header>
            
            <article>
                
<p>We will consider the dataset known as faithful, which consists of data that was obtained by observing the eruptions of the Old Faithful geyser in the Yellowstone National Park in the US. The two variables in the dataset are eruptions, which are the length of time the geyser erupts, and waiting, which is the time until the next eruption. There were 272 observations:</p>
<pre>    In [46]: import statsmodels.api as sma
             faithful=sma.datasets.get_rdataset("faithful")
             faithful
    Out[46]: &lt;class 'statsmodels.datasets.utils.Dataset'&gt;
    
    In [48]: faithfulDf=faithful.data
             faithfulDf.head()
    Out[48]:    eruptions   waiting
            0   3.600       79
            1   1.800       54
            2   3.333       74
            3   2.283       62
            4  4.533        85
    5 rows × 2 columns
    
    In [50]: len(faithfulDf)
    Out[50]: 272</pre>
<p>Let's calculate a 95 percent confidence interval for the mean waiting time of the geyser. To do this, we must obtain the sample mean and standard deviation of the data:</p>
<pre>    In [80]: mean,std=(np.mean(faithfulDf['waiting']),
                       np.std(faithfulDf['waiting']))  </pre>
<p>Now, we'll make use of the <kbd>scipy.stats</kbd> package to calculate the confidence interval:</p>
<pre>    In [81]: from scipy import stats<br/>          N=len(faithfulDf['waiting'])
            ci=stats.norm.interval(0.95,loc=mean,scale=std/np.sqrt(N))
    In [82]: ci
    Out[82]: (69.28440107709261, 72.509716569966201)</pre>
<p>Thus, we can state with 95 percent confidence that the [69.28, 72.51] interval contains the actual mean waiting time of the geyser.</p>
<div class="packt_infobox">This information can be found at <a href="http://statsmodels.sourceforge.net/devel/datasets/index.html"><span class="URLPACKT">http://statsmodels.sourceforge.net/devel/datasets/index.html</span></a> and <a href="http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.norm.html"><span class="URLPACKT">http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.norm.html</span></a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Correlation and linear regression</h1>
                </header>
            
            <article>
                
<p>One of the most common tasks in statistics when determining the relationship between two variables is whether there is dependence between them. Correlation is the general term we use in statistics for variables that express dependence with each other.</p>
<p>We can then use this relationship to try and predict the value of one set of variables from the other. This is known as regression.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Correlation</h1>
                </header>
            
            <article>
                
<p>The statistical dependence that's expressed in a correlation relationship does not imply a causal relationship between the two variables; the famous line regarding this is c<em>orrelation does not imply causation</em>. Thus, the correlation between two variables or datasets implies just a casual rather than a causal relationship or dependence. For example, there is a correlation between the amount of ice cream purchased on a given day and the weather.</p>
<div class="packt_infobox">For more information on correlation and dependency, refer to <a href="http://en.wikipedia.org/wiki/Correlation_and_dependence"><span class="URLPACKT">http://en.wikipedia.org/wiki/Correlation_and_dependence</span></a>.</div>
<p>The correlation measure, known as the correlation coefficient, is a number that describes the size and direction of the relationship between the two variables. It can vary from -1 to +1 in direction and 0 to 1 in magnitude. The direction of the relationship is expressed through the sign, with a <em>+</em> sign expressing a positive correlation and a <em>-</em> sign expressing a negative correlation. The higher the magnitude, the greater the correlation, with a 1 being termed as the perfect correlation.</p>
<p>The most popular and widely used correlation coefficient is the Pearson product-moment correlation coefficient, known as <em>r</em>. It measures the linear correlation or dependence between two <em>x</em> and <em>y</em> variables and takes values between -1 and +1.</p>
<p>The sample correlation coefficient, <em>r</em>, is defined as follows:</p>
<div style="padding-left: 150px" class="packt_figure CDPAlignLeft CDPAlign"><img src="assets/b0bc723b-d636-416a-ac94-c27380de54b2.png" style="width:12.67em;height:3.58em;"/></div>
<p>This can also be written as follows:</p>
<div style="padding-left: 150px" class="packt_figure CDPAlignLeft CDPAlign"><img src="assets/07d2c249-b46c-4baf-8b1f-d913123ea43b.png" style="width:18.08em;height:3.75em;"/></div>
<p>Here, we have omitted the summation limits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression</h1>
                </header>
            
            <article>
                
<p>As we mentioned previously, regression focuses on using the relationship between two variables for prediction. In order to make predictions using linear regression, the best-fitting straight line must be computed.</p>
<p class="mce-root"/>
<p>If all the points (values for the variables) lie on a straight line, then the relationship is deemed perfect. This rarely happens in practice and the points do not all fit neatly on a straight line. Because of this, the relationship is imperfect. In some cases, a linear relationship only occurs among log-transformed variables. This is a log-log model. An example of such a relationship would be a power law distribution in physics, where one variable varies as a power of another.</p>
<p>Thus, an expression such as this results in the linear relationship.</p>
<div class="packt_infobox">For more information, refer to <a href="http://en.wikipedia.org/wiki/Power_law"><span class="URLPACKT">http://en.wikipedia.org/wiki/Power_law</span>.</a></div>
<p>To construct the best-fit line, the method of least squares is used. In this method, the best-fit line is the optimal line that is constructed between the points for which the sum of the squared distance from each point to the line is the minimum. This is deemed to be the best linear approximation of the relationship between the variables we are trying to model using linear regression. The best-fit line in this case is called the least squares regression line.</p>
<p>More formally, the <span>least squares regression line</span> is the line that has the minimum possible value for the sum of squares of the vertical distance from the data points to the line. These vertical distances are also known as residuals.</p>
<p>Thus, by constructing the least squares regression line, we're trying to minimize the following expression:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1f90155b-138a-486d-9346-dcf72ac89226.png" style="width:8.08em;height:4.50em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An illustrative example</h1>
                </header>
            
            <article>
                
<p>We will now illustrate all the preceding points with an example. Suppose we're doing a study in which we would like to illustrate the effect of temperature on how often crickets chirp. The data for this example was obtained from the book <em>The Song of Insects</em>, by George W Pierce, which was written in 1948. George Pierce measured the frequency of chirps made by a ground cricket at various temperatures.</p>
<p>We want to investigate the frequency of cricket chirps and the temperature, as we suspect that there is a relationship between them. The data consists of 16 data points, and we will read it into a DataFrame.</p>
<p class="mce-root"/>
<p>The<span> data is sourced from </span><a href="http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr02.html"><span class="URLPACKT">http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr02.html</span></a>. Let's take a look at it:</p>
<pre>    In [38]: import pandas as pd
             import numpy as np
             chirpDf= pd.read_csv('cricket_chirp_temperature.csv')<br/>    In [39]: chirpDf
    Out[39]:chirpFrequency  temperature
    0       20.000000       88.599998
    1       16.000000       71.599998
    2       19.799999       93.300003
    3       18.400000       84.300003
    4       17.100000       80.599998
    5       15.500000       75.199997
    6       14.700000       69.699997
    7       17.100000       82.000000
    8       15.400000       69.400002
    9       16.200001       83.300003
    10      15.000000       79.599998
    11      17.200001       82.599998
    12      16.000000       80.599998
    13      17.000000       83.500000
    14      14.400000       76.300003
    15 rows × 2 columns
  </pre>
<p>First, let's make a scatter plot of the data, along with a regression line, or line of best fit:</p>
<pre>    In [29]: plt.scatter(chirpDf.temperature,chirpDf.chirpFrequency,
                marker='o',edgecolor='b',facecolor='none',alpha=0.5)
               plt.xlabel('Temperature')
               plt.ylabel('Chirp Frequency')
               slope, intercept = np.polyfit(chirpDf.temperature,chirpDf.chirpFrequency,1)
               plt.plot(chirpDf.temperature,chirpDf.temperature*slope + intercept,'r')
               plt.show()
  </pre>
<p><span>As you can see from the following diagram, there seems to be a linear relationship between temperature and the chirp frequency:</span></p>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1768 image-border" src="assets/746594a9-2159-4625-bc28-fba78e040a63.png" style="width:33.83em;height:26.75em;"/></div>
<p>We can now proceed to investigate further by using the <kbd>statsmodels.ols</kbd> (ordinary least squares) method:</p>
<pre>    [37]: chirpDf= pd.read_csv('cricket_chirp_temperature.csv')
          chirpDf=np.round(chirpDf,2)
          result=sm.ols('temperature ~ chirpFrequency',chirpDf).fit()
          result.summary()
    
    Out[37]: OLS Regression Results
       Dep. Variable: temperature     R-squared:      0.697
       Model: OLS     Adj. R-squared: 0.674
       Method:        Least Squares   F-statistic:    29.97
       Date:  Wed, 27 Aug 2014     Prob (F-statistic):     0.000107
       Time:  23:28:14        Log-Likelihood: -40.348
       No. Observations:      15      AIC:    84.70
       Df Residuals:  13      BIC:    86.11
       Df Model:      1               
                       coef     std err t     P&gt;|t| [95.0% Conf. Int.]
       Intercept     25.2323 10.060  2.508 0.026 3.499 46.966
       chirpFrequency 3.2911  0.601  5.475 0.000 1.992 4.590
    
       Omnibus:        1.003   Durbin-Watson:  1.818
       Prob(Omnibus):  0.606   Jarque-Bera (JB):       0.874
       Skew:   -0.391  Prob(JB):       0.646
       Kurtosis:       2.114   Cond. No.       171.</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>We will ignore most of the preceding results, except for the <kbd>R-squared</kbd>, <kbd>Intercept</kbd>, and <kbd>chirpFrequency</kbd> values.</p>
<p>From the preceding result, we can conclude that the slope of the regression line is <kbd>3.29</kbd> and that the intercept on the temperature axis is <kbd>25.23</kbd>. Thus, the regression line equation looks like <kbd>temperature = 25.23 + 3.29 * chirpFrequency</kbd>.</p>
<p>This means that as the chirp frequency increases by 1, the temperature increases by about 3.29 degrees Fahrenheit. However, note that the intercept value is not really meaningful as it is outside the bounds of the data. We can also only make predictions for values within the bounds of the data. For example, we cannot predict what <kbd>chirpFrequency</kbd> is at 32 degrees Fahrenheit as it is outside the bounds of the data; moreover, at 32 degrees Fahrenheit, the crickets would have frozen to death. The value of R—that is, the correlation coefficient—is given as follows:</p>
<pre>In [38]: R=np.sqrt(result.rsquared)
         R
Out[38]: 0.83514378678237422 <strong> </strong></pre>
<p>Thus, our correlation coefficient is <kbd>R = 0.835</kbd>. This would indicate that about 84 percent of the chirp frequency can be explained by the changes in temperature.</p>
<div class="packt_infobox">The book containing this data, <em>The Song of Insects</em>, can be found at<em> </em><a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674420663"><span class="URLPACKT">http://www.hup.harvard.edu/catalog.php?isbn=9780674420663</span></a>.</div>
<p>For a more in-depth treatment of single and multivariable regression, refer to the following websites:</p>
<ul>
<li><strong>Regression (Part I)</strong>: <a href="http://bit.ly/1Eq5kSx"><span class="URLPACKT">http://bit.ly/1Eq5kSx</span></a></li>
<li><strong>Regression (Part II)</strong>: <a href="http://bit.ly/1OmuFTV"><span class="URLPACKT">http://bit.ly/1OmuFTV</span></a></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we took a brief tour of the classical or frequentist approach to statistics and saw how to combine pandas with the <kbd>numpy</kbd> and <kbd>stats</kbd> packages<span>—</span><kbd>scipy.stats</kbd> and <kbd>statsmodels</kbd><span>—</span>to calculate, interpret, and make inferences from statistical data.</p>
<p>In the next chapter, we will examine an alternative approach to statistics called the Bayesian approach. For a deeper look at the statistics topics that we touched on, take a look at <em>Understanding Statistics in the Behavioral Sciences</em>, which can be found at <a href="http://www.amazon.com/Understanding-Statistics-Behavioral-Sciences-Robert/dp/0495596523"><span class="URLPACKT">http://www.amazon.com/Understanding-Statistics-Behavioral-Sciences-Robert/dp/0495596523</span></a>.</p>


            </article>

            
        </section>
    </body></html>