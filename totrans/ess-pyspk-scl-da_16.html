<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer086">
			<h1 id="_idParaDest-214"><a id="_idTextAnchor214"/>Chapter 13: Integrating External Tools with Spark SQL</h1>
			<p><strong class="bold">Business intelligence</strong> (<strong class="bold">BI</strong>) refers to the capabilities that enable organizations to make informed, data-driven decisions. BI is a combination of data processing capabilities, data visualizations, business analytics, and a set of best practices that enable, refine, and streamline organizations' business processes by helping them in both strategic and tactical decision making. Organizations typically rely on specialist software called BI tools for their BI needs. BI tools combine strategy and technology to gather, analyze, and interpret data from various sources and provide business analytics about the past and present state of a business.</p>
			<p>BI tools have traditionally relied on data warehouses as data sources and data processing engines. However, with the advent of big data and real-time data, BI tools have branched out to using data lakes and other new data storage and processing technologies as data sources. In this chapter, you will explore how Spark SQL can be used as a distributed <strong class="bold">Structured Query Language</strong> (<strong class="bold">SQL</strong>) engine for BI and SQL analysis tools via Spark Thrift <strong class="bold">Java Database Connectivity/Open Database Connectivity</strong> (<strong class="bold">JDBC/ODBC</strong>) Server. Spark SQL connectivity requirements with SQL analysis and BI tools will be presented, along with detailed configuration and setup steps. Finally, the chapter will also present options to connect to Spark SQL from arbitrary Python applications.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Apache Spark as a <strong class="bold">distributed SQL engine</strong></li>
				<li>Spark connectivity to SQL analysis tools</li>
				<li>Spark connectivity to BI tools</li>
				<li>Connecting Python applications to Spark SQL using <strong class="source-inline">pyodbc</strong></li>
			</ul>
			<p>Some of the skills gained in this chapter are an understanding of Spark Thrift JDBC/ODBC Server, how to connect SQL editors and BI tools to Spark SQL via JDBC and Spark Thrift Server, and connecting business applications built using Python with a Spark SQL engine using Pyodbc.</p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor215"/>Technical requirements</h1>
			<p>Here is what you'll need for this chapter:</p>
			<ul>
				<li>In this chapter, we will be using Databricks Community Edition to run our code (<a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a>). Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. </li>
				<li>We will be using a free and open source SQL editor tool called <strong class="bold">SQL Workbench/J</strong>, which can be downloaded from <a href="https://www.sql-workbench.eu/downloads.html">https://www.sql-workbench.eu/downloads.html</a>.</li>
				<li>You will need to download a JDBC driver for SQL Workbench/J to be able to connect with Databricks Community Edition. This can be downloaded from <a href="https://databricks.com/spark/jdbc-drivers-download">https://databricks.com/spark/jdbc-drivers-download</a>.</li>
				<li>We will also be using <strong class="bold">Tableau Online</strong> to demonstrate BI tool integration. You can request a free 14-day Tableau Online trial at <a href="https://www.tableau.com/products/online/request-trial">https://www.tableau.com/products/online/request-trial</a>.</li>
			</ul>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor216"/>Apache Spark as a distributed SQL engine</h1>
			<p>One common application of SQL has been its use with BI and SQL analysis tools. These SQL-based tools connect to a <strong class="bold">relational database management system</strong> (<strong class="bold">RDBMS</strong>) using a <a id="_idIndexMarker940"/>JDBC or ODBC connection and traditional RDBMS JDBC/ODBC connectivity built in. In the previous chapters, you <a id="_idIndexMarker941"/>have seen that Spark SQL can be used using notebooks and intermixed with PySpark, Scala, Java, or R applications. However, Apache Spark can also double up as a powerful and fast distributed SQL engine using a JDBC/OCBC connection or via the command line.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">JDBC is a SQL-based <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) used by Java applications to connect to an RDBMS. Similarly, ODBC is a SQL-based API created by Microsoft to <a id="_idIndexMarker942"/>provide RDBMS access to Windows-based applications. A JDBC/ODBC driver is a client-side software component either developed by the RDBMS vendor themselves or by a third party that can be used with external tools to connect to an RDBMS via the JDBC/ODBC standard.</p>
			<p>In the following section, we will explore how to make use of <strong class="bold">JDBC/ODBC</strong> serving capabilities with Apache Spark.</p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor217"/>Introduction to Hive Thrift JDBC/ODBC Server</h2>
			<p>While the JDBC/ODBC driver gives client-side software such as BI or SQL analytics tools the ability to connect to database servers, some server-side components are also required for the <a id="_idIndexMarker943"/>database server to utilize JDBC/ODBC standards. Most RDBMSes come built with these JDBC/ODBC serving capabilities, and Apache Spark can also be enabled with this server-side functionality using a Thrift JDBC/ODBC server. </p>
			<p><strong class="bold">HiveServer2</strong> is a server-side interface developed to enable Hadoop Hive clients to execute Hive queries against Apache Hive. HiveServer2 has been developed to provide multi-client <a id="_idIndexMarker944"/>concurrency with open APIs such as JDBC and ODBC. HiveServer2 itself is based on Apache Thrift, which is a binary communication protocol used for creating services in multiple programming languages. Spark Thrift Server is Apache Spark's implementation of HiveServer2 that allows JDBC/ODBC clients to execute Spark SQL queries on Apache Spark.</p>
			<p>Spark Thrift Server comes bundled with the Apache Spark distribution, and most Apache Spark vendors enable this service by default on their Spark clusters. In the case of Databricks, this service can be accessed from the <strong class="bold">Clusters</strong> page, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="Images/B16736_13_01.jpg" alt="Figure 13.1 – Databricks JDBC/ODBC interface&#13;&#10;" width="837" height="534"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.1 – Databricks JDBC/ODBC interface</p>
			<p>You can get to the Databricks JDBC/ODBC interface shown in the previous screenshot by navigating to the <strong class="bold">Clusters</strong> page within the Databricks web interface. Then, click on <strong class="bold">Advanced Options</strong> and then on the <strong class="bold">JDBC/ODBC</strong> tab. The Databricks JDBC/ODBC interface provides you with the hostname, port, protocol, the HTTP path, and the actual JDBC URL required by external tools for connectivity to the Databricks Spark cluster. In the following sections, we will <a id="_idIndexMarker945"/>explore how this Spark Thrift Server functionality can be used by external SQL-based clients to utilize Apache Spark as a distributed SQL engine.</p>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor218"/>Spark connectivity to SQL analysis tools</h1>
			<p>SQL analysis tools, as the same suggests, are tools with interfaces suited for quick and easy SQL analysis. They let you connect to an RDBMS, sometimes even multiple RDBMSes, at the same time, <a id="_idIndexMarker946"/>and browse through various databases, schemas, tables, and columns. They even help you visually analyze tables and their structure. They also have interfaces designed to perform SQL analysis quickly with multiple windows that let you browse tables and columns on one side, compose a SQL query in another window, and look at the results in <a id="_idIndexMarker947"/>another window. Once such SQL analysis tool, called <strong class="bold">SQL Workbench/J</strong>, is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="Images/B16736_13_02.jpg" alt="Figure 13.2 – SQL Workbench/J interface&#13;&#10;" width="1579" height="711"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.2 – SQL Workbench/J interface</p>
			<p>The previous screenshot depicts the interface of <strong class="bold">SQL Workbench/J</strong>, which represents a typical SQL editor interface with a database, schema, table, and column browser on the left-hand side pane. The top pane has a text interface for composing actual SQL queries, and the bottom pane shows the results of executed SQL queries and has other tabs to show any error <a id="_idIndexMarker948"/>messages, and so on. There is also a menu and a toolbar on the top to establish connections to databases, toggle between various databases, execute SQL queries, browse databases, save SQL queries, browse through query history, and so on. This type of SQL analysis interface is very intuitive and comes in very handy for quick SQL analysis, as well as for building SQL-based data processing jobs, as databases, tables, and columns can easily be browsed. Tables and column names can be easily dragged and dropped into the <strong class="bold">Query Composer</strong> window, and results can be quickly viewed and analyzed.</p>
			<p>There are, additionally, rather more sophisticated SQL analysis tools that also let you visualize the results of a <a id="_idIndexMarker949"/>query right within the same interface. Some open source tools to name <a id="_idIndexMarker950"/>are <strong class="bold">Redash</strong>, <strong class="bold">Metabase</strong>, and <strong class="bold">Apache Superset</strong>, and some <a id="_idIndexMarker951"/>cloud-native <a id="_idIndexMarker952"/>tools are <strong class="bold">Google Data Studio</strong>, <strong class="bold">Amazon QuickSight</strong>, and so on. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Redash was recently <a id="_idIndexMarker953"/>acquired by Databricks and is available to use with the Databricks paid versions; it is not available in Databricks Community Edition as of this writing.</p>
			<p>Now you have an idea of what SQL analysis tools look like and how they work, let's look at the steps required to connect a SQL analysis tool such as <strong class="bold">SQL Workbench/J</strong> to <strong class="bold">Databricks Community Edition</strong>.</p>
			<p><strong class="bold">SQL Workbench/J</strong> is a free, RDBMS-independent SQL analysis tool based on Java and can <a id="_idIndexMarker954"/>be used with any operating system of your choice. Instructions <a id="_idIndexMarker955"/>on downloading and running <strong class="bold">SQL Workbench/J</strong> can be found here: <a href="https://www.sql-workbench.eu/downloads.html">https://www.sql-workbench.eu/downloads.html</a>.</p>
			<p>Once you have <strong class="bold">SQL Workbench/J</strong> set up and running on your local machine, the following <a id="_idIndexMarker956"/>steps will <a id="_idIndexMarker957"/>help you get it connected with <strong class="bold">Databricks Community Edition</strong>:</p>
			<ol>
				<li value="1">Download the Databricks JDBC driver from <a href="https://databricks.com/spark/jdbc-drivers-download">https://databricks.com/spark/jdbc-drivers-download</a>, and store at a known location.</li>
				<li>Launch <strong class="bold">SQL Workbench/J</strong> and open the <strong class="bold">File</strong> menu. Then, click on <strong class="bold">Connect window</strong>, to take you to the following screen:<div id="_idContainer075" class="IMG---Figure"><img src="Images/B16736_13_03.jpg" alt="Figure 13.3 – SQL Workbench/J connect window&#13;&#10;" width="1034" height="326"/></div><p class="figure-caption">Figure 13.3 – SQL Workbench/J connect window</p></li>
				<li>In <a id="_idIndexMarker958"/>the previous window, click on <strong class="bold">Manage Drivers</strong> to take you to the following screen:<div id="_idContainer076" class="IMG---Figure"><img src="Images/B16736_13_04.jpg" alt="Figure 13.4 – Manage drivers screen&#13;&#10;" width="1263" height="676"/></div><p class="figure-caption">Figure 13.4 – Manage drivers screen</p></li>
				<li>As shown in the preceding <strong class="bold">Manage drivers</strong> window screenshot, click on the folder <a id="_idIndexMarker959"/>icon and navigate to the folder where you stored your previously downloaded Databricks drivers and open it, then click the <strong class="bold">OK</strong> button.</li>
				<li>Now, navigate to your Databricks <strong class="bold">Clusters</strong> page, click on a cluster, and navigate <a id="_idIndexMarker960"/>to <strong class="bold">Advanced Options</strong> and the <strong class="bold">JDBC/ODBC</strong> tab, as shown in the following screenshot:<div id="_idContainer077" class="IMG---Figure"><img src="Images/B16736_13_05.jpg" alt="Figure 13.5 – Databricks cluster JDBC URL&#13;&#10;" width="954" height="610"/></div><p class="figure-caption">Figure 13.5 – Databricks cluster JDBC URL</p><p>As shown <a id="_idIndexMarker961"/>in the previous <a id="_idIndexMarker962"/>screenshot, copy the JDBC URL without the <strong class="source-inline">UID</strong> and <strong class="source-inline">PWD</strong> parts and paste it into the <strong class="bold">URL</strong> field on the connection window of <strong class="bold">SQL Workbench/J</strong>, as shown in the following screenshot:</p><div id="_idContainer078" class="IMG---Figure"><img src="Images/B16736_13_06.jpg" alt="Figure 13.6 – SQL Workbench/J connection parameters&#13;&#10;" width="1579" height="880"/></div><p class="figure-caption">Figure 13.6 – SQL Workbench/J connection parameters</p></li>
				<li>After <a id="_idIndexMarker963"/>entering the required JDBC parameters from the Databricks <strong class="bold">Clusters</strong> page, enter your Databricks username and password in the <strong class="bold">Username</strong> and <strong class="bold">Password</strong> fields on the <strong class="bold">SQL Workbench/J</strong> connection window. Then, click on the <strong class="bold">Test</strong> button to test connectivity to the Databricks clusters. If all the connection <a id="_idIndexMarker964"/>parameters have been correctly provided, you should see a <strong class="bold">Connection Successful</strong> message pop up.<p class="callout-heading">Tip</p><p class="callout">Make sure the Databricks cluster is up and running if you see any connection failures or <strong class="bold">Host Not Found </strong>types of errors.</p></li>
			</ol>
			<p>This way, by following the previous steps, you can successfully connect a SQL analysis tool such as <strong class="bold">SQL Workbench/J</strong> to Databricks clusters and run Spark SQL queries remotely. It is also possible to connect to other Spark clusters running on other vendors' clusters—just make sure to procure the appropriate <strong class="bold">HiveServer2</strong> drivers directly from the vendor. Modern BI <a id="_idIndexMarker965"/>tools also recognize the importance of connecting to big data technologies and data lakes, and in the following section, we will explore how to connect BI tools with Apache Spark via a JDBC connection.</p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor219"/>Spark connectivity to BI tools</h1>
			<p>In the era of big data and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>), Hadoop and Spark have modernized data <a id="_idIndexMarker966"/>warehouses into distributed warehouses that can process up to <strong class="bold">petabytes</strong> (<strong class="bold">PB</strong>) of data. Thus, BI tools have also evolved to utilize <a id="_idIndexMarker967"/>Hadoop- and Spark-based analytical stores as their data sources, connecting to them using JDBC/ODBC. BI tools ranging from Tableau, Looker, Sisense, MicroStrategy, Domo, and so on all feature connectivity support and built-in drivers to Apache Hive and Spark SQL. In this section, we will explore how you can connect a BI tool such as Tableau Online with Databricks Community Edition, via a JDBC connection.</p>
			<p><strong class="bold">Tableau Online</strong> is a BI platform fully hosted in the cloud that lets you perform data analytics, publish <a id="_idIndexMarker968"/>reports and dashboards, and create interactive visualizations, all from a web browser. The following steps describe the process of connecting Tableau Online with Databricks Community Edition:</p>
			<ol>
				<li value="1">If you <a id="_idIndexMarker969"/>already have an existing Tableau Online account, sign in. If not, you can request a free trial here: <a href="https://www.tableau.com/products/online/request-trial">https://www.tableau.com/products/online/request-trial</a>.</li>
				<li>Once <a id="_idIndexMarker970"/>you have logged in, click on the <strong class="bold">New</strong> button near the top right-hand corner, as shown in the following screenshot:<div id="_idContainer079" class="IMG---Figure"><img src="Images/B16736_13_07.jpg" alt="Figure 13.7 – Tableau Online new workbook&#13;&#10;" width="1224" height="446"/></div><p class="figure-caption">Figure 13.7 – Tableau Online new workbook</p></li>
				<li>The newly <a id="_idIndexMarker971"/>created workbook will prompt you to <strong class="bold">Connect to Data</strong>. Click on the <strong class="bold">Connectors</strong> tab and choose <strong class="bold">Databricks</strong> from the list of available data sources, as shown in the following screenshot:<div id="_idContainer080" class="IMG---Figure"><img src="Images/B16736_13_08.jpg" alt="Figure 13.8 – Tableau Online data sources&#13;&#10;" width="1586" height="872"/></div><p class="figure-caption">Figure 13.8 – Tableau Online data sources</p></li>
				<li>Then, provide <a id="_idIndexMarker972"/>Databricks cluster details such as <strong class="bold">Server Hostname</strong>, <strong class="bold">HTTP Path</strong>, <strong class="bold">Authentication</strong>, <strong class="bold">Username</strong>, and <strong class="bold">Password</strong>, as shown in the following screenshot, and click the <strong class="bold">Sign In</strong> button. These details are found on the Databricks <strong class="bold">Clusters</strong> page:<div id="_idContainer081" class="IMG---Figure"><img src="Images/B16736_13_09.jpg" alt="Figure 13.9 – Tableau Online Databricks connection&#13;&#10;" width="501" height="450"/></div><p class="figure-caption">Figure 13.9 – Tableau Online Databricks connection</p></li>
				<li>Once <a id="_idIndexMarker973"/>your connection is successful, your new workbook will open in the <strong class="bold">Data Source</strong> tab, where <a id="_idIndexMarker974"/>you can browse through your existing databases and tables, as shown in the following screenshot:<div id="_idContainer082" class="IMG---Figure"><img src="Images/B16736_13_10.jpg" alt="Figure 13.10 – Tableau Online data sources&#13;&#10;" width="1263" height="794"/></div><p class="figure-caption">Figure 13.10 – Tableau Online data sources</p></li>
				<li>The <strong class="bold">Data Source</strong> tab <a id="_idIndexMarker975"/>also lets you drag and drop tables and define relationships and <a id="_idIndexMarker976"/>joins among tables as well, as shown in the following screenshot:<div id="_idContainer083" class="IMG---Figure"><img src="Images/B16736_13_11.jpg" alt="Figure 13.11 – Tableau Online: defining table joins&#13;&#10;" width="548" height="129"/></div><p class="figure-caption">Figure 13.11 – Tableau Online: defining table joins</p></li>
				<li>Tableau Online data sources also let you create two types of connections with the underlying data sources—a live connection that queries the underlying data sources <a id="_idIndexMarker977"/>with every request and an option to create an <strong class="bold">Extract</strong>, which extracts data from the data sources and stores it within Tableau. With big data sources such as Apache <a id="_idIndexMarker978"/>Spark, it is recommended to create a live connection because the amount of data being queried could be substantially larger than usual.</li>
				<li>Sample data can also be browsed in the <strong class="bold">Data Source</strong> tab by clicking on the <strong class="bold">Update Now</strong> button, as shown in the following screenshot:<div id="_idContainer084" class="IMG---Figure"><img src="Images/B16736_13_12.jpg" alt="Figure 13.12 – Tableau Online data source preview&#13;&#10;" width="1241" height="279"/></div><p class="figure-caption">Figure 13.12 – Tableau Online data source preview</p></li>
				<li>Once the data source connection is established, you can move on to visualizing the <a id="_idIndexMarker979"/>data and creating <a id="_idIndexMarker980"/>reports and dashboards by clicking on <strong class="source-inline">Sheet1</strong> or by creating new additional sheets.</li>
				<li>Once you are within a sheet, you can start visualizing data by dragging and dropping columns from the <strong class="bold">Data</strong> pane onto the blank sheet.</li>
				<li>Tableau automatically chooses the appropriate visualization based on the fields selected. The visualization can also be changed using the <strong class="bold">Visualization</strong> drop-down selector, and data filters can be defined using the <strong class="bold">Filters</strong> box. Aggregations can be defined on metrics in the <strong class="bold">Columns</strong> field, and columns can also be defined as <strong class="bold">Dimension</strong>, <strong class="bold">Attribute</strong>, or <strong class="bold">Metric</strong> as required. The top menu has additional settings to sort and pivot data and has other formatting options. There are also advanced analytics options such as defining quartiles, medians, and so on available within Tableau Online. This way, using Tableau Online with its <a id="_idIndexMarker981"/>built-in Databricks connector data can be analyzed at scale using the power and efficiency of Apache Spark, along with the ease of use and <strong class="bold">graphical user interface</strong> (<strong class="bold">GUI</strong>) of a prominent BI tool such as Tableau Online, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="Images/B16736_13_13.jpg" alt="Figure 13.13 – Tableau Online worksheet with visualizations&#13;&#10;" width="1194" height="914"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.13 – Tableau Online worksheet with visualizations</p>
			<p><strong class="bold">Tableau Online</strong> is one of the many popular BI tools that support native Databricks connectivity<a id="_idIndexMarker982"/> out of the box. Modern BI tools also offer Spark SQL connectivity options for connecting to Apache Spark distributions outside of Databricks.</p>
			<p>Connectivity to Apache Spark is not just limited to SQL analysis and BI tools. Since the JDBC protocol is <a id="_idIndexMarker983"/>based on Java and was meant to be used by Java-based applications, any applications built using <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>)-based programming languages such as <strong class="bold">Java</strong> or <strong class="bold">Scala</strong> can also make use of the JDBC connectivity options to Apache Spark.</p>
			<p>But what about applications based on popular programming languages such as Python that are not based on Java? There is a way to connect these types of Python applications to Apache Spark via <strong class="bold">Pyodbc</strong>, and we will explore this in the following section.</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor220"/>Connecting Python applications to Spark SQL using Pyodbc</h1>
			<p><strong class="bold">Pyodbc</strong> is an open source Python module for connecting Python applications to data sources using an <a id="_idIndexMarker984"/>ODBC connection. Pyodbc can be used with any of your local Python applications to connect to Apache Spark via an ODBC driver and access <a id="_idIndexMarker985"/>databases and tables defined with Apache Spark SQL. In this section, we will explore how you can connect Python running on your local machine to a Databricks cluster using <strong class="bold">Pyodbc</strong> with the following steps:</p>
			<ol>
				<li value="1">Download and install the Simba ODBC driver provided by Databricks on your local machine from here: <a href="https://databricks.com/spark/odbc-drivers-download">https://databricks.com/spark/odbc-drivers-download</a>.</li>
				<li>Install Pyodbc on your local machine's Python using <strong class="source-inline">pip</strong>, as shown in the following command:<p class="source-code">sudo pip install pyodbc</p></li>
				<li>Create a new Python file using a text editor of your choice and paste the following code into it:<p class="source-code">import pyodbc</p><p class="source-code">odbc_conn = pyodbc.connect("Driver /Library/simba/spark/lib/libsparkodbc_sbu.dylib;" +</p><p class="source-code">                      "HOST=community.cloud.databricks.com;" +</p><p class="source-code">                      "PORT=443;" +</p><p class="source-code">                      "Schema=default;" +</p><p class="source-code">                      "SparkServerType=3;" +</p><p class="source-code">                      "AuthMech=3;" +</p><p class="source-code">                      "UID=username;" +</p><p class="source-code">                      "PWD=password;" +</p><p class="source-code">                      "ThriftTransport=2;" +</p><p class="source-code">                      "SSL=1;" +</p><p class="source-code">                      "HTTPPath= sql/protocolv1/o/4211598440416462/0727-201300-wily320",</p><p class="source-code">                      autocommit=True)</p><p class="source-code">cursor = odbc_conn.cursor()</p><p class="source-code">cursor.execute(f"SELECT * FROM retail_features LIMIT 5")</p><p class="source-code">for row in cursor.fetchall():</p><p class="source-code">  print(row)</p></li>
				<li>The <a id="_idIndexMarker986"/>driver paths for the previous code configuration may vary based on your operating systems and are given as follows:<p class="source-code">macOS: /Library/simba/spark/lib/libsparkodbc_sbu.dylib</p><p class="source-code">Linux 64-bit: /opt/simba/spark/lib/64/libsparkodbc_sb64.so</p><p class="source-code">Linux 32-bit: /opt/simba/spark/lib/32/libsparkodbc_sb32.so</p></li>
				<li>The values for hostname, port, and the HTTP path can be obtained from your Databricks cluster <strong class="bold">JDBC/ODBC</strong> tab.</li>
				<li>Once you have added all the code and put in the appropriate configuration values, save the Python code file and name it <strong class="source-inline">pyodbc-databricks.py</strong>.</li>
				<li>You can now execute the code from your Python interpreter using the following command:<p class="source-code">python pyodbd-databricks.py</p></li>
				<li>Once the code runs successfully, the first five rows of the table you specified in your SQL query will be displayed on the console.<p class="callout-heading">Note</p><p class="callout">Instructions on configuring the ODBC driver on a Microsoft Windows machine to be used with <a id="_idIndexMarker987"/>Pyodbc can be found on the Databricks public documentation page here: <a href="https://docs.databricks.com/dev-tools/pyodbc.html#windows">https://docs.databricks.com/dev-tools/pyodbc.html#windows</a>.</p></li>
			</ol>
			<p>This way, using <strong class="bold">Pyodbc</strong>, you can integrate Apache Spark SQL into any of your Python applications that may be running locally on your machine or some remote machine in the cloud or a data center somewhere, but still take advantage of the fast and powerful distributed SQL engine that comes with Apache Spark.</p>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor221"/>Summary</h1>
			<p>In this chapter, you have explored how you can take advantage of Apache Spark's Thrift server to enable JDBC/ODBC connectivity and use Apache Spark as a distributed SQL engine. You learned how the HiveServer2 service allows external tools to connect to Apache Hive using JDBC/ODBC standards and how Spark Thrift Server extends HiveServer2 to enable similar functionality on Apache Spark clusters. Steps required for connecting SQL analysis tools such as SQL Workbench/J were presented in this chapter, along with detailed instructions required for connecting BI tools such as Tableau Online with Spark clusters. Finally, steps required for connecting arbitrary Python applications, either locally on your machine or on remote servers in the cloud or a data center, to Spark clusters using Pyodbc were also presented. In the following and final chapter of this book, we will explore the Lakehouse paradigm that can help organizations seamlessly cater to all three workloads of data analytics—data engineering, data science, and SQL analysis—using a single unified distributed and persistent storage layer that combines the best features of both data warehouses and data lakes.</p>
		</div>
	</div></body></html>