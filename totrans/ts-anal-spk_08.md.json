["```py\n# Initialize metadata object for the dataset\nmetadata = SingleTableMetadata()\n# Automatically detect and set the metadata from the Pandas DataFrame\nmetadata.detect_from_dataframe(pdf_main)\n# Initialize the Gaussian Copula Synthesizer with the dataset metadata\nsynthesizer = GaussianCopulaSynthesizer(metadata)\n# Fit the synthesizer model to the Pandas DataFrame\nsynthesizer.fit(pdf_main)\n…\n# Define the number of customer datasets to generate:\nnum_customers = 5\n# Count the number of rows in the original dataset:\nsample_size = df_main.count()\ni = 1\ndf_all = df_main.withColumn(\n    'cust_id', F.lit(i)\n) # Add a 'cust_id' column to the original dataset with a constant \n# value of 1\n…\n    synthetic_data = spark.createDataFrame(\n        synthesizer.sample(num_rows=sample_size)\n) # Generate synthetic data matching the original dataset's size\n…\n```", "```py\nfrom pyspark.sql import functions as F\n# Combine 'Date' and 'Time' into a single 'Date' column of timestamp \n# type\ndf_all = df_all.withColumn(\n    'Date',\n    F.to_timestamp(\n        F.concat_ws(' ', F.col('Date'), F.col('Time')),\n        'd/M/yyyy HH:mm:ss')\n)...\n# Select only the 'cust_id', 'Date' and 'Global_active_power' columns\ndf_all = df_all.select(\n    'cust_id', 'Date', 'Global_active_power'\n)\n# Replace '?' with None and convert 'Global_active_power' to float\ndf_all = df_all.withColumn(\n    'Global_active_power',\n    F.when(F.col('Global_active_power') == '?', None)\n    .otherwise(F.regexp_replace(\n        'Global_active_power', ',', '.').cast('float')\n    )\n)\n# Sort the DataFrame based on 'cust_id' and 'Date'\ndf_all = df_all.orderBy('cust_id', 'Date')\n```", "```py\nfrom pyspark.sql import functions as F\n# Convert the 'Date' column to a string representing the\n# start of the hour for each timestamp\ndata_hr = df_all.withColumn(\n    'Date',\n    F.date_format('Date', 'yyyy-MM-dd HH:00:00'))\n# Group the data by 'cust_id' and the hourly 'Date',\n# then calculate the mean 'Global_active_power' for each group\ndata_hr = data_hr.groupBy(\n    'cust_id', 'Date').agg(\n    F.mean('Global_active_power').alias('Global_active_power')\n)\n```", "```py\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\n# Define a window specification partitioned by -\n# 'cust_id' and ordered by the 'Date' column\nwindowSpec = Window.partitionBy(\"cust_id\").orderBy(\"Date\")\n# Add lagged features to the DataFrame to incorporate\n#  past values as features for forecasting\n# Apply the lag function to create the lagged column,\n#  separately for each 'cust_id'\n# Lag by 1, 2, 3, 4, 5, 12, 24, 168 hours (24 hours * 7 days)\nlags = [1, 2, 3, 4, 5, 12, 24, 24*7]\nfor l in lags:\n    data_hr = data_hr.withColumn(\n        'Global_active_power_lag' + str(l),\n        F.lag(F.col('Global_active_power'), l).over(windowSpec))\n# Remove rows with NaN values that were introduced by\n#  shifting (lagging) operations\ndata_hr = data_hr.dropna()\n```", "```py\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_percentage_error\nimport optuna\ndef objective(trial):\n    # Define the hyperparameter configuration space\n    params = {\n        # Specify the learning task and\n        #  the corresponding learning objective:\n        \"objective\": \"regression\",\n        # Evaluation metric for the model performance:\n        \"metric\": \"rmse\",\n        # Number of boosted trees to fit:\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n        # Learning rate for gradient descent:\n        \"learning_rate\": trial.suggest_float(\n            \"learning_rate\", 0.001, 0.1, log=True),\n        # Maximum tree leaves for base learners:\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 100),\n    }\n    # Initialize the LightGBM model with the trial's parameters:\n    model = lgb.LGBMRegressor(**params)\n    # Train the model with the training dataset:\n    model.fit(X_train, y_train)\n    # Generate predictions for the validation dataset:\n    y_pred = model.predict(X_test)\n    # Calculate the Mean Absolute Percentage Error (MAPE)\n    #  for model evaluation:\n    mape = mean_absolute_percentage_error(y_test, y_pred)\n    # Return the MAPE as the objective to minimize\n    return mape\n```", "```py\nfrom joblibspark import register_spark\n# This line registers Apache Spark as the backend for\n# parallel computing with Joblib, enabling distributed\n# computing capabilities for Joblib-based parallel tasks.\nregister_spark()\n…\n# Create a new study object with the goal of minimizing the objective # function\nstudy2 = optuna.create_study(direction='minimize')\n# Set Apache Spark as the backend for parallel execution of –\n# trials with unlimited jobs\nwith joblib.parallel_backend(\"spark\", n_jobs=-1):\n    # Optimize the study by evaluating the –\n    #  objective function over 10 trials:\n    study2.trial.value) and parameters (trial.params) for best_trial:\n\n```", "```py\n\n The outcome of the hyperparameter tuning, shown in *Figure 8**.2*, is the best hyperparameters found within the search space specified, as well as the related model accuracy.\n![](img/B18568_08_2.jpg)\n\nFigure 8.2: Hyperparameter tuning – best trials\nIn addition to the scaling of the hyperparameter tuning stage, which we have seen in this section, Spark clusters can also be used to parallelize the next step, that is, fitting the model to the training data. We will cover this next.\nSingle model in parallel\nEnsemble methods such as Random Forest and gradient boosting machines can benefit from task parallelism during the model training stage. Each tree in a Random Forest can be trained independently, making it possible to parallelize across multiple processors. Similarly in the case of Gradient Boosting models such as LightGBM and XGBoost, the tree’s construction can be parallelized, even though the boosting itself is sequential,\nIn [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133)’s example in the *Classical machine learning model* section, we used LightGBM. This model was not Spark enabled. Here, as we want to demonstrate training parallelism with a Spark-enabled Gradient Boosting model, we will use `SparkXGBRegressor` instead.\nAs a first step, we will build a vector of the features using `VectorAssember`, as shown in the following code:\n\n```", "```py\n\n We then create the `SparkXGBRegressor` model object, setting `num_workers` to all available workers, and specifying the target column with `label_col`:\n\n```", "```py\n\n As we have seen so far, hyperparameter tuning is an important step in finding the best model. In the following code example, we will use `ParamGridBuilder` to specify the range of parameters that are specific to the model and that we want to evaluate.\nWe then pass the parameters to `CrossValidator` together with `RegressionEvaluator`. We will use the root mean square error (`rmse`) as the evaluation metric. This is the default metric for `RegressionEvaluator`, making it suitable for our example here:\n\n```", "```py\n\n At this point, we are ready to build a pipeline (`Pipeline`) to train (`fit`) the model.  We will do this by combining in sequence the `VectorAssembler` (`assembler`) and `CrossValidator` (`cv`) stages:\n\n```", "```py\n# Filter the dataset for customer with cust_id equal to 1\ntrain_hr = data_hr.filter('cust_id == 1')\n# Create a Spark DataFrame excluding the last 48 records for training\ntrain_hr = spark.createDataFrame(\n    train_hr.head(train_hr.count() - 48)\n)\n# Fit the pipeline model to the training data\npipelineModel = pipeline.fit(train_hr)\n```", "```py\n# Filter the dataset for customer with cust_id equal to 1 for testing\ntest_hr = data_hr.filter('cust_id == 1')\n# Create a Spark DataFrame including the last 48 records for testing\ntest_hr = spark.createDataFrame(train_hr.tail(48))\n…\n# Apply the trained pipeline model to the test data to generate \n# predictions\npredictions = RegressionEvaluator (the evaluator object) to calculate (the evaluate function) the RMSE:\n\n```", "```py\n\n For comparison, we also calculate the **Symmetric Mean Absolute Percentage Error** (**SMAPE**) and **Weighted Average Percentage Error** (**WAPE**) similarly to how we have done in the *Classical machine learning model* section of [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133). The results are shown in *Figure 8**.3*.\n![](img/B18568_08_3.jpg)\n\nFigure 8.3: XGBoost evaluation metrics\nWe plot the forecast against the actual values in *Figures 8.4* and *8.5*.\n![](img/B18568_08_4.jpg)\n\nFigure 8.4: XGBoost forecast versus actuals (training and testing)\nWe zoom in on the testing period in *Figure 8**.5* for a visual comparison of the forecast and actuals.\n![](img/B18568_08_5.jpg)\n\nFigure 8.5: XGBoost forecast versus actuals (zoom on test data)\nIn this section, we have seen parallelism in single-model training. This requires the use of a library, such as XGBoost used here, which supports a multi-node processing backend such as Apache Spark. In addition to ensemble methods, other models, such as deep learning, can benefit from training parallelism.\nMultiple models can also be trained in parallel, which we will explore next.\nMultiple models in parallel\nEarlier in this chapter, we scaled the dataset to represent the household energy consumption of multiple customers. In this section, we will train a different machine learning model for each customer in parallel. This is required if we want to predict the energy consumption of individual customers based on their own historical consumption. There are several other use cases where such multi-model training is required, for example, in the retail industry when doing sales forecasting for individual products or stores.\nComing back to our energy consumption example, the `train_model` function does the following for each customer:\n\n1.  Get the customer ID (`cust_id`) from the pandas DataFrame passed as input.\n2.  Choose the features (`X`) and target (`y`) variables.\n3.  Split (`train_test_split`) the dataset into training and testing, specifying `shuffle` as `False` to preserve the time order. As discussed in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016), this is an important consideration for time-series datasets.\n4.  Perform hyperparameter tuning with `GridSearchCV` using `LGBMRegressor` as the model and `TimeSeriesSplit` for the dataset splits.\n5.  Train (`fit`) the final model with the best hyperparameters (`best_params`) on the full training dataset.\n6.  Test the final model on the test dataset and calculate the evaluation metrics (`rmse` and `mape`).\n7.  Return the result of `train_model` in a DataFrame with `cust_id`, `best_params`, `rmse`, and `mape`.\n\nThe following code shows the function definition with the preceding steps:\n\n```", "```py\n\n Now that the model training function is defined, we can launch it in parallel for each customer (the `groupBy` function), passing a pandas DataFrame of all the rows for this specific customer to the `applyInPandas` function.\npandas UDFs, mapInPandas, and applyInPandas\nUsing Spark-enabled libraries, as we did in the previous section with single-model parallel training, is usually faster for large datasets than single-machine libraries. There are, however, cases when we have to use a library that isn’t implemented natively for Spark’s parallel processing. In these situations, we can use pandas `mapInPandas`, or `applyInPandas`. These methods allow you to call pandas operations in a distributed way from Spark. The common use cases are as follows:\n- **pandas UDF**: One input row for one output row\n- **mapInPandas**: One input row for multiple output rows\n- **applyInPandas**: Multiple input rows for one output row\nNote that these are general guidance and that there is great flexibility in how these methods can be used.\nIn the example in this section, we use `applyInPandas` as we want to execute a pandas-enabled function for all the rows in the dataset corresponding to a specific customer for model training. We want the function to output one row with the result of model training for the specific customer.\nNote how, in the following code extract, we specified the `train_model_result_schema` schema of the function’s return value. This is a requirement for serializing the result that is added to the `train``_model_result_df` pandas DataFrame:\n\n```", "```py\n\n *Figure 8**.6* shows the outcome of the multi-model training. It shows the best hyperparameters (the `best_params` column) and evaluation metrics (the `rmse` and `mape` columns) for each customer.\n![](img/B18568_08_6.jpg)\n\nFigure 8.6: Multi-model training – best hyperparameters and evaluation metrics\nWith this example, we have trained five different models representing different customers. We have found the best hyperparameters to use for each model, which we are then able to use to do individual energy consumption forecasting.\nWith this, we conclude the different ways in which we can leverage Apache Spark to scale time-series analysis. Next, we will discuss some of the ways that the training process can be optimized.\nTraining optimization\nWhen training machine learning models at a large scale, several inefficiencies and overheads can impact resource utilization and performance. These include the following:\n\n*   Idle time waiting for resources such as GPU, network, and storage accesses, which can delay the training process.\n*   Frequent checkpointing, which saves the model during training to avoid restarting in case of failure. This results in additional storage and time during model training.\n*   Hardware or software failures during the training result in restarts, which waste resources and delay the training.\n\nThe following mitigation techniques can be used, depending on the model being trained and the library in use:\n\n*   Eliminate the cause of idle wait times by provisioning sufficient compute, network, and storage resources\n*   Avoid too frequent checkpointing\n*   Rearrange features based on correlation with the target variable or their importance to facilitate convergence during model training\n*   Reduce the dimensionality of the dataset, choosing the most informative features\n\nWhile the implementation details of these techniques are beyond our scope here, we recommend researching and addressing these points when operating at a large scale due to the potentially high impact on cost, efficiency, and scalability.\nSummary\nIn this chapter, we saw the need to scale the processing capacity for bigger datasets. We examined different ways of using Apache Spark to this end. Building on and extending the code examples from [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133), we focused on scaling the feature engineering and model training stages. We looked at leveraging Spark to scale transformations, aggregations, lag values calculation, hyperparameter tuning, and single- and multi-model training in parallel.\nIn the next chapter, we will cover the considerations for going to production with time-series analysis, using and extending what we have learned so far.\nJoin our community on Discord\nJoin our community’s Discord space for discussions with the authors and other readers:\n[https://packt.link/ds](https://packt.link/ds)\n![](img/ds_(1).jpg)\n\n```", "```py\n\n```"]