<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer004">
			<h1 id="_idParaDest-5"><a id="_idTextAnchor004"/>Preface</h1>
			<p>Apache Spark is a unified data analytics engine designed to process huge volumes of data in a fast and efficient way. PySpark is the Python language API of Apache Spark that provides Python developers an easy-to-use scalable data analytics framework.</p>
			<p><em class="italic">Essential PySpark for Scalable Data Analytics</em> starts by exploring the distributed computing paradigm and provides a high-level overview of Apache Spark. You'll then begin your data analytics journey with the data engineering process, learning to perform data ingestion, data cleansing, and integration at scale.</p>
			<p>This book will also help you build real-time analytics pipelines that enable you to gain insights much faster. Techniques for building cloud-based data lakes are presented along with Delta Lake, which brings reliability and performance to data lakes. </p>
			<p>A newly emerging paradigm called the Data Lakehouse is presented, which combines the structure and performance of a data warehouse with the scalability of cloud-based data lakes. You'll learn how to perform scalable data science and machine learning using PySpark, including data preparation, feature engineering, model training, and model productionization techniques. Techniques to scale out standard Python machine learning libraries are also presented, along with a new pandas-like API on top of PySpark called Koalas.</p>
			<h1 id="_idParaDest-6"><a id="_idTextAnchor005"/>Who this book is for</h1>
			<p>This book is intended for practicing data engineers, data scientists, data analysts, citizen data analysts, and data enthusiasts who are already using data analytics to delve into the world of distributed and scalable data analytics. It's recommended that you have knowledge of the field of data analytics and data manipulation to gain actionable insights.</p>
			<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>What this book covers</h1>
			<p><a href="B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Distributed Computing Primer</em>, introduces the distributed computing paradigm. It also talks about how distributed computing became a necessity with the ever-increasing data sizes over the last decade and ends with the in-memory data-parallel processing concept with the Map Reduce paradigm, and finally, contains introduction to the latest features in Apache Spark 3.0 engine.</p>
			<p><a href="B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Ingestion</em>, covers various data sources, such as databases, data lakes, message queues, and how to ingest data from these data sources. You will also learn about the uses, differences, and efficiency of various data storage formats at storing and processing data.</p>
			<p><a href="B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Cleansing and Integration</em>, discusses various data cleansing techniques, how to handle bad incoming data, data reliability challenges and how to cope with them, and data integration techniques to build a single integrated view of the data.</p>
			<p><a href="B16736_04_Final_JM_ePub.xhtml#_idTextAnchor075"><em class="italic">Chapter 4</em></a>, <em class="italic">Real-time Data Analytics</em>, explains how to perform real-time data ingestion and processing, discusses the unique challenges that real-time data integration presents and how to overcome, and also the benefits it provides.</p>
			<p><a href="B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094"><em class="italic">Chapter 5</em></a>, <em class="italic">Scalable Machine Learning with PySpark</em>, briefly talks about the need to scale out machine learning and discusses various techniques available to achieve this from using natively distributed machine learning algorithms to embarrassingly parallel processing to distributed hyperparameter search. It also provides an introduction to PySpark MLlib library and an overview of its various distributed machine learning algorithms.</p>
			<p><a href="B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Feature Engineering â€“ Extraction, Transformation, and Selection</em>, explores various techniques for converting raw data into features that are suitable to be consumed by machine learning models, including techniques for scaling, transforming features.</p>
			<p><a href="B16736_07_Final_JM_ePub.xhtml#_idTextAnchor128"><em class="italic">Chapter 7</em></a>, <em class="italic">Supervised Machine Learning</em>, explores supervised learning techniques for machine learning classification and regression problems including linear regression, logistic regression, and gradient boosted trees.</p>
			<p><a href="B16736_08_Final_JM_ePub.xhtml#_idTextAnchor150"><em class="italic">Chapter 8</em></a>, <em class="italic">Unsupervised Machine Learning</em>, covers unsupervised learning techniques such as clustering, collaborative filtering, and dimensionality reduction to reduce the number of features prior to applying supervised learning.</p>
			<p><a href="B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164"><em class="italic">Chapter 9</em></a>, <em class="italic">Machine Learning Life Cycle Management</em>, explains that it is not just sufficient to just build and train models, but in the real world, multiple versions of the same model are built and different versions are suitable for different applications. Thus, it is necessary to track various experiments, their hyperparameters, metrics, and also the version of the data they were trained on. It is also necessary to track and store the various models in a centrally accessible repository so models can be easily productionized and shared; and finally, mechanisms are needed to automate this repeatedly occurring process. This chapter introduces these techniques using an end-to-end open source machine learning life cycle management library called MLflow.</p>
			<p><a href="B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176"><em class="italic">Chapter 10</em></a><em class="italic">, Scaling Out Single-Node Machine Learning Using PySpark</em>, explains that in <a href="B16736_05_Final_JM_ePub.xhtml#_idTextAnchor094"><em class="italic">Chapter 5</em></a><em class="italic">, Scalable Machine Learning with PySpark</em>, you learned how to use the power of Apache Spark's distributed computing framework to train and score machine learning models at scale. Spark's native machine learning library provides good coverage of standard tasks that data scientists typically perform; however, there is a wide variety of functionality provided by standard single-node Python libraries that were not designed to work in a distributed manner. This chapter deals with techniques for horizontally scaling out standard Python data processing and machine learning libraries such as pandas, scikit-learn, and XGBoost. This chapter covers scaling out typical data science tasks such as exploratory data analysis, model training, model inference, and finally also covers a scalable Python library named Koalas that lets you effortlessly write PySpark code using very familiar and easy-to-use pandas-like syntax.</p>
			<p><a href="B16736_11_Final_JM_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 11</em></a><em class="italic">,</em> <em class="italic">Data Visualization with PySpark</em>, covers data visualizations, which are an important aspect of conveying meaning from data and gleaning insights into it. This chapter covers how the most popular Python visualization libraries can be used along with PySpark.</p>
			<p><a href="B16736_12_Final_JM_ePub.xhtml#_idTextAnchor199"><em class="italic">Chapter 12</em></a><em class="italic">, Spark SQL Primer</em>, covers SQL, which is an expressive language for ad hoc querying and data analysis. This chapter will introduce Spark SQL for data analysis and also show how to interchangeably use PySpark with data analysis.</p>
			<p><a href="B16736_13_Final_JM_ePub.xhtml#_idTextAnchor214"><em class="italic">Chapter 13</em></a><em class="italic">, Integrating External Tools with Spark SQL</em>, explains that once we have clean, curated, and reliable data in our performant data lake, it would be a missed opportunity to not democratize this data across the organization to citizen analysts. The most popular way of doing this is via various existing <strong class="bold">Business Intelligence</strong> (<strong class="bold">BI</strong>) tools. This chapter deals with requirements for BI tool integration.</p>
			<p><a href="B16736_14_Final_JM_ePub.xhtml#_idTextAnchor222"><em class="italic">Chapter 14</em></a><em class="italic">, The Data Lakehouse</em>, explains that traditional descriptive analytics tools such as BI tools are designed around data warehouses and expect data to be presented in a certain way and modern advanced analytics and data science tools are geared toward working with large amounts of data that's easily accessible in data lakes. It is also not practical or cost-effective to store redundant data in separate storage locations to be able to cater to these individual use cases. This chapter will present a new paradigm called Data Lakehouse that tries to overcome the limitations of data warehouses and data lakes and bridge the gap by combining the best elements of both.</p>
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>To get the most out of this book</h1>
			<p>Basic to intermediate knowledge of the disciplines of data engineering, data science, and SQL analytics is expected. A general level of proficiency using any programming language, especially Python, and a working knowledge of performing data analytics using frameworks such as pandas and SQL will help you to get the most out of this book.</p>
			<div>
				<div id="_idContainer003" class="IMG---Figure">
					<img src="Images/B16736_Preface_Table_01.jpg" alt="" width="1441" height="283"/>
				</div>
			</div>
			<p>The book makes use of Databricks Community Edition to run all code: <a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a>. Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. </p>
			<p>The entire code base used in this book can be downloaded from <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/all_chapters/ess_pyspark.dbc">https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/all_chapters/ess_pyspark.dbc</a>.</p>
			<p>The datasets used for this chapter can be found at <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data">https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data</a>.</p>
			<p><strong class="bold">If you are using the digital version of this book, we advise you to type the code yourself or access the code from the book's GitHub repository (a link is available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting of code.</strong></p>
			<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>Download the example code files</h1>
			<p>You can download the example code files for this book from GitHub at <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics">https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics</a>. If there's an update to the code, it will be updated in the GitHub repository.</p>
			<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check them out!</p>
			<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>Download the color images</h1>
			<p>We also provide a PDF file that has color images of the screenshots and diagrams used in this book. You can download it here: <a href="_ColorImages.pdf">https://static.packt-cdn.com/downloads/9781800568877_ColorImages.pdf</a></p>
			<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>Conventions used</h1>
			<p>There are a number of text conventions used throughout this book.</p>
			<p><strong class="source-inline">Code in text</strong>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: "The <strong class="source-inline">readStream()</strong> method of the DataStreamReader object is used to create the streaming DataFrame."</p>
			<p>A block of code is set as follows:</p>
			<p class="source-code">lines = sc.textFile("/databricks-datasets/README.md") </p>
			<p class="source-code">words = lines.flatMap(lambda s: s.split(" ")) </p>
			<p class="source-code">word_tuples = words.map(lambda s: (s, 1)) </p>
			<p class="source-code">word_count = word_tuples.reduceByKey(lambda x, y: x + y) </p>
			<p class="source-code">word_count.take(10) </p>
			<p class="source-code">word_count.saveAsTextFile("/tmp/wordcount.txt") </p>
			<p>Any command-line input or output is written as follows:</p>
			<p class="source-code">%fs ls /FileStore/shared_uploads/delta/online_retail</p>
			<p><strong class="bold">Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For instance, words in menus or dialog boxes appear in <strong class="bold">bold</strong>. Here is an example: "There can be multiple <strong class="bold">Map</strong> stages followed by multiple <strong class="bold">Reduce</strong> stages."</p>
			<p class="callout-heading">Tips or important notes</p>
			<p class="callout">Appear like this.</p>
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Get in touch</h1>
			<p>Feedback from our readers is always welcome.</p>
			<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, email us at <strong class="source-inline">customercare@packtpub.com</strong> and mention the book title in the subject of your message.</p>
			<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a> and fill in the form.</p>
			<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <strong class="source-inline">copyright@packt.com</strong> with a link to the material.</p>
			<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com">authors.packtpub.com</a>.</p>
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Share your thoughts</h1>
			<p>Once you've read <em class="italic">Essential PySpark for Scalable Data Analytics</em>, we'd love to hear your thoughts! Please <a href="https://packt.link/r/1-800-56887-8">https://packt.link/r/1-800-56887-8</a> for this book and share your feedback.</p>
			<p>Your review is important to us and the tech community and will help us make sure we're delivering excellent quality content.</p>
		</div>
	</div></body></html>