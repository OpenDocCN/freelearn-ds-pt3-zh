["```py\nwget -c ftp://ngs.sanger.ac.uk/production/ag1000g/phase1/AR3/variation/main/hdf5/ag1000g.phase1.ar3.pass.3L.h5\nwget -c ftp://ngs.sanger.ac.uk/production/ag1000g/phase1/AR3/variation/main/hdf5/ag1000g.phase1.ar3.pass.2L.h5\n```", "```py\n    import pickle\n    import gzip\n    import random\n    import numpy as np\n    import h5py\n    import pandas as pd\n    ```", "```py\n    samples = pd.read_csv('samples.tsv', sep='\\t')\n    print(len(samples))\n    print(samples['cross'].unique())\n    print(samples[samples['cross'] == 'cross-29-2'][['id', 'function']])\n    print(len(samples[samples['cross'] == 'cross-29-2']))\n    print(samples[samples['function'] == 'parent'])\n    ```", "```py\n    h5_3L = h5py.File('ag1000g.crosses.phase1.ar3sites.3L.h5', 'r')\n    samples_hdf5 = list(map(lambda sample: sample.decode('utf-8'), h5_3L['/3L/samples']))\n    calldata_genotype = h5_3L['/3L/calldata/genotype']\n    MQ0 = h5_3L['/3L/variants/MQ0']\n    MQ = h5_3L['/3L/variants/MQ']\n    QD = h5_3L['/3L/variants/QD']\n    Coverage = h5_3L['/3L/variants/Coverage']\n    CoverageMQ0 = h5_3L['/3L/variants/CoverageMQ0']\n    HaplotypeScore = h5_3L['/3L/variants/HaplotypeScore']\n    QUAL = h5_3L['/3L/variants/QUAL']\n    FS = h5_3L['/3L/variants/FS']\n    DP = h5_3L['/3L/variants/DP']\n    HRun = h5_3L['/3L/variants/HRun']\n    ReadPosRankSum = h5_3L['/3L/variants/ReadPosRankSum']\n    my_features = {\n        'MQ': MQ,\n        'QD': QD,\n        'Coverage': Coverage,\n        'HaplotypeScore': HaplotypeScore,\n        'QUAL': QUAL,\n        'FS': FS,\n        'DP': DP,\n        'HRun': HRun,\n        'ReadPosRankSum': ReadPosRankSum\n    }\n    num_features = len(my_features)\n    num_alleles = h5_3L['/3L/variants/num_alleles']\n    is_snp = h5_3L['/3L/variants/is_snp']\n    POS = h5_3L['/3L/variants/POS']\n    ```", "```py\n    #compute mendelian errors (biallelic)\n    def compute_mendelian_errors(mother, father, offspring):\n        num_errors = 0\n        num_ofs_problems = 0\n        if len(mother.union(father)) == 1:\n            # Mother and father are homogenous and the            same for ofs in offspring:\n                if len(ofs) == 2:\n                    # Offspring is het\n                    num_errors += 1\n                    num_ofs_problems += 1\n                elif len(ofs.intersection(mother)) == 0:\n                    # Offspring is homo, but opposite from parents\n                    num_errors += 2\n                    num_ofs_problems += 1\n        elif len(mother) == 1 and len(father) == 1:\n            # Mother and father are homo and different\n            for ofs in offspring:\n                if len(ofs) == 1:\n                    # Homo, should be het\n                    num_errors += 1\n                    num_ofs_problems += 1\n        elif len(mother) == 2 and len(father) == 2:\n            # Both are het, individual offspring can be anything\n            pass\n        else:\n            # One is het, the other is homo\n            homo = mother if len(mother) == 1 else father\n            for ofs in offspring:\n                if len(ofs) == 1 and ofs.intersection(homo):\n                    # homo, but not including the allele from parent that is homo\n                    num_errors += 1\n                    num_ofs_problems += 1\n        return num_errors, num_ofs_problems\n    ```", "```py\n    def acceptable_position_to_genotype():\n        for i, genotype in enumerate(calldata_genotype):\n            if is_snp[i] and num_alleles[i] == 2:\n                if len(np.where(genotype == -1)[0]) > 1:\n                    # Missing data\n                    continue\n                yield i\n    def acumulate(fun):\n        acumulator = {}\n        for res in fun():\n            if res is not None:\n                acumulator[res[0]] = res[1]\n        return acumulator\n    ```", "```py\n    def get_family_indexes(samples_hdf5, cross_pd):\n        offspring = []\n        for i, individual in cross_pd.T.iteritems():\n            index = samples_hdf5.index(individual.id)\n            if individual.function == 'parent':\n                if individual.sex == 'M':\n                    father = index\n                else:\n                    mother = index\n            else:\n                offspring.append(index)\n        return {'mother': mother, 'father': father, 'offspring': offspring}\n    cross_pd = samples[samples['cross'] == 'cross-29-2']\n    family_indexes = get_family_indexes(samples_hdf5, cross_pd)\n    ```", "```py\n    mother_index = family_indexes['mother']\n    father_index = family_indexes['father']\n    offspring_indexes = family_indexes['offspring']\n    all_errors = {}\n    def get_mendelian_errors():\n        for i in acceptable_position_to_genotype():\n            genotype = calldata_genotype[i]\n            mother = set(genotype[mother_index])\n            father = set(genotype[father_index])\n            offspring = [set(genotype[ofs_index]) for ofs_index in offspring_indexes]\n            my_mendelian_errors = compute_mendelian_errors(mother, father, offspring)\n            yield POS[i], my_mendelian_errors\n    mendelian_errors = acumulate(get_mendelian_errors)\n    pickle.dump(mendelian_errors, gzip.open('mendelian_errors.pickle.gz', 'wb'))\n    ```", "```py\n    ordered_positions = sorted(mendelian_errors.keys())\n    ordered_features = sorted(my_features.keys())\n    num_features = len(ordered_features)\n    feature_fit = np.empty((len(ordered_positions), len(my_features) + 2), dtype=float)\n    for column, feature in enumerate(ordered_features):  # 'Strange' order\n        print(feature)\n        current_hdf_row = 0\n        for row, genomic_position in enumerate(ordered_positions):\n            while POS[current_hdf_row] < genomic_position:\n                current_hdf_row +=1\n            feature_fit[row, column] = my_features[feature][current_hdf_row]\n    for row, genomic_position in enumerate(ordered_positions):\n        feature_fit[row, num_features] = genomic_position\n        feature_fit[row, num_features + 1] = 1 if mendelian_errors[genomic_position][0] > 0 else 0\n    np.save(gzip.open('feature_fit.npy.gz', 'wb'), feature_fit, allow_pickle=False, fix_imports=False)\n    pickle.dump(ordered_features, open('ordered_features', 'wb'))\n    ```", "```py\n    h5_2L = h5py.File('ag1000g.crosses.phase1.ar3sites.2L.h5', 'r')\n    samples_hdf5 = list(map(lambda sample: sample.decode('utf-8'), h5_2L['/2L/samples']))\n    calldata_DP = h5_2L['/2L/calldata/DP']\n    POS = h5_2L['/2L/variants/POS']\n    ```", "```py\n    def get_parent_indexes(samples_hdf5, parents_pd):\n        parents = []\n        for i, individual in parents_pd.T.iteritems():\n            index = samples_hdf5.index(individual.id)\n            parents.append(index)\n        return parents\n    parents_pd = samples[samples['function'] == 'parent']\n    parent_indexes = get_parent_indexes(samples_hdf5, parents_pd)\n    ```", "```py\n    all_dps = []\n    for i, pos in enumerate(POS):\n        if random.random() > 0.01:\n            continue\n        pos_dp = calldata_DP[i]\n        parent_pos_dp = [pos_dp[parent_index] for parent_index in parent_indexes]\n        all_dps.append(parent_pos_dp + [pos])\n    all_dps = np.array(all_dps)\n    np.save(gzip.open('DP_2L.npy.gz', 'wb'), all_dps, allow_pickle=False, fix_imports=False)\n    ```", "```py\n    import random\n    import matplotlib.pyplot as plt\n    ```", "```py\n    num_sims = 100000\n    num_ofs = 20\n    num_hets_AA_AT = []\n    for sim in range(num_sims):\n        sim_hets = 0\n        for ofs in range(20):\n            sim_hets += 1 if random.choice([0, 1]) == 1 else 0\n        num_hets_AA_AT.append(sim_hets)\n\n    fig, ax = plt.subplots(1,1, figsize=(16,9))\n    ax.hist(num_hets_AA_AT, bins=range(20))\n    print(len([num_hets for num_hets in num_hets_AA_AT if num_hets==20]))\n    ```", "```py\n    num_AAs_AT_AT = []\n    num_hets_AT_AT = []\n    for sim in range(num_sims):\n        sim_AAs = 0\n        sim_hets = 0\n        for ofs in range(20):\n            derived_cnt = sum(random.choices([0, 1], k=2))\n            sim_AAs += 1 if derived_cnt == 0 else 0\n            sim_hets += 1 if derived_cnt == 1 else 0\n        num_AAs_AT_AT.append(sim_AAs)\n        num_hets_AT_AT.append(sim_hets)\n    fig, ax = plt.subplots(1,1, figsize=(16,9))\n    ax.hist([num_hets_AT_AT, num_AAs_AT_AT], histtype='step', fill=False, bins=range(20), label=['het', 'AA'])\n    plt.legend()\n    ```", "```py\n    import gzip\n    import pickle\n    import random\n    import numpy as np\n    mendelian_errors = pickle.load(gzip.open('mendelian_errors.pickle.gz', 'rb'))\n    feature_fit = np.load(gzip.open('feature_fit.npy.gz', 'rb'))\n    ordered_features = np.load(open('ordered_features', 'rb'))\n    num_features = len(ordered_features)\n    ```", "```py\n    print(len(mendelian_errors), len(list(filter(lambda x: x[0] > 0,mendelian_errors.values()))))\n    ```", "```py\n(10905732, 541688)\n```", "```py\n    total_observations = len(mendelian_errors)\n    error_observations = len(list(filter(lambda x: x[0] > 0,mendelian_errors.values())))\n    ok_observations = total_observations - error_observations\n    fraction_errors = error_observations/total_observations\n    print (total_observations, ok_observations, error_observations, 100*fraction_errors)\n    del mendelian_errors\n    ```", "```py\n    prob_ok_choice = error_observations / ok_observations\n    def accept_entry(row):\n        if row[-1] == 1:\n            return True\n        return random.random() <= prob_ok_choice\n    accept_entry_v = np.vectorize(accept_entry, signature='(i)->()')\n    accepted_entries = accept_entry_v(feature_fit)\n    balanced_fit = feature_fit[accepted_entries]\n    del feature_fit\n    balanced_fit.shape\n    len([x for x in balanced_fit if x[-1] == 1]), len([x for x in balanced_fit if x[-1] == 0])\n    ```", "```py\n    np.save(gzip.open('balanced_fit.npy.gz', 'wb'), balanced_fit, allow_pickle=False, fix_imports=False)\n    ```", "```py\n    import gzip\n    import pickle\n    import random\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from pandas.plotting import scatter_matrix\n    ```", "```py\n    fit = np.load(gzip.open('balanced_fit.npy.gz', 'rb'))\n    ordered_features = np.load(open('ordered_features', 'rb'))\n    num_features = len(ordered_features)\n    fit_df = pd.DataFrame(fit, columns=ordered_features + ['pos', 'error'])\n    num_samples = 80\n    del fit\n    ```", "```py\n    fig,ax = plt.subplots(figsize=(16,9))\n    fit_df.hist(column=ordered_features, ax=ax)\n    ```", "```py\n    fit_df['MeanDP'] = fit_df['DP'] / 80\n    fig, ax = plt.subplots()\n    _ = ax.hist(fit_df[fit_df['MeanDP']<50]['MeanDP'], bins=100)\n    ```", "```py\n    errors_df = fit_df[fit_df['error'] == 1]\n    ok_df = fit_df[fit_df['error'] == 0]\n    ```", "```py\n    ok_qual_above_df = ok_df[ok_df['QUAL']>0.005]\n    errors_qual_above_df = errors_df[errors_df['QUAL']>0.005]\n    print(ok_df.size, errors_df.size, ok_qual_above_df.size, errors_qual_above_df.size)\n    print(ok_qual_above_df.size / ok_df.size, errors_qual_above_df.size / errors_df.size)\n    ```", "```py\n6507972 6500256 484932 6114096\n0.07451353509203788 0.9405931089483245\n```", "```py\n    ok_qd_above_df = ok_df[ok_df['QD']>0.05]\n    errors_qd_above_df = errors_df[errors_df['QD']>0.05]\n    print(ok_df.size, errors_df.size, ok_qd_above_df.size, errors_qd_above_df.size)\n    print(ok_qd_above_df.size / ok_df.size, errors_qd_above_df.size / errors_df.size)\n    ```", "```py\n6507972 6500256 460296 5760288\n0.07072802402960554 0.8861632526472804\n```", "```py\n    not_bad_area_errors_df = errors_df[(errors_df['QUAL']<0.005)&(errors_df['QD']<0.05)]\n    _ = scatter_matrix(not_bad_area_errors_df[['FS', 'ReadPosRankSum', 'MQ', 'HRun']], diagonal='kde', figsize=(16, 9), alpha=0.02)\n    ```", "```py\n    not_bad_area_ok_df = ok_df[(ok_df['QUAL']<0.005)&(ok_df['QD']<0.05)]\n    _ = scatter_matrix(not_bad_area_ok_df[['FS', 'ReadPosRankSum', 'MQ', 'HRun']], diagonal='kde', figsize=(16, 9), alpha=0.02)\n    ```", "```py\n    all_fit_df = pd.DataFrame(np.load(gzip.open('feature_fit.npy.gz', 'rb')), columns=ordered_features + ['pos', 'error'])\n    potentially_good_corner_df = all_fit_df[(all_fit_df['QUAL']<0.005)&(all_fit_df['QD']<0.05)]\n    all_errors_df=all_fit_df[all_fit_df['error'] == 1]\n    print(len(all_fit_df), len(all_errors_df), len(all_errors_df) / len(all_fit_df))\n    ```", "```py\n10905732 541688 0.04967002673456491\n```", "```py\n    potentially_good_corner_errors_df = potentially_good_corner_df[potentially_good_corner_df['error'] == 1]\n    print(len(potentially_good_corner_df), len(potentially_good_corner_errors_df), len(potentially_good_corner_errors_df) / len(potentially_good_corner_df))\n    print(len(potentially_good_corner_df)/len(all_fit_df))\n    ```", "```py\n9625754 32180 0.0033431147315836243\n0.8826325458942141\n```", "```py\n    from collections import defaultdict\n    import gzip\n    import numpy as np\n    import matplotlib.pylab as plt\n    ```", "```py\n    num_parents = 8\n    dp_2L = np.load(gzip.open('DP_2L.npy.gz', 'rb'))\n    print(dp_2L.shape)\n    ```", "```py\n    for i in range(num_parents):\n        print(np.median(dp_2L[:,i]), np.median(dp_2L[50000:150000,i]))\n    ```", "```py\n17.0 14.0\n23.0 22.0\n31.0 29.0\n28.0 24.0\n32.0 27.0\n31.0 31.0\n25.0 24.0\n24.0 20.0\n```", "```py\n    window_size = 200000\n    parent_DP_windows = [defaultdict(list) for i in range(num_parents)]\n    def insert_in_window(row):\n        for parent in range(num_parents):\n            parent_DP_windows[parent][row[-1] // window_size].append(row[parent])\n    insert_in_window_v = np.vectorize(insert_in_window, signature='(n)->()')\n    _ = insert_in_window_v(dp_2L)\n    ```", "```py\n    fig, axs = plt.subplots(2, num_parents // 2, figsize=(16, 9), sharex=True, sharey=True, squeeze=True)\n    for parent in range(num_parents):\n        ax = axs[parent // 4][parent % 4]\n        parent_data = parent_DP_windows[parent]\n        ax.set_ylim(10, 40)\n        ax.plot(*zip(*[(win*window_size, np.mean(lst)) for win, lst in parent_data.items()]), '.')\n    ```", "```py\nwget wget https://data.qiime2.org/distro/core/qiime2-2022.2-py38-osx-conda.yml\nconda env create -n qiime2-2022.2 --file qiime2-2022.2-py38-osx-conda.yml\n```", "```py\nwget wget https://data.qiime2.org/distro/core/qiime2-2022.2-py38-linux-conda.yml\nconda env create -n qiime2-2022.2 --file qiime2-2022.2-py38-linux-conda.yml\n```", "```py\nconda install jupyterlab jupytext\n```", "```py\njupyter serverextension enable --py qiime2 --sys-prefix\n```", "```py\n    import pandas as pd\n    from qiime2.metadata.metadata import Metadata\n    from qiime2.metadata.metadata import CategoricalMetadataColumn\n    from qiime2.sdk import Artifact\n    from qiime2.sdk import PluginManager\n    from qiime2.sdk import Result\n    pm = PluginManager()\n    demux_plugin = pm.plugins['demux']\n    #demux_emp_single = demux_plugin.actions['emp_single']\n    demux_summarize = demux_plugin.actions['summarize']\n    print(pm.plugins)\n    ```", "```py\n    print(demux_summarize.description)\n    demux_summarize_signature = demux_summarize.signature\n    print(demux_summarize_signature.inputs)\n    print(demux_summarize_signature.parameters)\n    print(demux_summarize_signature.outputs)\n    ```", "```py\nSummarize counts per sample for all samples, and generate interactive positional quality plots based on `n` randomly selected sequences.\n OrderedDict([('data', ParameterSpec(qiime_type=SampleData[JoinedSequencesWithQuality | PairedEndSequencesWithQuality | SequencesWithQuality], view_type=<class 'q2_demux._summarize._visualizer._PlotQualView'>, default=NOVALUE, description='The demultiplexed sequences to be summarized.'))])\n OrderedDict([('n', ParameterSpec(qiime_type=Int, view_type=<class 'int'>, default=10000, description='The number of sequences that should be selected at random for quality score plots. The quality plots will present the average positional qualities across all of the sequences selected. If input sequences are paired end, plots will be generated for both forward and reverse reads for the same `n` sequences.'))])\n OrderedDict([('visualization', ParameterSpec(qiime_type=Visualization, view_type=None, default=NOVALUE, description=NOVALUE))])\n```", "```py\n    seqs1 = Result.load('fmt-tutorial-demux-1-10p.qza')\n    sum_data1 = demux_summarize(seqs1)\n    sum_data1.visualization\n    ```", "```py\n    seqs2 = Result.load('fmt-tutorial-demux-2-10p.qza')\n    sum_data2 = demux_summarize(seqs2)\n    sum_data2.visualization\n    ```", "```py\n    dada2_plugin = pm.plugins['dada2']\n    dada2_denoise_single = dada2_plugin.actions['denoise_single']\n    qual_control1 = dada2_denoise_single(demultiplexed_seqs=seqs1,\n                                        trunc_len=150, trim_left=13)\n    qual_control2 = dada2_denoise_single(demultiplexed_seqs=seqs2,\n                                        trunc_len=150, trim_left=13)\n    ```", "```py\n    metadata_plugin = pm.plugins['metadata']\n    metadata_tabulate = metadata_plugin.actions['tabulate']\n    stats_meta1 = metadata_tabulate(input=qual_control1.denoising_stats.view(Metadata))\n    stats_meta1.visualization\n    ```", "```py\n    stats_meta2 = metadata_tabulate(input=qual_control2.denoising_stats.view(Metadata))\n    stats_meta2.visualization\n    ```", "```py\n    ft_plugin = pm.plugins['feature-table']\n    ft_merge = ft_plugin.actions['merge']\n    ft_merge_seqs = ft_plugin.actions['merge_seqs']\n    ft_summarize = ft_plugin.actions['summarize']\n    ft_tab_seqs = ft_plugin.actions['tabulate_seqs']\n    table_merge = ft_merge(tables=[qual_control1.table, qual_control2.table])\n    seqs_merge = ft_merge_seqs(data=[qual_control1.representative_sequences, qual_control2.representative_sequences])\n    ```", "```py\n    ft_sum = ft_summarize(table=table_merge.merged_table)\n    ft_sum.visualization\n    ```", "```py\n    tab_seqs = ft_tab_seqs(data=seqs_merge.merged_data)\n    tab_seqs.visualization\n    ```", "```py\ndata_type = 'EMPSingleEndSequences'\nconv = Artifact.import_data(data_type, 'data')\nconv.save('out.qza')\n```"]