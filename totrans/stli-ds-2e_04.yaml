- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning and AI with Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very common situation data scientists find themselves in is at the end of
    the model creation process, not knowing exactly how to convince non-data scientists
    that their model is worthwhile. They might have performance metrics from their
    model or some static visualizations but have no easy way to allow others to interact
    with their model.
  prefs: []
  type: TYPE_NORMAL
- en: Before Streamlit, there were a couple of other options, the most popular being
    creating a full-fledged app in Flask or Django or even turning a model into an
    **Application Programming Interface** (**API**) and pointing developers toward
    it. These are great options but tend to be time-consuming and suboptimal for valuable
    use cases such as prototyping an app.
  prefs: []
  type: TYPE_NORMAL
- en: The incentives for teams are a little misaligned here. Data scientists want
    to create the best models for their teams, but if they need to take a day or two
    (or, if they have experience, a few hours) of work to turn their model into a
    Flask or Django app, it doesn’t make much sense to build this out until they think
    they are nearly complete with the modeling process. It would be ideal for data
    scientists to also involve stakeholders early and often, so they can build things
    that people actually want!
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of Streamlit is that it helps us turn this arduous process into
    a frictionless app creation experience. In this chapter, we’ll go over how to
    create **Machine Learning** (**ML**) prototypes in Streamlit, how to add user
    interaction to your ML apps, and also how to understand the ML results. And we’ll
    do all this with the most popular ML libraries, including PyTorch, Hugging Face,
    OpenAI, and scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The standard ML workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting penguin species
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing a pre-trained ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training models inside Streamlit apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding ML results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating external ML libraries – a Hugging Face example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating external AI libraries – an OpenAI example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, we will need an OpenAI account. To create one, head over to
    ([https://platform.openai.com/](https://platform.openai.com/)) and follow the
    instructions on the page.
  prefs: []
  type: TYPE_NORMAL
- en: The standard ML workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step to creating an app that uses ML is creating the ML model itself.
    There are dozens of popular workflows for creating your own ML models. It’s likely
    you might have your own already! There are two parts of this process to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: The generation of the ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of the ML model in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the plan is to train a model once and then use this model in our Streamlit
    app, the best method is to create this model outside of Streamlit first (for example,
    in a Jupyter notebook or in a standard Python file), and then use this model within
    the app.
  prefs: []
  type: TYPE_NORMAL
- en: If the plan is to use the user input to train the model inside our app, then
    we can no longer create the model outside of Streamlit and instead will need to
    run the model training within the Streamlit app.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by building our ML models outside of Streamlit and move on to
    training our models inside Streamlit apps.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting penguin species
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset that we will primarily use in this chapter is the same Palmer Penguins
    dataset that we used earlier in *Chapter 1*, *An Introduction to Streamlit*. As
    is typical, we will create a new folder that will house our new Streamlit app
    and accompanying code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code creates this new folder within our `streamlit_apps` folder
    and copies the data from our `penguin_app` folder. If you haven’t downloaded the
    Palmer Penguins dataset yet, please follow the instructions in the *The setup:
    Palmer Penguins* section in *Chapter 2*, *Uploading, Downloading, and Manipulating
    Data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you may have noticed in the preceding code, there are two Python files here,
    one to create the ML model (`penguins_ml.py`) and the second to create the Streamlit
    app (`penguins_streamlit.py`). We will start with the `penguins_ml.py` file, and
    once we have a model that we are happy with, we will then move on to the `penguins_streamlit.py`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: You can also opt to create the model in a Jupyter notebook, which is less reproducible
    by design (as cells can be run out of order) but is still incredibly popular.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get re-familiarized with the `penguins.csv` dataset. The following code
    will read the dataset and print out the first five rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code, when we run our Python file `penguins_ml.py`
    in the terminal, will look something like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – First five penguins ](img/B18444_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: First five penguins'
  prefs: []
  type: TYPE_NORMAL
- en: For this app, we are going to attempt to create an app that will help researchers
    in the wild know what species of penguin they are looking at. It will predict
    the species of the penguin given some measurements of the bill, flippers, and
    body mass, and knowledge about the sex and location of the penguin.
  prefs: []
  type: TYPE_NORMAL
- en: 'This next section is not an attempt to make the best ML model possible, but
    just to create something as a quick prototype for our Streamlit app that we can
    iterate on. In that light, we are going to drop our few rows with null values,
    and not use the `year`variable in our features as it does not fit with our use
    case. We will need to define our features and output variables, do one-hot-encoding
    (or as *pandas* calls it, creating dummy variables for our text columns) on our
    features, and factorize our output variable (turn it from a string into a number).
    The following code should get our dataset in a better state to run through a classification
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we run our Python file `penguins_ml.py` again, we see that the output
    and feature variables are separated, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Output variables ](img/B18444_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Output variables'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we want to create a classification model using a subset (in this case,
    80%) of our data, and get the accuracy of said model. The following code runs
    through those steps using a random forest model, but you can use other classification
    algorithms if you would like. Again, the point here is to get a quick prototype
    to show to the penguin researchers for feedback!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a pretty good model for predicting the species of penguins! Our
    last step in the model-generating process is to save the two parts of this model
    that we need the most – the model itself and the `uniques` variable, which maps
    the factorized output variable to the species name that we recognize. To the previous
    code, we will add a few lines that will save these objects as pickle files (files
    that turn a Python object into something we can save directly and import easily
    from another Python file such as our Streamlit app). More specifically, the `open()`
    function creates two pickle files, the `pickle.dump()` function writes our Python
    files to said files, and the `close()` function closes the files. The `wb` in
    the `open()` function stands for **write bytes**, which tells Python that we want
    to write, not read, to this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have two more files in our `penguin_ml` folder: A file called `random_forest_penguin.pickle`,
    which contains our model, and `output_penguin_.pickle`, which has the mapping
    between penguin species and the output of our model. This is it for the `penguins_ml.py`
    function! We can move on to creating our Streamlit app, which uses the machine
    model we just created.'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing a pre-trained ML model in Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have our model, we want to load it (along with our mapping function
    as well) into Streamlit. In our file, `penguins_streamlit.py`, that we created
    before, we will again use the `pickle` library to load our files using the following
    code. We use the same functions as before, but instead of `wb`, we use the `rb`
    parameter, which stands for **read bytes**. To make sure these are the same Python
    objects that we used before, we will use the `st.write()` function that we are
    so familiar with already to check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As with our previous Streamlit apps, we run the following code in the terminal
    to run our app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have our random forest classifier, along with the penguin mapping! Our
    next step is to add Streamlit functions to get the user input. In our app, we
    used the island, bill length, bill depth, flipper length, body mass, and sex to
    predict the penguin species, so we will need to get each of these from our user.
    For island and sex, we know which options were in our dataset already and want
    to avoid having to parse through user text, so we will use `st.selectbox()`. For
    the other data, we just need to make sure that the user has input a positive number,
    so we will use the `st.number_input()` function and make the minimum value `0`.
    The following code takes these inputs in and prints them out on our Streamlit
    app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code should make the following app. Try it out and see if it works
    by changing the values and seeing if the output changes as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Streamlit is designed so that, by default, each time a value is changed, the
    entire app reruns. The following screenshot shows the app live, with some values
    that I’ve changed. We can either change numeric values with the **+** and **-**
    buttons on the right-hand side or we can just enter the values manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Model inputs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have all of our inputs and model ready, the next step is to format
    the data into the same format as our preprocessed data. For example, our model
    does not have one variable called `sex` but instead has two variables called `sex_female`
    and `sex_male`. Once our data is in the right shape, we can call the `predict`
    function and map the prediction to our original species list to see how our model
    functions. The following code does exactly this, and also adds some basic titles
    and instructions to the app to make it more usable. This app is rather long, so
    I will break it up into multiple sections for readability. We will start by adding
    instructions and a title to our app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have an app with our title and instructions for the user. The next step
    is to get the user inputs as we did before. We also need to put our `sex` and
    `island` variables into the correct format, as discussed before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'All of our data is in the correct format! The last step here is to use the
    `predict()` function on our model and our new data, which this final section takes
    care of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now our app should look like the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: I have added some example values to the inputs, but you should play around with
    changing the data to see if you can make the species change!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Full Streamlit app for prediction'
  prefs: []
  type: TYPE_NORMAL
- en: We now have a full Streamlit app that utilizes our pre-trained ML model, takes
    user input, and outputs the prediction. Next, we will discuss how to train models
    directly within Streamlit apps!
  prefs: []
  type: TYPE_NORMAL
- en: Training models inside Streamlit apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, we may want to have the user input change how our model is trained. We
    may want to accept data from the user or ask the user what features they would
    like to use, or even allow the user to pick the type of ML algorithm that they
    would like to use. All of these options are feasible in Streamlit, and in this
    section, we will cover the basics of using user input to affect the training process.
    As we discussed in the section above, if a model is going to be trained only once,
    it is probably best to train the model outside of Streamlit and import the model
    into Streamlit. But what if, in our example, the penguin researchers have the
    data stored locally, or do not know how to retrain the model but have the data
    in the correct format already? In cases like these, we can add the `st.file_uploader()`
    option and include a method for these users to input their own data, and get a
    custom model deployed for them without having to write any code. The following
    code will add a user option to accept data and will use the preprocessing/training
    code that we originally had in `penguins_ml.py` to make a unique model for this
    user. It is important to note here that this will only work if the user has data
    in the exact same format and style that we used, which may be unlikely. One other
    potential add-on here is to show the user what format the data needs to be in
    for this app to correctly train a model as expected!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This first section imports the libraries that we need, adds the title – as
    we have used before – and adds the `file_uploader()` function. What happens, however,
    when the user has yet to upload a file? We can set the default to load our random
    forest model if there is no penguin file, as shown in the next section of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The next problem we need to solve is how to take in the user’s data, clean
    it, and train a model based on it. Luckily, we can reuse the model training code
    that we have already created and put it within our `else` statement in the next
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now created our model within the app and need to get the inputs from
    the user for our prediction. This time, however, we can make an improvement on
    what we have done before. As of now, each time a user changes an input in our
    app, the entire Streamlit app will rerun. We can use the `st.form()` and `st.submit_form_button()`
    functions to wrap the rest of our user inputs in and allow the user to change
    all of the inputs and submit the entire form at once, instead of multiple times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the inputs with our new form, we need to create our prediction
    and write the prediction to the user, as shown in the next block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And there we go! We now have a Streamlit app that allows the user to input
    their own data, trains a model based on their data, and outputs the results, as
    shown in the next screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Penguin classifier app'
  prefs: []
  type: TYPE_NORMAL
- en: There are potential improvements that can be made here, such as using caching
    functions (explored in *Chapter 2*, *Uploading, Downloading, and Manipulating
    Data*), as one example. Apps like these, where users bring their own data, are
    significantly harder to build, especially without a universal data format. It
    is more common as of the time of writing to see Streamlit apps that show off impressive
    ML models and use cases rather than apps that build them directly in-app (especially
    with more computationally expensive model training). As we mentioned before, Streamlit
    developers often will provide references to the required data format before asking
    for user input in the form of a dataset. However, this option of allowing users
    to bring their own data is available and practical, especially to allow for quick
    iterations on model building.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding ML results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, our app might be useful, but often, just showing a result is not good
    enough for a data app. We should show some explanation of the results. In order
    to do this, we can include a section in the output of the app that we have already
    made that helps users in understanding the model better.
  prefs: []
  type: TYPE_NORMAL
- en: To start, random forest models already have a built-in feature importance method
    derived from the set of individual decision trees that make up the random forest.
    We can edit our `penguins_ml.py` file to graph this importance, and then call
    that image from within our Streamlit app. We could also graph this directly from
    within our Streamlit app, but it is more efficient to make this graph once in
    `penguins_ml.py` instead of every time our Streamlit app reloads (which is every
    time a user changes a user input!). The following code edits our `penguins_ml.py`
    file and adds the feature importance graph, saving it to our folder. We also call
    the `tight_layout()` feature, which helps format our graph better and makes sure
    we avoid any labels getting cut off. This set of code is long, and the top half
    of the file remains unchanged, so only the section on library importing and data
    cleaning has been omitted. One other note about this section is that we’re going
    to try out using other graphing libraries such as Seaborn and Matplotlib, just
    to get a bit of diversity in the graphing libraries used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now when we rerun `penguins_ml.py`, we should see a file called `feature_importance.png`,
    which we can call from our Streamlit app. Let’s do that now! We can use the `st.image()`
    function to load an image from our `.png` and print it to our penguin app. The
    following code adds our image to the Streamlit app and also improves our explanations
    around the prediction we made. Because of the length of this code block, we will
    just show the new code from the point where we start to predict using the user’s
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the bottom of your Streamlit app should look like the following screenshot
    (note that your string might be slightly different based on your inputs):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Feature importance screenshot ](img/B18444_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Feature importance screenshot'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, bill length, bill depth, and flipper length are the most important
    variables according to our random forest model. A final option for explaining
    how our model works is to plot the distributions of each of these variables by
    species, and also plot some vertical lines representing the user input. Ideally,
    the user can begin to understand the underlying data holistically and therefore,
    will understand the predictions that come from the model as well. To do this,
    we will need to actually import the data into our Streamlit app, which we have
    not done previously. The following code imports the penguin data that we used
    to build the model, and plots three histograms (for *bill length*, *bill depth*,
    and *flipper length*) along with the user input as a vertical line, starting from
    the model explanation section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have set up our app for displaying histograms, we can use the `displot()`
    function in the Seaborn visualization library to create our three histograms for
    our most important variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should create the app shown in the following figure, which
    is our app in its final form. For viewing ease, we will just show the first histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Bill length by species'
  prefs: []
  type: TYPE_NORMAL
- en: As always, the complete and final code can be found at [https://github.com/tylerjrichards/Streamlit-for-Data-Science](https://github.com/tylerjrichards/Streamlit-for-Data-Science).
    That completes this section. We have now created a fully formed Streamlit app
    that takes a pre-built model and user input and outputs both the result of the
    prediction and an explanation of the output as well. Now, let’s explore how to
    integrate your other favorite ML libraries into Streamlit!
  prefs: []
  type: TYPE_NORMAL
- en: Integrating external ML libraries – a Hugging Face example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the last few years, there has been a massive increase in the number of
    ML models created by startups and institutions. There is one that, in my opinion,
    has stood out above the rest for prioritizing the open sourcing and sharing of
    their models and methods, and that is Hugging Face. Hugging Face makes it incredibly
    easy to use ML models that some of the best researchers in the field have created
    for your own use cases, and in this bit, we’ll quickly show off how to integrate
    Hugging Face into Streamlit.
  prefs: []
  type: TYPE_NORMAL
- en: 'As part of the original setup for this book, we have already downloaded the
    two libraries that we need: PyTorch (the most popular deep learning Python framework)
    and transformers (a Hugging Face’s library that makes it easy to use their pre-trained
    models). So, for our app, let’s try one of the most basic tasks in natural language
    processing: Getting the sentiment of a bit of text! Hugging Face makes this incredibly
    easy with its pipeline function, which lets us ask for a model by name. This next
    code snippet gets a text input from the user and then retrieves the sentiment
    analysis model from Hugging Face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: When we run this, we should see the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18444_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Hugging Face Demo'
  prefs: []
  type: TYPE_NORMAL
- en: I put a random sentence in the app, but go ahead and play around with it! Try
    to give the model a bit of text that the confidence is low in (I tried “streamlit
    is a pizza pie” and sufficiently confused the model). To learn more about the
    models that are used here, Hugging Face has extensive documentation ([https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you play around with the app, you notice that the app often takes a long
    time to load. This is because each time the app is run, the transformers library
    fetches the model from Hugging Face, and then uses it in the app. We already learned
    how to cache data, but Streamlit has a similar caching function called `st.cache_resource`,
    which lets us cache objects like ML models and database connections. Let’s use
    it here to speed up our app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, our app should run much faster for multiple uses. This app is not perfect
    but shows us how easy it is to integrate some of the best-in-class libraries into
    Streamlit. Later in this book, we’ll go over how to deploy Streamlit apps directly
    on Hugging Face for free, but I would encourage you to explore the Hugging Face
    website ([https://huggingface.co/](https://huggingface.co/)) and see all that
    they have to offer.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating external AI libraries – an OpenAI example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2023 has surely been the year of generative AI, with ChatGPT taking the world
    and developer community by storm. The availability of generative models behind
    services like ChatGPT has also exploded, with each of the largest technology companies
    coming out with their own versions ([https://ai.meta.com/llama/](https://ai.meta.com/llama/)
    from Meta and [https://bard.google.com/](https://bard.google.com/) from Google,
    for example). The most popular series of these generative models is OpenAI’s **GPT**
    (**Generative Pre-trained Transformer**). This section will show you how to use
    the OpenAI API to add generative AI to your Streamlit apps!
  prefs: []
  type: TYPE_NORMAL
- en: Authenticating with OpenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our first step is to make an OpenAI account and get an API key. To do this,
    head over to [https://platform.openai.com](https://platform.openai.com) and create
    an account. Once you have created an account, go to the **API keys** section ([https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys))
    and press the button **Create new secret key**. Once you create the key, make
    sure to save it somewhere safe because OpenAI will not show you your key again!
    I saved mine in my password manager to ensure I wouldn’t lose it ([https://1password.com/](https://1password.com/)),
    but you can save yours wherever you want.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI API cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The OpenAI API is not free, but the one we will use (GPT-3.5 turbo) currently
    costs $.0015/1k tokens (~750 words) for input and $.002 /1k tokens for output
    (see [https://openai.com/pricing](https://openai.com/pricing) for updated info).
    You can also set a hard limit on the maximum you want to spend on this API at
    [https://platform.openai.com/account/billing/limits](https://platform.openai.com/account/billing/limits).
    If you set a hard limit, OpenAI will not allow you to spend above it. I certainly
    recommend setting a limit. Set one for this example section of 1 USD; we should
    stay well within that! Once you start to create generative AI apps of your own
    that you share publicly, this feature will become even more useful (often, developers
    either ask the user to enter their own API key or charge them for access to the
    Streamlit app with libraries like [https://github.com/tylerjrichards/st-paywall](https://github.com/tylerjrichards/st-paywall)
    to get around paying too much).
  prefs: []
  type: TYPE_NORMAL
- en: Streamlit and OpenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example, we’re going to recreate the sentiment analysis from our Hugging
    Face example but using GPT-3.5 turbo. As you play around with models like these,
    you will find that they are generally very intelligent, and can be used for almost
    any task you can think of without any extra training on top of them. Let me prove
    it to you!
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our API, we add it to a Secrets file (we’ll cover Secrets in
    more detail in the *Streamlit Secrets* section in *Chapter 5*, *Deploying Streamlit
    with Streamlit Community Cloud*). Create a folder called `.streamlit` and create
    a `secrets.toml` file inside it, and then put your API key in there assigned to
    a variable called `OPENAI_API_KEY` so that it becomes `OPENAI_API_KEY="sk-xxxxxxxxxxxx"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s open our existing Streamlit app and put a title at the bottom of it,
    button we can have the user click to analyze the text, and our authentication
    key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The OpenAI Python library (which we installed with our initial `requirements.txt`
    file) provides an easy way to interact with the OpenAI API all in Python, which
    is a wonderfully useful resource. The endpoint we want to hit here is called the
    chat completion endpoint ([https://platform.openai.com/docs/api-reference/chat/create](https://platform.openai.com/docs/api-reference/chat/create)),
    which takes in a system message (which is a way for us to instruct the OpenAPI
    model on how to respond, which in our case is a helpful sentiment analysis assistant)
    and a few other parameters about what underlying model we want to call. There
    are more up-to-date and expensive models than the one we will use, but I’ve found
    GPT 3.5 to be excellent and very fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can call the API and write the response back to our app like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s test it out! We can use the same text input as we did in the Hugging
    Face example to compare the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B18444_04_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: A comparison of the Hugging Face and OpenAI sentiment analyzers'
  prefs: []
  type: TYPE_NORMAL
- en: It looks like both versions think that this sentiment is positive with fairly
    high confidence. This is remarkable! The Hugging Face model is specifically trained
    for sentiment analysis, but OpenAI’s is not at all. For this trivial example,
    they both seem to work. What about if we try out giving each just a single word,
    like Streamlit?
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B18444_04_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Testing the sentiment for “Streamlit”'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the two methods disagree. OpenAI thinks this is neutral with medium
    confidence, and Hugging Face thinks the sentiment is positive with very high confidence.
    I think OpenAI is probably right here, which is endlessly fascinating. There is
    clearly a large number of use cases for a model like this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through Streamlit widgets, we can let the user change any part of the API call.
    We just add the correct widget type and the user’s input to the OpenAI function,
    and then we’re good to go! Let’s try one more thing. What if we let the user change
    the system message we started with? To do this, we’ll need to add a new text input.
    We will use a Streamlit input widget called `st.text_area`, which works the same
    as our familiar `st.text_input` but allows for a multi-line input for longer sections
    of text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The user can now change the system message, but our default message is the
    same. I went ahead and changed the system message here to something ridiculous.
    I asked the model to be a terrible sentiment analysis assistant, always messing
    up the sentiment that was input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B18444_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Changing the system message for the OpenAI text analyzer'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the model did what I asked and screwed up the sentiment analysis
    for **streamlit is awesome**, saying that the sentiment was negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick warning: When you allow user input into a large language model, users
    may try and inject undesirable prompts into your applications. Here is one example
    using the same app, where I ask the model to ignore all the other instructions
    and instead write a pirate themed story:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B18444_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: OpenAI and pirates'
  prefs: []
  type: TYPE_NORMAL
- en: This story continued for many more lines, but you can see how the more input
    I give the user control over, the more likely it is that they can use my application
    in ways I did not intend. There are many novel ways to get around this, including
    running the prompt through another API call, this time asking the model if it
    thinks the prompt is disingenuous, or preventing common injections like “ignore
    the previous prompt.”
  prefs: []
  type: TYPE_NORMAL
- en: There are also open-source libraries like Rebuff ([https://github.com/protectai/rebuff](https://github.com/protectai/rebuff)),
    which are extremely useful as well! I hesitate to give any specific advice here,
    as the field of generative AI moves extremely quickly, but the general principles
    of caution and intentional user input should be very useful.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested in more generative AI Streamlit apps, the Streamlit team
    has made a landing page that has all the most recent information and examples
    at [https://streamlit.io/generative-ai](https://streamlit.io/generative-ai).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about some ML basics: How to take a pre-built ML
    model and use it within Streamlit, how to create our own models from within Streamlit,
    how to use user input to understand and iterate on ML models, and even how to
    use models from Hugging Face and OpenAI. Hopefully, by the end of this chapter,
    you’ll feel comfortable with each of these. Next, we will dive into the world
    of deploying Streamlit apps using Streamlit Community Cloud!'
  prefs: []
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/sl](https://packt.link/sl)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code13440134443835796.png)'
  prefs: []
  type: TYPE_IMG
