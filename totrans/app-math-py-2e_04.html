<html><head></head><body>
		<div id="_idContainer704">
			<h1 class="chapter-number" id="_idParaDest-139"><a id="_idTextAnchor138"/>4</h1>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor139"/>Working with Randomness and Probability</h1>
			<p>In this chapter, we will discuss randomness and probability. We will start by briefly exploring the fundamentals of probability by selecting elements from a set of data. Then, we will learn how to generate (pseudo) random numbers using Python and NumPy, and how to generate samples according to a specific probability distribution. We will conclude the chapter by looking at a number of advanced topics covering random processes and Bayesian techniques and using <strong class="bold">Markov Chain Monte Carlo</strong> (<strong class="bold">MCMC</strong>) methods to estimate the parameters of a <span class="No-Break">simple model.</span></p>
			<p>Probability is a quantification of the likelihood of a specific event occurring. We use probabilities intuitively all of the time, although sometimes the formal theory can be quite counterintuitive. Probability theory aims to describe the behavior of <em class="italic">random variables</em> whose value is not known, but where the probabilities of the value of this random variable take some (range of) values that are known. These probabilities are usually in the form of one of several probability distributions. Arguably, the most famous probability distribution of this kind is normal distribution, which, for example, can describe the spread of a certain characteristic over a <span class="No-Break">large population.</span></p>
			<p>We will see probability again in a more applied setting in <a href="B19085_06.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Working with Data and Statistics</em>, where we will discuss statistics. Here, we will put probability theory to use to quantify errors and build a systematic theory of <span class="No-Break">analyzing data.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following recipes:</span></p>
			<ul>
				<li>Selecting items <span class="No-Break">at random</span></li>
				<li>Generating <span class="No-Break">random data</span></li>
				<li>Changing the random <span class="No-Break">number generator</span></li>
				<li>Generating normally distributed <span class="No-Break">random numbers</span></li>
				<li>Working with <span class="No-Break">random processes</span></li>
				<li>Analyzing conversion rates with <span class="No-Break">Bayesian techniques</span></li>
				<li>Estimating parameters with Monte <span class="No-Break">Carlo simulations</span></li>
			</ul>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor140"/>Technical requirements</h1>
			<p>For this chapter, we require the standard scientific Python packages: NumPy, Matplotlib, and SciPy. We will also require the PyMC package for the final recipe. You can install this using your favorite package manager, such as <strong class="source-inline">pip</strong>: </p>
			<pre class="console">
python3.10 -m pip install pymc</pre>
			<p>This command will install the most recent version of PyMC, which, at the time of writing, is 4.0.1. This package provides facilities for probabilistic programming, which involves performing many calculations driven by randomly generated data to understand the likely distribution of a solution to <span class="No-Break">a problem.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">In the previous edition, the current version of PyMC was 3.9.2, but since then, PyMC version 4.0 was released and the name reverted to PyMC with this update rather than PyMC3. </p>
			<p>The code for this chapter can be found in the <span class="No-Break"><strong class="source-inline">Chapter 04</strong></span> folder of the GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2004"><span class="No-Break">https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2004</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor141"/>Selecting items at random</h1>
			<p>At the core of probability<a id="_idIndexMarker363"/> and randomness<a id="_idIndexMarker364"/> is the idea of selecting an item from some kind of collection. As we know, the probability of selecting an item from a collection quantifies the likelihood of that item being selected. Randomness describes the selection of items from a collection according to probabilities without any additional bias. The opposite of a random selection might be described<a id="_idIndexMarker365"/> as a <em class="italic">deterministic</em> selection. In general, it is very difficult to replicate a purely random process using a computer because computers and their processing are inherently deterministic. However, we can generate sequences of pseudorandom numbers that, when properly constructed, demonstrate a reasonable approximation <span class="No-Break">of randomness.</span></p>
			<p>In this recipe, we will select items<a id="_idIndexMarker366"/> from a collection and learn about some of the key terminology associated with probability and randomness that we will need throughout <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor142"/>Getting ready</h2>
			<p>The <a id="_idIndexMarker367"/>Python Standard Library contains a module for generating (pseudo) random numbers<a id="_idIndexMarker368"/> called <strong class="source-inline">random</strong>, but in this recipe and throughout this chapter, we will use the NumPy <strong class="source-inline">random</strong> module instead. The routines in the NumPy <strong class="source-inline">random</strong> module can be used to generate arrays of random numbers and are slightly more flexible than their standard library counterparts. As usual, we import NumPy under the <span class="No-Break"><strong class="source-inline">np</strong></span><span class="No-Break"> alias.</span></p>
			<p>Before we can proceed, we need to fix some terminology. A <strong class="bold">sample space</strong> is a <a id="_idIndexMarker369"/>set (a collection with no repeated elements) and an <strong class="bold">event</strong> is a <a id="_idIndexMarker370"/>subset of the sample space. The <strong class="bold">probability</strong> that an<a id="_idIndexMarker371"/> event, <img alt="" src="image/Formula_04_001.png"/>, occurs is denoted as <img alt="" src="image/Formula_04_002.png"/>, and is a number between 0 and 1. A probability of 0 indicates that the event can never occur, while a probability of 1 indicates that an event will certainly occur. The probability of the whole sample space must <span class="No-Break">be 1.</span></p>
			<p>When the sample space is discrete, then probabilities are just numbers between 0 and 1 associated with each of the elements, where the sum of all these numbers is 1. This gives meaning to the probability of selecting a single item (an event consisting of a single element) from a collection. We will consider methods for selecting items from a discrete collection here and deal with the <em class="italic">continuous</em> case in the <em class="italic">Generating normally distributed random </em><span class="No-Break"><em class="italic">numbers</em></span><span class="No-Break"> recipe.</span></p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor143"/>How to do it...</h2>
			<p>Perform the following steps to select items at random from <span class="No-Break">a container:</span></p>
			<ol>
				<li value="1">The first step is to set up the random number generator. For the moment, we will use the default random number generator for NumPy, which is recommended in most cases. We can do this by calling the <strong class="source-inline">default_rng</strong> routine from the NumPy <strong class="source-inline">random</strong> module, which will return an instance of a random number generator. We will usually call this function without a seed, but for this recipe, we will add a <strong class="source-inline">12345</strong> seed so that our results <span class="No-Break">are repeatable:</span><pre class="console">
rng = np.random.default_rng(12345) </pre><pre class="console">
# changing seed for repeatability</pre></li>
				<li>Next, we need to create the data and probabilities that we will select from. This step can be skipped if you already have the data stored or if you want to select elements with <span class="No-Break">equal probabilities:</span><pre class="console">
data = np.arange(15)</pre><pre class="console">
probabilities = np.array(</pre><pre class="console">
     [0.3, 0.2, 0.1, 0.05, 0.05, 0.05, 0.05, 0.025,</pre><pre class="console">
     0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]</pre><pre class="console">
)</pre></li>
			</ol>
			<p>As a <a id="_idIndexMarker372"/>quick sanity test, we can use an assertion to check that these probabilities do indeed sum <span class="No-Break">to 1:</span></p>
			<pre class="console">
assert round(sum(probabilities), 10) == 1.0,
    "Probabilities must sum to 1"</pre>
			<p>Now, we can use the <strong class="source-inline">choice</strong> method on the random number generator, <strong class="source-inline">rng</strong>, to select the samples from <strong class="source-inline">data</strong> according to the probabilities just created. For this selection, we want to turn the replacement on, so calling the method multiple times can select from the <span class="No-Break">entire </span><span class="No-Break"><strong class="source-inline">data</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
selected = rng.choice(data,p=probabilities,replace=True)
# 0</pre>
			<p>To select multiple items from <strong class="source-inline">data</strong>, we can also supply the <strong class="source-inline">size</strong> argument, which specifies the shape of the array to be selected. This plays the same role as the <strong class="source-inline">shape</strong> keyword argument with many of the other NumPy array creation routines. The argument given to <strong class="source-inline">size</strong> can be either an integer or a tuple <span class="No-Break">of integers:</span></p>
			<pre class="console">
selected_array = rng.choice(data, p=probabilities,  replace=True, size=(5, 5))
#array([[ 1, 6, 4, 1, 1],
#         [ 2, 0, 4, 12, 0],
#         [12, 4, 0, 1, 10],
#         [ 4, 1, 5, 0, 0],
#         [ 0, 1, 1, 0, 7]])</pre>
			<p>We can see that there appear to be more 0s and 1s in the sampled data, for which we assigned probabilities of 0.3 and 0.2 respectively. Interestingly, only one 2 appears, and yet we have two 12s, despite the probability of a 12 appearing being half that of a 2. This is not a problem; a larger probability does not guarantee that individual numbers will appear in a sample, only that we’d expect to see roughly twice as many 2s as 12s in a large number <span class="No-Break">of samples.</span></p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor144"/>How it works...</h2>
			<p>The <strong class="source-inline">default_rng</strong> routine<a id="_idIndexMarker373"/> creates a new <strong class="bold">Pseudorandom Number Generator</strong> (<strong class="bold">PRNG</strong>) instance (with or without a seed) that can be used to generate random numbers <a id="_idIndexMarker374"/>or, as we saw in the recipe, select items at random from predefined data. NumPy also has an <strong class="bold">implicit state</strong>-based interface for generating random numbers using routines directly from the <strong class="source-inline">random</strong> module. However, it is generally advisable to create the generator explicitly, using <strong class="source-inline">default_rng</strong>, or create a <strong class="source-inline">Generator</strong> instance yourself. Being more explicit in this way is more Pythonic and should lead to more reproducible results (in <span class="No-Break">some sense).</span></p>
			<p>A <strong class="bold">seed</strong> is a<a id="_idIndexMarker375"/> value that is passed to a random number generator in order to generate the values. The generator generates a sequence of numbers in a completely deterministic way based only on the seed. This means that two instances of the same PRNGs provided with the same seed will generate the same sequence of random numbers. If no seed is provided, the generators typically produce a seed that depends on the <span class="No-Break">user’s system.</span></p>
			<p>The <strong class="source-inline">Generator</strong> class from NumPy is a wrapper around a low-level pseudorandom bit generator, which is where the random numbers are actually generated. In recent versions of NumPy, the default PRNG algorithm is the 128-bit <em class="italic">permuted congruential generator.</em> By contrast, Python’s built-in <strong class="source-inline">random</strong> module uses a Mersenne Twister PRNG. More information about the different options for PRNG algorithms is given in the <em class="italic">Changing the random number </em><span class="No-Break"><em class="italic">generator</em></span><span class="No-Break"> recipe.</span></p>
			<p>The <strong class="source-inline">choice</strong> method<a id="_idIndexMarker376"/> on a <strong class="source-inline">Generator</strong> instance performs selections according to random numbers generated by the underlying <strong class="source-inline">BitGenerator</strong>. The optional <strong class="source-inline">p</strong> keyword argument specifies the probability associated with each item from the data provided. If this argument isn’t provided, then a <em class="italic">uniform probability</em> is<a id="_idIndexMarker377"/> assumed, where each item has an equal probability of being selected. The <strong class="source-inline">replace</strong> keyword argument specifies whether selections should be made with or without a replacement. We turned replacement on so that the same element can be selected more than once. The <strong class="source-inline">choice</strong> method uses the random numbers given by the generator to make the selections, which means that two PRNGs of the same type using the same seed will select the same items when using the <span class="No-Break"><strong class="source-inline">choice</strong></span><span class="No-Break"> method.</span></p>
			<p>This process of choosing points from a <em class="italic">bag</em> of possible choices is a good way to think about <strong class="bold">discrete probability</strong>. This<a id="_idIndexMarker378"/> is where we assign a certain weight – for example, 1 over the number of points – to each of a finite number of points, where the sum of these weights is 1. Sampling<a id="_idIndexMarker379"/> is the process of choosing points at random according to the weights assigned by the probability (we can assign discrete probabilities to infinite sets too, but this is more complicated because of the constraint that the <a id="_idIndexMarker380"/>weights must sum to 1 and this is also impractical <span class="No-Break">for computation).</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor145"/>There’s more...</h2>
			<p>The <strong class="source-inline">choice</strong> method<a id="_idIndexMarker381"/> can also be used to create random samples of a given size by passing <strong class="source-inline">replace=False</strong> as an argument. This guarantees the selection of distinct items from the data, which is good for generating a random sample. This might be used, for example, to select users to test a new version of an interface from the whole group of users; most sample statistical techniques rely on randomly <span class="No-Break">selected samples.</span></p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor146"/>Generating random data</h1>
			<p>Many tasks involve generating large <a id="_idIndexMarker382"/>quantities of random numbers, which, in their most basic form, are either integers or floating-point numbers (double-precision) lying within the range <img alt="" src="image/Formula_04_003.png"/>. Ideally, these numbers should be selected uniformly, so that if we draw a large number of these numbers, they are distributed roughly evenly across the <span class="No-Break">range <img alt="" src="image/Formula_04_004.png"/>.</span></p>
			<p>In this recipe, we will see how to generate large quantities of random integers and floating-point numbers using NumPy, and show the distribution of these numbers using <span class="No-Break">a histogram.</span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor147"/>Getting ready</h2>
			<p>Before <a id="_idIndexMarker383"/>we start, we need to import the <strong class="source-inline">default_rng</strong> routine from the NumPy <strong class="source-inline">random</strong> module and create an instance of the default random number generator to use in <span class="No-Break">the recipe:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345) # changing seed for reproducibility</pre>
			<p>We have discussed this process in the <em class="italic">Selecting items at </em><span class="No-Break"><em class="italic">random</em></span><span class="No-Break"> recipe.</span></p>
			<p>We also import the Matplotlib <strong class="source-inline">pyplot</strong> module under the <span class="No-Break"><strong class="source-inline">plt</strong></span><span class="No-Break"> alias.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor148"/>How to do it...</h2>
			<p>Perform the following steps to generate uniform random data and plot a histogram to understand <span class="No-Break">its distribution:</span></p>
			<ol>
				<li value="1">To<a id="_idIndexMarker384"/> generate random floating-point numbers between 0 and 1, including 0 but not 1, we use the <strong class="source-inline">random</strong> method on the <span class="No-Break"><strong class="source-inline">rng</strong></span><span class="No-Break"> object:</span><pre class="console">
random_floats = rng.random(size=(5, 5))</pre><pre class="console">
# array([[0.22733602, 0.31675834, 0.79736546, 0.67625467, 0.39110955],</pre><pre class="console">
#           [0.33281393, 0.59830875, 0.18673419, 0.67275604, 0.94180287],</pre><pre class="console">
#           [0.24824571, 0.94888115, 0.66723745, 0.09589794, 0.44183967],</pre><pre class="console">
#           [0.88647992, 0.6974535 , 0.32647286, 0.73392816, 0.22013496],</pre><pre class="console">
#           [0.08159457, 0.1598956 , 0.34010018, 0.46519315, 0.26642103]])</pre></li>
				<li>To generate random integers, we use the <strong class="source-inline">integers</strong> method on the <strong class="source-inline">rng</strong> object. This will return integers in the <span class="No-Break">specified range:</span><pre class="console">
random_ints = rng.integers(1, 20, endpoint=True, size=10)</pre><pre class="console">
# array([12, 17, 10, 4, 1, 3, 2, 2, 3, 12])</pre></li>
				<li>To examine the distribution of the random floating-point numbers, we first need to generate a large array of random numbers, just as we did in <em class="italic">step 1</em>. While this is not strictly necessary, a larger sample will be able to show the distribution more clearly. We generate these numbers <span class="No-Break">as follows:</span><pre class="console">
dist = rng.random(size=1000)</pre></li>
				<li>To show the distribution of the numbers we have generated, we plot a <em class="italic">histogram</em> of <span class="No-Break">the data:</span><pre class="console">
fig, ax = plt.subplots()</pre><pre class="console">
ax.hist(dist, color="k", alpha=0.6)</pre><pre class="console">
ax.set_title("Histogram of random numbers")</pre><pre class="console">
ax.set_xlabel("Value")</pre><pre class="console">
ax.set_ylabel("Density")</pre></li>
			</ol>
			<p>The resulting plot is shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em>. As we can see, the data is roughly evenly distributed across the <span class="No-Break">whole range:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer595">
					<img alt="Figure 4.1 – Histogram of randomly generated random numbers between 0 and 1&#13;&#10;" src="image/4.1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Histogram of randomly generated random numbers between 0 and 1</p>
			<p>As the<a id="_idIndexMarker385"/> number of sampled points increases, we would expect these bars to “even out” and look more and more like the flat line that we expect from a uniform distribution. Compare this to the same histogram with 10,000 random points in <span class="No-Break"><em class="italic">Figure </em></span><span class="No-Break"><em class="italic">4</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break"> here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer596">
					<img alt="Figure 4.2 – Histogram of 10,000 uniformly distributed random numbers&#13;&#10;" src="image/4.2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Histogram of 10,000 uniformly distributed random numbers</p>
			<p>We can see here<a id="_idIndexMarker386"/> that, although not totally flat, the distribution is much more even across the whole range. </p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor149"/>How it works...</h2>
			<p>The <strong class="source-inline">Generator</strong> interface provides three simple methods for generating basic random numbers, not including the <strong class="source-inline">choice</strong> method that we discussed in the <em class="italic">Selecting items at random</em> recipe. In addition to the <strong class="source-inline">random</strong> method for generating random floating-point numbers and the <strong class="source-inline">integers</strong> method for generating random integers, there is also a <strong class="source-inline">bytes</strong> method for generating raw random bytes. Each of these methods calls a relevant method on the underlying <strong class="source-inline">BitGenerator</strong> instance. Each of these methods also enables the data type of the generated numbers to be changed, for<a id="_idIndexMarker387"/> example, from double- to single-precision <span class="No-Break">floating-point numbers.</span></p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor150"/>There’s more...</h2>
			<p>The <strong class="source-inline">integers</strong> method on the <strong class="source-inline">Generator</strong> class combines the functionality of the <strong class="source-inline">randint</strong> and <strong class="source-inline">random_integers</strong> methods on the old <strong class="source-inline">RandomState</strong> interface through the addition of the <strong class="source-inline">endpoint</strong> optional argument (in the old interface, the <strong class="source-inline">randint</strong> method excluded the upper endpoint, whereas the <strong class="source-inline">random_integers</strong> method included the upper endpoint). All of the random data generating methods on <strong class="source-inline">Generator</strong> allow the data type of the data they generate to be customized, which was not possible using the old interface (this interface was introduced in <span class="No-Break">NumPy 1.17).</span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em>, we can see that the histogram of the data that we generated is approximately uniform over the range <img alt="" src="image/Formula_04_005.png"/>. That is, all of the bars are approximately level (they are not completely level due to the random nature of the data). This is what we expect from uniformly distributed random numbers, such as those generated by the <strong class="source-inline">random</strong> method. We will explain distributions of random numbers in greater detail in the <em class="italic">Generating normally distributed random numbers</em> recipe. </p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor151"/>Changing the random number generator</h1>
			<p>The <strong class="source-inline">random</strong> module<a id="_idIndexMarker388"/> in NumPy provides several alternatives to the default PRNG, which uses a 128-bit permutation congruential generator. While this is a good general-purpose random number generator, it might not be sufficient for your particular needs. For example, this algorithm is very different from the one used in Python’s internal random number generator. We will follow the guidelines for best practice set out in the NumPy documentation for running repeatable but suitably <span class="No-Break">random simulations.</span></p>
			<p>In this recipe, we will show you how to change to an alternative PRNG and how to use seeds effectively in <span class="No-Break">your programs.</span></p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor152"/>Getting ready</h2>
			<p>As usual, we import NumPy under the <strong class="source-inline">np</strong> alias. Since we will be using multiple items from the <strong class="source-inline">random</strong> package, we import that module from NumPy, too, using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from numpy import random</pre>
			<p>You will need to select one of the alternative random number generators that are provided by NumPy (or define your own; refer to the <em class="italic">There’s more...</em> section in this recipe). For this recipe, we will use the <strong class="source-inline">MT19937</strong> random number generator, which uses a Mersenne Twister-based algorithm like the one used in Python’s internal random <span class="No-Break">number generator.</span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor153"/>How to do it...</h2>
			<p>The following <a id="_idIndexMarker389"/>steps show how to generate seeds and different random number generators in a <span class="No-Break">reproducible way:</span></p>
			<ol>
				<li value="1">We will generate a <strong class="source-inline">SeedSequence</strong> object that can reproducibly generate new seeds from a given source of entropy. We can either provide our own entropy as an integer, very much like how we provide the seed for <strong class="source-inline">default_rng</strong>, or we can let Python gather entropy from the operating system. We will pick the latter method here to demonstrate its use. For this, we do not provide any additional arguments to create the <span class="No-Break"><strong class="source-inline">SeedSequence</strong></span><span class="No-Break"> object:</span><pre class="console">
seed_seq = random.SeedSequence()</pre></li>
				<li>Now that we have the means to generate the seeds for random number generators for the rest of the session, we log the entropy next so that we can reproduce this session later if necessary. The following is an example of what the entropy should look like; your results will inevitably <span class="No-Break">differ somewhat:</span><pre class="console">
print(seed_seq.entropy)</pre><pre class="console">
# 9219863422733683567749127389169034574</pre></li>
				<li>Now, we can create the underlying <strong class="source-inline">BitGenerator</strong> instance that will provide the random numbers for the wrapping <span class="No-Break"><strong class="source-inline">Generator</strong></span><span class="No-Break"> object:</span><pre class="console">
bit_gen = random.MT19937(seed_seq)</pre></li>
				<li>Next, we create the wrapping <strong class="source-inline">Generator</strong> object around this <strong class="source-inline">BitGenerator</strong> instance to create a usable random <span class="No-Break">number generator:</span><pre class="console">
rng = random.Generator(bit_gen)</pre></li>
			</ol>
			<p>Once created, you can use this random number generator as we have seen in any of the <span class="No-Break">previous recipes.</span></p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor154"/>How it works...</h2>
			<p>As<a id="_idIndexMarker390"/> mentioned in the <em class="italic">Selecting items at random</em> recipe, the <strong class="source-inline">Generator</strong> class is a wrapper around an underlying <strong class="source-inline">BitGenerator</strong> that implements a given pseudorandom number algorithm. NumPy provides several implementations of pseudorandom number algorithms through the various subclasses of the <strong class="source-inline">BitGenerator</strong> class: <strong class="source-inline">PCG64</strong> (default); <strong class="source-inline">MT19937</strong> (as seen in this recipe); <strong class="source-inline">Philox</strong>; and <strong class="source-inline">SFC64</strong>. These bit generators are implemented <span class="No-Break">in Cython.</span></p>
			<p>The <strong class="source-inline">PCG64</strong> generator<a id="_idIndexMarker391"/> should provide high-performance random number generation with good statistical quality (this might not be the case on 32-bit systems). The <strong class="source-inline">MT19937</strong> generator<a id="_idIndexMarker392"/> is slower than more modern PRNGs and does not produce random numbers with good statistical properties. However, this is the random number generator algorithm that is used by the Python Standard Library <strong class="source-inline">random</strong> module. The <strong class="source-inline">Philox</strong> generator<a id="_idIndexMarker393"/> is relatively slow but produces random numbers of very high quality while the <strong class="source-inline">SFC64</strong> generator<a id="_idIndexMarker394"/> is fast and of reasonably good quality, but doesn’t have as good statistical properties as <span class="No-Break">other generators.</span></p>
			<p>The <strong class="source-inline">SeedSequence</strong> object<a id="_idIndexMarker395"/> created in this recipe is a means to create seeds for random number generators in an independent and reproducible manner. In particular, this is useful if you need to create independent random number generators for several parallel processes, but still need to be able to reconstruct each session later to debug or inspect results. The entropy stored on this object is a 128-bit integer that was gathered from the operating system and serves as a source of <span class="No-Break">random seeds.</span></p>
			<p>The <strong class="source-inline">SeedSequence</strong> object allows us to create a separate random number generator for each independent process or thread, which eliminates any data race problems that might make results unpredictable. It also generates seed values that are very different from one another, which can help avoid problems with some PRNGs (such as <strong class="source-inline">MT19937</strong>, which can produce very similar streams with two similar 32-bit integer seed values). Obviously, having two independent random number generators producing the same or very similar values will be problematic when we are depending on the independence <a id="_idIndexMarker396"/>of <span class="No-Break">these values.</span></p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor155"/>There’s more...</h2>
			<p>The <strong class="source-inline">BitGenerator</strong> class<a id="_idIndexMarker397"/> serves as a common interface for generators of raw random integers. The classes mentioned previously are those that are implemented in NumPy with the <strong class="source-inline">BitGenerator</strong> interface. You can also create your own <strong class="source-inline">BitGenerator</strong> subclasses, although this needs to be implemented <span class="No-Break">in Cython.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Refer to the NumPy <a id="_idIndexMarker398"/>documentation at <a href="https://numpy.org/devdocs/reference/random/extending.html#new-bit-generators">https://numpy.org/devdocs/reference/random/extending.html#new-bit-generators</a> for <span class="No-Break">more information.</span></p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor156"/>Generating normally distributed random numbers</h1>
			<p>In the <em class="italic">Generating random data</em> recipe, we generated random floating-point numbers following a uniform distribution between 0 and 1, but not including 1. However, in most cases where we require random data, we need to follow one of several different <strong class="bold">distributions</strong> instead. Roughly speaking, a <strong class="bold">distribution function</strong> is a function, <img alt="" src="image/Formula_04_006.png"/>, that describes the probability that a random variable has a<a id="_idIndexMarker399"/> value that is below <img alt="" src="image/Formula_04_007.png"/>. In practical terms, the distribution describes the spread of the random data over a range. In particular, if we create a histogram of data that follows a particular distribution, then it should roughly resemble the graph of the distribution function. This is best seen <span class="No-Break">by example.</span></p>
			<p>One of the most common <a id="_idIndexMarker400"/>distributions is <strong class="bold">normal distribution</strong>, which appears frequently in statistics and forms the basis for many statistical methods that we will see in <a href="B19085_06.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Working with Data and Statistics</em>. In this recipe, we will demonstrate how to generate data following normal distribution, and plot a histogram of this data to see the shape of <span class="No-Break">the distribution.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor157"/>Getting ready</h2>
			<p>As in the <em class="italic">Generating random data</em> recipe, we import the <strong class="source-inline">default_rng</strong> routine from the NumPy <strong class="source-inline">random</strong> module and create a <strong class="source-inline">Generator</strong> instance with a seeded generator for <span class="No-Break">demonstration purposes:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<p>As usual, we import the Matplotlib <strong class="source-inline">pyplot</strong> module as <strong class="source-inline">plt</strong>, and NumPy <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">np</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor158"/>How to do it...</h2>
			<p>In the following steps, we <a id="_idIndexMarker401"/>generate random data that follows a <span class="No-Break">normal distribution:</span></p>
			<ol>
				<li value="1">We use the <strong class="source-inline">normal</strong> method on our <strong class="source-inline">Generator</strong> instance to generate the random data according to the <strong class="source-inline">normal</strong> distribution. The normal distribution has two <em class="italic">parameters</em>: <em class="italic">location</em> and <em class="italic">scale</em>. There is also an optional <strong class="source-inline">size</strong> argument that specifies the shape of the generated data (see the <em class="italic">Generating random data</em> recipe for more information on the <strong class="source-inline">size</strong> argument). We generate an array of 10,000 values to get a reasonably <span class="No-Break">sized sample:</span><pre class="console">
mu = 5.0 # mean value</pre><pre class="console">
sigma = 3.0 # standard deviation</pre><pre class="console">
rands = rng.normal(loc=mu, scale=sigma, size=10000)</pre></li>
				<li>Next, we plot a histogram of this data. We have increased the number of <strong class="source-inline">bins</strong> in the histogram. This isn’t strictly necessary, as the default number (10) is perfectly adequate, but it does show the distribution <span class="No-Break">slightly better:</span><pre class="console">
fig, ax = plt.subplots()</pre><pre class="console">
ax.hist(rands, bins=20, color="k", alpha=0.6)</pre><pre class="console">
ax.set_title("Histogram of normally distributed data")</pre><pre class="console">
ax.set_xlabel("Value")</pre><pre class="console">
ax.set_ylabel("Density")</pre></li>
				<li>Next, we<a id="_idIndexMarker402"/> create a function that will generate the expected density for a range of values. This is given by multiplying the probability density function for normal distribution by the number of <span class="No-Break">samples (10,000):</span><pre class="console">
def normal_dist_curve(x):</pre><pre class="console">
    return 10000*np.exp(</pre><pre class="console">
        -0.5*((x-mu)/sigma)**2)/(sigma*np.sqrt(2*np.pi))</pre></li>
				<li>Finally, we plot our expected distribution over the histogram of <span class="No-Break">our data:</span><pre class="console">
x_range = np.linspace(-5, 15)</pre><pre class="console">
y = normal_dist_curve(x_range)</pre><pre class="console">
ax.plot(x_range, y, "k--")</pre></li>
			</ol>
			<p>The result is shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em>. We can see here that the distribution of our sampled data closely follows the expected distribution from a normal <span class="No-Break">distribution curve:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer600">
					<img alt="Figure 4.3 – Histogram of data drawn from a normal distribution, with the expected density overlaid&#13;&#10;" src="image/4.3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Histogram of data drawn from a normal distribution, with the expected density overlaid</p>
			<p>Again, if we <a id="_idIndexMarker403"/>took larger and larger samples, we’d expect that the roughness of the sample would begin to smooth out and approach the expected density (shown as the dashed line in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">).</span></p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor159"/>How it works...</h2>
			<p>Normal distribution has a probability density function defined by the <span class="No-Break">following formula:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_008.png"/></p>
			<p>This is related to the normal distribution function, <img alt="" src="image/Formula_04_009.png"/>, according to the <span class="No-Break">following formula:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_010.png"/></p>
			<p>This probability density function peaks at the mean value, which coincides with the location parameter, and the width of the <em class="italic">bell shape</em> is determined by the scale parameter. We can see in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em> that the histogram of the data generated by the <strong class="source-inline">normal</strong> method on the <strong class="source-inline">Generator</strong> object fits the expected distribution <span class="No-Break">very closely.</span></p>
			<p>The <strong class="source-inline">Generator</strong> class uses a 256-step ziggurat method to generate normally distributed random data, which is fast compared to the Box-Muller or inverse CDF implementations that are also available <span class="No-Break">in</span><span class="No-Break"><a id="_idIndexMarker404"/></span><span class="No-Break"> NumPy.</span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor160"/>There’s more...</h2>
			<p>The normal distribution<a id="_idIndexMarker405"/> is one example of <a id="_idIndexMarker406"/>a <em class="italic">continuous</em> probability distribution, in that it is defined for real numbers and the distribution function is defined by an integral (rather than a sum). An interesting feature of normal distribution (and other continuous probability distributions) is that the probability of selecting any given real number is 0. This is reasonable because it only makes sense to measure the probability that a value selected in this distribution lies within a <span class="No-Break">given range.</span></p>
			<p>Normal distribution is important in statistics, mostly due<a id="_idIndexMarker407"/> to the <em class="italic">central limit theorem</em>. Roughly speaking, this theorem states that sums of <strong class="bold">Independent and Identically Distributed</strong> (<strong class="bold">IID</strong>) random variables, with<a id="_idIndexMarker408"/> a common mean and variance, are eventually like normal distribution with a common mean and variance. This holds, regardless of the actual distribution of these random variables. This allows us to use statistical tests based on normal distribution in many cases even if the actual distribution of the variables is not necessarily normal (we do, however, need to be extremely cautious when appealing to the central <span class="No-Break">limit theorem).</span></p>
			<p>There are many other continuous probability distributions aside from normal distribution. We have already encountered <em class="italic">uniform</em> distribution<a id="_idIndexMarker409"/> over a range of 0 to 1. More generally, uniform distribution over the range <img alt="" src="image/Formula_04_011.png"/> has a probability density function given by the <span class="No-Break">following equation:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_012.png"/></p>
			<p>Other common examples of continuous probability density functions<a id="_idIndexMarker410"/> include <em class="italic">exponential</em> distribution, <em class="italic">beta</em> distribution, and <em class="italic">gamma</em> distribution. Each of these<a id="_idIndexMarker411"/> distributions <a id="_idIndexMarker412"/>has a corresponding <a id="_idIndexMarker413"/>method on the <strong class="source-inline">Generator</strong> class that generates random data from that distribution. These are typically named according to the name of the distribution, all in lowercase letters, so for the aforementioned distributions, the corresponding methods are <strong class="source-inline">exponential</strong>, <strong class="source-inline">beta</strong>, and <strong class="source-inline">gamma</strong>. These distributions each have one or more <em class="italic">parameters</em>, such as location and scale for normal distribution, that determine the final shape of the distribution. You may need to consult the NumPy documentation (<a href="https://numpy.org/doc/1.18/reference/random/generator.html#numpy.random.Generator">https://numpy.org/doc/1.18/reference/random/generator.html#numpy.random.Generator</a>) or other sources to see what parameters are required for each distribution. The NumPy documentation also lists the probability distributions from which random data can <span class="No-Break">be generated.</span></p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor161"/>Working with random processes</h1>
			<p>In this recipe, we will examine a simple example of a random process that models the number of bus arrivals at a stop over time. This process is <a id="_idIndexMarker414"/>called a <strong class="bold">Poisson process</strong>. A Poisson process, <img alt="" src="image/Formula_04_013.png"/>, has a single parameter, <img alt="" src="image/Formula_04_014.png"/>, which is usually called the <em class="italic">intensity</em> or <em class="italic">rate</em>, and the probability that <img alt="" src="image/Formula_04_015.png"/> takes the value <img alt="" src="image/Formula_04_016.png"/> at a given time <img alt="" src="image/Formula_04_017.png"/> is given by the <span class="No-Break">following formula:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_018.png"/></p>
			<p>This equation describes the probability that <img alt="" src="image/Formula_04_016.png"/> buses have arrived by time <img alt="" src="image/Formula_04_020.png"/>. Mathematically, this equation means that <img alt="" src="image/Formula_04_021.png"/> has a Poisson distribution with the parameter <img alt="" src="image/Formula_04_022.png"/>. There is, however, an easy way to construct a Poisson process by taking sums of inter-arrival times that follow an exponential distribution. For instance, let <img alt="" src="image/Formula_04_023.png"/> be the time between the (<img alt="" src="image/Formula_04_024.png"/>)-st arrival and the <img alt="" src="image/Formula_04_025.png"/>-th arrival, which are exponentially distributed with parameter <img alt="" src="image/Formula_04_026.png"/>. Now, we take the <span class="No-Break">following equation:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_027.png"/></p>
			<p>Here, the number <img alt="" src="image/Formula_04_028.png"/> is the maximum <img alt="" src="image/Formula_04_029.png"/> such that <img alt="" src="image/Formula_04_030.png"/>. This is the construction that we will work through in this recipe. We will also estimate the intensity of the process by taking the mean of the <span class="No-Break">inter-arrival times.</span></p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor162"/>Getting ready</h2>
			<p>Before we start, we<a id="_idIndexMarker415"/> import the <strong class="source-inline">default_rng</strong> routine from NumPy’s <strong class="source-inline">random</strong> module and create a new random number generator with a seed for the purpose <span class="No-Break">of demonstration:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<p>In addition to the random number generator, we also import NumPy as <strong class="source-inline">np</strong> and the Matplotlib <strong class="source-inline">pyplot</strong> module as <strong class="source-inline">plt</strong>. We also need to have the SciPy package available. </p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor163"/>How to do it...</h2>
			<p>The<a id="_idIndexMarker416"/> following steps show how to model the arrival of buses using a <a id="_idIndexMarker417"/><span class="No-Break">Poisson process:</span></p>
			<ol>
				<li value="1">Our first task is to create the sample inter-arrival times by sampling data from an exponential distribution. The <strong class="source-inline">exponential</strong> method on the NumPy <strong class="source-inline">Generator</strong> class requires a <strong class="source-inline">scale</strong> parameter, which is <img alt="" src="image/Formula_04_031.png"/>, where <img alt="" src="image/Formula_04_032.png"/> is the rate. We choose a rate of 4, and create 50 sample <span class="No-Break">inter-arrival times:</span><pre class="console">
rate = 4.0</pre><pre class="console">
inter_arrival_times = rng.exponential(</pre><pre class="console">
    scale=1./rate, size=50)</pre></li>
				<li>Next, we compute the actual arrival times by using the <strong class="source-inline">accumulate</strong> method of the NumPy <strong class="source-inline">add</strong> universal function. We also create an array containing the integers 0 to 49, representing the number of arrivals at <span class="No-Break">each point:</span><pre class="console">
arrivals = np.add.accumulate(inter_arrival_times)</pre><pre class="console">
count = np.arange(50)</pre></li>
				<li>Next, we<a id="_idIndexMarker418"/> plot the arrivals over time using the <strong class="source-inline">step</strong> plotting method: <pre class="console">
fig1, ax1 = plt.subplots()</pre><pre class="console">
ax1.step(arrivals, count, where="post")</pre><pre class="console">
ax1.set_xlabel("Time")</pre><pre class="console">
ax1.set_ylabel("Number of arrivals")</pre><pre class="console">
ax1.set_title("Arrivals over time")</pre></li>
			</ol>
			<p>The result is shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.4</em>, where the length of each horizontal line represents the <span class="No-Break">inter-arrival times:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer626">
					<img alt="Figure 4.4 – Arrivals over time where inter-arrival times are exponentially distributed&#13;&#10;" src="image/4.4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Arrivals over time where inter-arrival times are exponentially distributed</p>
			<ol>
				<li value="4">Next, we <a id="_idIndexMarker419"/>define a function that will evaluate the probability distribution of the counts at a time, which we will take as <strong class="source-inline">1</strong> here. This uses the formula for the Poisson<a id="_idIndexMarker420"/> distribution that we gave in the introduction to <span class="No-Break">this recipe:</span><pre class="console">
def probability(events, time=1, param=rate):</pre><pre class="console">
    return ((param*time)**events/factorial(</pre><pre class="console">
        events))*np.exp(- param*time)</pre></li>
				<li>Now, we plot the probability distribution over the count per unit of time, since we chose <strong class="source-inline">time=1</strong> in the previous step. We will add to this <span class="No-Break">plot later:</span><pre class="console">
fig2, ax2 = plt.subplots()</pre><pre class="console">
ax2.plot(N, probability(N), "k", label="True distribution")</pre><pre class="console">
ax2.set_xlabel("Number of arrivals in 1 time unit")</pre><pre class="console">
ax2.set_ylabel("Probability")</pre><pre class="console">
ax2.set_title("Probability distribution")</pre></li>
				<li>Now, we move on to estimate the rate from our sample data. We do this by computing the mean of the inter-arrival times, which, for exponential distribution, is an estimator of the <span class="No-Break">scale <img alt="" src="image/Formula_04_033.png"/>:</span><pre class="console">
estimated_scale = np.mean(inter_arrival_times)</pre><pre class="console">
estimated_rate = 1.0/estimated_scale</pre></li>
				<li>Finally, we plot the probability distribution with this estimated rate for the counts per unit of time. We plot this on top of the true probability distribution that we produced in <span class="No-Break"><em class="italic">step 5:</em></span><pre class="console">
ax2.plot(N, probability(</pre><pre class="console">
    N, param=estimated_rate),</pre><pre class="console">
    "k--",label="Estimated distribution")</pre><pre class="console">
ax2.legend()</pre></li>
			</ol>
			<p>The resulting <a id="_idIndexMarker421"/>plot is given in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.5</em>, where we can see that, apart from a <a id="_idIndexMarker422"/>small discrepancy, the estimated distribution is very close to the <span class="No-Break">true distribution:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer628">
					<img alt="Figure 4.5 – Distribution of the number of arrivals per time unit, estimated and true&#13;&#10;" src="image/4.5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Distribution of the number of arrivals per time unit, estimated and true</p>
			<p>The distribution shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.5</em> follows the Poisson distribution as described in the introduction to this recipe. You can see that moderate numbers of arrivals per unit of time are more likely than large<a id="_idIndexMarker423"/> numbers. The most likely counts are determined by the rate parameter <img alt="" src="image/Formula_04_034.png"/>, which is 4.0 in <span class="No-Break">this example.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor164"/>How it works...</h2>
			<p>Random processes exist everywhere. Roughly speaking, a random process is a system of related random variables, usually indexed with respect to time <img alt="" src="image/Formula_04_035.png"/> for a continuous random process, or by natural numbers <img alt="" src="image/Formula_04_036.png"/> for a discrete random process. Many (discrete) random processes satisfy <a id="_idIndexMarker424"/>the <strong class="bold">Markov property</strong>, which makes them a <strong class="bold">Markov chain</strong>. The Markov property is the statement that the process is <em class="italic">memoryless</em>, in that only the current value is important for the<a id="_idIndexMarker425"/> probabilities of the <span class="No-Break">next value.</span></p>
			<p>A Poisson process<a id="_idIndexMarker426"/> is a counting process that counts the number of events (bus arrivals) that occur in an amount of time if the events are randomly spaced (in time) with an exponential distribution with a fixed parameter. We constructed the Poisson process by sampling inter-arrival times from exponential distribution, following the construction we described in the introduction. However, it turns out that this fact (that the inter-arrival times are exponentially distributed) is a property of all Poisson processes when they are given their formal definition in terms <span class="No-Break">of probabilities.</span></p>
			<p>In this recipe, we sampled 50 points from an exponential distribution with a given <strong class="source-inline">rate</strong> parameter. We had to do a small conversion because the NumPy <strong class="source-inline">Generator</strong> method for sampling from an exponential distribution uses a related <strong class="source-inline">scale</strong> parameter, which is <strong class="source-inline">1</strong> over the <strong class="source-inline">rate</strong> parameter. Once we have these points, we create an array that contains cumulative sums of these exponentially distributed numbers. This creates our arrival times. The actual Poisson process is the one displayed in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.4</em> and is a combination of the arrival times with the corresponding number of events that have occurred at <span class="No-Break">that time.</span></p>
			<p>The mean (expected value) of an exponential distribution coincides with the scale parameter, so the mean of a sample drawn from an exponential distribution is one way to estimate the scale (<strong class="source-inline">rate</strong>) parameter. This estimate will not be perfect since our sample is relatively small. This is why there is a small discrepancy between the two plots in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor165"/>There’s more...</h2>
			<p>There are many types<a id="_idIndexMarker427"/> of random processes describing a wide variety of real-world scenarios. In this recipe, we modeled arrival times using a Poisson process. A <a id="_idIndexMarker428"/>Poisson process is a continuous random process, meaning that it is parameterized by a continuous variable, <img alt="" src="image/Formula_04_037.png"/>, rather than a discrete variable, <img alt="" src="image/Formula_04_038.png"/>. Poisson processes are actually Markov chains, under a suitably generalized definition of a Markov chain, and also an example of a <em class="italic">renewal process</em>. A renewal process<a id="_idIndexMarker429"/> is a process that describes the number of events that occur within a period of time. The Poisson process described here is an <a id="_idIndexMarker430"/>example of a <span class="No-Break">renewal process.</span></p>
			<p>Many Markov chains<a id="_idIndexMarker431"/> also satisfy some properties in addition to their defining Markov property. For example, a Markov chain is <em class="italic">homogeneous</em> if the following equality holds for all <img alt="" src="image/Formula_04_029.png"/>, <img alt="" src="image/Formula_04_025.png"/>, and <img alt="" src="image/Formula_04_041.png"/> <span class="No-Break">values:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_042.png"/></p>
			<p>In simple terms, this means that the probabilities of moving from one state to another over a single step do not change as we increase the number of steps. This is extremely useful for examining the long-term behavior of a <span class="No-Break">Markov chain.</span></p>
			<p>It is very easy to construct simple examples of homogeneous Markov chains. Suppose that we have two states, <img alt="" src="image/Formula_04_043.png"/> and <img alt="" src="image/Formula_04_044.png"/>. At any given step, we could be either at state <img alt="" src="image/Formula_04_045.png"/> or state <img alt="" src="image/Formula_04_044.png"/>. We move between states according to a certain probability. For instance, let’s say that the probability of transitioning from state <img alt="" src="image/Formula_04_047.png"/> to state <img alt="" src="image/Formula_04_045.png"/> is 0.4 and the probability of transitioning from <img alt="" src="image/Formula_04_045.png"/> to <img alt="" src="image/Formula_04_044.png"/> is 0.6. Similarly, let’s say that the probability of transitioning from <img alt="" src="image/Formula_04_044.png"/> to <img alt="" src="image/Formula_04_052.png"/> is 0.2, and transitioning from <img alt="" src="image/Formula_04_053.png"/> to <img alt="" src="image/Formula_04_054.png"/> is 0.8. Notice that both the probability of switching and the probability of staying the same sum 1 in both cases. We can represent the probability of transitioning from each state in matrix form given, in this case, with the <span class="No-Break">following equation:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_055.png"/></p>
			<p>This matrix is called the <em class="italic">transition matrix</em>. The idea <a id="_idIndexMarker432"/>here is that the probability of being in a particular state after a step is given by multiplying the vector containing the probability of being in state <img alt="" src="image/Formula_04_056.png"/> and <img alt="" src="image/Formula_04_044.png"/> (position 0 and 1, respectively). For example, if we start in state <img alt="" src="image/Formula_04_045.png"/>, then the probability vector will contain a 1 at index 0 and 0 at index 1. Then, the probability of being in state <img alt="" src="image/Formula_04_056.png"/> after 1 step is given by 0.4, and the probability of being in state <img alt="" src="image/Formula_04_044.png"/> is 0.6. This is what we expect given the probabilities we outlined previously. However, we could also write this calculation using the <span class="No-Break">matrix formula:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_061.png"/></p>
			<p>To get the probability of being in either state after two steps, we multiply the right-hand side vector again by the transition matrix, <img alt="" src="image/Formula_04_062.png"/>, to obtain <span class="No-Break">the following:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_063.png"/></p>
			<p>We can continue this process <em class="italic">ad infinitum</em> to obtain a sequence of state vectors, which constitute our Markov chain. This construction can be applied, with more states if necessary, to model many simple, <span class="No-Break">real-world problems.</span></p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor166"/>Analyzing conversion rates with Bayesian techniques</h1>
			<p>Bayesian probability<a id="_idIndexMarker433"/> allows us to systematically update our understanding (in a probabilistic sense) of a situation by considering data. In more technical language, we update the <em class="italic">prior</em> distribution (our current understanding) using data to obtain a <em class="italic">posterior</em> distribution. This is particularly useful, for example, when <a id="_idIndexMarker434"/>examining the proportion of users who go on to buy a product after viewing a website. We start with<a id="_idIndexMarker435"/> our prior belief distribution. For this, we will use <a id="_idIndexMarker436"/>the <em class="italic">beta</em> distribution, which models the probability of success given a number of observed successes (completed purchases) against failures (no purchases). For this recipe, we will assume that our prior belief is that we expect 25 successes from 100 views (75 fails). This means that our prior belief follows a beta (25, 75) distribution. Let’s say that we wish to calculate the probability that the true rate of success is at <span class="No-Break">least 33%.</span></p>
			<p>Our method is roughly divided into three steps. First, we need to understand our prior belief for the conversion rate, which we have decided follows a beta (25, 75) distribution. We compute the probability that the conversion rate is at least 33% by integrating (numerically) the probability density function for the prior distribution from 0.33 to 1. The next step is to apply Bayesian reasoning to update our prior belief with new information. Then, we can perform the same integration with the posterior (updated) belief to examine the probability that the conversion rate is at least 33% given this <span class="No-Break">new information.</span></p>
			<p>In this recipe, we will see how to use Bayesian techniques to update a prior belief based on new information for our <span class="No-Break">hypothetical website.</span></p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor167"/>Getting ready</h2>
			<p>As usual, we will need the NumPy and Matplotlib packages imported as <strong class="source-inline">np</strong> and <strong class="source-inline">plt</strong>, respectively. We will also require the SciPy package, imported <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">sp</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor168"/>How to do it...</h2>
			<p>The following <a id="_idIndexMarker437"/>steps show how to estimate and update <a id="_idIndexMarker438"/>conversion rate estimations using <span class="No-Break">Bayesian reasoning:</span></p>
			<ol>
				<li value="1">The first step is to set up the prior distribution. For this, we use the <strong class="source-inline">beta</strong> distribution object from the SciPy <strong class="source-inline">stats</strong> module, which has various methods for working with beta distribution. We import the <strong class="source-inline">beta</strong> distribution object from the <strong class="source-inline">stats</strong> module under a <strong class="source-inline">beta_dist</strong> alias and then create a convenience function for the probability <span class="No-Break">density function:</span><pre class="console">
from scipy.stats import beta as beta_dist</pre><pre class="console">
beta_pdf = beta_dist.pdf</pre></li>
				<li>Next, we need to compute the probability, under the prior belief distribution, that the success rate is at least 33%. To do this, we use the <strong class="source-inline">quad</strong> routine from the SciPy <strong class="source-inline">integrate</strong> module, which performs numerical integration of a function. We use this to integrate the probability density function for the beta distribution, imported in <em class="italic">step 1</em>, with our prior parameters. We print the probability according to our prior distribution to <span class="No-Break">the console:</span><pre class="console">
prior_alpha = 25</pre><pre class="console">
prior_beta = 75</pre><pre class="console">
args = (prior_alpha, prior_beta)</pre><pre class="console">
prior_over_33, err = sp.integrate.quad(</pre><pre class="console">
    beta_pdf, 0.33, 1, args=args)</pre><pre class="console">
print("Prior probability", prior_over_33)</pre><pre class="console">
# 0.037830787030165056</pre></li>
				<li>Now, suppose <a id="_idIndexMarker439"/>we have received some <a id="_idIndexMarker440"/>information about successes and failures over a new period of time. For example, we observed 122 successes and 257 failures over this period. We create new variables to reflect <span class="No-Break">these values:</span><pre class="console">
observed_successes = 122</pre><pre class="console">
observed_failures = 257</pre></li>
				<li>To obtain the parameter values for the posterior distribution with a beta distribution, we simply add the observed successes and failures to the <strong class="source-inline">prior_alpha</strong> and <strong class="source-inline">prior_beta</strong> <span class="No-Break">parameters, respectively:</span><pre class="console">
posterior_alpha = prior_alpha + observed_successes</pre><pre class="console">
posterior_beta = prior_beta + observed_failures</pre></li>
				<li>Now, we repeat our numerical integration to compute the probability that the success rate is now above 33% using the posterior distribution (with our new parameters computed earlier). Again, we print this probability to <span class="No-Break">the terminal:</span><pre class="console">
args = (posterior_alpha, posterior_beta)</pre><pre class="console">
posterior_over_33, err2 = sp.integrate.quad(</pre><pre class="console">
    beta_pdf, 0.33, 1, args=args)</pre><pre class="console">
print("Posterior probability", posterior_over_33)</pre><pre class="console">
# 0.13686193416281017</pre></li>
				<li>We can see<a id="_idIndexMarker441"/> here that the new<a id="_idIndexMarker442"/> probability, given the updated posterior distribution, is 14% as opposed to the prior 4%. This is a significant difference, although we are still not confident that the conversion rate is above 33% given these values. Now, we plot the prior and posterior distribution to visualize this increase in probability. To start with, we create an array of values and evaluate our probability density function based on <span class="No-Break">these values:</span><pre class="console">
p = np.linspace(0, 1, 500)</pre><pre class="console">
prior_dist = beta_pdf(p, prior_alpha, prior_beta)</pre><pre class="console">
posterior_dist = beta_pdf(</pre><pre class="console">
    p, posterior_alpha, posterior_beta)</pre></li>
				<li>Finally, we plot the two probability density functions computed in <em class="italic">step 6</em> onto a <span class="No-Break">new plot:</span><pre class="console">
fig, ax = plt.subplots()</pre><pre class="console">
ax.plot(p, prior_dist, "k--", label="Prior")</pre><pre class="console">
ax.plot(p, posterior_dist, "k", label="Posterior")</pre><pre class="console">
ax.legend()</pre><pre class="console">
ax.set_xlabel("Success rate")</pre><pre class="console">
ax.set_ylabel("Density")</pre><pre class="console">
ax.set_title("Prior and posterior distributions for success rate")</pre></li>
			</ol>
			<p>The resulting plot is shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.6</em>, where we can see that the posterior distribution is much more narrow and centered to the right of <span class="No-Break">the prior:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer659">
					<img alt="Figure 4.6 – Prior and posterior distributions of a success rate following a beta distribution&#13;&#10;" src="image/4.6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – Prior and posterior distributions of a success rate following a beta distribution</p>
			<p>We can see that the <a id="_idIndexMarker443"/>posterior distribution peaks at <a id="_idIndexMarker444"/>around 0.3, but most of the mass of the distribution lies close to <span class="No-Break">this peak.</span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/>How it works...</h2>
			<p>Bayesian techniques<a id="_idIndexMarker445"/> work by taking a prior belief (probability distribution) and using <em class="italic">Bayes’ theorem</em> to combine <a id="_idIndexMarker446"/>the prior belief with the likelihood of our data given this prior belief to form a posterior (updated) belief. This is similar to how we might understand things in real life. For example, when you wake up on a given day, you might have the belief (from a forecast or otherwise) that there is a 40% chance of rain outside. Upon opening the blinds, you see that it is very cloudy outside, which might indicate that rain is more likely, so we update our belief according to this new data to say a 70% chance <span class="No-Break">of rain.</span></p>
			<p>To <a id="_idIndexMarker447"/>understand how this works, we need to<a id="_idIndexMarker448"/> understand <em class="italic">conditional probability</em>. Conditional probability<a id="_idIndexMarker449"/> deals with the probability that one event will occur <em class="italic">given that</em> another event has already occurred. In symbols, the probability of event <img alt="" src="image/Formula_04_045.png"/> given that event <img alt="" src="image/Formula_04_044.png"/> has occurred is written <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_066.png"/></p>
			<p>Bayes’ theorem is a<a id="_idIndexMarker450"/> powerful tool that can be written (symbolically) <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_067.png"/></p>
			<p>The probability <img alt="" src="image/Formula_04_068.png"/> represents our prior belief. The event <img alt="" src="image/Formula_04_044.png"/> represents the data that we have gathered, so that <img alt="" src="image/Formula_04_070.png"/> is the likelihood that our data arose given our prior belief. The probability <img alt="" src="image/Formula_04_071.png"/> represents the probability that our data arose, and <img alt="" src="image/Formula_04_072.png"/> represents our posterior belief given the data. In practice, the probability <img alt="" src="image/Formula_04_073.png"/> can be difficult to calculate or otherwise estimate, so it is quite common to replace the strong equality above with a proportional version of <span class="No-Break">Bayes’ theorem:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_074.png"/></p>
			<p>In the recipe, we assumed that our prior belief was beta-distributed. The beta distribution has a probability density function given by the <span class="No-Break">following equation:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_075.png"/></p>
			<p>Here, <img alt="" src="image/Formula_04_076.png"/> is the gamma function. The likelihood is binomially distributed, which has a probability density function given by the <span class="No-Break">following equation:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_077.png"/></p>
			<p>Here, <img alt="" src="image/Formula_04_078.png"/> is the number of observations, and <img alt="" src="image/Formula_04_079.png"/> is one of those that were successful. In the recipe, we observed <img alt="" src="image/Formula_04_080.png"/> successes and <img alt="" src="image/Formula_04_081.png"/> failures, which gives <img alt="" src="image/Formula_04_082.png"/> and <img alt="" src="image/Formula_04_083.png"/>. To calculate the posterior distribution, we can use the fact that the beta distribution is a conjugate prior for the binomial distribution to see that the right-hand side of the proportional form of Bayes’ theorem is beta-distributed with parameters of <img alt="" src="image/Formula_04_084.png"/> and <img alt="" src="image/Formula_04_085.png"/>. This is what we used in the recipe. The fact that the beta distribution is a conjugate prior for binomial random variables makes them useful in <span class="No-Break">Bayesian statistics.</span></p>
			<p>The method we demonstrated in this recipe is a rather basic example of using a Bayesian method, but it is still useful for updating our prior beliefs when systematically given <span class="No-Break">new data.</span></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor170"/>There’s more...</h2>
			<p>Bayesian methods<a id="_idIndexMarker451"/> can be used for a wide variety of tasks, making it a powerful tool. In this recipe, we used a Bayesian approach to model the success rate of a website based on our prior belief of how it performs and additional data gathered from users. This is a rather complex example since we modeled our prior belief on a beta distribution. Here is another example of using Bayes’ theorem to examine two competing hypotheses using only simple probabilities (numbers between 0 <span class="No-Break">and 1).</span></p>
			<p>Suppose you place your keys in the same place every day when you return home, but one morning you wake up to find that they are not in this place. After searching for a short time, you cannot find them and so conclude that they must have vanished from existence. Let’s call this hypothesis <img alt="" src="image/Formula_04_086.png"/>. Now, <img alt="" src="image/Formula_04_087.png"/> certainly explains the data, <img alt="" src="image/Formula_04_088.png"/>, that you cannot find your keys – hence, the likelihood <img alt="" src="image/Formula_04_089.png"/> (if your keys vanished from existence, then you could not possibly find them). An alternative hypothesis is that you simply placed them somewhere else when you got home the night before. Let’s call this hypothesis <img alt="" src="image/Formula_04_090.png"/>. Now, this hypothesis also explains the data, so <img alt="" src="image/Formula_04_091.png"/>, but in reality, <img alt="" src="image/Formula_04_092.png"/> is far more plausible than <img alt="" src="image/Formula_04_093.png"/>. Let’s say that the probability that your keys completely vanished from existence is 1 in 1 million – this is a huge overestimation, but we need to keep the numbers reasonable – while you estimate that the probability that you placed them elsewhere the night before is 1 in 100. Computing the posterior probabilities, we have <span class="No-Break">the following:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_04_094.png"/></p>
			<p>This highlights the reality that it is 10,000 times more likely that you simply misplaced your keys as opposed to the fact that they simply vanished. Sure enough, you soon find your keys already in your pocket because you had picked them up earlier <span class="No-Break">that morning.</span></p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor171"/>Estimating parameters with Monte Carlo simulations</h1>
			<p>Monte Carlo methods<a id="_idIndexMarker452"/> broadly describe techniques that use random sampling to solve problems. These techniques are especially powerful when the underlying problem involves some kind of uncertainty. The general method involves performing large numbers of simulations, each sampling different inputs according to a given probability distribution, and then aggregating the results to give a better approximation of the true solution than any individual <span class="No-Break">sample solution.</span></p>
			<p>MCMC is a <a id="_idIndexMarker453"/>specific kind of Monte Carlo simulation in which we construct a Markov chain of successively better approximations of the true distribution that we seek. This works by accepting or rejecting a proposed state, sampled at random, based on carefully selected <em class="italic">acceptance probabilities</em> at each <a id="_idIndexMarker454"/>stage, with the aim of constructing a Markov chain whose unique stationary distribution is precisely the unknown distribution that we wish <span class="No-Break">to find.</span></p>
			<p>In this recipe, we will use the PyMC package and MCMC methods to estimate the parameters of a simple model. The package will deal with most of the technical details of running simulations, so we don’t need to go any further into the details of how the different MCMC algorithms <span class="No-Break">actually work.</span></p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor172"/>Getting ready</h2>
			<p>As usual, we import the NumPy package and Matplotlib <strong class="source-inline">pyplot</strong> module as <strong class="source-inline">np</strong> and <strong class="source-inline">plt</strong>, respectively. We also import and create a default random number generator, with a seed for the purpose of demonstration, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<p>We will also need a module from the SciPy package for this recipe as well as the PyMC package, which is a package for probabilistic programming. We import the PyMC package under the <span class="No-Break"><strong class="source-inline">pm</strong></span><span class="No-Break"> alias:</span></p>
			<pre class="source-code">
import pymc as pm</pre>
			<p>Let’s see how to use the PyMC package to estimate the parameters of a model given an observed, <span class="No-Break">noisy sample.</span></p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor173"/>How to do it...</h2>
			<p>Perform the<a id="_idIndexMarker455"/> following steps to use MCMC simulations<a id="_idIndexMarker456"/> to estimate the parameters of a simple model using <span class="No-Break">sample data:</span></p>
			<ol>
				<li value="1">Our first task is to create a function that represents the underlying structure that we wish to identify. In this case, we will be estimating the coefficients of a quadratic (a polynomial of degree 2). This function takes two arguments, which are the points in the range, which is fixed, and the variable parameters that we wish <span class="No-Break">to estimate:</span><pre class="console">
def underlying(x, params):</pre><pre class="console">
     return params[0]*x**2 + params[1]*x + params[2]</pre></li>
				<li>Next, we set up the <strong class="source-inline">true</strong> parameters and a <strong class="source-inline">size</strong> parameter that will determine how many points are in the sample that <span class="No-Break">we generate:</span><pre class="console">
size = 100</pre><pre class="console">
true_params = [2, -7, 6]</pre></li>
				<li>We generate the sample that we will use to estimate the parameters. This will consist of the underlying data, generated by the <strong class="source-inline">underlying</strong> function we defined in <em class="italic">step 1</em>, plus some random noise that follows a normal distribution. We first generate a range of <img alt="" src="image/Formula_04_095.png"/> values, which will stay constant throughout the recipe, and then use the <strong class="source-inline">underlying</strong> function and the <strong class="source-inline">normal</strong> method on our random number generator to generate the <span class="No-Break">sample data:</span><pre class="console">
x_vals = np.linspace(-5, 5, size)</pre><pre class="console">
raw_model = underlying(x_vals, true_params)</pre><pre class="console">
noise = rng.normal(loc=0.0, scale=10.0, size=size)</pre><pre class="console">
sample = raw_model + noise</pre></li>
				<li>It is a good idea to<a id="_idIndexMarker457"/> plot the sample data, with the <a id="_idIndexMarker458"/>underlying data overlaid, before we begin the analysis. We use the <strong class="source-inline">scatter</strong> plotting method to plot only the data points (without connecting lines), and then plot the underlying quadratic structure using a <span class="No-Break">dashed line:</span><pre class="console">
fig1, ax1 = plt.subplots()</pre><pre class="console">
ax1.scatter(x_vals, sample,</pre><pre class="console">
    label="Sampled data", color="k", </pre><pre class="console">
    alpha=0.6)</pre><pre class="console">
ax1.plot(x_vals, raw_model,</pre><pre class="console">
    "k--", label="Underlying model")</pre><pre class="console">
ax1.set_title("Sampled data")</pre><pre class="console">
ax1.set_xlabel("x")</pre><pre class="console">
ax1.set_ylabel("y")</pre></li>
			</ol>
			<p>The result is <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.7</em>, where we can see that the shape of the underlying model is still visible even with the noise, although the exact parameters of this model are no <span class="No-Break">longer obvious:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer692">
					<img alt="Figure 4.7 – Sampled data with the underlying model overlaid&#13;&#10;" src="image/4.7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – Sampled data with the underlying model overlaid</p>
			<ol>
				<li value="5">The basic object<a id="_idIndexMarker459"/> of PyMC programming is <a id="_idIndexMarker460"/>the <strong class="source-inline">Model</strong> class, which is usually created using the context manager interface. We also create our prior distributions for the parameters. In this case, we will assume that our prior parameters are normally distributed with a mean of 1 and a standard deviation of 1. We need three parameters, so we provide the <strong class="source-inline">shape</strong> argument. The <strong class="source-inline">Normal</strong> class creates random variables that will be used in the Monte <span class="No-Break">Carlo simulations:</span><pre class="console">
with pm.Model() as model:</pre><pre class="console">
    params = pm.Normal(</pre><pre class="console">
        "params", mu=1, sigma=1, shape=3)</pre></li>
				<li>We create a model for the underlying data, which can be done by passing the random variable, <strong class="source-inline">param</strong>, that we created in <em class="italic">step 6</em> into the <strong class="source-inline">underlying</strong> function that we defined in <em class="italic">step 1</em>. We also create a variable that handles our observations. For this, we use the <strong class="source-inline">Normal</strong> class since we know that our noise is normally distributed around the underlying data, <strong class="source-inline">y</strong>. We set a standard deviation of <strong class="source-inline">2</strong> and pass our observed <strong class="source-inline">sample</strong> data into the <strong class="source-inline">observed</strong> keyword argument (this is also inside the <span class="No-Break"><strong class="source-inline">Model</strong></span><span class="No-Break"> context):</span><pre class="console">
y = underlying(x_vals, params)</pre><pre class="console">
y_obs = pm.Normal("y_obs",</pre><pre class="console">
    mu=y, sigma=2, observed=sample)</pre></li>
				<li>To run the simulations, we need only call the <strong class="source-inline">sample</strong> routine inside the <strong class="source-inline">Model</strong> context. We pass the <strong class="source-inline">cores</strong> argument to speed up the calculations, but leave all of the other arguments as the default values: <pre class="console">
    trace = pm.sample(cores=4)</pre></li>
			</ol>
			<p>These simulations should take a short time <span class="No-Break">to execute.</span></p>
			<ol>
				<li value="8">Next, we plot the<a id="_idIndexMarker461"/> posterior distributions that use<a id="_idIndexMarker462"/> the <strong class="source-inline">plot_posterior</strong> routine from PyMC. This routine takes the <strong class="source-inline">trace</strong> result from the sampling step that performed the simulations. We create our own figures and axes using the <strong class="source-inline">plt.subplots</strong> routine in advance, but this isn’t strictly necessary. We are using three subplots on a single figure, and we pass the <strong class="source-inline">axs2</strong> tuple of <strong class="source-inline">Axes</strong> to the plotting routing under the <strong class="source-inline">ax</strong> <span class="No-Break">keyword argument:</span><pre class="console">
fig2, axs2 = plt.subplots(1, 3, tight_layout=True)</pre><pre class="console">
pm.plot_posterior(trace, ax=axs2, color="k")</pre></li>
			</ol>
			<p>The resulting plot is shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.8</em>, where you can see that each of these distributions is approximately normal, with a mean that is similar to the true <span class="No-Break">parameter values:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer693">
					<img alt="Figure 4.8 – Posterior distributions of estimated parameters&#13;&#10;" src="image/4.8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – Posterior distributions of estimated parameters</p>
			<ol>
				<li value="9">Now, retrieve<a id="_idIndexMarker463"/> the mean of each of the <a id="_idIndexMarker464"/>estimated parameters from the <strong class="source-inline">trace</strong> result. We access the estimated parameters from the posterior attribute on <strong class="source-inline">trace</strong> and then use the <strong class="source-inline">mean</strong> method on the <strong class="source-inline">params</strong> item (with <strong class="source-inline">axes=(0,1)</strong> to average over all chains and all samples) and convert this into a NumPy array. We print these estimated parameters in <span class="No-Break">the terminal:</span><pre class="console">
estimated_params = trace.posterior["params"].mean(</pre><pre class="console">
    axis=(0, 1)). to_numpy()</pre><pre class="console">
print("Estimated parameters", estimated_params)</pre><pre class="console">
# Estimated parameters [ 2.03220667 -7.09727509  5.27548983]</pre></li>
				<li>Finally, we use our estimated parameters to generate our estimated underlying data by passing the <img alt="" src="image/Formula_04_096.png"/> values and the estimated parameters to the <strong class="source-inline">underlying</strong> function defined in <em class="italic">step 1</em>. We then plot this estimated underlying data together with the true underlying data on the <span class="No-Break">same axes:</span><pre class="console">
estimated = underlying(x_vals, estimated_params)</pre><pre class="console">
fig3, ax3 = plt.subplots()</pre><pre class="console">
ax3.plot(x_vals, raw_model, "k", label="True model")</pre><pre class="console">
ax3.plot(x_vals, estimated, "k--", label="Estimated model")</pre><pre class="console">
ax3.set_title("Plot of true and estimated models")</pre><pre class="console">
ax3.set_xlabel("x")</pre><pre class="console">
ax3.set_ylabel("y")</pre><pre class="console">
ax3.legend()</pre></li>
			</ol>
			<p>The <a id="_idIndexMarker465"/>resulting plot is in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.9</em>, where<a id="_idIndexMarker466"/> there is only a small difference between these two models in <span class="No-Break">this range:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer695">
					<img alt="Figure 4.9 – True model and estimated model plotted on the same axes&#13;&#10;" src="image/4.9.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – True model and estimated model plotted on the same axes</p>
			<p>In <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.9</em> we can see that there is a small discrepancy between the true model and the <span class="No-Break">estimated model.</span></p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor174"/>How it works...</h2>
			<p>The<a id="_idIndexMarker467"/> interesting part of the code in this recipe<a id="_idIndexMarker468"/> can be found in the <strong class="source-inline">Model</strong> context manager. This object keeps track of the random variables, orchestrates the simulations, and keeps track of the state. The context manager gives us a convenient way to separate the probabilistic variables from the <span class="No-Break">surrounding code.</span></p>
			<p>We start by proposing a prior distribution for the distribution of the random variables representing our parameters, of which there are three. We proposed a normal distribution since we know that the parameters cannot stray too far from the value 1 (we can tell this by looking at the plot that we generated in <em class="italic">step 4</em>, for example). Using a normal distribution will give a higher probability to the values that are close to the current values. Next, we add the details relating to the observed data, which is used to calculate the acceptance probabilities that are used to either accept or reject a state. Finally, we start the sampler using the <strong class="source-inline">sample</strong> routine. This constructs the Markov chain and generates all of the <span class="No-Break">step data.</span></p>
			<p>The <strong class="source-inline">sample</strong> routine sets up the sampler based on the types of variables that will be simulated. Since the normal distribution is a continuous variable, the <strong class="source-inline">sample</strong> routine selected the <strong class="bold">No U-turn Sampler</strong> (<strong class="bold">NUTS</strong>). This is a<a id="_idIndexMarker469"/> reasonable general-purpose sampler for continuous variables. A common alternative to the NUTS is the Metropolis sampler, which is less reliable but faster than the NUTS in some cases. The PyMC documentation recommends using the NUTS <span class="No-Break">whenever possible.</span></p>
			<p>Once the sampling is complete, we plotted the posterior distribution of the trace (the states given by the Markov chain) to see the final shape of the approximations we generated. We can see here that all three of our random variables (parameters) are normally distributed around approximately the <span class="No-Break">correct value.</span></p>
			<p>Under the hood, PyMC uses Aesara – the successor to Theano used by PyMC3 – to speed up its calculations. This <a id="_idIndexMarker470"/>makes it possible for PyMC to perform computations on a <strong class="bold">Graphics Processing Unit</strong> (<strong class="bold">GPU</strong>) rather than on the <strong class="bold">Central Processing Unit</strong> (<strong class="bold">CPU</strong>) for a <a id="_idIndexMarker471"/>considerable boost to computation speed. </p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor175"/>There’s more...</h2>
			<p>The Monte Carlo method<a id="_idIndexMarker472"/> is very flexible and the example we gave here is one particular case where it can be used. A more typical basic example of where the Monte Carlo method is applied is for estimating the value of integrals – commonly, Monte Carlo integration. A really interesting case of Monte Carlo integration is estimating the value of <img alt="" src="image/Formula_04_097.png"/>. Let’s briefly look at how <span class="No-Break">this works.</span></p>
			<p>First, we take the unit disk, whose radius is 1 and therefore has an area of <img alt="" src="image/Formula_04_098.png"/>. We can enclose this disk inside a square with vertices at the points <img alt="" src="image/Formula_04_099.png"/>, <img alt="" src="image/Formula_04_100.png"/>, <img alt="" src="image/Formula_04_101.png"/>, and <img alt="" src="image/Formula_04_102.png"/>. This square has an area of 4 since the edge length is 2. Now, we can generate random points uniformly over this square. When we do this, the probability that any one of these random points lies inside a given region is proportional to the area of that region. Thus, the area of a region can be estimated by multiplying the proportion of randomly generated points that lie within the region by the total area of the square. In particular, we can estimate the area of the disk (when the radius is 1, this is <img alt="" src="image/Formula_04_103.png"/>) by simply multiplying the number of randomly generate points that lie within the disk by 4 and dividing by the total number of points <span class="No-Break">we generated.</span></p>
			<p>We can easily write a<a id="_idIndexMarker473"/> function in Python that performs this calculation, which might be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
import numpy as np
from numpy.random import default_rng
def estimate_pi(n_points=10000):
    rng = default_rng()
    points = rng.uniform(-1, 1, size=(2, n_points))
    inside = np.less(points[0, :]**2 + points[1, :]**2, 1)
    return 4.0*inside.sum() / n_points</pre>
			<p>Running this function just once will give a reasonable approximation <span class="No-Break">of π:</span></p>
			<pre class="source-code">
estimate_pi()  # 3.14224</pre>
			<p>We can improve the accuracy of our estimation by using more points, but we could also run this a number of times and average the results. Let’s run this simulation 100 times and average the results (we’ll use concurrent futures to parallelize this so that we can run larger numbers of samples if <span class="No-Break">we want):</span></p>
			<pre class="source-code">
from statistics import mean
results = list(estimate_pi() for _ in  range(100))
print(mean(results))</pre>
			<p>Running this code once prints the estimated value of <img alt="" src="image/Formula_04_104.png"/> as 3.1415752, which is an even better estimate of the <span class="No-Break">true value.</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor176"/>See also</h2>
			<p>The PyMC package has many features that are documented by numerous examples (<a href="https://docs.pymc.io/">https://docs.pymc.io/</a>). There is also another probabilistic programming library based on TensorFlow (<a href="https://www.tensorflow.org/probability"><span class="No-Break">https://www.tensorflow.org/probability</span></a><span class="No-Break">).</span></p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/>Further reading</h1>
			<p>A good, comprehensive reference for probability and random processes is the <span class="No-Break">following book:</span></p>
			<ul>
				<li><em class="italic">Grimmett, G. and Stirzaker, D. (2009). Probability and random processes. 3rd ed. Oxford: Oxford </em><span class="No-Break"><em class="italic">Univ. Press.</em></span></li>
			</ul>
			<p>An easy introduction to Bayes’ theorem and Bayesian statistics is <span class="No-Break">the following:</span></p>
			<ul>
				<li><em class="italic">Kurt, W. (2019). Bayesian statistics the fun way. San Francisco, CA: No Starch </em><span class="No-Break"><em class="italic">Press, Inc.</em></span></li>
			</ul>
		</div>
	</body></html>