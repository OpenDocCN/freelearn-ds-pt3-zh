- en: Chapter 9. Numerical Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the root of a mathematical function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimizing a mathematical function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting a function to data with nonlinear least squares
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the equilibrium state of a physical system by minimizing its potential
    energy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mathematical optimization** is a wide area of applied mathematics. It consists
    of finding the best solution to a given problem. Many real-world problems can
    be expressed in an optimization framework. What is the shortest path on the road
    from point A to point B? What is the best strategy to solve a puzzle? What is
    the most energy-efficient shape of a car (automotive aerodynamics)? Mathematical
    optimization is relevant in many domains including engineering, economics, finance,
    operations research, image processing, data analysis, and others.'
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, an optimization problem generally consists of finding the maximum
    or minimum value of a function. We sometimes use the terms **continuous optimization**
    or **discrete optimization**, according to whether the function variable is real-valued
    or discrete.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on numerical methods for solving continuous optimization
    problems. Many optimization algorithms are implemented in the `scipy.optimize`
    module. We will come across other instances of optimization problems in several
    other chapters of this book. For example, we will see discrete optimization problems
    in [Chapter 14](ch14.html "Chapter 14. Graphs, Geometry, and Geographic Information
    Systems"), *Graphs, Geometry, and Geographic Information Systems*. In this introduction,
    we will give a few important definitions and key concepts related to mathematical
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The objective function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will study methods to find a root or an **extremum** of a real-valued function
    *f* called the **objective function**. An extremum is either a maximum or a minimum
    of a function. This mathematical function is generally implemented in a Python
    function. It can accept one or several variables, it can be continuous or not,
    and so on. The more assumptions we have about the function, the easier it can
    be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A maximum of *f* is a minimum of *-f*, so any minimization algorithm can be
    used to maximize a function by considering the *opposite* of that function. Therefore,
    from now on, when we talk about *minimization*, we will really mean *minimization
    or maximization*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Convex functions** are generally easier to optimize than non-convex functions,
    as they satisfy certain useful properties. For example, any local minimum is necessarily
    a global minimum. The field of **convex optimization** deals with algorithms that
    are specifically adapted to the optimization of convex functions on convex domains.
    Convex optimization is an advanced topic, and we can''t cover much of it here.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Differentiable functions** have gradients, and these gradients can be particularly
    useful in optimization algorithms. Similarly, **continuous functions** are typically
    easier to optimize than non-continuous functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, functions with a single variable are easier to optimize than functions
    with multiple variables.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the most adequate optimization algorithm depends on the properties
    satisfied by the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: Local and global minima
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **minimum** of a function *f* is a point x[0] such that *f(x)* ![Local and
    global minima](img/4818OS_09_22.jpg) *f(x[0]**)*, for a particular set of points
    *x* in *E*. When this inequality is satisfied on the whole set *E*, we refer to
    *x[0]* as a **global minimum**. When it is only satisfied locally (around the
    point *x[0]*), we say that *x[0]* is a **local minimum**. A **maximum** is defined
    similarly.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *f* is differentiable, an extremum *x[0]* satisfies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Local and global minima](img/4818OS_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, finding the extrema of an objective function is closely related to
    finding the roots of the derivative. However, a point x[0] satisfying this property
    is not necessarily an extremum.
  prefs: []
  type: TYPE_NORMAL
- en: It is more difficult to find global minima than to find local minima. In general,
    when an algorithm finds a local minimum, there is no guarantee that it is also
    a global minimum. Frequently, an algorithm seeking a global minimum stays *stuck*
    in a local minimum. This problem needs to be accounted for, specifically in global
    minimization algorithms. However, things are simpler with convex functions since
    these do not have strictly local minima. Moreover, there are many cases where
    finding a local minimum is good enough (for example, when looking for a good solution
    to a problem rather than the absolute best solution). Finally, let's note that
    a global minimum or maximum does not necessarily exist (the function can go to
    infinity). In that case, it may be necessary to constrain the space search; this
    is the subject of **constrained optimization**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Local and global minima](img/4818OS_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Local and global extrema
  prefs: []
  type: TYPE_NORMAL
- en: Constrained and unconstrained optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Unconstrained optimization**: Finding the minimum of a function *f* on the
    full set *E* where *f* is defined'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constrained optimization**: Finding the minimum of a function *f* on a subset
    *E''* of *E*; this set is generally described by equalities and inequalities:![Constrained
    and unconstrained optimization](img/4818OS_09_03.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, the *g[i]* and *h[j]* are arbitrary functions defining the constraints.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For example, optimizing the aerodynamic shape of a car requires constraints
    on parameters such as the volume and mass of the car, the cost of the production
    process, and others.
  prefs: []
  type: TYPE_NORMAL
- en: Constrained optimization is generally harder than unconstrained optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic and stochastic algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some global optimization algorithms are **deterministic**, others are **stochastic**.
    Typically, deterministic methods are adapted to well-behaved functions, whereas
    stochastic methods may be useful with highly irregular and noisy functions.
  prefs: []
  type: TYPE_NORMAL
- en: The reason is that deterministic algorithms may be stuck in local minima, particularly
    if there are many non-global local minima. By spending some time exploring the
    space *E*, stochastic algorithms may have a chance of finding a global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SciPy lecture notes are an excellent reference on mathematical optimization
    with SciPy and are available at [http://scipy-lectures.github.io/advanced/mathematical_optimization/index.html](http://scipy-lectures.github.io/advanced/mathematical_optimization/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reference manual of `scipy.optimize` available at [http://docs.scipy.org/doc/scipy/reference/optimize.html](http://docs.scipy.org/doc/scipy/reference/optimize.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of mathematical optimization on Wikipedia, available at [http://en.wikipedia.org/wiki/Mathematical_optimization](http://en.wikipedia.org/wiki/Mathematical_optimization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extrema, minima, and maxima on Wikipedia, available at [http://en.wikipedia.org/wiki/Maxima_and_minima](http://en.wikipedia.org/wiki/Maxima_and_minima)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convex optimization on Wikipedia, available at [http://en.wikipedia.org/wiki/Convex_optimization](http://en.wikipedia.org/wiki/Convex_optimization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced optimization methods for image processing by Gabriel Peyré, available
    at [http://github.com/gpeyre/numerical-tours](http://github.com/gpeyre/numerical-tours)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the root of a mathematical function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this short recipe, we will see how to use SciPy to find the root of a simple
    mathematical function of a single real variable.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import NumPy, SciPy, `scipy.optimize`, and matplotlib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the mathematical function *f(x)=cos(x)-x* in Python. We will try
    to find a root of this function numerically. Here, a root corresponds to a fixed
    point of the cosine function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s plot this function on the interval *[-5, 5]* (using 1000 samples):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'We see that this function has a unique root on this interval (this is because
    the function''s sign changes on this interval). The `scipy.optimize` module contains
    a few root-finding functions that are adapted here. For example, the `bisect()`
    function implements the **bisection method** (also called the **dichotomy method**).
    It takes as input the function and the interval to find the root in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s visualize the root on the plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'A faster and more powerful method is `brentq()` (**Brent''s method**). This
    algorithm also requires *f* to be continuous and *f(a)* and *f(b)* to have different
    signs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `brentq()` method is faster than `bisect()`. If the conditions are satisfied,
    it is a good idea to try Brent''s method first:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The bisection method consists of iteratively cutting an interval in half and
    selecting a subinterval that necessarily contains a root. This method is based
    on the fact that, if *f* is a continuous function of a single real variable, *f(a)>0*,
    and *f(b)<0*, then *f* has a root in *(a,b)* (**intermediate value theorem**).
  prefs: []
  type: TYPE_NORMAL
- en: '**Brent''s method** is a popular hybrid algorithm combining root bracketing,
    interval bisection, and inverse quadratic interpolation. It is a default method
    that works in many cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's also mention **Newton's method**. The idea is to approximate *f(x)* by
    its tangent (found with *f'(x)*) and find the intersection with the *y=0* line.
    If *f* is regular enough, the intersection point will be closer to the actual
    root of *f*. By iterating this operation, the algorithm generally converges to
    the sought solution.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few references:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation of `scipy.optimize` available at [http://docs.scipy.org/doc/scipy/reference/optimize.html#root-finding](http://docs.scipy.org/doc/scipy/reference/optimize.html#root-finding)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A course on root finding with SciPy available at [http://quant-econ.net/scipy.html#roots-and-fixed-points](http://quant-econ.net/scipy.html#roots-and-fixed-points)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Bisection method on Wikipedia, available at [http://en.wikipedia.org/wiki/Bisection_method](http://en.wikipedia.org/wiki/Bisection_method)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intermediate value theorem on Wikipedia, available at [http://en.wikipedia.org/wiki/Intermediate_value_theorem](http://en.wikipedia.org/wiki/Intermediate_value_theorem)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brent's method on Wikipedia, available at [http://en.wikipedia.org/wiki/Brent%27s_method](http://en.wikipedia.org/wiki/Brent%27s_method)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newton's method on Wikipedia, available at [http://en.wikipedia.org/wiki/Newton%27s_method](http://en.wikipedia.org/wiki/Newton%27s_method)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Minimizing a mathematical function* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimizing a mathematical function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mathematical optimization deals mainly with the problem of finding a minimum
    or a maximum of a mathematical function. Frequently, a real-world numerical problem
    can be expressed as a function minimization problem. Such examples can be found
    in statistical inference, machine learning, graph theory, and other areas.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are many function minimization algorithms, a generic and universal
    method does not exist. Therefore, it is important to understand the differences
    between existing classes of algorithms, their specificities, and their respective
    use cases. We should also have a good understanding of our problem and our objective
    function; is it continuous, differentiable, convex, multidimensional, regular,
    or noisy? Is our problem constrained or unconstrained? Are we seeking local or
    global minima?
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will demonstrate a few usage examples of the function minimization
    algorithms implemented in SciPy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, let''s define a simple mathematical function (the opposite of the **cardinal
    sine**). This function has many local minima but a single global minimum ([http://en.wikipedia.org/wiki/Sinc_function](http://en.wikipedia.org/wiki/Sinc_function)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s plot this function on the interval *[-20, 20]* (with 1000 samples):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The `scipy.optimize` module comes with many function minimization routines.
    The `minimize()` function offers a unified interface to many algorithms. The **Broyden–Fletcher–Goldfarb–Shanno**
    (**BFGS**) algorithm (the default algorithm in `minimize()`) gives good results
    in general. The `minimize()` function requires an initial point as argument. For
    scalar univariate functions, we can also use `minimize_scalar()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Starting from *x[0]**=3*, the algorithm was able to find the actual global
    minimum, as shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now, if we start from an initial point that is further away from the actual
    global minimum, the algorithm converges towards a *local* minimum only:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Like most function minimization algorithms, the BFGS algorithm is efficient
    at finding *local* minima, but not necessarily *global* minima, especially on
    complicated or noisy objective functions. A general strategy to overcome this
    problem is to combine such algorithms with an exploratory grid search on the initial
    points. Another option is to use a different class of algorithms based on heuristics
    and stochastic methods. A popular example is the **simulated annealing method**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: This time, the algorithm was able to find the global minimum.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let's define a new function, in two dimensions this time, called the **Lévi
    function**:![How to do it…](img/4818OS_09_10.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This function is very irregular and may be difficult to minimize in general.
    It is one of the many **test functions for optimization** that researchers have
    developed to study and benchmark optimization algorithms ([http://en.wikipedia.org/wiki/Test_functions_for_optimization](http://en.wikipedia.org/wiki/Test_functions_for_optimization)):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s display this function with `imshow()`, on the square *[-10,10]²*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The BFGS algorithm also works in multiple dimensions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many function minimization algorithms are based on the fundamental idea of **gradient
    descent**. If a function *f* is differentiable, then at every point, the opposite
    of its gradient points to the direction of the greatest decrease rate of the function.
    By following this direction, we can expect to find a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: This operation is generally done iteratively, by following the direction of
    the gradient with a small step. The way this step is computed depends on the optimization
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Newton's method can also be used in this context of function minimization. The
    idea is to find a root of *f'* with Newton's method, thereby making use of the
    *second* derivative *f''*. In other words, we approximate *f* with a *quadratic*
    function instead of a *linear* function. In multiple dimensions, this is done
    by computing the **Hessian** (second derivatives) of *f*. By performing this operation
    iteratively, we can expect the algorithm to converge towards a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: When the computation of the Hessian is too costly, we can compute an *approximation*
    of the Hessian. Such methods are called **Quasi-Newton methods**. The BFGS algorithm
    belongs to this class of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: These algorithms make use of the objective function's gradient. If we can compute
    an analytical expression of the gradient, we should provide it to the minimization
    routine. Otherwise, the algorithm will compute an approximation of the gradient
    that may not be reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **simulated annealing** algorithm is a generic probabilistic metaheuristic
    for the global optimization problem. It is based on an analogy with thermodynamic
    systems: by increasing and decreasing the temperature, the configuration may converge
    to a state of low energy.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many stochastic global optimization methods based on metaheuristics.
    They are generally less well-theoretically grounded than the deterministic optimization
    algorithms previously described, and convergence is not always guaranteed. However,
    they may be useful in situations where the objective function is very irregular
    and noisy, with many local minima. The **Covariance Matrix Adaptation Evolution
    Strategy** (**CMA-ES**) algorithm is a metaheuristic that performs well in many
    situations. It is currently not implemented in SciPy, but there's a Python implementation
    in one of the references given later.
  prefs: []
  type: TYPE_NORMAL
- en: SciPy's `minimize()` function accepts a `method` keyword argument to specify
    the minimization algorithm to use. This function returns an object containing
    the results of the optimization. The `x` attribute is the point reaching the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few further references:'
  prefs: []
  type: TYPE_NORMAL
- en: The `scipy.optimize` reference documentation available at [http://docs.scipy.org/doc/scipy/reference/optimize.html](http://docs.scipy.org/doc/scipy/reference/optimize.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An excellent lecture on mathematical optimization with SciPy available at [http://scipy-lectures.github.io/advanced/mathematical_optimization/](http://scipy-lectures.github.io/advanced/mathematical_optimization/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Definition of the gradient on Wikipedia, available at [http://en.wikipedia.org/wiki/Gradient](http://en.wikipedia.org/wiki/Gradient)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newton's method on Wikipedia, available at [http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization](http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quasi-Newton methods on Wikipedia, available at [http://en.wikipedia.org/wiki/Quasi-Newton_method](http://en.wikipedia.org/wiki/Quasi-Newton_method)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metaheuristics for function minimization on Wikipedia, available at [http://en.wikipedia.org/wiki/Metaheuristic](http://en.wikipedia.org/wiki/Metaheuristic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulated annealing on Wikipedia, available at [http://en.wikipedia.org/wiki/Simulated_annealing](http://en.wikipedia.org/wiki/Simulated_annealing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CMA-ES algorithm described at [http://en.wikipedia.org/wiki/CMA-ES](http://en.wikipedia.org/wiki/CMA-ES)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Python implementation of CMA-ES available at [http://www.lri.fr/~hansen/cmaes_inmatlab.html#python](http://www.lri.fr/~hansen/cmaes_inmatlab.html#python)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Finding the root of a mathematical function* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting a function to data with nonlinear least squares
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will show an application of numerical optimization to **nonlinear
    least squares curve fitting**. The goal is to fit a function, depending on several
    parameters, to data points. In contrast to the linear least squares method, this
    function does not have to be linear in those parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We will illustrate this method on artificial data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import the usual libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We define a logistic function with four parameters:![How to do it…](img/4818OS_09_13.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define four random parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we generate random data points by using the sigmoid function and adding
    a bit of noise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is a plot of the data points, with the particular sigmoid used for their
    generation (in dashed black):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'We now assume that we only have access to the data points and not the underlying
    generative function. These points could have been obtained during an experiment.
    By looking at the data, the points appear to approximately follow a sigmoid, so
    we may want to try to fit such a curve to the points. That''s what **curve fitting**
    is about. SciPy''s `curve_fit()` function allows us to fit a curve defined by
    an arbitrary Python function to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s take a look at the fitted sigmoid curve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The fitted sigmoid appears to be reasonably close to the original sigmoid used
    for data generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In SciPy, nonlinear least squares curve fitting works by minimizing the following
    cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4818OS_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![How it works…](img/4818OS_09_20.jpg) is the vector of parameters (in
    our example, ![How it works…](img/4818OS_09_20.jpg) *=(a,b,c,d)*).
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear least squares is really similar to linear least squares for linear
    regression. Whereas the function *f* is *linear* in the parameters with the linear
    least squares method, it is *not linear* here. Therefore, the minimization of
    *S(*![How it works…](img/4818OS_09_20.jpg) *)* cannot be done analytically by
    solving the derivative of *S* with respect to ![How it works…](img/4818OS_09_20.jpg).
    SciPy implements an iterative method called the **Levenberg-Marquardt algorithm**
    (an extension of the Gauss–Newton algorithm).
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are further references:'
  prefs: []
  type: TYPE_NORMAL
- en: Reference documentation of `curvefit` available at [http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonlinear least squares on Wikipedia, available at [http://en.wikipedia.org/wiki/Non-linear_least_squares](http://en.wikipedia.org/wiki/Non-linear_least_squares)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levenberg-Marquardt algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm](http://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Minimizing a mathematical function* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the equilibrium state of a physical system by minimizing its potential
    energy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will give an application example of the function minimization
    algorithms described earlier. We will try to numerically find the equilibrium
    state of a physical system by minimizing its potential energy.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, we'll consider a structure made of masses and springs, attached
    to a vertical wall and subject to gravity. Starting from an initial position,
    we'll search for the equilibrium configuration where the gravity and elastic forces
    compensate.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import NumPy, SciPy, and matplotlib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define a few constants in the International System of Units:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the initial positions of the masses. They are arranged on a two-dimensional
    grid with two lines and *n/2* columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s define the connectivity matrix between the masses. Coefficient
    *(i,j)* is 1 if masses *i* and *j* are connected by a spring, 0 otherwise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also specify the spring stiffness of each spring. It is *l*, except for
    *diagonal* springs where it is ![How to do it…](img/4818OS_09_21.jpg):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get the indices of the spring connections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This `dist` function computes the distance matrix (distance between any pair
    of masses):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define a function that displays the system. The springs are colored according
    to their tension:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the system in its initial configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'To find the equilibrium state, we need to minimize the total potential energy
    of the system. The following function computes the energy of the system given
    the positions of the masses. This function is explained in the *How it works…*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s compute the potential energy of the initial configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s minimize the potential energy with a function minimization method.
    We need a **constrained optimization algorithm**, because we make the assumption
    that the first two masses are fixed to the wall. Therefore, their positions cannot
    change. The **L-BFGS-B** algorithm, a variant of the BFGS algorithm, accepts bound
    constraints. Here, we force the first two points to stay at their initial positions,
    whereas there are no constraints on the other points. The `minimize()` function
    accepts a `bounds` list containing, for each dimension, a pair of `[min, max]`
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s display the stable configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it…](img/4818OS_09_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: This configuration looks realistic. The tension appears to be maximal on the
    top springs near the wall.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This example is conceptually simple. The state of the system is only described
    by the positions of the masses. If we can write a Python function that returns
    the total energy of the system, finding the equilibrium is just a matter of minimizing
    this function. This is the **principle of minimum total potential energy**, due
    to the second law of thermodynamics.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we give an expression of the total energy of the system. Since we are
    only interested in the *equilibrium*, we omit any kinetic aspect and we only consider
    potential energy due to gravity (**gravitational force**) and spring forces (**elastic
    potential energy**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Letting *U* be the total potential energy of the system, *U* can be expressed
    as the sum of the gravitational potential energies of the masses and the elastic
    potential energies of the springs. Therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4818OS_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*m* is the mass'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*g* is the gravity of Earth'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* is the stiffness of the springs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p[i] = (x[i], y[i])* is the position of mass *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*a[ij]* is 1 if masses *i* and *j* are attached by a spring, 0 otherwise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*l[ij]* is the relaxed length of spring *(i,j)*, or 0 if masses *i* and *j*
    are not attached'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `energy()` function implements this formula using vectorized computations
    on NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following references contain details about the physics behind this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Potential energy on Wikipedia, available at [http://en.wikipedia.org/wiki/Potential_energy](http://en.wikipedia.org/wiki/Potential_energy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic potential energy on Wikipedia, available at [http://en.wikipedia.org/wiki/Elastic_potential_energy](http://en.wikipedia.org/wiki/Elastic_potential_energy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hooke's law, which is the linear approximation of the springs' response, described
    at [http://en.wikipedia.org/wiki/Hooke%27s_law](http://en.wikipedia.org/wiki/Hooke%27s_law)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principle of minimum energy on Wikipedia, available at [http://en.wikipedia.org/wiki/Minimum_total_potential_energy_principle](http://en.wikipedia.org/wiki/Minimum_total_potential_energy_principle)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a reference about the optimization algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: L-BFGS-B algorithm on Wikipedia, available at [http://en.wikipedia.org/wiki/Limited-memory_BFGS#L-BFGS-B](http://en.wikipedia.org/wiki/Limited-memory_BFGS#L-BFGS-B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Minimizing a mathematical function* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
