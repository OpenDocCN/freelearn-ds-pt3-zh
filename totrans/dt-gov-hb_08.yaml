- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The quality of decisions is tightly linked to the quality of data that supports
    them. As a result, the quality of data impacts every facet of the organization.
    It’s an (appropriately) bold statement that is often under-appreciated. From day-to-day
    operations to long-term strategic direction, having high-quality, reliable, and
    trustworthy information is not a “nice to have;” it’s a must-have for any company.
    Having quality data is critical for the success of any company of any size. How
    you operationalize ensuring you have quality data in your organization should
    vary based on the size and complexity of your organization, but the need for data
    quality is static. It’s at the very core of running any business.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the ever-evolving landscape of data-driven decision-making, there exists
    a profound truth that we, as data custodians and stewards, must humbly acknowledge:
    the quality of our data is the cornerstone upon which all our analytical endeavors
    stand. In this chapter, I will delve deep into the vital realm of data quality,
    for it is here that we will confront the very essence of our responsibilities
    as guardians of the information our companies depend on. With a direct and unwavering
    focus, I aim to illuminate the significance of data quality in the context of
    data governance. In this chapter, I will uncover not only the intrinsic value
    of high-quality data but also the transformative potential it holds for organizations.
    I can’t emphasize enough the undeniable importance of ensuring that our data is
    not just data, but reliable, insightful, and, above all, trustworthy.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s quite easy to suggest that data quality is “everyone’s job” (and it is);
    however, ultimately, it is up to you to define what is needed, define and provide
    the implementation of data quality capabilities, and drive the importance of data
    quality for the company you work for. You will be held accountable for the quality
    of data enterprise-wide, regardless of how much of it is under your direct control.
    This may feel unfair, but as the data leader, the duty is yours. When a major
    data error is uncovered, whether you are directly involved or not, you will be
    brought in to help solve it. The burden will rest on your shoulders. Therefore,
    this chapter and the guidance herein are critical for your success.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data quality defined
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an optimal data quality capability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up data quality for success
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the hallmarks of the success of a Chief Data & Analytics Officer is defining
    and implementing a trusted data capability. This will show the company not just
    that data is trustworthy, but provides transparency into *why* they can trust
    the data. When it comes to designing optimal capabilities for data quality, **Trusted
    Data** is one we will explore later in this chapter. First, let’s start with some
    key definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality defined
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data Quality is the data governance capability that refers to the degree to
    which data is accurate, reliable, and fit for its intended purpose in a given
    context. There are several dimensions by which data quality is assessed; these
    include completeness, accuracy, timeliness, consistency, and relevance. As mentioned
    previously, data quality is an essential capability for all organizations. Overall
    data quality across the company is critical, as is data quality in individual
    data elements, on key reports, as a part of operations, and for the overall functioning
    of the business. Data quality is the core of building trust in our information.
    Next, each data quality dimension is defined, along with a few examples to help
    contextualize what data quality is and how it may show up in your company. We
    will move quickly into the core capabilities needed to apply these core dimensions
    after we ground ourselves on the basics:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data** **Quality Dimension** | **Definition** | **Example(s)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | Accuracy refers to the correctness of data. Accurate data is free
    from errors, and it reflects the real-world entities and events it is supposed
    to represent. Inaccurate data can lead to incorrect conclusions and decisions.
    | In a customer database, a person’s birthdate is recorded as January 15, 1980,
    instead of January 25, 1980. |'
  prefs: []
  type: TYPE_TB
- en: '| Completeness | Completeness refers to whether all the required data elements
    are present. Incomplete data can hinder analysis and lead to gaps in understanding.
    | An inventory database lacks records for certain products, leaving gaps in the
    list of available items. |'
  prefs: []
  type: TYPE_TB
- en: '| Consistency | Consistency ensures that data is uniform and follows established
    standards. Inconsistent data can arise from variations in data entry, formatting,
    or terminology, leading to confusion and data integration challenges. | In a sales
    dataset, the currency is inconsistently recorded as “USD,” “US Dollars,” or “$,”
    making it difficult to aggregate sales figures accurately. |'
  prefs: []
  type: TYPE_TB
- en: '| Timeliness | Timeliness pertains to how up-to-date the data is. Timely data
    is relevant for decision-making and analysis, while outdated data can lead to
    missed opportunities or misinformed decisions. | A financial report for the first
    quarter of the year is not updated until several months into the second quarter,
    making it less relevant for decision-making. |'
  prefs: []
  type: TYPE_TB
- en: '| Relevance | Relevance is about whether the data is suitable for the specific
    task or analysis at hand. Irrelevant data can clutter datasets and make it harder
    to extract meaningful insights. | In a marketing campaign analysis, data on customer
    shoe sizes is included, even though it has no bearing on the campaign’s effectiveness.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Validity | Validity checks whether data conforms to predefined rules, constraints,
    or business logic. Valid data adheres to the defined criteria and ensures data
    integrity. | An email address field contains entries that do not follow a valid
    email format, such as `user(at)example(dot)com` instead of `user@example.com`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Integrity | Data integrity ensures that data remains accurate and consistent
    throughout its life cycle, preventing unauthorized changes or corruption. | A
    database administrator accidentally deletes or modifies records without proper
    authorization, resulting in data integrity issues. |'
  prefs: []
  type: TYPE_TB
- en: '| Trustworthiness | Trustworthiness reflects the reliability and credibility
    of the data source. Data from reputable sources is more likely to be of higher
    quality. | Data obtained from a well-established, government-regulated financial
    institution is considered more trustworthy compared to data from an anonymous
    online source. |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – Data quality dimensions
  prefs: []
  type: TYPE_NORMAL
- en: Data Quality Strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Data Quality Strategy is a foundational capability of any data quality program.
    The data quality strategy defines the company’s integrated, company-wide approach
    to achieve and maintain the quality level required to meet business goals. Set
    by the company’s Chief Data & Analytics Office, the strategy should include the
    goals, key objectives, plans, and measures to improve and maintain data quality
    for the organization. While the strategy is set centrally, the strategy should
    clearly articulate the remit and responsibilities of the central team, as compared
    to business data stewards, technical data stewards, architects, data analysts,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Data Quality Strategy should contain the following core components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data quality objectives**: Start by defining what the purpose of the data
    quality strategy is, what outcomes you will deliver through the strategy, and
    what success looks like, specifically for your company (for example, how data
    quality supports data strategy, including data governance outcomes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assessment of current state**: Tell the reader exactly how bad (or good)
    the company’s data really is. Be as specific as possible, and provide powerful
    examples that explain what the business impact is of current data quality issues
    wherever possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality standards**: Articulate the criteria for trustworthy data. Your
    standards may include minimum rules for data quality dimensions, as well as what
    is expected to be done for each critical data asset. The *Core capabilities* section
    is a great place to start if you aren’t sure what to include.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation plan for data quality enablement**: Define what processes
    and tooling are required to enable data quality for the company and how you and
    your team will implement the required enablement mechanisms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remediation and issue management approach**: Define how data quality will
    be measured, reported, and remediated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality ownership and accountability**: Clear articulation of which
    roles are necessary for effective data quality management and what is expected
    of each role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, your objective might be “Establish an enterprise data quality framework
    that supports prioritization, governance, and oversight of data quality to improve
    transparency into critical data and improve trust in key reports,” whereas an
    example assessment approach may be something like “You may benchmark a system,
    a report, or a process to show the current quality level for the data.” The output
    of this strategy should be actionable and translated into a roadmap for implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Purpose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of a **Data Quality Strategy** is to drive clarity and alignment
    across your company regarding what data quality is, why it matters, and how the
    company will know it has been successful in driving trust into its information.
    The strategy is a great forcing function to drive conversation about what the
    gaps are today and how you will be able to measure success in the future. Most
    executives struggle to understand the **return on investment** (**ROI**) of a
    strategy. I recommend focusing on the business outcomes that you will drive as
    a *result* of the strategy, not the strategy itself. The strategy sets the North
    Star for this work, but it will not deliver the results alone.
  prefs: []
  type: TYPE_NORMAL
- en: You may wish to implement a **Data Quality Standard** as a follow-on to your
    Enterprise Data Governance Policy. The Data Quality Standard should include specific
    information regarding the Data Quality Strategy, codifying the roles and responsibilities
    for managing data quality as outlined in this chapter (and curated for the needs
    of your specific business), and provide further detail about how to comply with
    policy, through adoption of the capabilities set forth in the *Data quality* *enablement*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Accountability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Chief Data & Analytics Officer’s organization owns the Data Quality Strategy
    for the company. Depending on your team’s size and scale, you may have someone
    who is fully dedicated to running data quality for the company, or you may have
    someone who runs all Data Governance capabilities; either is a great candidate
    to own and drive this for the Chief Data & Analytics Office. If possible, establish
    a head of data quality who will be ultimately responsible for the delivery of
    the strategy and the implementation of the capabilities needed to drive the strategy
    for the company. Other interested parties should include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Business data stewards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical data stewards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data engineers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysts and business analysts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executives who use and rely on quality data for decision-making
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Are we there yet?”
  prefs: []
  type: TYPE_NORMAL
- en: Inevitably, when you are setting out to implement a data quality program at
    your company, you will run into a stakeholder who will ask something along the
    lines of “How long until we are done?”
  prefs: []
  type: TYPE_NORMAL
- en: Data quality is like exercise. When you’re out of shape, you need to pick a
    plan and do the work to get into shape, but then you also have to maintain it.
    Data quality is the exercise plan for your company. You will need to define a
    plan, create a get-well plan (implementation), and support data quality on an
    ongoing basis. The work is never “finished,” and by failing to maintain it, your
    company will quickly regress backward.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality enablement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data quality enablement is the core of any data quality program. Data quality
    enablement provides the company with centralized and standardized capabilities
    that serve the company’s data quality needs. Enablement usually includes people
    to support the enterprise needs, processes to power the data quality needs of
    the company, and technology, usually in the form of tooling, to support managing
    data quality effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Purpose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By providing central capabilities for the company, you can ensure the company
    has consistent views into the quality of the data, a common process to remediate
    issues defined, a way to prioritize, and appropriate communication so that individuals
    who need to know about data quality issues know about them when they matter most.
  prefs: []
  type: TYPE_NORMAL
- en: It boils down to transparent information about the quality of data, common tooling,
    which focuses budget and costs on shared tooling (versus competing tooling), and
    standardized processes to enable people to maximize their time when it comes to
    data quality.
  prefs: []
  type: TYPE_NORMAL
- en: There’s also the key vector of reusability. For example, data quality enablement
    can ensure we are measuring critical data assets once and using them over and
    over again versus measuring data quality every time a data asset is used. The
    enablement team or function can provide a certification process to critical data
    assets so that users can see which data assets have been evaluated for data quality,
    passed expectations, and are reliable for use. This builds trust and reduces time
    spent searching for and measuring the reliability of common-use data assets across
    an organization. The bigger the company is, the more likely we are to be using
    common data assets without realizing it, creating the risk of mass redundancy.
    Imagine the time that can be saved when we can transparently see what critical
    assets we have (see *data marketplace* in the chapters on metadata) and know that
    the data is reliable (that’s data quality!).
  prefs: []
  type: TYPE_NORMAL
- en: Accountability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with a Data Quality Strategy and data quality standards, data quality enablement
    should be a function driven by the Chief Data & Analytics Office. The central
    function is best positioned to create the capabilities necessary for the company’s
    business and technical data stewards to drive data quality efforts for the data
    they are responsible for. Usually, this enablement team sits either within the
    data governance sub-team or with a tooling team. Given the close marriage of metadata
    management and data quality, it’s often bucketed together.
  prefs: []
  type: TYPE_NORMAL
- en: I have seen very effective programs where the data governance team leads the
    data quality strategy, data quality standards, and the data quality program enterprise-wide,
    while the data solutions or data tooling team provides the technology solutions
    to enable these capabilities. This can work very well if you have a large company,
    as it aligns the business and technical skills with the solutions necessary to
    deliver excellence from your data organization.
  prefs: []
  type: TYPE_NORMAL
- en: Spreadsheets don’t scale
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to deploying great data quality solutions, perfect can’t be the
    enemy of good. However, there is a balance of scalability that needs to be considered.
  prefs: []
  type: TYPE_NORMAL
- en: 'At one of my previous companies, we were enabling an end-to-end view of several
    key reports to manage the business at the executive level. Our objective was seemingly
    simple: Is this report trustworthy for our executives to make business decisions?
    Operationalizing this question, however, was much harder than we expected.'
  prefs: []
  type: TYPE_NORMAL
- en: We had to deconstruct the key metrics being used. We did not have the technical
    solutions to make this efficient or repeatable, but we moved forward anyway. The
    problem? The content of the reports changed frequently. Instead of looking at
    the sources and measuring the totality of the source at a feed or system level,
    we measured the individual metrics. However, as the metrics evolved or new calculations
    were introduced, we had to go back to square one to validate the data. If we had
    taken a more holistic approach and used technology to enable this work, we wouldn’t
    have been documenting data quality in spreadsheets (defining critical data elements,
    mapping to source systems, and manually measuring quality). Each change required
    a significant amount of manual work. We couldn’t scale.
  prefs: []
  type: TYPE_NORMAL
- en: The value of measuring data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I have no doubt you’ve experienced this if you’ve been in a data role for more
    than a day: Someone reaches out to you, convinced that their data is “wrong.”
    They want it fixed. Ultimately, what they are saying to you is: I don’t trust
    this data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we think about measuring the value of data quality, we have to challenge
    ourselves to think more broadly than one might expect to. When we are thinking
    about how to measure the value of data quality, we are really asking: How do I
    measure the value of trust? What does it mean to my company to be able to trust
    the information we use for decision-making, every single day? What is it worth
    to us to know we can trust the information being used to run our business? To
    drive value for our customers? To operate effectively? If you could place a dollar
    value on that, what would it be?'
  prefs: []
  type: TYPE_NORMAL
- en: The value of trust in our data is highly dependent on your business. Let’s start
    with a real-life example.
  prefs: []
  type: TYPE_NORMAL
- en: Building trust in data quality
  prefs: []
  type: TYPE_NORMAL
- en: 'Marketing is a function in a company that many do not necessarily correlate
    with data, but in my experience, the best marketing functions are highly data-driven.
    I worked with a marketing department that was struggling to understand which individuals
    they could market to. They had a plethora of lead data, but they were having a
    hard time determining the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Which leads had accurate email addresses and quality data
  prefs: []
  type: TYPE_NORMAL
- en: If the leads had consented to marketing holistically or for only specific products
    (further complicated by M&A)
  prefs: []
  type: TYPE_NORMAL
- en: If the most recent consent was valid for their country’s regulations (for example,
    how long is the consent valid for?)
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the Chief Marketing Officer needed to measure how much the marketing
    team was contributing to sales, but without quality lead information that they
    could trust, measuring the impact of marketing on sales was very difficult. They
    believed their team was contributing more than they were able to measure, but
    they couldn’t prove that with data.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the beginning of the process to improve the trustworthiness of the marketing
    lead data, we baselined the quality of the data by completing basic data profiling
    based on what was determined to be “key fields.” We defined key fields as the
    fields associated with a contact record that were required to be able to market
    to that contact, which included: first name, last name, email address, company,
    and valid consent. Our initial profiling put the quality of these key fields at
    about 43%, meaning only about 43% of the contacts had reliable data in these five
    fields.'
  prefs: []
  type: TYPE_NORMAL
- en: We deployed an enrichment service to be able to improve the reliability of the
    contact data. It allowed us to overwrite low-quality data (such as initials in
    fields for first name and last name, and fill in blanks), and to validate email
    addresses with a third-party service. This improved leads where the address had
    been typed incorrectly. Therefore, we were able to take instances where we had
    consent but an invalid email address and use the email address. Overall, this
    effort increased our marketable contacts from 43% to over 85%. As a result, the
    CMO was able to demonstrate that their contact data was of higher value and was
    directly tracible to sales at a higher rate. It was also a measure of their success
    as a CMO, which led to their ultimate success as a leader. This individual became
    a champion of our data and analytics team and supported further transformation
    work we led.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the value of having trustworthy data wasn’t just a simple calculation.
    But for the Marketing division, it was a combination of cost savings from being
    sure they could market to real leads, the confidence that they had consent to
    market to the quality leads they had, *AND* that they were compliant with laws.
    They also needed to measure the value of revenue contribution, generated from
    the quality leads. In short, it was a combination of cost savings + risk avoidance
    + fine and penalty avoidance + contribution to revenue. This calculation will
    vary based on the use case, based on division, and based on your company. The
    important takeaway here is that you measure the value in business terms.
  prefs: []
  type: TYPE_NORMAL
- en: Most data professionals measure the improvement in data quality based on the
    percentage of improvement or reduction in errors but don’t take it back to the
    business context. That’s where you, as a data leader, can demonstrate your value
    to your company, by showing them how data investments have a real business impact
    in dollars (increase in revenue and increase in savings), time, and risk avoidance.
    That’s how you demonstrate trust in data and return on your data investments.
    If you take nothing else away from this book, it should be this.
  prefs: []
  type: TYPE_NORMAL
- en: Core capabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing data quality capabilities can significantly enhance the integrity,
    accuracy, and reliability of data, making it a trustworthy asset for decision-making,
    analytics, and business operations. It’s worth noting that roles can vary based
    on the size and structure of the organization. In smaller companies, a single
    individual might handle multiple responsibilities, while larger organizations
    might have entire teams dedicated to specific tasks. Collaboration and clear communication
    among these roles are crucial to ensure cohesive and effective data quality management.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving maturity in data quality management indicates that an organization
    has not only implemented the core capabilities but also refined, optimized, and
    integrated them into daily operations.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the value of each data quality capability ensures that an organization
    can justify its investments in data quality management and recognize areas of
    improvement. For each capability, the value can further be translated into tangible
    benefits such as monetary savings and increased revenue or intangible benefits
    such as enhanced stakeholder trust, improved brand reputation, and organizational
    agility. Converting the value of data quality capabilities into dollars or time
    requires understanding the specific financial and operational context of an organization.
    To truly get an accurate dollar or time value for each capability, an organization
    would need to conduct detailed assessments, considering its operational costs,
    business context, and the consequences of data issues. The framework for calculating
    value should be defined by the data office; the inputs should be provided by the
    business. We won’t go into specifics here, but use the framework defined previously
    in *The value of measuring data quality* section to apply it to your organization.
  prefs: []
  type: TYPE_NORMAL
- en: Data profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Building an understanding of the existing state of data, including inconsistencies,
    anomalies, and patterns is delivered through a foundational capability called
    **data profiling**. Data profiling is usually performed by business data stewards
    and technical data stewards together. The business data steward can provide expertise
    on the business meaning and usage of the data, while the technical data steward
    can provide expertise on technical aspects of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Business Data Stewards** generally have a deep understanding of the business
    data they are stewarding/responsible for, which includes understanding (or defining)
    the meaning, how it should be used, and the quality requirements. They are best
    positioned to identify data quality issues and define the business impact of the
    issues identified. Business data stewards should do the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify business requirements for data quality for their data elements
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure data quality profiling is implemented for their data elements
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Define business rules that govern the data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the frequency of data profiling (consider how frequently the data
    is changing, the volume of data, and the volume of issues in the data elements
    they are stewarding)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Review and analyze data profiling reports to identify and understand data quality
    issues, and report relevant issues to users of the data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with data stewards (other business data stewards and technical data stewards)
    to resolve data quality issues
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure profiling is complete, measured appropriately, monitored, and published
    transparently for users
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical Data Stewards** generally have a technical understanding of the
    data, such as structures, formats, and systems. Technical data stewards are responsible
    for assisting the business data steward in identifying the physical locations
    of physical data elements that must be measured in support of the business data
    stewards’ needs. They use this technical understanding to identify and troubleshoot
    data issues at the technical level in support of business data stewards. Technical
    data stewards should do the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design and implement data profiling jobs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure and use data profiling tools
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze data profiling reports to identify and troubleshoot data quality issues
    in partnership with business data stewards
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with other technical data stewards to identify up and downstream impacts
    of data quality issues and remediation required
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Engineers** are responsible for the technical implementation of profiling
    requirements within systems to provide business data stewards, technical data
    stewards, and ultimately, users of the data with transparent information regarding
    the state of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Users** (data analysts, business analysts, executives) are responsible for
    understanding the results of the data profiling efforts and taking into consideration
    any issues identified through profiling when using the data for business needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The figure below illustrates the process flow of business data and technical
    data stewards’ partnership:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Simple process flow of business data and technical data stewards’
    partnership](img/B18846_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Simple process flow of business data and technical data stewards’
    partnership
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few simple measures you can deploy within your team to show the
    progress your team is making in improving the quality of your company’s data.
    Basic measures include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Percentage of datasets profiled per domain or system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction in surprises or issues when using data for analytics or operations
    (for example, how many issues you started with compared to how many you currently
    have)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost savings from avoiding incorrect data-driven decisions or regulatory reporting
    errors (for example, did you catch a material error that would cause a financial
    misstatement?)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction in hours spent identifying and addressing data issues manually by
    adopting automated data profiling solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data cleansing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of cleaning your data is referred to as **data cleansing**. Data
    cleansing identifies and rectifies errors, inconsistencies, and redundancies in
    datasets. In short, data cleansing corrects your data to meet the expectations
    of your users. Data cleansing should be conducted as close to the source as possible
    so that correct and trustworthy data flows through the organization already corrected.
    Wherever data is corrected, the business data steward and technical data steward
    have a shared responsibility to ensure users are aware of corrections (business
    data steward) and the corrected data is sent up and downstream (technical data
    steward).
  prefs: []
  type: TYPE_NORMAL
- en: When corrected data looks wrong
  prefs: []
  type: TYPE_NORMAL
- en: During a system implementation, my team improved the quality of the data for
    a specific dataset. The users of the data reported data quality issues. Why did
    this happen?
  prefs: []
  type: TYPE_NORMAL
- en: We hadn’t appropriately notified the users of the data that they should expect
    changes and what the changes would look like. Thus, when the data changed, although
    improved, it appeared different from what was expected by the user, thus leading
    them to believe it was wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Much like with data profiling, the partnership between the business data steward
    and technical data steward is critical to the success of this capability. The
    business data steward should identify which data elements should be cleansed (as
    every data element is not of equal importance, and there is a cost to cleansing)
    and work with the users of the data to determine the importance and priority of
    cleansing capabilities. The technical data steward is responsible for enabling
    cleansing activities in the system(s) and ensuring up- and downstream system(s)
    are aware of the cleansing being performed to preemptively alert consumers that
    the data is expected to change.
  prefs: []
  type: TYPE_NORMAL
- en: Because not every data element in a company can be cleansed, it is important
    to define what the strategy will be for data cleansing and which data elements
    are most critical for cleansing and to develop a prioritization mechanism to determine
    which data elements should be prioritized over others, and why. You may want to
    use your enterprise data committee or data governance council to support and approve
    your prioritization methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced data cleansing functions may offer a cleansing service centrally to
    support the company. One such example would be address standardization for contacts.
    Contact data often sits across an organization and may include marketing, sales,
    and customer service data. A central data cleansing service could be created to
    provide address standardization and enrichment for these divisions as a shared
    capability so that the company is paying for these services once and providing
    them out to multiple departments for value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Much like data profiling, data cleansing value is measured in very similar
    ways: in terms of business value, dollars generated or saved, and time reduction.
    Some examples include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Percentage of reduction in data errors post-cleansing (or count versus percentage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Increased confidence in the analysis used in decision-making (*note*: it is
    important to align on how to measure confidence in terms of time or money)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Money saved from preventing decisions based on erroneous data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction in hours spent manually correcting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of automated corrections made/number of errors identified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of manual interventions required/number of errors identified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over time, manual interventions should reduce, and the quality of data should
    uplift. You should be able to identify issues more easily as the overall quality
    of data improves. This should free up resources to dig into more critical issues
    and focus on business impact versus manually correcting data.
  prefs: []
  type: TYPE_NORMAL
- en: Data validation and standardization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process to determine if the data is valid is called **data validation**.
    Data validation determines what values are appropriate for a given field. For
    example, it may be a range of values (0-99) or a specific list of values (for
    example, **North American Industry Classification System** (**NAICS**) codes)
    that the field is permitted to contain. Validation may also include whether a
    field can have characters or numerical values. Validation requires fields to conform
    to specific expected values.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, **Data Standardization** harmonizes data formats, units, and definitions
    so that there is consistency across the organization. Often defined alongside
    validation, these two capabilities ensure the data is formatted and consistent
    as expected. Standardization efforts ensure that the same data shows up consistently
    across systems and, at its most mature state, at the reporting level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Much like data profiling and data cleansing, business data stewards and technical
    data stewards should work together to ensure these capabilities are adopted holistically:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Business Data Stewards** should define values that are appropriate for a
    field that meets the needs of the users of the data and are enabled across the
    company'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical Data Stewards** should work with engineers to implement validation
    rules in the system(s) and ensure that the validation is consistent (standardized)
    enterprise-wide'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Measuring the value of these two capabilities can be kept rather simple. A
    few easy metrics of success include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Rate of data entry errors (should be near zero over time)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number/rate of validation failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction in operational disruptions and reduced data remediation/correction
    costs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Savings from reduced data correction errors post integrations or ingestions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster data processing speeds due to a reduction in validation failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated validation checks at all data ingestion points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data enrichment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data Enrichment** is the process of adding new and supplemental information
    to existing datasets. This can be done by combining first-party data (your company’s
    data) from internal sources with disparate data from other internal systems or
    third-party data (data from external sources). There are several ways to enrich
    data. Some common methods include adding the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Demographic data**: This could include information such as age, gender, income,
    location, and education.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Firmographic data**: This could include information such as company size,
    industry, and revenue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Behavioral data**: This could include information such as website visits,
    product purchases, and social media engagement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual data**: This could include information such as time of day, location,
    and device type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data enrichment can be used to improve the accuracy, completeness, and relevance
    of data. This can be beneficial for a variety of purposes, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improving customer segmentation**: Enriched data can be used to create more
    accurate and detailed customer segments. This can help businesses to better understand
    their customers’ needs and preferences and target them with more relevant marketing
    and sales messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalizing customer experiences**: Enriched data can be used to personalize
    the customer experience across all channels. For example, businesses can use enriched
    data to recommend relevant products or services to customers or to provide them
    with more personalized support.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improving fraud detection**: Enriched data can be used to identify and prevent
    fraudulent activity. For example, businesses can use enriched data to verify the
    identities of new customers or to detect fraudulent transactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Making better business decisions**: Enriched data can be used to make better
    business decisions across all areas of the organization. For example, businesses
    can use enriched data to identify new market opportunities, optimize their pricing
    strategies, and improve their product development process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data enrichment can be a complex and time-consuming process, but it can be
    a valuable investment for businesses of all sizes. By enriching their data, businesses
    can gain a deeper understanding of their customers, improve their marketing and
    sales efforts, and make better business decisions. Simple value measures include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Increase in data attributes or features post-enrichment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhanced insights and better predictive modeling capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increased revenue from better-targeted marketing or analytics insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction in manual data-gathering processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More specifically, for the aforementioned examples, you can measure the value
    of these enrichment services with the following business metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved customer segmentation**: You can track the accuracy of your customer
    segmentation by comparing the predicted segment membership to the actual segment
    membership. You can also track the performance of your marketing campaigns that
    are targeted to specific customer segments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized customer experiences**: You can track customer satisfaction
    and retention rates to measure the impact of personalized customer experiences.
    You can also track metrics such as **click-through rate** (**CTR**) and conversion
    rate to measure the effectiveness of your personalized marketing campaigns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved fraud detection**: You can track the number of fraudulent transactions
    that are detected and prevented as a result of data enrichment. You can also track
    cost savings that are achieved by preventing fraudulent transactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better business decisions**: You can track the number of business decisions
    that are made using enriched data. You can also track the financial impact of
    these decisions, such as increased revenue or reduced costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These can be difficult to measure concretely. Here’s a specific example to
    show how you can break down the measure more specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Business goal**: Improve the accuracy of customer segmentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metric**: Percentage of customers that are correctly assigned to their customer
    segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Baseline**: 70% of customers are correctly assigned to their customer segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**After data enrichment**: 85% of customers are correctly assigned to their
    customer segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Result**: Customer segmentation accuracy has improved by 15%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedback loops, exception handling, and issue remediation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While users of data are engaged throughout the development of your data quality
    Program, the phase they are most engaged is once the program is up and running.
    As you operationalize business rules by configuring them into the systems through
    profiling, validation, enrichment, and so on, the results of the data quality
    program start to shine. The first set of results published is usually a bit jarring
    for business users to process. If data quality has never been an area of focus,
    the results are often alarming. As the data professional, your job is to help
    users understand that the first baseline of results is just that—a baseline. The
    initial results are used to show where we are today.
  prefs: []
  type: TYPE_NORMAL
- en: Feedback loops
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you start to work with users of the data and the various stewards, your role
    is to ensure the data quality program has appropriate **feedback loops** established
    for data stewards and your team so that you can improve the enablement of the
    data quality program and support the stewards as they work to improve the trust
    in their data. Feedback loops are required in a few different places in your program,
    but first, between the users and the business data stewards. You may want to help
    guide your business data stewards on how to set up appropriate forums for feedback
    about their data quality. They may find value in starting a **stewardship council**
    by data domain to help provide feedback and prioritization for where data quality
    remediation needs to be focused first.
  prefs: []
  type: TYPE_NORMAL
- en: You should also provide a forum for business data stewards and data domain executives
    to bring forward significant and cross-functional data quality issues for enterprise
    prioritization, where needed. The enterprise data committee and data governance
    council are great options for the escalation of these types of complex data issues
    that require enterprise funding, prioritization, or awareness. Let’s start by
    exploring how a data issue would be escalated to such a forum.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, the business data steward may have identified a data quality issue
    that the user ultimately decides is not significant and isn’t worthy of remediation
    efforts. It’s important to document the process by which data quality issues are
    prioritized and dispositioned. You don’t want to be in a situation where we’re
    not sure if a data quality issue is a true issue or perhaps just a data defect
    that is not worth the time or energy to remediate. I recommend creating a disposition
    log so that you can track issues you identified and who signed off on the disposition
    to have a record to back your decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: Exception handing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There may be times in the data life cycle when exceptions are made regarding
    the data quality process. For example, a data lineage process may be defined for
    a specific data metric, and financial reporting purposes, the data must be at
    a specific quality level. However, for ad hoc analytical purposes, we may accept
    a lower level of quality to allow for speed to that insight. It’s important to
    take the use of data into account when defining exceptions. In this example, the
    user may need a one-time pull of revenue data for a reasonableness check on progress
    to quarterly revenue targets. This extraction may not need to be perfect. The
    user should be informed that the data extract has not been through all the normal
    control processes, but if they understand this, an exception process may be very
    appropriate, especially if time-sensitive.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of exception handling is to ensure that exceptions do not become
    the rule. You should define appropriate times when exceptions may be made. If
    you find that a specific request is being made somewhat regularly, consider whether
    that is truly an exception or is simply circumventing the expected flow of information.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Helpful Hint** |'
  prefs: []
  type: TYPE_TB
- en: '| Let’s take a lesson from our IT counterparts and develop a severity methodology.
    In the IT world, IT issues are classified as *Sev1*, *Sev2*, and so on, based
    on a set of criteria. Based on that severity rating, a different process is followed
    to drive the sense of urgency.For data issues, we could follow the same methodology.
    If the issue is material or causes a system outage, perhaps that should be a *Sev1*
    and follow the highest level of escalation and remediation, versus a minor data
    issue, which can be remediated in weeks or months without much impact on the business
    and maybe a *Sev3*. |'
  prefs: []
  type: TYPE_TB
- en: 'At a data element level, you might find that you have anomalies in the data,
    and that may also be considered an exception. You should work with the business
    data steward to determine if the exception is truly an exception, and why it is
    an exception. Did a control fail? Did the data get corrupted in transit? Did you
    receive a bad file from a third party? There are a number of reasons that data
    could come through with some kind of exception. The important thing is that you
    follow a standardized process to handle exceptions and follow them to closure.
    Here are some examples of how to manage specific types of data quality exceptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing data**: If you have a large number of missing values in a dataset,
    you may want to consider imputing the missing values. This can be done using a
    variety of methods, such as using the average or median value of the column or
    using a **machine learning** (**ML**) model to predict missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inconsistent data**: If you have inconsistent data, you may need to manually
    correct the data or update the data quality rules to allow for inconsistencies.
    For example, if you have a dataset with different formats for the date column,
    you may need to normalize the date format before using the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duplicate data**: If you have duplicate data, you may need to remove duplicates
    or merge them into a single record. For example, if you have a dataset with two
    customer records with the same name and address, you may need to merge the two
    records into a single record.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through this process, you might determine that the exception isn’t an exception
    at all and that it is actually a data issue and needs to follow the issue remediation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Issue remediation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When a material data issue is identified, the business data steward should
    review and develop a comprehensive understanding of the issue and the source of
    the issue and identify options for remediating the issue. If the issue is significant,
    spans multiple data domains, or has a material impact on the company, it should
    get escalated to the appropriate forum. A simplified issue remediation process
    may look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Sample issue remediation process](img/B18846_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Sample issue remediation process
  prefs: []
  type: TYPE_NORMAL
- en: What’s most important is to define an issue remediation process. This will also
    enable you to define exceptions and feedback loops that make sense for your company.
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, the person closest to the data is the one most likely to understand
    the data the best and should be the one presenting the issue in these forums.
    That may mean that someone at an analyst level is presenting to senior executives.
    As the data leader, you should support the person with the most information and
    help them to shape the presentation for the right audience, which may result in
    a variance from the aforementioned standardized process.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of great ways to measure the value of feedback loops, exception
    handling, and issue remediation. One consideration: At the earliest stages of
    data quality management, you will have a surge in issues. I would expect that
    to normalize over 12-18 months, but it may be shorter or longer depending on the
    amount of time and resources applied to data quality measurement and remediation
    efforts. Either way, the following measures are a great way to show the value
    of your feedback loops, exception-handling processes, and issue remediation efforts:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of feedback instances leading to improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase in user satisfaction (similar to **net promoter** **score** (**NPS**))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficiency gains from remediation efforts (time, money)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduced iterative cycles in data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average time to resolve a data exception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average time to remediate an issue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of open issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an optimal data quality capability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A mature data-driven company not only implements the individual capabilities
    from the preceding section but also integrates them in a holistic manner, ensuring
    that data quality is embedded in the organization’s operations and overall culture—a
    culture where trusted information is valued and expected. This state, where trusted
    information is the baseline and anything else is rejected, includes the expectation
    that the quality of the data used in reporting, operations, and analytics is measured,
    monitored, and reported for *all* in-scope data elements, assets, and integrations.
  prefs: []
  type: TYPE_NORMAL
- en: What does it mean to be in scope? It does not mean that every single data element,
    every single report or dashboard, or every single integration is measured, monitored,
    and reported. That would be just as irrational as measuring, monitoring, or reporting
    nothing at all (for the opposite reasons). Rather, optimal data quality requires
    thoughtful, intentional scoping of the right data elements, the right reports,
    and the right integrations to ensure that the company’s assets (money, resources,
    time) are applied judiciously across the company’s most important and relevant
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the quality of data and being able to have a defendable stance
    when it comes to “Can I trust it?” is key for any user of the data, or information,
    that’s leveraged for decision-making. Establishing a data quality capability enables
    the CDO, and their teams, to stand behind their data and be able to defend the
    quality of the information.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we consider what an optimal data quality capability looks like, it comes
    down to a few key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying what data is critical, leveraging a prioritization framework
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Profiling the data to deliver insights about the underlying data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partnering with users to define appropriate business rules for the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Writing and testing rules on data elements
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Promoting rules to production to generate insights into the quality of the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monitoring and reporting on the quality of the data to business data stewards
    and users of the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remediating appropriate issues, leveraging feedback loops, exception management,
    and issue management processes, leveraging councils, stewardship groups, and data
    committees where appropriate
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The flow of this process could look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Optimal DQ Monitoring from onboarding to reporting](img/B18846_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Optimal DQ Monitoring from onboarding to reporting
  prefs: []
  type: TYPE_NORMAL
- en: The role of data stewards (both business and technical) cannot be overstated.
    They are the critical success factor in an optimal data quality capability. Without
    high-quality stewards who interact with users of information and have an intimate
    understanding of the data, its use, and the challenges, data quality will be a
    mere check-the-box activity. When stewardship is strong, the company’s data will
    be highly trusted because it will be cared for exceptionally well. Therefore,
    ensuring your data stewards have accepted responsibility for data quality is of
    the utmost importance for the success of your data program.
  prefs: []
  type: TYPE_NORMAL
- en: Certified data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you are standing up a strong data quality program, you may want to consider
    enabling a **certified data** process. When coupled with metadata and data lineage,
    data quality can lead you to deploy a data certification process that not only
    enables the reusability of common data elements or curated datasets but also increases
    the value of the data quality work.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, identify what data element or dataset is used often across the organization.
    You might want to start with a metric that is used for operational and reporting
    purposes (such as Customer Count). Average Customer Count is a good example because
    it is seemingly simple but often controversial. To certify Customer Count for
    the company, you first need to define who is the business data steward for Average
    Customer Count. This individual will work with relevant stakeholders to align
    on the definition of Customer Count. For the purposes of this example, let’s assume
    the definition of a customer is: “An individual or company that has purchased
    one or more products in the last 12 months.” To calculate the average, we will
    need to pull information from several systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to align on the calculation of the term “Average Customer
    Count” and the source(s) for the data. For this example, let’s assume the calculation
    is as follows (as of 1/1/2023):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Number of customers = Total number of transactions in the last 12 months /
    Average number of transactions* *per customer*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The average number of transactions per customer can be calculated by dividing
    the total number of transactions in a given period by the number of customers
    in that period. For example, if a company has 100,000 transactions in the last
    12 months and an average of 2 transactions per customer, then the number of customers
    would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Number of customers = 100,000 transactions / 2 transactions per customer =*
    *50,000 customers*'
  prefs: []
  type: TYPE_NORMAL
- en: This formula can be used for any company, regardless of industry or size. However,
    it is important to note that the accuracy of the calculation will depend on the
    quality of the data used. For example, if the company does not have a good system
    for tracking customer transactions, then the results may not be accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a step-by-step guide on how to calculate the number of customers that
    have transacted with a company in the last 12 months:'
  prefs: []
  type: TYPE_NORMAL
- en: Gather data on the total number of transactions in the last 12 months. This
    data can be found in the company’s sales records or **customer relationship management**
    (**CRM**) system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the average number of transactions per customer. This can be done
    by dividing the total number of transactions in a given period by the number of
    customers in that period.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the total number of transactions in the last 12 months by the average
    number of transactions per customer to get the number of customers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Note*: *Once you have calculated the number of customers that have transacted
    with the company in the last 12 months, you can use this information to track
    customer churn, identify customer segments, and develop targeted* *marketing campaigns.*'
  prefs: []
  type: TYPE_NORMAL
- en: From here, you should identify the optimal sourcing for this data. For example,
    you may pull customer information from your **Customer Data Platform** (**CDP**),
    and your sales information might come from your company’s CRM platform. The business
    data steward would be responsible for determining the authorized source for this
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Once a definition, calculation, and sourcing of the data are determined, the
    business data steward would then work with the data office to evaluate the lineage
    of the data, validate the lineage (see [*Chapter 7*](B18846_07.xhtml#_idTextAnchor152)
    for details on lineage capabilities), and measure the quality of the data. Any
    issues with lineage and quality should be remediated (or disclosed to the users).
    You will need to use your data lineage capability to trace the data through the
    systems and identify where to measure data quality. Apply data profiling, enrichment,
    and de-duplication efforts where necessary to strengthen and improve your data
    quality. Once the metric is validated as trustworthy, the data office can mark
    the metric as certified.
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve deployed a data marketplace, this should be the place where the metric
    is published for consumption and marked as certified. This will show any user
    of the data marketplace that the Average Customer Count metric is trustworthy
    and reliable for use. It also shows the user that any other metrics are *not*
    the authorized and certified metric and should not be as trusted as the certified
    metric. This process can be repeated for all key metrics in your organization
    and applied to common-use datasets, reports, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best way to build trust is through transparency. When the quality of data
    is low, the best way to lose trust is to not share the results openly. When the
    results of your data quality work show that data should not be relied upon, you
    should publish these results and leverage the response from users to drive remediation
    efforts. If important data is low quality, providing visibility into that deficiency
    will garner support for your remediation needs and, oftentimes, can lead to increases
    in funding and resourcing to help drive your data quality program. Don’t be shy
    about reporting your deficiencies. It will help you.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up data quality for success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My best advice for setting up your data quality program is to start small, be
    specific, be transparent, and scale rapidly. Pick a handful of metrics or a couple
    of key reports, and show the value of understanding the data, tracing the lineage,
    and measuring the quality of the data. Remediate issues quickly and visibly and
    show improvement in the data for these key metrics and/or reports.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be very clear with your stakeholders, both users and executives alike: the
    goal is not to create perfect data for the enterprise. The goal is to deploy resources
    (time, money, people) for the most effective return on those resources. You could
    spend unlimited time and money remediating every data element, but it would be
    like trying to plug every hole in every boat in every ocean. Sometimes, you just
    need to build a better boat. You may run into instances where a report is incredibly
    low quality. It might be easier to completely recreate the report with different
    data sources or different metrics than trying to fix every single error. You’ll
    have to use your professional judgment to determine the best course of action
    for your key metrics and reports. Choose wisely and deliver measurable improvements
    quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: The real-time request
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may run into instances when business users request “real-time” insight into
    the quality of their data. It’s your job as a data professional to credibly challenge
    this request. From a position of curiosity, seek to understand the reasons why
    the business wants real-time measurement of the quality of their data. For starters,
    measuring real-time data quality requires you to test data in production, which
    can impact the performance of the production system. Oftentimes, data is profiled
    in a lower environment with a copy of production data, not to impact performance.
    Secondly, most “real-time” requests are satisfied with intra-day or hourly reporting
    instead of “real-time” reporting. Work to determine the business need driving
    the request, and then react accordingly. Sometimes, data professionals are eager
    to please and say yes without assessing the true business need against the cost
    to deliver. Real-time data often comes with a significant cost over even slight
    delays (a few hours) and requires different configurations of underlying system
    performance and integrations, often at a significant cost in infrastructure, processing,
    and people to monitor the delivery.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time can backfire
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, real-time can backfire on you. When you have systems integrated in
    real time, in the spirit of ensuring high-quality data available for operations,
    that produces quick access to quality data. However, when things go wrong, you
    are propagating data problems in real time too.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not uncommon to design a CDP to push cleansed and enriched customer data
    into a CRM platform. Customer data is usually captured in the CRM, pushed to the
    CDP for enrichment, and then pushed back to the CRM. In one instance, a bug was
    introduced into the CDP, such that the hierarchy information (what connects individual
    companies to their parent company, which is used for transactions, sales territory
    mapping, and revenue reporting by geolocation, by account executive, and so on),
    was removed. When the hierarchy data was deleted in the CDP, it was replicated
    in the CRM. In the CRM, the now-orphaned customer records lost their parent company,
    which removed the territory assignment. The data was then sent back to the CDP
    without the territory assigned. This replicated blanks in the customer data records
    in both systems. We went from having a few orphaned accounts to thousands in a
    matter of minutes, to hundreds of thousands in a matter of hours, before we had
    to turn the synchronization process off. It took weeks to remediate and restore
    the full dataset in both systems to accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: If the real-time requirement had been a few hours instead of real time, we would
    have been able to catch the replication issue without such a huge blast radius.
    Sometimes, intentional friction in the process can prevent these sorts of issues
    without such a big impact. A simple validation check for blank fields may have
    also stopped the replication job and alerted the team to the problem before causing
    such a widespread impact.
  prefs: []
  type: TYPE_NORMAL
- en: Integrations with other systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another facet of a strong data quality program is evaluating data pipelines.
    Often, data quality profiling and testing are performed at a system level or an
    integration level, versus an element level. There are reasons to do both, and
    the right application at the right time is the best way to allocate resources
    appropriately. Integration data quality can be measured in a few ways to determine
    the value of the integration measurement:'
  prefs: []
  type: TYPE_NORMAL
- en: Latency between the ingestion and its validation and subsequent correction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time to decision** (**TTD**) from data ingestion from the source system to
    report'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Savings from immediate or real-time error detection and avoidance of downstream
    system implications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of real-time corrections without manual intervention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of successful integrations without data quality issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speed of data flow between systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimate success with data quality comes down to understanding the business
    needs, what quality is required, and producing transparent results that business
    data stewards can take action to fix in a timeline that meets their needs. Start
    small, scale rapidly, and share results broadly. This is how you drive trust in
    your data.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored the fundamental concept of data quality and its
    critical role in driving informed decision-making. We established a framework
    for understanding the various dimensions of data quality, including accuracy,
    completeness, consistency, and timeliness. We further delved into different sources
    of data errors and imperfections, highlighting the importance of proactive data
    quality management practices. You should now have a firm understanding of the
    following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of data quality, including why it is required for building trust
    in data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to define a data quality strategy and framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific needs when it comes to implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to design a data quality solution with impact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to set data quality up for success in your company
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By prioritizing data quality, organizations can unlock the true potential of
    their information assets. Clean, consistent, and reliable data empowers effective
    analytics, fosters trust in insights, and ultimately, fuels better business outcomes.
    Moving forward, the following chapters will delve deeper into specific data capabilities
    but keep data quality in mind. Data quality is key in the implementation of all
    data management capabilities and should be considered as you implement primary
    data and data operations, and will come to light in the use case in [*Chapter
    17*](B18846_17.xhtml#_idTextAnchor351). You are now equipped with practical knowledge
    to assess, improve, and maintain the quality of your data for optimal results.
  prefs: []
  type: TYPE_NORMAL
