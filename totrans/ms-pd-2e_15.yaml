- en: A Brief Tour of Bayesian Statistics and Maximum Likelihood Estimates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we take a brief tour of an alternative approach to statistical
    inference called **Bayesian statistics**. It is not intended to be a full primer,
    but will simply serve as an introduction to the Bayesian approach. We will also
    explore the associated Python-related libraries and learn how to use `pandas`
    and `matplotlib` to help with the data analysis. The various topics that will
    be discussed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Bayesian statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mathematical framework for Bayesian statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian versus frequentist statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to PyMC and Monte Carlo simulations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian analysis example – switchpoint detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Bayesian statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of Bayesian statistics is built on the work of Reverend Thomas Bayes,
    an 18th-century statistician, philosopher, and Presbyterian minister. His famous
    Bayes' theorem, which forms the theoretical underpinnings of Bayesian statistics,
    was published posthumously in 1763 as a solution to the problem of *inverse probability*.
    For more details on this topic, refer to [http://en.wikipedia.org/wiki/Thomas_Bayes](http://en.wikipedia.org/wiki/Thomas_Bayes).
  prefs: []
  type: TYPE_NORMAL
- en: Inverse probability problems were all the rage in the early 18th century, and
    were often formulated as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you play a game with a friend. There are 10 green balls and 7 red balls
    in bag 1 and 4 green balls and 7 red balls in bag 2\. Your friend tosses a coin
    (without telling you the result), picks a ball from one of the bags at random,
    and shows it to you. The ball is red. What is the probability that the ball was
    drawn from bag 1?
  prefs: []
  type: TYPE_NORMAL
- en: 'These problems were called inverse probability problems because we are trying
    to estimate the probability of an event that has already occurred (which bag the
    ball was drawn from) in light of the subsequent result (the ball is red):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41d6fc3b-2cd9-4dc9-b7e7-6404259f6133.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayesian balls illustration
  prefs: []
  type: TYPE_NORMAL
- en: Let's quickly illustrate how one would go about solving the inverse probability
    problem illustrated earlier. We want to calculate the probability that the ball
    was drawn from bag 1, given that it is red. This can be denoted as *P(Bag 1 |
    Red Ball)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by calculating the probability of selecting a red ball. This can
    be calculated by following the two paths in red, as shown in the preceding figure.
    Therefore, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b27f4fd8-be83-4294-9c5d-a0e227ded600.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, the probability of choosing a red ball from bag 1 is shown by only taking
    the upper path and is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0e3c44d-5792-43db-aaca-6bd6712e2b0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability of choosing a red ball from bag 2 is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0bcf020-58a3-4253-96ae-9201e437e95d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that this probability can also be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bd4d489-3825-4d8d-9693-5f63c39ab262.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that *P(Bag 1)=1/2*, and the final branch of the tree is only traversed
    if the ball is firstly in bag 1 and is a red ball. Therefore, we''ll get the following
    outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f911fb7-4eb6-43bb-a4ce-0c96cdcff090.png)![](img/e3efe7e1-3a87-4354-b21e-ac7937c9f667.png)![](img/e66819cb-c421-45ce-8648-29f5ad15af27.png)'
  prefs: []
  type: TYPE_IMG
- en: The mathematical framework for Bayesian statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian methods are an alternative way of making a statistical inference. We
    will first look at Bayes' theorem, the fundamental equation from which all Bayesian
    inference is derived.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few definitions regarding probability are in order before we begin:'
  prefs: []
  type: TYPE_NORMAL
- en: '***A***,***B***: These are events that can occur with a certain probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***P*(*A*)** and ***P*(*B*)**: This is the probability of the occurrence of
    a particular event.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***P*(*A*|*B*)**: This is the probability of A happening, given that B has
    occurred. This is known as a **conditional probability**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***P*(*AB*) = *P*(*A* and *B*)**: This is the probability of A and B occurring
    together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We begin with the following basic assumption:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(AB) = P(B) * P(A|B)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding equation shows the relation of the joint probability of *P*(*AB*)
    to the conditional probability *P*(*A*|*B*) and what is known as the marginal
    probability, *P*(*B*). If we rewrite the equation, we have the following expression
    for conditional probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A|B) = P(AB)/P(B)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is somewhat intuitive—the probability of *A* given *B* is obtained by
    dividing the probability of both *A* and *B* occurring by the probability that
    *B* occurred. The idea is that *B* is given, so we divide by its probability.
    A more rigorous treatment of this equation can be found at [http://bit.ly/1bCYXRd](http://bit.ly/1bCYXRd),
    which is titled *Probability: Joint, Marginal and Conditional Probabilities*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, by symmetry we have *P(AB) = P(BA) = P(A) * P(B|A*). Thus, we have
    *P(A) * P(B|A) = P(B) * P(A|B)*. By dividing the expression by *P(B)* on both
    sides and assuming *P(B)!=0*, we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb804081-dd3b-48b3-b61f-00b5b173de93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation is referred to as **Bayes'' theorem**, the bedrock of
    all Bayesian statistical inference. In order to link Bayes'' theorem to inferential
    statistics, we will recast the equation as what is called the **diachronic** **interpretation**,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/940489d3-fe93-4be2-977a-4deeff0004a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ***H*** represents a hypothesis and ***D*** represents an event that has
    already occurred that we are using in our statistical study, and is also referred
    to as data.
  prefs: []
  type: TYPE_NORMAL
- en: The expression **(*H*)** is the probability of our hypothesis before we observe
    the data. This is known as the **prior probability**. The use of prior probability
    is often touted as an advantage by Bayesian statisticians since prior knowledge
    of previous results can be used as input for the current model, resulting in increased
    accuracy. For more information on this, refer to [http://www.bayesian-inference.com/advantagesbayesian](http://www.bayesian-inference.com/advantagesbayesian).
  prefs: []
  type: TYPE_NORMAL
- en: '***P*(*D*)** is the probability of obtaining the data that we observe regardless
    of the hypothesis. This is called the **normalizing constant**. The normalizing
    constant doesn''t always need to be calculated, especially in many popular algorithms,
    such as **Markov Chain Monte Carlo** (**MCMC**), which we will examine later in
    this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '***P*(*H*|*D*)** is the probability that the hypothesis is true, given the
    data that we observe. This is called the **posterior**.'
  prefs: []
  type: TYPE_NORMAL
- en: '***P*(*D*|*H*)**is the probability of obtaining the data, considering our hypothesis.
    This is called the **likelihood**.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, Bayesian statistics amounts to applying Bayes' rule to solve problems
    in inferential statistics with ***H*** representing our hypothesis and ***D***
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Bayesian statistical model is cast in terms of parameters, and the uncertainty
    in these parameters is represented by probability distributions. This is different
    from the frequentist approach where the values are regarded as deterministic.
    An alternative representation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31826c2d-6a8a-4fe8-8aed-8d5e58794e57.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ***Θ*** is our unknown data and ***x*** is our observed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Bayesian statistics, we make assumptions about the prior data and use the
    likelihood to update the posterior probability using Bayes'' rule. As an illustration,
    let''s consider the following problem. Here is a classic case of what is commonly
    known as the **urn problem**:'
  prefs: []
  type: TYPE_NORMAL
- en: Two urns contain colored balls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urn 1 contains 50 red and 50 blue balls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urn 2 contains 30 red and 70 blue balls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the two urns is randomly chosen (50 percent probability) and then a ball
    is drawn at random from one of the two urns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a red ball is drawn, what is the probability that it came from urn 1? We
    want *P(H|D)*—that is *P(ball came from Urn 1 | Red ball is drawn)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *H* denotes that the ball is drawn from urn 1, and *D *denotes that the
    drawn ball is red:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/5f51ffcc-a0b4-4d21-9d20-49ffa0df0aed.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that *P(H|D) = P(H) * P(D|H)/P(D), P(D|H) = 0.5, P(D) = (50 + 30)/(100
    + 100) = 0.4*. This can also be phrased as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34d84e7c-fa89-49c5-beb1-a2a842d7dba6.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, we conclude that *P(H|D) = 0.5 * 0.5/0.4 = 0.25/0.4 = 0.625*.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theory and odds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayes' theorem can sometimes be represented in a more natural and convenient
    form by using an alternative formulation of probability called *odds*. Odds are
    generally expressed in terms of ratios and are commonly used. A 3-to-1 odds (written
    often as 3:1) of a horse winning a race represents the fact that there is a 75
    percent probability that the horse will win.
  prefs: []
  type: TYPE_NORMAL
- en: Given a probability *p*, the odds can be calculated as odds = *p:(1 - p)*, which
    in the case of *p=0.75* becomes 0.75:0.25, which is 3:1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using odds, we can rewrite Bayes'' theorem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18ea1ab8-a709-4713-a7ca-807d79b3cbb3.png)'
  prefs: []
  type: TYPE_IMG
- en: Applications of Bayesian statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bayesian statistics can be applied to many problems that we encounter in classical
    statistics, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Parameter estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypothesis testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many compelling reasons for studying Bayesian statistics, such as
    using prior information to better inform a current model. The Bayesian approach
    works with probability distributions rather than point estimates, thus producing
    more realistic predictions. Bayesian inference bases a hypothesis on the available
    data—*P(hypothesis|data)*. The frequentist approach tries to fit the data based
    on a hypothesis. It can be argued that the Bayesian approach is the more logical
    and empirical one as it tries to base its belief on the facts rather than the
    other way round. For more information on this, refer to [http://www.bayesian-inference.com/advantagesbayesian](http://www.bayesian-inference.com/advantagesbayesian).
  prefs: []
  type: TYPE_NORMAL
- en: Probability distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will briefly examine the properties of various probability
    distributions. Many of these distributions are used for Bayesian analysis, and
    so a brief synopsis is needed before we can proceed. We will also illustrate how
    to generate and display these distributions using `matplotlib`. In order to avoid
    repeating the `import` statements for every code snippet in each section, we will
    be presenting the following standard set of Python code imports that need to be
    run before any of the code snippets mentioned in the following command. You only
    need to run these imports once per session. The imports are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Fitting a distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the steps that we have to take in a Bayesian analysis is to fit our
    data to a probability distribution. Selecting the correct distribution can be
    something of an art, and often requires statistical knowledge and experience,
    but we can follow a few guidelines to help us along the way. These guidelines
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine whether the data is discrete or continuous
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examine the skewness/symmetry of the data, and if skewed, determine the direction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the lower and upper limits, if there are any
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the likelihood of observing extreme values in the distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A statistical trial is a repeatable experiment with a set of well-defined outcomes
    that is known as the sample space. A Bernoulli trial is a yes/no experiment where
    the random *X* variable is assigned the value of 1 in the case of a yes and 0
    in the case of a no. The event of tossing a coin and seeing whether it lands heads
    up is an example of a Bernoulli trial.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two classes of probability distributions: discrete and continuous.
    In the following sections, we will discuss the differences between these two classes
    of distributions and learn about the main distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: Discrete probability distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this scenario, the variable can take only certain distinct values, such
    as integers. An example of a discrete random variable is the number of heads obtained
    when we flip a coin five times: the possible values are {0,1,2,3,4,5}—we cannot
    obtain 3.82 heads, for example. The range of values that the random variable can
    take is specified by what is known as a **probability mass function** (**PMF**).'
  prefs: []
  type: TYPE_NORMAL
- en: Discrete uniform distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The discrete uniform distribution is a distribution that models an event with
    a finite set of possible outcomes where each outcome is equally likely to be observed.
    For *n* outcomes, each has a probability of occurrence of *1/n*.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this is throwing a fair die. The probability of any of the six
    outcomes is *1**/6*. The PMF is given by *1/n*, and the expected value and variance
    are given by *(max + min)/2* and *(n^(2 )-1)/12 *respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/196aa30a-0528-4fda-bad5-776bb8d864aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Discrete uniform distribution
  prefs: []
  type: TYPE_NORMAL
- en: The Bernoulli distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Bernoulli distribution measures the probability of success in a trial, for
    example, the probability that a coin toss turns up a head or a tail. This can
    be represented by a random *X* variable that takes a value of 1 if the coin turns
    up as heads and 0 if it is tails. The probability of it turning up heads or tails
    is denoted by *p* and *q=1-p* respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be represented by the following PMF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0cd5693-c6e5-413f-b4cf-e32f8c6c49e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The expected value and variance are given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b5c6df3-d4ad-4b41-a70d-eab3c0e750af.png)![](img/6a5f86c1-346d-4596-a895-87c509126ba3.png)'
  prefs: []
  type: TYPE_IMG
- en: For more information, go to [http://en.wikipedia.org/wiki/Bernoulli_distribution](http://en.wikipedia.org/wiki/Bernoulli_distribution).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now plot the Bernoulli distribution using `matplotlib` and `scipy.stats`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4098dfe-edfe-4110-9326-c6d2ca5cc42a.png)'
  prefs: []
  type: TYPE_IMG
- en: Bernoulli distribution output
  prefs: []
  type: TYPE_NORMAL
- en: The binomial distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The binomial distribution is used to represent the number of successes in *n*-independent
    Bernoulli trials, which can be expressed as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y = X[1] + X[2] + ..**. + X[n]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the coin toss example, this distribution models the chance of getting
    *X* heads over *n* trials. For 100 tosses, the binomial distribution models the
    likelihood of getting 0 heads (extremely unlikely) to 50 heads (highest likelihood)
    to 100 heads (also extremely unlikely). This ends up making the binomial distribution
    symmetrical when the odds are perfectly even and skewed when the odds are far
    less even. The PMF is given by the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e13d328-8677-46e3-b3a9-ed0b87abaaf7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The expectation and variance are given respectively by the following expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/639a8dc3-b41c-4c7b-b121-237eb9450624.png)![](img/7d232401-9732-454a-984d-462be5c08854.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is shown using the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37442818-7135-48bc-94ff-1bc568e34525.png)'
  prefs: []
  type: TYPE_IMG
- en: Binomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: The Poisson distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Poisson distribution models the probability of a number of events within
    a given time interval, assuming that these events occur with a known average rate
    and successive events occur independently of the time that has passed since the
    previous event.
  prefs: []
  type: TYPE_NORMAL
- en: A concrete example of a process that can be modeled by a Poisson distribution
    would be if an individual received an average of, say, 23 emails per day. If we
    assume that the arrival times for the emails are independent of each other, then
    the total number of emails an individual receives each day can be modeled by a
    Poisson distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example could be the number of trains that stop at a particular station
    each hour. The PMF for a Poisson distribution is given by the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12e39851-b53f-4f77-8290-d223a896ceea.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **λ** is the rate parameter, which, represents the expected number of
    events/arrivals that occur per unit time, and ***k*** is the random variable that
    represents the number of events/arrivals.
  prefs: []
  type: TYPE_NORMAL
- en: 'The expectation and variance are given respectively by the following expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17e854ab-f1df-4d2f-9048-055fe6cd38e1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/bf6fc199-c828-43c3-bbbe-a40943d3655f.png)'
  prefs: []
  type: TYPE_IMG
- en: For more information, refer to [http://en.wikipedia.org/wiki/Poisson_process](http://en.wikipedia.org/wiki/Poisson_process).
  prefs: []
  type: TYPE_NORMAL
- en: 'The PMF is plotted using `matplotlib` for various values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d76cc92f-0a38-40f2-9b7a-5d4ac0835b5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Poisson distribution
  prefs: []
  type: TYPE_NORMAL
- en: The geometric distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The geometric distribution is used for independent Bernoulli trials and measures the
    number of trials (*X*) that are needed to get one success. It can also represent
    the number of failures (*Y = X-1*) before the first success.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PMF is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bb196e0-45af-40d2-9840-129a97a651d6.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding expression makes sense because *f(k) = P(X =k)*, and if it takes
    *k* trials to get one success (*p*), then this means that we must have had *k-1 *failures,
    which are equal to *(1-p)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The expectation and variance are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edd2251a-db95-48b2-931d-bb8ec4f49ef6.png)![](img/ea239973-c582-4e22-9605-66a7ff59a7a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following command explains the preceding formula clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9da8cd2c-f58f-4eae-aeac-222598dc2050.png)'
  prefs: []
  type: TYPE_IMG
- en: Geometric distribution
  prefs: []
  type: TYPE_NORMAL
- en: The negative binomial distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The negative binomial distribution is used for independent Bernoulli trials
    and measures the number of tries (*X=k*) that are needed before a specified number
    of successes (*r*) occur. An example would be the number of coin tosses it would
    take to obtain five heads. The PMF is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30d65342-aed7-42aa-8d37-4a612d9eafca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The expectation and variance are given respectively by the following expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d92080e8-a583-4b9f-8aca-c84c0625d789.png)![](img/88bb3682-ee52-4bd6-b669-5d9471efc8ed.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the negative binomial is a generalization of the geometric distribution,
    with the geometric distribution being a special case of the negative binomial,
    where *r=1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code and plot are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c38c6b79-474d-4e5a-80b8-7db8bf94687f.png)'
  prefs: []
  type: TYPE_IMG
- en: Negative binomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: Continuous probability distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a continuous probability distribution, the variable can take on any real
    number. It is not limited to a finite set of values, as it is in a discrete probability
    distribution; for example, the average weight of a healthy newborn baby can range
    from approximately 6–9 lbs (its weight can be 7.3 lbs, for example). A continuous
    probability distribution is characterized by a **probability density function**
    (**PDF**).
  prefs: []
  type: TYPE_NORMAL
- en: The sum of all the probabilities that the random variable can assume is 1\.
    Thus, the area under the graph of the probability density function adds up to
    1.
  prefs: []
  type: TYPE_NORMAL
- en: The continuous uniform distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The uniform distribution models a random variable, *X,* that can take any value
    within the range *[a, b] *with equal probability.
  prefs: []
  type: TYPE_NORMAL
- en: The PDF is given by ![](img/9f0003f2-af42-4de4-9b82-37aeab3a755f.png) for *a ≤
    x ≤ b*, and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The expectation and variance are given respectively by the following expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/177876da-7f87-4816-9d9f-8a3c150d43ab.png)![](img/470ae43e-1c0e-4d2d-a58c-4ceb432e1707.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A continuous uniform probability distribution is generated and plotted for
    various sample sizes in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5d15705-521e-4d31-be41-fdadad5af9f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Continuous uniform distribution
  prefs: []
  type: TYPE_NORMAL
- en: The exponential distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The exponential distribution models the waiting time between two events in a
    Poisson process. A Poisson process is a process that follows a Poisson distribution
    in which events occur unpredictably with a known average rate. The exponential
    distribution can be described as the *continuous limit* of the geometric distribution,
    and is also Markovian (memoryless).
  prefs: []
  type: TYPE_NORMAL
- en: A memoryless random variable exhibits the property that its future state depends
    only on relevant information about the current time and not the information from
    further in the past. An example of modeling a Markovian/memoryless random variable
    is modeling short-term stock price behavior based on the idea that it follows
    a random walk. This leads to what is called the efficient market hypothesis in
    finance. For more information, refer to [http://en.wikipedia.org/wiki/Random_walk_hypothesis](http://en.wikipedia.org/wiki/Random_walk_hypothesis).
  prefs: []
  type: TYPE_NORMAL
- en: 'The PDF of the exponential distribution is given by *f(x)= λe^(-λx)*. The expectation
    and variance are given respectively by the following expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(X) = 1/λ**Var(X) = 1/λ²*'
  prefs: []
  type: TYPE_NORMAL
- en: For reference, refer to the link at [http://en.wikipedia.org/wiki/Exponential_distribution](http://en.wikipedia.org/wiki/Exponential_distribution).
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot of the distribution and code is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/827f7798-a218-482a-bd30-480b22484ccd.png)'
  prefs: []
  type: TYPE_IMG
- en: Exponential distribution
  prefs: []
  type: TYPE_NORMAL
- en: The normal distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most important distribution in statistics is arguably the normal/Gaussian
    distribution. It models the probability distribution around a central value with
    no left or right bias. There are many examples of phenomena that follow the normal
    distribution, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The birth weights of babies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measurement errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blood pressure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The normal distribution''s importance is underlined by the central limit theorem,
    which states that the mean of many random variables drawn independently from the
    same distribution is approximately normal, regardless of the form of the original
    distribution. Its expected value and variance are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(X) = μ**Var(X) = σ²*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The PDF of the normal distribution is given by the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9b5cd60-9150-4c82-a3c7-130e320c1deb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code and plot explain the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/016bcf9c-4410-447c-8170-ca277b48cf20.png)'
  prefs: []
  type: TYPE_IMG
- en: Normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: A reference for the Python code to the plot the distributions can be found at [http://bit.ly/1E17nYx](http://bit.ly/1E17nYx).
  prefs: []
  type: TYPE_NORMAL
- en: 'The normal distribution can also be regarded as the continuous limit of the
    binomial distribution, and other distributions as ![](img/17367aee-d5be-4fec-9960-9563349eaa5c.png).
    We can see this in the following code and plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02ca92ac-5aec-421d-8aa9-19baf2f6669a.png)'
  prefs: []
  type: TYPE_IMG
- en: As *n* increases, the binomial distribution approaches the normal distribution.
    In fact, this is clearly seen in the preceding plots for *n>=30*.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian statistics versus frequentist statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In statistics today, there are two schools of thought as to how we interpret
    data and make statistical inferences. The classical and more dominant approach
    to date has been what is termed the frequentist approach (refer to [Chapter 7](4ff075ff-5491-4503-a1ca-eb62f1704cf4.xhtml), *A
    Tour of Statistics – The Classical Approach*). We are looking at the Bayesian
    approach in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What is probability?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the heart of the debate between the Bayesian and frequentist worldview is
    the question of how we define probability.
  prefs: []
  type: TYPE_NORMAL
- en: In the frequentist worldview, probability is a notion that is derived from the
    frequencies of repeated events—for example, when we define the probability of
    getting heads when a fair coin is tossed as being equal to half. This is because
    when we repeatedly toss a fair coin, the number of heads divided by the total
    number of coin tosses approaches 0.5 when the number of coin tosses is sufficiently
    large.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian worldview is different, and the notion of probability is that it
    is related to one's degree of belief in the event happening. Thus, for a Bayesian
    statistician, having a belief that the probability of a fair die turning up five
    is *1/6* relates to our belief in the chances of that event occurring.
  prefs: []
  type: TYPE_NORMAL
- en: How the model is defined
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the model definition point of view, frequentists analyze how data and
    calculated metrics vary by making use of repeated experiments while keeping the
    model parameters fixed. Bayesians, on the other hand, utilize fixed experimental
    data, but vary their degrees of belief in the model parameters. This is explained
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequentists**: If the models are fixed, data varies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesians**: If the data is fixed, models vary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The frequentist approach is to use what is known as the maximum likelihood method
    to estimate model parameters. It involves generating data from a set of independent
    and identically distributed observations and fitting the observed data to the
    model. The value of the model parameter that best fits the data is the **maximum
    likelihood estimator** (**MLE**), which can sometimes be a function of the observed
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesianism approaches the problem differently from a probabilistic framework.
    A probability distribution is used to describe the uncertainty in the values.
    Bayesian practitioners estimate probabilities using observed data. In order to
    compute these probabilities, they make use of a single estimator, which is the
    Bayes formula. This produces a distribution rather than just a point estimate,
    as in the case of the frequentist approach.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence (frequentist) versus credible (Bayesian) intervals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's compare what is meant by a 95 percent confidence interval (a term used
    by frequentists) with a 95 percent credible interval (a term used by Bayesian
    practitioners).
  prefs: []
  type: TYPE_NORMAL
- en: In a frequentist framework, a 95 percent confidence interval means that if you
    repeat your experiment an infinite number of times, generating intervals in the
    process, 95 percent of these intervals would contain the parameter we're trying
    to estimate, which is often referred to as θ. In this case, the interval is the
    random variable and not the parameter estimate, θ, which is fixed in the frequentist
    worldview.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the Bayesian credible interval, we have an interpretation that
    is more in line with the conventional interpretation ascribed to that of a frequentist
    confidence interval. Thus, we conclude that *Pr(a(Y) < θ < b(Y)|θ) = 0.95*. In
    this case, we can properly conclude that there is a 95 percent chance that θ lies
    within the interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, refer to *Frequentism and Bayesianism: What''s the Big
    Deal?* (Jake VanderPlas*, **SciPy, 2014**)* at [https://www.youtube.com/watch?v=KhAUfqhLakw](https://www.youtube.com/watch?v=KhAUfqhLakw).'
  prefs: []
  type: TYPE_NORMAL
- en: Conducting Bayesian statistical analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Conducting a Bayesian statistical analysis involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specifying a probability model**: In this step, we fully describe the model
    using a probability distribution. Based on the distribution of a sample that we
    have taken, we try to fit a model to it and attempt to assign probabilities to
    unknown parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calculating a posterior distribution**: The posterior distribution is a distribution
    that we calculate in light of observed data. In this case, we will directly apply
    Bayes'' formula. It will be specified as a function of the probability model that
    we specified in the previous step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Checking our model**: This is a necessary step where we review our model
    and its outputs before we make inferences. Bayesian inference methods use probability
    distributions to assign probabilities to possible outcomes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monte Carlo estimation of the likelihood function and PyMC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian statistics isn't just another method. It is an entirely different paradigm
    for practicing statistics. It uses probability models for making inferences, given
    the data that has been collected. This can be expressed in a fundamental expression
    as *P*(*H*|*D*).
  prefs: []
  type: TYPE_NORMAL
- en: Here, *H* is our hypothesis, that is, the thing we're trying to prove, and *D*
    is our data or observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder of our previous discussion, the diachronic form of Bayes'' theorem
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f977f4b1-d41e-4066-8fb5-68df68f63ef2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *P(H)* is an unconditional prior probability that represents what we know
    before we conduct our trial. *P(D|H)* is our likelihood function, or probability
    of obtaining the data we observe, given that our hypothesis is true.
  prefs: []
  type: TYPE_NORMAL
- en: '*P(D)* is the probability of the data, also known as the normalizing constant.
    This can be obtained by integrating the numerator over *H*.'
  prefs: []
  type: TYPE_NORMAL
- en: The likelihood function is the most important piece of our Bayesian calculation
    and encapsulates all the information about the unknowns in the data. It has some
    semblance to a reverse probability mass function.
  prefs: []
  type: TYPE_NORMAL
- en: One argument against adopting a Bayesian approach is that the calculation of
    the prior can be subjective. There are many arguments in favor of this approach,
    one being that external prior information can be included, as mentioned previously.
  prefs: []
  type: TYPE_NORMAL
- en: The likelihood value represents an unknown integral, which in simple cases can
    be obtained by analytic integration.
  prefs: []
  type: TYPE_NORMAL
- en: '**Monte Carlo** (**MC**) integration is needed for more complicated use cases
    involving higher-dimensional integrals and can be used to compute the likelihood
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MC integration can be computed through a variety of sampling methods, such
    as uniform sampling, stratified sampling, and importance sampling. In Monte Carlo
    integration, we can approximate the integral as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f0e1ceb-ffad-4e02-afdb-f9cdaff9f523.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the finite sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e024771a-b52d-449d-ab59-4f62abef098c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* is a sample vector from *g*. The proof that this estimate is a good
    one can be obtained from the law of large numbers and by making sure that the
    simulation error is small.
  prefs: []
  type: TYPE_NORMAL
- en: 'When conducting Bayesian analysis in Python, we will need a module that will
    enable us to calculate the likelihood function using the Monte Carlo method. The
    `PyMC` library fulfills that need. It provides a Monte Carlo method known commonly
    as MCMC. We will not delve further into the technical details of MCMC, but the
    interested reader can find out more about MCMC implementation in `PyMC` from the
    following sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Monte Carlo Integration in Bayesian Estimation*:[http://bit.ly/1bMALeu](http://bit.ly/1bMALeu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Markov Chain Monte Carlo Maximum Likelihood*: [http://bit.ly/1KBP8hH](http://bit.ly/1KBP8hH)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bayesian Statistical Analysis Using Python–Part 1*, *SciPy 2014*, *Chris Fonnesbeck*:
    [http://www.youtube.com/watch?v=vOBB_ycQ0RA](http://www.youtube.com/watch?v=vOBB_ycQ0RA)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MCMC is not a universal panacea; there are some drawbacks to the approach, and
    one of them is the slow convergence of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian analysis example – switchpoint detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will try to use Bayesian inference and model an interesting dataset.
    The dataset in question consists of the author''s **Facebook** (**FB**) post history
    over time. We have scrubbed the FB history data and saved the dates in the `fb_post_dates.txt`
    file. Here is what the data in the file looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, we see a datetime series, representing the date and time at which the
    author posted on FB. First, we read the file into DataFrame, separating the timestamp
    into `Date` and `Time` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we inspect the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we index the data by `Date`, creating a `DatetimeIndex` so that we can
    run resample on it to count by month, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We then display information about the index as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we obtain a count of posts by month using resample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Date` format is shown as the last day of the month. Now, we create a scatter
    plot of FB post counts from 2007–2015, and we make the size of the dots proportional
    to the values in `matplotlib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8f54069-a899-4d51-b70c-973445ee602d.png)'
  prefs: []
  type: TYPE_IMG
- en: The question we would like to investigate is whether there was a change in behavior
    at some point over the time period. Specifically, we want to identify whether
    there was a specific period at which the mean number of FB posts changed. This
    is often referred to as the switchpoint or changepoint in a time series.
  prefs: []
  type: TYPE_NORMAL
- en: We can make use of the Poisson distribution to model this. You might recall
    that the Poisson distribution can be used to model time series count data. (Refer
    to [http://bit.ly/1JniIqy](http://bit.ly/1JniIqy) for more about this.)
  prefs: []
  type: TYPE_NORMAL
- en: 'If we represent our monthly FB post count by *C[i]*, we can represent our model
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e251d82-2dfc-4635-b9d2-7e9283ff1894.png)'
  prefs: []
  type: TYPE_IMG
- en: The *r[i]* parameter is the rate parameter of the Poisson distribution, but
    we don't know what its value is. If we examine the scatter plot of the FB time
    series count data, we can see that there was a jump in the number of posts sometime
    around mid to late 2010, perhaps coinciding with the start of the 2010 World Cup
    in South Africa, which the author attended.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *s* parameter is the switchpoint, which is when the rate parameter changes,
    while *e* and *l* are the values of the *r[i]* parameter before and after the
    switchpoint respectively. This can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/361ba479-3bcd-49ed-bd8c-a0ec8ca56a8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the variables specified here—*C, s, e, r* and *l—*are all Bayesian
    random variables. For Bayesian random variables that represent one''s beliefs
    about their values, we need to model them using a probability distribution. We
    would like to infer the values of *e* and *l*, which are unknown. In `PyMC`, we
    can represent random variables using the stochastic and deterministic classes.
    We note that the exponential distribution is the amount of time between Poisson
    events. Therefore, in the case of *e* and *l*, we choose the exponential distribution
    to model them since they can be any positive number:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7756ac98-afa8-4b65-8691-33945a71f9e7.png)![](img/df4d13c3-7654-43ff-a018-4deed64fff6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of *s*, we will choose to model it using the uniform distribution,
    which reflects our belief that it is equally likely that the switchpoint can occur
    on any day within the entire time period. This means that we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1e4231b-aff6-4f0c-a7e3-9b297f2c112d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *t[0] , t[f]* corresponds to the lower and upper boundaries of the year, *i*.
    Let''s now use `PyMC` to represent the model that we developed earlier. We will
    now use `PyMC` to see whether we can detect a switchpoint in the FB post data.
    In addition to the scatter plot, we can also display the data in a bar chart.
    To do that, we first need to obtain a count of FB posts ordered by month in a
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We render the bar plot using `matplotlib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55734643-b76f-4798-972c-d844d30bbd60.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at the preceding bar chart, can we conclude that there was a change
    in FB frequency posting behavior over time? We can use `PyMC` on the model that
    we have developed to help us find out the change as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define a method for the rate parameter, *r*, and we model the count
    data using a Poisson distribution as discussed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, `@pm.deterministic` is a decorator that denotes
    that the rate function is deterministic, meaning that its values are entirely
    determined by other variables—in this case, *e*, *s*, and *l*. The decorator is
    necessary to tell `PyMC` to convert the rate function into a deterministic object.
    If we do not specify the decorator, an error occurs. For more information about Python
    decorators, refer to [http://bit.ly/1zj8U0o](http://bit.ly/1zj8U0o).
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, refer to the following web pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://en.wikipedia.org/wiki/Poisson_process](http://en.wikipedia.org/wiki/Poisson_process)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://pymc-devs.github.io/pymc/tutorial.html](http://pymc-devs.github.io/pymc/tutorial.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now create a model with the FB count data (`fb_activity`) and the *e,
    s, l* (`early_mean`, `late_mean`, and `rate` respectively) parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, using `PyMC`, we create an `MCMC` object that enables us to fit our data
    using MCMC methods. We then call sample on the resulting `MCMC` object to do the
    fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Fitting the model using MCMC involves using Markov Chain Monte Carlo methods
    to generate a probability distribution for the posterior, *P(s,e,l | D)*. It uses
    the Monte Carlo process to repeatedly simulate sampling of the data and does this
    until the algorithm seems to converge to a steady state, based on multiple criteria.
    This is a Markov process because successive samples are dependent only on the
    previous sample. For further information on Markov chain convergence, refer to
    [http://bit.ly/1IETkhC](http://bit.ly/1IETkhC).
  prefs: []
  type: TYPE_NORMAL
- en: 'The generated samples are referred to as **traces**. We can view what the marginal
    posterior distribution of the parameters looks like by looking at a histogram
    of its trace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1491b72e-3d78-41fe-91be-3f54de7b51fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we find the early mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29eaa6b2-eb05-44a5-bb86-c1bd8bf65222.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we see what the switchpoint looks like in terms of the number of months:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be583c20-b4c2-4dcd-ad02-1153c778bdf7.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram of the switchpoint for the number of months
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the switchpoint is in the neighborhood of the months 35–38\.
    Here, we use `matplotlib` to display the marginal posterior distributions of *e*,
    *s*, and *l* in a single diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b3b9538-cad8-46fb-b4b0-6e5a0ef48b40.png)'
  prefs: []
  type: TYPE_IMG
- en: Marginal posterior distributions
  prefs: []
  type: TYPE_NORMAL
- en: '`PyMC` also has plotting functionality as it uses `matplotlib`. In the following
    plots, we display a time series plot, an **autocorrelation plot** (**acorr**),
    and a histogram of the samples drawn for the early mean, late mean, and switchpoint.
    The histogram is useful for visualizing the posterior distribution. The autocorrelation
    plot shows whether values in the previous period are strongly related to values
    in the current period:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the late mean plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfa3afc4-e774-4ae0-918a-c51fd38c37d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph for pymc_comprehensive_late_mean
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we display the switchpoint plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d093d961-60f7-4e48-aebd-1c755d1fc781.png)'
  prefs: []
  type: TYPE_IMG
- en: PyMC comprehensive switchpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we display the early mean plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/027d6b82-1340-4f1f-bec2-929cba701772.png)'
  prefs: []
  type: TYPE_IMG
- en: PyMC comprehensive early mean
  prefs: []
  type: TYPE_NORMAL
- en: From the output of `PyMC`, we can conclude that the switchpoint is around 35–38
    months from the start of the time series. This corresponds to some time between
    March and July 2010\. The author can testify that this was a banner year for him
    with respect to the use of FB since it was the year of the FIFA World Cup finals
    that were held in South Africa, which he attended.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Maximum likelihood estimate** (**MLE**) is a method that is used to estimate
    the probability distribution parameters of a population from the available sample
    data. MLE methods can also be considered as a Bayesian alternative.'
  prefs: []
  type: TYPE_NORMAL
- en: Probability distribution gives the probability of observing a data point given
    distribution parameters such as mean, standard deviation, and degree of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of data point given distribution parameters is expressed as
    Prob(*X*|µ,α) -------1.
  prefs: []
  type: TYPE_NORMAL
- en: MLE deals with the inverse problem. It is used to find the most likely values
    of distribution parameters given the set of data points. For that purpose, another
    statistic called likelihood is defined. The likelihood is defined as the probability
    of observing the distribution parameters given the data points.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of observing the distribution parameters given the data point
    is expressed as L(µ,α|X)----2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quantities in equations 1 and 2 are the same probabilities, just stated
    differently. Therefore, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bb4acad-a045-490d-9c6a-feb61b320ef3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To understand this concept better, have a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9637d944-50f0-4d7e-a0e2-5bcc15c752be.png)'
  prefs: []
  type: TYPE_IMG
- en: MLE estimation illustration for normal distribution with two data points
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows three normal distributions with different means and standard
    deviations. The two vertical lines represent values V[1]=-2.5 and V[2]=2.5.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we represent the probability that a data point V[1]=-2.5 belongs
    to the red (mean=0, std dev=1.5) distribution by *P*(red|*V*[1]=-2.5). Similarly,
    the probability that a data point V[1]=-2.5 belongs to the blue (mean=0, std dev=4)
    distribution is *P*(blue|*V*[1]=-2.5).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, looking at the graph shown here, we can state the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fae0992a-f40e-494e-b8c4-b12c1e0a001a.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/249c806c-3a4a-46bb-bea2-5876169e662e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/561cc532-0621-47b9-9bd8-f9fe21a0b783.png)'
  prefs: []
  type: TYPE_IMG
- en: If we had to make a decision based on only available datapoints, then we would
    decide that *V*[1]=2.5 belongs to the blue distribution as *V*[1]*B*[1] > *V*[1]*R*[1]
    > *V*[1]*G*[1], and we choose the distribution with the largest probability of
    that data point belonging to that distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if we have one more data point, or many more? For this situation,
    let''s add another data point called *V*[2] to our dataset. The individual probabilities
    for *V*[2] are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5dc480e2-4bdc-460c-8411-8333f2c3f09d.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/8875ef97-7d76-42d3-aa77-4ce4aa42b299.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/893ba73f-a3e6-4723-8fb3-909a56cd3be9.png)'
  prefs: []
  type: TYPE_IMG
- en: Since there are two points now, we can't base our decisions on individual probabilities
    and would have to calculate joint probabilities. If we assume that the event of
    one data point occurring is independent of the event of another data point occurring,
    then the joint probabilities are given by the individual probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the joint probability that the two points belong to the red distribution
    given the two data points is denoted by *P(red|V[1]=-2.5, V[2]=2.5)*. Then the
    following will be true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72265fda-4817-48b5-a819-5ce4803a8558.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/90249c71-fee9-45da-9088-adf998f56c71.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/8b8bd5ac-db67-4b7d-a29d-60f40025f122.png)'
  prefs: []
  type: TYPE_IMG
- en: We should choose the distribution that has the maximum joint probability. In
    this case, the blue would have the highest joint probability, as we can conclude
    by looking at the graph.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of points in the dataset increase, it no longer remains feasible
    to draw inferences about the maximum joint probability by looking at charts like
    the preceding charts. We need to resort to algebraic and calculus methods to find
    the parameters (defining the distribution) to find the distribution that maximises
    the joint probability of the dataset to belong to that distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we assume that the individual data points are independent of each other,
    then the likelihood or probability of observing distribution parameters given
    all the data points is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a60f846-d1f4-405a-87ac-5d0921e1f3c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the MLE calculations, we try to find the values of µ, α that maximize *L*(µ,α|*X*[1],
    *X*[2], *X*[3], ...., *X[n]*). In that endeavor, taking the log of both sides
    helps a lot. It doesn''t change the objective function as the log is a monotonically
    increasing function that makes the calculations a lot easier. The log of the likelihood
    is often called the log-likelihood and is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09f0ced7-26d1-453c-926d-edb3a6f70f50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To find the maxima of the log-likelihood function, log(*L*(µ,α|*X*[1], *X*[2],
    *X*[3], ...., *X[n]*)), we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Take the first derivative of *log(L(µ,α|X[1], X[2], X[3], ...., X[n]))*, function
    w.r.t µ,α and equate it to 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take the second derivative of *log(L(µ,α|X[1], X[2], X[3], ...., X[n]))*, function
    w.r.t µ,α and confirm that it is negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLE calculation examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now look at two MLE calculation examples.
  prefs: []
  type: TYPE_NORMAL
- en: Uniform distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose we have a probability distribution of *X* that means the following
    is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pr(X=x) = 1/b-a for all a<X<b*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pr(X=x) = 0 for all other X*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *a* and *b* are parameters of uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Since the probability is the same (or uniform) for all the values, it is called
    a uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose there are *n* data points in a dataset that is hypothesized to follow
    a uniform distribution. Based on these points, we aim to find the values of *a*
    and *b* to define the distribution to which these data points most likely belong.
    For this, we can use the maximum likelihood estimate method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7fcec66-609f-4180-903d-8e1f4231b71c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/6f4b8981-5693-494e-a6ec-3c9f19c7f053.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/04d35de8-0bd9-40d2-9839-9d5c023f042f.png)'
  prefs: []
  type: TYPE_IMG
- en: We have to find the *a*, *b* that maximises *log*(*L*(*a*,*b*|*x*[1],*x*[2],*x*[3],.....,*x[n]*).
  prefs: []
  type: TYPE_NORMAL
- en: For this, we would differentiate *log*(*L*(*a*,*b*|*x*[1],*x*[2],*x*[3],.....,*x[n]*)
    with respect to *b*-*a*.
  prefs: []
  type: TYPE_NORMAL
- en: This gives -*n*/(*b*-*a*), which is always less than zero, indicating that *log*(*L*(*a*,*b*|*x*[1],*x*[2],*x*[3],.....,*x[n]*)
    is a monotonically decreasing function and its value would decrease as (*b*-*a*)
    increased. Therefore, a large *b*-*a* would maximize the probability.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping that in mind, we get *b*=*max*(*X*), *a* = *min*(*X*).
  prefs: []
  type: TYPE_NORMAL
- en: Poisson distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Poisson distribution has been explained in an earlier section of this chapter.
    In a nutshell, the Poisson distribution is a binomial distribution with an infinitely
    large number of samples, so large that the discrete nature of the binomial distribution
    gives way to the Poisson distribution. The Poisson distribution also deals with
    the probability of the occurrence of events. But rather than thinking in terms
    of the probability of the occurrence of the event in each trial, we think in terms
    of time intervals and ask ourselves how many times the event of interest would
    occur in that time interval. The parameter also moves from the probability of
    success for each trial to the number of successes in a given time interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binomial**: Probability of a number of successes in a given number of trials
    given a probability of success for each trial'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poisson**: Probability of a number of successes in a given interval of time
    given the arrival or success rate—that is, the average number of successes in
    a given time interval'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Poisson probability distribution is expressed by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2343d4cc-4b5a-4091-b203-3af20735921a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **λ** is the arrival or success rate.
  prefs: []
  type: TYPE_NORMAL
- en: This expression gives the probability of observing *x* successes in the given
    time interval (the same interval in which the arrival rate is defined).
  prefs: []
  type: TYPE_NORMAL
- en: 'We are interested in the MLE estimation of λ given a set of datasets that are
    supposed to be following a Poisson distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d678950-6278-43a9-b0b0-952420fcad0c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/014a90e3-7e2c-4b2b-a30c-b7b3cf5da8e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Maths of the MLE calculation for a Poisson distribution
  prefs: []
  type: TYPE_NORMAL
- en: Note how taking the log eases the calculation algebraically. It introduces some
    numerical challenges though—for example, making sure that the likelihood is never
    0, as log cannot be defined as 0\. Numerical methods also result in an invalid
    value if the log-likelihood is infinitely small.
  prefs: []
  type: TYPE_NORMAL
- en: The MLE finds that the estimate of the arrival rate is equal to the mean of
    the dataset—that is, the number of observed arrivals in the given time interval
    in the past. The preceding calculation can be done using NumPy and other supporting
    packages in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several steps that we need to take to perform this calculation in
    Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a function to calculate the Poisson probability for each point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a function to calculate the log-likelihood over the data given a value
    for the arrival rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a function to calculate the derivative of the log-likelihood for the
    arrival rate λ:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate test data with 100 data points—a random number of arrivals/unit time
    between 3 and 12:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the log-likelihood for different values of arrival rates (1 to 9)
    and plot them to find the arrival rate that maximizes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'From this, we get the following plot, which shows that the maximum value of
    the log-likelihood on the test data is obtained when the arrival rate is 6/unit
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4734d99-d1b2-4af6-9f62-ce306d04020b.png)'
  prefs: []
  type: TYPE_IMG
- en: Log-likelihood values at different values of lambda (that is, arrival rate)
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the Newton–Raphson method to find the global maximum of the log-likelihood:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '**Note**: The `lbd` parameter in the function definition is the initial value
    to start the search with.'
  prefs: []
  type: TYPE_NORMAL
- en: The Newton–Raphson method is a popular computation method used to find the roots
    of complex equations. It is an iterative process that finds different values of
    independent variables until the dependent variable reaches 0\. More information
    can be found at [http://www.math.ubc.ca/~anstee/math104/newtonmethod.pdf](http://www.math.ubc.ca/~anstee/math104/newtonmethod.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The results are greatly affected by the initial value of the parameter that
    is provided to start the search. The iterative search can go in very different
    directions if the start values are different, so be careful while using it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of MLE can be extended to perform distribution-based regression.
    Suppose that we hypothesize that the arrival rate is a function of one or several
    parameters. Then the lambda would be defined by a function of the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06c870f1-c4fe-4900-afcf-02c8044f7b36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the arrival rate would be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the value of the arrival rate from the previous equation in the log-likelihood
    calculation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the partial derivative of the log-likelihood with regard to *w*[0], *w*[1],
    and *w*[2].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Equate all the partial derivates to 0 and find the optimum values of *w*[0],
    *w*[1], and *w*[2].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the optimum value of the arrival rate based on these parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To use the MLE calculations, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Find population parameters, such as the mean, std dev, arrival rate, and density,
    from the sample parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the distribution-based regression on data where simple linear regression
    wouldn't work, such as the parameter-based arrival rate example discussed previously,
    or logistic regression weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Its usage in fitting the regression model brings it into the same league of
    optimization methods such as OLS, gradient descent, Adam optimization, RMSprop,
    and other methods.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a more in-depth look at other Bayesian statistics topics that we touched
    upon, please take a look at the following references:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Probabilistic Programming and Bayesian Methods for Hackers:* [https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bayesian Data Analysis*, *Third Edition*, *Andrew Gelman*: [http://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954](http://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Bayesian Choice*, *Christian P Robert* (this is more theoretical): [http://www.springer.com/us/book/9780387952314](http://www.springer.com/us/book/9780387952314)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PyMC documentation*: [http://pymc-devs.github.io/pymc/index.html](http://pymc-devs.github.io/pymc/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we undertook a whirlwind tour of one of the hottest trends
    in statistics and data analysis in the past few years—the Bayesian approach to
    statistical inference. We covered a lot of ground here.
  prefs: []
  type: TYPE_NORMAL
- en: We examined what the Bayesian approach to statistics entails and discussed the
    various reasons why the Bayesian view is a compelling one, such as the fact that
    it values facts over belief. We explained the key statistical distributions and
    showed how we can use the various statistical packages to generate and plot them
    in `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: We tackled a rather difficult topic without too much oversimplification and
    demonstrated how we can use the `PyMC` package and Monte Carlo simulation methods
    to showcase the power of Bayesian statistics to formulate models, perform trend
    analysis, and make inferences on a real-world dataset (Facebook user posts). The
    concept of maximum likelihood estimation was also introduced and explained with
    several examples. It is a popular method for estimating distribution parameters
    and fitting a probability distribution to a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how we can solve real-life data case studies
    using pandas.
  prefs: []
  type: TYPE_NORMAL
