<html><head></head><body>
<div epub:type="chapter" id="_idContainer045">
<h1 class="chapter-number" id="_idParaDest-63"><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-64"><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.2.1">Introduction to Apache Spark</span></h1>
<p><span class="koboSpan" id="kobo.3.1">This chapter provides an overview of Apache Spark, explaining its distributed computing capabilities and suitability for processing large-scale time series data. </span><span class="koboSpan" id="kobo.3.2">It explains how Spark addresses the challenges of parallel processing, scalability, and fault tolerance. </span><span class="koboSpan" id="kobo.3.3">This foundational knowledge is essential as it sets the stage for leveraging Spark’s strengths in handling vast temporal datasets, facilitating efficient time series analysis. </span><span class="koboSpan" id="kobo.3.4">Practical knowledge of Spark’s role enhances practitioners’ ability to harness its power for complex computations, making it a valuable resource for scalable, high-performance time </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">series applications.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">We’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Apache Spark and </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">its architecture</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">How Apache </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">Spark works</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Installation of </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">Apache Spark</span></span></li>
</ul>
<h1 id="_idParaDest-65"><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.13.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.14.1">The hands-on focus of this chapter will be to deploy a multi-node Apache Spark cluster to get familiar with important components of a deployment. </span><span class="koboSpan" id="kobo.14.2">The code for this chapter can be found in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.15.1">ch3</span></strong><span class="koboSpan" id="kobo.16.1"> folder of this book’s GitHub repository at this </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">URL: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3</span></span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.20.1">The hands-on section of this chapter will go into further detail. </span><span class="koboSpan" id="kobo.20.2">This requires some skills in building an open source environment. </span><span class="koboSpan" id="kobo.20.3">If you do not intend to build your own Apache Spark environment and your focus is instead on time series and using but not deploying Spark, you can skip the hands-on section of this chapter. </span><span class="koboSpan" id="kobo.20.4">You can use a managed platform such as Databricks, which comes pre-built with Spark, as we will do in </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">future chapters.</span></span></p>
<h1 id="_idParaDest-66"><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.22.1">What is Apache Spark?</span></h1>
<p><span class="koboSpan" id="kobo.23.1">Apache Spark is a distributed computing system that is open source, with a programming interface</span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.24.1"> and clusters for parallel data processing at scale and with fault tolerance. </span><span class="koboSpan" id="kobo.24.2">Started as a project at Berkeley’s AMPLab in 2009, Spark became open source in 2010 as part of the Apache Software Foundation. </span><span class="koboSpan" id="kobo.24.3">The original creators of Spark have since founded the Databricks company, which provides a managed version of Spark on their </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">multi-cloud platform.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">Spark can handle both batch and stream processing, making it a widely usable tool for big data processing. </span><span class="koboSpan" id="kobo.26.2">Bringing significant performance improvement over existing big data systems, Spark uses in-memory computing and optimized query execution for very fast analytic queries on data of any size. </span><span class="koboSpan" id="kobo.26.3">It is built on the concept of </span><strong class="bold"><span class="koboSpan" id="kobo.27.1">Resilient Distributed Datasets</span></strong><span class="koboSpan" id="kobo.28.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.29.1">RDDs</span></strong><span class="koboSpan" id="kobo.30.1">) and DataFrames. </span><span class="koboSpan" id="kobo.30.2">These are collections of data elements distributed across a cluster of </span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.31.1">computers that can be operated on in parallel with fault tolerance. </span><span class="koboSpan" id="kobo.31.2">We will expand further on these concepts in the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">this chapter.</span></span></p>
<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.33.1">Why use Apache Spark?</span></h2>
<p><span class="koboSpan" id="kobo.34.1">There are numerous </span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.35.1">benefits to using Spark, which explains its popularity as a large-scale data processing solution, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.36.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.37.1">.1</span></em><span class="koboSpan" id="kobo.38.1"> based on Google Trends. </span><span class="koboSpan" id="kobo.38.2">We can see here the increasing interest in Apache Spark software in line with the big data topic, while the trend for Hadoop software had been increasing, then decreased when it was overtaken by Apache Spark software in </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">March 2017.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<span class="koboSpan" id="kobo.40.1"><img alt="" src="image/B18568_03_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.41.1">Figure 3.1: Increasing interest in Apache Spark compared to Hadoop and big data</span></p>
<p><span class="koboSpan" id="kobo.42.1">This surge in interest can be explained by some key benefits, </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.44.1">Speed</span></strong><span class="koboSpan" id="kobo.45.1">: Spark runs up to 100 times faster in memory and up to 10 times faster even when running on disk, when compared to non-Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">Hadoop clusters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.47.1">Fault tolerance</span></strong><span class="koboSpan" id="kobo.48.1">: With the use of distributed computing, Spark provides a fault-tolerant mechanism with recovery </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">on failure.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.50.1">Modularity</span></strong><span class="koboSpan" id="kobo.51.1">: Spark includes support for SQL and structured data processing, machine learning, graph processing, and stream data processing. </span><span class="koboSpan" id="kobo.51.2">With libraries for diverse tasks, it can handle a wide range of data </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">processing tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.53.1">Usability</span></strong><span class="koboSpan" id="kobo.54.1">: With APIs in Python, Java, Scala, and R, as well as Spark Connect, Spark is accessible to a wide </span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.55.1">range of developers and </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">data scientists.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.57.1">Compatibility</span></strong><span class="koboSpan" id="kobo.58.1">: Spark can run on different platforms – including Databricks, Hadoop, Apache Mesos, and Kubernetes, standalone, or in the cloud. </span><span class="koboSpan" id="kobo.58.2">It can also access diverse data sources, which will be discussed in the </span><em class="italic"><span class="koboSpan" id="kobo.59.1">Interfaces and </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.60.1">integrations</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.61.1"> section.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.62.1">The growing popularity of Spark, and the numerous benefits explaining it, came over several years of evolution, which we will look </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">at next.</span></span></p>
<h2 id="_idParaDest-68"><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.64.1">Evolutions</span></h2>
<p><span class="koboSpan" id="kobo.65.1">Apache Spark has </span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.66.1">gone through several evolutions over the years, with the following major </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">release versions:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.68.1">1.x</span></strong><span class="koboSpan" id="kobo.69.1">: These were early versions of Spark, starting with RDDs and some distributed data </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">processing capabilities.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.71.1">2.x</span></strong><span class="koboSpan" id="kobo.72.1">: Spark 2.0, in 2016, had significant improvements with the introduction of Spark SQL, structured streaming, and the Dataset API, which is more efficient </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">than RDDs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.74.1">3.x</span></strong><span class="koboSpan" id="kobo.75.1">: From 2020, Spark 3.0 had further improvements, with </span><strong class="bold"><span class="koboSpan" id="kobo.76.1">Adaptive Query Execution</span></strong><span class="koboSpan" id="kobo.77.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.78.1">AQE</span></strong><span class="koboSpan" id="kobo.79.1">), which dynamically </span><a id="_idIndexMarker246"/><span class="koboSpan" id="kobo.80.1">adjusts query plans based on runtime statistics, enhanced performance optimizations, and dynamic partition pruning. </span><span class="koboSpan" id="kobo.80.2">It also included support for newer Python versions as well as additions</span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.81.1"> to the </span><strong class="bold"><span class="koboSpan" id="kobo.82.1">machine learning </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.83.1">library</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.84.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.85.1">MLlib</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.87.1">As of the time of writing, the</span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.88.1"> latest version is 3.5.3. </span><span class="koboSpan" id="kobo.88.2">To understand the direction the project is going in, let’s now zoom in on the highlights of some of the most recent versions, which are </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.90.1">PySpark</span></strong><span class="koboSpan" id="kobo.91.1"> gains user-friendly support for Python-type hints, the pandas API on Spark, and enhanced performance </span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.92.1">thanks </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">to optimizations.</span></span></li>
<li><span class="koboSpan" id="kobo.94.1">Adaptive Query Execution  improvements drive more efficient query execution and </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">resource utilization.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.96.1">Structured Streaming</span></strong><span class="koboSpan" id="kobo.97.1"> enhancements give better stability </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">and performance.</span></span></li>
<li><span class="koboSpan" id="kobo.99.1">Kubernetes supports better integration and resource management capabilities for running Spark on Kubernetes. </span><span class="koboSpan" id="kobo.99.2">This results in greater efficiency and ease </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">of use.</span></span></li>
<li><span class="koboSpan" id="kobo.101.1">API and SQL enhancements bring more efficient data processing and analysis, with new functions and improvements to existing ones. </span><span class="koboSpan" id="kobo.101.2">The key themes here are better usability </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">and performance.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.103.1">As we can see from the preceding, the recent focus is on support for modern infrastructure, performance, and usability. </span><span class="koboSpan" id="kobo.103.2">As a tool for large-scale data processing and analysis, this is turning Spark into an even more widely </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">adopted tool.</span></span></p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.105.1">Distributions of Spark</span></h2>
<p><span class="koboSpan" id="kobo.106.1">With its popularity and wide adoption have come several distributions of Spark. </span><span class="koboSpan" id="kobo.106.2">These have been developed by different organizations, with Apache Spark, at its core, providing different integration</span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.107.1"> capabilities, usability features, and enhancements to functionalities. </span><span class="koboSpan" id="kobo.107.2">Bundled with other big data tools, these distributions often offer improved management interfaces, enhanced security, and different </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">storage integrations.</span></span></p>
<p><span class="koboSpan" id="kobo.109.1">The following distributions are the most </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">common ones:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.111.1">Apache Spark</span></strong><span class="koboSpan" id="kobo.112.1"> is the original open source version maintained by the Apache Software Foundation. </span><span class="koboSpan" id="kobo.112.2">It is the basis for the </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">other distributions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.114.1">Databricks Runtime</span></strong><span class="koboSpan" id="kobo.115.1"> is developed by </span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.116.1">Databricks, the company founded by the creators of </span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.117.1">Spark. </span><span class="koboSpan" id="kobo.117.2">It is optimized for cloud environments, with a unified analytics platform facilitating collaboration between data engineers, data scientists, and business analysts. </span><span class="koboSpan" id="kobo.117.3">Databricks provides optimized Spark performance with a C++ rewritten version called </span><strong class="bold"><span class="koboSpan" id="kobo.118.1">Photon</span></strong><span class="koboSpan" id="kobo.119.1">, interactive</span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.120.1"> notebooks, integrated workflows for data</span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.121.1"> engineering with </span><strong class="bold"><span class="koboSpan" id="kobo.122.1">Delta Live Tables</span></strong><span class="koboSpan" id="kobo.123.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.124.1">DLT</span></strong><span class="koboSpan" id="kobo.125.1">), and machine learning with MLflow, along with enterprise-grade compliance and security as part of its Unity Catalog-based </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">governance capabilities.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.127.1">Cloudera Data Platform</span></strong><span class="koboSpan" id="kobo.128.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.129.1">CDP</span></strong><span class="koboSpan" id="kobo.130.1">) includes Spark </span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.131.1">as part of its data platform, which includes Hadoop and other big </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">data tools.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.133.1">Hortonworks Data Platform</span></strong><span class="koboSpan" id="kobo.134.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.135.1">HDP</span></strong><span class="koboSpan" id="kobo.136.1">), before merging with Cloudera, offered its own distribution that </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">included </span></span><span class="No-Break"><a id="_idIndexMarker256"/></span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">Spark.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.139.1">Microsoft Azure</span></strong><span class="koboSpan" id="kobo.140.1"> includes Spark </span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.141.1">as part of </span><strong class="bold"><span class="koboSpan" id="kobo.142.1">Azure Databricks</span></strong><span class="koboSpan" id="kobo.143.1">, which is a</span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.144.1"> first-party service on Azure, HDInsight, Synapse, and, moving </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">forward, Fabric.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.146.1">Amazon Web Services</span></strong><span class="koboSpan" id="kobo.147.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.148.1">AWS</span></strong><span class="koboSpan" id="kobo.149.1">) offers </span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.150.1">Databricks in its Marketplace, as well as </span><strong class="bold"><span class="koboSpan" id="kobo.151.1">Elastic MapReduce</span></strong><span class="koboSpan" id="kobo.152.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.153.1">EMR</span></strong><span class="koboSpan" id="kobo.154.1">) running as a cloud service to run big data</span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.155.1"> frameworks such as Apache Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">on AWS.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.157.1">Google Cloud Platform</span></strong><span class="koboSpan" id="kobo.158.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.159.1">GCP</span></strong><span class="koboSpan" id="kobo.160.1">) hosts Databricks, as well as </span><strong class="bold"><span class="koboSpan" id="kobo.161.1">Dataproc</span></strong><span class="koboSpan" id="kobo.162.1">, which is Google’s managed service for </span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.163.1">Apache Spark and Hadoop </span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.164.1">clusters in </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">the cloud.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.166.1">From on-premises to cloud-native solutions to those that integrate with other data platforms, each distribution of Apache Spark answers different needs. </span><span class="koboSpan" id="kobo.166.2">When organizations choose a distribution, factors typically considered are performance requirements, ease of</span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.167.1"> management, the existing technology stack, and specific capabilities provided by </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">each distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.169.1">Now that we have gone through what Apache Spark is, its benefits, and its evolutions, let’s dive deeper into its architecture </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">and components.</span></span></p>
<h1 id="_idParaDest-70"><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.171.1">Apache Spark architecture</span></h1>
<p><span class="koboSpan" id="kobo.172.1">The primary objective of an architecture with Apache Spark is to process large datasets across distributed clusters. </span><span class="koboSpan" id="kobo.172.2">Architectures </span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.173.1">can vary based on the specific requirements of the application, whether it is batch processing, stream processing, machine learning, querying for reports, or even a combination of these. </span><span class="koboSpan" id="kobo.173.2">A typical Spark architecture includes</span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.174.1"> several key components that contribute to the data processing requirements. </span><span class="koboSpan" id="kobo.174.2">An example of such architecture is represented in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.175.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.176.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<span class="koboSpan" id="kobo.178.1"><img alt="" src="image/B18568_03_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.179.1">Figure 3.2: Example of Apache Spark-based architecture (standalone mode)</span></p>
<p><span class="koboSpan" id="kobo.180.1">Let’s now drill down into what each of these </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">parts does.</span></span></p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.182.1">Cluster manager</span></h2>
<p><span class="koboSpan" id="kobo.183.1">Cluster managers are responsible for allocating resources to the clusters, which are the operating system environments on </span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.184.1">which the Spark workloads</span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.185.1"> execute. </span><span class="koboSpan" id="kobo.185.2">These include </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.187.1">Standalone</span></strong><span class="koboSpan" id="kobo.188.1">: A basic </span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.189.1">cluster manager is included with Spark, making it easy to set up a cluster to get started. </span><span class="koboSpan" id="kobo.189.2">This </span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.190.1">cluster manager node is also known as the </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">master node:</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.192.1">Kubernetes</span></strong><span class="koboSpan" id="kobo.193.1">: Spark can</span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.194.1"> be deployed to Kubernetes, which is an open source container-based system that automates the deployment, management, and scaling of </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">containerized applications.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.196.1">Apache Mesos</span></strong><span class="koboSpan" id="kobo.197.1">: As a cluster</span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.198.1"> manager, Mesos supports Spark, in addition to running </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">Hadoop MapReduce.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.200.1">Hadoop YARN</span></strong><span class="koboSpan" id="kobo.201.1">: Spark can </span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.202.1">share clusters and datasets with other Hadoop components when running </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">with YARN.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.204.1">Proprietary and commercial</span></strong><span class="koboSpan" id="kobo.205.1">: The solutions that incorporate Spark have their own cluster </span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.206.1">managers – usually a variation and improvement of the preceding open </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">source versions.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.208.1">Next, we will look at what is within these </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">Spark clusters.</span></span></p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.210.1">Spark Core, libraries, and API</span></h2>
<p><span class="koboSpan" id="kobo.211.1">Once we have one or more clusters provided by the cluster manager, Spark Core then manages memory and fault </span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.212.1">recovery, as well as everything related to Spark jobs, such as scheduling, distributing, and monitoring. </span><span class="koboSpan" id="kobo.212.2">Spark Core abstracts storage read and write, using RDDs and, more recently, DataFrames as the </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">data structure.</span></span></p>
<p><span class="koboSpan" id="kobo.214.1">On top of (and working very closely with) Core, several</span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.215.1"> libraries and APIs provide additional functionalities specific to the data processing requirements. </span><span class="koboSpan" id="kobo.215.2">These are </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.217.1">Spark SQL</span></strong><span class="koboSpan" id="kobo.218.1"> allows querying structured </span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.219.1">data </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">via SQL</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.221.1">Spark Structured Streaming</span></strong><span class="koboSpan" id="kobo.222.1"> processes</span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.223.1"> data streaming from various sources, such as Kafka </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">and Kinesis</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.225.1">MLlib</span></strong><span class="koboSpan" id="kobo.226.1"> provides multiple types of machine learning algorithms for classification, regression, and clustering, </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">among </span></span><span class="No-Break"><a id="_idIndexMarker278"/></span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">others</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.229.1">GraphX</span></strong><span class="koboSpan" id="kobo.230.1"> allows the </span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.231.1">use of graph algorithms for the creation, transformation, and </span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.232.1">querying </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">of graphs</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.234.1">Spark is about data processing, and as such, an important part of the solution is the data structure, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">discuss next.</span></span></p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.236.1">RDDs, DataFrames, and Datasets</span></h2>
<p><span class="koboSpan" id="kobo.237.1">We have mentioned RDDs and DataFrames</span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.238.1"> a few times since the start of the chapter without going into detail, which we will do now, as well as </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">introducing</span></span><span class="No-Break"><a id="_idIndexMarker282"/></span><span class="No-Break"><span class="koboSpan" id="kobo.240.1"> Datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.241.1">In short, these are the in-memory </span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.242.1">data structures representing the data and providing us with a programmatic way, more formerly termed an abstraction, to manipulate the data. </span><span class="koboSpan" id="kobo.242.2">Each of these data structures has its use cases, </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.244.1">An </span><strong class="bold"><span class="koboSpan" id="kobo.245.1">RDD</span></strong><span class="koboSpan" id="kobo.246.1"> is Spark’s fundamental </span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.247.1">data structure. </span><span class="koboSpan" id="kobo.247.2">Immutable and distributed, it can store data in memory across a cluster. </span><span class="koboSpan" id="kobo.247.3">Fault-tolerant, an RDD can automatically recover from failures. </span><span class="koboSpan" id="kobo.247.4">Note that in case of insufficient memory on the cluster, Spark does store part of the RDD on disk, but as this is managed behind the scenes, we will keep referring to RDDs as being </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">in memory.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.249.1">You are less and less likely to use RDDs, as more operations become possible with easier-to-use DataFrames, which we will see next. </span><span class="koboSpan" id="kobo.249.2">RDDs are more suitable for low-level transformations with direct manipulation of data, useful when you need low-level control </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">over computations.</span></span></p></li>
<li><span class="koboSpan" id="kobo.251.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.252.1">DataFrame</span></strong><span class="koboSpan" id="kobo.253.1"> is built upon an </span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.254.1">RDD as a distributed collection of data with named columns. </span><span class="koboSpan" id="kobo.254.2">This is like a table in a relational database. </span><span class="koboSpan" id="kobo.254.3">In addition to the more user-friendly higher-level API, which makes code more concise and easier to understand, DataFrames benefit from performance gain over RDDs thanks to Spark’s Catalyst optimizer, which we will discuss later in </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">the chapter.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.256.1">We have already started using DataFrames as part of the hands-on exercises done so far. </span><span class="koboSpan" id="kobo.256.2">You may have noticed pandas DataFrames in addition to Spark DataFrames while doing the exercises. </span><span class="koboSpan" id="kobo.256.3">While similar in concept, they are part of different libraries and have their </span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.257.1">underlying implementation differences. </span><span class="koboSpan" id="kobo.257.2">Fundamentally, pandas DataFrames are on single machines while Spark DataFrames are distributed. </span><span class="koboSpan" id="kobo.257.3">pandas DataFrames can be converted to pandas-on-Spark DataFrames, with the benefit of pandas DataFrame API support in addition </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">to parallelism.</span></span></p></li>
<li><span class="koboSpan" id="kobo.259.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.260.1">Dataset</span></strong><span class="koboSpan" id="kobo.261.1"> provides the type safety of </span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.262.1">RDDs with the optimizations of DataFrame. </span><span class="koboSpan" id="kobo.262.2">Type safety means that you can catch data type errors at compilation time, resulting in more runtime reliability. </span><span class="koboSpan" id="kobo.262.3">This is, however, dependent on the programming language supporting data type definition at the time of coding and verification and enforcement during compilation. </span><span class="koboSpan" id="kobo.262.4">As such, Datasets are only supported in Scala and Java, with Python and R, being dynamically typed, </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">using DataFrames.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.264.1">In summary, you will get low-level control with RDDs, optimized higher-level abstraction with DataFrames, and type safety with Datasets. </span><span class="koboSpan" id="kobo.264.2">Which data structure to use depends on the specific requirements of </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">your application.</span></span></p>
<p><span class="koboSpan" id="kobo.266.1">So far, we have considered the internal components. </span><span class="koboSpan" id="kobo.266.2">We will next go into the external facing parts and how Spark integrates in the backend with storage and in the frontend with applications </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">and users.</span></span></p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.268.1">Interfaces and integrations</span></h2>
<p><span class="koboSpan" id="kobo.269.1">When considering interfacing </span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.270.1">and integrating with the </span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.271.1">environment, there are a few ways in which this is fulfilled with Apache Spark. </span><span class="koboSpan" id="kobo.271.2">These are </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.273.1">Storage</span></strong><span class="koboSpan" id="kobo.274.1">: The first one (and one that is key) is integration with storage to read the source data to be processed and write back the results. </span><span class="koboSpan" id="kobo.274.2">Apache Spark supports several native and third-party </span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.275.1">connectors, including to local file systems, </span><strong class="bold"><span class="koboSpan" id="kobo.276.1">Hadoop Distributed File System</span></strong><span class="koboSpan" id="kobo.277.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.278.1">HDFS</span></strong><span class="koboSpan" id="kobo.279.1">), and cloud storage, among many others. </span><span class="koboSpan" id="kobo.279.2">The data itself can be read and stored in different file formats such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">csv</span></strong><span class="koboSpan" id="kobo.281.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.282.1">json</span></strong><span class="koboSpan" id="kobo.283.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.284.1">xml</span></strong><span class="koboSpan" id="kobo.285.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.286.1">orc</span></strong><span class="koboSpan" id="kobo.287.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">avro</span></strong><span class="koboSpan" id="kobo.289.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.290.1">parquet</span></strong><span class="koboSpan" id="kobo.291.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">protobuf</span></strong><span class="koboSpan" id="kobo.293.1">. </span><span class="koboSpan" id="kobo.293.2">Of these, Parquet is the most common as it gives good performance with snappy compression. </span><span class="koboSpan" id="kobo.293.3">In addition, Spark</span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.294.1"> can be extended with packages to support several storage</span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.295.1"> protocols and external data sources. </span><span class="koboSpan" id="kobo.295.2">Delta is one of these, which we will discuss further in </span><a href="B18568_04.xhtml#_idTextAnchor087"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.296.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.297.1"> and </span><a href="B18568_05.xhtml#_idTextAnchor103"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.298.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.299.1">. </span><span class="koboSpan" id="kobo.299.2">Other formats include Iceberg and Hudi. </span><span class="koboSpan" id="kobo.299.3">Note that we are talking here about the disk representation of the data, which is loaded into the memory-based data structures in RDDs and DataFrames </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">discussed previously.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.301.1">We already have some experience with Spark and storage as part of the hands-on exercises done so far, where we have been reading CSV files from the local storage on the Databricks Community Edition’s </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">Spark clusters.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.303.1">Applications</span></strong><span class="koboSpan" id="kobo.304.1">: This is the code with the logic for data processing, calling the Spark APIs and libraries for tasks such as data transformations, streaming, SQL queries, or machine learning. </span><span class="koboSpan" id="kobo.304.2">Developers can write in Python, R, Scala, or Java. </span><span class="koboSpan" id="kobo.304.3">The code is then executed on the </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">Spark clusters.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.306.1">Our experience with the application side has started as well, with the hands-on code used </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">so far.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.308.1">Platform user interface</span></strong><span class="koboSpan" id="kobo.309.1">: In addition to the web interface for Databricks Community Edition, which we have seen in the hands-on exercises, open source Apache Spark has a web </span><strong class="bold"><span class="koboSpan" id="kobo.310.1">user interface</span></strong><span class="koboSpan" id="kobo.311.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.312.1">UI</span></strong><span class="koboSpan" id="kobo.313.1">) for monitoring the cluster and Spark applications. </span><span class="koboSpan" id="kobo.313.2">This provides</span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.314.1"> insights into stages of job execution, resource usage, and the execution environment. </span><span class="koboSpan" id="kobo.314.2">Other data platforms that incorporate Apache Spark have their </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">own UIs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.316.1">Application end user interface</span></strong><span class="koboSpan" id="kobo.317.1">: Another type of UI is for end users consuming the outcome of the processing by Apache Spark. </span><span class="koboSpan" id="kobo.317.2">This can be reporting tools or, for example, an application using Apache Spark in the backend for </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">data processing.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.319.1">In this section on Apache Spark architecture, we saw how the architecture enables data to be ingested from various sources into the Spark system, to be processed using Spark’s libraries, and then stored or served to users or downstream applications. </span><span class="koboSpan" id="kobo.319.2">The chosen architecture is dependent on requirements, such as latency, throughput, data size, and the complexity and type of data processing tasks. </span><span class="koboSpan" id="kobo.319.3">In the next section, we will focus on how Spark performs distributed processing </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">at scale.</span></span></p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.321.1">How Apache Spark works</span></h1>
<p><span class="koboSpan" id="kobo.322.1">So far in this chapter, we have viewed the components and their roles, but not so much about their interactions. </span><span class="koboSpan" id="kobo.322.2">We </span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.323.1">will now cover this part, to understand how Spark manages distributed data processing across a cluster, starting with transformations </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">and actions.</span></span></p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.325.1">Transformations and actions</span></h2>
<p><span class="koboSpan" id="kobo.326.1">Apache Spark does, at a high level, two types of </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">data operations:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.328.1">Transformations</span></strong><span class="koboSpan" id="kobo.329.1"> are operations </span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.330.1">on RDDs, DataFrames, or Datasets, returning another RDD, DataFrame, or Dataset. </span><span class="koboSpan" id="kobo.330.2">The original data structure is not altered, that is, it is immutable. </span><span class="koboSpan" id="kobo.330.3">Transformations are not executed immediately and are called lazy operations, and as such, enable Spark to optimize the execution</span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.331.1"> plan. </span><span class="koboSpan" id="kobo.331.2">They are part of a </span><strong class="bold"><span class="koboSpan" id="kobo.332.1">Directed Acyclic Graph</span></strong><span class="koboSpan" id="kobo.333.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.334.1">DAG</span></strong><span class="koboSpan" id="kobo.335.1">) of transformations and get executed when an action, which we</span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.336.1"> will define next, is called. </span><span class="koboSpan" id="kobo.336.2">Examples of transformations are </span><strong class="source-inline"><span class="koboSpan" id="kobo.337.1">filter</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.338.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.339.1">groupBy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.341.1">Actions</span></strong><span class="koboSpan" id="kobo.342.1"> are eager, that is, executed</span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.343.1"> immediately. </span><span class="koboSpan" id="kobo.343.2">Examples of actions are </span><strong class="source-inline"><span class="koboSpan" id="kobo.344.1">count</span></strong><span class="koboSpan" id="kobo.345.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.346.1">save</span></strong><span class="koboSpan" id="kobo.347.1"> types of operations, such as writing to Parquet files or using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.348.1">saveAsTable</span></strong><span class="koboSpan" id="kobo.349.1"> operation. </span><span class="koboSpan" id="kobo.349.2">Actions trigger the execution of all transformations defined as prior steps in the DAG. </span><span class="koboSpan" id="kobo.349.3">This results in Spark computing the result of the series </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">of transformations.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.351.1">The distinction between</span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.352.1"> transformations and actions is an important consideration when writing efficient Spark code. </span><span class="koboSpan" id="kobo.352.2">This enables Spark to use its execution engine for high-performance processing of jobs, which will be </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">explained next.</span></span></p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.354.1">Jobs, stages, and tasks</span></h2>
<p><span class="koboSpan" id="kobo.355.1">Spark applications are executed as jobs, which are split into stages, and further into tasks, </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.357.1">Job</span></strong><span class="koboSpan" id="kobo.358.1">: Spark submits a job when</span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.359.1"> an action is called on an RDD, DataFrame, or Dataset. </span><span class="koboSpan" id="kobo.359.2">The job is converted into a physical execution plan with several stages, which we will explain next. </span><span class="koboSpan" id="kobo.359.3">The purpose </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.360.1">of a Spark job is to execute a sequence of computational steps as a logical unit of work to achieve a specific goal, such </span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.361.1">as aggregating data or sorting, with the aim of producing </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">an output.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.363.1">Stage</span></strong><span class="koboSpan" id="kobo.364.1">: A job can have multiple</span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.365.1"> stages, as defined in its physical execution plan. </span><span class="koboSpan" id="kobo.365.2">A stage</span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.366.1"> is a group of contiguous tasks that can be completed without moving data across the cluster. </span><span class="koboSpan" id="kobo.366.2">The data movement between stages is referred to as shuffle. </span><span class="koboSpan" id="kobo.366.3">The </span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.367.1">separation of a job into stages is beneficial as shuffling is costly in terms of performance impact. </span><span class="koboSpan" id="kobo.367.2">A stage is further broken down into tasks, which we will look </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">at next.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.369.1">Task</span></strong><span class="koboSpan" id="kobo.370.1">: As the most granular unit of </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.371.1">processing, a task is a single operation on a Spark in-memory partition of data. </span><span class="koboSpan" id="kobo.371.2">Each task processes a different set of data and can run in parallel with other tasks. </span><span class="koboSpan" id="kobo.371.3">These run on worker nodes, which we will look </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">at next.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.373.1">In summary, jobs, stages, and tasks are related hierarchically. </span><span class="koboSpan" id="kobo.373.2">Spark applications can have multiple jobs, which are divided into stages based on data shuffling boundaries. </span><span class="koboSpan" id="kobo.373.3">Stages are further broken down into tasks, which run on different partitions in parallel on the cluster. </span><span class="koboSpan" id="kobo.373.4">This execution hierarchy allows Spark to efficiently distribute the workload across several nodes in a cluster, thus efficiently processing data </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">at scale.</span></span></p>
<p><span class="koboSpan" id="kobo.375.1">Now that we have seen the units of processing, the next consideration is how these units are run on compute resources with driver and </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">worker nodes.</span></span></p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.377.1">Driver and worker nodes</span></h2>
<p><span class="koboSpan" id="kobo.378.1">Driver and worker nodes are the compute resources created by the cluster manager to form part of a Spark cluster. </span><span class="koboSpan" id="kobo.378.2">They</span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.379.1"> work together for Spark to process large datasets in parallel, using the resources of multiple machines. </span></p>
<p><span class="koboSpan" id="kobo.380.1">Let’s discuss these resources </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">in detail:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.382.1">Driver nodes</span></strong><span class="koboSpan" id="kobo.383.1">: The driver node is</span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.384.1"> where the main process of a Spark application runs. </span><span class="koboSpan" id="kobo.384.2">It principally does </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">the following:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.386.1">Resources</span></strong><span class="koboSpan" id="kobo.387.1">: The driver requests resources from the cluster manager for processes to run on the </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">worker nodes.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.389.1">SparkSession</span></strong><span class="koboSpan" id="kobo.390.1">: This is an object created by the driver and used to programmatically access Spark for data processing operations on </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">the cluster.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.392.1">Tasks</span></strong><span class="koboSpan" id="kobo.393.1">: The driver translates code into tasks, schedules the tasks on worker nodes, and thereafter manages the </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">tasks’ execution.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.395.1">Worker nodes</span></strong><span class="koboSpan" id="kobo.396.1">: The worker node is where the data processing happens, via what is called the executor process. </span><span class="koboSpan" id="kobo.396.2">The executors </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.397.1">interact with the storage </span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.398.1">and keep the data in their own memory space, as well as having their own set of CPU cores. </span><span class="koboSpan" id="kobo.398.2">The tasks are scheduled by the driver nodes to execute on the executors with direct </span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.399.1">communication between drivers and executors. </span><span class="koboSpan" id="kobo.399.2">They communicate on task status </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">and results.</span></span></li>
</ul>
<p><strong class="bold"><span class="koboSpan" id="kobo.401.1">Driver and worker node interaction</span></strong><span class="koboSpan" id="kobo.402.1">: </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.403.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.404.1">.3</span></em><span class="koboSpan" id="kobo.405.1"> summarizes the sequence of interactions between driver and </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">worker nodes.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<span class="koboSpan" id="kobo.407.1"><img alt="" src="image/B18568_03_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.408.1">Figure 3.3: Driver and worker nodes in action</span></p>
<p><span class="koboSpan" id="kobo.409.1">The steps are</span><a id="_idIndexMarker312"/> <span class="No-Break"><span class="koboSpan" id="kobo.410.1">as follows:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.411.1">Initialization</span></strong><span class="koboSpan" id="kobo.412.1">: When the Spark application is</span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.413.1"> started, the driver converts jobs into stages, further broken </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">into</span></span><span class="No-Break"><a id="_idIndexMarker314"/></span><span class="No-Break"><span class="koboSpan" id="kobo.415.1"> tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.416.1">Scheduling</span></strong><span class="koboSpan" id="kobo.417.1">: The driver node schedules tasks on executors on the worker nodes, keeping track of status and rescheduling in case </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">of failure.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.419.1">Execution</span></strong><span class="koboSpan" id="kobo.420.1">: The tasks assigned by the driver are run by the executor on the worker node. </span><span class="koboSpan" id="kobo.420.2">In addition, the driver coordinates between executors when data needs to be shuffled across executors. </span><span class="koboSpan" id="kobo.420.3">This is required for certain operations such </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">as joins.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.422.1">Result</span></strong><span class="koboSpan" id="kobo.423.1">: Finally, the</span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.424.1"> results of processing tasks by the executors are sent back to the driver node, which aggregates the results and sends them back </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.425.1">to </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">the user.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.427.1">This cooperative process</span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.428.1"> between the driver and worker nodes is at the core of Spark, enabling data processing at scale, in parallel across a cluster, while handling </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">fault tolerance.</span></span></p>
<p><span class="koboSpan" id="kobo.430.1">Now that we have seen the workings of Spark clusters, let’s zoom in on what makes it even more performant </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">and efficient.</span></span></p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.432.1">Catalyst optimizer and the Tungsten execution engine</span></h2>
<p><span class="koboSpan" id="kobo.433.1">So far we’ve discussed that among the successive improvements brought to Apache Spark over the different versions, two notable ones are the Catalyst optimizer and the Tungsten execution engine. </span><span class="koboSpan" id="kobo.433.2">They play crucial roles in ensuring the Spark processes are optimized for fast execution time and efficient use </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">of resources.</span></span></p>
<h3><span class="koboSpan" id="kobo.435.1">Catalyst optimizer</span></h3>
<p><span class="koboSpan" id="kobo.436.1">Introduced in Spark SQL, the</span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.437.1"> Catalyst optimizer is a query optimization framework that significantly improves the performance of queries by using tree transformation on the </span><strong class="bold"><span class="koboSpan" id="kobo.438.1">abstract syntax tree</span></strong><span class="koboSpan" id="kobo.439.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.440.1">AST</span></strong><span class="koboSpan" id="kobo.441.1">) of queries. </span><span class="koboSpan" id="kobo.441.2">It does this through several </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.442.1">stages, </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">as follows:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.444.1">Analysis</span></strong><span class="koboSpan" id="kobo.445.1">: The query is transformed into a tree of operators called a </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">logical plan.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.447.1">Logical optimization</span></strong><span class="koboSpan" id="kobo.448.1">: The optimizer uses rule-based transformations to optimize the </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">logical plan.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.450.1">Physical planning</span></strong><span class="koboSpan" id="kobo.451.1">: The logical plan is converted to physical plans, which are based on the choice of algorithm to use for the </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">query operation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.453.1">Cost model</span></strong><span class="koboSpan" id="kobo.454.1">: The physical plans are then compared based on a cost model to find the most efficient one in terms of time </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">and resources.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.456.1">Code generation</span></strong><span class="koboSpan" id="kobo.457.1">: As a final stage, the physical plan is converted to </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">executable code.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.459.1">With these stages, the</span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.460.1"> Catalyst optimizer ensures that the most performant and efficient code </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">is run.</span></span></p>
<h3><span class="koboSpan" id="kobo.462.1">Tungsten execution engine</span></h3>
<p><span class="koboSpan" id="kobo.463.1">Another area of focus is the efficient use of CPU and memory by Spark processes. </span><span class="koboSpan" id="kobo.463.2">The Tungsten execution engine achieves this in the </span><a id="_idIndexMarker321"/><span class="No-Break"><span class="koboSpan" id="kobo.464.1">following ways:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.465.1">Code generation</span></strong><span class="koboSpan" id="kobo.466.1">: Tungsten works in conjunction with the Catalyst optimizer to generate optimized, compact code, which reduces runtime overhead while </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">maximizing speed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.468.1">Cache-awareness</span></strong><span class="koboSpan" id="kobo.469.1">: Reducing cache misses improves the computation speed. </span><span class="koboSpan" id="kobo.469.2">Tungsten achieves this by making algorithms and data </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">structures cache-aware.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.471.1">Memory management</span></strong><span class="koboSpan" id="kobo.472.1">: Tungsten manages memory efficiently, improving the impact of the cache while reducing the overhead of </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">garbage collection.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.474.1">Working together, the Catalyst optimizer and the Tungsten execution engine significantly contribute to Spark’s performance by optimizing query plans, generating efficient code, and reducing computation overhead. </span><span class="koboSpan" id="kobo.474.2">This improves Spark’s efficiency for big data processing, at scale </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">and fast.</span></span></p>
<p><span class="koboSpan" id="kobo.476.1">Now that we understand how Apache Spark works, we will move on to how to set up our own </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">Spark environment.</span></span></p>
<h1 id="_idParaDest-80"><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.478.1">Installing Apache Spark</span></h1>
<p><span class="koboSpan" id="kobo.479.1">So far, in the previous chapters, we</span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.480.1"> have successfully executed Spark code on Databricks Community </span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.481.1">Edition. </span><span class="koboSpan" id="kobo.481.2">This has, however, been on a single-node cluster. </span><span class="koboSpan" id="kobo.481.3">If we want to make full use of Spark’s parallel processing power, we will need multiple nodes. </span><span class="koboSpan" id="kobo.481.4">We have the option of using a Databricks-managed </span><strong class="bold"><span class="koboSpan" id="kobo.482.1">Platform as a Service</span></strong><span class="koboSpan" id="kobo.483.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.484.1">PaaS</span></strong><span class="koboSpan" id="kobo.485.1">) cloud solution, another equivalent cloud PaaS, or we </span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.486.1">can build our own Apache Spark platform. </span><span class="koboSpan" id="kobo.486.2">This is what we will do now to deploy the </span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.487.1">environment as per </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.488.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.489.1">.2</span></em><span class="koboSpan" id="kobo.490.1"> shown in the section on </span><em class="italic"><span class="koboSpan" id="kobo.491.1">Apache </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.492.1">Spark architecture</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.494.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.495.1">If you do not intend to build your own Apache Spark environment, you can skip the practical part of this section</span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.496.1"> and use a managed Spark platform such as Databricks, as we will do in </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">future chapters.</span></span></p>
<h2 id="_idParaDest-81"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.498.1">Using a container for deployment</span></h2>
<p><span class="koboSpan" id="kobo.499.1">We can install Apache </span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.500.1">Spark directly on our local machine, but this will give us only one node. </span><span class="koboSpan" id="kobo.500.2">By deploying it in containers, such as Docker, we can have multiple containers running on the same machine. </span><span class="koboSpan" id="kobo.500.3">This effectively provides us with a way to have a multi-node cluster. </span><span class="koboSpan" id="kobo.500.4">Other advantages of this method include maintaining separation with the local execution environment, as well as providing a portable and </span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.501.1">repeatable way to deploy to other </span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.502.1">machines, including to cloud-based container services </span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.503.1">such as Amazon </span><strong class="bold"><span class="koboSpan" id="kobo.504.1">Elastic Kubernetes Service</span></strong><span class="koboSpan" id="kobo.505.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.506.1">EKS</span></strong><span class="koboSpan" id="kobo.507.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.508.1">Azure Kubernetes Service</span></strong><span class="koboSpan" id="kobo.509.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.510.1">AKS</span></strong><span class="koboSpan" id="kobo.511.1">), or </span><strong class="bold"><span class="koboSpan" id="kobo.512.1">Google Kubernetes </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.513.1">Engine</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.514.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.515.1">GKE</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.517.1">In what follows, we will be </span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.518.1">using Docker containers, starting by first installing Docker, then building and starting the containers with Apache Spark, and finally validating </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">our deployment.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.520.1">Alternative to Docker</span></p>
<p class="callout"><span class="koboSpan" id="kobo.521.1">You can use Podman as an open source </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.522.1">alternative to Docker. </span><span class="koboSpan" id="kobo.522.2">See more information </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">here: </span></span><a href="https://podman.io/"><span class="No-Break"><span class="koboSpan" id="kobo.524.1">https://podman.io/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.525.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.526.1">Docker</span></h3>
<p><span class="koboSpan" id="kobo.527.1">The following instructions guide</span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.528.1"> you on how to</span><a id="_idIndexMarker334"/> <span class="No-Break"><span class="koboSpan" id="kobo.529.1">install Docker:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.530.1">Refer to the following link to download and install Docker to your local environment, based on </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">your OS:</span></span><p class="list-inset"><a href="https://docs.docker.com/get-docker/"><span class="No-Break"><span class="koboSpan" id="kobo.532.1">https://docs.docker.com/get-docker/</span></span></a></p><p class="list-inset"><span class="koboSpan" id="kobo.533.1">For macOS users, follow the </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">instructions here:</span></span></p><p class="list-inset"><a href="https://docs.docker.com/desktop/install/mac-install/"><span class="No-Break"><span class="koboSpan" id="kobo.535.1">https://docs.docker.com/desktop/install/mac-install/</span></span></a></p></li>
<li><span class="koboSpan" id="kobo.536.1">Once Docker is</span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.537.1"> installed, launch</span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.538.1"> it as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.539.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.540.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer041">
<span class="koboSpan" id="kobo.542.1"><img alt="" src="image/B18568_03_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.543.1">Figure 3.4: Docker Desktop</span></p>
<p><span class="koboSpan" id="kobo.544.1">On macOS, you may see a Docker Desktop warning: “</span><strong class="bold"><span class="koboSpan" id="kobo.545.1">Another application changed your Desktop configurations</span></strong><span class="koboSpan" id="kobo.546.1">”. </span><span class="koboSpan" id="kobo.546.2">Depending on your setup, the following command may resolve </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">the warning:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.548.1">
ln -sf /Applications/Docker.app/Contents/Resources/bin/docker-credential-ecr-login /usr/local/bin/docker-credential-ecr-login</span></pre> <p><span class="koboSpan" id="kobo.549.1">Once Docker Desktop is up and</span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.550.1"> running, we can build </span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.551.1">the containers with </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">Apache Spark.</span></span></p>
<h3><span class="koboSpan" id="kobo.553.1">Network ports</span></h3>
<p><span class="koboSpan" id="kobo.554.1">The following network ports</span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.555.1"> need to be available on your local machine or </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">development environment:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.557.1">Apache Spark: </span><strong class="source-inline"><span class="koboSpan" id="kobo.558.1">7077</span></strong><span class="koboSpan" id="kobo.559.1">, </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.560.1">8080</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.562.1">8081</span></strong></span></li>
<li><span class="koboSpan" id="kobo.563.1">Jupyter Notebook: </span><strong class="source-inline"><span class="koboSpan" id="kobo.564.1">4040</span></strong><span class="koboSpan" id="kobo.565.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">4041</span></strong><span class="koboSpan" id="kobo.567.1">, </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.568.1">4042</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.570.1">8888</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.571.1">You can check for </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.572.1">the current use of these ports by existing applications with the following command, run from the command line </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">or terminal:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.574.1">
% netstat -an | grep LISTEN</span></pre> <p><span class="koboSpan" id="kobo.575.1">If you see the required ports in the list of ports already in use, you must either stop the application using that port or change the docker-compose file to use </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">another port.</span></span></p>
<p><span class="koboSpan" id="kobo.577.1">As an example, let’s assume that the output of the above </span><strong class="source-inline"><span class="koboSpan" id="kobo.578.1">netstat</span></strong><span class="koboSpan" id="kobo.579.1"> command reveals that port </span><strong class="source-inline"><span class="koboSpan" id="kobo.580.1">8080</span></strong><span class="koboSpan" id="kobo.581.1"> is already in use on your local machine or development environment, and you are not able to stop the existing application using </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">this port.</span></span></p>
<p><span class="koboSpan" id="kobo.583.1">In this case, you will need to change port </span><strong class="source-inline"><span class="koboSpan" id="kobo.584.1">8080</span></strong><span class="koboSpan" id="kobo.585.1"> (meant for Apache Spark) in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.586.1">docker-compose.yaml</span></strong><span class="koboSpan" id="kobo.587.1"> file to another, unused port. </span><span class="koboSpan" id="kobo.587.2">Just search and replace </span><strong class="source-inline"><span class="koboSpan" id="kobo.588.1">8080</span></strong><span class="koboSpan" id="kobo.589.1"> on the left of </span><strong class="source-inline"><span class="koboSpan" id="kobo.590.1">:</span></strong><span class="koboSpan" id="kobo.591.1"> to, say, </span><strong class="source-inline"><span class="koboSpan" id="kobo.592.1">8070</span></strong><span class="koboSpan" id="kobo.593.1"> if this port is free, as per the </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">following example:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.595.1">From:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.596.1">
     ports:
      - '7077:7077'
      - '</span><strong class="bold"><span class="koboSpan" id="kobo.597.1">8080</span></strong><span class="koboSpan" id="kobo.598.1">:8080'</span></pre></li> <li><span class="No-Break"><span class="koboSpan" id="kobo.599.1">To:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.600.1">
     ports:
      - '7077:7077'
      - '</span><strong class="bold"><span class="koboSpan" id="kobo.601.1">8070</span></strong><span class="koboSpan" id="kobo.602.1">:8080'</span></pre></li> </ul>
<p><span class="koboSpan" id="kobo.603.1">Keep note of the new port and use this instead of the existing one whenever you need to type the corresponding URL. </span><span class="koboSpan" id="kobo.603.2">In this example, port </span><strong class="source-inline"><span class="koboSpan" id="kobo.604.1">8080</span></strong><span class="koboSpan" id="kobo.605.1"> is changed to </span><strong class="source-inline"><span class="koboSpan" id="kobo.606.1">8070</span></strong><span class="koboSpan" id="kobo.607.1">, and the matching URL change</span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.608.1"> for the Airflow web server</span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.609.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">as follows:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.611.1">From: </span></span><a href="http://localhost:8080/"><span class="No-Break"><span class="koboSpan" id="kobo.612.1">http://localhost:8080/</span></span></a></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.613.1">To: </span></span><a href="http://localhost:8070/"><span class="No-Break"><span class="koboSpan" id="kobo.614.1">http://localhost:8070/</span></span></a></li>
</ul>
<p class="callout-heading"><span class="koboSpan" id="kobo.615.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.616.1">You will need to change the network port in all URLs in the following sections that you had to modify as per </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">this section.</span></span></p>
<h3><span class="koboSpan" id="kobo.618.1">Building and deploying Apache Spark</span></h3>
<p><span class="koboSpan" id="kobo.619.1">The following instructions guide you</span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.620.1"> on how to build and deploy the </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">Docker images:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.622.1">We first download the </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.623.1">deployment script from the Git repository for this chapter, which is at the </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">following URL:</span></span><p class="list-inset"><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3"><span class="No-Break"><span class="koboSpan" id="kobo.625.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch3</span></span></a></p><p class="list-inset"><span class="koboSpan" id="kobo.626.1">We will be using the git clone-friendly URL, which is </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">the following:</span></span></p><p class="list-inset"><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git"><span class="No-Break"><span class="koboSpan" id="kobo.628.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git</span></span></a></p><p class="list-inset"><span class="koboSpan" id="kobo.629.1">To do this, start a terminal or command line and run the </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">following commands:</span></span></p><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.631.1">git clone https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.632.1">cd Time-Series-Analysis-with-Spark/ch3</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.633.1">Note that the preceding is for a macOS or Linux/Unix-based system, and you will need to run the equivalent </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">for Windows.</span></span></p></li> <li><span class="koboSpan" id="kobo.635.1">On macOS, you may see the following error when you run </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">this command:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.637.1">xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.638.1">In this case, you will need to reinstall the command-line tools with the </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">following command:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.640.1">xcode-select --install</span></strong></pre></li> <li><span class="koboSpan" id="kobo.641.1">We can now start the </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.642.1">container build and startup. </span><span class="koboSpan" id="kobo.642.2">A makefile is provided to simplify the </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.643.1">process of starting and stopping the containers. </span><span class="koboSpan" id="kobo.643.2">The following command builds the Docker images for the containers and then </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">starts them:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.645.1">make up</span></strong></pre></li> </ol>
<p class="callout-heading"><span class="koboSpan" id="kobo.646.1">Windows environment</span></p>
<p class="callout"><span class="koboSpan" id="kobo.647.1">If you are using a Windows environment, you can install a Windows version of Make, as per the following </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">documentation: </span></span><a href="https://gnuwin32.sourceforge.net/packages/make.htm"><span class="No-Break"><span class="koboSpan" id="kobo.649.1">https://gnuwin32.sourceforge.net/packages/make.htm</span></span></a></p>
<p class="list-inset"><span class="koboSpan" id="kobo.650.1">This will give the following or </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">equivalent output:</span></span></p>
<pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.652.1">docker-compose</span></strong><span class="koboSpan" id="kobo.653.1"> up -d
[+] Running 4/4
...
 </span><span class="koboSpan" id="kobo.653.2">✔ Container ts-spark-env-</span><strong class="bold"><span class="koboSpan" id="kobo.654.1">spark-master</span></strong><span class="koboSpan" id="kobo.655.1">-1    Started
 ✔ Container ts-spark-env-</span><strong class="bold"><span class="koboSpan" id="kobo.656.1">jupyter</span></strong><span class="koboSpan" id="kobo.657.1">-1         Started
 ✔ Container ts-spark-env-</span><strong class="bold"><span class="koboSpan" id="kobo.658.1">spark-worker</span></strong><span class="koboSpan" id="kobo.659.1">-1-1  Started
 ✔ Container ts-spark-env-</span><strong class="bold"><span class="koboSpan" id="kobo.660.1">spark-worker</span></strong><span class="koboSpan" id="kobo.661.1">-2-1  Started</span></pre> <p><span class="koboSpan" id="kobo.662.1">By the end of the process, you will have a running Spark cluster with a master node (</span><strong class="source-inline"><span class="koboSpan" id="kobo.663.1">ts-spark-env-spark-master-1</span></strong><span class="koboSpan" id="kobo.664.1">), which is where the cluster manager runs, and two worker nodes (</span><strong class="source-inline"><span class="koboSpan" id="kobo.665.1">ts-spark-env-spark-worker-1-1 and ts-spark-env-spark-worker-2-1</span></strong><span class="koboSpan" id="kobo.666.1">). </span><span class="koboSpan" id="kobo.666.2">In addition, there is a separate node (</span><strong class="source-inline"><span class="koboSpan" id="kobo.667.1">ts-spark-env-jupyter-1</span></strong><span class="koboSpan" id="kobo.668.1">) for a notebook environment, called Jupyter Notebook, similar to what you have used in the previous chapters on </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.669.1">Databricks Community Edition. </span><span class="koboSpan" id="kobo.669.2">In this deployment, this Jupyter node is also the </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">driver node.</span></span></p>
<p><span class="koboSpan" id="kobo.671.1">Let’s now validate the </span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.672.1">environment that we have </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">just deployed.</span></span></p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.674.1">Accessing the UIs</span></h2>
<p><span class="koboSpan" id="kobo.675.1">We will now access the UIs of the </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.676.1">different components as a quick way to validate </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">the deployment:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.678.1">We start with Jupyter Notebook at the following local </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">URL: </span></span><a href="http://localhost:8888/lab"><span class="No-Break"><span class="koboSpan" id="kobo.680.1">http://localhost:8888/lab</span></span></a></li>
</ol>
<p class="callout-heading"><span class="koboSpan" id="kobo.681.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.682.1">You will need to change the network port in the preceding URL if you need to modify it as discussed in the </span><em class="italic"><span class="koboSpan" id="kobo.683.1">Network </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.684.1">ports</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.685.1"> section.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.686.1">This will open the web page as per </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.687.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.688.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer042">
<span class="koboSpan" id="kobo.690.1"><img alt="" src="image/B18568_03_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.691.1">Figure 3.5: Jupyter Notebook</span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.692.1">The next (and important) UI is for the Apache Spark master node, accessible via the following local </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">URL: </span></span><a href="http://localhost:8080/"><span class="No-Break"><span class="koboSpan" id="kobo.694.1">http://localhost:8080/</span></span></a><p class="list-inset"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.695.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.696.1">.6</span></em><span class="koboSpan" id="kobo.697.1"> shows this </span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.698.1">master node UI, as well as the worker </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">nodes connected.</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer043">
<span class="koboSpan" id="kobo.700.1"><img alt="" src="image/B18568_03_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.701.1">Figure 3.6: Spark master node UI</span></p>
<p><span class="koboSpan" id="kobo.702.1">We now have our own Apache Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">cluster running.</span></span></p>
<p><span class="koboSpan" id="kobo.704.1">As a final step to conclude this chapter, you can stop the containers with the </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">following command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.706.1">
make down</span></pre> <p><span class="koboSpan" id="kobo.707.1">If you do not intend to</span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.708.1"> use it further, you can additionally delete the Docker containers created with the Delete action as explained </span><span class="No-Break"><span class="koboSpan" id="kobo.709.1">here: </span></span><a href="https://docs.docker.com/desktop/use-desktop/container/#container-actions"><span class="No-Break"><span class="koboSpan" id="kobo.710.1">https://docs.docker.com/desktop/use-desktop/container/#container-actions</span></span></a></p>
<h1 id="_idParaDest-83"><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.711.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.712.1">In this chapter, we dove deep into the Apache Spark architecture, its key components, and its features. </span><span class="koboSpan" id="kobo.712.2">The key concepts, how it works, and what makes it such a great tool were explained. </span><span class="koboSpan" id="kobo.712.3">We then deployed a multi-node cluster representing an example architecture. </span><span class="koboSpan" id="kobo.712.4">The concepts presented in this chapter, while essential, cover only a part of an Apache Spark project. </span><span class="koboSpan" id="kobo.712.5">We will view such a project end to end in the </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">next chapter.</span></span></p>
<h1 id="_idParaDest-84"><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.714.1">Further reading</span></h1>
<p><span class="koboSpan" id="kobo.715.1">This section serves as a repository of sources that can help you build on your understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">the topic:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.717.1">Apache Spark official web </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">page: </span></span><a href="https://spark.apache.org/"><span class="No-Break"><span class="koboSpan" id="kobo.719.1">https://spark.apache.org/</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.720.1">Mastering Apache Spark</span></em><span class="koboSpan" id="kobo.721.1"> (Packt Publishing) by Timothy Chen, Mike Frampton, and </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">Tim Seear</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.723.1">Azure Databricks Cookbook</span></em><span class="koboSpan" id="kobo.724.1"> (Packt Publishing) by Phani Raj and </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">Vinod Jaiswal</span></span></li>
<li><span class="koboSpan" id="kobo.726.1">Google Trends </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">comparison: </span></span><a href="https://trends.google.com/trends/explore?date=2009-01-01%202024-08-28&amp;q=%2Fm%2F0bs2j8q,%2Fm%2F0ndhxqz,%2Fm%2F0fdjtq&amp;hl=en"><span class="No-Break"><span class="koboSpan" id="kobo.728.1">https://trends.google.com/trends/explore?date=2009-01-01%202024-08-28&amp;q=%2Fm%2F0bs2j8q,%2Fm%2F0ndhxqz,%2Fm%2F0fdjtq&amp;hl=en</span></span></a></li>
<li><span class="koboSpan" id="kobo.729.1">Cluster </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">Overview: </span></span><a href="https://spark.apache.org/docs/latest/cluster-overview.html"><span class="No-Break"><span class="koboSpan" id="kobo.731.1">https://spark.apache.org/docs/latest/cluster-overview.html</span></span></a></li>
<li><span class="koboSpan" id="kobo.732.1">Spark </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">Connect: </span></span><a href="https://spark.apache.org/docs/latest/spark-connect-overview.html"><span class="No-Break"><span class="koboSpan" id="kobo.734.1">https://spark.apache.org/docs/latest/spark-connect-overview.html</span></span></a></li>
<li><span class="koboSpan" id="kobo.735.1">Docker </span><span class="No-Break"><span class="koboSpan" id="kobo.736.1">Compose: </span></span><a href="https://docs.docker.com/compose/"><span class="No-Break"><span class="koboSpan" id="kobo.737.1">https://docs.docker.com/compose/</span></span></a></li>
<li><span class="koboSpan" id="kobo.738.1">Make and </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">Makefile: </span></span><a href="https://www.gnu.org/software/make/manual/make.html"><span class="No-Break"><span class="koboSpan" id="kobo.740.1">https://www.gnu.org/software/make/manual/make.html</span></span></a></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.741.1">Jupyter: </span></span><a href="https://jupyter.org/"><span class="No-Break"><span class="koboSpan" id="kobo.742.1">https://jupyter.org/</span></span></a></li>
</ul>
<h1 id="_idParaDest-85"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.743.1">Join our community on Discord</span></h1>
<p><span class="koboSpan" id="kobo.744.1">Join our community’s Discord space for discussions with the authors and </span><span class="No-Break"><span class="koboSpan" id="kobo.745.1">other readers:</span></span></p>
<p><a href="https://packt.link/ds"><span class="No-Break"><span class="koboSpan" id="kobo.746.1">https://packt.link/ds</span></span></a></p>
<div>
<div class="IMG---Figure" id="_idContainer044">
<span class="koboSpan" id="kobo.747.1"><img alt="" src="image/ds_(1).jpg"/></span>
</div>
</div>
</div>


<div class="Content" epub:type="part" id="_idContainer046">
<h1 id="_idParaDest-86" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.1.1">Part 2:  From Data to Models</span></h1>
</div>
<div id="_idContainer047">
<p><span class="koboSpan" id="kobo.2.1">Building on the foundations, in this part, you will get a holistic view of all the stages involved in a time series analysis project, with a focus on the data and models. </span><span class="koboSpan" id="kobo.2.2">Starting with the ingestion and preparation of time series data, we will then do exploratory analysis to understand the nature of the time series. </span><span class="koboSpan" id="kobo.2.3">The data readiness and analysis will then lead us to the choice of model for analysis, development, </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">and testing.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B18568_04.xhtml#_idTextAnchor087"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.7.1">, </span><em class="italic"><span class="koboSpan" id="kobo.8.1">End-to-End View of a Time Series Analysis Project</span></em></li>
<li><a href="B18568_05.xhtml#_idTextAnchor103"><em class="italic"><span class="koboSpan" id="kobo.9.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.10.1">, </span><em class="italic"><span class="koboSpan" id="kobo.11.1">Data Preparation</span></em></li>
<li><a href="B18568_06.xhtml#_idTextAnchor116"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 6</span></em></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Exploratory Data Analysis</span></em></li>
<li><a href="B18568_07.xhtml#_idTextAnchor133"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.16.1">, </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Building and Testing Models</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer048">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer049">
</div>
</div>
</body></html>