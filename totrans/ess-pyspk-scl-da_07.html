<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer034">
			<h1 id="_idParaDest-94"><a id="_idTextAnchor094"/>Chapter 5: Scalable Machine Learning with PySpark</h1>
			<p>In the previous chapters, we have established that modern-day data is growing at a rapid rate, with a volume, velocity, and veracity not possible for traditional systems to keep pace with. Thus, we learned about distributed computing to keep up with the ever-increasing data processing needs and saw practical examples of ingesting, cleansing, and integrating data to bring it to a level that is conducive to business analytics using the power and ease of use of Apache Spark's unified data analytics platform. This chapter, and the chapters that follow, will explore the data science and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) aspects of data analytics.</p>
			<p>Today, the computer science disciplines of AI and ML have made a massive comeback and are pervasive. Businesses everywhere need to leverage these techniques to remain competitive, expand their customer base, introduce novel product lines, and stay profitable. However, traditional ML and data science techniques were designed to deal with limited samples of data and are not inherently scalable.</p>
			<p>This chapter provides you with an overview of traditional ML algorithms, including supervised and unsupervised ML techniques and explores real-world use cases of ML in business applications. Then you will learn about the need for scalable ML. A few techniques for scaling out ML algorithms in a distributed fashion to handle very large data samples will be presented. Then, we will dive into the ML library of Apache Spark, called MLlib, along with code examples, to perform data wrangling using Apache Spark's MLlib to explore, clean, and manipulate data in order to get it ready for ML applications.</p>
			<p>This chapter covers the following main topics:</p>
			<ul>
				<li>ML overview</li>
				<li>Scaling out machine learning</li>
				<li>Data wrangling with Apache Spark and MLlib</li>
			</ul>
			<p>By the end of this chapter, you will have gained an appreciation for scalable ML and its business applications and acquired a basic understanding of Apache Spark's scalable ML library, named MLlib. You will have acquired the skills necessary to utilize MLlib to clean and transform data and get it ready for ML applications at scale, helping you to reduce the time taken for data cleansing tasks, and making your overall ML life cycle much more efficient.</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor095"/>Technical requirements</h1>
			<p>In this chapter, we will be using the Databricks Community Edition to run our code: <a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a>. </p>
			<ul>
				<li>Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. </li>
				<li>The code and data used in this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter05">https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter05</a>.</li>
			</ul>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor096"/>ML overview</h1>
			<p><strong class="bold">Machine Learning</strong> is a field of AI and computer science that leverages statistical models and computer science algorithms for learning patterns inherent in data, without being explicitly programmed. ML consists of algorithms that automatically convert patterns within data <a id="_idIndexMarker417"/>into models. Where pure mathematical or rule-based models perform the same task over and over again, an ML model learns from data and its performance can be greatly improved by exposing it to vast amounts of data.</p>
			<p>A typical ML process involves applying an ML algorithm to a known dataset called the training dataset, to generate a new ML model. This process is generally termed <em class="italic">model training</em> or <em class="italic">model fitting</em>. Some ML models are trained on datasets containing a known correct answer that we intend to predict in an unknown dataset. This known, correct value in the training dataset is termed the <em class="italic">label</em>.</p>
			<p>Once the model is trained, the <a id="_idIndexMarker418"/>resultant model is applied to new data in order to <a id="_idIndexMarker419"/>â€¨predict the required values. This process is generally referred to as <strong class="bold">model inference</strong> or <strong class="bold">model scoring</strong>.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Instead of training a single model, it is a best practice to train multiple models using various <a id="_idIndexMarker420"/>different model parameters called <strong class="bold">hyperparameters</strong> and select the best model among all the trained models based on well-defined <strong class="bold">accuracy metrics</strong>. This process <a id="_idIndexMarker421"/>of training multiple models based on different parameters <a id="_idIndexMarker422"/>is generally <a id="_idIndexMarker423"/>referred to as hyperparameter <strong class="bold">tuning</strong> or <strong class="bold">cross-validation</strong>.</p>
			<p>Examples of ML algorithms include classification, regression, clustering, collaborative filtering, and dimensionality reduction.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor097"/>Types of ML algorithms</h2>
			<p>ML algorithms can be <a id="_idIndexMarker424"/>classified into three broad categories, namely, supervised learning, unsupervised learning, and reinforcement learning, as discussed in the following sections.</p>
			<h3>Supervised learning</h3>
			<p><strong class="bold">Supervised learning</strong> is a type <a id="_idIndexMarker425"/>of ML where a model is trained on a dataset with a known value called the <strong class="bold">label</strong>. This label is tagged in the <a id="_idIndexMarker426"/>training dataset and represents the correct answer <a id="_idIndexMarker427"/>to the problem we are trying to solve. Our intention with supervised learning is to predict the label in an unknown dataset once the model is trained on a known dataset with the tagged label.</p>
			<p>Examples of supervised learning algorithms include Linear Regression, Logistic Regression, Naive Bayes Classifiers, K-Nearest Neighbor, Decision Trees, Random Forest, Gradient Boosted Trees, and Support Vector Machine.</p>
			<p>Supervised learning can be divided into two main classes, namely, regression and classification problems. While regression deals with predicting an unknown label, classification tries <a id="_idIndexMarker428"/>to classify the training dataset into known categories. Detailed implementation of supervised learning using <strong class="bold">Apache Spark</strong> <strong class="bold">MLlib</strong> will be introduced in <a href="B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Supervised Learning</em>.</p>
			<h3>Unsupervised learning</h3>
			<p><strong class="bold">Unsupervised learning</strong> is a type of<a id="_idIndexMarker429"/> ML where the training data is <a id="_idIndexMarker430"/>unknown to the algorithm and is not already labeled with a correct answer. Unsupervised learning involves learning the structure of an unknown, unlabeled dataset, without any guidance from the user. Here, the task of the machine is to group <a id="_idIndexMarker431"/>data into cohorts or groups according to certain similarities or differences without any prior training.</p>
			<p>Unsupervised learning can be further <a id="_idIndexMarker432"/>divided into <strong class="bold">clustering</strong> and <strong class="bold">association</strong> problems. Clustering deals with discovering cohorts within the training dataset, while association deals with discovering rules within the data that describe <a id="_idIndexMarker433"/>the relationship between entities. Examples of unsupervised learning include K-means <a id="_idIndexMarker434"/>clustering and collaborative filtering. Unsupervised learning will be explored at length with coding examples using Apache Spark MLlib in <a href="B16736_07_Final_JM_ePub.xhtml#_idTextAnchor128"><em class="italic">Chapter 7</em></a>, <em class="italic">Unsupervised Machine Learning</em>.</p>
			<h3>Reinforcement learning</h3>
			<p><strong class="bold">Reinforcement learning</strong> is employed by software systems and machines to find the best possible behavior or path that should be taken in a given situation. Unlike supervised learning, which <a id="_idIndexMarker435"/>already holds the correct answer within the training dataset, in reinforcement learning, there is no correct answer, but the reinforcement agent employs trial and error to decide the outcome and is designed to learn from experience. The reinforcement agent is either rewarded or penalized depending on the path chosen and the goal here is to maximize the reward.</p>
			<p>Reinforcement learning is used in applications such as self-driving cars, robotics, industrial automation, and natural <a id="_idIndexMarker436"/>language processing for chatbot agents. There aren't any out-of-the-box implementations of reinforcement learning within Apache Spark MLlib, so further exploration of this concept is beyond the scope of this book.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Another branch of data science and ML is <strong class="bold">deep learning</strong>, which leverages advanced techniques for ML, such as <a id="_idIndexMarker437"/>neural networks, which are also becoming very prominent these days. Although Apache Spark does support certain deep learning algorithms, these concepts are too advanced to be included within the scope of this book.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor098"/>Business use cases of ML</h2>
			<p>So far, we have discussed various categories of ML and briefly introduced you to the tasks the ML models <a id="_idIndexMarker438"/>can perform. In this section, you will learn about some real-life applications of ML algorithms that help solve actual business problems across various industry verticals.</p>
			<h3>Customer churn prevention</h3>
			<p>Building customer churn models using ML can be very useful in identifying all those customers who are <a id="_idIndexMarker439"/>likely to stop engaging with your business and also help you gain insight into the factors that might lead them to churn. A churn model can simply be a regression model <a id="_idIndexMarker440"/>that estimates the risk score of each individual. Customer churn models can be very useful in identifying customers at risk of churning, thereby allowing businesses to implement strategies for customer retention.</p>
			<h3>Customer lifetime value modeling</h3>
			<p>Retail businesses generate a huge share of their revenue from a small cohort of high-value customers who <a id="_idIndexMarker441"/>provide repeat business. Customer lifetime value models can estimate a customer's lifetime, a period after which they might churn. They can also predict the total <a id="_idIndexMarker442"/>revenue that a customer would probably generate over their lifetime. Thus, estimating the revenue that a potential high-value customer might bring over their lifetime could be essential in redirecting marketing dollars to attracting and retaining such customers.</p>
			<h3>Demand forecasting</h3>
			<p>Brick and mortar businesses, as well as online businesses, have limited storage space within their <a id="_idIndexMarker443"/>actual stores as well as at the warehouse. Hence, it is important to properly stock the available storage space <a id="_idIndexMarker444"/>with products that will actually be in demand. You could develop a simple model based on seasonality and the month of the year. However, building a sophisticated ML model that includes not just seasonality and historical data, but external data such as current trends on social media, weather forecast data, and customer sentiment on social media, could lead to better forecasting of demand and help maximize revenues as a result.</p>
			<h3>Shipment lead-time prediction</h3>
			<p>Any business that involves delivery and logistics operations, whether it be an online retailer or a food delivery aggregator, needs to be able to estimate the amount of time it would take for <a id="_idIndexMarker445"/>the order to reach the customer. Often, this shipment lead time is an essential decision-making <a id="_idIndexMarker446"/>factor by the customer in terms of doing business with you versus moving on to a competitor. Regression models can be used to accurately estimate the amount of time required to deliver the product to the customer's zip code, based on factors such as product origin and destination locations, weather, and other seasonal data.</p>
			<h3>Market basket analysis</h3>
			<p>Market basket analysis is a technique for making product recommendations to customers based <a id="_idIndexMarker447"/>on items already in their basket. ML can be used to discover association rules <a id="_idIndexMarker448"/>within product categories by leveraging the collaborative filtering algorithm in order to make product recommendations to online customers based on the items already in their cart and their past purchases. This is a prominent use case used by pretty much every e-tailer.</p>
			<h3>Financial fraud detection</h3>
			<p>ML has the inherent capability to detect patterns within data. Thus, ML can be leveraged to build <a id="_idIndexMarker449"/>models that can detect anomalies across financial transactions to flag certain transactions as being fraudulent. Traditionally, financial institutions have already been leveraging <a id="_idIndexMarker450"/>rule-based models for fraud detection, but incorporating ML models into the mix makes the fraud models even more potent, thereby helping to detect novel fraud patterns.</p>
			<h3>Information extraction using natural language processing</h3>
			<p>Pharma companies and businesses that generate a large corpus of knowledge are faced with a unique challenge specific to their industry. Trying to identify whether a certain piece of <a id="_idIndexMarker451"/>knowledge has already been created by another group within the vast organization is not a straightforward <a id="_idIndexMarker452"/>problem at organizations with tens of thousands of employees. ML's natural language processing techniques can be applied to sort, group, classify, and label a large corpus of documents so that users can easily search if a similar piece of knowledge already exists.</p>
			<p>So far, you have explored the basics of ML, the different types of ML algorithms, and their applications in real-world business use cases. In the following section, we will discuss the need for scalable ML and a few techniques for scaling our ML algorithms, and get an introduction to Apache Spark's native, scalable ML library called <strong class="bold">MLlib</strong> and its application for performing data wrangling.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor099"/>Scaling out machine learning</h1>
			<p>In the previous sections, we<a id="_idIndexMarker453"/> learned that ML is a set of algorithms that, instead of being explicitly programmed, automatically learn patterns hidden within data. Thus, an ML algorithm exposed to a larger dataset can potentially result in a better-performing model. However, traditional ML algorithms were designed to be trained on a limited data sample and on a single machine at a time. This means that the existing ML libraries are not inherently scalable. One solution to this problem is to down-sample a larger dataset to fit in the memory of a single machine, but this also potentially means that the resulting models aren't as accurate as they could be.</p>
			<p>Also, typically, several ML models are built on the same dataset, simply varying the parameters supplied <a id="_idIndexMarker454"/>to the algorithm. Out of these several models, the best model is chosen for production purposes, using a technique called <strong class="bold">hyperparameter tuning</strong>. Building several models using a single machine, one model after another, in a linear manner takes a very long time to arrive at the best possible model, leading to a longer time to production and, hence, a longer time to market.</p>
			<p>Given these scalability challenges with traditional ML algorithms, there is a need to either scale out existing ML algorithms or new scalable ML algorithms. We will explore some techniques for scaling out ML algorithms in the following section.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor100"/>Techniques for scaling ML</h2>
			<p>Two of the prominent techniques for scaling out ML algorithms are described in the following sections.</p>
			<h3>Embarrassingly parallel processing</h3>
			<p><strong class="bold">Embarrassingly parallel processing</strong> is a type of parallel computing technique where little to no effort is <a id="_idIndexMarker455"/>required to split a given <a id="_idIndexMarker456"/>computational problem into smaller parallel tasks. This is possible when the parallelized tasks do not have any interdependencies, and all tasks can execute completely independently of one another.</p>
			<p>Now, let's try to apply this to the problem of scaling out single machine ML algorithms on very large datasets, and at the outset, this doesn't seem like a simple task at all. However, consider the problem of hyperparameter tuning or cross-validation. Here, we can run multiple parallel models, each with different model parameters, but on the same smaller dataset <a id="_idIndexMarker457"/>that can fit into a single machine's memory. Here, we can easily train multiple models on the same dataset by varying the model parameters. Thus, by leveraging the embarrassingly parallel processing technique, we can accelerate our model-building process by several orders of magnitude, helping us get to the best possible model within hours instead of several weeks or even months, and thereby accelerating your business time to value. You will learn more about applying this technique to scale out single-node Python ML libraries using Apache Spark in <a href="B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176"><em class="italic">Chapter 10</em></a><em class="italic">, Scaling Out Single-Node Machine Learning Using PySpark</em>.</p>
			<h3>Scalable ML algorithms</h3>
			<p>While the embarrassingly parallel computing technique helps us get better models with greater accuracy in a faster time, it is still limited by smaller dataset sizes. This means that we might still be <a id="_idIndexMarker458"/>missing out on potential patterns of data because of down-sampling. To overcome this, we need ML algorithms that can inherently scale out across multiple machines and can train on a very large dataset in a distributed manner. Apache Spark's native ML library, called MLlib, consists of such inherently scalable ML algorithms, and we will explore MLlib further in the following section.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor101"/>Introduction to Apache Spark's ML library</h2>
			<p>MLlib is Apache Spark's native ML library. Being a native library, MLlib has tight integration with the rest of Spark's APIs and libraries, including Spark SQL Engine, DataFrame APIs, Spark SQL API, and even Structured Streaming. This gives Apache Spark the unique advantage of being a truly unified data analytics platform that can perform all tasks pertaining to data analytics, starting from data ingestion to data transformation, the ad hoc analysis of data, building sophisticated ML models, and even leveraging those models for production use cases. In the following section, you will explore more of Spark MLlib and its core components.</p>
			<h3>Spark MLlib overview</h3>
			<p>In the early versions of Apache Spark, MLlib was based on Spark's RDD API. Starting with Spark version 2.0, a new ML library based on DataFrame APIs was introduced. Now, in Spark 3.0 and versions <a id="_idIndexMarker459"/>above this, the DataFrame API-based MLlib is standard, while the older RDD-based MLlib is in maintenance mode with no future enhancements planned.</p>
			<p>The DataFrame-based MLlib closely mimics traditional single-machine Python-based ML libraries such as scikit-learn and consists of three major components, called transformers, estimators, and pipelines, as described in the following sections.</p>
			<h3>Transformers</h3>
			<p>A <strong class="bold">transformer</strong> is an algorithm that takes a DataFrame as input, performs processing on the DataFrame columns, and returns <a id="_idIndexMarker460"/>another DataFrame. An ML model trained using Spark MLlib is a <a id="_idIndexMarker461"/>transformer that takes a raw DataFrame and returns another DataFrame with the original raw data along with the new prediction columns. A typical transformer pipeline is shown in the following diagram:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="Images/B16736_05_01.jpg" alt="Figure 5.1 â€“ A transformer pipeline&#13;&#10;" width="1272" height="501"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 â€“ A transformer pipeline</p>
			<p>In the previous <a id="_idIndexMarker462"/>diagram, a typical transformer pipeline is depicted, where a series of transformer stages, including a <strong class="bold">VectorIndexer</strong> and an already trained <strong class="bold">Linear Regression Model</strong>, are applied to the raw DataFrame. The result is a new DataFrame with all the original columns, along with some new columns containing predicted values.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The transformation DataFrame operation is a different concept compared to transformers within Spark's MLlib. While both transform one DataFrame into another DataFrame and are lazily evaluated, the former is an operation performed on a DataFrame, while the latter is an actual ML algorithm.</p>
			<h3>Estimators</h3>
			<p>An <strong class="bold">estimator</strong> is another algorithm that accepts a DataFrame as input and results in a transformer. Any ML <a id="_idIndexMarker463"/>algorithm is an estimator in that it transforms a DataFrame with raw data into a DataFrame with actual predictions. An <a id="_idIndexMarker464"/>estimator pipeline is depicted in the following diagram:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="Images/B16736_05_02.jpg" alt="Figure 5.2 â€“ An estimator pipeline&#13;&#10;" width="1421" height="501"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 â€“ An estimator pipeline</p>
			<p>In the preceding diagram, a <strong class="bold">Transformer</strong> is first applied to a DataFrame with raw data to result in a <strong class="bold">Feature Vector</strong> DataFrame. An <strong class="bold">Estimator</strong> in the form of a <strong class="bold">Linear Regression</strong> <strong class="bold">Algorithm</strong> <a id="_idIndexMarker465"/>is then applied to the DataFrame containing <strong class="bold">Feature Vectors</strong> to result in a <strong class="bold">Transformer</strong> in the form of a newly trained <strong class="bold">Linear Regression Model</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A feature vector is a special data structure within the Spark MLlib library. It is a DataFrame column <a id="_idIndexMarker466"/>consisting of actual vector objects of the floating-point type. Since ML is based on mathematics and statistics, all ML algorithms exclusively operate on vectors of floating-point values. Raw data is converted into feature vectors using feature extraction and feature engineering techniques.</p>
			<h3>Pipelines</h3>
			<p>An ML pipeline within Spark MLlib chains together several stages of transformers and estimators into a DAG that performs an end-to-end ML operation ranging from data cleansing, to feature <a id="_idIndexMarker467"/>engineering, to actual model training. A pipeline could be a transformer-only pipeline or an estimator-only pipeline or a mix of the two.</p>
			<p>Using the available transformers and estimators within Spark MLlib, an entire end-to-end ML pipeline can <a id="_idIndexMarker468"/>be constructed. A typical ML pipeline consists of several stages, starting with data wrangling, feature engineering, model training, and model inferencing. You will learn more about data wrangling techniques in the following section.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor102"/>Data wrangling with Apache Spark and MLlib</h1>
			<p><strong class="bold">Data wrangling</strong>, also referred to within the data science community as <strong class="bold">data munging</strong>, or simply <strong class="bold">data preparation</strong>, is the first step in a typical data science process. Data wrangling involves <a id="_idIndexMarker469"/>sampling, exploring, selecting, manipulating, and <a id="_idIndexMarker470"/>cleansing data to make <a id="_idIndexMarker471"/>it ready for ML applications. Data wrangling takes up <a id="_idIndexMarker472"/>to 60 to 80 percent of the whole data science process and is the most crucial step in guaranteeing the <a id="_idIndexMarker473"/>accuracy of the ML model being built. The following sections explore the data wrangling process using Apache Spark and MLlib.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor103"/>Data preprocessing</h2>
			<p>Data preprocessing is the first step in the data wrangling process and involves gathering, exploring, and selecting the data elements useful for solving the problem at hand. The data science process typically <a id="_idIndexMarker474"/>succeeds the data engineering process and the assumption here is that clean and integrated data is already available in the data lake. However, data that is clean enough for BI may not be good enough for data science. Also, data science applications require additional datasets, which may not be useful for other analytics use cases, and hence, may not yet be clean.</p>
			<p>Before we start manipulating and wrangling our data, we need to load it into a Spark DataFrame and explore the data to gain some understanding of its structure. The following code example will make use of the integrated dataset produced toward the end of <a href="B16736_03_Final_JM_ePub.xhtml#_idTextAnchor056"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Cleansing and Integration</em>, named <strong class="source-inline">retail_silver.delta</strong>:</p>
			<p class="source-code">raw_data = spark.read.format("delta").load("dbfs:/FileStore/shared_uploads/delta/retail_silver.delta")</p>
			<p class="source-code">raw_data.printSchema()</p>
			<p class="source-code">(select_data = raw_data.select("invoice_num", "stock_code",</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "quantity", "invoice_date", </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "unit_price","country_code",</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "age", "work_class", </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "final_weight")</p>
			<p class="source-code">select_data.describe().show()</p>
			<p>In the preceding <a id="_idIndexMarker475"/>code snippet, we perform the following operations:</p>
			<ol>
				<li>We load data from the data lake into a Spark DataFrame using the <strong class="source-inline">spark.read()</strong> function.</li>
				<li>We print its schema to check the data types of the columns using the <strong class="source-inline">printSchema()</strong> function.</li>
				<li>We display a few columns from the DataFrame to check their values using the <strong class="source-inline">select()</strong> operation.</li>
				<li>We generate basic statistics on the DataFrame using the <strong class="source-inline">describe()</strong> operation.</li>
			</ol>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor104"/>Data cleansing</h2>
			<p>In the code example in the previous section, you must have noticed that most data types are just strings types. The dataset might also contain duplicates, and there are also <strong class="source-inline">NULL</strong> values <a id="_idIndexMarker476"/>in the data. Let's fix these inconsistencies in the dataset, as shown in the following code snippet:</p>
			<p class="source-code">dedupe_data = select_data.drop_duplicates(["invoice_num",</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "invoice_date", </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "stock_code"])</p>
			<p class="source-code">interim_data = (select_data</p>
			<p class="source-code">Â Â Â Â .withColumn("invoice_time", to_timestamp("invoice_date", </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'dd/M/yy HH:mm'))</p>
			<p class="source-code">Â Â Â Â .withColumn("cust_age", col("age").cast(FloatType()))</p>
			<p class="source-code">Â Â Â Â .withColumn("working_class", </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â col("work_class").cast(FloatType()))</p>
			<p class="source-code">Â Â Â Â .withColumn("fin_wt", </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â col("final_weight").cast(FloatType()))</p>
			<p class="source-code">)</p>
			<p class="source-code">clean_data = interim_data.na.fill(0)</p>
			<p>In the preceding <a id="_idIndexMarker477"/>code snippet, we perform the following operations:</p>
			<ol>
				<li value="1">We de-duplicate data using the <strong class="source-inline">dropduplicates()</strong> operation using the key columns.</li>
				<li>We then cast datetime columns into the proper timestamp type, using the <strong class="source-inline">to_timestamp()</strong> function, by supplying the correct format of the timestamp.</li>
				<li>We change the data types of the DataFrame using the <strong class="source-inline">CAST()</strong> method.</li>
				<li>We replace missing values and <strong class="source-inline">NULL</strong> values with <strong class="source-inline">0</strong> using the <strong class="source-inline">na.fill()</strong> operation.</li>
			</ol>
			<p>This section showed you how to perform basic data cleansing at scale using PySpark. The following section will show you how to manipulate data steps such as filtering and renaming.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor105"/>Data manipulation</h2>
			<p>Once you have a cleaner dataset, you can perform operations to filter out any data not required by <a id="_idIndexMarker478"/>your use case, rename columns to follow your naming conventions, and drop any unwanted data columns, as shown in the following code block:</p>
			<p class="source-code">final_data = (clean_data.where("year(invoice_time) = 2009")</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â .withColumnRenamed("working_class", </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "work_type")</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â .withColumnRenamed("fin_wt", </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â "final_weight")</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â .drop("age")</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â .drop("work_class")</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â .drop("fn_wt"))</p>
			<p class="source-code">pd_data = final_data.toPandas()</p>
			<p>In the preceding code snippet, we perform the following operations:</p>
			<ol>
				<li value="1">We filter, slice, and <a id="_idIndexMarker479"/>dice data using the <strong class="source-inline">where()</strong> function.</li>
				<li>We rename columns using the <strong class="source-inline">withColumnsRenamed()</strong> function and drop unwanted columns using the <strong class="source-inline">drop()</strong> function.</li>
				<li>We convert the Spark DataFrame to a PySpark DataFrame using the <strong class="source-inline">toPandas()</strong> function.</li>
			</ol>
			<p>Sometimes, there is an ML algorithm that's not available in Spark MLlib, or there's a custom algorithm built using single-node Python libraries. For these use cases, you can convert your Spark DataFrame into a pandas DataFrame, as shown in <em class="italic">step 3</em> previously.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Converting a Spark DataFrame to a pandas Dataframe involves collecting all the data from the Executors onto the Spark driver. Thus, care needs to be taken that this conversion is only applied to smaller datasets, otherwise this could lead to an <strong class="source-inline">OutOfMemory</strong> error on the Driver node.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor106"/>Summary</h1>
			<p>In this chapter, you learned about the concept of ML and the different types of ML algorithms. You also learned about some of the real-world applications of ML to help businesses minimize losses and maximize revenues and accelerate their time to market. You were introduced to the necessity of scalable ML and two different techniques for scaling out ML algorithms. Apache Spark's native ML Library, MLlib, was introduced, along with its major components. </p>
			<p>Finally, you learned a few techniques to perform data wrangling to clean, manipulate, and transform data to make it more suitable for the data science process. In the following chapter, you will learn about the send phase of the ML process, called feature extraction and feature engineering, where you will learn to apply various scalable algorithms to transform individual data fields to make them even more suitable for data science applications.</p>
		</div>
	</div></body></html>