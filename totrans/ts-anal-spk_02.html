<html><head></head><body>
<div epub:type="chapter" id="_idContainer037">
<h1 class="chapter-number" id="_idParaDest-44"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-45"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.2.1">Why Time Series  Analysis?</span></h1>
<p><span class="koboSpan" id="kobo.3.1">This chapter delves into the practical significance of analyzing time-dependent data. </span><span class="koboSpan" id="kobo.3.2">It elucidates how time series analysis enables predictive modeling, trend identification, and anomaly detection. </span><span class="koboSpan" id="kobo.3.3">By illustrating real-world applications across industries, the chapter emphasizes the critical role of temporal insights in decision-making. </span><span class="koboSpan" id="kobo.3.4">Grasping the importance of time series analysis is crucial for professionals, as it highlights the impact on forecasting accuracy, resource optimization, and strategic planning, fostering a comprehensive appreciation for the utility of time-oriented </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">data analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">We will cover the following topics in </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">The need for time </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">series analysis</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Industry-specific </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">use cases</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Hands-on with selected </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">use cases</span></span></li>
</ul>
<h1 id="_idParaDest-46"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.13.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.14.1">Following the first chapter, we will go one notch up with the code here. </span><span class="koboSpan" id="kobo.14.2">The objective will be to showcase the use of time series for selected use cases. </span><span class="koboSpan" id="kobo.14.3">The code for this chapter can be found in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.15.1">ch2</span></strong><span class="koboSpan" id="kobo.16.1"> folder of the GitHub repository of this </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">book: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch2</span></span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.20.1">Refer to this GitHub repository for the latest revisions of the code, which will be commented on if updated post-publication from what is presented in the code sections of </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">this book.</span></span></p>
<p><span class="koboSpan" id="kobo.22.1">The hands-on section of this chapter will go into </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">further detail.</span></span></p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.24.1">Understanding the need for time series analysis</span></h1>
<p><span class="koboSpan" id="kobo.25.1">As</span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.26.1"> we discussed in the previous chapter, time series are present in all avenues of life and across all industries. </span><span class="koboSpan" id="kobo.26.2">Hence, the need for analyzing time series is everywhere. </span><span class="koboSpan" id="kobo.26.3">We will explore the different use cases for different industries in this chapter. </span><span class="koboSpan" id="kobo.26.4">Before we get to that, in this section, we will look at the underlying approaches. </span><span class="koboSpan" id="kobo.26.5">These can be broadly categorized as forecasting, pattern detection and categorization, and anomaly detection. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.27.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.28.1">.1</span></em><span class="koboSpan" id="kobo.29.1"> shows several key time series analysis </span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.30.1">concepts that will be discussed in </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">this chapter.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<span class="koboSpan" id="kobo.32.1"><img alt="" src="image/B18568_02_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.33.1">Figure 2.1: Concepts in time series analysis</span></p>
<p><span class="koboSpan" id="kobo.34.1">Let’s now go into further detail on each </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">of these.</span></span></p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.36.1">Forecasting</span></h2>
<p><span class="koboSpan" id="kobo.37.1">Forecasting time series</span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.38.1"> is the prediction of future values based on previously observed values. </span><span class="koboSpan" id="kobo.38.2">This is achieved by modeling the underlying patterns in the time series data – such as trends, seasonality, and cycles – to make predictions about future data points. </span><span class="koboSpan" id="kobo.38.3">For example, in the case of the temperature time series we visualized in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.39.1">Chapter 1</span></em></span><span class="koboSpan" id="kobo.40.1">, we can use forecasting models to predict next month’s temperatures based on the </span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.41.1">pattern learned from previous months. </span><span class="koboSpan" id="kobo.41.2">Forecasting is the most common approach used for time series analysis, and we will spend the most time on it in this book. </span><span class="koboSpan" id="kobo.41.3">This can be single-step, multi-step, univariate, </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">or multivariate.</span></span></p>
<h3><span class="koboSpan" id="kobo.43.1">Single-step forecasting</span></h3>
<p><span class="koboSpan" id="kobo.44.1">With single-step forecasting, we </span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.45.1">predict the next occurrence in the time</span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.46.1"> series based on our analysis of the historical data points, and the model built accordingly. </span><span class="koboSpan" id="kobo.46.2">The granularity of the step is usually the same as in the dataset from which we are learning the historical patterns. </span><span class="koboSpan" id="kobo.46.3">For example, if in our historical time series, we have daily temperatures, then the next step will be the next day. </span><span class="koboSpan" id="kobo.46.4">If we have aggregated the data points to, for example, monthly averages, and modeled the pattern of monthly changes, then the next step will be the average temperature for the </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">next month.</span></span></p>
<p><span class="koboSpan" id="kobo.48.1">While single-step forecasting is usually the most reliable forecast we can get, it is, unfortunately, insufficient for many requirements because, in real life, we plan way more ahead than just one (time) step at a time. </span><span class="koboSpan" id="kobo.48.2">If we are doing daily forecasts, we want to forecast not just for tomorrow. </span><span class="koboSpan" id="kobo.48.3">We want to predict days, weeks, and even months into the future. </span><span class="koboSpan" id="kobo.48.4">A single step is just not sufficient </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">for planning.</span></span></p>
<h3><span class="koboSpan" id="kobo.50.1">Multi-step forecasting</span></h3>
<p><span class="koboSpan" id="kobo.51.1">With muti-step forecasting, we </span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.52.1">predict multiple next steps in the time series using</span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.53.1"> the model built from the historical data points. </span><span class="koboSpan" id="kobo.53.2">We also use the forecasted prior steps as input. </span><span class="koboSpan" id="kobo.53.3">With our daily temperature example, this could mean forecasting day by day for the entire </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">upcoming week.</span></span></p>
<h4><span class="koboSpan" id="kobo.55.1">Challenges</span></h4>
<p><span class="koboSpan" id="kobo.56.1">The challenge with </span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.57.1">multi-step forecasting is that further predictions are built upon prior predictions, which contrasts with single-step forecasting, where predictions are based on actual data points. </span><span class="koboSpan" id="kobo.57.2">Practically, this means recursively applying the forecasting algorithm one step at a time, each step adding the forecast to the dataset, and using the historical and forecasted data points to predict the next step. </span><span class="koboSpan" id="kobo.57.3">Hence, inaccuracies in the forecast are cumulated further and further as we go each step into </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">the future.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.59.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.60.1">This cumulation of forecasting errors with multi-step forecasting is the kind of limitation you want to be upfront about with the business or anyone you are building the forecast for. </span><span class="koboSpan" id="kobo.60.2">You want to be sure to set the expectations on </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">longer-term forecasting.</span></span></p>
<h4><span class="koboSpan" id="kobo.62.1">Solutions</span></h4>
<p><span class="koboSpan" id="kobo.63.1">There are a few ways to </span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.64.1">address the multi-step forecast challenge, besides limiting it to a very </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">short horizon:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.66.1">First and foremost, build as accurate a model as possible so that the initial step forecasted is close </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">to reality</span></span></li>
<li><span class="koboSpan" id="kobo.68.1">Another approach is to use a combination of models, aiming to average out the </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">forecasting errors</span></span></li>
<li><span class="koboSpan" id="kobo.70.1">Finally, limit the </span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.71.1">forecasting interval or number of steps and recalculate the forecast when new measurements </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">come in</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.73.1">Univariate forecasting</span></h3>
<p><span class="koboSpan" id="kobo.74.1">So far, with the</span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.75.1"> temperature time series in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.76.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.77.1">, we have considered only </span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.78.1">one variable (univariate) at a time, that is, the temperature at a specific location. </span><span class="koboSpan" id="kobo.78.2">Another example of a univariate time series, this time in the economic sector, is the rate of unemployment for a specific region or country. </span><span class="koboSpan" id="kobo.78.3">A single time series is univariate by definition, be it for temperature or unemployment rate. </span><span class="koboSpan" id="kobo.78.4">In real-world scenarios, the requirement is likely going to be to forecast multiple time series at the same time so that we can get a more comprehensive view of the future than just one time series can give us. </span><span class="koboSpan" id="kobo.78.5">In the case of temperature, this can mean taking multiple locations into account or additionally forecasting the level of </span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.79.1">pollutants in the air. </span><span class="koboSpan" id="kobo.79.2">For economic forecasting, it </span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.80.1">can be about forecasting the </span><strong class="bold"><span class="koboSpan" id="kobo.81.1">Gross Domestic Product</span></strong><span class="koboSpan" id="kobo.82.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.83.1">GDP</span></strong><span class="koboSpan" id="kobo.84.1">) in </span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.85.1">addition to the level of unemployment. </span><span class="koboSpan" id="kobo.85.2">This leads us to </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">multivariate forecasting.</span></span></p>
<h3><span class="koboSpan" id="kobo.87.1">Multivariate forecasting</span></h3>
<p><span class="koboSpan" id="kobo.88.1">While univariate is about a single</span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.89.1"> time series, there are a couple of ways to characterize </span><a id="_idIndexMarker113"/><span class="No-Break"><span class="koboSpan" id="kobo.90.1">multivariate forecasting:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.91.1">Multiple input</span></strong><span class="koboSpan" id="kobo.92.1"> dimensions are the case when we provide several variables (time series and non time series) as input to the forecasting model – for example, using past temperatures and pollutant levels to predict </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">future temperatures.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.94.1">Multiple output</span></strong><span class="koboSpan" id="kobo.95.1"> prediction uses the forecasting model to predict multiple variables. </span><span class="koboSpan" id="kobo.95.2">With the preceding temperature example, this means forecasting both the temperature and the </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">pollutant level.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.97.1">As</span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.98.1"> we have seen with the preceding examples, there can be several scenarios with multiple time series. </span><span class="koboSpan" id="kobo.98.2">These series can be </span><strong class="bold"><span class="koboSpan" id="kobo.99.1">correlated</span></strong><span class="koboSpan" id="kobo.100.1">, and if</span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.101.1"> they have the same underlying cause (which can itself be represented by yet another time series), can have </span><strong class="bold"><span class="koboSpan" id="kobo.102.1">co-movement</span></strong><span class="koboSpan" id="kobo.103.1">. </span><span class="koboSpan" id="kobo.103.2">They can have </span><strong class="bold"><span class="koboSpan" id="kobo.104.1">causal dependency</span></strong><span class="koboSpan" id="kobo.105.1">, where </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.106.1">one time series has a causality relationship to another. </span><span class="koboSpan" id="kobo.106.2">They can also be </span><strong class="bold"><span class="koboSpan" id="kobo.107.1">independent</span></strong><span class="koboSpan" id="kobo.108.1">, and we</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.109.1"> just predict them simultaneously. </span><span class="koboSpan" id="kobo.109.2">We will address some of these considerations in </span><a href="B18568_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.110.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.111.1"> on exploratory </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">data analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.113.1">In summary, forecasting</span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.114.1"> is rarely done in isolation, and the number of time series that we need to analyze at the same time can be in the hundreds or even thousands. </span><span class="koboSpan" id="kobo.114.2">This need to scale to multiple time series is a good reason to use a parallel execution tool such as Apache Spark, as we will see in </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">later chapters.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">Now that we have discussed forecasting, let’s see another type of analysis, which will enable us to classify </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">time series.</span></span></p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.118.1">Pattern detection and categorization</span></h2>
<p><span class="koboSpan" id="kobo.119.1">Pattern detection and categorization</span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.120.1"> is about identifying and</span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.121.1"> classifying time series based on certain patterns. </span><span class="koboSpan" id="kobo.121.2">In general, time series follow a certain pattern, which we can identify and label. </span><span class="koboSpan" id="kobo.121.3">These labels allow us to classify time series by matching the labeled pattern to new occurrences of time series. </span><span class="koboSpan" id="kobo.121.4">We can follow different approaches to achieve this, broadly categorized as distance-based, interval-based, frequency-based, dictionary-based, shapelets, ensembles, and deep learning. </span><span class="koboSpan" id="kobo.121.5">These approaches will now </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">be detailed.</span></span></p>
<h3><span class="koboSpan" id="kobo.123.1">Distance-based</span></h3>
<p><span class="koboSpan" id="kobo.124.1">Distance-based</span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.125.1"> time series classification </span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.126.1">using </span><strong class="bold"><span class="koboSpan" id="kobo.127.1">k-Nearest Neighbors</span></strong><span class="koboSpan" id="kobo.128.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.129.1">kNN</span></strong><span class="koboSpan" id="kobo.130.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.131.1">Dynamic Time Warping</span></strong><span class="koboSpan" id="kobo.132.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.133.1">DTW</span></strong><span class="koboSpan" id="kobo.134.1">), which </span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.135.1">will be explained here, is a proven</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.136.1"> method for analyzing time-sequential data. </span><span class="koboSpan" id="kobo.136.2">Due to shifts and distortions in time series data, standard Euclidean distance is a poor metric for similarity measurement. </span><span class="koboSpan" id="kobo.136.3">DTW offers an alternative by aligning the sequences in time. </span><span class="koboSpan" id="kobo.136.4">It calculates the minimum distance between two time series, considering all possible alignments, which makes it compute-intensive. </span><span class="koboSpan" id="kobo.136.5">kNN is then used to classify the time series based on the similarity of </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">their shapes.</span></span></p>
<p><span class="koboSpan" id="kobo.138.1">The following chart shows the outcome of DTW to calculate the distance between the shares of Google (lower, black) and Amazon, and the shares of Google and Meta. </span><span class="koboSpan" id="kobo.138.2">We will execute this example in the hands-on section of </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">this chapter.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.140.1"><img alt="" src="image/B18568_02_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.141.1">Figure 2.2: DTW distance (GOOG – black line)</span></p>
<h3><span class="koboSpan" id="kobo.142.1">Interval-based</span></h3>
<p><span class="koboSpan" id="kobo.143.1">With this approach, the time series</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.144.1"> is first partitioned into </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.145.1">intervals, for which descriptive statistics are calculated. </span><span class="koboSpan" id="kobo.145.2">These intervals are then used as feature vectors together with classifiers such as random forests or support vector machines. </span><span class="koboSpan" id="kobo.145.3">The advantage of this method is that it captures the properties of time series over different phases, which works well for time series with non-uniform patterns over time. </span><span class="koboSpan" id="kobo.145.4">This summarization of the time series into intervals with statistical features is also a great way to reduce complexity and </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">improve interpretability.</span></span></p>
<h3><span class="koboSpan" id="kobo.147.1">Frequency-based</span></h3>
<p><span class="koboSpan" id="kobo.148.1">This classification </span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.149.1">method, such </span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.150.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.151.1">Random Interval Spectral Ensemble</span></strong><span class="koboSpan" id="kobo.152.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.153.1">RISE</span></strong><span class="koboSpan" id="kobo.154.1">), involves first </span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.155.1">transforming the time series to the frequency domain, using, for example, the Fourier transform. </span><span class="koboSpan" id="kobo.155.2">RISE is an ensemble approach based on classifiers, such as decision trees, built from random intervals and spectral features extracted for these intervals. </span><span class="koboSpan" id="kobo.155.3">The advantage of this method is in identifying frequency-related or periodic characteristics, and as an ensemble approach, it is robust while </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">providing accuracy.</span></span></p>
<h3><span class="koboSpan" id="kobo.157.1">Dictionary-based</span></h3>
<p><span class="koboSpan" id="kobo.158.1">Dictionary-based</span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.159.1"> time series classification methods</span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.160.1"> convert time series into symbolic representations, allowing the use of text-based techniques for classification. </span><span class="koboSpan" id="kobo.160.2">Prominent methods following this approach </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">are these:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.162.1">Bag of Patterns</span></strong><span class="koboSpan" id="kobo.163.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.164.1">BoP</span></strong><span class="koboSpan" id="kobo.165.1">): BoP</span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.166.1"> creates a “bag” of patterns by applying a sliding window to capture local </span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.167.1">patterns. </span><span class="koboSpan" id="kobo.167.2">This is then hashed into a frequency histogram, which is used as the feature vector </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">for classification.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.169.1">Bag of SFA Symbols</span></strong><span class="koboSpan" id="kobo.170.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.171.1">BOSS</span></strong><span class="koboSpan" id="kobo.172.1">): BOSS is </span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.173.1">a variation that is more robust to noise and effective at capturing </span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.174.1">key patterns. </span><span class="koboSpan" id="kobo.174.2">It uses </span><strong class="bold"><span class="koboSpan" id="kobo.175.1">Symbolic Fourier Approximation</span></strong><span class="koboSpan" id="kobo.176.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.177.1">SFA</span></strong><span class="koboSpan" id="kobo.178.1">) to capture </span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.179.1">the time series at </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">different resolutions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.181.1">RandOm Convolutional KErnel Transform</span></strong><span class="koboSpan" id="kobo.182.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.183.1">ROCKET</span></strong><span class="koboSpan" id="kobo.184.1">): ROCKET offers further speed and efficiency for large </span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.185.1">datasets. </span><span class="koboSpan" id="kobo.185.2">It </span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.186.1">generates and uses random convolutional kernels to convert time series into </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">feature vectors.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.188.1">Shapelets</span></h3>
<p><span class="koboSpan" id="kobo.189.1">These are sub-sequences within the </span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.190.1">time series, which</span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.191.1"> are representative of class-specific patterns. </span><span class="koboSpan" id="kobo.191.2">By finding shapelets that match within the time series or another correlated time series, it is possible to classify the time series accordingly. </span><span class="koboSpan" id="kobo.191.3">This works well when the class-defining features are localized in time – for example, a sudden spike in transaction amount, which could correspond to a credit card theft. </span><span class="koboSpan" id="kobo.191.4">Shapelets can also help with interpretability—once it is well understood, a shapelet can be used to explain the time series at the points in time where </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">they match.</span></span></p>
<h3><span class="koboSpan" id="kobo.193.1">Ensembles</span></h3>
<p><span class="koboSpan" id="kobo.194.1">The</span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.195.1"> ensemble classification methods mentioned so far group a similar type of </span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.196.1">classifier. </span><span class="koboSpan" id="kobo.196.2">Another approach, such as the </span><strong class="bold"><span class="koboSpan" id="kobo.197.1">Hierarchical Vote Collective of Transformation-based Ensembles</span></strong><span class="koboSpan" id="kobo.198.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.199.1">HIVE-COTE</span></strong><span class="koboSpan" id="kobo.200.1">), is</span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.201.1"> with different types of classifiers. </span><span class="koboSpan" id="kobo.201.2">The idea is to have an ensemble based on different aspects of the time series as captured by the different nature of classifiers used. </span><span class="koboSpan" id="kobo.201.3">These</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.202.1"> are trained independently, and the predictions are combined based on a hierarchical voting method. </span><span class="koboSpan" id="kobo.202.2">As with other ensemble methods, this can yield greater robustness and accuracy. </span><span class="koboSpan" id="kobo.202.3">Also, due to</span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.203.1"> the various techniques used, HIVE-COTE is a good candidate for complex multi-pattern time series. </span><span class="koboSpan" id="kobo.203.2">This does, however, come with a high </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">compute cost.</span></span></p>
<h3><span class="koboSpan" id="kobo.205.1">Deep learning</span></h3>
<p><span class="koboSpan" id="kobo.206.1">Methods such as </span><strong class="bold"><span class="koboSpan" id="kobo.207.1">TimeNet</span></strong><span class="koboSpan" id="kobo.208.1"> leverage</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.209.1"> deep</span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.210.1"> neural networks to automatically extract </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.211.1">features, patterns, and relationships within time series. </span><span class="koboSpan" id="kobo.211.2">TimeNet is pre-trained, which makes it </span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.212.1">quickly usable. </span><span class="koboSpan" id="kobo.212.2">It combines layers of </span><strong class="bold"><span class="koboSpan" id="kobo.213.1">Convolutional Neural Networks</span></strong><span class="koboSpan" id="kobo.214.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.215.1">CNNs</span></strong><span class="koboSpan" id="kobo.216.1">) for local features and </span><strong class="bold"><span class="koboSpan" id="kobo.217.1">Recurrent Neural Networks</span></strong><span class="koboSpan" id="kobo.218.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.219.1">RNNs</span></strong><span class="koboSpan" id="kobo.220.1">) for sequential patterns. </span><span class="koboSpan" id="kobo.220.2">This allows TimeNet to capture both </span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.221.1">low-level and high-level patterns, effectively learning the hierarchical representation. </span><span class="koboSpan" id="kobo.221.2">The benefit is adaptability to various time series classification problems while limiting the need for manual feature engineering. </span><span class="koboSpan" id="kobo.221.3">The cons, similar to other deep learning approaches, are the high volume of data needed for pre-training, the computing resources required, and the lack of interpretability. </span><span class="koboSpan" id="kobo.221.4">Despite these, they are state-of-the-art in performance in several </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">complex cases.</span></span></p>
<p><span class="koboSpan" id="kobo.223.1">While we will not spend as much time on the classification of time series in this book as on forecasting, this is a promising area for further research, together with its own set of </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">operational challenges.</span></span></p>
<p><span class="koboSpan" id="kobo.225.1">This brings us to the final type of analysis, which is about detecting anomalies from time </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">series data.</span></span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.227.1">Anomaly detection</span></h2>
<p><span class="koboSpan" id="kobo.228.1">The third category of use</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.229.1"> cases for time series analysis is anomaly detection, which is </span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.230.1">about flagging unexpected patterns or occurrences. </span><span class="koboSpan" id="kobo.230.2">While this is related to pattern detection and forecasting, the purpose here is different: to identify an unexpected deviation in the behavior of the source system. </span><span class="koboSpan" id="kobo.230.3">Anomaly detection is crucial in various domains, such as finance, healthcare, and industrial systems. </span><span class="koboSpan" id="kobo.230.4">These anomalies can be indicative of critical incidents, such as system failures, financial </span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.231.1">fraud, or </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">network intrusions.</span></span></p>
<p><span class="koboSpan" id="kobo.233.1">In addition to being uni- or multivariate, an anomaly can be </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.235.1">Point</span></strong><span class="koboSpan" id="kobo.236.1">: This is the </span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.237.1">case when a single data point is identified as </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">an anomaly</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.239.1">Collective</span></strong><span class="koboSpan" id="kobo.240.1">: It can be that </span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.241.1">multiple data points as a group of near measurements are </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">all flagged</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.243.1">Contextual</span></strong><span class="koboSpan" id="kobo.244.1">: A </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.245.1">data point or a collective can be anomalous in the context of surrounding measurements, and the same point or collective may not be an issue in </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">another context</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.247.1">Anomalies can also be distinguished as outliers and novelties, where outliers may indicate errors or faults, and novelties are previously unseen patterns that may not </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">be problematic.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.249.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.250.1">For anomaly detection to work, the prior data preparation stage must keep the outliers in the dataset, which is the opposite of what is usually done during the data </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">curation process.</span></span></p>
<p><span class="koboSpan" id="kobo.252.1">In addition to</span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.253.1"> traditional statistical and rule-based approaches, there are newer machine learning techniques. </span><span class="koboSpan" id="kobo.253.2">The methods for anomaly detection in time series data can be categorized into unsupervised, supervised, and semi-supervised approaches, each with its own set of techniques and algorithms. </span><span class="koboSpan" id="kobo.253.3">An anomaly score is usually calculated with a threshold configured to </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">flag anomalies.</span></span></p>
<h3><span class="koboSpan" id="kobo.255.1">Unsupervised anomaly detection</span></h3>
<p><span class="koboSpan" id="kobo.256.1">Unsupervised anomaly detection</span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.257.1"> does </span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.258.1">not need labeled data. </span><span class="koboSpan" id="kobo.258.2">This assumes that anomalies are different enough from the normal that they can be detected without prior knowledge. </span><span class="koboSpan" id="kobo.258.3">Common methods include </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.260.1">Statistical</span></strong><span class="koboSpan" id="kobo.261.1"> methods</span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.262.1"> such </span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.263.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.264.1">z-score</span></strong><span class="koboSpan" id="kobo.265.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.266.1">box plot analysis</span></strong><span class="koboSpan" id="kobo.267.1"> are used to identify</span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.268.1"> outliers based on </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">statistical properties</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.270.1">Clustering-based</span></strong><span class="koboSpan" id="kobo.271.1"> methods </span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.272.1">such as </span><strong class="bold"><span class="koboSpan" id="kobo.273.1">DBSCAN</span></strong><span class="koboSpan" id="kobo.274.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.275.1">k-means</span></strong><span class="koboSpan" id="kobo.276.1"> cluster</span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.277.1"> similar data </span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.278.1">points, with anomalies as points that do not fit into </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">any cluster</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.280.1">Density-based</span></strong><span class="koboSpan" id="kobo.281.1"> methods </span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.282.1">such as kNN </span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.283.1">and </span><strong class="bold"><span class="koboSpan" id="kobo.284.1">Local Outlier Factor</span></strong><span class="koboSpan" id="kobo.285.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.286.1">LOF</span></strong><span class="koboSpan" id="kobo.287.1">) use the density of the local neighborhood to </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">identify anomalies</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.289.1">Isolation forest</span></strong><span class="koboSpan" id="kobo.290.1"> is a </span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.291.1">tree-based model that works well for high-dimensional data, </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">isolating </span></span><span class="No-Break"><a id="_idIndexMarker169"/></span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">anomalies</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.294.1">The chart in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.295.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.296.1">.3</span></em><span class="koboSpan" id="kobo.297.1"> shows the </span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.298.1">outcome of an isolation forest model, used to detect anomalies in a household’s </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">energy consumption.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<span class="koboSpan" id="kobo.300.1"><img alt="" src="image/B18568_02_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.301.1">Figure 2.3: Anomaly detection on energy consumption</span></p>
<p><span class="koboSpan" id="kobo.302.1">The model was </span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.303.1">fitted to consumption data up to November 9 and thereafter used on previously unseen data in the box. </span><span class="koboSpan" id="kobo.303.2">The anomalies are shown in red/paler color. </span><span class="koboSpan" id="kobo.303.3">We will run this example in the hands-on section of </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">this chapter.</span></span></p>
<h3><span class="koboSpan" id="kobo.305.1">Supervised anomaly detection</span></h3>
<p><span class="koboSpan" id="kobo.306.1">Supervised </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.307.1">anomaly detection </span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.308.1">works with a labeled dataset having both normal and anomalous cases. </span><span class="koboSpan" id="kobo.308.2">Better at detecting anomalies, it does, however, require labeled data, which can be difficult to obtain. </span><span class="koboSpan" id="kobo.308.3">Techniques include </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.310.1">Classification models</span></strong><span class="koboSpan" id="kobo.311.1"> such </span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.312.1">as traditional classifiers such as logistic regression, </span><strong class="bold"><span class="koboSpan" id="kobo.313.1">Support Vector Machines</span></strong><span class="koboSpan" id="kobo.314.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.315.1">SVMs</span></strong><span class="koboSpan" id="kobo.316.1">), or</span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.317.1"> more complex models such as CNNs and RNNs, trained to distinguish between normal and </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">anomalous instances</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.319.1">Ensemble methods</span></strong><span class="koboSpan" id="kobo.320.1"> such as</span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.321.1"> random forest or gradient boosting can be used with improved detection accuracy as they combine </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">multiple models</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.323.1">Semi-supervised anomaly detection</span></h3>
<p><span class="koboSpan" id="kobo.324.1">Semi-supervised </span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.325.1">anomaly detection </span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.326.1">needs a smaller amount of labeled data together with a larger set of unlabeled data. </span><span class="koboSpan" id="kobo.326.2">An example is the case of industrial monitoring when we have limited data points from sensor readings. </span><span class="koboSpan" id="kobo.326.3">These measurements correspond mostly to normal operations of the equipment and can be labeled as such. </span><span class="koboSpan" id="kobo.326.4">New readings falling out of the normal label can then be flagged </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">as anomalous.</span></span></p>
<p><span class="koboSpan" id="kobo.328.1">Also useful when labeling a large dataset is expensive, semi-supervised techniques include </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.330.1">Modifying unsupervised techniques to include the limited available labels – for example, modifying density-based or clustering methods for added sensitivity to </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">labeled anomalies.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.332.1">Novelty detection</span></strong><span class="koboSpan" id="kobo.333.1"> trains </span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.334.1">a model on the normal data to find its distribution, akin to unsupervised statistical approaches, and then deviation from this distribution is flagged. </span><span class="koboSpan" id="kobo.334.2">One-class SVMs and autoencoders are examples of techniques in </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">this case.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.336.1">Advanced deep learning methods</span></h3>
<p><span class="koboSpan" id="kobo.337.1">Methods using deep learning</span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.338.1"> techniques include </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.340.1">Autoencoders</span></strong><span class="koboSpan" id="kobo.341.1"> are </span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.342.1">neural networks that compress and then reconstruct the input data. </span><span class="koboSpan" id="kobo.342.2">The idea is that these models will reconstruct normal data well and have higher reconstruction errors with </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">anomalous data.</span></span></li>
<li><span class="koboSpan" id="kobo.344.1">Sequence types of models </span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.345.1">such as </span><strong class="bold"><span class="koboSpan" id="kobo.346.1">Long Short-Term Memory</span></strong><span class="koboSpan" id="kobo.347.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.348.1">LSTM</span></strong><span class="koboSpan" id="kobo.349.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.350.1">RNNs</span></strong><span class="koboSpan" id="kobo.351.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.352.1">Transformers</span></strong><span class="koboSpan" id="kobo.353.1"> identify temporal dependencies in</span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.354.1"> time series data, hence </span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.355.1">are useful for anomaly detection </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">in sequences.</span></span></li>
</ul>
<p class="callout-heading"><span class="koboSpan" id="kobo.357.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.358.1">From an operational point of view, an anomaly detection system is part of a comprehensive monitoring and alerting architecture. </span><span class="koboSpan" id="kobo.358.2">Note that Kalman filters are used in cases where low-latency or real-time detection and alerting </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">are required.</span></span></p>
<p><span class="koboSpan" id="kobo.360.1">This summarizes the various methods for anomaly detection. </span><span class="koboSpan" id="kobo.360.2">The choice of method for anomaly detection depends on the characteristics of the time series and anomalies that need to be detected, the availability of labeled data, the computational resources available, and the requirement for real-time detection. </span><span class="koboSpan" id="kobo.360.3">Hybrid and advanced methods, especially those based on deep learning, have shown promising results in various applications due to their ability to model complex patterns and dependencies in time </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">series data.</span></span></p>
<p><span class="koboSpan" id="kobo.362.1">Moving on from an overview of the landscape of time series analysis, let’s now consider how they are used and their impact in </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">various sectors.</span></span></p>
<h1 id="_idParaDest-51"><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.364.1">Industry-specific use cases</span></h1>
<p><span class="koboSpan" id="kobo.365.1">We looked at different types of time series</span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.366.1"> analysis in the </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.367.1">previous section. </span><span class="koboSpan" id="kobo.367.2">The question remains on their applicability across different sectors, which we will now dive into. </span><span class="koboSpan" id="kobo.367.3">But before we do that, the chart in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.368.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.369.1">.4</span></em><span class="koboSpan" id="kobo.370.1"> gives you a sense of the multitude of applications </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">across industries.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<span class="koboSpan" id="kobo.372.1"><img alt="" src="image/B18568_02_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.373.1">Figure 2.4: Application of time series analysis across industries</span></p>
<p><span class="koboSpan" id="kobo.374.1">Let’s look at the application of time series analysis in each of these sectors </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">in detail.</span></span></p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.376.1">Financial services</span></h2>
<p><span class="koboSpan" id="kobo.377.1">Time series analysis in financial services</span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.378.1"> is crucial for</span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.379.1"> understanding trends, patterns, and future behaviors. </span><span class="koboSpan" id="kobo.379.2">The applications are diverse, offering valuable insights for decision-making, strategic planning, risk management, and regulatory compliance. </span><span class="koboSpan" id="kobo.379.3">The following is how time series analysis is used across various functions within </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">financial services:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.381.1">Market analysis</span></strong><span class="koboSpan" id="kobo.382.1">: The future </span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.383.1">prices of assets are forecast by analyzing their historical prices, including trends and seasonality. </span><span class="koboSpan" id="kobo.383.2">This helps traders and investors decide on which asset to trade </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">and when.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.385.1">Risk management</span></strong><span class="koboSpan" id="kobo.386.1">: In addition to the </span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.387.1">preceding, another important aspect of financial instruments’ prices is their volatility, which needs to be analyzed to better manage risks and design mitigation strategies. </span><span class="koboSpan" id="kobo.387.2">This includes </span><strong class="bold"><span class="koboSpan" id="kobo.388.1">Value at Risk</span></strong><span class="koboSpan" id="kobo.389.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.390.1">VaR</span></strong><span class="koboSpan" id="kobo.391.1">) modeling, which</span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.392.1"> estimates the potential loss for an investment over a period based on historical volatility and correlations. </span><span class="koboSpan" id="kobo.392.2">Another area of risk management is credit risk, which covers time series data on repayment histories, defaults, and economic conditions. </span><span class="koboSpan" id="kobo.392.3">This helps assess the likelihood of future defaults and losses. </span><span class="koboSpan" id="kobo.392.4">The counter to this, estimation for provisioning, ensures enough funds are set aside to cover potential loan losses, while liquidity management ensures sufficient liquidity is maintained. </span><span class="koboSpan" id="kobo.392.5">Finally, at the macro-economic level, stress testing involves analyzing historical </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">worst-case scenarios.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.394.1">Portfolio management</span></strong><span class="koboSpan" id="kobo.395.1">: The two main </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.396.1">aspects here are optimizing asset allocation and related performance evaluation of the portfolio. </span><span class="koboSpan" id="kobo.396.2">By analyzing the historical returns and correlations between assets, portfolio managers can define the asset allocation to meet the desired risk-return profiles. </span><span class="koboSpan" id="kobo.396.3">Looking back at their performance over time, the portfolio can then be adjusted </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">as needed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.398.1">Algorithmic trading</span></strong><span class="koboSpan" id="kobo.399.1">: At its </span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.400.1">core, this </span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.401.1">involves using time series data on a microseconds or milliseconds scale to make high-frequency trading decisions. </span><span class="koboSpan" id="kobo.401.2">The complete cycle includes developing strategies, doing backtesting, and then, once strategies are in active use, generating the correct signals </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">for trading.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.403.1">Fraud detection</span></strong><span class="koboSpan" id="kobo.404.1">: The idea</span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.405.1"> here is to </span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.406.1">analyze transactions to identify and flag patterns that could indicate fraudulent activities, including market manipulation or </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">insider trading.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.408.1">Economic forecasting</span></strong><span class="koboSpan" id="kobo.409.1">: This is used to </span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.410.1">project, for example, interest rates and other economic indicators with an impact on policymaking by central banks, governments, and </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">financial institutions.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.412.1">In essence, time</span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.413.1"> series analysis in financial services is foundational to support a wide range of activities, from trading decisions to portfolio management and regulatory compliance. </span><span class="koboSpan" id="kobo.413.2">It leverages historical data to forecast future events, manage risks, and uncover valuable insights. </span><span class="koboSpan" id="kobo.413.3">Thus, time series analysis drives informed decision-making across the </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">financial ecosystem.</span></span></p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor053"/><span class="koboSpan" id="kobo.415.1">Retail</span></h2>
<p><span class="koboSpan" id="kobo.416.1">Time series analysis in the retail</span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.417.1"> industry uses chronological data for decision-making, to optimize operations, and to enhance customer experiences. </span><span class="koboSpan" id="kobo.417.2">Retailers can gain insights into trends, seasonal variations, and cyclical behaviors that affect their business. </span><span class="koboSpan" id="kobo.417.3">The following are some of the key </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">use cases:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.419.1">Sales forecasting and revenue prediction</span></strong><span class="koboSpan" id="kobo.420.1">: Predicting future sales based on historical data, considering seasonal</span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.421.1"> variations, trends, and </span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.422.1">external factors such as holidays and economic conditions, helps plan inventory, staffing, and marketing activities. </span><span class="koboSpan" id="kobo.422.2">This is crucial for financial planning and </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">investment decisions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.424.1">Inventory management and supply chain optimization</span></strong><span class="koboSpan" id="kobo.425.1">: Retailers can optimize stock levels by </span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.426.1">analyzing purchasing </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.427.1">patterns and lead times. </span><span class="koboSpan" id="kobo.427.2">Better demand planning and scheduling of replenishment minimizes stockouts and reduces excess inventory. </span><span class="koboSpan" id="kobo.427.3">Forecasting can be done at each product level, facilitating efficient inventory replenishment. </span><span class="koboSpan" id="kobo.427.4">This has also a positive impact on supply chain management. </span><span class="koboSpan" id="kobo.427.5">A related use case in retail that is growing in adoption is food waste prediction </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">and reduction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.429.1">Price optimization and marketing planning</span></strong><span class="koboSpan" id="kobo.430.1">: Optimal pricing strategies over time, maximizing</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.431.1"> sales and </span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.432.1">profits, can be determined by analyzing the impact of price changes on sales volumes. </span><span class="koboSpan" id="kobo.432.2">This includes insight into seasonal price sensitivities, impacts of promotions and marketing campaigns, and competitive pricing. </span><span class="koboSpan" id="kobo.432.3">This also optimizes marketing spend, while better aligning to </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">seasonal patterns.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.434.1">Customer behavior analysis and product life cycle management</span></strong><span class="koboSpan" id="kobo.435.1">: Retailers can inform marketing strategies</span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.436.1"> and product development by understanding changes in </span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.437.1">customer buying habits over time. </span><span class="koboSpan" id="kobo.437.2">This analysis helps identify trends, such as changes in purchasing channels or increasing interest in product categories. </span><span class="koboSpan" id="kobo.437.3">This, in turn, improves decisions about product introductions, discontinuation, or relaunches. </span><span class="koboSpan" id="kobo.437.4">With better customer behavior analysis, it is also possible to develop</span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.438.1"> effective loyalty projects, resulting in improved </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">customer retention.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.440.1">Store performance analysis and workforce planning</span></strong><span class="koboSpan" id="kobo.441.1">: By comparing the sales trends across different </span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.442.1">locations, this analysis</span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.443.1"> identifies high-performing stores and those needing help. </span><span class="koboSpan" id="kobo.443.2">This informs decisions about store expansions or closures and aligns staffing levels to the right store and busy times. </span><span class="koboSpan" id="kobo.443.3">The impact here is both on operational efficiency and improved </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">customer service.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.445.1">The use of</span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.446.1"> time series analysis in retail influences every aspect of the business, from inventory and pricing to marketing and workforce management. </span><span class="koboSpan" id="kobo.446.2">By leveraging historical data, retailers can make informed decisions that enhance operational efficiency, customer satisfaction, </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">and profitability.</span></span></p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.448.1">Healthcare</span></h2>
<p><span class="koboSpan" id="kobo.449.1">Time series analysis in healthcare</span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.450.1"> is an essential tool for tracking health-related data over chronological intervals. </span><span class="koboSpan" id="kobo.450.2">This method allows for the observation of patterns, trends, and changes in health metrics, which can be critical in improving patient care, operational efficiency, and clinical outcomes. </span><span class="koboSpan" id="kobo.450.3">Here’s an overview of applications </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">in healthcare:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.452.1">Patient monitoring</span></strong><span class="koboSpan" id="kobo.453.1">: In the</span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.454.1"> continuous monitoring of vital signs (heart rate, blood pressure, etc.), time series analysis can be used for real-time assessment of patient health and early detection of acute medical events. </span><span class="koboSpan" id="kobo.454.2">Similarly, by analyzing data from wearable devices, one can monitor trends in physical activity, sleep patterns, and other health indicators over time. </span><span class="koboSpan" id="kobo.454.3">This can be either in a clinical setting or for personal </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">health awareness.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.456.1">Epidemiology and disease surveillance</span></strong><span class="koboSpan" id="kobo.457.1">: The significance of this requirement was highlighted by </span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.458.1">COVID-19. </span><span class="koboSpan" id="kobo.458.2">The spread of infectious diseases must be tracked over time to understand transmission patterns, identify outbreaks, and plan public interventions accordingly. </span><span class="koboSpan" id="kobo.458.3">At the individual level, patients with a chronic disease may need to adjust their treatment plans based on the progression of the disease. </span><span class="koboSpan" id="kobo.458.4">Though similar, this is different from monitoring vitals in terms of the timescale </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">of interventions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.460.1">Management of hospital resources</span></strong><span class="koboSpan" id="kobo.461.1">: With most public hospitals stretched thin, it is a huge benefit to</span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.462.1"> be able to predict hospital admission rates in order to optimize bed allocation </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">and staffing.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.464.1">There are numerous other applications of time series analysis in healthcare, such as healthcare quality monitoring, drug development, and medical research, and with the COVID-19 pandemic, public </span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.465.1">health monitoring, analysis, </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">and policy-making.</span></span></p>
<p><span class="koboSpan" id="kobo.467.1">By using time series analysis, the healthcare sector can enhance patient care, improve operational efficiencies, and contribute to medical research, ultimately leading to better health outcomes and more informed </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">healthcare policies.</span></span></p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.469.1">Manufacturing and utilities</span></h2>
<p><span class="koboSpan" id="kobo.470.1">Time series analysis is important in the manufacturing and utilities</span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.471.1"> sectors for ensuring safety, optimizing operations, and improving efficiency. </span><span class="koboSpan" id="kobo.471.2">Here’s an overview of its use cases in </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">these industries:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.473.1">Manufacturing</span></strong><span class="koboSpan" id="kobo.474.1">: First, in </span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.475.1">terms of planning, with demand forecasting, scheduling production, and inventory management, time series help meet market demand without overproduction. </span><span class="koboSpan" id="kobo.475.2">Then, to keep production going, machine sensors send data to predictive maintenance models to generate warnings of potential failures, resulting in prompt maintenance. </span><span class="koboSpan" id="kobo.475.3">This can significantly reduce downtime while saving on unnecessary preventive maintenance. </span><span class="koboSpan" id="kobo.475.4">Anomaly detection further helps with early detection and limitation of </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">quality issues.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.477.1">Oil and gas</span></strong><span class="koboSpan" id="kobo.478.1">: As in</span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.479.1"> manufacturing, predictive maintenance and anomaly detection ensure reduced downtime, while maximizing output. </span><span class="koboSpan" id="kobo.479.2">Also, with the significant upfront investment required in infrastructure in this sector, it is crucial to have good forecasting of demand and prices to </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">guide planning.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.481.1">Utilities</span></strong><span class="koboSpan" id="kobo.482.1">: The </span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.483.1">primary use cases in the utilities sector are demand and load forecasting, which result in planning, grid management, and development. </span><span class="koboSpan" id="kobo.483.2">This further leads to optimal grid utilization, with improved customer service, while preventing outages. </span><span class="koboSpan" id="kobo.483.3">Finally, time series analysis and forecasting of new renewal energy sources ensures they </span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.484.1">can be optimally integrated into the overall </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">energy mix.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.486.1">In all these sectors, time series analysis contributes to resource optimization, cost reduction, and strategic planning, ultimately leading to more resilient and </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">efficient operations.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.488.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.489.1">A predominant and fast-growing source of time series data </span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.490.1">is </span><strong class="bold"><span class="koboSpan" id="kobo.491.1">Internet of Things</span></strong><span class="koboSpan" id="kobo.492.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.493.1">IoT</span></strong><span class="koboSpan" id="kobo.494.1">) devices and sensors. </span><span class="koboSpan" id="kobo.494.2">This is due to the explosion in the number of connected devices. </span><span class="koboSpan" id="kobo.494.3">This data is collected, stored, and analyzed – usually in real time – with use cases across industries, some of which have been discussed, from machine sensor data used for predictive maintenance to energy meter data to forecast consumption to health trackers, and </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">many more.</span></span></p>
<p><span class="koboSpan" id="kobo.496.1">This concludes the section on industry-specific use cases, on the many applications of time series analysis to different sectors of activity. </span><span class="koboSpan" id="kobo.496.2">The list is ever-expanding with frequent innovation on new use cases, driven by new analysis methods and new business requirements. </span><span class="koboSpan" id="kobo.496.3">Next, we will see some of the time series analysis methods in action with </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">industry-specific datasets.</span></span></p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.498.1">Hands-on with selected use cases</span></h1>
<p><span class="koboSpan" id="kobo.499.1">In this hands-on section, we will go through some selected use cases with </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">industry-specific datasets.</span></span></p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.501.1">Forecasting</span></h2>
<p><span class="koboSpan" id="kobo.502.1">For the forecasting use case, we started with </span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.503.1">an example on</span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.504.1"> temperatures in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.505.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.506.1">, where we loaded the dataset, analyzed its components, and visualized the result. </span><span class="koboSpan" id="kobo.506.2">The focus was on the past – that is, historical data. </span><span class="koboSpan" id="kobo.506.3">In the following steps, we will highlight the specific part of the code related to the future – that is, forecasting. </span><span class="koboSpan" id="kobo.506.4">This is based on the code in </span><strong class="source-inline"><span class="koboSpan" id="kobo.507.1">ts-spark_ch1_2fp</span></strong><span class="koboSpan" id="kobo.508.1">, which we imported into Databricks Community Edition in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.509.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.510.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.511.1">The </span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.512.1">forecasting </span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.513.1">steps are </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.515.1">Load the dataset, which was covered in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.516.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.517.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.518.1">Using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.519.1">Prophet</span></strong><span class="koboSpan" id="kobo.520.1"> library, the model is created and trained (fit) on </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">the data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.522.1">
model = </span><strong class="bold"><span class="koboSpan" id="kobo.523.1">Prophet</span></strong><span class="koboSpan" id="kobo.524.1">(
    n_changepoints=20, 
    yearly_seasonality=True,
    changepoint_prior_scale=0.001)
model.fit(df2_pd)</span></pre></li> <li><span class="koboSpan" id="kobo.525.1">We then use a very handy function in Prophet to generate future dates, </span><strong class="source-inline"><span class="koboSpan" id="kobo.526.1">make_future_dataframe</span></strong><span class="koboSpan" id="kobo.527.1">. </span><span class="koboSpan" id="kobo.527.2">These will be required as input, and passed as parameters, to do the prediction part, which is with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">predict</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.529.1"> function:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.530.1">
future_dates = model.</span><strong class="bold"><span class="koboSpan" id="kobo.531.1">make_future_dataframe</span></strong><span class="koboSpan" id="kobo.532.1">(
    periods=</span><strong class="bold"><span class="koboSpan" id="kobo.533.1">365</span></strong><span class="koboSpan" id="kobo.534.1">, freq=</span><strong class="bold"><span class="koboSpan" id="kobo.535.1">'D'</span></strong><span class="koboSpan" id="kobo.536.1">)
forecast = model.</span><strong class="bold"><span class="koboSpan" id="kobo.537.1">predict</span></strong><span class="koboSpan" id="kobo.538.1">(future_dates)</span></pre></li> <li><span class="koboSpan" id="kobo.539.1">The call to </span><strong class="source-inline"><span class="koboSpan" id="kobo.540.1">plot_plotly</span></strong><span class="koboSpan" id="kobo.541.1"> generates </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.542.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.543.1">.5</span></em><span class="koboSpan" id="kobo.544.1">. </span><span class="koboSpan" id="kobo.544.2">The right-most part of the graph does not have any collected data points as it is for the </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">forecasted dates:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.546.1">plot_plotly</span></strong><span class="koboSpan" id="kobo.547.1">(model, forecast, changepoints=</span><strong class="bold"><span class="koboSpan" id="kobo.548.1">True</span></strong><span class="koboSpan" id="kobo.549.1">)</span></pre></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer033">
<span class="koboSpan" id="kobo.550.1"><img alt="" src="image/B18568_02_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.551.1">Figure 2.5: Forecasting temperature</span></p>
<p><span class="koboSpan" id="kobo.552.1">This was </span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.553.1">a brief hands-on introduction to forecasting. </span><span class="koboSpan" id="kobo.553.2">We will be doing more </span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.554.1">forecasting, including with other libraries in addition to Prophet, in the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">this book.</span></span></p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/><span class="koboSpan" id="kobo.556.1">Pattern classification</span></h2>
<p><span class="koboSpan" id="kobo.557.1">For pattern classification, we will use</span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.558.1"> financial time series – more</span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.559.1"> specifically, share prices of technology companies. </span><span class="koboSpan" id="kobo.559.2">We will explore the use of two different open source libraries for DTW, </span><strong class="source-inline"><span class="koboSpan" id="kobo.560.1">fastdtw</span></strong><span class="koboSpan" id="kobo.561.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.562.1">dtw-python</span></strong><span class="koboSpan" id="kobo.563.1">. </span><span class="koboSpan" id="kobo.563.2">This is based on the code in </span><strong class="source-inline"><span class="koboSpan" id="kobo.564.1">ts-spark_ch2_1.dbc</span></strong><span class="koboSpan" id="kobo.565.1">, which we can import from the GitHub location for </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.566.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.567.1"> into Databricks Community Edition, as per the approach explained in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.568.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.569.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.570.1">The code URL is </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">as follows:</span></span></p>
<p><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch2/ts-spark_ch2_1.dbc"><span class="No-Break"><span class="koboSpan" id="kobo.572.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch2/ts-spark_ch2_1.dbc</span></span></a></p>
<p><span class="koboSpan" id="kobo.573.1">Let’s start with </span><strong class="source-inline"><span class="koboSpan" id="kobo.574.1">fastdtw</span></strong><span class="koboSpan" id="kobo.575.1"> with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">code steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.577.1">First, we import the </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">necessary libraries:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.579.1">import</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.580.1">yfinance</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.581.1">as</span></strong><span class="koboSpan" id="kobo.582.1"> yf
</span><strong class="bold"><span class="koboSpan" id="kobo.583.1">import</span></strong><span class="koboSpan" id="kobo.584.1"> numpy </span><strong class="bold"><span class="koboSpan" id="kobo.585.1">as</span></strong><span class="koboSpan" id="kobo.586.1"> np
</span><strong class="bold"><span class="koboSpan" id="kobo.587.1">from</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.588.1">fastdtw</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.589.1">import</span></strong><span class="koboSpan" id="kobo.590.1"> fastdtw
</span><strong class="bold"><span class="koboSpan" id="kobo.591.1">import</span></strong><span class="koboSpan" id="kobo.592.1"> plotly.express </span><strong class="bold"><span class="koboSpan" id="kobo.593.1">as</span></strong><span class="koboSpan" id="kobo.594.1"> px</span></pre></li> <li><span class="koboSpan" id="kobo.595.1">We’ll use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.596.1">yfinance</span></strong><span class="koboSpan" id="kobo.597.1"> library to download the share prices from Yahoo Finance for several technology companies for a </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">date range:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.599.1">
from_date = </span><strong class="bold"><span class="koboSpan" id="kobo.600.1">"2019-01-01"</span></strong><span class="koboSpan" id="kobo.601.1">
to_date = "2024-01-01"
yftickers = [
    "AAPL", "AMZN", "GOOG", "META",
    "MSFT", "NVDA", "PYPL", "TSLA"]
yfdata = {
    yftick: </span><strong class="bold"><span class="koboSpan" id="kobo.602.1">yf.download</span></strong><span class="koboSpan" id="kobo.603.1">(
        yftick, start=from_date, end=to_date, multi_level_index=False)[</span><strong class="bold"><span class="koboSpan" id="kobo.604.1">'Close'</span></strong><span class="koboSpan" id="kobo.605.1">].tolist() </span><strong class="bold"><span class="koboSpan" id="kobo.606.1">for</span></strong><span class="koboSpan" id="kobo.607.1"> yftick </span><strong class="bold"><span class="koboSpan" id="kobo.608.1">in</span></strong><span class="koboSpan" id="kobo.609.1"> yftickers}</span></pre></li> <li><span class="koboSpan" id="kobo.610.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.611.1">fastdtw</span></strong><span class="koboSpan" id="kobo.612.1"> library </span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.613.1">is used to calculate the </span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.614.1">DTW distances for each pair </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">of stocks:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.616.1">for</span></strong><span class="koboSpan" id="kobo.617.1"> i </span><strong class="bold"><span class="koboSpan" id="kobo.618.1">in</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.619.1">range</span></strong><span class="koboSpan" id="kobo.620.1">(num_tickers):
    </span><strong class="bold"><span class="koboSpan" id="kobo.621.1">for</span></strong><span class="koboSpan" id="kobo.622.1"> j </span><strong class="bold"><span class="koboSpan" id="kobo.623.1">in</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.624.1">range</span></strong><span class="koboSpan" id="kobo.625.1">(num_tickers):
        dtwdistance, </span><strong class="bold"><span class="koboSpan" id="kobo.626.1">_</span></strong><span class="koboSpan" id="kobo.627.1"> = </span><strong class="bold"><span class="koboSpan" id="kobo.628.1">fastdtw</span></strong><span class="koboSpan" id="kobo.629.1">(X[i], X[j])
        dtwmatrix[i, j] = </span><strong class="bold"><span class="koboSpan" id="kobo.630.1">float</span></strong><span class="koboSpan" id="kobo.631.1">(dtwdistance)</span></pre></li> <li><span class="koboSpan" id="kobo.632.1">We then plot the distance matrix using a heatmap with the </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.634.1">
fig = px.</span><strong class="bold"><span class="koboSpan" id="kobo.635.1">imshow</span></strong><span class="koboSpan" id="kobo.636.1">(
    dtwmatrix,
    labels=</span><strong class="bold"><span class="koboSpan" id="kobo.637.1">dict</span></strong><span class="koboSpan" id="kobo.638.1">(x=</span><strong class="bold"><span class="koboSpan" id="kobo.639.1">"Tickers"</span></strong><span class="koboSpan" id="kobo.640.1">, y=</span><strong class="bold"><span class="koboSpan" id="kobo.641.1">""</span></strong><span class="koboSpan" id="kobo.642.1">, color=</span><strong class="bold"><span class="koboSpan" id="kobo.643.1">"DTW distance"</span></strong><span class="koboSpan" id="kobo.644.1">),
    x=yftickers,
    y=yftickers
)
fig.update_xaxes(side=</span><strong class="bold"><span class="koboSpan" id="kobo.645.1">"top"</span></strong><span class="koboSpan" id="kobo.646.1">)
fig.show()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.647.1">This creates the visualization in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.648.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.649.1">.6</span></em><span class="koboSpan" id="kobo.650.1">, where the value of the DTW distance between AMZN and GOOG share prices is highlighted. </span><span class="koboSpan" id="kobo.650.2">Of the combination of shares analyzed, these two are the nearest to each other compared to the other </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">DTW distances.</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer034">
<span class="koboSpan" id="kobo.652.1"><img alt="" src="image/B18568_02_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.653.1">Figure 2.6: DTW distance heatmap</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.654.1">The line </span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.655.1">showing the DTW distance </span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.656.1">measurement between the two time series, AMZN and GOOG, is visualized in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.657.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.658.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.660.1">The plot of the time series for all the tickers is simply done with </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">the following:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.662.1">
fig = px.line(yfdata, y=yftickers)
fig.show()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.663.1">This creates </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.664.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.665.1">.7</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer035">
<span class="koboSpan" id="kobo.667.1"><img alt="" src="image/B18568_02_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.668.1">Figure 2.7: Selected technology share prices</span></p>
<ol>
<li value="6"><span class="koboSpan" id="kobo.669.1">We use the</span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.670.1"> following code with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.671.1">dtw-python</span></strong><span class="koboSpan" id="kobo.672.1"> library</span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.673.1"> to generate the alignment plot in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.674.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.675.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.677.1">
from d</span><strong class="bold"><span class="koboSpan" id="kobo.678.1">tw </span></strong><span class="koboSpan" id="kobo.679.1">import *
alignment = dtw(
    yfdata[</span><strong class="bold"><span class="koboSpan" id="kobo.680.1">'GOOG'</span></strong><span class="koboSpan" id="kobo.681.1">], yfdata[</span><strong class="bold"><span class="koboSpan" id="kobo.682.1">'AMZN'</span></strong><span class="koboSpan" id="kobo.683.1">], 
    keep_internals=True,
    step_pattern=rabinerJuangStepPattern</span><strong class="bold"><span class="koboSpan" id="kobo.684.1">(6</span></strong><span class="koboSpan" id="kobo.685.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.686.1">"c"</span></strong><span class="koboSpan" id="kobo.687.1">))
</span><strong class="bold"><span class="koboSpan" id="kobo.688.1">alignment.plot</span></strong><span class="koboSpan" id="kobo.689.1">(
    type=</span><strong class="bold"><span class="koboSpan" id="kobo.690.1">"twoway"</span></strong><span class="koboSpan" id="kobo.691.1">, offset=-2, 
    xlab=</span><strong class="bold"><span class="koboSpan" id="kobo.692.1">"time_index"</span></strong><span class="koboSpan" id="kobo.693.1">, ylab=</span><strong class="bold"><span class="koboSpan" id="kobo.694.1">"GOOG / AMZN"</span></strong><span class="koboSpan" id="kobo.695.1">)
alignment = dtw(
    yfdata[</span><strong class="bold"><span class="koboSpan" id="kobo.696.1">'GOOG'</span></strong><span class="koboSpan" id="kobo.697.1">], yfdata[</span><strong class="bold"><span class="koboSpan" id="kobo.698.1">'META'</span></strong><span class="koboSpan" id="kobo.699.1">], 
    keep_internals=True,
    step_pattern=rabinerJuangStepPattern(</span><strong class="bold"><span class="koboSpan" id="kobo.700.1">6</span></strong><span class="koboSpan" id="kobo.701.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.702.1">"c"</span></strong><span class="koboSpan" id="kobo.703.1">))
</span><strong class="bold"><span class="koboSpan" id="kobo.704.1">alignment.plot</span></strong><span class="koboSpan" id="kobo.705.1">(
    type=</span><strong class="bold"><span class="koboSpan" id="kobo.706.1">"twoway"</span></strong><span class="koboSpan" id="kobo.707.1">, offset=-2, 
    xlab=</span><strong class="bold"><span class="koboSpan" id="kobo.708.1">"time_index"</span></strong><span class="koboSpan" id="kobo.709.1">, ylab=</span><strong class="bold"><span class="koboSpan" id="kobo.710.1">"GOOG / META"</span></strong><span class="koboSpan" id="kobo.711.1">)</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.712.1">This concludes a brief hands-on introduction to pattern classification – more specifically, the initial step of distance calculation for the distance-based method using DTW, as applied to financial time series. </span><span class="koboSpan" id="kobo.712.2">Following this step, you can then apply the kNN </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">classification algorithm.</span></span></p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.714.1">Anomaly detection</span></h2>
<p><span class="koboSpan" id="kobo.715.1">In the final hands-on example </span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.716.1">of this chapter, we will explore an</span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.717.1"> anomaly detection use case applied to energy consumption for a household. </span><span class="koboSpan" id="kobo.717.2">This is based on the code in </span><strong class="source-inline"><span class="koboSpan" id="kobo.718.1">ts-spark_ch2_2.dbc</span></strong><span class="koboSpan" id="kobo.719.1">, and the dataset in </span><strong class="source-inline"><span class="koboSpan" id="kobo.720.1">ts-spark_ch2_ds2.csv</span></strong><span class="koboSpan" id="kobo.721.1">. </span><span class="koboSpan" id="kobo.721.2">We import the code into Databricks Community Edition, as per the approach explained in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.722.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.723.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.724.1">The code URL is as </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">follows: </span></span><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch2/ts-spark_ch2_2.dbc"><span class="No-Break"><span class="koboSpan" id="kobo.726.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch2/ts-spark_ch2_2.dbc</span></span></a></p>
<p><span class="koboSpan" id="kobo.727.1">The code </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">steps follow:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.729.1">First, we import the </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">necessary libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.731.1">
from pyspark import SparkFiles
from sklearn.ensemble import </span><strong class="bold"><span class="koboSpan" id="kobo.732.1">IsolationForest</span></strong><span class="koboSpan" id="kobo.733.1">
import plotly.express as px</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.734.1">As discussed earlier in this chapter, isolation forest is a tree-based model that can be used for </span><span class="No-Break"><span class="koboSpan" id="kobo.735.1">isolating anomalies.</span></span></p></li> <li><span class="koboSpan" id="kobo.736.1">Spark is used to read </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">the dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.738.1">
df = </span><strong class="bold"><span class="koboSpan" id="kobo.739.1">spark.read.csv</span></strong><span class="koboSpan" id="kobo.740.1">(
    "file:///" + SparkFiles.get(DATASET_FILE),
    header=True, sep=";", inferSchema=True
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.741.1">Note that this is a different yet equivalent syntax to </span><strong class="source-inline"><span class="koboSpan" id="kobo.742.1">spark.load()</span></strong><span class="koboSpan" id="kobo.743.1">, which we used in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.744.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.745.1">.</span></span></p></li> <li><span class="koboSpan" id="kobo.746.1">To enable calculations on the columns’ value, we need to change the data types of the columns from string </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">to double:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.748.1">
df = df.dropna() \
    .withColumn(
        </span><strong class="bold"><span class="koboSpan" id="kobo.749.1">'Global_active_power'</span></strong><span class="koboSpan" id="kobo.750.1">,
        df.Global_active_power.cast(</span><strong class="bold"><span class="koboSpan" id="kobo.751.1">'double'</span></strong><span class="koboSpan" id="kobo.752.1">)) \
    .withColumn(
        </span><strong class="bold"><span class="koboSpan" id="kobo.753.1">'Global_reactive_power'</span></strong><span class="koboSpan" id="kobo.754.1">, 
        df.Global_reactive_power.cast(</span><strong class="bold"><span class="koboSpan" id="kobo.755.1">'double'</span></strong><span class="koboSpan" id="kobo.756.1">)) \
    .withColumn(
        </span><strong class="bold"><span class="koboSpan" id="kobo.757.1">'Voltage'</span></strong><span class="koboSpan" id="kobo.758.1">, df.Voltage.cast(</span><strong class="bold"><span class="koboSpan" id="kobo.759.1">'double'</span></strong><span class="koboSpan" id="kobo.760.1">)) \
    .withColumn(
        'Global_intensity', 
        df.Global_intensity.</span><strong class="bold"><span class="koboSpan" id="kobo.761.1">cast</span></strong><span class="koboSpan" id="kobo.762.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.763.1">'double'</span></strong><span class="koboSpan" id="kobo.764.1">)) \</span></pre></li> <li><span class="koboSpan" id="kobo.765.1">We then choose the first part of the dataset to use for training </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">the model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.767.1">
df_train = df_pd.</span><strong class="bold"><span class="koboSpan" id="kobo.768.1">iloc</span></strong><span class="koboSpan" id="kobo.769.1">[:</span><strong class="bold"><span class="koboSpan" id="kobo.770.1">35000</span></strong><span class="koboSpan" id="kobo.771.1">,:]</span></pre></li> <li><span class="koboSpan" id="kobo.772.1">The Isolation Forest model can then be created and fitted to the </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">training dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.774.1">
isoforest_model = </span><strong class="bold"><span class="koboSpan" id="kobo.775.1">IsolationForest(</span></strong><span class="koboSpan" id="kobo.776.1">
    n_estimators=</span><strong class="bold"><span class="koboSpan" id="kobo.777.1">100</span></strong><span class="koboSpan" id="kobo.778.1">, 
    max_samples=</span><strong class="bold"><span class="koboSpan" id="kobo.779.1">'auto'</span></strong><span class="koboSpan" id="kobo.780.1">,
    contamination=float(</span><strong class="bold"><span class="koboSpan" id="kobo.781.1">0.0025</span></strong><span class="koboSpan" id="kobo.782.1">), 
    random_state=</span><strong class="bold"><span class="koboSpan" id="kobo.783.1">123</span></strong><span class="koboSpan" id="kobo.784.1">)
isoforest_model.</span><strong class="bold"><span class="koboSpan" id="kobo.785.1">fit</span></strong><span class="koboSpan" id="kobo.786.1">(feature_col_train)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.787.1">You can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.788.1">contamination</span></strong><span class="koboSpan" id="kobo.789.1"> level to specify the expected proportion of outliers in </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">the dataset.</span></span></p></li> <li><span class="koboSpan" id="kobo.791.1">The model can then be used to flag the anomalies in the </span><span class="No-Break"><span class="koboSpan" id="kobo.792.1">full dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.793.1">
df_pd[</span><strong class="bold"><span class="koboSpan" id="kobo.794.1">'anomaly_'</span></strong><span class="koboSpan" id="kobo.795.1">] = isoforest_model.</span><strong class="bold"><span class="koboSpan" id="kobo.796.1">predict</span></strong><span class="koboSpan" id="kobo.797.1">(feature_col)</span></pre></li> <li><span class="koboSpan" id="kobo.798.1">Finally, to show</span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.799.1"> the result</span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.800.1"> in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.801.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.802.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.804.1">
fig = px.</span><strong class="bold"><span class="koboSpan" id="kobo.805.1">scatter</span></strong><span class="koboSpan" id="kobo.806.1">(
    df_pd, x=</span><strong class="bold"><span class="koboSpan" id="kobo.807.1">'Date'</span></strong><span class="koboSpan" id="kobo.808.1">, y=feature_name,
    color=</span><strong class="bold"><span class="koboSpan" id="kobo.809.1">'anomaly_'</span></strong><span class="koboSpan" id="kobo.810.1">, 
    color_continuous_scale=px.colors.sequential.Bluered_r)
fig.update_traces(marker=</span><strong class="bold"><span class="koboSpan" id="kobo.811.1">dict</span></strong><span class="koboSpan" id="kobo.812.1">(size=</span><strong class="bold"><span class="koboSpan" id="kobo.813.1">3</span></strong><span class="koboSpan" id="kobo.814.1">))
fig.add_vrect(x0=df_train_lastdate, x1=df_lastdate)
fig.show()</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.815.1">This completes the hands-on introduction to anomaly detection using an energy consumption time series. </span><span class="koboSpan" id="kobo.815.2">As discussed in this chapter, Isolation Forest, used here, is just one of many </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">methods available.</span></span></p>
<h1 id="_idParaDest-60"><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.817.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.818.1">In this chapter, we focused on the practical significance of analyzing time series data for predictive modeling, trend identification, and anomaly detection. </span><span class="koboSpan" id="kobo.818.2">We viewed real-world applications across industries, highlighting the importance of time series analysis while getting some practice with two different </span><span class="No-Break"><span class="koboSpan" id="kobo.819.1">sector-specific datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.820.1">Before we can scale these and other use cases, we need an additional key component, Apache Spark, to which you will be introduced in the </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">next chapter.</span></span></p>
<h1 id="_idParaDest-61"><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.822.1">Further reading</span></h1>
<p><span class="koboSpan" id="kobo.823.1">This section serves as a repository of sources that can help you build on your understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">the topic:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.825.1">Time Series Analysis - Data, Methods, and Applications</span></em><span class="koboSpan" id="kobo.826.1">, edited by Chun-Kit </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">Ngan: </span></span><a href="https://www.intechopen.com/books/8362"><span class="No-Break"><span class="koboSpan" id="kobo.828.1">https://www.intechopen.com/books/8362</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.829.1">Financial services:</span></strong></span><ul><li><em class="italic"><span class="koboSpan" id="kobo.830.1">Essentials of Time Series for Financial Applications</span></em><span class="koboSpan" id="kobo.831.1"> by Massimo Guidolin and Manuela </span><span class="No-Break"><span class="koboSpan" id="kobo.832.1">Pedio: </span></span><a href="https://www.sciencedirect.com/book/9780128134092/essentials-of-time-series-for-financial-applications"><span class="No-Break"><span class="koboSpan" id="kobo.833.1">https://www.sciencedirect.com/book/9780128134092/essentials-of-time-series-for-financial-applications</span></span></a></li><li><em class="italic"><span class="koboSpan" id="kobo.834.1">Time-Series Forecasting Techniques for Banking Variables</span></em><span class="koboSpan" id="kobo.835.1"> by Arindam </span><span class="No-Break"><span class="koboSpan" id="kobo.836.1">Bandyopadhyay: </span></span><a href="https://academic.oup.com/book/43110/chapter-abstract/361614151?redirectedFrom=fulltext&amp;login=false"><span class="No-Break"><span class="koboSpan" id="kobo.837.1">https://academic.oup.com/book/43110/chapter-abstract/361614151?redirectedFrom=fulltext&amp;login=false</span></span></a></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.838.1">Retail:</span></strong></span><ul><li><em class="italic"><span class="koboSpan" id="kobo.839.1">A profit prediction model with time series analysis for retail store </span></em><span class="koboSpan" id="kobo.840.1">by Sridevi U. </span><span class="koboSpan" id="kobo.840.2">K. </span><span class="koboSpan" id="kobo.840.3">and Shanthi </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">P: </span></span><a href="https://www.researchgate.net/publication/325882164_A_profit_prediction_model_with_time_series_analysis_for_retail_store"><span class="No-Break"><span class="koboSpan" id="kobo.842.1">https://www.researchgate.net/publication/325882164_A_profit_prediction_model_with_time_series_analysis_for_retail_store</span></span></a></li><li><em class="italic"><span class="koboSpan" id="kobo.843.1">A Comparative Study on Forecasting of Retail Sales</span></em><span class="koboSpan" id="kobo.844.1"> (Hasan et al., </span><span class="No-Break"><span class="koboSpan" id="kobo.845.1">2022): </span></span><a href="https://arxiv.org/pdf/2203.06848.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.846.1">https://arxiv.org/pdf/2203.06848.pdf</span></span></a></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.847.1">Healthcare:</span></strong></span><ul><li><em class="italic"><span class="koboSpan" id="kobo.848.1">AI in Healthcare: Time-Series Forecasting Using Statistical, Neural, and Ensemble Architectures</span></em><span class="koboSpan" id="kobo.849.1"> (Kaushik et al., </span><span class="No-Break"><span class="koboSpan" id="kobo.850.1">2020): </span></span><a href="https://www.frontiersin.org/articles/10.3389/fdata.2020.00004/full"><span class="No-Break"><span class="koboSpan" id="kobo.851.1">https://www.frontiersin.org/articles/10.3389/fdata.2020.00004/full</span></span></a></li><li><em class="italic"><span class="koboSpan" id="kobo.852.1">Time Series Forecasting for Healthcare Diagnosis and Prognostics with the Focus on Cardiovascular Diseases</span></em><span class="koboSpan" id="kobo.853.1"> (Bui et al., </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">2018): </span></span><a href="https://www.researchgate.net/publication/320002542_Time_Series_Forecasting_for_Healthcare_Diagnosis_and_Prognostics_with_the_Focus_on_Cardiovascular_Diseases"><span class="No-Break"><span class="koboSpan" id="kobo.855.1">https://www.researchgate.net/publication/320002542_Time_Series_Forecasting_for_Healthcare_Diagnosis_and_Prognostics_with_the_Focus_on_Cardiovascular_Diseases</span></span></a></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.856.1">Manufacturing </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.857.1">and utilities:</span></strong></span><ul><li><em class="italic"><span class="koboSpan" id="kobo.858.1">Time Series Prediction in Industry 4.0: A Comprehensive Review and Prospects for Future Advancements</span></em><span class="koboSpan" id="kobo.859.1"> (Kashpruk et al., </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">2023): </span></span><a href="https://www.mdpi.com/2076-3417/13/22/12374"><span class="No-Break"><span class="koboSpan" id="kobo.861.1">https://www.mdpi.com/2076-3417/13/22/12374</span></span></a></li><li><em class="italic"><span class="koboSpan" id="kobo.862.1">Time-series pattern recognition in Smart Manufacturing Systems: A literature review and ontology</span></em><span class="koboSpan" id="kobo.863.1"> (Farahani et al., </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">2023): </span></span><a href="https://www.sciencedirect.com/science/article/pii/S0278612523000997"><span class="No-Break"><span class="koboSpan" id="kobo.865.1">https://www.sciencedirect.com/science/article/pii/S0278612523000997</span></span></a></li><li><em class="italic"><span class="koboSpan" id="kobo.866.1">Measuring the energy intensity of domestic activities from smart meter data</span></em><span class="koboSpan" id="kobo.867.1"> (Stankovic et al., </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">2016): </span></span><a href="https://www.sciencedirect.com/science/article/pii/S0306261916313897"><span class="No-Break"><span class="koboSpan" id="kobo.869.1">https://www.sciencedirect.com/science/article/pii/S0306261916313897</span></span></a></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.870.1">Libraries:</span></strong></span><ul><li><span class="No-Break"><span class="koboSpan" id="kobo.871.1">FastDTW: </span></span><a href="http://cs.fit.edu/~pkc/papers/tdm04.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.872.1">http://cs.fit.edu/~pkc/papers/tdm04.pdf</span></span></a></li><li><span class="No-Break"><span class="koboSpan" id="kobo.873.1">dtw-python: </span></span><a href="https://dynamictimewarping.github.io/python/"><span class="No-Break"><span class="koboSpan" id="kobo.874.1">https://dynamictimewarping.github.io/python/</span></span></a></li></ul></li>
</ul>
<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.875.1">Join our community on Discord</span></h1>
<p><span class="koboSpan" id="kobo.876.1">Join our community’s Discord space for discussions with the authors and </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">other readers:</span></span></p>
<p><a href="https://packt.link/ds"><span class="No-Break"><span class="koboSpan" id="kobo.878.1">https://packt.link/ds</span></span></a></p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<span class="koboSpan" id="kobo.879.1"><img alt="" src="image/ds_(1).jpg"/></span>
</div>
</div>
</div>
</body></html>