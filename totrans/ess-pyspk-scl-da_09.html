<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer044">
			<h1 id="_idParaDest-128"><a id="_idTextAnchor128"/>Chapter 7: Supervised Machine Learning</h1>
			<p>In the previous two chapters, you were introduced to the machine learning process, the various stages involved, and the first step of the process, namely <strong class="bold">feature engineering</strong>. Equipped with the fundamental knowledge of the machine learning process and with a usable set of machine learning features, you are ready to move on to the core part of the machine learning process, namely <strong class="bold">model training</strong>.</p>
			<p>In this chapter, you will be introduced to the <strong class="bold">supervised learning</strong> category of machine learning algorithms, where you will learn about <strong class="bold">parametric</strong> and <strong class="bold">non-parametric</strong> algorithms, as well as gain the knowledge required to solve <strong class="bold">regression</strong> and <strong class="bold">classification</strong> problems using machine learning. Finally, you will implement a few regression algorithms using the Spark machine learning library, such as <strong class="bold">linear regression</strong> and <strong class="bold">decision trees</strong>, and a few classification algorithms such as <strong class="bold">logistic regression</strong>, <strong class="bold">naïve Bayes</strong>, and <strong class="bold">support vector machines</strong>. <strong class="bold">Tree ensemble</strong> methods will also be presented, which can improve the performance and accuracy of decision trees. A few real-world applications of both regression and classification will also be presented to help you gain an appreciation of how machine learning can be leveraged in some day-to-day scenarios.</p>
			<p>The following main topics will be covered in this chapter:</p>
			<ul>
				<li>Introduction to supervised machine learning</li>
				<li>Regression</li>
				<li>Classification</li>
				<li>Tree ensembles</li>
				<li>Real-world supervised learning applications</li>
			</ul>
			<p>Toward the end of this chapter, you should have gained sufficient knowledge and the skills required for building your own regression and classification models at scale using Spark MLlib.</p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor129"/>Technical requirements</h1>
			<p>In this chapter, we will be using Databricks Community Edition to run our code (<a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a>). </p>
			<ul>
				<li>Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. </li>
				<li>The code for this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter07">https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter07</a>. </li>
				<li>The datasets for this chapter can be found at <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data">https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data</a>.</li>
			</ul>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor130"/>Introduction to supervised machine learning</h1>
			<p>A machine learning problem can be considered as a process where an unknown variable is derived from a set of known variables using a mathematical or statistical function. The difference here is that a machine learning algorithm learns the mapping function from a given dataset.</p>
			<p>Supervised learning is a class of machine learning algorithms where a model is trained on a dataset and <a id="_idIndexMarker545"/>the outcome for each set of inputs is already known. This is known as supervised learning as the algorithm here behaves like a teacher, guiding the training process until the desired level of model performance is achieved. Supervised learning requires data that is already labeled. Supervised learning algorithms can be further classified as parametric and non-parametric algorithms. We will look at these in the following sections.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor131"/>Parametric machine learning</h2>
			<p>A machine learning algorithm that simplifies the learning process by summarizing the data with a fixed set <a id="_idIndexMarker546"/>of parameters is called a parametric learning algorithm. It achieves this by assuming a known form for the learning function and learning the coefficients of the linear function from the given dataset. The assumed form of the learning function is usually a linear function or an algebraic <a id="_idIndexMarker547"/>equation describing a straight line. Thus, parametric learning functions are also known as linear machine learning algorithms.</p>
			<p>One important property of parametric learning algorithms is that the number of parameters needed for the linear learning function is independent of the input training dataset. This <a id="_idIndexMarker548"/>greatly simplifies the learning <a id="_idIndexMarker549"/>process and makes it relatively faster. One disadvantage here is that the underlying learning function for the given dataset might not necessarily be a straight line, hence oversimplifying the learned model. However, most practical machine learning algorithms are parametric learning algorithms, such as linear regression, logistic regression, and naïve Bayes.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor132"/>Non-parametric machine learning</h2>
			<p>Non-parametric learning algorithms do not make any assumptions regarding the form of the learning function. These <a id="_idIndexMarker550"/>algorithms make the best use of the training dataset by learning a mapping function, while still maintaining the ability to conform to unseen data. This means <a id="_idIndexMarker551"/>that non-parametric learning algorithms can learn from a wider variety of learning functions. The advantage of these algorithms is that that they are flexible and yield better performing models, while the disadvantages are that they usually require more data to learn, have relatively slow training times, and may sometimes lead to model overfitting. Some examples of non-parametric learning algorithms include K-nearest neighbors, decision trees, and support vector machines.</p>
			<p>Supervised learning algorithms have two major applications, namely regression and classification. We will explore these in the following sections.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor133"/>Regression</h1>
			<p>Regression is a supervised learning technique that helps us learn the correlation between a <a id="_idIndexMarker552"/>continuous output parameter <a id="_idIndexMarker553"/>called <strong class="bold">Label</strong> and a set of input <a id="_idIndexMarker554"/>parameters called <strong class="bold">Features</strong>. Regression produces machine learning models that predict <a id="_idIndexMarker555"/>a continuous label, given a feature vector. The concept of regression can be best explained using the following diagram:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="Images/B16736_07_01.jpg" alt="Figure 7.1 – Linear regression&#13;&#10;" width="538" height="447"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Linear regression</p>
			<p>In the preceding diagram, the scatterplot represents data points spread across a two-dimensional space. The linear regression algorithm, being a parametric learning algorithm, assumes that <a id="_idIndexMarker556"/>the learning function will have a linear form. Thus, it learns the coefficients that are required to represent a straight line that approximately fits the data points on the scatterplot.</p>
			<p>Spark MLlib has distributed and scalable implementations of a few prominent regression algorithms, such as linear regression, decision trees, random forests, and gradient boosted trees. In the following sections, we will implement a few of these regression algorithms using Spark MLlib.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor134"/>Linear regression</h2>
			<p>In the previous chapters, we cleaned, integrated, and curated a dataset containing online retail sales <a id="_idIndexMarker557"/>transactions<a id="_idIndexMarker558"/> by customers and captured their demographic <a id="_idIndexMarker559"/>information in the same integrated dataset. In <a href="B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Feature Engineering – Extraction, Transformation, and Selection</em>, we also converted the <a id="_idIndexMarker560"/>pre-processed data into a feature vector that's ready for machine learning training and stored it in <strong class="bold">Delta Lake</strong> as our offline <strong class="bold">feature store</strong>. Let's make use of this feature-engineered dataset to train a regression algorithm that can find out <a id="_idIndexMarker561"/>the age of a customer <a id="_idIndexMarker562"/>by providing other features as parameters, as shown in the following code block:</p>
			<p class="source-code">from pyspark.ml.regression import LinearRegression</p>
			<p class="source-code">retail_features = spark.read.table("retail_features")</p>
			<p class="source-code">train_df = retail_features.selectExpr("cust_age as label", "selected_features as features")</p>
			<p class="source-code">lr = LinearRegression(maxIter=10, regParam=0.3, </p>
			<p class="source-code">                      elasticNetParam=0.8)</p>
			<p class="source-code">lr_model = lr.fit(train_df)</p>
			<p class="source-code">print("Coefficients: %s" % str(lr_model.coefficients))</p>
			<p class="source-code">print("Intercept: %s" % str(lr_model.intercept))</p>
			<p class="source-code">summary = lr_model.summary</p>
			<p class="source-code">print("RMSE: %f" % summary.rootMeanSquaredError)</p>
			<p class="source-code">print("r2: %f" % summary.r2)</p>
			<p>In the preceding block of code, we have done the following:</p>
			<ol>
				<li>First, we imported the <strong class="source-inline">LinearRegression</strong> algorithm from Spark MLlib.</li>
				<li>The retail features were loaded from a Delta table and loaded into a Spark DataFrame.</li>
				<li>We only needed the feature vector and the label column for training a <strong class="source-inline">LinearRegression</strong> model, so we only selected these two columns in the training DataFrame.</li>
				<li>Then, we initialized a <strong class="source-inline">LinearRegression</strong> transformer by specifying the hyperparameters required by this algorithm.</li>
				<li>After, we called the <strong class="source-inline">fit</strong> method on the training dataset to start the training process, which starts a Spark job behind the scenes that carries out the training task in a distributed manner.</li>
				<li>Once the <a id="_idIndexMarker563"/>model has been successfully trained, we printed the model training summary, including the learned coefficients of the linear learning function and the intercept.</li>
				<li>We also <a id="_idIndexMarker564"/>displayed the model accuracy metrics, such as the RMSE and R-squared metrics. It is generally desirable to get a model with as lower an RMSE as possible.</li>
			</ol>
			<p>Thus, utilizing Spark MLlib's distributed implementation of linear regression, you can train a regression model in a distributed manner on a large dataset, without having to deal with any of the underlying complexities of distributed computing. The model can then be applied to a new set of data to generate predictions. Spark MLlib models can also be persisted to disk or a data lake using built-in methods and then reused later.</p>
			<p>Now that we have trained a simple linear regression model using a parametric learning algorithm, let's look at using a non-parametric learning algorithm to solve the same regression problem.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor135"/>Regression using decision trees</h2>
			<p>Decision trees are a popular form of non-parametric learning algorithm for solving both regression and classification machine learning problems. Decision trees are popular because they <a id="_idIndexMarker565"/>are easy to use, they can handle a wide variety of categorical as well as continuous features, and are also easy to interpret and explain.</p>
			<p>Spark MLlib's implementation of decision trees allows for distributed training by partitioning data by rows. Since non-parametric learning algorithms typically require large amounts of data, Spark's implementation of decision trees can scale to a very large number of rows, even millions or billions.</p>
			<p>Let's train a decision tree to predict the age of a customer while using other online retail transactional features as input, as shown in the following code block:</p>
			<p class="source-code">from pyspark.ml.evaluation import RegressionEvaluator</p>
			<p class="source-code">from pyspark.ml.regression import DecisionTreeRegressor</p>
			<p class="source-code">retail_features = spark.read.table("retail_features").selectExpr("cust_age as label", </p>
			<p class="source-code">           "selected_features as features")</p>
			<p class="source-code">(train_df, test_df) = retail_features.randomSplit([0.8, 0.2])</p>
			<p class="source-code">dtree = DecisionTreeRegressor(featuresCol="features")</p>
			<p class="source-code">model = dtree.fit(train_df)</p>
			<p class="source-code">predictions = model.transform(test_df)</p>
			<p class="source-code">evaluator = RegressionEvaluator(</p>
			<p class="source-code">    labelCol="label", predictionCol="prediction", </p>
			<p class="source-code">    metricName="rmse")</p>
			<p class="source-code">rmse = evaluator.evaluate(predictions)</p>
			<p class="source-code">print("RMSE for test data = %g" % rmse)</p>
			<p class="source-code">print(model.toDebugString)</p>
			<p>In the preceding code snippet, we have done the following:</p>
			<ol>
				<li value="1">First, we imported the <strong class="source-inline">DecisionTreeRegressor</strong> Spark ML library, along with a utility <a id="_idIndexMarker566"/>method to help <a id="_idIndexMarker567"/>evaluate the accuracy of the trained model.</li>
				<li>We loaded our feature vector dataset from Delta Lake into a Spark DataFrame and only selected the feature and label columns.</li>
				<li>To be able to evaluate our model accuracy after the training process, we needed a dataset that wouldn't be used for training. Thus, we split our dataset into two sets for training and testing, respectively. We used 80% of the data for model training while preserving 20% for model evaluation.</li>
				<li>Then, we initialized the <strong class="source-inline">DecisionTreeRegressor</strong> class with the required hyperparameters, resulting in a <strong class="source-inline">Transformer</strong> object.</li>
				<li>We fit the <strong class="source-inline">DecisionTreeRegressor</strong> transformer to our training dataset, which resulted in a decision tree model estimator.</li>
				<li>We applied <a id="_idIndexMarker568"/>the model's <strong class="source-inline">Estimator</strong> object to the test dataset to produce actual predictions.</li>
				<li>This prediction <a id="_idIndexMarker569"/>DataFrame was then used along with the <strong class="source-inline">RegressionEvaluator</strong> utility method to derive the RMSE of the model, which can be used to evaluate the accuracy of the trained model.</li>
			</ol>
			<p>By using Spark MLlib's built-in decision tree regression algorithms, we can train regressions models in a distributed fashion on very large amounts of data, in a very fast and efficient manner. One thing to note is that the RMSE value of both regression models is about the same. These models can be tuned further using model tuning techniques, which help improve their accuracy. You will learn more about model tuning in <a href="B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164"><em class="italic">Chapter 9</em></a>, <em class="italic">Machine Learning Life Cycle Management</em>.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor136"/>Classification</h1>
			<p>Classification is another type of supervised learning technique, where the task is to categorize a <a id="_idIndexMarker570"/>given dataset <a id="_idIndexMarker571"/>into different classes. Machine learning classifiers learn a mapping function from input <a id="_idIndexMarker572"/>parameters called <strong class="bold">Features</strong> that go to a discreet output parameter called <strong class="bold">Label</strong>. Here, the learning function tries to predict whether <a id="_idIndexMarker573"/>the label belongs to one of several known classes. The following diagram depicts the concept of classification:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="Images/B16736_07_02.jpg" alt="Figure 7.2 – Logistic regression&#13;&#10;" width="557" height="464"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Logistic regression</p>
			<p>In the preceding diagram, a logistic regression algorithm is learning a mapping function that divides the data points in a two-dimensional space into two distinct classes. The learning algorithm learns the coefficients of a <strong class="bold">Sigmoid function</strong>, which classifies a set of input parameters <a id="_idIndexMarker574"/>into one of two possible classes. This type of classification can be split into two distinct classes. This is known as <strong class="bold">binary classification</strong> or <strong class="bold">binomial classification</strong>.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor137"/>Logistic regression</h2>
			<p>Logistic regression is<a id="_idIndexMarker575"/> a popular classification algorithm that can learn a model from labeled data to predict the class of an output variable. Spark MLlib's implementation of logistic regression supports both binomial and multinomial classification problems.</p>
			<h3>Binomial classification</h3>
			<p>Binomial classification or binary classification is where the learning algorithm needs to classify whether <a id="_idIndexMarker576"/>the output variable belongs to one of two possible outcomes. Building on the example from previous sections, let's train a model using logistic regression that tries to predict the gender of a customer, given other features from an online retail transaction. Let's see how we can implement this using Spark MLlib, as shown in the following code example:</p>
			<p class="source-code">from pyspark.ml import Pipeline</p>
			<p class="source-code">from pyspark.ml.feature import StringIndexer</p>
			<p class="source-code">from pyspark.ml.classification import LogisticRegression</p>
			<p class="source-code">train_df = spark.read.table("retail_features").selectExpr("gender", "selected_features as features")</p>
			<p class="source-code">string_indexer = StringIndexer(inputCol="gender", </p>
			<p class="source-code">                               outputCol="label", </p>
			<p class="source-code">                               handleInvalid="skip" )</p>
			<p class="source-code">lr = LogisticRegression(maxIter=10, regParam=0.9, </p>
			<p class="source-code">                        elasticNetParam=0.6)</p>
			<p class="source-code">pipeline = Pipeline(stages=[string_indexer, lr])</p>
			<p class="source-code">model = pipeline.fit(train_df)</p>
			<p class="source-code">lr_model = model.stages[1]</p>
			<p class="source-code">print("Coefficients: " + str(lr_model.coefficientMatrix))</p>
			<p class="source-code">print("Intercepts: " + str(lr_model.interceptVector))</p>
			<p class="source-code">summary.roc.show()</p>
			<p class="source-code">print("areaUnderROC: " + str(summary.areaUnderROC))</p>
			<p>In the preceding block of code, we have done the following:</p>
			<ol>
				<li value="1">The gender <a id="_idIndexMarker577"/>in our training dataset is a string data type, so it needs to be converted into numeric <a id="_idIndexMarker578"/>format first. For this, we made use of <strong class="source-inline">StringIndexer</strong> to convert it into a numeric label column.</li>
				<li>Then, we initialized a <strong class="source-inline">LogisticRegression</strong> class by specifying the hyperparameters required by this algorithm.</li>
				<li>Then, we stitched the <strong class="source-inline">StringIndexer</strong> and <strong class="source-inline">LogisticRegression</strong> stages together into a pipeline.</li>
				<li>Then, we called the <strong class="source-inline">fit</strong> method on the training dataset to start the training process using the pipeline's <strong class="source-inline">Transformer</strong> object.</li>
				<li>Once the <a id="_idIndexMarker579"/>model had been successfully trained, we printed the model's coefficients and intercepts, along with the receiver-operating characteristic and the area under the ROC curve <a id="_idIndexMarker580"/>metric, to measure the accuracy of the trained model.</li>
			</ol>
			<p>With that, we have seen how, using the logistic regression algorithm from the Spark machine learning library, to implement binary classification in a scalable manner.</p>
			<h3>Multinomial classification</h3>
			<p>In <strong class="bold">multinomial classification</strong>, the learning algorithm needs to predict more than two possible outcomes. Let's extend the example from the previous section to build a model that, using logistic <a id="_idIndexMarker581"/>regression, tries to predict the country of origin of a customer, given other features from an online retail transaction, as shown in the following code snippet:</p>
			<p class="source-code">train_df = spark.read.table("retail_features").selectExpr("country_indexed as label", "selected_features as features")</p>
			<p class="source-code">mlr = LogisticRegression(maxIter=10, regParam=0.5, </p>
			<p class="source-code">                         elasticNetParam=0.3, </p>
			<p class="source-code">                         family="multinomial")</p>
			<p class="source-code">mlr_model = mlr.fit(train_df)</p>
			<p class="source-code">print("Coefficients: " + str(mlr_model.coefficientMatrix))</p>
			<p class="source-code">print("Intercepts: " + str(mlr_model.interceptVector))</p>
			<p class="source-code">print("areaUnderROC: " + str(summary.areaUnderROC))</p>
			<p class="source-code">summary.roc.show()</p>
			<p>The previous code snippet is almost the same as the binary classification example, except the label <a id="_idIndexMarker582"/>column has more than two possible values and we specified the family parameter for the <strong class="source-inline">LogisticRegression</strong> class as <strong class="source-inline">multinomial</strong>. Once the model has been trained, the receiver-operating characteristics of the model and the area under the ROC curve metric can be displayed to measure the model's accuracy.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor138"/>Classification using decision trees</h2>
			<p>Spark MLlib comes with a <strong class="source-inline">DecsionTreeClassifier</strong> class to solve classification problems <a id="_idIndexMarker583"/>as well. In the following <a id="_idIndexMarker584"/>code, we will implement binary classification using decision trees:</p>
			<p class="source-code">retail_df = spark.read.table("retail_features").selectExpr("gender", "selected_features as features")</p>
			<p class="source-code">(train_df, test_df) = retail_df.randomSplit([0.8, 0.2])</p>
			<p class="source-code">string_indexer = StringIndexer(inputCol="gender", </p>
			<p class="source-code">                               outputCol="label", </p>
			<p class="source-code">                               handleInvalid="skip" )</p>
			<p class="source-code">dtree = DecisionTreeClassifier(labelCol="label", </p>
			<p class="source-code">                               featuresCol="features")</p>
			<p class="source-code">pipeline = Pipeline(stages=[string_indexer, dtree])</p>
			<p class="source-code">model = pipeline.fit(train_df)</p>
			<p class="source-code">predictions = model.transform(test_df)</p>
			<p class="source-code">evaluator = MulticlassClassificationEvaluator(</p>
			<p class="source-code">    labelCol="label", predictionCol="prediction", </p>
			<p class="source-code">    metricName="accuracy")</p>
			<p class="source-code">accuracy = evaluator.evaluate(predictions)</p>
			<p class="source-code">print("Accuracy = %g " % (accuracy))</p>
			<p class="source-code">dtree_model = model.stages[1]</p>
			<p class="source-code">#print(dtree_model.toDebugString)</p>
			<p>In the previous block of code, we did the following:</p>
			<ol>
				<li value="1">First, we split our dataset into two sets for training and testing. This allows us to evaluate <a id="_idIndexMarker585"/>the model's accuracy once it has been trained.</li>
				<li>Then, we made use of <strong class="source-inline">StringIndexer</strong> to convert the gender string column into a numeric label column.</li>
				<li>After, we <a id="_idIndexMarker586"/>initialized a <strong class="source-inline">DecsionTreeClassifier</strong> class with the required hyperparameters.</li>
				<li>Then, we combined the <strong class="source-inline">StringIndexer</strong> and <strong class="source-inline">DecisionTreeClassifier</strong> stages into a pipeline.</li>
				<li>After, we called the <strong class="source-inline">fit</strong> method on the training dataset to start the model training process and applied the model's <strong class="source-inline">Estimator</strong> object on the test dataset to calculate predictions.</li>
				<li>Finally, we used this DataFrame, along with the <strong class="source-inline">MulticlassClassificationEvaluator</strong> utility method, to derive the accuracy of the trained model.</li>
			</ol>
			<p>This way, we have seen how the Spark machine learning library's decision trees can be used to solve classification problems at scale.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor139"/>Naïve Bayes</h2>
			<p>Naïve Bayes is a family of probabilistic classification algorithms based on the Bayes' theorem, which assumes independence among features that are used as input to the learning algorithm. Bayes' theorem can be used to predict the probability of an event happening, given <a id="_idIndexMarker587"/>that another event has already taken place. The naïve Bayes algorithm calculates the probability of the given set of input features for all possible values of the category of the output label, and then it picks the output with the maximum probability. Naïve Bayes can used for binomial as well as multinomial classification problems. Let's see how naïve Bayes can be implemented using Spark MLlib, as shown in the following code example:</p>
			<p class="source-code">from pyspark.ml.classification import NaiveBayes</p>
			<p class="source-code">from pyspark.ml.evaluation import MulticlassClassificationEvaluator</p>
			<p class="source-code">retail_df = spark.read.table("retail_features").selectExpr("gender", "selected_features as features")</p>
			<p class="source-code">(train_df, test_df) = retail_df.randomSplit([0.8, 0.2])</p>
			<p class="source-code">string_indexer = StringIndexer(inputCol="gender", </p>
			<p class="source-code">                               outputCol="label", </p>
			<p class="source-code">                               handleInvalid="skip" )</p>
			<p class="source-code">nb = NaiveBayes(smoothing=0.9, modelType="gaussian")</p>
			<p class="source-code">pipeline = Pipeline(stages=[string_indexer, nb])</p>
			<p class="source-code">model = pipeline.fit(train_df)</p>
			<p class="source-code">predictions = model.transform(test_df)</p>
			<p class="source-code">evaluator = MulticlassClassificationEvaluator(</p>
			<p class="source-code">    labelCol="label",</p>
			<p class="source-code">    predictionCol="prediction",</p>
			<p class="source-code">    metricName="accuracy")</p>
			<p class="source-code">accuracy = evaluator.evaluate(predictions)</p>
			<p class="source-code">print("Model accuracy = %f" % accuracy)</p>
			<p>In the preceding <a id="_idIndexMarker588"/>block of code, we did the following:</p>
			<ol>
				<li value="1">First, we split our dataset into two sets for training and testing, respectively.</li>
				<li>Then, we <a id="_idIndexMarker589"/>made use of <strong class="source-inline">StringIndexer</strong> to convert the gender string column into a numeric label column.</li>
				<li>After, we initialized a <strong class="source-inline">NaiveBayes</strong> class with the required hyperparameters.</li>
				<li>Then we combined the <strong class="source-inline">StringIndexer</strong> and <strong class="source-inline">NaiveBayes</strong> stages into a pipeline.</li>
				<li>After, we called the <strong class="source-inline">fit</strong> method on the training dataset to start the model training process, which applied the model's <strong class="source-inline">Estimator</strong> object to the test dataset to calculate predictions.</li>
				<li>This DataFrame was then used with the <strong class="source-inline">MulticlassClassificationEvaluator</strong> utility method to derive the accuracy of the trained model.<p class="callout-heading">Note</p><p class="callout">Multinomial and Bernoulli naïve Bayes models require non-negative features. Thus, it is recommended to select only features with positive values or to use another classification algorithm that can handle features with non-negative values.</p></li>
			</ol>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>Support vector machines</h2>
			<p><strong class="bold">Support vector machines</strong> (<strong class="bold">SVM</strong>) is a class of classification algorithms that takes data points as input and outputs the hyperplane that best separates the given data points into two <a id="_idIndexMarker590"/>separate classes, represented on a two-dimensional plane. Thus, SVM supports binomial classification problems only. Let's implement binary classification using Spark MLlib's implementation of SVM, as shown in the following code block:</p>
			<p class="source-code">from pyspark.ml.classification import LinearSVC</p>
			<p class="source-code">train_df = spark.read.table("retail_features").selectExpr("gender", "selected_features as features")</p>
			<p class="source-code">string_indexer = StringIndexer(inputCol="gender",</p>
			<p class="source-code">                               outputCol="label", </p>
			<p class="source-code">                               handleInvalid="skip" )</p>
			<p class="source-code">svm = LinearSVC(maxIter=10, regParam=0.1)</p>
			<p class="source-code">pipeline = Pipeline(stages=[string_indexer, svm])</p>
			<p class="source-code">model = pipeline.fit(train_df)</p>
			<p class="source-code">svm_model = model.stages[1]</p>
			<p class="source-code"># Print the coefficients and intercept for linear SVC</p>
			<p class="source-code">print("Coefficients: " + str(svm_model.coefficients))</p>
			<p class="source-code">print("Intercept: " + str(svm_model.intercept))</p>
			<p>In the preceding<a id="_idIndexMarker591"/> code block, we did the following:</p>
			<ol>
				<li value="1">First, we made use of <strong class="source-inline">StringIndexer</strong> to convert the gender column into a numeric label column.</li>
				<li>Then, we initialized a <strong class="source-inline">LinearSVC</strong> class by specifying the hyperparameters required by this algorithm.</li>
				<li>Then, we combined the <strong class="source-inline">StringIndexer</strong> and <strong class="source-inline">LinearSVC</strong> stages into a pipeline.</li>
				<li>After, we called the <strong class="source-inline">fit</strong> method on the training dataset to start the training process using the pipeline's <strong class="source-inline">Transformer</strong> object.</li>
				<li>Once the model had been successfully trained, we printed the model coefficients and intercepts.</li>
			</ol>
			<p>So far, you have learned about the most popular supervised learning algorithms for solving regression and classification problems and saw their implementation details in Spark MLlib using working code examples. In the following section, you will be introduced to the concept of tree ensembles and how they can be used to combine multiple decision tree models to arrive at the best possible model.</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor141"/>Tree ensembles</h1>
			<p>Non-parametric<a id="_idIndexMarker592"/> learning algorithms such as decision trees do not make any assumptions on the form of the learning function being learned and try to fit a model to the data at hand. However, decision trees run the risk of overfitting training data. Tree ensemble methods are a great way to leverage the benefits of decision trees while minimizing the risk of overfitting. Tree ensemble methods combine several decision trees to produce better-performing predictive models. Some popular tree ensemble methods include random forests and gradient boosted trees. We will explore how these ensemble methods can be used to build regression and classification models using Spark MLlib.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor142"/>Regression using random forests</h2>
			<p>Random forests build multiple decision trees and merge them to produce a more accurate <a id="_idIndexMarker593"/>model and reduce the risk <a id="_idIndexMarker594"/>of overfitting. Random forests can be used to train regression models, as shown in the following code example:</p>
			<p class="source-code">from pyspark.ml.regression import RandomForestRegressor</p>
			<p class="source-code">from pyspark.ml.evaluation import RegressionEvaluator</p>
			<p class="source-code">retail_features = spark.read.table("retail_features").selectExpr("cust_age as label", "selected_features as features")</p>
			<p class="source-code">(train_df, test_df) = retail_features.randomSplit([0.8, 0.2])</p>
			<p class="source-code">rf = RandomForestRegressor(labelCol="label", </p>
			<p class="source-code">                           featuresCol="features", </p>
			<p class="source-code">                           numTrees=5)</p>
			<p class="source-code">rf_model = rf.fit(train_df)</p>
			<p class="source-code">predictions = rf_model.transform(test_df)</p>
			<p class="source-code">evaluator = RegressionEvaluator(</p>
			<p class="source-code">    labelCol="label", predictionCol="prediction", </p>
			<p class="source-code">    metricName="rmse")</p>
			<p class="source-code">rmse = evaluator.evaluate(predictions)</p>
			<p class="source-code">print("RMSE for test data = %g" % rmse)</p>
			<p class="source-code">print(rf_model.toDebugString)</p>
			<p>In the preceding code snippet, we did the following:</p>
			<ol>
				<li value="1">First, we <a id="_idIndexMarker595"/>split our dataset into two sets for training and testing, respectively. </li>
				<li>Then, we initialized the <strong class="source-inline">RandomForestRegressor</strong> class with several trees to be trained. We set this to <strong class="source-inline">5</strong>.</li>
				<li>Next, we <a id="_idIndexMarker596"/>fit the <strong class="source-inline">RandomForestRegressor</strong> transformer to our training dataset to get a random forest model.</li>
				<li>After, we applied the model's <strong class="source-inline">Estimator</strong> object to the test dataset to produce actual predictions.</li>
				<li>This DataFrame was then used with the <strong class="source-inline">RegressionEvaluator</strong> utility method to derive the <strong class="source-inline">RMSE</strong> value.</li>
				<li>Finally, we printed the trained random forest using the <strong class="source-inline">toDebugString</strong> attribute of the model object.</li>
			</ol>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor143"/>Classification using random forests</h2>
			<p>Just like <a id="_idIndexMarker597"/>decision trees, random <a id="_idIndexMarker598"/>forests also support training multi-class classification models, as shown in the following code block:</p>
			<p class="source-code">retail_df = spark.read.table("retail_features").selectExpr("gender", "selected_features as features")</p>
			<p class="source-code">(train_df, test_df) = retail_df.randomSplit([0.8, 0.2])</p>
			<p class="source-code">string_indexer = StringIndexer(inputCol="gender", </p>
			<p class="source-code">                               outputCol="label", </p>
			<p class="source-code">                               handleInvalid="skip" )</p>
			<p class="source-code">rf = RandomForestClassifier(labelCol="label", </p>
			<p class="source-code">                            featuresCol="features", </p>
			<p class="source-code">                            numTrees=5)</p>
			<p class="source-code">pipeline = Pipeline(stages=[string_indexer, rf])</p>
			<p class="source-code">model = pipeline.fit(train_df)</p>
			<p class="source-code">predictions = model.transform(test_df)</p>
			<p class="source-code">evaluator = MulticlassClassificationEvaluator(</p>
			<p class="source-code">    labelCol="label", predictionCol="prediction", </p>
			<p class="source-code">    metricName="accuracy")</p>
			<p class="source-code">accuracy = evaluator.evaluate(predictions)</p>
			<p class="source-code">print("Accuracy = %g " % (accuracy))</p>
			<p class="source-code">rf_model = model.stages[1]</p>
			<p class="source-code">print(rf_model.toDebugString)</p>
			<p>In the previous code snippet, we did the following:</p>
			<ol>
				<li value="1">First, we split our dataset into two sets for training and testing, respectively. This will allow us to evaluate the model's accuracy once it has been trained.</li>
				<li>We made use of <strong class="source-inline">StringIndexer</strong> to convert the gender string column into a numeric label column.</li>
				<li>Then, we initialized a <strong class="source-inline">RandomForestClassifier</strong> class with the required hyperparameters and specified the number of decision trees to be trained as <strong class="source-inline">5</strong>.</li>
				<li>Then, we <a id="_idIndexMarker599"/>joined the <strong class="source-inline">StringIndexer</strong> and <strong class="source-inline">RandomForestClassifier</strong> stages into a pipeline.</li>
				<li>After, we <a id="_idIndexMarker600"/>called the <strong class="source-inline">fit</strong> method on the training dataset to start the model training process and applied the model's <strong class="source-inline">Estimator</strong> object to the test dataset to calculate predictions.</li>
				<li>This DataFrame was then used with the <strong class="source-inline">MulticlassClassificationEvaluator</strong> utility method to derive the accuracy of the trained model.</li>
				<li>The random forest model can also be printed using the <strong class="source-inline">toDebugString</strong> attribute, which is available on the model object.</li>
			</ol>
			<p>This way, machine learning classification can be implemented at scale using the Spark machine learning library.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor144"/>Regression using gradient boosted trees</h2>
			<p><strong class="bold">Gradient boosted trees</strong> (<strong class="bold">GBTs</strong>) is another type of ensemble method based on decision trees that also <a id="_idIndexMarker601"/>improve the stability and accuracy of the trained model while minimizing the risk of overfitting. GBTs iteratively train several decision trees while minimizing a loss function through a process called gradient boosting. Let's explore an example of training regression models using GBTs in Spark, as shown in the following code example:</p>
			<p class="source-code">from pyspark.ml.regression import GBTRegressor</p>
			<p class="source-code">from pyspark.ml.evaluation import RegressionEvaluator</p>
			<p class="source-code">retail_features = spark.read.table("retail_features").selectExpr("cust_age as label", "selected_features as features")</p>
			<p class="source-code">(train_df, test_df) = retail_features.randomSplit([0.8, 0.2])</p>
			<p class="source-code">gbt = GBTRegressor(labelCol="label",featuresCol="features",</p>
			<p class="source-code">                   maxIter=5)</p>
			<p class="source-code">gbt_model = gbt.fit(train_df)</p>
			<p class="source-code">predictions = gbt_model.transform(test_df)</p>
			<p class="source-code">evaluator = RegressionEvaluator(</p>
			<p class="source-code">    labelCol="label", predictionCol="prediction", </p>
			<p class="source-code">    metricName="rmse")</p>
			<p class="source-code">rmse = evaluator.evaluate(predictions)</p>
			<p class="source-code">print("RMSE for test data = %g" % rmse)</p>
			<p class="source-code">print(gbt_model.toDebugString)</p>
			<p>In the preceding code snippet, we did the following:</p>
			<ol>
				<li value="1">First, we split <a id="_idIndexMarker602"/>our dataset into two sets for training and testing, respectively. </li>
				<li>Then, we initialized the <strong class="source-inline">GBTRegressor</strong> class with the max iterations set to <strong class="source-inline">5</strong>.</li>
				<li>Next, we <a id="_idIndexMarker603"/>fit the <strong class="source-inline">RandomForestRegressor</strong> transformer on our training dataset. This resulted in a random forest model estimator. After, we set the number of trees to be trained to <strong class="source-inline">5</strong>.</li>
				<li>After, we applied the model's <strong class="source-inline">Estimator</strong> object to the test dataset to produce actual predictions.</li>
				<li>This DataFrame was then with the <strong class="source-inline">RegressionEvaluator</strong> utility method to derive the <strong class="source-inline">RMSE</strong> value.</li>
				<li>The <a id="_idIndexMarker604"/>trained random <a id="_idIndexMarker605"/>forest can also be printed using the <strong class="source-inline">toDebugString</strong> attribute of the model object.</li>
			</ol>
			<p>This way, the GBTs algorithm from the Spark MLlib can be used to implement regression at scale.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor145"/>Classification using GBTs</h2>
			<p>GBTs can <a id="_idIndexMarker606"/>also be used to train <a id="_idIndexMarker607"/>classification models, as shown in the following code example:</p>
			<p class="source-code">retail_df = spark.read.table("retail_features").selectExpr("gender", "selected_features as features")</p>
			<p class="source-code">(train_df, test_df) = retail_df.randomSplit([0.8, 0.2])</p>
			<p class="source-code">string_indexer = StringIndexer(inputCol="gender", </p>
			<p class="source-code">                               outputCol="label", </p>
			<p class="source-code">                               handleInvalid="skip" )</p>
			<p class="source-code">gbt = GBTClassifier(labelCol="label", </p>
			<p class="source-code">                    featuresCol="features",</p>
			<p class="source-code">                    maxIter=5)</p>
			<p class="source-code">pipeline = Pipeline(stages=[string_indexer, gbt])</p>
			<p class="source-code">model = pipeline.fit(train_df)</p>
			<p class="source-code">predictions = model.transform(test_df)</p>
			<p class="source-code">evaluator = MulticlassClassificationEvaluator(</p>
			<p class="source-code">    labelCol="label", predictionCol="prediction", </p>
			<p class="source-code">    metricName="accuracy")</p>
			<p class="source-code">accuracy = evaluator.evaluate(predictions)</p>
			<p class="source-code">print("Accuracy = %g " % (accuracy))</p>
			<p class="source-code">gbt_model = model.stages[1]</p>
			<p class="source-code">print(gbt_model.toDebugString)</p>
			<p>In the previous code snippet, we did the following:</p>
			<ol>
				<li value="1">First, we made use of <strong class="source-inline">StringIndexer</strong> to convert the gender string column into a numeric label column.</li>
				<li>Then, we <a id="_idIndexMarker608"/>initialized the <strong class="source-inline">GBTClassifier</strong> class and set the number of decision trees to be trained to <strong class="source-inline">5</strong>.</li>
				<li>Then, we <a id="_idIndexMarker609"/>joined the <strong class="source-inline">StringIndexer</strong> and <strong class="source-inline">RandomForestClassifier</strong> stages into a pipeline.</li>
				<li>After, we called the <strong class="source-inline">fit</strong> method on the training dataset to start the model training process and applied the model's <strong class="source-inline">Estimator</strong> object to the test dataset to calculate predictions.</li>
				<li>This DataFrame was then used with the <strong class="source-inline">MulticlassClassificationEvaluator</strong> utility method to derive the accuracy of the trained model.</li>
			</ol>
			<p>So far, you have explored how to use tree ensemble methods to combine multiple decision trees to produce better and more accurate machine learning models for solving both regression and classification problems. In the following section, you will be introduced to some real-world applications of machine learning classification and regression models that can be applied to day-to-day scenarios.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor146"/>Real-world supervised learning applications</h1>
			<p>In the past, data<a id="_idIndexMarker610"/> science and machine learning were used exclusively for academic research purposes. However, over the past decade, this field has found its use in actual business applications to help businesses find their competitive edge, improve overall business performance, and become profitable. In this section, we will look at some real-world applications of machine learning.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor147"/>Regression applications</h2>
			<p>Some of the<a id="_idIndexMarker611"/> applications of machine learning regression models and how they help improve business performance will be presented in this section.</p>
			<h3>Customer lifetime value estimation</h3>
			<p>In any retail or CPG kind of business where customer churn is a huge factor, it is necessary to direct <a id="_idIndexMarker612"/>marketing spend at those customers who are profitable. In non-subscription kinds of businesses, typically 20% of the customer <a id="_idIndexMarker613"/>base generates up to 80% of revenue. Machine learning models can be leveraged to model and predict each customer's <strong class="bold">lifetime value</strong>. <strong class="bold">Customer lifetime value</strong> (<strong class="bold">CLV</strong>) models help predict an individual customer's <strong class="bold">estimated lifetime</strong>, which is an indicator of how much longer we can expect the customer to be profitable. CLV models can also be used to predict the amount of <a id="_idIndexMarker614"/>revenue that individual customers can be expected to generate over their expected lifetime. Thus, regression models can be used to estimate CLV and help direct marketing dollars toward promoting to and retaining profitable customers over their expected lifetimes.</p>
			<h3>Shipment lead time estimation</h3>
			<p>Retailers, logistics firms, food service aggregators, or any businesses that are in the business of <a id="_idIndexMarker615"/>delivering products to customers need to be able to predict the amount of time it will take <a id="_idIndexMarker616"/>them to ship a product to a customer. Regression models can be used to take factors such as origin and destination ZIP codes, the past performance of shipments between these locations, inventory availability, and also seasonality, weather conditions, and even local traffic into consideration to build models that can estimate the amount of time it would take for the product to reach the customer. This helps the business with inventory optimization, supply chain planning, and even improving overall customer satisfaction.</p>
			<h3>Dynamic price optimization</h3>
			<p><strong class="bold">Dynamic price optimization</strong>, also known as <strong class="bold">dynamic pricing</strong>, is where you set a price for a <a id="_idIndexMarker617"/>product or service based on current product demand or market conditions. It is a common practice in many industries, ranging from <a id="_idIndexMarker618"/>transportation to travel and hospitality, e-commerce, entertainment, and digital aggregators to perform dynamic price optimization. Businesses <a id="_idIndexMarker619"/>can take advantage of the massive amounts of data that is generated in the digital economy by adjusting prices in real time. Although dynamic pricing is an <strong class="bold">optimization</strong> problem, regression models can be used to predict the price at a given point in time, current demand, market conditions, and competitor pricing.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor148"/>Classification applications</h2>
			<p>A few examples of how <a id="_idIndexMarker620"/>classification models can be used to solve business scenarios will be discussed in this section.</p>
			<h3>Financial fraud detection</h3>
			<p>Financial fraud and identity theft are some of the biggest challenges facing the financial industry. Financial organizations have historically used statistical models and rules-based engines to <a id="_idIndexMarker621"/>detect financial fraud; however, fraudsters have managed to circumvent legacy fraud detection <a id="_idIndexMarker622"/>mechanisms using novel types of fraud. Classification models can be built using something rudimentary, such as naïve Bayes, or something much more robust, such as decision tree ensemble methods. These can be leveraged to keep up with emerging fraud patterns and flag financial transactions as fraudulent.</p>
			<h3>Email spam detection</h3>
			<p>This is a common scenario that anyone using email must have witnessed; that is, getting unwanted and <a id="_idIndexMarker623"/>soliciting or sometimes <a id="_idIndexMarker624"/>outright offensive content via email. Classification models are being used by email providers to classify and flag emails as spam and exclude them from user inboxes.</p>
			<h3>Industrial machinery and equipment and failure prediction</h3>
			<p>Heavy industries <a id="_idIndexMarker625"/>such as oil and construction companies have already installed or started installing IoT devices on their heavy industrial equipment, which sends out a constant stream of telemetry and diagnostic data to backend servers. Classification models that have been trained on the swath of telemetry and diagnostic data can help predict machine failures, help industries prevent downtime, flag problematic ancillary part vendors, and even save huge costs by preventing massive machinery recalls.</p>
			<h3>Object detection</h3>
			<p>Classification models have always been part of high-end cameras that have built-in object tracking <a id="_idIndexMarker626"/>and autofocus functions. Modern-day mobile phone applications also leverage classification models to isolate the subject <a id="_idIndexMarker627"/>of the photograph from the background, as well as to identify and tag individuals in photographs. </p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor149"/>Summary</h1>
			<p>In this chapter, you were introduced to a class of machine learning algorithms called supervised learning algorithms, which can learn from well-labeled existing data. You explored the concepts of parametric and non-parametric learning algorithms and their pros and cons. Two major use cases of supervised learning algorithms called regression and classification were presented. Model training examples, along with code from Spark MLlib, were explored so that we could look at a few prominent types of regression and classification models. Tree ensemble methods, which improve the stability, accuracy, and performance of decision tree models by combining several models and preventing overfitting, were also presented.</p>
			<p>Finally, you explored some real-world business applications of the various machine learning models presented in this chapter. We explained how supervised learning can be leveraged for business use cases, and working code samples were presented to help you train your models at scale using Spark MLlib and solve business problems efficiently.</p>
			<p>In the next chapter, we will explore unsupervised machine learning algorithms, how they are different from supervised learning models, and their application in solving real-world business problems. We will also provide working code examples to showcase this.</p>
		</div>
	</div></body></html>