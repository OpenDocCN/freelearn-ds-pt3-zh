- en: '*Chapter 3*: Data Wrangling with Pandas'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about the main `pandas` data structures,
    how to create `DataFrame` objects with our collected data, and various ways to
    inspect, summarize, filter, select, and work with `DataFrame` objects. Now that
    we are well versed in the initial data collection and inspection stage, we can
    begin our foray into the world of data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in [*Chapter 1*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015),
    *Introduction to Data Analysis*, preparing data for analysis is often the largest
    portion of the job time-wise for those working with data, and often the least
    enjoyable. On the bright side, `pandas` is well equipped to help with these tasks,
    and, by mastering the skills presented in this book, we will be able to get to
    the more interesting parts sooner.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that data wrangling isn't something we do merely once in
    our analysis; it is highly likely that we will do some data wrangling and move
    on to another analysis task, such as data visualization, only to find that we
    need to do additional data wrangling. The more familiar we are with the data,
    the better we will be able to prepare the data for our analysis. It's crucial
    to form an intuition of what types our data should be, what format we need our
    data to be in for the visualization that would best convey what we are looking
    to show, and the data points we should collect for our analysis. This comes with
    experience, so we must practice the skills that will be covered in this chapter
    on our own data every chance we get.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a very large topic, our coverage of data wrangling will be split
    between this chapter and [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*. In this chapter, we will get an overview of data
    wrangling before exploring the `requests` library. Then, we will discuss data
    wrangling tasks that deal with preparing data for some initial analyses and visualizations
    (which we will learn about in [*Chapter 5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106),
    *Visualizing Data with Pandas and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*). We will address some more
    advanced aspects of data wrangling that relate to aggregations and combining datasets
    in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082), *Aggregating
    Pandas DataFrames*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data wrangling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring an API to find and collect temperature data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshaping data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling duplicate, missing, or invalid data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter materials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The materials for this chapter can be found on GitHub at [https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03).
    There are five notebooks that we will work through, each numbered according to
    when they will be used, and two directories, `data/` and `exercises/`, which contain
    all the CSV files necessary for the aforementioned notebooks and end-of-chapter
    exercises, respectively. The following files are in the `data/` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Breakdown of the datasets used in this chapter'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.1_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Breakdown of the datasets used in this chapter
  prefs: []
  type: TYPE_NORMAL
- en: We will begin in the `1-wide_vs_long.ipynb` notebook by discussing wide versus
    long format data. Then, we will collect daily temperature data from the NCEI API,
    which can be found at [https://www.ncdc.noaa.gov/cdo-web/webservices/v2](https://www.ncdc.noaa.gov/cdo-web/webservices/v2),
    in the `2-using_the_weather_api.ipynb` notebook. The documentation for the **Global
    Historical Climatology Network – Daily** (**GHCND**) dataset we will be using
    can be found at [https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf](https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The NCEI is part of the **National Oceanic and Atmospheric Administration**
    (**NOAA**). As indicated by the URL for the API, this resource was created when
    the NCEI was called the NCDC. Should the URL for this resource change in the future,
    search for *NCEI weather API* to find the updated one.
  prefs: []
  type: TYPE_NORMAL
- en: In the `3-cleaning_data.ipynb` notebook, we will learn how to perform an initial
    round of cleaning on the temperature data and some financial data, which was collected
    using the `stock_analysis` package that we will build in [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146),
    *Financial Analysis – Bitcoin and the Stock Market*. Afterward, we will walk through
    ways to reshape our data in the `4-reshaping_data.ipynb` notebook. Finally, in
    the `5-handling_data_issues.ipynb` notebook, we will learn about some strategies
    for dealing with duplicate, missing, or invalid data using some dirty data that
    can be found in `data/dirty_data.csv`. The text will indicate when it's time to
    switch between notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data wrangling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like any professional field, data analysis is filled with buzzwords, and it
    can often be difficult for newcomers to understand the lingo—the topic of this
    chapter is no exception. When we perform **data wrangling**, we are taking our
    input data from its original state and putting it in a format where we can perform
    meaningful analysis on it. **Data manipulation** is another way to refer to this
    process. There is no set list of operations; the only goal is that the data post-wrangling
    is more useful to us than when we started. In practice, there are three common
    tasks involved in the data wrangling process:'
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data enrichment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It should be noted that there is no inherent order to these tasks, and it is
    highly probable that we will perform each many times throughout the data wrangling
    process. This idea brings up an interesting conundrum: if we need to wrangle our
    data to prepare it for our analysis, isn''t it possible to wrangle it in such
    a way that we tell the data what to say instead of us learning what it is saying?'
  prefs: []
  type: TYPE_NORMAL
- en: '"If you torture the data long enough, it will confess to anything."'
  prefs: []
  type: TYPE_NORMAL
- en: — Ronald Coase, winner of a Nobel Prize in Economics
  prefs: []
  type: TYPE_NORMAL
- en: Those working with data will find it is very easy to distort the truth by manipulating
    the data. However, it is our duty to do our best to avoid deceit by keeping the
    effect our actions have on the data's integrity in mind, and by explaining the
    process we took to draw our conclusions to the people who consume our analyses,
    so that they too may make their own judgments.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have collected our data, brought it into a `DataFrame` object, and
    used the skills we discussed in [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),
    *Working with Pandas DataFrames*, to familiarize ourselves with the data, we will
    need to perform some data cleaning. An initial round of data cleaning will often
    give us the bare minimum we need to start exploring our data. Some essential data
    cleaning tasks to master include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Renaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sorting and reordering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data type conversions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling duplicate data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing missing or invalid data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering to the desired subset of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data cleaning is the best starting point for data wrangling, since having the
    data stored as the correct data types and easy-to-reference names will open up
    many avenues for exploration, such as summary statistics, sorting, and filtering.
    Since we covered filtering in [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),
    *Working with Pandas DataFrames*, we will focus on the other topics from the preceding
    list in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Frequently, we will reach the data transformation stage after some initial data
    cleaning, but it is entirely possible that our dataset is unusable in its current
    shape, and we must restructure it before attempting to do any data cleaning. In
    **data transformation**, we focus on changing our data's structure to facilitate
    our downstream analyses; this usually involves changing which data goes along
    the rows and which goes down the columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most data we will find is either **wide format** or **long format**; each of
    these formats has its merits, and it''s important to know which one we will need
    for our analysis. Often, people will record and present data in wide format, but
    there are certain visualizations that require the data to be in long format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – (Left) wide format versus (right) long format'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.2_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – (Left) wide format versus (right) long format
  prefs: []
  type: TYPE_NORMAL
- en: Wide format is preferred for analysis and database design, while long format
    is considered poor design because each column should be its own data type and
    have a singular meaning. However, in cases where new fields will be added (or
    old ones removed) from a table in a relational database, rather than having to
    alter all the tables each time, the database's maintainers may decide to use the
    long format. This allows them to provide a fixed schema for users of the database,
    while being able to update the data it contains as needed. When building an API,
    the long format may be chosen if flexibility is required. Perhaps the API will
    provide a generic response format (for instance, date, field name, and field value)
    that can support various tables from a database. This may also have to do with
    making the response easier to form, depending on how the data is stored in the
    database the API uses. Since we will find data in both of these formats, it's
    important we understand how to work with both of them and go from one to the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s navigate to the `1-wide_vs_long.ipynb` notebook to see some examples.
    First, we will import `pandas` and `matplotlib` (to help illustrate the strengths
    and weaknesses of each format when it comes to visualizations, which we will discuss
    in [*Chapter 5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106), *Visualizing
    Data with Pandas and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*) and read in the CSV files
    containing wide and long format data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The wide data format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With wide format data, we represent measurements of variables with their own
    columns, and each row represents an observation of those variables. This makes
    it easy for us to compare variables across observations, get summary statistics,
    perform operations, and present our data; however, some visualizations don't work
    with this data format because they may rely on the long format to split, size,
    and/or color the plot content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the top six observations from the wide format data in `wide_df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Each column contains the top six observations of a specific class of temperature
    data in degrees Celsius—maximum temperature (**TMAX**), minimum temperature (**TMIN**),
    and temperature at the time of observation (**TOBS**)—at a daily frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Wide format temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.3_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – Wide format temperature data
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with wide format data, we can easily grab summary statistics on
    this data by using the `describe()` method. Note that while older versions of
    `pandas` treated `datetimes` as categorical, `pandas` is moving toward treating
    them as numeric, so we pass `datetime_is_numeric=True` to suppress the warning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With hardly any effort on our part, we get summary statistics for the dates,
    maximum temperature, minimum temperature, and temperature at the time of observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Summary statistics for the wide format temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.4_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Summary statistics for the wide format temperature data
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed previously, the summary data in the preceding table is easy
    to obtain and is informative. This format can easily be plotted with `pandas`
    as well, provided we tell it exactly what we want to plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Pandas plots the daily maximum temperature, minimum temperature, and temperature
    at the time of observation as their own lines on a single line plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Plotting the wide format temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.5_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Plotting the wide format temperature data
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Don't worry about understanding the visualization code right now; it's here
    just to illustrate how each of these data formats can make certain tasks easier
    or harder. We will cover visualizations with `pandas` and `matplotlib` in [*Chapter
    5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106), *Visualizing Data with Pandas
    and Matplotlib*.
  prefs: []
  type: TYPE_NORMAL
- en: The long data format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Long format data will have a row for each observation of a variable; this means
    that, if we have three variables being measured daily, we will have three rows
    for each day we record observations. The long format setup can be achieved by
    turning the variable column names into a single column, where the data is the
    variable name, and putting their values in a separate column.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can look at the top six rows of the long format data in `long_df` to see
    the difference between wide format and long format data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we now have three entries for each date, and the **datatype** column
    tells us what the data in the **value** column is for that row:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Long format temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.6_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Long format temperature data
  prefs: []
  type: TYPE_NORMAL
- en: 'If we try to get summary statistics, like we did with the wide format data,
    the result isn''t as helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The **value** column shows us summary statistics, but this is summarizing the
    daily maximum temperatures, minimum temperatures, and temperatures at the time
    of observation. The maximum will be the maximum of the daily maximum temperatures
    and the minimum will be the minimum of the daily minimum temperatures. This means
    that this summary data is not very helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Summary statistics for the long format temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.7_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Summary statistics for the long format temperature data
  prefs: []
  type: TYPE_NORMAL
- en: 'This format is not very easy to digest and certainly shouldn''t be how we present
    data; however, it makes it easy to create visualizations where our plotting library
    can color lines by the name of the variable, size the points by the values of
    a certain variable, and perform splits for faceting. Pandas expects its data for
    plotting to be in wide format, so, to easily make the same plot that we did with
    the wide format data, we must use another plotting library, called `seaborn`,
    which we will cover in [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Seaborn can subset based on the `datatype` column to give us individual lines
    for the daily maximum temperature, minimum temperature, and temperature at the
    time of observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Plotting the long format temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.8_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – Plotting the long format temperature data
  prefs: []
  type: TYPE_NORMAL
- en: 'Seaborn lets us specify the column to use for `hue`, which colored the lines
    in *Figure 3.8* by the temperature type. We aren''t limited to this, though; with
    long format data, we can easily facet our plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Seaborn can use long format data to create subplots for each distinct value
    in the `datatype` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Plotting subsets of the long format temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.9_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 – Plotting subsets of the long format temperature data
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: While it is possible to create a plot similar to the preceding one with `pandas`
    and `matplotlib` using subplots, more complicated combinations of facets will
    make using `seaborn` infinitely easier. We will cover `seaborn` in [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Reshaping data* section, we will cover how to transform our data from
    wide to long format by melting, and from long to wide format by pivoting. Additionally,
    we will learn how to transpose data, which flips the columns and the rows.
  prefs: []
  type: TYPE_NORMAL
- en: Data enrichment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have our cleaned data in the format we need for our analysis, we may
    find the need to enrich the data a bit. **Data enrichment** improves the quality
    of the data by adding to it in one way or another. This process becomes very important
    in modeling and in machine learning, where it forms part of the **feature engineering**
    process (which we will touch on in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217),
    *Making Better Predictions – Optimizing Models*).
  prefs: []
  type: TYPE_NORMAL
- en: 'When we''re looking to enrich our data, we can either **merge** new data with
    the original data (by appending new rows or columns) or use the original data
    to create new data. The following are ways to enhance our data using the original
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding new columns**: Using functions on the data from existing columns to
    create new values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binning**: Turning continuous data or discrete data with many distinct values
    into buckets, which makes the column discrete while letting us control the number
    of possible values in the column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregating**: Rolling up the data and summarizing it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resampling**: Aggregating time series data at specific intervals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand what data wrangling is, let's collect some data to work
    with. Note that we will cover data cleaning and transformation in this chapter,
    while data enrichment will be covered in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring an API to find and collect temperature data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: "In [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035), *Working\
    \ with Pandas DataFrames*, we worked on data collection and how \Lto perform an\
    \ initial inspection and filtering of the data; this usually gives us ideas of\
    \ things that need to be addressed before we move further in our analysis. Since\
    \ this chapter builds on those skills, we will get to practice some of them here\
    \ as well. To begin, we will start by exploring the weather API that's provided\
    \ by the NCEI. Then, in the next section, we will learn about data wrangling using\
    \ temperature data that was previously obtained from this API."
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the NCEI API, you will have to request a token by filling out this form
    with your email address: [https://www.ncdc.noaa.gov/cdo-web/token](https://www.ncdc.noaa.gov/cdo-web/token).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, we will be working in the `2-using_the_weather_api.ipynb`
    notebook to request temperature data from the NCEI API. As we learned in [*Chapter
    2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),*Working with Pandas DataFrames*,
    we can use the `requests` library to interact with APIs. In the following code
    block, we import the `requests` library and create a convenience function for
    making the requests to a specific endpoint, sending our token along. To use this
    function, we need to provide a token, as indicated in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is making use of `format()` method: `''api/v2/{}''.format(endpoint)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the `make_request()` function, we need to learn how to form our request.
    The NCEI has a helpful getting started page ([https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted](https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted))
    that shows us how to form requests; we can progress through the tabs on the page
    to figure out what filters we want on our query. The `requests` library takes
    care of turning our dictionary of search parameters (passed in as `payload`) into
    a `2018-08-28` for `start` and `2019-04-15` for `end`, we will get `?start=2018-08-28&end=2019-04-15`),
    just like the examples on the website. This API provides many different endpoints
    for exploring what is offered and building up our ultimate request for the actual
    dataset. We will start by figuring out the ID of the dataset we want to query
    for (`datasetid`) using the `datasets` endpoint. Let''s check which datasets have
    data within the date range of October 1, 2018 through today:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that we check the `status_code` attribute to make sure the request
    was successful. Alternatively, we can use the `ok` attribute to get a Boolean
    indicator if everything went as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The API limits us to 5 requests per second and 10,000 requests per day. If we
    exceed these limits, the status code will indicate a client error (meaning that
    the error appears to have been caused by us). Client errors have status codes
    in the 400s; for example, 404, if the requested resource can't be found, or 400,
    if the server can't understand our request (or refuses to process it). Sometimes,
    the server has an issue on its side when processing our request, in which case
    we see status codes in the 500s. You can find a listing of common status codes
    and their meanings at [https://restfulapi.net/http-status-codes/](https://restfulapi.net/http-status-codes/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our response, we can use the `json()` method to get the payload.
    Then, we can use dictionary methods to determine which part we want to look at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `metadata` portion of the JSON payload tells us information about the result,
    while the `results` section contains the actual results. Let''s see how much data
    we got back, so that we know whether we can print the results or whether we should
    try to limit the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We got back 11 rows, so let''s see what fields are in the `results` portion
    of the JSON payload. The `results` key contains a list of dictionaries. If we
    select the first one, we can look at the keys to see what fields the data contains.
    We can then reduce the output to the fields we care about:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For our purposes, we want to look at the IDs and names of the datasets, so
    let''s use a list comprehension to look at those only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The first entry in the result is what we are looking for. Now that we have
    a value for `datasetid` (`GHCND`), we proceed to identify one for `datacategoryid`,
    which we need to request temperature data. We do so using the `datacategories`
    endpoint. Here, we can print the JSON payload since it isn''t that large (only
    nine entries):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the previous result, we know that we want a value of `TEMP` for `datacategoryid`.
    Next, we use this to identify the data types we want by using the `datatypes`
    endpoint. We will use a list comprehension once again to only print the names
    and IDs; this is still a rather large list, so the output has been abbreviated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We are looking for the `TAVG`, `TMAX`, and `TMIN` data types. Now that we have
    everything we need to request temperature data for all locations, we need to narrow
    it down to a specific location. To determine a value for `locationcategoryid`,
    we must use the `locationcategories` endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we can use `pprint` from the Python standard library ([https://docs.python.org/3/library/pprint.html](https://docs.python.org/3/library/pprint.html))
    to print our JSON payload in an easier-to-read format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We want to look at New York City, so, for the `locationcategoryid` filter, `CITY`
    is the proper value. The notebook we are working in has a function to search for
    a field by name using **binary search** on the API; binary search is a more efficient
    way of searching through an ordered list. Since we know that the fields can be
    sorted alphabetically, and the API gives us metadata about the request, we know
    how many items the API has for a given field and can tell whether we have passed
    the one we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: 'With each request, we grab the middle entry and compare its location in the
    alphabet with our target; if the result comes before our target, we look at the
    half of the data that''s greater than what we just got; otherwise, we look at
    the smaller half. Each time, we are slicing the data in half, so when we grab
    the middle entry to test, we are moving closer to the value we seek (see *Figure
    3.10*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This is a **recursive** implementation of the algorithm, meaning that we call
    the function itself from inside; we must be very careful when we do this to define
    a **base condition** so that it will eventually stop and not enter an infinite
    loop. It is possible to implement this iteratively. See the *Further reading*
    section at the end of this chapter for additional reading on binary search and
    **recursion**.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In a traditional implementation of binary search, it is trivial to find the
    length of the list that we are searching. With the API, we have to make one request
    to get the count; therefore, we must ask for the first entry (offset of 1) to
    orient ourselves. This means that we make an extra request here compared to what
    we would have needed if we knew how many locations were in the list before starting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s use the binary search implementation to find the ID for New York
    City, which will be the value we will use for `locationid` in subsequent queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'By using binary search here, we find **New York** in **8** requests, despite
    it being close to the middle of 1,983 entries! For comparison, using linear search,
    we would have looked at 1,254 entries before finding it. In the following diagram,
    we can see how binary search eliminates sections of the list of locations systematically,
    which is represented by black on the number line (white means it is still possible
    that the desired value is in that section):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Binary search to find New York City'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.10_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 – Binary search to find New York City
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Some APIs (such as the NCEI API) restrict the number of requests we can make
    within certain periods of time, so it's important to be smart about our requests.
    When searching a very long ordered list, think of binary search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optionally, we can drill down to the ID of the station that is collecting the
    data. This is the most granular level. Using binary search again, we can grab
    the station ID for the Central Park station:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s request NYC''s temperature data in Celsius for October 2018, recorded
    from Central Park. For this, we will use the `data` endpoint and provide all the
    parameters we picked up throughout our exploration of the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we will create a `DataFrame` object; since the `results` portion of
    the JSON payload is a list of dictionaries, we can pass it directly to `pd.DataFrame()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We get back long format data. The **datatype** column is the temperature variable
    being measured, and the **value** column contains the measured temperature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Data retrieved from the NCEI API'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.11_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 – Data retrieved from the NCEI API
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We can use the previous code to turn any of the JSON responses we worked with
    in this section into a `DataFrame` object, if we find that easier to work with.
    However, it should be stressed that JSON payloads are pretty much ubiquitous when
    it comes to APIs (and, as Python users, we should be familiar with dictionary-like
    objects), so it won't hurt to get comfortable with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We asked for `TAVG`, `TMAX`, and `TMIN`, but notice that we didn''t get `TAVG`.
    This is because the Central Park station isn''t recording average temperature,
    despite being listed in the API as offering it—real-world data is dirty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Time for plan B: let''s use LaGuardia Airport as the station instead of Central
    Park for the remainder of this chapter. Alternatively, we could have grabbed data
    for all the stations that cover New York City; however, since this would give
    us multiple entries per day for some of the temperature measurements, we won''t
    do so here—we would need skills that will be covered in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, to work with that data.'
  prefs: []
  type: TYPE_NORMAL
- en: The process of collecting the weather data from the LaGuardia Airport station
    is the same as with the Central Park station, but in the interest of brevity,
    we will read in the data for LaGuardia in the next notebook when we discuss cleaning
    the data. Note that the bottom cells of the current notebook contain the code
    that's used to collect this data.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s move on to the `3-cleaning_data.ipynb` notebook for our discussion of
    data cleaning. As usual, we will begin by importing `pandas` and reading in our
    data. For this section, we will be using the `nyc_temperatures.csv` file, which
    contains the maximum daily temperature (`TMAX`), minimum daily temperature (`TMIN`),
    and the average daily temperature (`TAVG`) from the LaGuardia Airport station
    in New York City for October 2018:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We retrieved long format data from the API; for our analysis, we want wide
    format data, but we will address that in the *Pivoting DataFrames* section, later
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – NYC temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.12_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – NYC temperature data
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we will focus on making little tweaks to the data that will make it
    easier for us to use: renaming columns, converting each column into the most appropriate
    data type, sorting, and reindexing. Often, this will be the time to filter the
    data down, but we did that when we worked on requesting data from the API; for
    a review of filtering with `pandas`, refer to [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),
    *Working with Pandas DataFrames*.'
  prefs: []
  type: TYPE_NORMAL
- en: Renaming columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the API endpoint we used could return data of any units and category,
    it had to call that column `value`. We only pulled temperature data in Celsius,
    so all our observations have the same units. This means that we can rename the
    `value` column so that it''s clear what data we are working with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DataFrame` class has a `rename()` method that takes a dictionary mapping
    the old column name to the new column name. In addition to renaming the `value`
    column, let''s rename the `attributes` column to `flags` since the API documentation
    mentions that that column contains flags for information about data collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Most of the time, `pandas` will return a new `DataFrame` object; however, since
    we passed in `inplace=True`, our original dataframe was updated instead. Always
    be careful with in-place operations, as they might be difficult or impossible
    to undo. Our columns now have their new names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Both `Series` and `Index` objects can also be renamed using their `rename()`
    methods. Simply pass in the new name. For example, if we have a `Series` object
    called `temperature` and we want to rename it `temp_C`, we can run `temperature.rename('temp_C')`.
    The variable will still be called `temperature`, but the name of the data in the
    series itself will now be `temp_C`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also do transformations on the column names with `rename()`. For instance,
    we can put all the column names in uppercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This method even lets us rename the values of the index, although this is something
    we don't have use for now since our index is just numbers. However, for reference,
    we would simply change `axis='columns'` in the preceding code to `axis='rows'`.
  prefs: []
  type: TYPE_NORMAL
- en: Type conversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that the column names are indicative of the data they contain, we can check
    what types of data they hold. We should have formed an intuition as to what the
    data types should be after looking at the first few rows when we inspected the
    dataframe with the `head()` method previously. With type conversion, we aim to
    reconcile what the current data types are with what we believe they should be;
    we will be changing how our data is represented.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, sometimes, we may have data that we believe should be a certain type,
    such as a date, but it is stored as a string; this could be for a very valid reason—data
    could be missing. In the case of missing data encoded as text (for example, `?`
    or `N/A`), `pandas` will store it as a string when reading it in to allow for
    this data. It will be marked as `object` when we use the `dtypes` attribute on
    our dataframe. If we try to convert (or **cast**) these columns, we will either
    get an error or our result won't be what we expected. For example, if we have
    strings of decimal numbers, but try to convert the column into integers, we will
    get an error since Python knows they aren't integers; however, if we try to convert
    decimal numbers into integers, we will lose any information after the decimal
    point.
  prefs: []
  type: TYPE_NORMAL
- en: 'That being said, let''s examine the data types in our temperature data. Note
    that the `date` column isn''t actually being stored as a datetime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `pd.to_datetime()` function to convert it into a datetime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This is much better. Now, we can get useful information when we summarize the
    `date` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Dealing with dates can be tricky since they come in many different formats
    and time zones; fortunately, `pandas` has more methods we can use for dealing
    with converting datetime objects. For example, when working with a `DatetimeIndex`
    object, if we need to keep track of time zones, we can use the `tz_localize()`
    method to associate our datetimes with a time zone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This also works with `Series` and `DataFrame` objects that have an index of
    type `DatetimeIndex`. We can read in the CSV file again and, this time, specify
    that the `date` column will be our index and that we should parse any dates in
    the CSV file into datetimes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We had to read the file in again for this example because we haven''t learned
    how to change the index of our data yet (covered in the *Reordering, reindexing,
    and sorting data* section, later this chapter). Note that we have added the Eastern
    Standard Time offset (-05:00 from UTC) to the datetimes in the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Time zone-aware dates in the index'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.13_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.13 – Time zone-aware dates in the index
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `tz_convert()` method to change the time zone into a different
    one. Let''s change our data into UTC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the offset is UTC (+00:00), but note that the time portion of the date
    is now 5 AM; this conversion took into account the -05:00 offset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Converting data into another time zone'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.14_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 – Converting data into another time zone
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also truncate datetimes with the `to_period()` method, which comes in
    handy if we don''t care about the full date. For example, if we wanted to aggregate
    our data by month, we could truncate our index to just the month and year and
    then perform the aggregation. Since we will cover aggregation in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, we will just do the truncation here. Note that
    we first remove the time zone information to avoid a warning from `pandas` that
    the `PeriodArray` class doesn''t have time zone information, and therefore it
    will be lost. This is because the underlying data for a `PeriodIndex` object is
    stored as a `PeriodArray` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `to_timestamp()` method to convert our `PeriodIndex` object
    into a `DatetimeIndex` object; however, the datetimes all start at the first of
    the month now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can use the `assign()` method to handle any type conversions
    by passing the column names as named parameters and their new values as the value
    for that argument to the method call. In practice, this will be more beneficial
    since we can perform many tasks in one call and use the columns we create in that
    call to calculate additional columns. For example, let''s cast the `date` column
    to a datetime and add a new column for the temperature in Fahrenheit (`temp_F`).
    The `assign()` method returns a new `DataFrame` object, so we must remember to
    assign it to a variable if we want to keep it. Here, we will create a new one.
    Note that our original conversion of the dates modified the column, so, in order
    to illustrate that we can use `assign()`, we need to read our data in once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have datetimes in the `date` column and a new column, `temp_F`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Simultaneous type conversion and column creation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.15_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.15 – Simultaneous type conversion and column creation
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we can use the `astype()` method to convert one column at a time.
    As an example, let''s say we only cared about the temperatures at every whole
    number, but we don''t want to round. In this case, we simply want to chop off
    the information after the decimal. To accomplish this, we can cast the floats
    as integers. This time, we will use `temp_F` column to create the `temp_F_whole`
    column, even though `df` doesn''t have this column before we call `assign()`.
    It is very common (and useful) to use lambda functions with `assign()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we can refer to columns we''ve just created if we use a lambda function.
    It''s also important to mention that we don''t have to know whether to convert
    the column into a float or an integer: we can use `pd.to_numeric()`, which will
    convert the data into floats if it sees decimals. If all the numbers are whole,
    they will be converted into integers (obviously, we will still get errors if the
    data isn''t numeric at all):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Creating columns with lambda functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.16_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.16 – Creating columns with lambda functions
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we have two columns with data currently being stored as strings that
    can be represented in a better way for this dataset. The `station` and `datatype`
    columns only have one and three distinct values, respectively, meaning that we
    aren't being efficient with our memory use since we are storing them as strings.
    We could potentially have issues with analyses further down the line. Pandas has
    the ability to define columns as `pandas` and other packages will be able to handle
    this data, provide meaningful statistics on them, and use them properly. Categorical
    variables can take on one of a few values; for example, blood type would be a
    categorical variable—people can only have one of A, B, AB, or O.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the temperature data, we only have one value for the `station`
    column and only three distinct values for the `datatype` column (`TAVG`, `TMAX`,
    `TMIN`). We can use the `astype()` method to cast these into categories and look
    at the summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The summary statistics for categories are just like those for strings. We can
    see the number of non-null entries (**count**), the number of unique values (**unique**),
    the mode (**top**), and the number of occurrences of the mode (**freq**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Summary statistics for the categorical columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.17_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 – Summary statistics for the categorical columns
  prefs: []
  type: TYPE_NORMAL
- en: 'The categories we just made don''t have any order to them, but `pandas` does
    support this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: When the columns in our dataframe are stored in the appropriate type, it opens
    up additional avenues for exploration, such as calculating statistics, aggregating
    the data, and sorting the values. For example, depending on our data source, it's
    possible that the numeric data is represented as a string, in which case attempting
    to sort on the values will reorder the contents lexically, meaning the result
    could be 1, 10, 11, 2, rather than 1, 2, 10, 11 (numerical sort). Similarly, if
    we have dates represented as strings in a format other than YYYY-MM-DD, sorting
    on this information may result in non-chronological order; however, by converting
    the date strings with `pd.to_datetime()`, we can chronologically sort dates that
    are provided in any format. Type conversion makes it possible to reorder both
    the numeric data and the dates according to their values, rather than their initial
    string representations.
  prefs: []
  type: TYPE_NORMAL
- en: Reordering, reindexing, and sorting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will often find the need to sort our data by the values of one or many columns.
    Say we wanted to find the days that reached the highest temperatures in New York
    City during October 2018; we could sort our values by the `temp_C` (or `temp_F`)
    column in descending order and use `head()` to select the number of days we wanted
    to see. To accomplish this, we can use the `sort_values()` method. Let''s look
    at the top 10 days:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us that on October 7th and October 10th the temperature reached
    its highest value during the month of October 2018, according to the LaGuardia
    station. We also have ties between October 2nd and 4th, October 1st and 9th, and
    October 5th and 8th, but notice that the dates aren''t always sorted—the 10th
    is after the 7th, but the 4th comes before the 2nd:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Sorting the data to find the warmest days'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.18_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.18 – Sorting the data to find the warmest days
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sort_values()` method can be used with a list of column names to break
    ties. The order in which the columns are provided will determine the sort order,
    with each subsequent column being used to break ties. As an example, let''s make
    sure the dates are sorted in ascending order when breaking ties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are sorting in ascending order, in the case of a tie, the date that
    comes earlier in the year will be above the later one. Notice how October 2nd
    is now above October 4th, despite both having the same temperature reading:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Sorting the data with multiple columns to break ties'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.19_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.19 – Sorting the data with multiple columns to break ties
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: In `pandas`, the index is tied to the rows—when we drop rows, filter, or do
    anything that returns only some of the rows, our index may look out of order (as
    we saw in the previous examples). At the moment, the index just represents the
    row number in our data, so we may be interested in changing the values so that
    we have the first entry at index `0`. To have `pandas` do so automatically, we
    can pass `ignore_index=True` to `sort_values()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas also provides an additional way to look at a subset of the sorted values;
    we can use `nlargest()` to grab the `n` rows with the largest values according
    to specific criteria and `nsmallest()` to grab the `n` smallest rows, without
    the need to sort the data beforehand. Both accept a list of column names or a
    string for a single column. Let''s grab the top 10 days by average temperature
    this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the warmest days (on average) in October:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Sorting to find the 10 warmest days on average'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.20_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.20 – Sorting to find the 10 warmest days on average
  prefs: []
  type: TYPE_NORMAL
- en: 'We aren''t limited to sorting values; if we wish, we can even order the columns
    alphabetically and sort the rows by their index values. For these tasks, we can
    use the `sort_index()` method. By default, `sort_index()` will target the rows
    so that we can do things such as order the index after performing an operation
    that shuffles it. For instance, the `sample()` method will give us randomly selected
    rows, which will lead to a jumbled index, so we can use `sort_index()` to order
    them afterward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If we need the result of `sample()` to be reproducible, we can pass in a `random_state`
    argument). The seed initializes a pseudorandom number generator, so, provided
    that the same seed is used, the results will be the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we want to target columns, we must pass in `axis=1`; rows will be the
    default (`axis=0`). Note that this argument is present in many `pandas` methods
    and functions (including `sample()`), so it''s important to understand what it
    means. Let''s use this knowledge to sort the columns of our dataframe alphabetically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Having our columns in alphabetical order can come in handy when using `loc[]`
    because we can specify a range of columns with similar names; for example, we
    could now use `df.loc[:,''station'':''temp_F_whole'']` to easily grab all of our
    temperature columns, along with the station information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Sorting the columns by name'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.21_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.21 – Sorting the columns by name
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Both `sort_index()` and `sort_values()` return new `DataFrame` objects. We must
    pass in `inplace=True` to update the dataframe we are working with.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sort_index()` method can also help us get an accurate answer when we''re
    testing two dataframes for equality. Pandas will check that, in addition to having
    the same data, both have the same values for the index when it compares the rows.
    If we sort our dataframe by temperature in Celsius and check whether it is equal
    to the original dataframe, `pandas` tells us that it isn''t. We must sort the
    index to see that they are the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, we don''t care too much about the numeric index, but we would like
    to use one (or more) of the other columns as the index instead. In this case,
    we can use the `set_index()` method. Let''s set the `date` column as our index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the `date` column has moved to the far left where the index goes,
    and we no longer have the numeric index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Setting the date column as the index'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.22_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.22 – Setting the date column as the index
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We can also provide a list of columns to use as the index. This will create
    a `MultiIndex` object, where the first element in the list is the outermost level
    and the last is the innermost. We will discuss this further in the *Pivoting DataFrames*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the index to a datetime lets us take advantage of datetime slicing
    and indexing, which we briefly discussed in [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035),
    *Working with Pandas DataFrames*. As long as we provide a date format that `pandas`
    understands, we can grab the data. To select all of 2018, we can use `df.loc[''2018'']`;
    for the fourth quarter of 2018, we can use `df.loc[''2018-Q4'']`; and for October,
    we can use `df.loc[''2018-10'']`. These can also be combined to build ranges.
    Note that `loc[]` is optional when using ranges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the data from October 11, 2018 through October 12, 2018 (inclusive
    of both endpoints):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Selecting date ranges'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.23_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.23 – Selecting date ranges
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `reset_index()` method to restore the `date` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Our index now starts at `0`, and the dates are now in a column called `date`.
    This is especially useful if we have data that we don''t want to lose in the index,
    such as the date, but need to perform an operation as if it weren''t in the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Resetting the index'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.24_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.24 – Resetting the index
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, we may have an index we want to continue to use, but we need
    to align it to certain values. For this purpose, we have the `reindex()` method.
    We provide it with an index to align our data to, and it adjusts the index accordingly.
    Note that this new index isn't necessarily part of the data—we simply have an
    index and want to match the current data up to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we will turn to the S&P 500 stock data in the `sp500.csv` file.
    It contains the `date` column as the index and parsing the dates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what our data looks like and mark the day of the week for each row
    in order to understand what the index contains. We can easily isolate the date
    part from an index of type `DatetimeIndex`. When isolating date parts, `pandas`
    will give us the numeric representation of what we are looking for; if we are
    looking for the string version, we should look to see whether there is already
    a method before writing our own conversion function. In this case, it''s `day_name()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We can also do this with a series, but first, we need to access the `dt` attribute.
    For example, if we had a `date` column in the `sp` dataframe, we could grab the
    month with `sp.date.dt.month`. You can find the full list of what can be accessed
    at [https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the stock market is closed on the weekend (and holidays), we only have
    data for weekdays:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – S&P 500 OHLC data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.25_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.25 – S&P 500 OHLC data
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were analyzing the performance of a group of assets in a portfolio that
    included the S&P 500 and something that trades on the weekend, such as bitcoin,
    we would need to have values for every day of the year for the S&P 500\. Otherwise,
    when looking at the daily value of our portfolio we would see huge drops every
    day the market was closed. To illustrate this, let''s read in the bitcoin data
    from the `bitcoin.csv` file and combine the S&P 500 and bitcoin data into a portfolio.
    The bitcoin data also contains OHLC data and volume traded, but it comes with
    a column called `market_cap` that we don''t need, so we have to drop that first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'To analyze the portfolio, we will need to aggregate the data by day; this is
    a topic for [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082), *Aggregating
    Pandas DataFrames*, so, for now, don''t worry too much about how this aggregation
    is being performed—just know that we are summing up the data by day. For example,
    each day''s closing price will be the sum of the closing price of the S&P 500
    and the closing price of bitcoin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we examine our portfolio, we will see that we have values for every
    day of the week; so far, so good:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 3.26 – Portfolio of the S&P 500 and bitcoin'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.26_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.26 – Portfolio of the S&P 500 and bitcoin
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a problem with this approach, which is much easier to see
    with a visualization. Plotting will be covered in depth in [*Chapter 5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106),
    *Visualizing Data with Pandas and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*, so don''t worry about the
    details for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how there is a cyclical pattern here? It is dropping every day the market
    is closed because the aggregation only had bitcoin data to sum for those days:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27 – Portfolio closing price without accounting for stock market
    closures'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.27_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.27 – Portfolio closing price without accounting for stock market closures
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, this is a problem; an asset''s value doesn''t drop to zero whenever
    the market is closed. If we want `pandas` to fill the missing data in for us,
    we will need to reindex the S&P 500 data with bitcoin''s index using the `reindex()`
    method, passing one of the following strategies to the `method` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''ffill''`: This method brings values forward. In the previous example, this
    fills the days the market was closed with the data for the last time the market
    was open before those days.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''bfill''`: This method backpropagates the values, which will result in carrying
    future results to past dates, meaning that this isn''t the right option here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''nearest''`: This method fills according to the rows closest to the missing
    ones, which in this case will result in Sundays getting the data for the following
    Mondays, and Saturdays getting the data from the previous Fridays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Forward-filling seems to be the best option, but since we aren''t sure, we
    will see how this works on a few rows of the data first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice any issues with this? Well, the volume traded (`volume`) column makes
    it seem like the days we used forward-filling for are actually days when the market
    is open:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28 – Forward-filling dates with missing values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.28_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.28 – Forward-filling dates with missing values
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The `compare()` method will show us the values that differ across identically-labeled
    dataframes (same index and columns); we can use it to isolate the changes in our
    data when forward-filling here. There is an example in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, we only want to maintain the value of the stock when the stock market
    is closed—the volume traded should be zero. In order to handle the `NaN` values
    in a different manner for each column, we will turn to the `assign()` method.
    To fill any `NaN` values in the `volume` column with `0`, we will use the `fillna()`
    method, which we will see more of in the *Handling duplicate, missing, or invalid
    data* section, later in this chapter. The `fillna()` method also allows us to
    pass in a method instead of a value, so we can forward-fill the `close` column,
    which was the only column that made sense from our previous attempt. Lastly, we
    can use the `np.where()` function for the remaining columns, which allows us to
    build a vectorized `if...else`. It takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '`pandas`, we should avoid writing loops in favor of vectorized operations for
    better performance. NumPy functions are designed to work on arrays, so they are
    perfect candidates for high-performance `pandas` code. This will make it easy
    for us to set any `NaN` values in the `open`, `high`, or `low` columns to the
    value in the `close` column for the same day. Since these come after the `close`
    column gets worked on, we will have the forward-filled value for `close` to use
    for the other columns where necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'On Saturday, January 7th and Sunday, January 8th, we now have volume traded
    at zero. The OHLC prices are all equal to the closing price on Friday, the 6th:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29 – Reindexing the S&P 500 data with specific strategies per column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.29_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.29 – Reindexing the S&P 500 data with specific strategies per column
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use `np.where()` to both introduce a function we will see throughout
    this book and to make it easier to understand what is going on, but note that
    `np.where(x.open.isnull(), x.close, x.open)` can be replaced with the `combine_first()`
    method, which (for this use case) is equivalent to `x.open.combine_first(x.close)`.
    We will use the `combine_first()` method in the *Handling duplicate, missing,
    or invalid data* section, later this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s recreate the portfolio with the reindexed S&P 500 data and use
    a visualization to compare it with the previous attempt (again, don''t worry about
    the plotting code, which will be covered in [*Chapter 5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106),
    *Visualizing Data with Pandas and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The orange dashed line is our original attempt at studying the portfolio (without
    reindexing), while the blue solid line is the portfolio we just built with reindexing
    and different filling strategies per column. Keep this strategy in mind for the
    exercises in [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial
    Analysis – Bitcoin and the Stock Market*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.30 – Visualizing the effect of reindexing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.30_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.30 – Visualizing the effect of reindexing
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use the `reindex()` method to reorder the rows. For example, if
    our data is stored in `x`, then `x.reindex([32, 20, 11])` will return a new `DataFrame`
    object of three rows: 32, 20, and 11 (in that order). This can be done along the
    columns with `axis=1` (the default is `axis=0` for rows).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's turn our attention to reshaping data. Recall that we had to first
    filter the temperature data by the `datatype` column and then sort to find the
    warmest days; reshaping the data will make this easier, and also make it possible
    for us to aggregate and summarize the data.
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data isn't always given to us in the format that's most convenient for our analysis.
    Therefore, we need to be able to restructure data into both wide and long formats,
    depending on the analysis we want to perform. For many analyses, we will want
    wide format data so that we can look at the summary statistics easily and share
    our results in that format.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this isn''t always as black and white as going from long format to
    wide format or vice versa. Consider the following data from the *Exercises* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.31 – Data with some long and some wide format columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.31_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.31 – Data with some long and some wide format columns
  prefs: []
  type: TYPE_NORMAL
- en: It's possible to have data where some of the columns are in wide format (`describe()`
    on this data aren't helpful unless we first filter on `pandas`—we would need `seaborn`.
    Alternatively, we could restructure the data for that visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the motivation for restructuring data, let''s move to
    the next notebook, `4-reshaping_data.ipynb`. We will begin by importing `pandas`
    and reading in the `long_data.csv` file, adding the temperature in Fahrenheit
    column (`temp_F`), and performing some of the data cleaning we just learned about:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Our long format data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.32 – Long format temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.32_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.32 – Long format temperature data
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss transposing, pivoting, and melting our data.
    Note that after reshaping the data, we will often revisit the data cleaning tasks
    as things may have changed, or we may need to change things we couldn't access
    easily before. For example, we will want to perform some type conversion if the
    values were all turned into strings in long format, but in wide format some columns
    are clearly numeric.
  prefs: []
  type: TYPE_NORMAL
- en: Transposing DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While we will be pretty much only working with wide or long formats, `pandas`
    provides ways to restructure our data as we see fit, including taking the **transpose**
    (flipping the rows with the columns), which we may find useful to make better
    use of our display area when we''re printing parts of our dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the index is now in the columns, and that the column names are
    in the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.33 – Transposed temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.33_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.33 – Transposed temperature data
  prefs: []
  type: TYPE_NORMAL
- en: It may not be immediately apparent how useful this can be, but we will see this
    a quite few times throughout this book; for example, to make content easier to
    display in [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial
    Analysis – Bitcoin and the Stock Market*, and to build a particular visualization
    for machine learning in [*Chapter 9*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188),
    *Getting Started with Machine Learning in Python*.
  prefs: []
  type: TYPE_NORMAL
- en: Pivoting DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We `pivot()` method performs this restructuring of our `DataFrame` object.
    To pivot, we need to tell `pandas` which column currently holds the values (with
    the `values` argument) and the column that contains what will become the column
    names in wide format (the `columns` argument). Optionally, we can provide a new
    index (the `index` argument). Let''s pivot into wide format, where we have a column
    for each of the temperature measurements in Celsius and use the dates as the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'In our starting dataframe, there was a `datatype` column that contained only
    `TMAX`, `TMIN`, or `TOBS` as strings. Now, these are column names because we passed
    in `columns=''datatype''`. By passing in `index=''date''`, the `date` column became
    our index, without needing to run `set_index()`. Finally, the values for each
    combination of `date` and `datatype` are the corresponding temperatures in Celsius
    since we passed in `values=''temp_C''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.34 – Pivoting the long format temperature data into wide format'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.34_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.34 – Pivoting the long format temperature data into wide format
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed at the beginning of this chapter, with the data in wide format,
    we can easily get meaningful summary statistics with the `describe()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that we have 31 observations for all three temperature measurements
    and that this month has a wide range of temperatures (highest daily maximum of
    26.7°C and lowest daily minimum of -1.1°C):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.35 – Summary statistics on the pivoted temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.35_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.35 – Summary statistics on the pivoted temperature data
  prefs: []
  type: TYPE_NORMAL
- en: 'We lost the temperature in Fahrenheit, though. If we want to keep it, we can
    provide multiple columns to `values`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we now get an extra level above the column names. This is called a
    **hierarchical index**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.36 – Pivoting with multiple value columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.36_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.36 – Pivoting with multiple value columns
  prefs: []
  type: TYPE_NORMAL
- en: 'With this hierarchical index, if we want to select `TMIN` in Fahrenheit, we
    will first need to select `temp_F` and then `TMIN`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In cases where we need to perform an aggregation as we pivot (due to duplicate
    values in the index), we can use the `pivot_table()` method, which we will discuss
    in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082), *Aggregating
    Pandas DataFrames*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have been working with a single index throughout this chapter; however,
    we can create an index from any number of columns with `set_index()`. This gives
    us an index of type `MultiIndex`, where the outermost level corresponds to the
    first element in the list provided to `set_index()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we now have two levels in the index—`date` is the outermost level
    and `datatype` is the innermost:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.37 – Working with a multi-level index'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.37_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.37 – Working with a multi-level index
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pivot()` method expects the data to only have one column to set as the
    index; if we have a multi-level index, we should use the `unstack()` method instead.
    We can use `unstack()` on `multi_index_df` and get a similar result to what we
    had previously. Order matters here because, by default, `unstack()` will move
    the innermost level of the index to the columns; in this case, that means we will
    keep the `date` level in the index and move the `datatype` level to the column
    names. To unstack a different level, simply pass in the index of the level to
    unstack, where 0 is the leftmost and -1 is the rightmost, or the name of the level
    (if it has one). Here, we will use the default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'With `multi_index_df`, we had `datatype` as the innermost level of the index,
    so, after using `unstack()`, it is along the columns. Note that we once again
    have a hierarchical index in the columns. In [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, we will discuss a way to squash this back into
    a single level of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.38 – Unstacking a multi-level index to pivot data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.38_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.38 – Unstacking a multi-level index to pivot data
  prefs: []
  type: TYPE_NORMAL
- en: 'The `unstack()` method has the added benefit of allowing us to specify how
    to fill in missing values that come into existence upon reshaping the data. To
    do so, we can use the `fill_value` parameter. Imagine a case where we have been
    given the data for `TAVG` for October 1, 2018 only. We could append this to `long_df`
    and set our index to the `date` and `datatype` columns, as we did previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have four temperature measurements for October 1, 2018, but only three
    for the remaining days:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.39 – Introducing an additional temperature measurement into the
    data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.39_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.39 – Introducing an additional temperature measurement into the data
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `unstack()`, as we did previously, will result in `NaN` values for most
    of the `TAVG` data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the `TAVG` columns after we unstack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.40 – Unstacking can lead to null values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.40_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.40 – Unstacking can lead to null values
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this, we can pass in an appropriate `fill_value`. However, we are
    restricted to passing in a value for this, not a strategy (as we saw when we discussed
    reindexing), so while there is no good value for this case, we can use `-40` to
    illustrate how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The `NaN` values have now been replaced with `-40.0`. However, note that now
    both `temp_C` and `temp_F` have the same temperature reading. Actually, this is
    the reason we picked `-40` for `fill_value`; it is the temperature at which both
    Fahrenheit and Celsius are equal, so we won''t confuse people with them both being
    the same number; for example, `0` (since 0°C = 32°F and 0°F = -17.78°C). Since
    this temperature is also much colder than the temperatures measured in New York
    City and is below `TMIN` for all the data we have, it is more likely to be deemed
    a data entry error or a signal that data is missing compared to if we had used
    `0`. Note that, in practice, it is better to be explicit about the missing data
    if we are sharing this with others and leave the `NaN` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.41 – Unstacking with a default value for missing combinations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.41_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.41 – Unstacking with a default value for missing combinations
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, `unstack()` should be our method of choice when we have a multi-level
    index and would like to move one or more of the levels to the columns; however,
    if we are simply using a single index, the `pivot()` method's syntax is likely
    to be easier to specify correctly since it's more apparent which data will end
    up where.
  prefs: []
  type: TYPE_NORMAL
- en: Melting DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To go from wide format to long format, we need to `wide_data.csv` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Our wide data contains a column for the date and a column for each temperature
    measurement we have been working with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.42 – Wide format temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.42_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.42 – Wide format temperature data
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `melt()` method for flexible reshaping—allowing us to turn this
    into long format, similar to what we got from the API. Melting requires that we
    specify the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Which column(s) uniquely identify a row in the wide format data with the `id_vars`
    argument
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which column(s) contain(s) the variable(s) with the `value_vars` argument
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, we can also specify how to name the column containing the variable
    names in the long format data (`var_name`) and the name for the column containing
    their values (`value_name`). By default, these will be `variable` and `value`,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s use the `melt()` method to turn the wide format data into long
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The `date` column was the identifier for our rows, so we provided that as `id_vars`.
    We turned the values in the `TMAX`, `TMIN`, and `TOBS` columns into a single column
    with the temperatures (`value_vars`) and used their column names as the values
    for a measurement column (`var_name=''measurement''`). Lastly, we named the values
    column (`value_name=''temp_C''`). We now have just three columns; the date, the
    temperature reading in Celsius (`temp_C`), and a column indicating which temperature
    measurement is in that row''s `temp_C` cell (`measurement`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.43 – Melting the wide format temperature data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.43_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.43 – Melting the wide format temperature data
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we had an alternative way of pivoting data with the `unstack()` method,
    we also have another way of melting data with the `stack()` method. This method
    will pivot the columns into the innermost level of the index (resulting in an
    index of type `MultiIndex`), so we need to double-check our index before calling
    it. It also lets us drop row/column combinations that result in no data, if we
    choose. We can do the following to get a similar output to the `melt()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the result came back as a `Series` object, so we will need to create
    the `DataFrame` object once more. We can use the `to_frame()` method and pass
    in a name to use for the column once it is a dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have a dataframe with a multi-level index, containing `date` and `datatype`,
    with `values` as the only column. Notice, however, that only the `date` portion
    of our index has a name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.44 – Stacking to melt the temperature data into long format'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.44_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.44 – Stacking to melt the temperature data into long format
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, we used `set_index()` to set the index to the `date` column because
    we didn''t want to melt that; this formed the first level of the multi-level index.
    Then, the `stack()` method moved the `TMAX`, `TMIN`, and `TOBS` columns into the
    second level of the index. However, this level was never named, so it shows up
    as `None`, but we know that the level should be called `datatype`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `set_names()` method to address this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Now that we understand the basics of data cleaning and reshaping, we will walk
    through an example of how these techniques can be combined when working with data
    that contains various issues.
  prefs: []
  type: TYPE_NORMAL
- en: Handling duplicate, missing, or invalid data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have discussed things we could change with the way the data was
    represented with zero ramifications. However, we didn''t discuss a very important
    part of data cleaning: how to deal with data that appears to be duplicated, invalid,
    or missing. This is separated from the rest of the data cleaning discussion because
    it is an example where we will do some initial data cleaning, then reshape our
    data, and finally look to handle these potential issues; it is also a rather hefty
    topic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be working in the `5-handling_data_issues.ipynb` notebook and using
    the `dirty_data.csv` file. Let''s start by importing `pandas` and reading in the
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The `dirty_data.csv` file contains wide format data from the weather API that
    has been altered to introduce many common data issues that we will encounter in
    the wild. It contains the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PRCP`: Precipitation in millimeters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SNOW`: Snowfall in millimeters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SNWD`: Snow depth in millimeters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TMAX`: Maximum daily temperature in Celsius'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TMIN`: Minimum daily temperature in Celsius'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TOBS`: Temperature at the time of observation in Celsius'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WESF`: Water equivalent of snow in millimeters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This section is divided into two parts. In the first part, we will discuss some
    tactics to uncover issues within a dataset, and in the second part, we will learn
    how to mitigate some of the issues present in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the problematic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035), *Working
    with Pandas DataFrames*, we learned the importance of examining our data when
    we get it; it''s not a coincidence that many of the ways to inspect the data will
    help us find these issues. Examining the results of calling `head()` and `tail()`
    on the data is always a good first step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'In practice, `head()` and `tail()` aren''t as robust as the rest of what we
    will discuss in this section, but we can still get some useful information by
    starting here. Our data is in wide format, and at a quick glance, we can see that
    we have some potential issues. Sometimes, the `station` field is recorded as `?`,
    while other times, it has a station ID. We have values of negative infinity (`-inf`)
    for snow depth (`SNWD`), along with some very hot temperatures for `TMAX`. Lastly,
    we can observe many `NaN` values in several columns, including the `inclement_weather`
    column, which appears to also contain Boolean values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.45 – Dirty data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.45_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.45 – Dirty data
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `describe()`, we can see if we have any missing data and look at the
    5-number summary to spot potential issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The `SNWD` column appears to be useless, and the `TMAX` column seems unreliable.
    For perspective, the temperature of the Sun''s photosphere is around 5,505°C,
    so we certainly wouldn''t expect to observe those air temperatures in New York
    City (or anywhere on Earth, for that matter). This likely means that the `TMAX`
    column was set to a nonsensical, large number when it wasn''t available. The fact
    that it is so large is actually what helps identify it using the summary statistics
    we get from `describe()`. If unknowns were encoded with another value, say 40°C,
    we couldn''t be sure it wasn''t actual data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.46 – Summary statistics for the dirty data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.46_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.46 – Summary statistics for the dirty data
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `info()` method to see if we have any missing values and check
    that our columns have the expected data types. In doing so, we immediately see
    two issues: we have 765 rows, but for five of the columns, we have many fewer
    non-null entries. This output also shows us that the data type of the `inclement_weather`
    column is not Boolean, though we may have thought so from the name. Notice that
    the `?` value that we saw for the `station` column when we used `head()` doesn''t
    show up here—it''s important to inspect our data from many different angles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s track down those null values. Both `Series` and `DataFrame` objects
    provide two methods to do so: `isnull()` and `isna()`. Note that if we use the
    method on the `DataFrame` object, the result will tell us which rows have all
    null values, which isn''t what we want in this case. Here, we want to examine
    the rows that have null values in the `SNOW`, `SNWD`, `TOBS`, `WESF`, or `inclement_weather`
    columns. This means that we will need to combine checks for each of the columns
    with the `|` (bitwise OR) operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at the `shape` attribute of our `contain_nulls` dataframe, we will
    see that every single row contains some null data. Looking at the top 10 rows,
    we can see some `NaN` values in each of these rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.47 – Rows in the dirty data with nulls'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.47_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.47 – Rows in the dirty data with nulls
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: By default, the `sort_values()` method that we discussed earlier in this chapter
    will put any `NaN` values last. We can change this behavior (to put them first)
    by passing in `na_position='first'`, which can also be helpful when looking for
    patterns in the data when the sort columns have null values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can''t check whether the value of the column is equal to `NaN`
    because `NaN` is not equal to anything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We must use the aforementioned options (`isna()`/`isnull()`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `inf` and `-inf` are actually `np.inf` and `-np.inf`. Therefore,
    we can find the number of rows with `inf` or `-inf` values by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'This only tells us about a single column, though, so we could write a function
    that will use a dictionary comprehension to return the number of infinite values
    per column in our dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Using our function, we find that the `SNWD` column is the only column with
    infinite values, but the majority of the values in the column are infinite:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can decide on how to handle the infinite values of snow depth, we
    should look at the summary statistics for snowfall (`SNOW`), which forms a big
    part of determining the snow depth (`SNWD`). To do so, we can make a dataframe
    with two series, where one contains the summary statistics for the snowfall column
    when the snow depth is `np.inf`, and the other for when it is `-np.inf`. In addition,
    we will use the `T` attribute to transpose the data for easier viewing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The snow depth was recorded as negative infinity when there was no snowfall;
    however, we can''t be sure this isn''t just a coincidence going forward. If we
    are just going to be working with this fixed date range, we can treat that as
    having a depth of `0` or `NaN` because it didn''t snow. Unfortunately, we can''t
    really make any assumptions with the positive infinity entries. They most certainly
    aren''t that, but we can''t decide what they should be, so it''s probably best
    to leave them alone or not look at this column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.48 – Summary statistics for snowfall when snow depth is infinite'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.48_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.48 – Summary statistics for snowfall when snow depth is infinite
  prefs: []
  type: TYPE_NORMAL
- en: 'We are working with a year of data, but somehow, we have 765 rows, so we should
    check why. The only columns we have yet to inspect are the `date` and `station`
    columns. We can use the `describe()` method to see the summary statistics for
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'In 765 rows of data, the `date` column only has 324 unique values (meaning
    that some dates are missing), with some dates being present as many as eight times
    (`station` column, with the most frequent being `?` when we used `head()` earlier
    (*Figure 3.45*), we know that is the other value; however, we can use `unique()`
    to see all the unique values if we hadn''t. We also know that `?` occurs 367 times
    (765 - 398), without the need to use `value_counts()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.49 – Summary statistics for the non-numeric columns in the dirty
    data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.49_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.49 – Summary statistics for the non-numeric columns in the dirty data
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we may not know why the station is sometimes recorded as `?`—it
    could be intentional to show that they don't have the station, an error in the
    recording software, or an accidental omission that got encoded as `?`. How we
    deal with this would be a judgment call, as we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon seeing that we had 765 rows of data and two distinct values for the station
    ID, we might have assumed that each day had two entries—one per station. However,
    this would only account for 730 rows, and we also now know that we are missing
    some dates. Let''s see whether we can find any duplicate data that could account
    for this. We can use the result of the `duplicated()` method as a Boolean mask
    to find the duplicate rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on what we are trying to achieve, we may handle duplicates differently.
    The rows that are returned can be modified with the `keep` argument. By default,
    it is `''first''`, and, for each row that is present more than once, we will get
    only the additional rows (besides the first). However, if we pass in `keep=False`,
    we will get all the rows that are present more than once, not just each additional
    appearance they make:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'There is also a `subset` argument (first positional argument), which allows
    us to focus just on the duplicates of certain columns. Using this, we can see
    that when the `date` and `station` columns are duplicated, so is the rest of the
    data because we get the same result as before. However, we don''t know if this
    is actually a problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s examine a few of the duplicated rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Just looking at the first five rows shows us that some rows are repeated at
    least three times. Remember that the default behavior of `duplicated()` is to
    not show the first occurrence, which means that rows **1** and **2** have another
    matching value in the data (same for rows **5** and **6**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.50 – Examining the duplicate data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.50_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.50 – Examining the duplicate data
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to find problems in our data, let's learn about some ways
    we can try to address them. Note that there is no panacea here, and it will often
    come down to knowing the data we are working with and making judgment calls.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating the issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are in an unsatisfactory state with our data, and while we can work to make
    it better, the best plan of action isn't always evident. Perhaps the easiest thing
    we can do when faced with this class of data issues is to remove the duplicate
    rows. However, it is crucial that we evaluate the ramifications such a decision
    may have on our analysis. Even in cases where it appears that the data we are
    working with was collected from a larger dataset that had additional columns,
    thus making all our data distinct, we can't be sure that removing these columns
    is the reason the remaining data was duplicated—we would need to consult the source
    of the data and any available documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we know that both stations will be for New York City, we may decide to
    drop the `station` column—they may have just been collecting different data. If
    we then decide to remove duplicate rows using the `date` column and keep the data
    for the station that wasn''t `?`, in the case of duplicates, we will lose all
    data we have for the `WESF` column because the `?` station is the only one reporting
    `WESF` measurements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'One satisfactory solution in this case may be to carry out the following actions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform type conversion on the `date` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the `WESF` column as a series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sort the dataframe by the `station` column in descending order to put the station
    with no ID (`?`) last:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove rows that are duplicated based on the date, keeping the first occurrences,
    which will be ones where the `station` column has an ID (if that station has measurements).
    Note that `drop_duplicates()` can be done in-place, but if what we are trying
    to do is complicated, it''s best not to start out with the in-place operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Drop the `station` column and set the index to the `date` column (so that it
    matches the `WESF` data):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the `WESF` column using the `combine_first()` method to `?`). Since
    both `df_deduped` and `station_qm_wesf` are using the date as the index, the values
    are properly matched to the appropriate date:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This may sound a little complicated, but that''s largely because we haven''t
    learned about aggregation yet. In [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, we will look at another way to go about this.
    Let''s take a look at the result using the aforementioned implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now left with 324 rows—one for each unique date in our data. We were
    able to save the `WESF` column by putting it alongside the data from the other
    station:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.51 – Using data wrangling to keep the information in the WESF column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.51_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.51 – Using data wrangling to keep the information in the WESF column
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We could have also specified to keep the last entry instead of the first or
    drop all duplicates with the `keep` argument, just like when we checked for duplicates
    with `duplicated()`. Keep this in mind as the `duplicated()` method can be useful
    in giving the results of a dry run on a deduplication task.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's deal with the null data. We can choose to drop it, replace it with
    some arbitrary value, or impute it using surrounding data. Each of these options
    has its ramifications. If we drop the data, we are going about our analysis with
    only part of the data; if we end up dropping half the rows, this is going to have
    a big impact. When changing the values of the data, we may be affecting the outcome
    of our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To drop all the rows with any null data (this doesn''t have to be true for
    all the columns of the row, so be careful), we use the `dropna()` method; in our
    case, this leaves us with just 4 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'We can change the default behavior to only drop a row if all the columns are
    null with the `how` argument, except this doesn''t get rid of anything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Thankfully, we can also use a subset of columns to determine what to drop.
    Say we wanted to look at snow data; we would most likely want to make sure that
    our data had values for `SNOW`, `SNWD`, and `inclement_weather`. This can be achieved
    with the `subset` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this operation can also be performed along the columns, and that
    we can provide a threshold for the number of null values that must be observed
    to drop the data with the `thresh` argument. For example, if we say that at least
    75% of the rows must be null to drop the column, we will drop the `WESF` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Since we have a lot of null values, we will likely be more interested in keeping
    these values, and perhaps finding a better way to represent them. If we replace
    the null data, we must exercise caution when deciding what to fill in instead;
    filling in all the values we don't have with some other value may yield strange
    results later on, so we must think about how we will use this data first.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fill in null values with other data, we use the `fillna()` method, which
    gives us the option of specifying a value or a strategy for how to perform the
    filling. We will discuss filling with a single value first. The `WESF` column
    contains mostly null values, but since it is a measurement in milliliters that
    takes on the value of `NaN` when there is no water equivalent of snowfall, we
    can fill in the nulls with zeros. Note that this can be done in-place (again,
    as a general rule of thumb, we should use caution with in-place operations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'The `WESF` column no longer contains `NaN` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.52 – Filling in null values in the WESF column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.52_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.52 – Filling in null values in the WESF column
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have done everything we can without distorting the data. We
    know that we are missing dates, but if we reindex, we don't know how to fill in
    the resulting `NaN` values. With the weather data, we can't assume that because
    it snowed one day that it will snow the next, or that the temperature will be
    the same. For this reason, note that the following examples are just for illustrative
    purposes only—just because we can do something doesn't mean we should. The right
    solution will most likely depend on the domain and the problem we are looking
    to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'That being said, let''s try to address some of the remaining issues with the
    temperature data. We know that when `TMAX` is the temperature of the Sun, it must
    be because there was no measured value, so let''s replace it with `NaN`. We will
    also do so for `TMIN`, which currently uses -40°C for its placeholder, despite
    the coldest temperature ever recorded in NYC being -15°F (-26.1°C) on February
    9, 1934 ([https://www.weather.gov/media/okx/Climate/CentralPark/extremes.pdf](https://www.weather.gov/media/okx/Climate/CentralPark/extremes.pdf)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also make an assumption that the temperature won''t change drastically
    from day to day. Note that this is actually a big assumption, but it will allow
    us to understand how the `fillna()` method works when we provide a strategy through
    the `method` parameter: `''ffill''` to forward-fill or `''bfill''` to back-fill.
    Notice we don''t have the `''nearest''` option, like we did when we were reindexing,
    which would have been the best option; so, to illustrate how this works, let''s
    use forward-filling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the `TMAX` and `TMIN` columns on January 1st and 4th. Both are
    `NaN` on the 1st because we don''t have data before then to bring forward, but
    the 4th now has the same values as the 3rd:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.53 – Forward-filling null values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.53_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.53 – Forward-filling null values
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to handle the nulls and infinite values in the `SNWD` column, we
    can use the `np.nan_to_num()` function; it turns `NaN` into 0 and `inf`/`-inf`
    into very large positive/negative finite numbers, making it possible for machine
    learning models (discussed in [*Chapter 9*](B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188)*,
    Getting Started with Machine Learning in Python*) to learn from this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'This doesn''t make much sense for our use case though. For instances of `-np.inf`,
    we may choose to set `SNWD` to 0 since we saw there was no snowfall on those days.
    However, we don''t know what to do with `np.inf`, and the large positive numbers,
    arguably, make this more confusing to interpret:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.54 – Replacing infinite values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.54_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.54 – Replacing infinite values
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the data we are working with, we may choose to use the `clip()`
    method as an alternative to the `np.nan_to_num()` function. The `clip()` method
    makes it possible to cap values at a specific minimum and/or maximum threshold.
    Since the snow depth can''t be negative, let''s use `clip()` to enforce a lower
    bound of zero. To show how the upper bound works, we will use the snowfall (`SNOW`)
    as an estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'The values of `SNWD` for January 1st through 3rd are now `0` instead of `-inf`,
    while the values of `SNWD` for January 4th and 5th went from `inf` to that day''s
    value for `SNOW`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.55 – Capping values at thresholds'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.55_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.55 – Capping values at thresholds
  prefs: []
  type: TYPE_NORMAL
- en: Our last strategy is imputation. When we replace a missing value with a new
    value derived from the data, using summary statistics or data from other observations,
    it is called **imputation**. For example, we can impute with the mean to replace
    temperature values. Unfortunately, if we are only missing values for the end of
    the month of October, and we replace them with the mean of the values from the
    rest of the month, this is likely to be skewed toward the extreme values, which
    are the warmer temperatures at the beginning of October, in this case. Like everything
    else that was discussed in this section, we must exercise caution and think about
    any potential consequences or side effects of our actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can combine imputation with the `fillna()` method. As an example, let''s
    fill in the `NaN` values for `TMAX` and `TMIN` with their medians and `TOBS` with
    the average of `TMIN` and `TMAX` (after imputing them):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice from the changes to the data for January 1st and 4th that the median
    maximum and minimum temperatures were 14.4°C and 5.6°C, respectively. This means
    that when we impute `TOBS` and also don''t have `TMAX` and `TMIN` in the data,
    we get 10°C:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.56 – Imputing missing values with summary statistics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.56_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.56 – Imputing missing values with summary statistics
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to run the same calculation on all the columns, we should use the
    `apply()` method instead of `assign()`, since it saves us the redundancy of having
    to write the same calculation for each of the columns. For example, let''s fill
    in all the missing values with the rolling 7-day median of their values, setting
    the number of periods required for the calculation to zero to ensure that we don''t
    introduce extra null values. We will cover rolling calculations and `apply()`
    in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082), *Aggregating
    Pandas DataFrames*, so this is just a preview:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s kind of hard to tell where our imputed values are here—temperatures can
    fluctuate quite a bit day to day. We know that January 4th had missing data from
    our previous attempt; our imputed temperatures are colder that day than those
    around it with this strategy. In reality, it was slightly warmer that day (around
    -3°C):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.57 – Imputing missing values with the rolling median'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.57_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.57 – Imputing missing values with the rolling median
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It's important to exercise caution when imputing. If we pick the wrong strategy
    for the data, we can make a real mess of things.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way of imputing missing data is to have `pandas` calculate what the
    values should be with the `interpolate()` method. By default, it will perform
    linear interpolation, making the assumption that all the rows are evenly spaced.
    Our data is daily data, although some days are missing, so it is just a matter
    of reindexing first. Let''s combine this with the `apply()` method to interpolate
    all of our columns at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'Check out January 9th, which we didn''t have previously—the values for `TMAX`,
    `TMIN`, and `TOBS` are the average of the values for the day prior (January 8th)
    and the day after (January 10th):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.58 – Interpolating missing values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.58_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.58 – Interpolating missing values
  prefs: []
  type: TYPE_NORMAL
- en: Different strategies for interpolation can be specified via the `method` argument;
    be sure to check out the `interpolate()` method documentation to view the available
    options.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on making it through this chapter! Data wrangling may not be
    the most exciting part of the analytics workflow, but we will spend a lot of time
    on it, so it's best to be well versed in what `pandas` has to offer.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned more about what data wrangling is (aside from a
    data science buzzword) and got some firsthand experience with cleaning and reshaping
    our data. Utilizing the `requests` library, we once again practiced working with
    APIs to extract data of interest; then, we used `pandas` to begin our introduction
    to data wrangling, which we will continue in the next chapter. Finally, we learned
    how to deal with duplicate, missing, and invalid data points in various ways and
    discussed the ramifications of those decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Building on these concepts, in the next chapter, we will learn how to aggregate
    dataframes and work with time series data. Be sure to complete the end-of-chapter
    exercises before moving on.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Complete the following exercises using what we have learned so far in this
    book and the data in the `exercises/` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to look at data for the `stock_analysis` package we will build in [*Chapter
    7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial Analysis – Bitcoin
    and the Stock Market*). Combine them into a single file and store the dataframe
    of the FAANG data as `faang` for the rest of the exercises:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Read in the `aapl.csv`, `amzn.csv`, `fb.csv`, `goog.csv`, and `nflx.csv`
    files.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Add a column to each dataframe, called `ticker`, indicating the ticker symbol
    it is for (Apple's is AAPL, for example); this is how you look up a stock. In
    this case, the filenames happen to be the ticker symbols.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Append them together into a single dataframe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Save the result in a CSV file called `faang.csv`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With `faang`, use type conversion to cast the values of the `date` column into
    datetimes and the `volume` column into integers. Then, sort by `date` and `ticker`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the seven rows in `faang` with the lowest value for `volume`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Right now, the data is somewhere between long and wide format. Use `melt()`
    to make it completely long format. Hint: `date` and `ticker` are our ID variables
    (they uniquely identify each row). We need to melt the rest so that we don''t
    have separate columns for `open`, `high`, `low`, `close`, and `volume`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose we found out that on July 26, 2018 there was a glitch in how the data
    was recorded. How should we handle this? Note that there is no coding required
    for this exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `covid19_cases.csv` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b) Create a `date` column using the data in the `dateRep` column and the `pd.to_datetime()`
    function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Set the `date` column as the index and sort the index.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) Replace all occurrences of `United_States_of_America` and `United_Kingdom`
    with `USA` and `UK`, respectively. Hint: the `replace()` method can be run on
    the dataframe as a whole.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Using the `countriesAndTerritories` column, filter the cleaned COVID-19 cases
    data down to Argentina, Brazil, China, Colombia, India, Italy, Mexico, Peru, Russia,
    Spain, Turkey, the UK, and the USA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f) Pivot the data so that the index contains the dates, the columns contain
    the country names, and the values are the case counts (the `cases` column). Be
    sure to fill in `NaN` values with `0`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to determine the case totals per country efficiently, we need the
    aggregation skills we will learn about in [*Chapter 4*](B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Aggregating Pandas DataFrames*, so the ECDC data in the `covid19_cases.csv` file
    has been aggregated for us and saved in the `covid19_total_cases.csv` file. It
    contains the total number of cases per country. Use this data to find the 20 countries
    with the largest COVID-19 case totals. Hints: when reading in the CSV file, pass
    in `index_col=''cases''`, and note that it will be helpful to transpose the data
    before isolating the countries.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out the following resources for more information on the topics that were
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A Quick-Start Tutorial on Relational Database Design*: [https://www.ntu.edu.sg/home/ehchua/programming/sql/relational_database_design.html](https://www.ntu.edu.sg/home/ehchua/programming/sql/relational_database_design.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Binary search*: [https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/binary-search](https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/binary-search)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How Recursion Works—explained with flowcharts and a video*: [https://www.freecodecamp.org/news/how-recursion-works-explained-with-flowcharts-and-a-video-de61f40cb7f9/](https://www.freecodecamp.org/news/how-recursion-works-explained-with-flowcharts-and-a-video-de61f40cb7f9/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python f-strings*: [https://realpython.com/python-f-strings/](https://realpython.com/python-f-strings/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tidy Data (article by Hadley Wickham)*: [https://www.jstatsoft.org/article/view/v059i10](https://www.jstatsoft.org/article/view/v059i10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*5 Golden Rules for Great Web API Design*: [https://www.toptal.com/api-developers/5-golden-rules-for-designing-a-great-web-api](https://www.toptal.com/api-developers/5-golden-rules-for-designing-a-great-web-api)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
