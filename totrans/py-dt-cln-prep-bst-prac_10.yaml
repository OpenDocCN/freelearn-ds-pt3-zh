- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Handling Categorical Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Handling categorical features involves representing and processing information
    that isn’t inherently numerical. **Categorical features** are attributes that
    can take on a limited, fixed number of values or categories, and they often define
    distinct categories or groups within a dataset, such as types of products, genres
    of books, or customer segments. Effectively managing categorical data is crucial
    because most **machine learning** (**ML**) algorithms require numerical inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Label encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Target encoding (mean encoding)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The complete code for this chapter can be found in the following GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter10](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter10)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s install the necessary libraries we will use in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Label encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Label encoding** is a technique for handling categorical data by converting
    each category into a unique integer. It’s suitable for categorical features with
    ordinal relationships, where there is a clear ranking or order among the categories.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, when dealing with educational levels such as “high school,” “bachelor’s,”
    “master’s,” and “Ph.D.,” label encoding can be used because there’s a clear order
    from the least to most advanced level of education.
  prefs: []
  type: TYPE_NORMAL
- en: Use case – employee performance analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **Human Resources** (**HR**) department wants to analyze employee performance
    data to understand the relationship between employee ratings and other factors
    such as salary, years of experience, and department. They plan to use ML to predict
    employee ratings based on these factors.
  prefs: []
  type: TYPE_NORMAL
- en: The data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s have a quick look at the data we have available for the performance analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Employee Rating`: Categorical feature with `Poor`, `Satisfactory`, `Good`,
    and `Excellent` values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Salary`: Numeric feature representing employee salaries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Years of Experience`: Numeric feature indicating the number of years an employee
    has worked'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Department`: Categorical feature specifying the department in which the employee
    works'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s have a look at the original DataFrame before the encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Having understood the data, we can move to the objective of the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Objective of the use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The use case’s objective is to encode the `Employee Rating` feature using label
    encoding to prepare the data for ML analysis. Let’s see how we can do this using
    scikit-learn the complete code can be found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/1a.label_encoding.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/1a.label_encoding.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a sample dataset and turn it into a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a `LabelEncoder` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply label encoding to the `Employee` `Rating` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s have a look at the encoded output we created.
  prefs: []
  type: TYPE_NORMAL
- en: Encoded output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this use case, label encoding is applied to the `Employee Rating` feature
    to convert it into numeric values while preserving the ordinal relationship. The
    following table shows the output of the encoding operation.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `Employee Rating` | `Salary` | `Years` `of Experience` | `Department`
    | `Employee` `Rating (Encoded)` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `Poor` | `35000` | `2` | `HR` | `2` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `Good` | `50000` | `5` | `IT` | `1` |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `Satisfactory` | `42000` | `3` | `FINANCE` | `3` |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | `Excellent` | `60000` | `8` | `IT` | `0` |'
  prefs: []
  type: TYPE_TB
- en: '| `5` | `Good` | `52000` | `6` | `MARKETING` | `1` |'
  prefs: []
  type: TYPE_TB
- en: Table 10.1 – Output dataset after label encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, an `Employee Rating (Encoded)` feature has been added, and
    all the items are now numeric. Let’s have a look at the distribution graphs for
    the encoded column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Distribution before and after the encoding](img/B19801_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Distribution before and after the encoding
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, there are no changes in the distribution before and after encoding.
    Label encoding converts categorical labels into numerical values *while preserving
    the original data distribution*. It simply assigns unique integer values to each
    category *without altering their frequency*. However, visually, the labels on
    the *x* axis will change from categorical values to numerical values, but the
    counts (or frequencies) of each label will remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the data is shuffled or the order of the categories changes between different
    runs of the encoder, the encoded values might differ. This is because the assignment
    of integers to categories can depend on the order in which they appear. Also,
    if you initialize a new instance of the label encoder each time, the mapping of
    categories to integers might vary as well. For consistent results, you should
    fit the encoder once and then use it for transforming data.
  prefs: []
  type: TYPE_NORMAL
- en: The encoded values can then be used as input features for an ML model to predict
    employee ratings based on salary, years of experience, and department. Let’s now
    discuss some things to keep in mind when encoding features with a label encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for label encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When performing label encoding, especially on large datasets, there are several
    important considerations to keep in mind. Ensure that categorical features have
    a meaningful order. If there’s no natural order among the categories, label encoding
    might not be appropriate. Label encoding assigns *integer values to categories
    based on alphabetical order*. This introduces a potential issue if the categories
    do not have an inherent order, but the model might interpret the numerical values
    as ordered. For example, `Poor`, `Good`, and `Excellent` might be encoded as `2`,
    `1`, and `0`, respectively, but `Poor` is not inherently greater than `Good`.
    This is exactly what happened in the use case presented previously. What we could
    have done to ensure that the label encoding reflects the correct ordinal order
    (that is, `Poor` < `Satisfactory` < `Good` < `Excellent`) was to manually set
    the order by specifying the desired mapping, as shown next the complete code can
    be found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/1b.label_encoding_forced.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/1b.label_encoding_forced.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the correct order of categories with prefixes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Map the `Employee Rating` column to the prefixed categories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting DataFrame is presented as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Always keep consistency in mind when encoding *across training and test datasets*.
    The encoder should be fitted *on the training data* and used to transform both
    training and test datasets. This prevents issues where unseen categories in the
    test set lead to errors or incorrect encoding. Follow the next steps as best practice:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Apply label encoding to the `Employee` `Rating` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the encoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the encoder (in another script or session):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the new data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The last point to mention, important when dealing with large datasets, is that
    label encoding is generally more memory-efficient than one-hot encoding, which
    can create many binary columns.
  prefs: []
  type: TYPE_NORMAL
- en: While label encoding is a straightforward approach for converting categorical
    data into numerical form, it can inadvertently introduce ordinal relationships
    between categories that don’t inherently exist. To avoid this issue and ensure
    that each category is treated independently, one-hot encoding is often a more
    suitable method.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**One-hot encoding** is a technique used to convert categorical data into a
    binary matrix (1s and 0s). Each category is transformed into a new column, and
    a 1 is placed in the column corresponding to the category present for each observation,
    while all other columns get a 0\. This method is particularly useful when dealing
    with categorical data where there is **no ordinal relationship** **among categories**.'
  prefs: []
  type: TYPE_NORMAL
- en: When to use one-hot encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One-hot encoding is suitable for categorical data that lacks a natural order
    or ranking among categories. Here are some scenarios where it is appropriate:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nominal categorical data**: When dealing with nominal data, where categories
    are distinct and have no inherent order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithms that don’t handle ordinal data**: Some ML algorithms (for example,
    decision trees and random forests) are not designed to handle ordinal data correctly.
    One-hot encoding ensures that each category is treated as a separate entity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preventing misinterpretation**: To prevent a model from assuming any ordinal
    relationship that doesn’t exist, one-hot encoding is used to represent categorical
    data as binary values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s look at a use case where we can use one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Use case – customer churn prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A telecommunications company is experiencing high customer churn and wants to
    build an ML model to predict which customers are likely to leave their service.
    They have collected data on customer demographics, contract details, and services
    used.
  prefs: []
  type: TYPE_NORMAL
- en: The data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s have a quick look at the data we have available for the analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Contract Type`: Categorical feature with `Month-to-Month`, `One Year`, and
    `Two` `Year` values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Internet Service`: Categorical feature with `DSL`, `Fiber Optic`, and `No
    Internet` `Service` values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Payment Method`: Categorical feature with `Electronic Check`, `Mailed Check`,
    `Bank Transfer`, and `Credit` `Card` values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s have a look at the sample data for the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Having understood the data, we can move to the objective of the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Objective of the use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The objective of the use case is to encode the categorical features using one-hot
    encoding to prepare the data for ML analysis. The code for this example can be
    found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/2.one_hot_encoding.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/2.one_hot_encoding.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize a `OneHotEncoder` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit and transform the categorical columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new DataFrame with the one-hot encoded columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Concatenate the one-hot encoded DataFrame with the original DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Drop the original categorical columns as they are now encoded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s have a look at the encoded output we created.
  prefs: []
  type: TYPE_NORMAL
- en: Encoded output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this use case, we are preparing customer data for a churn prediction model.
    Categorical features such as `Contract Type`, `Internet Service`, and `Payment
    Method` are one-hot encoded to convert them into binary representations suitable
    for ML. These encoded features can be used to train a predictive model that helps
    the telecommunications company identify customers at risk of churning and take
    proactive measures to retain them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see with some plots how the distribution of the features changes when
    we are applying the encoding. Let’s have a look at the original distribution before
    the encoding first:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Distribution before the one-hot encoding](img/B19801_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Distribution before the one-hot encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'After encoding, each value of the categorical variables is turned into a unique
    column, showcasing binary values (0 or 1) reflecting the presence of that category
    in each row of the dataset. Let’s see the distribution graphs for the `Contract`
    `Type` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Distribution after the one-hot encoding for the Contract Type
    feature](img/B19801_10_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Distribution after the one-hot encoding for the Contract Type
    feature
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the original categorical data helps understand the data distribution
    and identify any imbalances. Visualizing the encoded columns ensures that the
    transformation has been applied correctly. Each binary column should only have
    values of 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discuss some things to keep in mind when encoding features with a
    one-hot encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for one-hot encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When performing one-hot encoding, especially on large datasets, there are several
    important considerations to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding can significantly increase the dimensionality of your dataset,
    especially when you have many categories. This can lead to the “curse of dimensionality,”
    which can be problematic for some algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collinearity**: Since each category is represented as a separate binary column,
    there can be collinearity between these columns. This means some columns might
    be highly correlated, which can affect the performance of linear models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling missing values**: Decide how to handle missing values in categorical
    features before applying one-hot encoding. You may choose to create a separate
    column for missing values or use imputation techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling one-hot encoding on large datasets can be challenging due to the increase
    in the number of features and the potential for high memory usage. Process the
    data in smaller batches if the dataset is too large to fit into memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving from one-hot encoding to target encoding can be particularly beneficial
    when dealing with high-cardinality categorical features. Let’s explore target
    encoding in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Target encoding (mean encoding)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Target encoding**, also known as **mean encoding**, is a technique used for
    encoding categorical features by replacing each category with the **mean** of
    the target variable (or another relevant aggregation function) for that category.
    This method is particularly useful for classification tasks when dealing with
    **high-cardinality categorical features**, where one-hot encoding would result
    in a significant increase in dimensionality.'
  prefs: []
  type: TYPE_NORMAL
- en: In more detail, target encoding replaces categorical values with the mean (or
    other aggregation metric) of the target variable for each category. It leverages
    the relationship between the categorical feature and the target variable to encode
    the information.
  prefs: []
  type: TYPE_NORMAL
- en: When to use target encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you have categorical features with many unique categories, using one-hot
    encoding might lead to a high-dimensional dataset. Target encoding can be an effective
    alternative in such cases.
  prefs: []
  type: TYPE_NORMAL
- en: If there’s a strong relationship between the categorical feature and the target
    variable, target encoding can capture this relationship and potentially improve
    predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use target encoding when you have memory constraints and need to
    reduce the dimensionality of your dataset, as target encoding doesn’t create additional
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: Use case – sales prediction for retail stores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A retail chain with multiple stores wants to build an ML model to predict daily
    sales for each store. They have collected data on various features, including
    the `Store Type` feature, which has a high cardinality. Instead of using one-hot
    encoding, which would result in a large number of features, the retail chain decides
    to use target encoding to encode the `Store` `Type` feature.
  prefs: []
  type: TYPE_NORMAL
- en: The data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s have a quick look at the data we have available for the analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Store Type`: The type of store (categorical variable with `Type A`, `Type
    B`, `Type C`, and `Type` `D` values)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number of Employees`: The number of employees working at the store (integer
    variable)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Advertising Budget`: The budget allocated for advertising by the store (continuous
    variable in dollars)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Daily Sales`: The sales made by the store in a day (target variable in dollars)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s have a look at a sample of the data for the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Having understood the data, we can move to the objective of the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Objective of the use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The use case’s objective is to encode the categorical features using target
    encoding to prepare the data for ML modeling. Let’s see how we can do this using
    scikit-learn. The code for this example can be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/3.target_encoding.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/3.target_encoding.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure you have installed and imported the libraries mentioned in the *Technical
    requirements* section at the beginning of the chapter. Once that is done, let’s
    begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a synthetic dataset of sample size 1000:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate some random data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Put the data into a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a target variable and features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a `TargetEncoder` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit and transform on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code provided on GitHub for this section, we are using the data and
    the encoded feature to train a random forest regressor model and calculate validation
    metrics. If you are interested, explore the code file here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/3.target_encoding.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/3.target_encoding.py).'
  prefs: []
  type: TYPE_NORMAL
- en: This encoding technique helps capture the relationship between different store
    types and daily sales, so let’s have a look at the encoded output.
  prefs: []
  type: TYPE_NORMAL
- en: Encoded output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s have a look at the encoded data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s focus on the `Store Type` column, which is now encoded into numerical
    values. We can see the difference in more detail before and after the encoding
    in the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Distribution of store type before and after the encoding](img/B19801_10_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Distribution of store type before and after the encoding
  prefs: []
  type: TYPE_NORMAL
- en: Target encoding can be advantageous in this scenario as it efficiently encodes
    the categorical feature, making it suitable for regression tasks such as sales
    prediction while avoiding the dimensionality issues associated with one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discuss some things to keep in mind when encoding features with a
    target encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for target encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When performing target encoding, especially on large datasets, there are several
    important considerations to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting**: Target encoding can lead to overfitting if not applied carefully
    or if some categories have only a few samples. To mitigate this, techniques such
    as smoothing or adding regularization terms are often used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smoothing (regularization)**: Smoothing involves blending the mean of the
    target variable for each category with a global mean. This reduces the impact
    of extreme values or noise in the training data. The formula for smoothed target
    encoding is often the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: smoothed_mean =(n*category_mean+m*global_mean)/(n+m)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n* is the number of observations in the category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*m* is a hyperparameter that controls the strength of smoothing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusting the value of *m* allows you to control the level of regularization.
    Smaller values of *m* give more weight to the category’s actual mean, while larger
    values give more weight to the global mean.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross-validation**: Perform target encoding within each fold of a cross-validation
    scheme. This helps ensure that the encoding is based on a portion of the data
    independent of the one being predicted. Cross-validation can provide a more reliable
    estimate of the target variable’s distribution for each category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave-one-out encoding**: In this approach, you compute the mean of the target
    variable for a category excluding the current observation. It can be more robust
    to overfitting because it considers the effect of the category without including
    the target value of the instance being encoded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adding noise**: Introducing a small amount of random noise to the encoded
    values can help reduce overfitting. This is often referred to as **Bayesian**
    **target encoding**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be cautious of data leakage. It’s crucial to calculate the mean on the training
    dataset only and apply the same encoding to the validation and test datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute encoding statistics on training data only**: Calculate the encoding
    statistics (for example, mean) based solely on the training dataset. This ensures
    that the model is trained on unbiased information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apply the same encoding to all datasets**: Once you’ve calculated the encoding
    statistics on the training data, use the same encoding when preprocessing the
    validation and test datasets. Do not recalculate the statistics separately for
    these datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While target encoding can improve model performance, it may reduce the interpretability
    of your model, as the original categorical values are lost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After exploring target encoding, another effective technique for handling high-cardinality
    categorical features is frequency encoding. Frequency encoding replaces each category
    with its frequency or count in the dataset, which can help capture the inherent
    importance of each category and maintain the overall distribution of the data.
    Let’s deep dive into frequency encoding and its advantages in processing categorical
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Frequency encoding**, also known as **count encoding**, is a technique used
    for encoding categorical features by replacing each category with its corresponding
    frequency or count in the dataset. In this encoding method, the more frequent
    a category is, the higher its encoded value. Frequency encoding can be a valuable
    tool in certain situations where the frequency of occurrence of a category carries
    valuable information.'
  prefs: []
  type: TYPE_NORMAL
- en: When to use frequency encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Frequency encoding can be considered in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Informative frequency**: The frequency or count of categories is informative
    and has a direct or indirect relationship with the target variable. For example,
    in a customer churn prediction problem, the frequency of product purchases by
    a customer may correlate with their likelihood to churn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: You need an efficient encoding method that requires minimal
    computational resources and memory compared to one-hot encoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This encoding method often works well with tree-based models such as decision
    trees, random forests, and gradient boosting, as these models can effectively
    capture the relationship between the encoded frequency and the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Use case – customer product preference analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A retail company wants to analyze customer product preferences based on their
    purchase history. They have a dataset with information about customer purchases,
    including the product category they buy the most.
  prefs: []
  type: TYPE_NORMAL
- en: The data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, we will use frequency encoding on the `Product Category` feature
    to determine the most frequently purchased product categories by customers. This
    encoding method allows the retail company to analyze customer preferences and
    understand how to optimize product recommendations or marketing strategies based
    on popular product categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Having understood the data, we can move to the objective of the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Objective of the use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The use case’s objective is to encode the categorical features using frequency
    encoding to prepare the data for ML modeling. Let’s see how we can do that using
    scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a sample dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a `CountEncoder` class for `Product Category`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit and transform the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The company wants to encode this categorical feature using frequency encoding
    to understand which product categories are most frequently purchased. Let’s have
    a look at the encoded data.
  prefs: []
  type: TYPE_NORMAL
- en: Encoded output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s have a look at the encoded data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s focus on the `Product Category` feature, which is now encoded into numerical
    values based on frequency. We can see the difference in more detail before and
    after the encoding on the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Distribution of Product Category before and after the encoding](img/B19801_10_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Distribution of Product Category before and after the encoding
  prefs: []
  type: TYPE_NORMAL
- en: The first subplot shows the distribution of `Product Category` in the training
    set before encoding. The second subplot shows the distribution of the encoded
    `Product Category` feature in the training set after encoding. As we can see,
    each category in the `Product Category` column is replaced by the **frequency
    count** of that category within the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Frequency encoding preserves information about the prevalence of each category
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discuss some things to keep in mind when encoding features with a
    frequency encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for frequency encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When performing frequency encoding, there are several important considerations
    to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Frequency encoding can lead to overfitting, especially if the dataset is small
    or if there are categories with very few observations. This is because the model
    might learn to rely too heavily on the frequency counts, which may not generalize
    well to new data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When two or more categories have the same frequency, they will end up with the
    same encoded value. This can be a limitation if those categories have different
    effects on the target variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency encoding is generally not suitable for linear models, as it does not
    create a linear relationship between the encoded values and the target variable.
    It may be necessary to normalize the encoded values to a similar scale, especially
    if you’re using linear models that are sensitive to feature scaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, frequency encoding is straightforward to implement and does not expand
    the feature space, unlike one-hot encoding, making it efficient for handling high-cardinality
    features without creating many new columns.
  prefs: []
  type: TYPE_NORMAL
- en: While frequency encoding offers simplicity and efficiency for handling high-cardinality
    features, another effective technique is binary encoding. Binary encoding represents
    categories as binary numbers, providing a more compact representation than one-hot
    encoding and preserving ordinal relationships. Let’s explore how binary encoding
    can further enhance the processing of categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: Binary encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Binary encoding** is a technique used for encoding categorical features by
    converting each category into binary code. Each unique category is represented
    by a unique binary pattern, where each digit (0 or 1) in the pattern corresponds
    to the presence or absence of that category. Binary encoding is particularly useful
    for handling high-cardinality categorical features while reducing dimensionality.'
  prefs: []
  type: TYPE_NORMAL
- en: When to use binary encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Binary encoding can be considered in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: You want to reduce the dimensionality of the
    dataset while still capturing information contained within the categorical feature.
    Binary encoding is particularly useful in this scenario.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: You need an efficient encoding method that results in a compact
    representation of categorical data and can be easily processed by ML algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at a use case.
  prefs: []
  type: TYPE_NORMAL
- en: Use case – customer subscription prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A subscription-based service provider wants to predict whether customers will
    subscribe to a premium plan based on various features, including the `Country`
    feature, which has a high cardinality. Binary encoding will be used to efficiently
    encode the `Country` feature.
  prefs: []
  type: TYPE_NORMAL
- en: The data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s have a look at the sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Country`: This categorical feature represents the country of the customers.
    It helps to understand if the location influences the subscription status.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Age`: This numerical feature represents the age of the customers. Age can
    be a significant factor in determining the likelihood of a customer subscribing
    to a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Income`: This numerical feature represents the annual income of the customers.
    Income can indicate the financial capability of the customers to subscribe to
    a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Subscription`: This binary target variable indicates whether a customer has
    subscribed to a service. This is the target variable that we want to predict using
    the other features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s have a look at a sample of the data for the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The distribution of `Country` can be seen in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Distribution of Country before and after the encoding](img/B19801_10_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Distribution of Country before and after the encoding
  prefs: []
  type: TYPE_NORMAL
- en: Objective of the use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of this analysis is to predict the Subscription status of customers
    based on their country, age, and income. We use binary encoding for the `Country`
    feature to convert it from a categorical variable to a numerical format that can
    be used in the ML algorithm. The code for this use case can be found here: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/5.binary_encoding.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter10/5.binary_encoding.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a sample dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply binary encoding to the `Country` feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the encoded DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s have a look at the encoded data.
  prefs: []
  type: TYPE_NORMAL
- en: Encoded output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, binary encoding is applied to the `Country` feature, as we
    can see in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see from the encoded output, the binary digits are split into separate
    columns. Let’s also have a look at the changes in the distribution after the encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Distribution of Country encoded feature](img/B19801_10_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Distribution of Country encoded feature
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discuss some things to keep in mind when encoding features with a
    binary encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for binary encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When performing binary encoding, there are several important considerations
    to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary encoding doesn’t provide direct interpretability for encoded features.
    The encoded binary patterns may not have a clear meaning, unlike one-hot encoding,
    where each binary feature corresponds to a specific category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The binary representation can become complex for categories with very high cardinality
    as the number of binary digits increases logarithmically with the number of categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some ML algorithms, particularly linear models, may not work well with binary-encoded
    features. Careful evaluation of algorithm compatibility is necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve explored the nuances of the different encoding methods, let’s
    transition to summarizing their key differences and considerations for practical
    application in ML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we explored various techniques for encoding categorical
    variables essential for ML tasks. Label encoding, which assigns unique integers
    to each category, is straightforward but may inadvertently impose ordinality where
    none exists. One-hot encoding transforms each category into a binary feature,
    maintaining categorical independence but potentially leading to high-dimensional
    datasets. Binary encoding condenses categorical values into binary representations,
    balancing interpretability, and efficiency particularly well for high-cardinality
    datasets. Frequency encoding replaces categories with their occurrence frequencies,
    capturing valuable information about distributional patterns. Target encoding
    incorporates target variable statistics into categorical encoding, enhancing predictive
    power while requiring careful handling to avoid data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s summarize our learning in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Encoding Method | High Cardinality | Preserves Ordinal Information | Collisions
    | Interpretability | Good for | Not Good for |'
  prefs: []
  type: TYPE_TB
- en: '| Label encoding | Good | Yes | No | Moderate | Tree-based models | Linear
    models |'
  prefs: []
  type: TYPE_TB
- en: '| One-hot encoding | Poor | No | No | High | Linear models, **neural** **networks**
    (**NNs**) | High-cardinality features |'
  prefs: []
  type: TYPE_TB
- en: '| Target encoding | Good | No | Possible | Low | Most algorithms | Small datasets
    (risk of overfitting) |'
  prefs: []
  type: TYPE_TB
- en: '| Frequency encoding | Good | No | Possible | Moderate | Tree-based models
    | Linear models |'
  prefs: []
  type: TYPE_TB
- en: '| Binary encoding | Good | Partially | Possible | Low | Tree-based models |
    Linear models |'
  prefs: []
  type: TYPE_TB
- en: Table 10.2 – Comparison of all the encoding techniques
  prefs: []
  type: TYPE_NORMAL
- en: Each method offers distinct advantages depending on the dataset’s characteristics
    and the specific requirements of the modeling task. In the next chapter, we will
    shift focus to the considerations and methodologies involved in analyzing time
    series data. Time series data introduces temporal dependencies that require specialized
    techniques for feature engineering, as we will expand upon in the next chapter.
  prefs: []
  type: TYPE_NORMAL
