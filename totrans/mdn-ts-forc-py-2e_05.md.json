["```py\nts_train = train_df.loc[train_df.LCLid==\"MAC000193\",['LCLid',\"timestamp\",\"energy_consumption\"]]\nts_val = val_df.loc[val_df.LCLid==\"MAC000193\", ['LCLid',\"timestamp\",\"energy_consumption\"]]\nts_test = test_df.loc[test_df.LCLid==\"MAC000193\", ['LCLid',\"timestamp\",\"energy_consumption\"]] \n```", "```py\nsf = StatsForecast(\n    models=[model],\n    freq=freq,\n    n_jobs=-1,\n    fallback_model=Naive()\n)\nsf.fit(df = _ts_train,          id_col = 'LCLid',\n       time_col = 'timestamp',\n       target_col = 'energy_consumption',\n)\nbaseline_test_pred_df = sf.predict(len(ts_test) ) \n```", "```py\n# Efficiently fit and predict without storing memory\ny_pred = sf.forecast(\n    h=len(ts_test),\n    df=ts_train,\n    id_col = 'LCLid',    time_col = 'timestamp',    target_col = 'energy_consumption',\n) \n```", "```py\n# Calculate metrics\nmetrics = [mase, mae, mse, rmse, smape, forecast_bias]\nfor metric in metrics:\n    metric_name = metric.__name__\n    if metric_name == 'mase':\n        evaluation[metric_name] = \t\nmetric(results[target_col].values,            results[model_name].values,\nts_train[target_col].values, seasonality=48)\n    else:\n        evaluation[metric_name] =\nmetric(results[target_col].values,\nresults[model_name].values) \n```", "```py\nfrom statsforecast.models import Naive\nmodels = Naive() \n```", "```py\nfrom src.forecasting.baselines import NaiveMovingAverage\n#Taking a moving average over 48 timesteps, i.e, one day\nnaive_model = NaiveMovingAverage(window=48) \n```", "```py\nfrom statsforecast.models import SeasonalNaive\nseasonal_naive = SeasonalNaive(season_length=48*7) \n```", "```py\nfrom statsforecast.models import (SimpleExponentialSmoothing, Holt, HoltWinters, AutoETS)\nexp_smooth = HoltWinters(error_type = 'A', season_length = 48)] \n```", "```py\nfrom statsforecast.models import (ARIMA, AutoARIMA)\n#ARIMA model by specifying parameters\narima_model = ARIMA(order = (2,1,1), seasonal_order = (1,1,1), season_length = 48)\n#AutoARIMA model by specifying max limits for parameters and letting the algorithm find the best ones\nauto_arima_model = AutoARIMA( max_p = 2, max_d=1, max_q = 2, max_P=2, max_D = 1, max_Q = 2, stepwise = True, season_length=48) \n```", "```py\ntheta_model = Theta(season_length =48, decomposition_type = 'additive' ) \n```", "```py\nTBATS_model = TBATS(seasonal_periods  = 48, use_trend=True, use_damped_trend=True) \n```", "```py\nMSTL_model = MSTL(season_length  = 48) \n```", "```py\nblock_df[\"rv\"] = block_df.progress_apply(lambda x: calc_norm_sd(x['residuals'],x['energy_consumption']), axis=1) \n```", "```py\n# Creating an array with a well balanced probability distribution\nflat = np.array([0.1,0.2, 0.3,0.2, 0.2])\n# Calculating Entropy\nprint((-np.log2(flat)* flat).sum()) \n```", "```py\n>> 2.2464393446710154 \n```", "```py\n# Creating an array with a peak in probability\nsharp = np.array([0.1,0.6, 0.1,0.1, 0.1])\n# Calculating Entropy\nprint((-np.log2(sharp)* sharp).sum()) \n```", "```py\n>> 1.7709505944546688 \n```", "```py\nfrom src.forecastability.entropy import spectral_entropy\nblock_df[\"spectral_entropy\"] = block_df.energy_consumption.progress_apply(lambda x: spectral_entropy(x, transform_stationary=True))\nblock_df[\"residual_spectral_entropy\"] = block_df.residuals.progress_apply(spectral_entropy) \n```", "```py\nfrom src.forecastability.kaboudan import kaboudan_metric, modified_kaboudan_metric\nmodel = Theta(theta=3, seasonality_period=48*7, season_mode=SeasonalityMode.ADDITIVE)\nblock_df[\"kaboudan_metric\"] = [kaboudan_metric(r[0], model=model, block_size=5, backtesting_start=0.5, n_folds=1) for r in tqdm(zip(*block_df[[\"energy_consumption\"]].to_dict(\"list\").values()), total=len(block_df))]\nblock_df[\"modified_kaboudan_metric\"] = [modified_kaboudan_metric(r[0], model=model, block_size=5, backtesting_start=0.5, n_folds=1) for r in tqdm(zip(*block_df[[\"energy_consumption\"]].to_dict(\"list\").values()), total=len(block_df))] \n```"]