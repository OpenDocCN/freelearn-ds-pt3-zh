["```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n```", "```py\npython3.10 -m pip install statsmodels sklearn prophet\n```", "```py\nconda install prophet\n```", "```py\nimport statsmodels.api as sm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.random import default_rng\nrng = default_rng(12345)\n```", "```py\n    x = np.linspace(0, 5, 25)\n    ```", "```py\n    rng.shuffle(x)\n    ```", "```py\n    trend = 2.0\n    ```", "```py\n    shift = 5.0\n    ```", "```py\n    y1 = trend*x + shift + rng.normal(0, 0.5, size=25)\n    ```", "```py\n    y2 = trend*x + shift + rng.normal(0, 5, size=25)\n    ```", "```py\n    fig, ax = plt.subplots()\n    ```", "```py\n    ax.scatter(x, y1, c=\"k\", marker=\"x\",\n    ```", "```py\n        label=\"Good correlation\")\n    ```", "```py\n    ax.scatter(x, y2, c=\"k\", marker=\"o\",\n    ```", "```py\n        label=\"Bad correlation\")\n    ```", "```py\n    ax.legend()\n    ```", "```py\n    ax.set_xlabel(\"X\"),\n    ```", "```py\n    ax.set_ylabel(\"Y\")\n    ```", "```py\n    ax.set_title(\"Scatter plot of data with best fit lines\")\n    ```", "```py\n    pred_x = sm.add_constant(x)\n    ```", "```py\n    model1 = sm.OLS(y1, pred_x).fit()\n    ```", "```py\n    print(model1.summary())\n    ```", "```py\n    model2 = sm.OLS(y2, pred_x).fit()\n    ```", "```py\n    print(model2.summary())\n    ```", "```py\n    model_x = sm.add_constant(np.linspace(0, 5))\n    ```", "```py\n    model_y1 = model1.predict(model_x)\n    ```", "```py\n    model_y2 = model2.predict(model_x)\n    ```", "```py\n    ax.plot(model_x[:, 1], model_y1, 'k')\n    ```", "```py\n    ax.plot(model_x[:, 1], model_y2, 'k--')\n    ```", "```py\nfrom numpy.random import default_rng\nrng = default_rng(12345)\n```", "```py\nimport statsmodels.api as sm\n```", "```py\n    p_vars = pd.DataFrame({\n    ```", "```py\n        \"const\": np.ones((100,)),\n    ```", "```py\n        \"X1\": rng.uniform(0, 15, size=100),\n    ```", "```py\n        \"X2\": rng.uniform(0, 25, size=100),\n    ```", "```py\n        \"X3\": rng.uniform(5, 25, size=100)\n    ```", "```py\n    })\n    ```", "```py\n    residuals = rng.normal(0.0, 12.0, size=100)\n    ```", "```py\n    Y = -10.0 + 5.0*p_vars[\"X1\"] - 2.0*p_vars[\"X2\"] +    residuals\n    ```", "```py\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True,\n    ```", "```py\n        tight_layout=True)\n    ```", "```py\n    ax1.scatter(p_vars[\"X1\"], Y, c=\"k\")\n    ```", "```py\n    ax2.scatter(p_vars[\"X2\"], Y, c=\"k\")\n    ```", "```py\n    ax3.scatter(p_vars[\"X3\"], Y, c=\"k\")\n    ```", "```py\n    ax1.set_title(\"Y against X1\")\n    ```", "```py\n    ax1.set_xlabel(\"X1\")\n    ```", "```py\n    ax1.set_ylabel(\"Y\")\n    ```", "```py\n    ax2.set_title(\"Y against X2\")\n    ```", "```py\n    ax2.set_xlabel(\"X2\")\n    ```", "```py\n    ax3.set_title(\"Y against X3\")\n    ```", "```py\n    ax3.set_xlabel(\"X3\") \n    ```", "```py\n    model = sm.OLS(Y, p_vars).fit()\n    ```", "```py\n    print(model.summary())\n    ```", "```py\n                   OLS Regression Results           \n===========================================\nDep. Variable:        y            R-squared:0.769\nModel:              OLS             Adj. R-squared:0.764\nMethod: Least Squares  F-statistic:161.5\nDate: Fri, 25 Nov 2022 Prob (F-statistic):1.35e-31\nTime: 12:38:40      Log-Likelihood:-389.48\nNo. Observations: 100 AIC:        785.0           \nDf Residuals: 97              BIC:     792.8\nDf Model: 2\nCovariance Type:  nonrobust \n```", "```py\n=========================================\n         coef           std err      t     P>|t|  [0.025    0.975]\n-----------------------------------------------------------------------\nconst -11.1058  2.878  -3.859 0.000  -16.818  -5.393\nX1      4.7245  0.301  15.672   0.00    4.126      5.323\nX2     -1.9050  0.164 -11.644  0.000   -2.230   -1.580\n=========================================\nOmnibus:           0.259        Durbin-Watson:     1.875\nProb(Omnibus): 0.878       Jarque-Bera (JB):  0.260\nSkew: 0.115             Prob(JB):         0.878\nKurtosis: 2.904         Cond. No          38.4\n=========================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```", "```py\n    second_model = sm.OLS(\n    ```", "```py\n        Y, p_vars.loc[:, \"const\":\"X2\"]).fit()\n    ```", "```py\n    print(second_model.summary())\n    ```", "```py\nfrom numpy.random import default_rng\nrng = default_rng(12345)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n```", "```py\n    df = pd.DataFrame({\n    ```", "```py\n        \"var1\": np.concatenate([\n    ```", "```py\n            rng.normal(3.0, 1.5, size=50),\n    ```", "```py\n            rng.normal(-4.0, 2.0, size=50)]),\n    ```", "```py\n        \"var2\": rng.uniform(size=100),\n    ```", "```py\n        \"var3\": np.concatenate([\n    ```", "```py\n            rng.normal(-2.0, 2.0, size=50),\n    ```", "```py\n            rng.normal(1.5, 0.8, size=50)])\n    ```", "```py\n    })\n    ```", "```py\n    score = 4.0 + df[\"var1\"] - df[\"var3\"]\n    ```", "```py\n    Y = score >= 0\n    ```", "```py\n    fig1, ax1 = plt.subplots()\n    ```", "```py\n    ax1.plot(df.loc[Y, \"var1\"], df.loc[Y, \"var3\"],\n    ```", "```py\n        \"ko\", label=\"True data\")\n    ```", "```py\n    ax1.plot(df.loc[~Y, \"var1\"], df.loc[~Y, \"var3\"],\n    ```", "```py\n        \"kx\", label=\"False data\")\n    ```", "```py\n    ax1.legend()\n    ```", "```py\n    ax1.set_xlabel(\"var1\")\n    ```", "```py\n    ax1.set_ylabel(\"var3\")\n    ```", "```py\n    ax1.set_title(\"Scatter plot of var3 against var1\")\n    ```", "```py\n    model = LogisticRegression()\n    ```", "```py\n    model.fit(df, Y)\n    ```", "```py\n    test_df = pd.DataFrame({\n    ```", "```py\n        \"var1\": np.concatenate([\n    ```", "```py\n            rng.normal(3.0, 1.5, size=50),\n    ```", "```py\n            rng.normal(-4.0, 2.0, size=50)]),\n    ```", "```py\n        \"var2\": rng.uniform(size=100),\n    ```", "```py\n        \"var3\": np.concatenate([\n    ```", "```py\n            rng.normal(-2.0, 2.0, size=50),\n    ```", "```py\n            rng.normal(1.5, 0.8, size=50)])\n    ```", "```py\n    })\n    ```", "```py\n    test_scores = 4.0 + test_df[\"var1\"] - test_df[\"var3\"]\n    ```", "```py\n    test_Y = test_scores >= 0\n    ```", "```py\n    test_predicts = model.predict(test_df)\n    ```", "```py\n    print(classification_report(test_Y, test_predicts))\n    ```", "```py\n              precision    recall      f1-score   support\n       False       0.82      1.00        0.90        18\n        True        1.00      0.88        0.93        32\naccuracy                     0.92                   50\n   macro avg     0.91      0.94       0.92         50\nweighted avg    0.93      0.92       0.92         50\n```", "```py\nfrom tsdata import generate_sample_data\n```", "```py\nfrom matplotlib.rcsetup import cycler\nplt.rc(\"axes\", prop_cycle=cycler(c=\"k\"))\n```", "```py\n    sample_ts, _ = generate_sample_data()\n    ```", "```py\n    ts_fig, ts_ax = plt.subplots()\n    ```", "```py\n    sample_ts.plot(ax=ts_ax, label=\"Observed\",\n    ```", "```py\n        ls=\"--\", alpha=0.4)\n    ```", "```py\n    ts_ax.set_title(\"Time series data\")\n    ```", "```py\n    ts_ax.set_xlabel(\"Date\")\n    ```", "```py\n    ts_ax.set_ylabel(\"Value\")\n    ```", "```py\n    adf_results = sm.tsa.adfuller(sample_ts)\n    ```", "```py\n    adf_pvalue = adf_results[1]\n    ```", "```py\n    print(\"Augmented Dickey-Fuller test:\\nP-value:\",\n    ```", "```py\n        adf_pvalue)\n    ```", "```py\n    ap_fig, (acf_ax, pacf_ax) = plt.subplots(\n    ```", "```py\n        2, 1, tight_layout=True)\n    ```", "```py\n    sm.graphics.tsa.plot_acf(sample_ts, ax=acf_ax, \n    ```", "```py\n        title=\"Observed autocorrelation\")\n    ```", "```py\n    sm.graphics.tsa.plot_pacf(sample_ts, ax=pacf_ax, \n    ```", "```py\n        title=\"Observed partial autocorrelation\")\n    ```", "```py\n    acf_ax.set_xlabel(\"Lags\")\n    ```", "```py\n    pacf_ax.set_xlabel(\"Lags\")\n    ```", "```py\n    pacf_ax.set_ylabel(\"Value\")\n    ```", "```py\n    acf_ax.set_ylabel(\"Value\")\n    ```", "```py\n    arma_model = sm.tsa.ARIMA(sample_ts, order=(1, 0, 1))\n    ```", "```py\n    arma_results = arma_model.fit()\n    ```", "```py\n    print(arma_results.summary())\n    ```", "```py\n                         ARMA Model Results\n    ```", "```py\n    =========================================\n    ```", "```py\n    Dep. Variable: y No.             Observations:         366\n    ```", "```py\n    Model: ARMA(1, 1)               Log Likelihood -513.038\n    ```", "```py\n    Method: css-mle S.D. of innovations        \n    ```", "```py\n                                                     0.982\n    ```", "```py\n    Date: Fri, 01 May 2020 AIC  1034.077\n    ```", "```py\n    Time: 12:40:00              BIC  1049.687\n    ```", "```py\n    Sample: 01-01-2020     HQIC  1040.280\n    ```", "```py\n                - 12-31-2020\n    ```", "```py\n    ==================================================\n    ```", "```py\n    coef         std       err            z  P>|z|  [0.025   0.975]\n    ```", "```py\n    ---------------------------------------------------------------------\n    ```", "```py\n    const  -0.0242  0.143  -0.169  0.866  -0.305  0.256\n    ```", "```py\n    ar.L1.y 0.8292  0.057  14.562  0.000   0.718  0.941\n    ```", "```py\n    ma.L1.y -0.5189 0.090  -5.792 0.000  -0.695  -0.343\n    ```", "```py\n                                                 Roots\n    ```", "```py\n    =========================================\n    ```", "```py\n                        Real  Imaginary  Modulus \n    ```", "```py\n    Frequency \n    ```", "```py\n    ---------------------------------------------------------\n    ```", "```py\n    AR.1        1.2059  +0.0000j  1.2059\n    ```", "```py\n    0.0000\n    ```", "```py\n    MA.1        1.9271  +0.0000j  1.9271\n    ```", "```py\n    0.0000\n    ```", "```py\n    ---------------------------------------------------\n    ```", "```py\n    residuals = arma_results.resid\n    ```", "```py\n    rap_fig, (racf_ax, rpacf_ax) = plt.subplots(\n    ```", "```py\n        2, 1, tight_layout=True)\n    ```", "```py\n    sm.graphics.tsa.plot_acf(residuals, ax=racf_ax, \n    ```", "```py\n        title=\"Residual autocorrelation\")\n    ```", "```py\n    sm.graphics.tsa.plot_pacf(residuals, ax=rpacf_ax, \n    ```", "```py\n        title=\"Residual partial autocorrelation\")\n    ```", "```py\n    racf_ax.set_xlabel(\"Lags\")\n    ```", "```py\n    rpacf_ax.set_xlabel(\"Lags\")\n    ```", "```py\n    rpacf_ax.set_ylabel(\"Value\")\n    ```", "```py\n    racf_ax.set_ylabel(\"Value\")\n    ```", "```py\n    fitted = arma_results.fittedvalues\n    ```", "```py\n    fitted.plot(ax=ts_ax, label=\"Fitted\")\n    ```", "```py\n    ts_ax.legend()\n    ```", "```py\nfrom tsdata import generate_sample_data\n```", "```py\nfrom matplotlib.rcsetup import cycler\nplt.rc(\"axes\", prop_cycle=cycler(c=\"k\"))\n```", "```py\n    sample_ts, test_ts = generate_sample_data(\n    ```", "```py\n        trend=0.2, undiff=True)\n    ```", "```py\n    ts_fig, ts_ax = plt.subplots(tight_layout=True)\n    ```", "```py\n    sample_ts.plot(ax=ts_ax, label=\"Observed\")\n    ```", "```py\n    ts_ax.set_title(\"Training time series data\")\n    ```", "```py\n    ts_ax.set_xlabel(\"Date\")\n    ```", "```py\n    ts_ax.set_ylabel(\"Value\")\n    ```", "```py\n    diffs = sample_ts.diff().dropna()\n    ```", "```py\n    ap_fig, (acf_ax, pacf_ax) = plt.subplots(2, 1, \n    ```", "```py\n        tight_layout=True)\n    ```", "```py\n    sm.graphics.tsa.plot_acf(diffs, ax=acf_ax)\n    ```", "```py\n    sm.graphics.tsa.plot_pacf(diffs, ax=pacf_ax)\n    ```", "```py\n    acf_ax.set_ylabel(\"Value\")\n    ```", "```py\n    acf_ax.set_xlabel(\"Lag\")\n    ```", "```py\n    pacf_ax.set_xlabel(\"Lag\")\n    ```", "```py\n    pacf_ax.set_ylabel(\"Value\")\n    ```", "```py\n    model = sm.tsa.ARIMA(sample_ts, order=(1,1,1))\n    ```", "```py\n    fitted = model.fit()\n    ```", "```py\n    print(fitted.summary())\n    ```", "```py\n             SARIMAX Results                   \n===========================================\nDep. Variable:      y   No.  Observations:  366\nModel:  ARIMA(1, 0, 1)         Log Likelihood  -513.038\nDate:  Fri, 25 Nov 2022         AIC            1034.077\nTime:              13:17:24         BIC            1049.687\nSample:      01-01-2020        HQIC           1040.280\n                 - 12-31-2020                         \nCovariance Type:                  opg           \n=========================================\n              coef    std err     z       P>|z|   [0.025   0.975]\n-----------------------------------------------------------------------\nconst  -0.0242  0.144  -0.168  0.866   -0.307   0.258\nar.L1   0.8292  0.057  14.512  0.000   0.717    0.941\nma.L1  -0.5189  0.087  -5.954  0.000  -0.690  -0.348\nsigma2  0.9653  0.075  12.902  0.000  0.819   1.112\n=========================================\nLjung-Box (L1) (Q):      0.04   Jarque-Bera (JB):  0.59\nProb(Q):                        0.84  Prob(JB):                0.74\nHeteroskedasticity (H): 1.15   Skew:                    -0.06\nProb(H) (two-sided):     0.44   Kurtosis:                 2.84\n=========================================\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n```", "```py\n    forecast =fitted.get_forecast(steps=50).summary_frame()\n    ```", "```py\n    forecast[\"mean\"].plot(\n    ```", "```py\n        ax=ts_ax, label=\"Forecast\", ls=\"--\")\n    ```", "```py\n    ts_ax.fill_between(forecast.index,\n    ```", "```py\n                       forecast[\"mean_ci_lower\"],\n    ```", "```py\n                       forecast[\"mean_ci_upper\"],\n    ```", "```py\n                       alpha=0.4)\n    ```", "```py\n    test_ts.plot(ax=ts_ax, label=\"Actual\", ls=\"-.\")\n    ```", "```py\n    ts_ax.legend()\n    ```", "```py\nfrom tsdata import generate_sample_data\n```", "```py\n    sample_ts, test_ts = generate_sample_data(undiff=True,\n    ```", "```py\n        seasonal=True)\n    ```", "```py\n    ts_fig, ts_ax = plt.subplots(tight_layout=True)\n    ```", "```py\n    sample_ts.plot(ax=ts_ax, title=\"Time series\",\n    ```", "```py\n        label=\"Observed\")\n    ```", "```py\n    ts_ax.set_xlabel(\"Date\")\n    ```", "```py\n    ts_ax.set_ylabel(\"Value\")\n    ```", "```py\n    ap_fig, (acf_ax, pacf_ax) = plt.subplots(2, 1,\n    ```", "```py\n        tight_layout=True)\n    ```", "```py\n    sm.graphics.tsa.plot_acf(sample_ts, ax=acf_ax)\n    ```", "```py\n    sm.graphics.tsa.plot_pacf(sample_ts, ax=pacf_ax)\n    ```", "```py\n    acf_ax.set_xlabel(\"Lag\")\n    ```", "```py\n    pacf_ax.set_xlabel(\"Lag\")\n    ```", "```py\n    acf_ax.set_ylabel(\"Value\")\n    ```", "```py\n    pacf_ax.set_ylabel(\"Value\")\n    ```", "```py\n    diffs = sample_ts.diff().dropna()\n    ```", "```py\n    dap_fig, (dacf_ax, dpacf_ax) = plt.subplots(\n    ```", "```py\n        2, 1, tight_layout=True)\n    ```", "```py\n    sm.graphics.tsa.plot_acf(diffs, ax=dacf_ax, \n    ```", "```py\n        title=\"Differenced ACF\")\n    ```", "```py\n    sm.graphics.tsa.plot_pacf(diffs, ax=dpacf_ax, \n    ```", "```py\n        title=\"Differenced PACF\")\n    ```", "```py\n    dacf_ax.set_xlabel(\"Lag\")\n    ```", "```py\n    dpacf_ax.set_xlabel(\"Lag\")\n    ```", "```py\n    dacf_ax.set_ylabel(\"Value\")\n    ```", "```py\n    dpacf_ax.set_ylabel(\"Value\")\n    ```", "```py\n    model = sm.tsa.SARIMAX(sample_ts, order=(1, 1, 1), \n    ```", "```py\n        seasonal_order=(1, 0, 0, 7))\n    ```", "```py\n    fitted_seasonal = model.fit()\n    ```", "```py\n    print(fitted_seasonal.summary())\n    ```", "```py\n             SARIMAX Results                   \n===========================================\nDep. Variable:      y   No.  Observations:     366\nModel:ARIMA(1, 0, 1)       Log Likelihood  -513.038\nDate:   Fri, 25 Nov 2022      AIC    1027.881\nTime:  14:08:54                   BIC    1043.481\nSample:01-01-2020            HQIC  1034.081\n            - 12-31-2020    \nCovariance Type:         opg\n```", "```py\n=========================================\n              coef   std err       z      P>|z|   [0.025  0.975]\n-----------------------------------------------------\nar.L1  0.7939   0.065  12.136  0.000   0.666  0.922\nma.L1  -0.4544  0.095  -4.793  0.000  -0.640  -0.269\nar.S.L7 0.7764  0.034  22.951  0.000   0.710  0.843\nsigma2  0.9388  0.073  2.783   0.000   0.795  1.083\n=========================================\nLjung-Box (L1) (Q):     0.03        Jarque-Bera (JB): 0.47\nProb(Q):                0.86               Prob(JB): 0.79\nHeteroskedasticity (H): 1.15        Skew: -0.03\nProb(H) (two-sided):    0.43         Kurtosis: 2.84\n=========================================\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n```", "```py\n    forecast_result = fitted_seasonal.get_forecast(steps=50)\n    ```", "```py\n    forecast_index = pd.date_range(\"2021-01-01\", periods=50)\n    ```", "```py\n    forecast = forecast_result.predicted_mean\n    ```", "```py\n    forecast.plot(ax=ts_ax, label=\"Forecasts\", ls=\"--\")\n    ```", "```py\n    conf = forecast_result.conf_int()\n    ```", "```py\n    ts_ax.fill_between(forecast_index, conf[\"lower y\"],\n    ```", "```py\n        conf[\"upper y\"], alpha=0.4)\n    ```", "```py\n    test_ts.plot(ax=ts_ax, label=\"Actual future\", ls=\"-.\")\n    ```", "```py\n    ts_ax.legend()\n    ```", "```py\nfrom prophet import Prophet\n```", "```py\nfrom tsdata import generate_sample_data\n```", "```py\n    sample_ts, test_ts = generate_sample_data(\n    ```", "```py\n        undiffTrue,trend=0.2)\n    ```", "```py\n    df_for_prophet = pd.DataFrame({\n    ```", "```py\n        \"ds\": sample_ts.index,    # dates\n    ```", "```py\n        \"y\": sample_ts.values    # values\n    ```", "```py\n    })\n    ```", "```py\n    model = Prophet()\n    ```", "```py\n    model.fit(df_for_prophet)\n    ```", "```py\n    forecast_df = model.make_future_dataframe(periods=50)\n    ```", "```py\n    forecast = model.predict(forecast_df)\n    ```", "```py\n    fig, ax = plt.subplots(tight_layout=True)\n    ```", "```py\n    sample_ts.plot(ax=ax, label=\"Observed\", title=\"Forecasts\", c=\"k\")\n    ```", "```py\n    forecast.plot(x=\"ds\", y=\"yhat\", ax=ax, c=\"k\", \n    ```", "```py\n        label=\"Predicted\", ls=\"--\")\n    ```", "```py\n    ax.fill_between(forecast[\"ds\"].values, forecast[\"yhat_lower\"].values, \n    ```", "```py\n        forecast[\"yhat_upper\"].values, color=\"k\", alpha=0.4)\n    ```", "```py\n    test_ts.plot(ax=ax, c=\"k\", label=\"Future\", ls=\"-.\")\n    ```", "```py\n    ax.legend()\n    ```", "```py\n    ax.set_xlabel(\"Date\")\n    ```", "```py\n    ax.set_ylabel(\"Value\")\n    ```", "```py\nrng = np.random.default_rng(12345)\n```", "```py\n    upper_limit = 2*np.pi\n    ```", "```py\n    depth = 2\n    ```", "```py\n    noise_variance = 0.1\n    ```", "```py\n    def make_noisy(signal):\n    ```", "```py\n        return signal + rng.normal(0.0, noise_variance, size=signal.shape)\n    ```", "```py\n    def signal_a(count):\n    ```", "```py\n        t = rng.exponential(\n    ```", "```py\n            upper_limit/count, size=count).cumsum()\n    ```", "```py\n        return t, np.column_stack(\n    ```", "```py\n            [t/(1.+t)**2, 1./(1.+t)**2])\n    ```", "```py\n    def signal_b(count):\n    ```", "```py\n        t = rng.exponential(\n    ```", "```py\n            upper_limit/count, size=count).cumsum()\n    ```", "```py\n        return t, np.column_stack(\n    ```", "```py\n            [np.cos(t), np.sin(t)])\n    ```", "```py\n    params_a, true_signal_a = signal_a(100)\n    ```", "```py\n    params_b, true_signal_b = signal_b(100)\n    ```", "```py\n    fig, ((ax11, ax12), (ax21, ax22)) = plt.subplots(\n    ```", "```py\n        2, 2,tight_layout=True)\n    ```", "```py\n    ax11.plot(params_a, true_signal_a[:, 0], \"k\")\n    ```", "```py\n    ax11.plot(params_a, true_signal_a[:, 1], \"k--\")\n    ```", "```py\n    ax11.legend([\"x\", \"y\"])\n    ```", "```py\n    ax12.plot(params_b, true_signal_b[:, 0], \"k\")\n    ```", "```py\n    ax12.plot(params_b, true_signal_b[:, 1], \"k--\")\n    ```", "```py\n    ax12.legend([\"x\", \"y\"])\n    ```", "```py\n    ax21.plot(true_signal_a[:, 0], true_signal_a[:, 1], \"k\")\n    ```", "```py\n    ax22.plot(true_signal_b[:, 0], true_signal_b[:, 1], \"k\")\n    ```", "```py\n    ax11.set_title(\"Components of signal a\")\n    ```", "```py\n    ax11.set_xlabel(\"parameter\")\n    ```", "```py\n    ax11.set_ylabel(\"value\")\n    ```", "```py\n    ax12.set_title(\"Components of signal b\")\n    ```", "```py\n    ax12.set_xlabel(\"parameter\")\n    ```", "```py\n    ax12.set_ylabel(\"value\")\n    ```", "```py\n    ax21.set_title(\"Signal a\")\n    ```", "```py\n    ax21.set_xlabel(\"x\")\n    ```", "```py\n    ax21.set_ylabel(\"y\")\n    ```", "```py\n    ax22.set_title(\"Signal b\")\n    ```", "```py\n    ax22.set_xlabel(\"x\")\n    ```", "```py\n    ax22.set_ylabel(\"y\")\n    ```", "```py\n    signature_a = esig.stream2sig(true_signal_a, depth)\n    ```", "```py\n    signature_b = esig.stream2sig(true_signal_b, depth)\n    ```", "```py\n    print(signature_a, signature_b, sep=\"\\n\")\n    ```", "```py\n[ 1\\. 0.11204198 -0.95648657 0.0062767 -0.15236199 0.04519534 0.45743328]\n[ 1.00000000e+00 7.19079669e-04 -3.23775977e-02 2.58537785e-07 3.12414826e+00 -3.12417155e+00 5.24154417e-04]\n```", "```py\n    sigs_a = np.vstack([esig.stream2sig(\n    ```", "```py\n        make_noisy(signal_a(\n    ```", "```py\n            rng.integers(50, 100))[1]), depth)\n    ```", "```py\n        for _ in range(50)])\n    ```", "```py\n    sigs_b = np.vstack([esig.stream2sig(\n    ```", "```py\n        make_noisy(signal_b(\n    ```", "```py\n            rng.integers(50, 100))[1]), depth)\n    ```", "```py\n        for _ in range(50)])\n    ```", "```py\n    expected_sig_a = np.mean(sigs_a, axis=0)\n    ```", "```py\n    expected_sig_b = np.mean(sigs_b, axis=0)\n    ```", "```py\n    print(expected_sig_a, expected_sig_b, sep=\"\\n\")\n    ```", "```py\n[ 1\\. 0.05584373 -0.82468682 0.01351423 -0.1040297 0.0527106 0.36009198]\n[ 1\\. -0.22457304 -0.05130969 0.07368485 3.0923422 -3.09672887 0.17059484]\n```", "```py\n    print(\"Signal a\", np.max(\n    ```", "```py\n        np.abs(expected_sig_a - signature_a)))\n    ```", "```py\n    print(\"Signal b\", np.max(\n    ```", "```py\n        np.abs(expected_sig_b -signature_b)))\n    ```", "```py\n    print(\"Signal a vs signal b\", np.max(\n    ```", "```py\n        np.abs(expected_sig_a - expected_sig_b)))\n    ```", "```py\nSignal a 0.13179975589137582\nSignal b 0.22529211936796972\nSignal a vs signal b 3.1963719013938148\n```", "```py\n[1\\. 0.11204198 -0.95648657 0.0062767 -0.15236199 0.04519534 0.45743328]\n```", "```py\n[ 1.00000000e+00  7.19079669e-04 -3.23775977e-02  2.58537785e-07  3.12414826e+00 -3.12417155e+00  5.24154417e-04]\n```"]