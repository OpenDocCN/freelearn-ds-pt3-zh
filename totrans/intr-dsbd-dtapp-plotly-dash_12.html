<html><head></head><body>
		<div id="_idContainer164">
			<h1 id="_idParaDest-133"><em class="italic"><a id="_idTextAnchor133"/>Chapter 9</em>: Letting Your Data Speak for Itself with Machine Learning</h1>
			<p>While making histograms we got a glimpse of a technique that visualizes aggregates, and not data points directly. In other words, we visualized data about our data. We will take this concept several steps further in this chapter, by using a machine learning technique to demonstrate some options that can be used to categorize or cluster our data. As you will see in this chapter, and even while using a single technique, there are numerous options and combinations of options that can be explored. This is where the value of interactive dashboards comes into play. It would be very tedious if users were to explore every single option by manually creating a chart for it.</p>
			<p>This chapter is not an introduction to machine learning, nor does it assume any prior knowledge of it. We will explore a clustering technique called <strong class="bold">KMeans clustering</strong> and use the <strong class="source-inline">sklearn</strong> machine learning package. This will help us in grouping our data into clusters of observations that are similar to one another, and yet distinct from observations in other clusters. We will build a very simple model with a single-dimensional dataset, and then see how this can be applied to clustering countries in our <strong class="source-inline">poverty</strong> dataset.</p>
			<p>If you are familiar with machine learning, then this chapter will hopefully give you some ideas of how to empower your users and allow them to tune and explore several aspects of the models used. If not, you should be able to complete the chapter, and it will hopefully inspire you to explore more machine learning concepts and techniques.</p>
			<p>We will go through the following topics in this chapter:</p>
			<ul>
				<li>Understanding clustering</li>
				<li>Finding the optimal number of clusters</li>
				<li>Clustering countries by population</li>
				<li>Preparing data with <strong class="source-inline">scikit-learn</strong></li>
				<li>Creating an interactive K-Means clustering app</li>
			</ul>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor134"/>Technical requirements</h1>
			<p>We will be exploring a few options from <strong class="source-inline">sklearn</strong>, as well as <strong class="source-inline">NumPy</strong>. Otherwise, we will be using the same tools we have been using. For visualization and building interactivity, Dash, JupyterDash, the Dash Core Component library, Dash HTML Components, Dash Bootstrap Components, Plotly, and Plotly Express will be used. For data manipulation and preparation, we will use <strong class="source-inline">pandas</strong> and <strong class="source-inline">NumPy</strong>. JupyterLab will be used for exploring and building independent functionality. Finally, <strong class="source-inline">sklearn</strong> will be used for building our machine learning models, as well as for preparing our data.</p>
			<p>The code files of this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Interactive-Dashboards-and-Data-Apps-with-Plotly-and-Dash/tree/master/chapter_09">https://github.com/PacktPublishing/Interactive-Dashboards-and-Data-Apps-with-Plotly-and-Dash/tree/master/chapter_09</a>.</p>
			<p>Check out the following video to see the Code in Action at <a href="https://bit.ly/3x8PAmt">https://bit.ly/3x8PAmt</a>.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor135"/>Understanding clustering</h1>
			<p>So, what exactly is<a id="_idIndexMarker472"/> clustering and when might it be helpful? Let's start with a very simple example. Imagine you have a group of people for whom we want to make T-shirts. We can make a T-shirt for each one of them, in whatever size required. The main restriction is that we can only make one size. The sizes are as follows: [1, 2, 3, 4, 5, 7, 9, 11]. Think how you might tackle this problem. We will use the <strong class="source-inline">KMeans</strong> algorithm for that, so let's start right away, as follows:</p>
			<ol>
				<li value="1">Import the required packages and models. <strong class="source-inline">NumPy</strong> will be imported as a package, but from <strong class="source-inline">sklearn</strong> we will import the only model that we will be using for now, as illustrated in the following code snippet:<p class="source-code">import numpy as np</p><p class="source-code">from sklearn.cluster import KMeans</p></li>
				<li>Create a dataset of sizes in the required format. Note that each observation (person's size) should be represented as a list, so we use the <strong class="source-inline">reshape</strong> method of <strong class="source-inline">NumPy</strong> arrays to get the data in the required format, as follows:<p class="source-code">sizes = np.array([1, 2, 3, 4, 5, 7, 9, 11]).reshape(-1, 1)</p><p class="source-code">sizes</p><p class="source-code"><strong class="bold">array([[ 1],</strong></p><p class="source-code"><strong class="bold">       [ 2],</strong></p><p class="source-code"><strong class="bold">       [ 3],</strong></p><p class="source-code"><strong class="bold">       [ 4],</strong></p><p class="source-code"><strong class="bold">       [ 5],</strong></p><p class="source-code"><strong class="bold">       [ 7],</strong></p><p class="source-code"><strong class="bold">       [ 9],</strong></p><p class="source-code"><strong class="bold">       [11]])</strong></p></li>
				<li>Create an instance of the <strong class="source-inline">KMeans</strong> model with the required number of clusters. An important<a id="_idIndexMarker473"/> feature of this model is that we provide the desired number of clusters for it. In this case, we were given a constraint, which is that we can only make T-shirts in one size, so we want to discover a single point that would be the center of our discovered cluster. We will explore the effect of the chosen number of clusters after that. Run the following code:<p class="source-code">kmeans1 = KMeans(n_clusters=1)</p></li>
				<li>Fit the model to the data using the <strong class="source-inline">fit</strong> method. This means that we want the model we just created to "learn" the dataset based on this particular algorithm, and with the provided parameters/options. This is the code you will need:<p class="source-code">kmeans1.fit(sizes)</p><p class="source-code"><strong class="bold">KMeans(n_clusters=1)</strong></p></li>
			</ol>
			<p>We now have a model that has been trained on this dataset, and we can go on to check some of its attributes. As a convention, the resulting attributes of the fitted models have a trailing underscore to them, as we will see now. We can now ask for the clusters that we asked for. The <strong class="source-inline">cluster_centers_</strong> attribute provides the answer to this. The centers (in this case, one center) are basically the means of the clusters of our data points. Let's check the result, as follows:</p>
			<p class="source-code">kmeans1.cluster_centers_</p>
			<p class="source-code"><strong class="bold">array([[5.25]])</strong></p>
			<p>We received the attribute in the form of a list of lists. The center of our cluster is <strong class="source-inline">5.25</strong>, apparently. You might be thinking that this is a convoluted way of calculating the mean of our dataset, and you would be right. Have a look at the following code snippet:</p>
			<p class="source-code">sizes.mean()</p>
			<p class="source-code"><strong class="bold">5.25</strong></p>
			<p>Indeed, our cluster center happens to be the mean of our dataset, which is exactly what we asked for. To visualize this, the following screenshot shows the relative position of the cluster center, related<a id="_idIndexMarker474"/> to our data points:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B16780_09_1.jpg" alt="Figure 9.1 – The Sizes data points, with the cluster center provided by KMeans"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 9.1 – The Sizes data points, with the cluster center provided by KMeans</p>
			<p>The chart depicted in the previous screenshot is very simple—we simply plot the sizes on the <em class="italic">x</em> axis, and an arbitrary constant value for the <em class="italic">y</em> axis.</p>
			<p>In order to evaluate the performance of our model and how well it fits the data, there are several ways to do this—one way is by checking the <strong class="source-inline">inertia_</strong> attribute. This is an attribute of the instance we created and can be accessed after fitting it to the data, using dot notation, as follows:</p>
			<p class="source-code">kmeans1.inertia_</p>
			<p class="source-code"><strong class="bold">85.5</strong></p>
			<p>The <strong class="source-inline">inertia_</strong> metric is the sum of squared distances of samples, to their closest cluster center. If the model performs well, the distances of the samples to the provided cluster centers should be as short as possible (data points are close to the cluster centers). A perfect model would have an inertia rate of <strong class="source-inline">0</strong>. Also, looking at it from the other side, we also know that asking for one cluster would give us the worst possible outcome because it is just one cluster, and to be the average point it has to be very far from the extreme data points.</p>
			<p>Accordingly, we can improve the performance of the model simply by adding more clusters, because their distances to the centers would be reduced.</p>
			<p>Now, imagine that I called you and shared some good news. We have an additional budget for one more size, and now we want to make T-shirts in two sizes. Translating to machine learning language, this means we need to create a new model with two clusters. We repeat the same steps and modify the <strong class="source-inline">n_clusters</strong> argument, as follows:</p>
			<p class="source-code">kmeans2 = KMeans(n_clusters=2)</p>
			<p class="source-code">kmeans2.fit(sizes)</p>
			<p class="source-code">kmeans2.cluster_centers_</p>
			<p class="source-code"><strong class="bold">array([[3.],</strong></p>
			<p class="source-code"><strong class="bold">       [9.]])</strong></p>
			<p>We now have <a id="_idIndexMarker475"/>two new centers, as specified.</p>
			<p>It's not enough to know the centers of our clusters. For each point, we need to know which cluster it belongs to, or we want to know the size of T-shirt that we will give to each person in our group. We can also count them and check the number of points in each cluster.</p>
			<p>The <strong class="source-inline">labels_</strong> attribute contains this information, and can be seen here:</p>
			<p class="source-code">kmeans2.labels_</p>
			<p class="source-code"><strong class="bold">array([0, 0, 0, 0, 0, 1, 1, 1], dtype=int32)</strong></p>
			<p>Note that the labels are given using integers starting at <strong class="source-inline">0</strong>. Also note that the numbers don't mean anything quantitative. Points that have a zero label are not from the first cluster; nor are points with the label 1 "more" than the others in any way. These are just labels, such as calling them <strong class="source-inline">group A</strong>, <strong class="source-inline">group B</strong>, and so on. </p>
			<p>We can map the labels to their respective values by using the <strong class="source-inline">zip</strong> function, as follows:</p>
			<p class="source-code">list(zip(sizes, kmeans2.labels_))</p>
			<p class="source-code"><strong class="bold">[(array([1]), 0),</strong></p>
			<p class="source-code"><strong class="bold"> (array([2]), 0),</strong></p>
			<p class="source-code"><strong class="bold"> (array([3]), 0),</strong></p>
			<p class="source-code"><strong class="bold"> (array([4]), 0),</strong></p>
			<p class="source-code"><strong class="bold"> (array([5]), 0),</strong></p>
			<p class="source-code"><strong class="bold"> (array([7]), 1),</strong></p>
			<p class="source-code"><strong class="bold"> (array([9]), 1),</strong></p>
			<p class="source-code"><strong class="bold"> (array([11]), 1)]</strong></p>
			<p>This will be very <a id="_idIndexMarker476"/>important later when use those labels in our charts.</p>
			<p>Let's also visualize the two centers to get a better idea of how this works. The following screenshot shows where the cluster centers are located relative to other data points:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B16780_09_2.jpg" alt=" Figure 9.2 – The Sizes data points, with two cluster centers provided by KMeans"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 9.2 – The Sizes data points, with two cluster centers provided by KMeans</p>
			<p>The centers make sense visually. We can see that the first five points are close to each other and that the last three are distinct and far from them, with larger gaps. Having 3 and 9 as cluster centers makes sense, because each of them is the mean of the values of its own cluster. Let's now numerically validate that we have improved the performance of our model by checking the inertia rate, as follows:</p>
			<p class="source-code">kmeans2.inertia_</p>
			<p class="source-code"><strong class="bold">18.0</strong></p>
			<p>Indeed, the performance was tremendously improved and reduced from 85.5 to 18.0. Nothing surprising here. As you can expect, every additional cluster would improve the performance until we reach the perfect result with an inertia rate of <strong class="source-inline">0</strong>. So, how do we evaluate the best options available for the number of clusters?</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor136"/>Finding the optimal number of clusters</h1>
			<p>We will now<a id="_idIndexMarker477"/> see the options we have in choosing the<a id="_idIndexMarker478"/> optimal number of clusters and what that entails, but let's first take a look at the following screenshot to visualize how things progress from having one cluster to eight clusters:</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B16780_09_3.jpg" alt="Figure 9.3 – Data points and cluster centers for all possible cluster numbers"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – Data points and cluster centers for all possible cluster numbers</p>
			<p>We can see the full spectrum of possible clusters and how they relate to data points. At the end, when we specified 8, we got the perfect solution, where every data point is a cluster center.</p>
			<p>In reality, you might not want to go for the full solution, for two main reasons. Firstly, it is probably going to be prohibitive from a cost perspective. Imagine making 1,000 T-shirts with a few hundred sizes. Secondly, in practical situations, it usually wouldn't add much value to add more clusters after a certain fit has been achieved. Using our T-shirt example, imagine if we have two people with sizes 5.3 and 5.27. They would most likely wear the same size anyway.</p>
			<p>So, we know that the optimal number of clusters is between one and the number of unique data points we have. We now want to explore the trade-offs and options of figuring out that optimal number. One of the strategies we can use is to check the value of additional—or incremental—clusters. When adding a new cluster, does it result in a meaningful reduction (improvement) in inertia? One such technique is called the "elbow technique." We plot the inertia values against the number of clusters and see where there is a sharp change in the direction of the curve. Let's do this now.</p>
			<p>We loop through the numbers from 1 to 8, and for each number we go through the same process of instantiating a <strong class="source-inline">KMeans</strong> object and getting the inertia for that number of clusters. We <a id="_idIndexMarker479"/>then add that value to our <strong class="source-inline">inertia</strong> list, as<a id="_idIndexMarker480"/> illustrated in the following code snippet:</p>
			<p class="source-code">inertia = []</p>
			<p class="source-code">for i in range(1, 9):</p>
			<p class="source-code">    kmeans = KMeans(i)</p>
			<p class="source-code">    kmeans.fit(sizes)</p>
			<p class="source-code">    inertia.append(kmeans.inertia_)</p>
			<p class="source-code">inertia</p>
			<p class="source-code"><strong class="bold">[85.5, 18.0, 10.5, 4.5, 2.5, 1.0, 0.5, 0.0]</strong></p>
			<p>As expected, our inertia improved from 85.5 to a perfect zero at the end. </p>
			<p>We now plot those to see where the elbow lies, as follows:</p>
			<p class="source-code">import plotly.graph_objects as go</p>
			<p class="source-code">fig = go.Figure()</p>
			<p class="source-code">fig.add_scatter(x=list(range(1, 9)), y=inertia)</p>
			<p class="source-code">fig.layout.xaxis.title = 'Number of clusters'</p>
			<p class="source-code">fig.layout.yaxis.title = 'Inertia'</p>
			<p class="source-code">fig.show()</p>
			<p>Running the preceding code produces the chart shown in the following screenshot: </p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B16780_09_4.jpg" alt="Figure 9.4 – The &quot;elbow&quot; method, showing inertia values for all possible cluster numbers"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – The "elbow" method, showing inertia values for all possible cluster numbers</p>
			<p>You can clearly see a sudden drop when moving from 1 to 2 and that inertia keeps decreasing, but at a lower rate as we move toward the final value. So, three or maybe four clusters might be the point where we start to get diminishing returns, and that could be our<a id="_idIndexMarker481"/> optimal number of clusters. We also cheated <a id="_idIndexMarker482"/>a little by including one cluster, because we already knew that it would be the worst-performing number of clusters. You can see the same plot without the first value in the following screenshot:</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B16780_09_5.jpg" alt="Figure 9.5 – The &quot;elbow&quot; method, showing inertia values for all possible cluster numbers excluding 1"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – The "elbow" method, showing inertia values for all possible cluster numbers excluding 1</p>
			<p>This looks quite different and also shows that we cannot mechanically make a decision without knowing more about the data, the use case, and whatever constraints we might have.</p>
			<p>The example we explored was extremely simple in terms of the number of observations, as well as the number of dimensions, which was one dimension. <strong class="source-inline">KMeans</strong> clustering (and machine learning in general) usually handles multiple dimensions, and the concept is basically the same: we try to find centers of clusters that minimize the distance between them and the data points. For example, the following screenshot shows what a similar problem might look like in two dimensions:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B16780_09_6.jpg" alt="Figure 9.6 – Clustered points in two dimensions"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Clustered points in two dimensions</p>
			<p>This could correspond to additional measurements relating to our group of people. So, we might have their height on the <em class="italic">x</em> axis and their weight on the <em class="italic">y</em> axis, for example. You can imagine what <strong class="source-inline">KMeans</strong> would give us in this case. Of course, in real life, data is rarely so neatly<a id="_idIndexMarker483"/> clustered. You can also see how much accuracy<a id="_idIndexMarker484"/> we might lose by selecting the wrong number of clusters. If we specify three clusters, for example, the three blobs in the middle would probably be considered a single cluster, even though we can see that they are quite distinct from one another and that their points are quite close to each other. Also, if we specify seven or eight clusters, we could get unnecessary divisions between clusters, or we would have passed the elbow on the elbow chart in this case.</p>
			<p>We are now ready to use this understanding of clustering in our dataset.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor137"/>Clustering countries by population</h1>
			<p>We will first <a id="_idIndexMarker485"/>understand this with one indicator that we are familiar with (population), and then make it interactive. We will cluster groups of countries based on their population.</p>
			<p>Let's start with a possible practical situation. Imagine you were asked to group countries by population. You are supposed to have two groups of countries, of high and low populations. How do you do that? Where do you draw the line(s), and what does the total of the population have to be in order for it to qualify as "high"? Imagine that you were then asked to group countries into three or four groups based on their population. How would you update your clusters?</p>
			<p>We can easily see how <strong class="source-inline">KMeans</strong> clustering is ideal for that.</p>
			<p>Let's now do the same exercise with <strong class="source-inline">KMeans</strong> using one dimension, and then combine that with our knowledge of mapping, as follows:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">pandas</strong> and open the <strong class="source-inline">poverty</strong> dataset, like this:<p class="source-code">import pandas as pd</p><p class="source-code">poverty = pd.read_csv('data/poverty.csv')</p></li>
				<li>Create variables for the year and desired indicators, as follows:<p class="source-code">year = 2018</p><p class="source-code">indicators = ['Population, total']</p></li>
				<li>Instantiate <a id="_idIndexMarker486"/>a <strong class="source-inline">KMeans</strong> object with the desired number of clusters, like this:<p class="source-code">kmeans = KMeans(n_clusters=2)</p></li>
				<li>Create a <strong class="source-inline">df</strong> object, which is the <strong class="source-inline">poverty</strong> DataFrame containing only countries and data from the selected year only. Run the following code to do this:<p class="source-code">df = poverty[poverty['year'].eq(year) &amp; poverty['is_country']]</p></li>
				<li>Create a <strong class="source-inline">data</strong> object, which is a list of columns that we choose (in this case, we only chose one). Note in the following code snippet that we get its <strong class="source-inline">values</strong> attribute, which returns the underlying <strong class="source-inline">NumPy</strong> array:<p class="source-code">data = df[indicators].values</p></li>
				<li>Fit the model to the data, as follows:<p class="source-code">kmeans.fit(data)</p></li>
			</ol>
			<p>We have now trained the model on our data and are ready to visualize the results. Remember our discussion in <a href="B16780_07_Final_NM_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 7</em></a>, <em class="italic">Exploring Map Plots and Enriching Your Dashboards with Markdown</em>, that in order to create a map, we simply need a DataFrame with a column containing country names (or codes)? This is enough to produce a map. If we want to color our countries, we need another column (or any list-like object) containing corresponding values.</p>
			<p>The <strong class="source-inline">kmeans</strong> object we just trained contains the labels of the countries and would tell us which country belongs to which cluster. We will use that to color countries, so we do this with one function call. Note that we can convert the labels to strings, which would cause Plotly Express to treat them as categorical and not continuous variables. The code is shown in the following snippet: </p>
			<p class="source-code">px.choropleth(df,</p>
			<p class="source-code">              locations='Country Name',</p>
			<p class="source-code">              locationmode='country names',</p>
			<p class="source-code">              color=<strong class="bold">[str(x) for x in  </strong><strong class="bold">kmeans.labels_]</strong>)</p>
			<p>This code<a id="_idIndexMarker487"/> produces the chart shown in the following screenshot: </p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B16780_09_7.jpg" alt="Figure 9.7 – Countries clustered by population"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Countries clustered by population</p>
			<p>Since we have already developed a template of options for maps, we can copy this and use it to enhance this map and make it consistent with the theme of our app. Let's use that and see the effect of having <strong class="bold">1</strong>, <strong class="bold">2</strong>, <strong class="bold">3</strong>, and <strong class="bold">4</strong> clusters on the same map, and discuss the details. The following screenshot shows four maps, each with a different number of clusters:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B16780_09_8.jpg" alt="Figure 9.8 – Countries clustered by population using different numbers of clusters"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Countries clustered by population using different numbers of clusters</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The colors may<a id="_idIndexMarker488"/> not be easy to distinguish on these maps if you are reading the grayscale version, and I encourage you to check out the online version and repository.</p>
			<p>As you can see, coloring the map with one cluster (one label for all countries) produces a map with a single color. Things start to get interesting when there are two clusters involved, and this also makes intuitive sense. The two countries forming the cluster with a higher population (namely, China and India) have very large populations that are also close to each <a id="_idIndexMarker489"/>other—1.39 billion and 1.35 billion, in this case. The third country, the <strong class="bold">United States</strong> (<strong class="bold">US</strong>), has a population of 327 million. This is exactly what KMeans is supposed to do. It gave us two groups of countries where countries in each cluster are very close to each other and far from countries in the other cluster. Of course, we introduced an important bias by selecting two as the number of clusters, and we saw how that might not be the optimal case.</p>
			<p>When we chose three clusters, we can see that we have a medium-population cluster, with the US being one of them. Then, when we chose four, you can see that Russia and Japan were moved to the third category, even though they were in the second category when we had three clusters.</p>
			<p>We now have enough code and knowledge to take this to the next level. We want to give our users<a id="_idIndexMarker490"/> the option to select the number of clusters and indicator(s) that they want. We need to address some issues in our data first, so let's explore that.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor138"/>Preparing data with scikit-learn</h1>
			<p><strong class="source-inline">scikit-learn</strong> is one<a id="_idIndexMarker491"/> of the most widely used and comprehensive machine learning libraries in Python. It plays very well with the rest of the data-science ecosystem libraries, such as <strong class="source-inline">NumPy</strong>, <strong class="source-inline">pandas</strong>, and <strong class="source-inline">matplotlib</strong>. We will be using it for modeling our data and for some preprocessing as well.</p>
			<p>We now have two issues that we need to tackle first: missing values and scaling data. Let's see two simple examples for each, and then tackle them in our dataset. Let's start with missing values.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor139"/>Handling missing values</h2>
			<p>Models need <a id="_idIndexMarker492"/>data, and they can't know what to do with a set of numbers containing missing values. In such cases (and there are many in our dataset), we need to make a decision on what to do with those missing values.</p>
			<p>There are several options, and the right choice depends on the application as well as the nature of the data, but we won't get into those details. For simplicity, we will make a generic choice of replacing missing data with suitable values.</p>
			<p>Let's explore how we can impute missing values with a simple example, as follows:</p>
			<ol>
				<li value="1">Create a simple dataset containing a missing value, in a suitable format, as illustrated in the following code snippet:<p class="source-code">data = np.array([1, 2, 1, 2, np.nan]).reshape(-1, 1)</p></li>
				<li>Import the <strong class="source-inline">SimpleImputer</strong> for <strong class="source-inline">scikit-learn</strong>, as follows:<p class="source-code">from sklearn.impute import SimpleImputer</p></li>
				<li>Create an instance of this class with a <strong class="source-inline">mean</strong> strategy, which is the default. As you might have guessed, there are other strategies for imputing missing values. The code is shown in the following snippet:<p class="source-code">imp = SimpleImputer(strategy='mean')</p></li>
				<li>Fit the model to the data. This is where the model learns the data, given the conditions and options we set while instantiating it. The code is shown in the following snippet:<p class="source-code">imp.fit(data)</p></li>
				<li>Transform the<a id="_idIndexMarker493"/> data. Now that the model has learned the data, it is able to transform it according to the rules that we set. The <strong class="source-inline">transform</strong> method exists in many models and has a different meaning, depending on the context. In this case, transforming means imputing the missing data, using the <strong class="source-inline">mean</strong> strategy. The code can be seen in the following snippet:<p class="source-code">imp.transform(data)</p><p class="source-code">array([[1. ],</p><p class="source-code">       [2. ],</p><p class="source-code">       [1. ],</p><p class="source-code">       [2. ],</p><p class="source-code">       <strong class="bold">[1.5]</strong>])</p></li>
			</ol>
			<p>As you can see, the model has transformed the data by replacing the missing value with 1.5. If you look at the other non-missing values [<strong class="source-inline">1</strong>, <strong class="source-inline">2</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">2</strong>], you can easily see that their mean is 1.5, and this is the result that we got. We could have specified a different strategy for imputing missing values, such as the median or the most frequent strategy. Each has its own advantages and disadvantages; we are simply exploring what can be done with <a id="_idIndexMarker494"/>machine learning in Dash.</p>
			<p>Next, we move on to scaling our data.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>Scaling data with scikit-learn</h2>
			<p>In <em class="italic">Figure 9.6</em>, we<a id="_idIndexMarker495"/> saw how clustering might look in two dimensions. If we want to cluster our poverty data by two indicators, one of them would be on the <em class="italic">x</em> axis and the other would be on the <em class="italic">y</em> axis. Now, imagine if we had a population on one of the axes and a percentage indicator on the other axis. The data on the population axis would range from 0 to 1.4 billion, and the data on the other axis would range from 0 to 1 (or from 0 to 100). Any differences in the percentage indicator would have negligible influence on the distances, and the means would mainly be calculated using the disproportionate size of the population numbers. One solution to this problem is to scale values.</p>
			<p>There are different strategies for scaling data and we will explore one of them—namely, standard scaling. The <strong class="source-inline">StandardScaler</strong> class assigns <strong class="source-inline">z</strong>-scores (or standard scores) to the data points and normalizes them. The z-score is simply calculated by subtracting each value from the mean and dividing by the standard deviation. There are other ways of calculating this but we will focus on a simple example to better illustrate this concept, as follows:</p>
			<ol>
				<li value="1">Create a simple dataset, like this:<p class="source-code">data = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)</p></li>
				<li>Import <strong class="source-inline">StandardScaler</strong> and create an instance of it, like this:<p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">scaler = StandardScaler()</p></li>
				<li>Fit <strong class="source-inline">scaler</strong> to the data and transform it. For convenience, many models that have a <strong class="source-inline">fit</strong> and a <strong class="source-inline">transform</strong> method and also have a <strong class="source-inline">fit_transform</strong> method, which we will use now, as follows:<p class="source-code">scaler.fit_transform(data)</p><p class="source-code">array([[-1.41421356],</p><p class="source-code">       [-0.70710678],</p><p class="source-code">       [ 0.        ],</p><p class="source-code">       [ 0.70710678],</p><p class="source-code">       [ 1.41421356]])</p></li>
			</ol>
			<p>We have now transformed our numbers to their equivalent z-scores. Note that the mean value 3 has now become 0. Anything higher than 3 is positive, and anything lower is negative. The numbers also indicate how far (high or low) the corresponding value is from the mean.</p>
			<p>This way, when we have multiple features in our dataset, we can normalize them, compare them, and use them together. At the end of the day, what we care about is how extreme a certain value is and how close it is to the mean. A country with a Gini index of 90 is a very extreme case. It's as extreme as a country with a population of 1 billion. If we use those two together, the 1 billion will dominate and distort the calculation. Standardization helps us achieve a better way of dealing with data at different<a id="_idIndexMarker496"/> scales. It's still not perfect but is a very big improvement over using data in different scales. Now, we are able to use more than one feature while clustering our data.</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor141"/>Creating an interactive KMeans clustering app</h1>
			<p>Let's now put <a id="_idIndexMarker497"/>everything together and make an interactive clustering application using our dataset. We will give users the option to choose the year, as well as the indicator(s) that they want. They can also select the number of clusters and get a visual representation of those clusters, in the form of a colored choropleth map, based on the discovered clusters.</p>
			<p>Please note that it is challenging to interpret such results with multiple indicators because we will be handling more than one dimension. It can also be difficult if you are not an economist and don't know which indicators make sense to be checked with which other indicators, and so on.</p>
			<p>The following screenshot shows what we will be working toward:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B16780_09_9.jpg" alt="Figure 9.9 – An interactive KMeans clustering application"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – An interactive KMeans clustering application</p>
			<p>As you can see, this is a fairly rich application in terms of the combinations of options that it <a id="_idIndexMarker498"/>provides. As I also mentioned, it's not straightforward to interpret, but as mentioned several times in the chapter, we are simply exploring what can be done with only one technique and using only some of its options.</p>
			<p>We have created many sliders and dropdowns so far in the book, so we won't go into how to make them. We will simply make sure we have descriptive IDs for them, and I'll leave it to you to fill in the blanks. As shown in the preceding screenshot, we have two sliders, one dropdown, and one graph component, so let's set ID names for them. As usual, the following components should go wherever you want them to be in <strong class="source-inline">app.layout</strong>:</p>
			<p class="source-code">dcc.Slider(id='year_cluster_slider', …),</p>
			<p class="source-code">dcc.Slider(id='ncluster_cluster_slider', …),</p>
			<p class="source-code">dcc.Dropdown(id='cluster_indicator_dropdown', …),</p>
			<p class="source-code">dcc.Graph(id='clustered_map_chart', …)</p>
			<p>We will now go through creating our callback function step by step, as follows: </p>
			<ol>
				<li value="1">Associate the inputs and outputs in the callback, as follows: <p class="source-code">@app.callback(Output('clustered_map_chart', 'figure'),</p><p class="source-code">              Input('year_cluster_slider', 'value'),</p><p class="source-code">              Input('ncluster_cluster_slider', 'value'),</p><p class="source-code">              Input('cluster_indicator_dropdown', 'value'))</p></li>
				<li>Create the<a id="_idIndexMarker499"/> signature of the function with suitable parameter names, as follows: <p class="source-code">def clustered_map(year, n_clusters, indicators):</p></li>
				<li>Instantiate a missing values imputer, a standard scaler, and a <strong class="source-inline">KMeans</strong> object. Note that with <strong class="source-inline">SimpleImputer</strong>, we also specify how missing values are encoded. In this case, they are encoded as <strong class="source-inline">np.nan</strong>, but in other cases they might be encoded differently, such as <strong class="source-inline">N/A</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">-1</strong>, or others. The code is shown in the following snippet:<p class="source-code">imp = SimpleImputer(missing_values=np.nan, strategy='mean')</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">kmeans = KMeans(n_clusters=n_clusters)</p></li>
				<li>Create <strong class="source-inline">df</strong>, a subset of <strong class="source-inline">poverty</strong> that has only country's data and data from the selected year, and then select the <strong class="source-inline">year</strong> and <strong class="source-inline">Country Name</strong> columns, and the selected indicators. The code is shown in the following snippet: <p class="source-code">df = poverty[poverty['is_country'] &amp; poverty['year'].eq(year)][indicators + ['Country Name', 'year']]</p></li>
				<li>Create <strong class="source-inline">data</strong>, a subset of <strong class="source-inline">df</strong> that only contains the selected indicators' columns. The reason we have two distinct objects is that <strong class="source-inline">df</strong> is going to be used for plotting the map and will also use the year and country name. At the same time, <strong class="source-inline">data</strong> will only contain numeric values in order for our models to work with it. The code can be seen here:<p class="source-code">data = df[indicators]</p></li>
				<li>In some cases, as we saw several times in the book, we might come across a situation where we have a column that is completely empty. In this case, we can't impute any missing values because we don't have a mean, and we have absolutely no clue what to do with it. In this case, I think it's best to not produce a chart and inform the user that for the selected combination of options, there is not enough data to run the models. We first check if we have such a situation. DataFrame <a id="_idIndexMarker500"/>objects have an <strong class="source-inline">isna</strong> method. When we run it, it returns the same DataFrame filled with <strong class="source-inline">True</strong> and <strong class="source-inline">False</strong> values, indicating whether or not the respective value is missing. We then run the <strong class="source-inline">all</strong> method on the resulting DataFrame. This will tell us if all values are missing for each column. Now, we have a pandas Series with <strong class="source-inline">True</strong> and <strong class="source-inline">False</strong> values. We check if any of them is <strong class="source-inline">True</strong> by using the <strong class="source-inline">any</strong> method. In this case, we create an empty chart with an informative title, as follows:<p class="source-code">if df.isna().all().any():</p><p class="source-code">    return px.scatter(title='No available data for the selected combination of year/indicators.')</p></li>
				<li>If everything went fine, and we don't have a column with all its values missing, we proceed by creating a variable that has no missing values (imputed if some are missing), as follows: <p class="source-code">data_no_na = imp.fit_transform(data)</p></li>
				<li>Next, we scale <strong class="source-inline">data_no_na</strong> using our instance of <strong class="source-inline">StandardScaler</strong>, like this:<p class="source-code">scaled_data = scaler.fit_transform(data_no_na)</p></li>
				<li>Then, we fit the model to our scaled data, as follows:<p class="source-code">kmeans.fit(scaled_data)</p></li>
				<li>We now have everything we need to produce our chart—most importantly, the <strong class="source-inline">labels_</strong> attribute—and we can do so with a single call to <strong class="source-inline">px.choropleth</strong>. There is nothing new in the options we use in this function, as you can observe in the following code snippet: <p class="source-code">fig = px.choropleth(df,</p><p class="source-code">                    locations='Country Name',</p><p class="source-code">                    locationmode='country names',</p><p class="source-code">                    <strong class="bold">color=[str(x) for x in  kmeans.labels_]</strong>,</p><p class="source-code">                    labels={'color': 'Cluster'},</p><p class="source-code">                    hover_data=indicators,</p><p class="source-code">                    height=650,</p><p class="source-code">                    title=f'Country clusters - {year}. Number of clusters: {n_clusters}&lt;br&gt;Inertia: {kmeans.inertia_:,.2f}')</p></li>
			</ol>
			<p>After that, we<a id="_idIndexMarker501"/> copy the geographic attributes we already used to customize the map and make it consistent with the app as a whole.</p>
			<p>Here is the full function, including the geographic options for your reference: </p>
			<p class="source-code">@app.callback(Output('clustered_map_chart', 'figure'),</p>
			<p class="source-code">              Input('year_cluster_slider', 'value'),</p>
			<p class="source-code">              Input('ncluster_cluster_slider', 'value'),</p>
			<p class="source-code">              Input('cluster_indicator_dropdown', 'value'))</p>
			<p class="source-code">def clustered_map(year, n_clusters, indicators):</p>
			<p class="source-code">    imp = SimpleImputer(missing_values=np.nan, strategy='mean')</p>
			<p class="source-code">    scaler = StandardScaler()</p>
			<p class="source-code">    kmeans = KMeans(n_clusters=n_clusters)</p>
			<p class="source-code">    df = poverty[poverty['is_country'] &amp; poverty['year'].eq(year)][indicators + ['Country Name', 'year']]</p>
			<p class="source-code">    data = df[indicators]</p>
			<p class="source-code">    if df.isna().all().any():</p>
			<p class="source-code">        return px.scatter(title='No available data for the selected combination of year/indicators.')</p>
			<p class="source-code">    data_no_na = imp.fit_transform(data)</p>
			<p class="source-code">    scaled_data = scaler.fit_transform(data_no_na)</p>
			<p class="source-code">    kmeans.fit(scaled_data)</p>
			<p class="source-code">    fig = px.choropleth(df,</p>
			<p class="source-code">                        locations='Country Name',</p>
			<p class="source-code">                        locationmode='country names',</p>
			<p class="source-code">                        color=[str(x) for x in  kmeans.labels_],</p>
			<p class="source-code">                        labels={'color': 'Cluster'},</p>
			<p class="source-code">                        hover_data=indicators,</p>
			<p class="source-code">                        height=650,</p>
			<p class="source-code">                        title=f'Country clusters - {year}. Number of clusters: {n_clusters}&lt;br&gt;Inertia: {kmeans.inertia_:,.2f}',</p>
			<p class="source-code">                  color_discrete_sequence=px.colors.qualitative.T10)</p>
			<p class="source-code">    fig.layout.geo.showframe = False</p>
			<p class="source-code">    fig.layout.geo.showcountries = True</p>
			<p class="source-code">    fig.layout.geo.projection.type = 'natural earth'</p>
			<p class="source-code">    fig.layout.geo.lataxis.range = [-53, 76]</p>
			<p class="source-code">    fig.layout.geo.lonaxis.range = [-137, 168]</p>
			<p class="source-code">    fig.layout.geo.landcolor = 'white'</p>
			<p class="source-code">    fig.layout.geo.bgcolor = '#E5ECF6'</p>
			<p class="source-code">    fig.layout.paper_bgcolor = '#E5ECF6'</p>
			<p class="source-code">    fig.layout.geo.countrycolor = 'gray'</p>
			<p class="source-code">    fig.layout.geo.coastlinecolor = 'gray'</p>
			<p class="source-code">    return fig</p>
			<p>We have made a big jump in this chapter in what can be visualized and interactively explored. We also <a id="_idIndexMarker502"/>got a minimal introduction to a single machine learning technique to cluster our data. Ideally, the options provided to your users will depend on the discipline you are working in. You might be an expert yourself in the domain you are dealing with, or you might work closely with such an expert. It is not only a matter of visualization and statistics, but domain knowledge is also a crucial aspect of analyzing data, and with machine learning this is critical.</p>
			<p>I encourage you to learn more and see what you can achieve. Having the skills to create interactive dashboards is a big advantage for running machine learning models, as we saw, and allows you to discover trends and make decisions at a much faster rate. Eventually, you will be able to create automated solutions that provide recommendations, or make certain decisions based on certain inputs.</p>
			<p>Let's now recap on what we learned in this chapter.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor142"/>Summary</h1>
			<p>We first got an idea of how clustering works. We built the simplest possible model for a tiny dataset. We ran the model a few times and evaluated the performance and outcomes for each of the numbers of clusters that we chose.</p>
			<p>We then explored the elbow technique to evaluate different clusters and saw how we might discover the point of diminishing returns, where not much improvement is achieved by adding new clusters. With that knowledge, we used the same technique for clustering countries by a metric with which most of us are familiar and got firsthand experience in how it might work on real data.</p>
			<p>After that, we planned an interactive KMeans app and explored two techniques for preparing data before running our model. We mainly explored imputing missing values and scaling data.</p>
			<p>This gave us enough knowledge to get our data in a suitable format for us to create our interactive app, which we did at the end of the chapter.</p>
			<p>We next explored advanced features of Dash callbacks—most notably, pattern-matching callbacks. The callbacks we have run so far have been straightforward and fixed. Many times, we want to create more dynamic interfaces for our users. For example, based on the selection of a certain value in a dropdown, we might want to display a special type of chart or create another dropdown. We will explore how this works in the next chapter.</p>
		</div>
	</body></html>