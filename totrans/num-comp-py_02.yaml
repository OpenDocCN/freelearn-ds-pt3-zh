- en: Tree-Based Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of tree-based methods is to segment the feature space into a number
    of simple rectangular regions, to subsequently make a prediction for a given observation
    based on either mean or mode (mean for regression and mode for classification,
    to be precise) of the training observations in the region to which it belongs.
    Unlike most other classifiers, models produced by decision trees are easy to interpret.
    In this chapter, we will be covering the following decision tree-based models
    on HR data examples for predicting whether a given employee will leave the organization
    in the near future or not. In this chapter, we will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees - simple model and model with class weight tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging (bootstrap aggregation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest - basic random forest and application of grid search on hyperparameter
    tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting (AdaBoost, gradient boost, extreme gradient boost - XGBoost)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble of ensembles (with heterogeneous and homogeneous models)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing decision tree classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision tree classifiers produce rules in simple English sentences, which can
    easily be interpreted and presented to senior management without any editing.
    Decision trees can be applied to either classification or regression problems.
    Based on features in data, decision tree models learn a series of questions to
    infer the class labels of samples.
  prefs: []
  type: TYPE_NORMAL
- en: In the following figure, simple recursive decision rules have been asked by
    a programmer himself to do relevant actions. The actions would be based on the
    provided answers for each question, whether yes or no.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e2511d1-0a30-4f0b-83cb-9b5822697197.png)'
  prefs: []
  type: TYPE_IMG
- en: Terminology used in decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Decision Trees do not have much machinery as compared with logistic regression.
    Here we have a few metrics to study. We will majorly focus on impurity measures;
    decision trees split variables recursively based on set impurity criteria until
    they reach some stopping criteria (minimum observations per terminal node, minimum
    observations for split at any node, and so on):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Entropy:** Entropy came from information theory and is the measure of impurity
    in data. If the sample is completely homogeneous, the entropy is zero, and if
    the sample is equally divided, it has entropy of one. In decision trees, the predictor
    with most heterogeneousness will be considered nearest to the root node to classify
    the given data into classes in a greedy mode. We will cover this topic in more
    depth in this chapter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c53d01d9-81b5-4d64-8d55-96510a38f5b3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where n = number of classes. Entropy is maximum in the middle, with a value
    of *1* and minimum at the extremes with a value of *0*. The low value of entropy
    is desirable, as it will segregate classes better.
  prefs: []
  type: TYPE_NORMAL
- en: '**Information Gain:** Information gain is the expected reduction in entropy
    caused by partitioning the examples according to a given attribute. The idea is
    to start with mixed classes and to continue partitioning until each node reaches
    its observations of purest class. At every stage, the variable with maximum information
    gain is chosen in a greedy fashion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Information Gain = Entropy of Parent - sum (weighted % * Entropy of Child)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Weighted % = Number of observations in particular child/sum (observations
    in all child nodes)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gini:** Gini impurity is a measure of misclassification, which applies in
    a multi-class classifier context. Gini works similar to entropy, except Gini is
    quicker to calculate:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/17db81ae-57f3-4898-9b29-6b92262d8793.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *i = Number of classes*. The similarity between Gini and entropy is shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65f8a2ff-dbcb-45b3-9f21-0f6fc05025bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision tree working methodology from first principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, the response variable has only two classes: whether
    to play tennis or not. But the following table has been compiled based on various
    conditions recorded on various days. Now, our task is to find out which output
    the variables are resulting in most significantly: YES or NO.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This example comes under the Classification tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **Day** | **Outlook** | **Temperature** | **Humidity** | **Wind** | **Play
    tennis** |'
  prefs: []
  type: TYPE_TB
- en: '| D1 | Sunny | Hot | High | Weak | No |'
  prefs: []
  type: TYPE_TB
- en: '| D2 | Sunny | Hot | High | Strong | No |'
  prefs: []
  type: TYPE_TB
- en: '| D3 | Overcast | Hot | High | Weak | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| D4 | Rain | Mild | High | Weak | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| D5 | Rain | Cool | Normal | Weak | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| D6 | Rain | Cool | Normal | Strong | No |'
  prefs: []
  type: TYPE_TB
- en: '| D7 | Overcast | Cool | Normal | Strong | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| D8 | Sunny | Mild | High | Weak | No |'
  prefs: []
  type: TYPE_TB
- en: '| D9 | Sunny | Cool | Normal | Weak | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| D10 | Rain | Mild | Normal | Weak | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| D11 | Sunny | Mild | Normal | Strong | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| D12 | Overcast | Mild | High | Strong | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| D13 | Overcast | Hot | Normal | Weak | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| D14 | Rain | Mild | High | Strong | No |'
  prefs: []
  type: TYPE_TB
- en: 'Taking the Humidity variable as an example to classify the Play Tennis field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**CHAID:** Humidity has two categories and our expected values should be evenly
    distributed in order to calculate how distinguishing the variable is:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0853cd9c-7533-48da-88b5-4ad9287d3272.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculating *x²* (Chi-square) value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46a70084-0231-46b4-a8d7-bb2497f161fa.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Calculating degrees of freedom = (r-1) * (c-1)*'
  prefs: []
  type: TYPE_NORMAL
- en: Where r = number of row components/number of variable categories, C = number
    of response variables.
  prefs: []
  type: TYPE_NORMAL
- en: Here, there are two row categories (High and Normal) and two column categories
    (No and Yes).
  prefs: []
  type: TYPE_NORMAL
- en: Hence = *(2-1) * (2-1) = 1*
  prefs: []
  type: TYPE_NORMAL
- en: p-value for Chi-square 2.8 with 1 d.f = 0.0942
  prefs: []
  type: TYPE_NORMAL
- en: 'p-value can be obtained with the following Excel formulae: *= CHIDIST (2.8,
    1) = 0.0942*'
  prefs: []
  type: TYPE_NORMAL
- en: In a similar way, we will calculate the *p-value* for all variables and select
    the best variable with a low p-value.
  prefs: []
  type: TYPE_NORMAL
- en: '**ENTROPY**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entropy = - Σ p * log [2] p
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d60403c-6574-4fec-9eb5-760ca7cc1add.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b78a09fc-92e5-4213-a650-2b0faab40b9b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/78599af8-1208-4e50-a737-4cefced3788c.jpg)![](img/ad1509db-8136-49bf-805c-7f39fbad70a9.jpg)![](img/282a23e5-2e0e-4b69-a044-e12f4cb82e38.jpg)![](img/1c2922a8-38c8-4b82-b30b-e150ca90fd9f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In a similar way, we will calculate *information gain* for all variables and
    select the best variable with the *highest information gain.*
  prefs: []
  type: TYPE_NORMAL
- en: '**GINI**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gini = 1- Σp²*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8dad0ee-8f1a-44b2-8464-e17aa9dfcd66.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/da771f33-f6a8-44cf-9684-5e8a42a8407b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/786732ae-2efc-4ae4-a84a-c7a9f4fcdb8e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/02ac841a-7b0b-4b73-82c4-d6ec41a5c7d7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In a similar way, we will calculate *Expected Gini* for all variables and select
    the best with the *lowest expected value***.**
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of a better understanding, we will also do similar calculations
    for the Wind variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAID:** Wind has two categories and our expected values should be evenly
    distributed in order to calculate how distinguishing the variable is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/771f6483-31db-49ab-b694-294ac635db7d.jpg)![](img/499cc86e-2728-4d27-833e-9f95e49d385e.jpg)![](img/1859ff9c-69c4-4515-a8c3-a8a3d5a16c80.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**ENTROPY**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8a213ea5-6d1a-49d6-968b-ae26b41ff13a.png)![](img/09b3a3a8-b83b-4048-ae99-1c15c46cdde5.jpg)![](img/5856ac10-a258-4e0f-a058-bee5ed73bd2e.jpg)![](img/39bac9cf-cbe4-4815-892a-c1e1b9e0e9ec.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**GINI**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8646761f-f269-446b-acc8-63511d734c7c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ed77db34-70a2-434a-a6c2-a09158776786.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/7e42d1c1-3b19-41e2-8258-1ef9531d2b8f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we will compare both variables for all three metrics so that we can understand
    them better.
  prefs: []
  type: TYPE_NORMAL
- en: '| Variables | CHAID (p-value) | Entropy information gain | Gini expected value
    |'
  prefs: []
  type: TYPE_TB
- en: '| Humidity | 0.0942 | 0.1518 | 0.3669 |'
  prefs: []
  type: TYPE_TB
- en: '| Wind | 0.2733 | 0.0482 | 0.4285 |'
  prefs: []
  type: TYPE_TB
- en: '| Better | Low value | High value | Low value |'
  prefs: []
  type: TYPE_TB
- en: For all three calculations, Humidity is proven to be a better classifier than
    Wind. Hence, we can confirm that all methods convey a similar story.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison between logistic regression and decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into the coding details of decision trees, here, we will quickly
    compare the differences between logistic regression and decision trees, so that
    we will know which model is better and in what way.
  prefs: []
  type: TYPE_NORMAL
- en: '| Logistic regression | Decision trees |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression model looks like an equation between independent variables
    with respect to its dependent variable. | Tree classifiers produce rules in simple
    English sentences, which can be easily explained to senior management. |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression is a parametric model, in which the model is defined
    by having parameters multiplied by independent variables to predict the dependent
    variable. | Decision Trees are a non-parametric model, in which no pre-assumed
    parameter exists. Implicitly performs variable screening or feature selection.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Assumptions are made on response (or dependent) variable, with binomial or
    Bernoulli distribution. | No assumptions are made on the underlying distribution
    of the data. |'
  prefs: []
  type: TYPE_TB
- en: '| Shape of the model is predefined (logistic curve). | Shape of the model is
    not predefined; model fits in best possible classification based on the data instead.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Provides very good results when independent variables are continuous in nature,
    and also linearity holds true. | Provides best results when most of the variables
    are categorical in nature. |'
  prefs: []
  type: TYPE_TB
- en: '| Difficult to find complex interactions among variables (non-linear relationships
    between variables). | Non-linear relationships between parameters do not affect
    tree performance. Often uncover complex interactions. Trees can handle numerical
    data with highly skewed or multi-modal, as well as categorical predictors with
    either ordinal or non-ordinal structure. |'
  prefs: []
  type: TYPE_TB
- en: '| Outliers and missing values deteriorate the performance of logistic regression.
    | Outliners and missing values are dealt with grace in decision trees. |'
  prefs: []
  type: TYPE_TB
- en: Comparison of error components across various styles of models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Errors need to be evaluated in order to measure the effectiveness of the model
    in order to improve the model''s performance further by tuning various knobs.
    Error components consist of a bias component, variance component, and pure white
    noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38da4c33-d21a-4098-bd1c-13e9b1274166.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Out of the following three regions:'
  prefs: []
  type: TYPE_NORMAL
- en: The first region has high bias and low variance error components. In this region,
    models are very robust in nature, such as linear regression or logistic regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whereas the third region has high variance and low bias error components, in
    this region models are very wiggly and vary greatly in nature, similar to decision
    trees, but due to the great amount of variability in the nature of their shape,
    these models tend to overfit on training data and produce less accuracy on test
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, the middle region, also called the second region, is the
    ideal sweet spot, in which both bias and variance components are moderate, causing
    it to create the lowest total errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b1e7525f-15f4-4e4f-8600-07c0646be1b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Remedial actions to push the model towards the ideal region
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Models with either high bias or high variance error components do not produce
    the ideal fit. Hence, some makeovers are required to do so. In the following diagram,
    the various methods applied are shown in detail. In the case of linear regression,
    there would be a high bias component, meaning the model is not flexible enough
    to fit some non-linearities in data. One turnaround is to break the single line
    into small linear pieces and fit them into the region by constraining them at
    knots, also called **Linear Spline**. Whereas decision trees have a high variance
    problem, meaning even a slight change in *X* values leads to large changes in
    its corresponding *Y* values, this issue can be resolved by performing an ensemble
    of the decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f13d344e-600a-4c0c-b8dd-b354271975f8.png)'
  prefs: []
  type: TYPE_IMG
- en: In practice, implementing splines would be a difficult and not so popular method,
    due to the involvement of the many equations a practitioner has to keep tabs on,
    in addition to checking the linearity assumption and other diagnostic KPIs (p-values,
    AIC, multi-collinearity, and so on) of each separate equation. Instead, performing
    ensemble on decision trees is most popular in the data science community, similar
    to bagging, random forest, and boosting, which we will be covering in depth in
    later parts of this chapter. Ensemble techniques tackle variance problems by aggregating
    the results from highly variable individual classifiers such as decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: HR attrition data example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be using IBM Watson''s HR Attrition data (the data
    has been utilized in the book after taking prior permission from the data administrator)
    shared in Kaggle datasets under open source license agreement [https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset) to
    predict whether employees would attrite or not based on independent explanatory
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are about 1470 observations and 35 variables in this data, the top five
    rows are shown here for a quick glance of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c3e832c-2172-439c-809a-5e3b39697062.png)![](img/a5fced1b-56eb-4b4d-a8e5-a49da3e29e3f.png)![](img/51cfc8bd-f18c-4cca-8a57-b2a87239a318.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code is used to convert Yes or No categories into 1 and 0 for
    modeling purposes, as scikit-learn does not fit the model on character/categorical
    variables directly, hence dummy coding is required to be performed for utilizing
    the variables in models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Dummy variables are created for all seven categorical variables (shown here
    in alphabetical order), which are `Business Travel`, `Department`, `Education
    Field`, `Gender`, `Job Role`, `Marital Status`, and `Overtime`. We have ignored
    four variables from the analysis, as they do not change across the observations,
    which are `Employee count`, `Employee number`, `Over18`, and `Standard Hours`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Continuous variables are separated and will be combined with the created dummy
    variables later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following step, both derived dummy variables from categorical variables
    and straight continuous variables are combined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have not removed one extra derived dummy variable for each categorical
    variable due to the reason that multi-collinearity does not create a problem in
    decision trees as it does in either logistic or linear regression, hence we can
    simply utilize all the derived variables in the rest of the chapter, as all the
    models utilize decision trees as an underlying model, even after performing ensembles
    of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once basic data has been prepared, it needs to be split by 70-30 for training
    and testing purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'R Code for Data Preprocessing on HR Attrition Data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Decision tree classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `DecisionTtreeClassifier` from scikit-learn has been utilized for modeling
    purposes, which is available in the `tree` submodule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters selected for the DT classifier are in the following code with
    splitting criterion as Gini, Maximum depth as 5, the minimum number of observations
    required for qualifying split is 2, and the minimum samples that should be present
    in the terminal node is 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/038fabe1-a81c-4dff-8be0-040bbd1e4e2b.png)'
  prefs: []
  type: TYPE_IMG
- en: By carefully observing the results, we can infer that, even though the test
    accuracy is high (84.6%), the precision and recall of one category (*Attrition
    = Yes*) is low (*precision = 0.39* and *recall = 0.20*). This could be a serious
    issue when management tries to use this model to provide some extra benefits proactively
    to the employees with a high chance of attrition prior to actual attrition, as
    this model is unable to identify the real employees who will be leaving. Hence,
    we need to look for other modifications; one way is to control the model by using
    class weights. By utilizing class weights, we can increase the importance of a
    particular class at the cost of an increase in other errors.
  prefs: []
  type: TYPE_NORMAL
- en: For example, by increasing class weight to category *1*, we can identify more
    employees with the characteristics of actual attrition, but by doing so, we will
    mark some of the non-potential churner employees as potential attriters (which
    should be acceptable).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another classic example of the important use of class weights is, in banking
    scenarios. When giving loans, it is better to reject some good applications than
    accepting bad loans. Hence, even in this case, it is a better idea to use higher
    weight to defaulters over non-defaulters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'R Code for Decision Tree Classifier Applied on HR Attrition Data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Tuning class weights in decision tree classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following code, class weights are tuned to see the performance change
    in decision trees with the same parameters. A dummy DataFrame is created to save
    all the results of various precision-recall details of combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Metrics to be considered for capture are weight for zero and one category (for
    example, if the weight for zero category given is 0.2, then automatically, weight
    for the one should be 0.8, as total weight should be equal to 1), training and
    testing accuracy, precision for zero category, one category, and overall. Similarly,
    recall for zero category, one category, and overall are also calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Weights for the zero category are verified from 0.01 to 0.5, as we know we
    do not want to explore cases where the zero category will be given higher weightage
    than one category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6d71e76b-50e7-4fcb-93b3-7eacb2a96bc0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding screenshot, we can see that at class weight values of 0.3
    (for zero) and 0.7 (for one) it is identifying a higher number of attriters (25
    out of 61) without compromising test accuracy 83.9% using decision trees methodology:'
  prefs: []
  type: TYPE_NORMAL
- en: 'R Code for Decision Tree Classifier with class weights Applied on HR Attrition
    Data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Bagging classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have discussed already, decision trees suffer from high variance, which
    means if we split the training data into two random parts separately and fit two
    decision trees for each sample, the rules obtained would be very different. Whereas
    low variance and high bias models, such as linear or logistic regression, will
    produce similar results across both samples. Bagging refers to bootstrap aggregation
    (repeated sampling with replacement and perform aggregation of results to be precise),
    which is a general purpose methodology to reduce the variance of models. In this
    case, they are decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aggregation reduces the variance, for example, when we have n independent observations
    *x[1], x[2 ],..., x[n]* each with variance *σ²* and the variance of the mean *x̅* of
    the observations is given by *σ²/n*, which illustrates by averaging a set of observations
    that it reduces variance. Here, we are reducing variance by taking many samples
    from training data (also known as bootstrapping), building a separate decision
    tree on each sample separately, averaging the predictions for regression, and
    calculating mode for classification problems in order to obtain a single low-variance
    model that will have both low bias and low variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0e4f294-1efc-45f2-bcfa-d32a18f025d0.jpg)![](img/5b837eae-9191-42fa-ae93-57220340d1da.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In a bagging procedure, rows are sampled while selecting all the columns/variables
    (whereas, in a random forest, both rows and columns would be sampled, which we
    will cover in the next section) and fitting individual trees for each sample.
    In the following diagram, two colors (pink and blue) represent two samples, and
    for each sample, a few rows are sampled, but all the columns (variables) are selected
    every time. One issue that exists due to the selection of all columns is that
    most of the trees will describe the same story, in which the most important variable
    will appear initially in the split, and this repeats in all the trees, which will
    not produce de-correlated trees, so we may not get better performance when applying
    variance reduction. This issue will be avoided in random forest (we will cover
    this in the next section of the chapter), in which we will sample both rows and
    columns as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3dd53ad-e3cd-45c5-81a2-dfe75cec6ffe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following code, the same HR data has been used to fit the bagging classifier
    in order to compare the results apple to apple with respect to decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The base classifier used here is Decision Trees with the same parameter setting
    that we used in the decision tree example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters used in bagging are, `n_estimators` to represent the number of individual
    decision trees used as 5,000, maximum samples and features selected are 0.67 and
    1.0 respectively, which means it will select 2/3^(rd) of observations for each
    tree and all the features. For further details, please refer to the scikit-learn
    manual [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9e97501c-06fe-4fc4-b346-0e0fa32889c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After analyzing the results from bagging, the test accuracy obtained was 87.3%,
    whereas for decision tree it was 84.6%. Comparing the number of actual attrited
    employees identified, there were 13 in bagging, whereas in decision tree there
    were 12, but the number of 0 classified as 1 significantly reduced to 8 compared
    with 19 in DT. Overall, bagging improves performance over the single tree:'
  prefs: []
  type: TYPE_NORMAL
- en: 'R Code for Bagging Classifier Applied on HR Attrition Data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Random forest classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests provide an improvement over bagging by doing a small tweak that
    utilizes de-correlated trees. In bagging, we build a number of decision trees
    on bootstrapped samples from training data, but the one big drawback with the
    bagging technique is that it selects all the variables. By doing so, in each decision
    tree, the order of candidate/variable chosen to split remains more or less the
    same for all the individual trees, which look correlated with each other. Variance
    reduction on correlated individual entities does not work effectively while aggregating
    them.
  prefs: []
  type: TYPE_NORMAL
- en: In random forest, during bootstrapping (repeated sampling with replacement),
    samples were drawn from training data; not just simply the second and third observations
    randomly selected, similar to bagging, but it also selects the few predictors/columns
    out of all predictors (m predictors out of total p predictors).
  prefs: []
  type: TYPE_NORMAL
- en: The thumb rule for variable selection of m variables out of total variables
    *p* is *m = sqrt(p)* for classification and *m = p/3* for regression problems
    randomly to avoid correlation among the individual trees. By doing so, significant
    improvement in the accuracy can be achieved. This ability of RF makes it one of
    the favorite algorithms used by the data science community, as a winning recipe
    across various competitions or even for solving practical problems in various
    industries.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, different colors represent different bootstrap samples.
    In the first sample, the 1^(st), 3^(rd), 4^(th,) and 7^(th) columns are selected,
    whereas, in the second bootstrap sample, the 2^(nd), 3^(rd), 4^(th,) and 5^(th)
    columns are selected respectively. In this way, any columns can be selected at
    random, whether they are adjacent to each other or not. Though the thumb rules
    of *sqrt (p)* or *p/3* are given, readers are encouraged to tune the number of
    predictors to be selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f1f70f5-eeea-43f7-9e8b-4751f7cd44e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sample plot shows the impact of a test error change while changing the
    parameters selected, and it is apparent that a *m = sqrt(p)* scenario gives better
    performance on test data compared with *m =p* (we can call this scenario bagging):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ab1c7e8-50f3-451c-b1cc-d21901c3ceb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Random forest classifier has been utilized from the `scikit-learn` package
    here for illustration purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters used in random forest are: `n_estimators` representing the number
    of individual decision trees used is 5000, maximum features selected are *auto*,
    which means it will select *sqrt(p)* for classification and *p/3* for regression
    automatically. Here is the straightforward classification problem though. Minimum
    samples per leaf provide the minimum number of observations required in the terminal
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e668098b-6e9d-45f6-90be-9fc73710f1a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Random forest classifier produced 87.8% test accuracy compared with bagging
    87.3%, and also identifies 14 actually attrited employees in contrast with bagging,
    for which 13 attrited employees have been identified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/dcd3b9c0-2961-4cb9-acdc-e29dc119fcaa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the variable importance plot, it seems that the monthly income variable
    seems to be most significant, followed by overtime, total working years, stock
    option levels, years at company, and so on. This provides us with some insight
    into what are major contributing factors that determine whether the employee will
    remain with the company or leave the organization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'R Code for Random Forest Classifier Applied on HR Attrition Data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Random forest classifier - grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tuning parameters in a machine learning model play a critical role. Here, we
    are showing a grid search example on how to tune a random forest model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Tuning parameters are similar to random forest parameters apart from verifying
    all the combinations using the pipeline function. The number of combinations to
    be evaluated will be *(3 x 3 x 2 x 2) *5 =36*5 = 180* combinations. Here 5 is
    used in the end, due to the cross-validation of five-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/544ea2c9-c18a-45a9-80bf-246993602bf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding results, grid search seems to not provide many advantages
    compared with the already explored random forest result. But, practically, most
    of the times, it will provide better and more robust results compared with a simple
    exploration of models. However, by carefully evaluating many different combinations,
    it will eventually discover the best parameters combination:'
  prefs: []
  type: TYPE_NORMAL
- en: 'R Code for random forest classifier with grid search applied on HR attrition
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: AdaBoost classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Boosting is another state-of-the-art model that is being used by many data
    scientists to win so many competitions. In this section, we will be covering the
    **AdaBoost** algorithm, followed by **gradient boost** and **extreme gradient
    boost** (**XGBoost**). Boosting is a general approach that can be applied to many
    statistical models. However, in this book, we will be discussing the application
    of boosting in the context of decision trees. In bagging, we have taken multiple
    samples from the training data and then combined the results of individual trees
    to create a single predictive model; this method runs in parallel, as each bootstrap
    sample does not depend on others. Boosting works in a sequential manner and does
    not involve bootstrap sampling; instead, each tree is fitted on a modified version
    of an original dataset and finally added up to create a strong classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8dbad23-3a24-4337-a7f2-46d70806d109.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/d67d7eae-1176-4fe6-8a40-00c74a987933.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure is the sample methodology on how AdaBoost works. We will
    cover step-by-step procedures in detail in the following algorithm description.
    Initially, a simple classifier has been fitted on the data (also called a decision
    stump, which splits the data into just two regions) and whatever the classes correctly
    classified will be given less weightage in the next iteration (iteration 2) and
    higher weightage for misclassified classes (observer + blue icons), and again
    another decision stump/weak classifier will be fitted on the data and will change
    the weights again for the next iteration (iteration 3, here check the - symbols
    for which weight has been increased). Once it finishes the iterations, these are
    combined with weights (weights automatically calculated for each classifier at
    each iteration based on error rate) to come up with a strong classifier, which
    predicts the classes with surprising accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm for AdaBoost consists of the following steps:**'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the observation weights *w[i] = 1/N, i=1, 2, …, N*. Where *N = Number
    of observations.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For m = 1 to M:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a classifier *Gm(x)* to the training data using weights *w[i]*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compute:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/cf136375-279e-47a3-9584-fd97f3841f66.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Compute:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9d970c77-ab57-497e-aaa9-10feed4790c5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Set:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/10e696b5-b9c1-4029-8e6e-3a679166bfaf.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/af921fd3-a852-4c86-b988-abc62a45c382.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All the observations are given equal weight.
  prefs: []
  type: TYPE_NORMAL
- en: In bagging and random forest algorithms, we deal with the columns of the data;
    whereas, in boosting, we adjust the weights of each observation and don't elect
    a few columns.
  prefs: []
  type: TYPE_NORMAL
- en: We fit a classifier on the data and evaluate overall errors. The error used
    for calculating weight should be given for that classifier in the final additive
    model (**α**) evaluation. The intuitive sense is that the higher weight will be
    given for the model with fewer errors. Finally, weights for each observation will
    be updated. Here, weight will be increased for incorrectly classified observations
    in order to give more focus to the next iterations, and weights will be reduced
    for correctly classified observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the weak classifiers are combined with their respective weights to form
    a strong classifier. In the following figure, a quick idea is shared on how weights
    changed in the last iteration compared with the initial iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/154913f0-cfe9-41dc-b813-cecc75c0a0fa.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Decision stump is used as a base classifier for AdaBoost. If we observe the
    following code, the depth of the tree remains as 1, which has decision taking
    ability only once (also considered a weak classifier):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In AdaBoost, decision stump has been used as a base estimator to fit on whole
    datasets and then fits additional copies of the classifier on the same dataset
    up to 5000 times. The learning rate shrinks the contribution of each classifier
    by 0.05\. There is a trade-off between the learning rate and number of estimators.
    By carefully choosing a low learning rate and a long number of estimators, one
    can converge optimum very much, however at the expense of computing power:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2da58a78-f500-42c4-b9d3-e2c77f56cfb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The result of the AdaBoost seems to be much better than the known best random
    forest classifiers in terms of the recall of 1 value. Though there is a slight
    decrease in accuracy to 86.8% compared with the best accuracy of 87.8%, the number
    of 1''s predicted is 23 from the RF, which is 14 with some expense of an increase
    in 0''s, but it really made good progress in terms of identifying actual attriters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'R Code for AdaBoost classifier applied on HR attrition data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Gradient boosting classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosting is one of the competition-winning algorithms that work on
    the principle of boosting weak learners iteratively by shifting focus towards
    problematic observations that were difficult to predict in previous iterations
    and performing an ensemble of weak learners, typically decision trees. It builds
    the model in a stage-wise fashion as other boosting methods do, but it generalizes
    them by allowing optimization of an arbitrary differentiable loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start understanding Gradient Boosting with a simple example, as GB challenges
    many data scientists in terms of understanding the working principle:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, we fit the model on observations producing 75% accuracy and the
    remaining unexplained variance is captured in the ***error*** term:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6cc1f87f-915a-4141-baa9-b7b829f34fe5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we will fit another model on the error term to pull the extra explanatory
    component and add it to the original model, which should improve the overall accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2d7f49d3-84b7-41ec-8314-42b706ff263d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, the model is providing 80% accuracy and the equation looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/635f7efa-5e62-4cc8-aa89-898d720a2556.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We continue this method one more time to fit a model on the **error2** component
    to extract a further explanatory component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4b7f8e8d-a0aa-4a58-9e8f-42948e8b620c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, model accuracy is further improved to 85% and the final model equation
    looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/749e5e90-0a8a-4e88-9292-4abfb987f360.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, if we use weighted average (higher importance given to better models that
    predict results with greater accuracy than others) rather than simple addition,
    it will improve the results further. In fact, this is what the gradient boosting
    algorithm does!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/558d6b99-dc6b-4627-a674-e61e99137f73.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After incorporating weights, the name of the error changed from **error3** to
    **error4**, as both errors may not be exactly the same. If we find better weights,
    we will probably get an accuracy of 90% instead of simple addition, where we have
    only got 85%.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient boosting involves three elements:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss function to be optimized:** Loss function depends on the type of problem
    being solved. In the case of regression problems, mean squared error is used,
    and in classification problems, the logarithmic loss will be used. In boosting,
    at each stage, unexplained loss from prior iterations will be optimized rather
    than starting from scratch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weak learner to make predictions:** Decision trees are used as a weak learner
    in gradient boosting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additive model to add weak learners to minimize the loss function:** Trees
    are added one at a time and existing trees in the model are not changed. The gradient
    descent procedure is used to minimize the loss when adding trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The algorithm for Gradient boosting consists of the following steps:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2ddc97ad-e0da-44a7-b79d-c449607be372.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For *m = 1* to M:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) For *i = 1, 2, …, N* compute:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/95586651-da5d-4724-aa5b-66e0d87c8136.jpg)'
  prefs: []
  type: TYPE_IMG
- en: b) Fit a regression tree to the targets r[im] giving terminal regions R[jm],
    j = 1, 2, …, J[m],
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'c) For j = 1, 2, …, J[m], compute:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7c070b8d-1742-4565-9c69-c730b94ae07d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'd) Update:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f3a737c5-daa7-4843-bfa9-93a634fc461b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c8ab0b80-4bb0-4e32-a52f-7a8a6d311fe1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Initializes the constant optimal constant model, which is just a single terminal
    node that will be utilized as a starting point to tune it further in the next
    steps. *(2a)*, calculates the residuals/errors by comparing actual outcome with
    predicted results, followed by (*2b* and *2c*) in which the next decision tree
    will be fitted on error terms to bring in more explanatory power to the model,
    and in (*2d*) add the extra component to the model at last iteration. Finally,
    ensemble all weak learners to create a strong learner.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison between AdaBoosting versus gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After understanding both AdaBoost and gradient boost, readers may be curious
    to see the differences in detail. Here, we are presenting exactly that to quench
    your thirst!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ca41b37-732c-4c61-8225-0bc5962acab3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The gradient boosting classifier from the scikit-learn package has been used
    for computation here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Parameters used in the gradient boosting algorithms are as follows. Deviance
    has been used for loss, as the problem we are trying to solve is 0/1 binary classification.
    The learning rate has been chosen as 0.05, number of trees to build is 5000 trees,
    minimum sample per leaf/terminal node is 1, and minimum samples needed in a bucket
    for qualification for splitting is 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/96bd6b48-620a-4932-ba93-231c6acff064.png)'
  prefs: []
  type: TYPE_IMG
- en: If we analyze the results, Gradient boosting has given better results than AdaBoost
    with the highest possible test accuracy of 87.5% with most 1's captured as 24,
    compared with AdaBoost with which the test accuracy obtained was 86.8%. Hence,
    it has been proven that it is no wonder why every data scientist tries to use
    this algorithm to win competitions!
  prefs: []
  type: TYPE_NORMAL
- en: 'The R code for gradient boosting classifier applied on HR attrition data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Extreme gradient boosting - XGBoost classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'XGBoost is the new algorithm developed in 2014 by *Tianqi Chen* based on the
    Gradient boosting principles. It has created a storm in the data science community
    since its inception. XGBoost has been developed with both deep consideration in
    terms of system optimization and principles in machine learning. The goal of the
    library is to push the extremes of the computation limits of machines to provide
    scalable, portable, and accurate results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9c1aa18d-2475-4698-8275-1048a2e9405a.png)'
  prefs: []
  type: TYPE_IMG
- en: The results obtained from **XGBoost** are almost similar to gradient boosting.
    The test accuracy obtained was 87.1%, whereas boosting got 87.5%, and also the
    number of 1's identified is 23 compared with 24 in gradient boosting. The greatest
    advantage of XGBoost over Gradient boost is in terms of performance and the options
    available to control model tune. By changing a few of them, makes XGBoost even
    beat gradient boost as well!
  prefs: []
  type: TYPE_NORMAL
- en: 'The R code for xtreme gradient boosting (XGBoost) classifier applied on HR
    attrition data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Ensemble of ensembles - model stacking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ensemble of ensembles or model stacking is a method to combine different classifiers
    into a meta-classifier that has a better generalization performance than each
    individual classifier in isolation. It is always advisable to take opinions from
    many people when you are in doubt, when dealing with problems in your personal
    life too! There are two ways to perform ensembles on models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensemble with different types of classifiers:** In this methodology, different
    types of classifiers (for example, logistic regression, decision trees, random
    forest, and so on) are fitted on the same training data and results are combined
    based on either majority voting or average, based on if it is classification or
    regression problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble with a single type of classifiers, but built separately on various
    bootstrap samples:** In this methodology, bootstrap samples are drawn from training
    data and, each time, separate models will be fitted (individual models could be
    decision trees, random forest, and so on) on the drawn sample, and all these results
    are combined at the end to create an ensemble. This method suits dealing with
    highly flexible models where variance reduction still improves performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble of ensembles with different types of classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As briefly mentioned in the preceding section, different classifiers will be
    applied on the same training data and the results ensembled either taking majority
    voting or applying another classifier (also known as a meta-classifier) fitted
    on results obtained from individual classifiers. This means, for meta-classifier
    *X*, variables would be model outputs and Y variable would be an actual 0/1 result.
    By doing this, we will obtain the weightage that should be given for each classifier
    and those weights will be applied accordingly to classify unseen observations.
    All three methods of application of ensemble of ensembles are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Majority voting or average:** In this method, a simple mode function (classification
    problem) is applied to select the category with the major number of appearances
    out of individual classifiers. Whereas, for regression problems, an average will
    be calculated to compare against actual values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method of application of meta-classifiers on outcomes:** Predict actual outcome
    either 0 or 1 from individual classifiers and apply a meta-classifier on top of
    0''s and 1''s. A small problem with this type of approach is that the meta-classifier
    will be a bit brittle and rigid. I mean 0''s and 1''s just gives the result, rather
    than providing exact sensibility (such as probability).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method of application of meta-classifiers on probabilities:** In this method,
    probabilities are obtained from individual classifiers instead of 0''s and 1''s.
    Applying meta-classifier on probabilities makes this method a bit more flexible
    than the first method. Though users can experiment with both methods to see which
    one performs better. After all, machine learning is all about exploration and
    trial and error methodologies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following diagram, the complete flow of model stacking has been described
    with various stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4230cd2-6bd4-49ff-b898-787f83c39c09.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Steps in the following ensemble with multiple classifiers example**:'
  prefs: []
  type: TYPE_NORMAL
- en: Four classifiers have been used separately on training data (logistic regression,
    decision tree, random forest, and AdaBoost)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probabilities have been determined for all four classifiers, however, only the
    probability for category 1 has been utilized in meta-classifier due to the reason
    that the probability of class 0 + probability of class 1 = 1, hence only one probability
    is good enough to represent, or else multi-collinearity issues appearing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression has been used as a meta-classifier to model the relationship
    between four probabilities (obtained from each individual classifier) with respect
    to a final 0/1 outcome
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coefficients have been calculated for all four variables used in meta-classifier
    and applied on new data for calculating the final aggregated probability for classifying
    observations into the respective categories:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following step, we perform an ensemble of classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In the following step, we take probabilities only for category 1, as it gives
    intuitive sense for high probability and indicates the value towards higher class
    1\. But this should not stop someone if they really want to fit probabilities
    on a 0 class instead. In that case, low probability values are preferred for category
    1, which gives us a little bit of a headache!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/403279b7-d323-47be-bbdd-6db25232febb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Though code prints **Train**, **Test accuracies**, **Confusion Matrix**, and
    **Classification Reports**, we have not shown them here due to space constraints.
    Users are advised to run and check the results on their computers. Test accuracy
    came as *87.5%*, which is the highest value (the same as gradient boosting results).
    However, by careful tuning, ensembles do give much better results based on adding
    better models and removing models with low weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e972efd9-6953-40fa-bba5-075c99e48b1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It seems that, surprisingly, AdaBoost is dragging down performance of the ensemble.
    A tip is to either change the parameters used in AdaBoost and rerun the entire
    exercise, or remove the AdaBoost classifier from the ensemble and rerun the ensemble
    step to see if there is any improvement in ensemble test accuracy, precision,
    and recall values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'R Code for Ensemble of Ensembles with different Classifiers Applied on HR Attrition
    Data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Ensemble of ensembles with bootstrap samples using a single type of classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this methodology, bootstrap samples are drawn from training data and, each
    time, separate models will be fitted (individual models could be decision trees,
    random forest, and so on) on the drawn sample, and all these results are combined
    at the end to create an ensemble. This method suits dealing with highly flexible
    models where variance reduction will still improve performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7349014a-3a2e-47b1-a9d7-5209f4ee1256.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following example, AdaBoost is used as a base classifier and the results
    of individual AdaBoost models are combined using the bagging classifier to generate
    final outcomes. Nonetheless, each AdaBoost is made up of decision trees with a
    depth of 1 (decision stumps). Here, we would like to show that classifier inside
    classifier inside classifier is possible (sounds like the Inception movie though!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the base classifier (decision stump) used in the AdaBoost
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Each AdaBoost classifier consists of 500 decision trees with a learning rate
    of 0.05:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The bagging classifier consists of 50 AdaBoost classifiers to ensemble the
    ensembles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cc16a73e-c924-4552-b7f6-5ffa61b480bd.png)'
  prefs: []
  type: TYPE_IMG
- en: The results of the ensemble on AdaBoost have shown some improvements, in which
    the test accuracy obtained is 87.1%, which is almost to that of gradient boosting
    at 87.5%, which is the best value we have seen so far. However, the number of
    1's identified is 25 here, which is greater than Gradient Boosting. Hence, it
    has been proven that an ensemble of ensembles does work! Unfortunately, these
    types of functions are not available in R software, hence we are not writing the
    equivalent R-code here.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned the complete details about tree-based models,
    which are currently the most used in the industry, including individual decision
    trees with grid search and an ensemble of trees such as bagging, random forest,
    boosting (including AdaBoost, gradient boost and XGBoost), and finally, ensemble
    of ensembles, also known as model stacking, for further improving accuracy by
    reducing variance errors by aggregating results further. In model stacking, you
    have learned how to determine the weights for each model, so that decisions can
    be made as to which model to keep in the final results to obtain the best possible
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will be learning k-nearest neighbors and Naive Bayes,
    which are less computationally intensive than tree-based models. The Naive Bayes
    model will be explained with an NLP use case. In fact, Naive Bayes and SVM are
    often used where variables (number of dimensions) are very high in number to classify.
  prefs: []
  type: TYPE_NORMAL
