<html><head></head><body>
  <div id="_idContainer630" class="Basic-Text-Frame">
    <h1 class="chapterNumber">14</h1>
    <h1 id="_idParaDest-298" class="chapterTitle">Attention and Transformers for Time Series</h1>
    <p class="normal">In the previous chapter, we rolled up our sleeves and implemented a few <strong class="keyWord">deep learning</strong> (<strong class="keyWord">DL</strong>) systems for time series forecasting. We used the common building blocks we discussed in <em class="chapterRef">Chapter 12</em>, <em class="italic">Building Blocks of Deep Learning for Time Series</em>, put them together in an encoder-decoder architecture, and trained them to produce the forecast we desired.</p>
    <p class="normal">Now, let’s talk about another key concept in DL that has taken the field by storm over the past few years—<strong class="keyWord">attention</strong>. Attention<a id="_idIndexMarker1075"/> has a long-standing history, which has culminated in it being one of the most sought-after tools in the DL toolkit. This chapter takes you on a journey to understand attention and transformer models from the ground up from a theoretical perspective and solidify that understanding with practical examples.</p>
    <p class="normal">In this chapter, we will be covering these main topics:</p>
    <ul>
      <li class="bulletList">What is attention?</li>
      <li class="bulletList">Generalized attention model</li>
      <li class="bulletList">Forecasting with sequence-to-sequence models and attention</li>
      <li class="bulletList">Transformers—Attention is all you need</li>
      <li class="bulletList">Forecasting with Transformers</li>
    </ul>
    <h1 id="_idParaDest-299" class="heading-1">Technical requirements</h1>
    <p class="normal">You will need to set up the <strong class="keyWord">Anaconda</strong> environment by following the instructions in the <em class="italic">Preface</em> of the book to get a working environment with all the libraries and datasets required for the code in this book. Any additional libraries will be installed while running the notebooks.</p>
    <p class="normal">You need to run the following notebooks for this chapter:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">02-Preprocessing_London_Smart_Meter_Dataset.ipynb</code> in <code class="inlineCode">Chapter02</code></li>
      <li class="bulletList"><code class="inlineCode">01-Setting_up_Experiment_Harness.ipynb</code> in <code class="inlineCode">Chapter04</code></li>
      <li class="bulletList"><code class="inlineCode">01-Feature_Engineering.ipynb</code> in <code class="inlineCode">Chapter06</code></li>
      <li class="bulletList"><code class="inlineCode">02-One-Step_RNN.ipynb</code> and <code class="inlineCode">03-Seq2Seq_RNN.ipynb</code> in <code class="inlineCode">Chapter13</code> (for benchmarking)</li>
      <li class="bulletList"><code class="inlineCode">00-Single_Step_Backtesting_Baselines.ipynb</code> and <code class="inlineCode">01-Forecasting_with_ML.ipynb</code> in <code class="inlineCode">Chapter08</code></li>
    </ul>
    <p class="normal">The associated code for the chapter can be found at <a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter14"><span class="url">https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-/tree/main/notebooks/Chapter14</span></a>.</p>
    <h1 id="_idParaDest-300" class="heading-1">What is attention?</h1>
    <p class="normal">The idea of attention<a id="_idIndexMarker1076"/> was inspired by human cognitive function. At any moment, the optic nerves in our eyes, the olfactory nerves in our noses, and the auditory nerves in our ears send a massive amount of sensory input to the brain. This is way too much information, definitely more than the brain can handle. But our brains have developed a mechanism that helps us to pay <em class="italic">attention</em> to only the stimuli that matter—such as a sound or a smell that doesn’t belong. Years of evolution have <em class="italic">trained</em> our brains to pick out anomalous sounds or smells because that was key for us surviving in the wild, where predators roamed free. In cognitive science, attention is defined as the cognitive process that allows an individual to selectively focus on specific information while ignoring other irrelevant stimuli.</p>
    <p class="normal">Apart from this kind of instinctive attention, we are also able to control our attention by what we call <em class="italic">focusing</em> on something. You are doing it right now by choosing to ignore all the other stimuli that you are getting and focusing your attention on the contents of this book. While you are reading, your mobile phone pings you, the screen lights up, and your brain decides to focus its attention on the mobile screen, even though the book is still open in front of you. This feature of the human cognitive function has been the inspiration behind the attention mechanism in DL. Giving learning machines the ability to acquire this kind of attention has led to big breakthroughs in all fields of AI today.</p>
    <p class="normal">The idea was<a id="_idIndexMarker1077"/> first applied to DL in Seq2Seq models, which we learned about in <em class="italic">Chapter 13</em>, <em class="italic">Common Modeling Patterns for Time Series</em>. In that chapter, we saw how the handshake between the encoder and decoder was done. For the <strong class="keyWord">recurrent neural network</strong> (<strong class="keyWord">RNN</strong>) family <a id="_idIndexMarker1078"/>of models, we use the hidden states from the encoder at the end of the sequence as the initial hidden states in the decoder. Let’s call this handshake the <strong class="keyWord">context</strong>. The<a id="_idIndexMarker1079"/> assumption here is that all the information required for the decoding task is encoded in the context and this is done in a timestep-by-timestep manner. So, for long context windows, the information from the first timestep has to be retained through multiple writes and re-writes until it’s used in the last timestep. This creates a kind of information bottleneck (<em class="italic">Figure 14.1</em>), where the model may struggle to retain important information through this limited context. There may be information in previous hidden states that can be useful for the decoding task. In 2015, Bahdanau et al. (Reference <em class="italic">1</em>) proposed the first known attention model in the context of DL. They proposed to learn attention weights, <img src="../Images/B22389_14_001.png" alt=""/>, for each hidden state corresponding to the input sequence and combine them into a single context vector while decoding. </p>
    <p class="normal">These attention weights are re-calculated for each decoding step using the similarity between the hidden states during decoding and all the hidden states in the input sequence (<em class="italic">Figure 14.2</em>):</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_01.png" alt="Figure 14.1 – Traditional (top) versus attention model (bottom) in Seq2Seq models "/></figure>
    <p class="packt_figref">Figure 14.1: Traditional (top) versus attention model (bottom) in Seq2Seq models</p>
    <p class="normal">To make<a id="_idIndexMarker1080"/> things clearer, let’s adopt a formal way of describing the mechanism. Let <img src="../Images/B22389_14_002.png" alt=""/> be the hidden states generated during the encoding process and <img src="../Images/B22389_14_003.png" alt=""/> be the hidden states generated during decoding. The context vector will be <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">j</sub>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_02.png" alt="Figure 14.2 – Decoding using attention "/></figure>
    <p class="packt_figref">Figure 14.2: Decoding using attention</p>
    <p class="normal">So, now we have <a id="_idIndexMarker1081"/>the hidden states from the encoding stage (<em class="italic">H</em>), and we need to have a way to use this information in each step of decoding. The key here is that in each step of the decoding process, information from different hidden states might be relevant. This is exactly what attention weights do. So, for decoding step <em class="italic">j</em>, we use <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">j</sub><sub class="subscript">-1</sub> and calculate attention weights (we’ll look at how attention weights are learned in detail soon), <em class="italic">a</em><sub class="subscript">i</sub>, <em class="italic">j</em>, using the similarity between <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">j</sub><sub class="subscript">-1</sub> and each hidden state in <em class="italic">H</em>. Now, we calculate the context vector, which combines the information in <em class="italic">H</em> in the right way:</p>
    <p class="center"><img src="../Images/B22389_14_004.png" alt=""/></p>
    <p class="normal">There are two main ways we can use this context vector, which we will look at in more detail later in the chapter. This breaks the information bottleneck that was present in the traditional Seq2Seq model and allows the models to access a larger pool of information and decide which information is relevant at each step of the decoding process.</p>
    <p class="normal">Now, let’s see how these attention weights, <img src="../Images/B22389_04_009.png" alt=""/>, are calculated.</p>
    <h1 id="_idParaDest-301" class="heading-1">The generalized attention model</h1>
    <p class="normal">Over <a id="_idIndexMarker1082"/>the course of years, researchers have come up with different ways of calculating attention weights and using attention in DL models. Sneha Chaudhari et al. (Reference <em class="italic">8</em>) published a survey paper on attention models that proposes a generalized attention model that tries to incorporate all the variations in a single framework. Let’s structure our discussion around this generalized framework.</p>
    <p class="normal">We can think of an attention model as learning an attention distribution (<img src="../Images/B22389_04_009.png" alt=""/>) for a set of keys, <em class="italic">K</em>, using a set of queries, <em class="italic">q</em>. In the example we discussed in the last section, the query would be <em class="italic">S</em><sub class="subscript-italic" style="font-style: italic;">j</sub><sub class="subscript">-1</sub>—the hidden state from the last timestep during decoding—and the keys would be <em class="italic">H</em>—all the hidden states generated using the input sequence. In some cases, the generated attention distribution is applied to another set of inputs called values, <em class="italic">V</em>. In many cases, <em class="italic">K</em> and <em class="italic">V</em> are the same, but to maintain the general form of the framework, we consider these separately. Using this terminology, we can define an attention model as a function of <em class="italic">q</em>, <em class="italic">K</em>, and <em class="italic">V</em>:</p>
    <p class="center"><img src="../Images/B22389_14_007.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">a</em> is <a id="_idIndexMarker1083"/>an <strong class="keyWord">alignment function</strong> that calculates a similarity or a notion of similarity between the query (<em class="italic">q</em>) and keys (<em class="italic">k</em><sub class="subscript">i</sub>), and <em class="italic">v</em><sub class="subscript">i</sub> is the corresponding value for index <img src="../Images/B22389_14_008.png" alt=""/>. In the example we discussed in the previous section, this alignment function calculates how relevant an encoder hidden state is to a decoder hidden<a id="_idIndexMarker1084"/> state, and <em class="italic">p</em> is a <strong class="keyWord">distribution function</strong> that converts this score into attention weights that sum up to 1.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check:</strong></p>
      <p class="normal">The research papers by Sneha Choudhari et al. are cited in the <em class="italic">References</em> section as reference <em class="italic">8</em>.</p>
    </div>
    <p class="normal">Now that we <a id="_idIndexMarker1085"/>have the generalized attention model, let’s also see how we can implement this in PyTorch. The full implementation can be found in the <code class="inlineCode">Attention</code> class in <code class="inlineCode">src/dl/attention.py</code>, but we will cover the key parts of it here.</p>
    <p class="normal">The only information we require beforehand to initialize such a module is the hidden dimension of the queries and keys (encoder and decoder). So, the class definition and the <code class="inlineCode">__init__</code> function of the class look like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Attention</span>(nn.Module, metaclass=ABCMeta):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, encoder_dim: </span><span class="hljs-built_in">int</span><span class="hljs-params">, decoder_dim: </span><span class="hljs-built_in">int</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable">self</span>.encoder_dim = encoder_dim
        <span class="hljs-variable">self</span>.decoder_dim = decoder_dim
</code></pre>
    <p class="normal">Now, we need to define a <code class="inlineCode">forward</code> function, which takes in two inputs:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">query</code>: The query vector of size (<em class="italic">batch size</em>, <em class="italic">decoder dimension</em>), which we are going to use to find the attention weights with which to combine the keys. This is the <em class="italic">q</em> in <em class="italic">A</em>(<em class="italic">q</em>, <em class="italic">K</em>, <em class="italic">V</em>).</li>
      <li class="bulletList"><code class="inlineCode">key</code>: The key vector of size (<em class="italic">batch size</em>, <em class="italic">sequence length</em>, <em class="italic">encoder dimension</em>), which is the sequence of hidden states across which we will be calculating the attention weights. This is the <em class="italic">K</em> in <em class="italic">A</em>(<em class="italic">q</em>, <em class="italic">K</em>, <em class="italic">V</em>).</li>
    </ul>
    <p class="normal">We are assuming keys and values are the same because, in most cases, they are. So, from the generalized attention model, we know that there are a few steps we need to perform:</p>
    <ol>
      <li class="numberedList" value="1">Calculate an alignment score—<em class="italic">a</em>(<em class="italic">k</em><sub class="subscript-italic" style="font-style: italic;">i</sub>, <em class="italic">q</em>)—for each query and key combination.</li>
      <li class="numberedList">Convert the scores to weights by applying a function—<em class="italic">p</em>(<em class="italic">a</em>(<em class="italic">k</em><sub class="subscript-italic" style="font-style: italic;">i</sub>, <em class="italic">q</em>)).</li>
      <li class="numberedList">Use the learned weights to combine the values—<img src="../Images/B22389_14_009.png" alt=""/>.</li>
    </ol>
    <p class="normal">So, let’s see those steps in code in the <code class="inlineCode">forward</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(
<span class="hljs-params">        self,</span>
<span class="hljs-params">        query: torch.Tensor,  </span><span class="hljs-comment"># [batch_size, decoder_dim]</span>
<span class="hljs-params">        values: torch.Tensor,  </span><span class="hljs-comment"># [batch_size, seq_length, encoder_dim]</span>
<span class="hljs-params">    </span>):
        scores = <span class="hljs-variable">self</span>._get_scores(query, values)  <span class="hljs-comment"># [batch_size, seq_length]</span>
        weights = torch.nn.functional.softmax(scores, dim=-<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> (values*weights.unsqueeze(-<span class="hljs-number">1</span>)).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># [batch_size, encoder_dim]</span>
</code></pre>
    <p class="normal">The three<a id="_idIndexMarker1086"/> lines of code in the <code class="inlineCode">forward</code> method correspond to the three steps we discussed earlier. The first step, which is calculating the scores, is the key step that has led to many different types of attention, and therefore we have generalized that into a <code class="inlineCode">_get_scores</code> abstract method that must be implemented by any class inheriting the <code class="inlineCode">Attention</code> class. For the second line, we have used the <code class="inlineCode">softmax</code> function for converting the scores to weights, and in the last line, we are doing an element-wise multiplication (<code class="inlineCode">*</code>) between weights and values and summing across the sequence length to get the weighted value.</p>
    <p class="normal">Now, let’s turn our attention toward alignment functions.</p>
    <h2 id="_idParaDest-302" class="heading-2">Alignment functions</h2>
    <p class="normal">There <a id="_idIndexMarker1087"/>are many<a id="_idIndexMarker1088"/> variations of the alignment function that have come up over the years. Let’s review a few popular ones that are used today.</p>
    <h3 id="_idParaDest-303" class="heading-3">Dot product</h3>
    <p class="normal">This is <a id="_idIndexMarker1089"/>probably the simplest alignment function of all. Luong et al. proposed this form of attention in 2015. From linear algebra, we know that the dot product of two vectors tells us what amount of one vector goes in the direction of the other. It measures some kind of similarity between the two vectors, and this similarity considers both the magnitude of the vectors and the angle between them in the vector space. Therefore, when<a id="_idIndexMarker1090"/> we take the dot product of our query and key vectors, we get a notion of similarity between them. One thing to note here is that the hidden dimensions of the query and the key should be the same for dot product attention to be applied. Formally, the similarity function can be defined as follows:</p>
    <p class="center"><img src="../Images/B22389_14_010.png" alt=""/></p>
    <p class="normal">We need to<a id="_idIndexMarker1091"/> calculate this score for each of the elements, <em class="italic">K</em><sub class="subscript-italic" style="font-style: italic;">i</sub>, in the key, <em class="italic">K</em>, and instead of running a loop over each element in <em class="italic">K</em>, we can use a clever matrix <a id="_idIndexMarker1092"/>multiplication trick to calculate the scores for all the keys in <em class="italic">K</em> in one shot. Let’s see how we can define the <code class="inlineCode">_get_scores</code> function for dot product attention.</p>
    <p class="normal">We know from the previous section that the query and values (which are the same as keys in our case) are of (<em class="italic">batch size</em>, <em class="italic">decoder dimension</em>) and (<em class="italic">batch size</em>, <em class="italic">sequence length</em>, <em class="italic">encoder dimension</em>) dimensions respectively, and will be called <code class="inlineCode">q</code> and <code class="inlineCode">v</code> in the <code class="inlineCode">_get_scores</code> function. In this particular case, the decoder dimension and the encoder dimension are the same, so the scores can be calculated as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">scores = (q @ v.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))
</code></pre>
    <p class="normal">Here, <code class="inlineCode">@</code> is shorthand for <code class="inlineCode">torch.matmul</code>, which does matrix multiplication. The entire implementation is named <code class="inlineCode">DotProductAttention</code> and can be found in <code class="inlineCode">src/dl/attention.py</code>.</p>
    <h3 id="_idParaDest-304" class="heading-3">Scaled dot product attention</h3>
    <p class="normal">In 2017, Vaswani et al. proposed this type of attention in the seminal paper, <em class="italic">Attention Is All You Need</em>. We <a id="_idIndexMarker1093"/>will delve into that paper later in this chapter, but now, let’s understand one key modification they suggested to <a id="_idIndexMarker1094"/>the dot product attention. The modification is motivated by the concern that when the input is large, the <em class="italic">softmax function</em> we use to <a id="_idIndexMarker1095"/>convert scores to weights may have very small gradients, which makes efficient learning difficult.</p>
    <p class="normal">This is because the <em class="italic">softmax</em> function is not scale-invariant. The exponential function in the <em class="italic">softmax</em> function is the reason for this behavior. So, the higher we scale the inputs to the function, the more the largest input dominates the output, and this throttles the gradient flow in the network. If we assume <em class="italic">q</em> and <em class="italic">v</em> are <em class="italic">d</em><sub class="subscript-italic" style="font-style: italic;">k</sub> dimensional vectors with 0 mean and a variance of 1, then their dot product would have a mean of zero and a variance of <em class="italic">d</em><sub class="subscript-italic" style="font-style: italic;">k</sub>. Therefore, if we scale the output of the dot product by <img src="../Images/B22389_14_011.png" alt=""/>, then we bring the variance of the dot product back to 1. So, by controlling for the scale of the inputs to the <em class="italic">softmax</em> function, we manage a healthy gradient flow through the network. The <em class="italic">Further reading</em> section has a link to a <a id="_idIndexMarker1096"/>blog post that goes into this in more depth. Therefore, the scaled dot product alignment function can be defined as follows:</p>
    <p class="center"><img src="../Images/B22389_14_012.png" alt=""/></p>
    <p class="normal">Consequently, the only change we will have to make to the <code class="inlineCode">PyTorch</code> implementation is one additional line:</p>
    <pre class="programlisting code"><code class="hljs-code">scores = scores/math.sqrt(encoder_dim)
</code></pre>
    <p class="normal">This has <a id="_idIndexMarker1097"/>been implemented as a parameter in <code class="inlineCode">DotProductAttention</code> in <code class="inlineCode">src/dl/attention.py</code>. If you pass <code class="inlineCode">scaled=True</code> while initializing the class, it will perform scaled dot product attention. We need to keep in mind that, similar to dot product attention, the scaled variant also requires the query and values to have the same dimensions.</p>
    <h3 id="_idParaDest-305" class="heading-3">General attention</h3>
    <p class="normal">In 2015, Luong et al. (Reference <em class="italic">2</em>) proposed a slight variation of dot product attention by introducing<a id="_idIndexMarker1098"/> a learnable <em class="italic">W</em> matrix into the calculation. They called it general attention. We can think of it as an attention mechanism that allows the query to be projected into a learned plane of the same dimension as the values/keys using the <em class="italic">W</em> matrix before computing the similarity score using a dot product. The alignment function can be written as follows:</p>
    <p class="center"><img src="../Images/B22389_14_013.png" alt=""/></p>
    <p class="normal">The corresponding <code class="inlineCode">PyTorch</code> implementation can be found under the name <code class="inlineCode">GeneralAttention</code> in <code class="inlineCode">src/dl/attention.py</code>. The key line calculating the attention scores can be written as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">scores = (q @ <span class="hljs-variable">self</span>.W) @ v.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Here, <code class="inlineCode">self.W</code> is a tensor of size (<em class="italic">encoder hidden dimension x decoder hidden dimension</em>). General attention can be used in cases where the query and key/value dimensions are different.</p>
    <h3 id="_idParaDest-306" class="heading-3">Additive/concat attention</h3>
    <p class="normal">In 2015, Bahdanau et al. proposed additive attention, which was one of the first attempts <a id="_idIndexMarker1099"/>at introducing attention to DL systems. Instead of using a defined similarity function such as the dot product, Bahdanau et al. proposed that the similarity function can be learned, giving the network more flexibility in deciding what it deems to be similar. They suggested that we can concatenate the query and the key into a single tensor and use a learnable matrix, <em class="italic">W</em>, to calculate the attention scores. This alignment function can be written as follows:</p>
    <p class="center"><img src="../Images/B22389_14_014.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">v</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">q</sub>, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">k</sub> are learnable matrices. In cases where the query and key have different hidden dimensions, we can use <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">q</sub> and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">k</sub> to project them into a single dimension and then perform a similarity calculation on them. If the query and key have the same hidden dimension, this is also equivalent to the variant of attention used in Luong et al., which they call <em class="italic">concat</em> attention, represented as follows:</p>
    <p class="center"><img src="../Images/B22389_14_015.png" alt=""/></p>
    <p class="normal">It is simple linear algebra to see that both are the same and for engineering simplicity. The <em class="italic">Further reading</em> section has a link to a Stack Overflow answer that explains the equivalence.</p>
    <p class="normal">We have included both implementations in <code class="inlineCode">src/dl/attention.py</code> under <code class="inlineCode">ConcatAttention</code> and <code class="inlineCode">AdditiveAttention</code>.</p>
    <p class="normal">For <code class="inlineCode">AdditiveAttention</code>, the key lines calculating the score are as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">q = q.repeat(<span class="hljs-number">1</span>, v.size(<span class="hljs-number">1</span>), <span class="hljs-number">1</span>)  <span class="hljs-comment"># [batch_size, seq_length, decoder_dim]</span>
scores = <span class="hljs-variable">self</span>.W_q(q) + <span class="hljs-variable">self</span>.W_v(v)  <span class="hljs-comment"># [batch_size, seq_length, decoder_dim]</span>
torch.tanh(scores) @ <span class="hljs-variable">self</span>.v  <span class="hljs-comment"># [batch_size, seq_length]</span>
</code></pre>
    <p class="normal">The first line repeats the query vector to the sequence length. This is just a linear algebra trick to calculate the score for all the encoder hidden states in a single operation rather than looping through them. <em class="italic">Line 2</em> projects both query and value into the same dimension using <code class="inlineCode">self.W_q</code> and <code class="inlineCode">self.W_v</code>, and <em class="italic">line 3</em> applies the <code class="inlineCode">tanh</code> activation function and uses matrix multiplication with <code class="inlineCode">self.v</code> to produce the final scores. <code class="inlineCode">self.W_q</code>, <code class="inlineCode">self.W_v</code>, and <code class="inlineCode">self.v</code> are learnable matrices, defined as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-variable">self</span>.W_q = torch.nn.Linear(<span class="hljs-variable">self</span>.decoder_dim, <span class="hljs-variable">self</span>.decoder_dim)
<span class="hljs-variable">self</span>.W_v = torch.nn.Linear(<span class="hljs-variable">self</span>.encoder_dim, <span class="hljs-variable">self</span>.decoder_dim)
<span class="hljs-variable">self</span>.v = torch.nn.Parameter(torch.FloatTensor(<span class="hljs-variable">self</span>.decoder_dim)
</code></pre>
    <p class="normal">The only <a id="_idIndexMarker1100"/>difference in <code class="inlineCode">ConcatAttention</code> is that instead of two separate weights—<code class="inlineCode">self.W_q</code> and <code class="inlineCode">self.W_v</code>—we just have a single weight—<code class="inlineCode">self.W</code>—defined as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-variable">self</span>.W = torch.nn.Linear(<span class="hljs-variable">self</span>.decoder_dim + <span class="hljs-variable">self</span>.encoder_dim, <span class="hljs-variable">self</span>.decoder_dim)
</code></pre>
    <p class="normal">And instead of adding the projections (<em class="italic">line 2</em>), we use the following line:</p>
    <pre class="programlisting code"><code class="hljs-code">scores = <span class="hljs-variable">self</span>.W(
            torch.cat([q, v], dim=-<span class="hljs-number">1</span>)
        )  <span class="hljs-comment"># [batch_size, seq_length, decoder_dim]</span>
</code></pre>
    <p class="normal">Therefore, we can think of <code class="inlineCode">AdditiveAttention</code> and <code class="inlineCode">ConcatAttention</code> doing the same operation, but <code class="inlineCode">AdditiveAttention</code> is adapted to handle different encoder and decoder dimensions.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check:</strong></p>
      <p class="normal">The research papers by Luong et al., Badahnau et al., and Vaswani et al. are cited in the <em class="italic">References</em> section as references <em class="italic">2</em>, <em class="italic">1</em>, and <em class="italic">5</em> respectively.</p>
    </div>
    <p class="normal">Now that we have learned about a few popular alignment functions, let’s turn our attention toward the distribution function of the attention model.</p>
    <h2 id="_idParaDest-307" class="heading-2">The distribution function</h2>
    <p class="normal">The <a id="_idIndexMarker1101"/>primary goal of the distribution function is to<a id="_idIndexMarker1102"/> convert the learned scores from the alignment function into a set of weights that add up to 1. The <em class="italic">softmax</em> function is the most popular<a id="_idIndexMarker1103"/> choice as a distribution function. It converts the score into a set of weights that sum up to one. This also gives us the freedom to interpret the learned weights as probabilities—the probability that the corresponding element is the most relevant.</p>
    <p class="normal">Although <em class="italic">softmax</em> is the <a id="_idIndexMarker1104"/>most popular choice, it is not without its drawbacks. The <em class="italic">softmax</em> weights are typically <em class="italic">dense</em>. What that means is that a probability mass (some weight) will be assigned to every element in the sequence over which we calculated the attention. The weights can be low, but still not 0. There are situations where sparsity in the distribution function is desirable. Maybe we want to make sure we don’t give any weights some implausible options. Maybe we want to make the attention mechanism more interpretable.</p>
    <p class="normal">There are<a id="_idIndexMarker1105"/> alternate distribution functions <a id="_idIndexMarker1106"/>such as <code class="inlineCode">sparsemax</code> (Martins et al. 2016, Reference <em class="italic">3</em>) and <code class="inlineCode">entmax</code> (Peters et al. 2019, Reference <em class="italic">4</em>) that are capable of assigning probability mass to a<a id="_idIndexMarker1107"/> select few relevant elements and assigning zero to the rest of them. When we know that the output is only dependent on a few timesteps in the encoder, we can use such distribution functions to encode that knowledge into the model. Space activation functions like <code class="inlineCode">Sparsemax</code> have the advantage of interpretability, as they provide a clearer distinction between the elements that are important (non-zero probabilities) and those that are not (zero probabilities).</p>
    <div class="packt_tip">
      <p class="normal"><strong class="keyWord">Reference check:</strong></p>
      <p class="normal">The research papers by Martins et al. and Peters et al. are cited in the <em class="italic">References</em> section as references <em class="italic">3</em> and <em class="italic">4</em> respectively.</p>
    </div>
    <p class="normal">Now that we have learned about a few attention mechanisms, it’s time to put them into practice.</p>
    <h1 id="_idParaDest-308" class="heading-1">Forecasting with sequence-to-sequence models and attention</h1>
    <p class="normal">Let’s pick up <a id="_idIndexMarker1108"/>the thread from <em class="chapterRef">Chapter 13</em>, <em class="italic">Common Modeling Patterns for Time Series</em>, where we used Seq2Seq models to forecast a sample household (if you have not read the previous chapter, I strongly suggest you do it now) and modify the <code class="inlineCode">Seq2SeqModel</code> class to also include an attention mechanism.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert:</strong></p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">01-Seq2Seq_RNN_with_Attention.ipynb</code> in the <code class="inlineCode">Chapter14</code> folder and the code in the <code class="inlineCode">src</code> folder.</p>
    </div>
    <p class="normal">We are<a id="_idIndexMarker1109"/> still going to inherit the <code class="inlineCode">BaseModel</code> class <a id="_idIndexMarker1110"/>we have defined in <code class="inlineCode">src/dl/models.py</code>, and the overall structure is going to be very similar to the <code class="inlineCode">Seq2SeqModel</code> class. The key difference will be that in our new model, with attention, we do not accept a fully connected layer as the decoder. It is not because it is not possible, but for convenience and brevity of the implementation. In fact, implementing a Seq2Seq model with a fully connected decoder is something you can do on your own to really internalize the concept.</p>
    <p class="normal">Similar to the <code class="inlineCode">Seq2SeqConfig</code> class, we define a very similar <code class="inlineCode">Seq2SeqwAttnConfig</code> class that has the exact same set of parameters, but with some additional validation checks. One of the validation checks is disallowing a fully connected decoder. Another validation check would be making sure the decoder input size allows for the attention mechanism as well. We will see those requirements in detail shortly.</p>
    <p class="normal">In addition to <code class="inlineCode">Seq2SeqwAttnConfig</code>, we also define a <code class="inlineCode">Seq2SeqwAttnModel</code> class to enable attention-enabled decoding. The only additional parameter here is <code class="inlineCode">attention_type</code>, which is a string parameter that takes the following values:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">dot</code>: Dot product attention</li>
      <li class="bulletList"><code class="inlineCode">scaled dot</code>: Scaled dot product attention</li>
      <li class="bulletList"><code class="inlineCode">general</code>: General attention</li>
      <li class="bulletList"><code class="inlineCode">additive</code>: Additive attention</li>
      <li class="bulletList"><code class="inlineCode">concat</code>: Concat attention</li>
    </ul>
    <p class="normal">The entire code is available in <code class="inlineCode">src/dl/models.py</code>. We will be covering only the <code class="inlineCode">forward</code> function in detail in the book because that is the only place where there is a key difference. The rest of the class is about defining the right attention model based on input parameters and so on.</p>
    <p class="normal">The encoder part is exactly the same as <code class="inlineCode">SeqSeqModel</code>, which we saw in the last chapter. The only difference is in the decoding where we will be using attention.</p>
    <p class="normal">Now, let’s talk<a id="_idIndexMarker1111"/> about how we are going to use the attention output in decoding.</p>
    <p class="normal">As I mentioned before, there are two schools of thought on how to use attention while decoding. Using the same terminology we have been using for attention, let’s see the difference between them.</p>
    <p class="normal">Luong et al. use <a id="_idIndexMarker1112"/>the decoder hidden state at step <em class="italic">j</em>, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">j</sub>, to calculate the similarity between itself and all the encoder hidden states, <em class="italic">H</em>, to calculate the context vector, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">j</sub>. This context vector, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">j</sub>, is then concatenated with the decoder hidden state, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">j</sub>, and this combined tensor is used as the input to the linear layer that generates the output.</p>
    <p class="normal">Bahdanau et al. use attention in another way. They use the decoder hidden state from the previous timestep, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">j-</sub><sub class="subscript">1</sub>, and calculate the similarity with all the encoder hidden states, <em class="italic">H</em>, to calculate the context vector, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">j</sub>. And now, this context vector, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">j</sub>, is concatenated with the input to decoding step <em class="italic">j</em>, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">j</sub>. It is this concatenated input that is used in the decoding step that uses an RNN.</p>
    <p class="normal">We can see the differences visually in <em class="italic">Figure 14.3</em>. The <em class="italic">Further reading</em> section also has another brilliant animation of attention in <em class="italic">Attn: Illustrated Attention</em>. This can also help you understand the mechanism well:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_03.png" alt="Figure 14.3 – Attention-based decoding: Bahdanau versus Luong "/></figure>
    <p class="packt_figref">Figure 14.3: Attention-based decoding: Bahdanau versus Luong</p>
    <p class="normal">In our<a id="_idIndexMarker1113"/> implementation, we have chosen the Bahdanau way of decoding, where we use the concatenated context vector and input as the input for decoding. Because of that, there is a condition the decoder must satisfy: the <code class="inlineCode">input_size</code> parameter of the decoder should be <a id="_idIndexMarker1114"/>equal to the sum of the <code class="inlineCode">input_size</code> parameter of the encoder and the <code class="inlineCode">hidden_size</code> parameter of the encoder. This validation is built into <code class="inlineCode">Seq2SeqwAttnConfig</code>.</p>
    <p class="normal">The following <a id="_idIndexMarker1115"/>code block has all the code necessary for decoding with attention and has line numbers so that we can go line by line and explain what we are doing:</p>
    <pre class="programlisting code"><code class="hljs-code">01        y_hat = torch.zeros_like(y, device=y.device)
02        dec_input = x[:, -<span class="hljs-number">1</span>:, :]
03        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(y.size(<span class="hljs-number">1</span>)):
04            top_h = <span class="hljs-variable">self</span>._get_top_layer_hidden_state(h)
05            context = <span class="hljs-variable">self</span>.attention(
06                top_h.unsqueeze(<span class="hljs-number">1</span>), o
07            )
08            dec_input = torch.cat((dec_input, context.unsqueeze(<span class="hljs-number">1</span>)), dim=-<span class="hljs-number">1</span>)
09            out, h = <span class="hljs-variable">self</span>.decoder(dec_input, h)
10            out = <span class="hljs-variable">self</span>.fc(out)
11            y_hat[:, i, :] = out.squeeze(<span class="hljs-number">1</span>)
12            teacher_force = random.random() &lt; <span class="hljs-variable">self</span>.hparams.teacher_forcing_ratio
13            <span class="hljs-keyword">if</span> teacher_force:
14                dec_input = y[:, i, :].unsqueeze(<span class="hljs-number">1</span>)
15            <span class="hljs-keyword">else</span>:
16                dec_input = out
</code></pre>
    <p class="normal"><em class="italic">Lines 1</em> and <em class="italic">2</em> are<a id="_idIndexMarker1116"/> the same as in the <code class="inlineCode">Seq2SeqMode</code>l class where we set up the variable to store the prediction and extract the starting input to be passed to the decoder, and <em class="italic">line 3</em> starts the loop for decoding step by step.</p>
    <p class="normal">Now, in each step, we need to use the hidden state from the previous timestep to calculate the context vector. If you remember the output shapes of an RNN (<em class="chapterRef">Chapter 12</em>, <em class="italic">Building Blocks of Deep Learning for Time Series</em>), we know that it is (<em class="italic">number of layers</em>, <em class="italic">batch size</em>, <em class="italic">hidden size</em>). But we need our query hidden state to be of the dimension (<em class="italic">batch size</em>, <em class="italic">hidden size</em>). Luong et al. suggested using the hidden states from the top layer of a stacked RNN model as the query, and we are doing just that here:</p>
    <pre class="programlisting code"><code class="hljs-code">hidden_state[-<span class="hljs-number">1</span>, :, :]
</code></pre>
    <p class="normal">If the RNN is bi-directional, we would need to slightly alter the retrieval because now, the last two rows of the tensor would be the output from the last layer (one forward and one backward). There are many ways to combine them into a single tensor—we can concatenate them, we can sum them, or we can even mix them using a linear layer. Here, we just concatenate them:</p>
    <pre class="programlisting code"><code class="hljs-code">torch.cat((hidden_state[-<span class="hljs-number">1</span>, :, :], hidden_state[-<span class="hljs-number">2</span>, :, :]), dim=-<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Now that we<a id="_idIndexMarker1117"/> have the hidden state, we use it as the query in the attention layer (<em class="italic">line 5</em>). In <em class="italic">line 8</em>, we concatenate the context with the input. Lines <em class="italic">9</em> to <em class="italic">16</em> do the rest of the decoding in a similar way to <code class="inlineCode">Seq2SeqModel</code>.</p>
    <p class="normal">The notebook trains a multi-step Seq2Seq model (the best-performing variant with teacher forcing) with all the different types of attention we covered in the chapter using the same setup we developed in the last chapter. The results are summarized in the following table:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_04.png" alt="Figure 14.4 – Summary table for Seq2Seq models with attention "/></figure>
    <p class="packt_figref">Figure 14.4: Summary table for Seq2Seq models with attention</p>
    <p class="normal">We can<a id="_idIndexMarker1118"/> see that it is showing considerable improvements in <strong class="keyWord">MAE</strong>, <strong class="keyWord">MSE</strong>, and <strong class="keyWord">MASE</strong> by including attention, and out of all the variants of attention, the simple dot product attention performed the best, closely followed by additive attention. At this point, some of you might have a question in your mind—<em class="italic">Why didn’t the scaled dot product work better than the dot product attention?</em> Scaling was supposed to make the dot product work better, wasn’t it?</p>
    <p class="normal">There is a<a id="_idIndexMarker1119"/> lesson to be learned here (which applies to all <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>)). No matter how much better a particular technique is in theory, you can always find examples in which it performs worse. Here, we saw just one household, and it is not surprising that we saw that scaled dot product attention didn’t work better than the normal dot product attention. But if we had evaluated at scale and realized that this is a pattern across multiple datasets, then it would be concerning.</p>
    <p class="normal">So, we have seen that attention does make the models better. There was a lot of research done on using attention in various forms to enhance the performance of <strong class="keyWord">neural network</strong> (<strong class="keyWord">NN</strong>) models. Most <a id="_idIndexMarker1120"/>of that research was carried out in <strong class="keyWord">natural language processing</strong> (<strong class="keyWord">NLP</strong>), specifically <a id="_idIndexMarker1121"/>in language translation and language modeling. Soon, researchers stumbled upon a surprising result that changed the course of DL progress drastically—the Transformer model.</p>
    <h1 id="_idParaDest-309" class="heading-1">Transformers—Attention is all you need</h1>
    <p class="normal">While the<a id="_idIndexMarker1122"/> introduction of attention was a shot in the arm for RNNs and Seq2Seq models, they still had one problem. The RNNs were recurrent, and that meant they needed to process each word in a sentence in a sequential manner. </p>
    <p class="normal">For popular Seq2Seq model applications such as language translation, it meant processing long sequences of words became really time-consuming. In short, it was difficult to scale them to a large corpus of data. In 2017, Vaswani et al. (Reference <em class="italic">5</em>) authored a seminal paper titled <em class="italic">Attention Is All You Need</em>. Just as the title of the paper implies, they explored an architecture that used attention (scaled dot product attention) and threw away recurrent networks altogether. To the surprise of NLP researchers around the world, these models (which were dubbed Transformers) outperformed the then state-of-the-art Seq2Seq models in language translation.</p>
    <p class="normal">This spurred a flurry of research activity around this new class of models, and pretty soon, in 2018, Devlin et al. (Reference <em class="italic">6</em>) from Google developed a bi-directional version of Transformers and trained the now famous language model <strong class="keyWord">BERT</strong> (which stands for <strong class="keyWord">Bidirectional Encoder Representations from Transformers</strong>), and broke many state-of-the-art results in multiple tasks. This is considered to be the moment when Transformers as a model class really <em class="italic">arrived</em>. Fast-forward to 2022—Transformer models are ubiquitous. They are used in almost all tasks in NLP, and in many other sequence-based tasks such as time series forecasting and <strong class="keyWord">reinforcement learning</strong> (<strong class="keyWord">RL</strong>). They have also been successfully used in <strong class="keyWord">computer vision</strong> (<strong class="keyWord">CV</strong>) tasks as well.</p>
    <p class="normal">There have been numerous modifications and adaptations to the vanilla Transformer model to make it more <a id="_idIndexMarker1123"/>suitable for time series forecasting. But let’s discuss the vanilla Transformer architecture that Vaswani et al. proposed in 2017 first.</p>
    <h2 id="_idParaDest-310" class="heading-2">Attention is all you need</h2>
    <p class="normal">The model Vaswani et al. proposed (hereby referred to as the vanilla Transformer) is also an encoder-decoder model, but both the encoder and decoder are non-recurrent. They are entirely composed of attention mechanisms and feed-forward networks. Since the Transformer model was developed first for text sequences, let’s use the same example to understand and then adapt to the time series context.</p>
    <p class="normal">There are a few key components of the model that need to be understood to put the whole thing together. Let’s take them one by one.</p>
    <h3 id="_idParaDest-311" class="heading-3">Self-attention</h3>
    <p class="normal">We saw how scaled <a id="_idIndexMarker1124"/>dot product attention works earlier in this chapter (in the <em class="italic">Alignment functions</em> section), but there, we were calculating attention between the encoder and decoder hidden states. Self-attention is when we have an input sequence, and we calculate the attention scores between that input sequence itself. Intuitively, we can think of this operation as enhancing the contextual information and enabling the downstream components to use this enhanced information for further processing.</p>
    <p class="normal">We saw the <code class="inlineCode">PyTorch</code> implementation for encoder-decoder attention earlier, but that implementation was more aligned toward the step-by-step decoding of an RNN. Computing the attention scores for each query-key pair in one shot is something very simple to achieve using standard matrix multiplication and is essential to computing efficiency.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert:</strong></p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">02-Self-Attention_and_Multi-Headed_Attention.ipynb</code> in the <code class="inlineCode">Chapter14</code> folder.</p>
    </div>
    <p class="normal">In NLP, it is standard practice to represent each word as a learnable vector called an embedding. This is because text or strings have no place in a mathematical model. For our example’s sake, let’s assume we use an embedding vector of size 512 for each word, and let’s assume that the attention mechanism has an internal dimension of 64. Let’s elucidate the attention mechanism using a sentence with 10 words.</p>
    <p class="normal">After embedding, the sentence would be a tensor with dimensions <code class="inlineCode">(10, 512)</code>. We need to have three learnable weight matrices, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">q</sub>, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">k</sub>, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">v</sub>, to project the input embedding into the attention dimension <code class="inlineCode">(64)</code>. See <em class="italic">Figure 14.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_05.png" alt="Figure 14.5 – Self-attention layer: input sentence and the learnable weights "/></figure>
    <p class="packt_figref">Figure 14.5: Self-attention layer: input sentence and the learnable weights</p>
    <p class="normal">The first operation projects the sentence tensor into a query, key, and value with dimensions equal to (<em class="italic">sequence length</em>, <em class="italic">attention dim</em>). This is done by using a matrix multiplication between the sentence tensor and learnable matrices. See <em class="italic">Figure 14.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_06.png" alt="Figure 14.6 – Self-attention layer: query, key, and value projection "/></figure>
    <p class="packt_figref">Figure 14.6: Self-attention layer: query, key, and value projection</p>
    <p class="normal">Now that we<a id="_idIndexMarker1125"/> have the query, key, and value, we can calculate the attention weights of every query-key pair using matrix multiplication between the query and the transpose of the keys. The matrix multiplication is nothing but the dot product of each query with each of the values and gives us a square matrix of (<em class="italic">sequence length</em>, <em class="italic">sequence length</em>). See <em class="italic">Figure 14.7</em>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.7: Self-attention layer: attention scores between Q and K</p>
    <p class="normal">Converting <a id="_idIndexMarker1126"/>the attention scores to attention weights is just about scaling and applying the <em class="italic">softmax</em> function, as we discussed in the <em class="italic">Scaled dot product attention</em> section.</p>
    <p class="normal">Now that we have the attention weights, we can use them to combine the value. The element-wise multiplication and then summing across the weights can be done efficiently using another matrix multiplication. See <em class="italic">Figure 14.8</em>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_08.png" alt="Figure 14.8 – Self-attention layer: combining V using the learned attention weights "/></figure>
    <p class="packt_figref">Figure 14.8: Self-attention layer: combining V using the learned attention weights</p>
    <p class="normal">Now, we have seen how attention is applied to all the query-key pairs in monolithic matrix operations rather than doing the same operation for each query in a sequential way. But <em class="italic">Attention Is All You Need</em> proposed something even better.</p>
    <h3 id="_idParaDest-312" class="heading-3">Multi-headed attention</h3>
    <p class="normal">Since Transformers <a id="_idIndexMarker1127"/>intended to take away the entire recurrent architecture, they needed to beef up the attention mechanism because that was the workhorse of the model. So, instead of using a single attention head, the authors of the paper proposed multiple attention heads acting together in different subspaces. We know that attention helps the model focus on a few elements of the many. <strong class="keyWord">Multi-headed attention</strong> (<strong class="keyWord">MHA</strong>) does <a id="_idIndexMarker1128"/>the same thing but focuses on different aspects or different sets of elements, thereby increasing the capacity of the model. If we want to draw an analogy to the human mind, we consider many aspects of a situation before we make a decision.</p>
    <p class="normal">For instance, if <a id="_idIndexMarker1129"/>we decide to step out of the house, we will pay attention to the weather, we will pay attention to the time so that whatever we want to accomplish is still possible, we will pay attention to how punctual that friend you made a plan with has been in the past, and leave our house accordingly. You can think of each of these things as one head of attention. Therefore, MHA enables Transformers to <em class="italic">attend</em> to multiple aspects at the same time.</p>
    <p class="normal">Normally, if there are eight heads, we would assume that we would have to do the computation that we saw in the last section eight times. But thankfully, that is not the case. There are clever ways of accomplishing this MHA using the same kind of matrix multiplication, but now with larger matrices. Let’s see how that is done.</p>
    <p class="normal">We will continue the same example and see a case where we have eight attention heads. There is one condition that needs to be satisfied to do this efficient calculation of MHA—the attention dimension should be divisible by the number of heads we are using.</p>
    <p class="normal">The initial steps are exactly the same. We take in the input sentence tensor and project it into the query, key, and value. Now, we split the query, key, and value into separate query, key, and value subspaces for each head by doing some basic tensor re-arrangement. See <em class="italic">Figure 14.9</em>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_09.png" alt="Figure 14.9 – Multi-headed attention: reshaping Q, K, and V into subspaces for each head "/></figure>
    <p class="packt_figref">Figure 14.9: Multi-headed attention: reshaping Q, K, and V into subspaces for each head</p>
    <p class="normal">Now, we calculate the attention scores for each head in a single operation and combine them with the value to get the attention output for each head. See <em class="italic">Figure 14.10</em>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.10: Multi-headed attention: calculating attention weights and combining the value</p>
    <p class="normal">We have the<a id="_idIndexMarker1130"/> attention output of each head in the <code class="inlineCode">attn_output</code> variable. Now, all we need to do is reshape the array so that we stack the outputs from all the attention heads in a single dimension. See <em class="italic">Figure 14.11</em>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_11.png" alt="Figure 14.11 – Multi-headed attention: reshaping and stacking all the attention head outputs "/></figure>
    <p class="packt_figref">Figure 14.11: Multi-headed attention: reshaping and stacking all the attention head outputs</p>
    <p class="normal">In this way, we can do MHA in a fast and efficient manner. Now, let’s look at another key innovation that makes Transformers work.</p>
    <h3 id="_idParaDest-313" class="heading-3">Positional encoding</h3>
    <p class="normal">Transformers <a id="_idIndexMarker1131"/>successfully prevented recurrence and unlocked a performance bottleneck of sequential operations. This also means that the Transformer model is agnostic of the order of the sequence. In mathematical terms, if RNNs were considering the sequence as a sequence, Transformers consider it as a set of values. For the Transformer, each position is independent of the other, and hence one key aspect we would seek from a model that processes sequences is missing. The original authors did propose a way to make sure we do not lose this information—<strong class="keyWord">positional encoding</strong>.</p>
    <p class="normal">There have been many variants <a id="_idIndexMarker1132"/>of positional encoding that have come up in subsequent years of research, but the most common one is still the variant that is used in the vanilla Transformer.</p>
    <p class="normal">The solution proposed by Vaswani et al. was to add a particular vector, which encodes the position mathematically using sine and cosine functions, to each of the input tokens before processing them through the self-attention layer. If the input <em class="italic">X</em>, is a <em class="italic">d</em><sub class="subscript-italic" style="font-style: italic;">model</sub>-dimensional embedding for <em class="italic">n</em> tokens in a sequence, positional embeddings, <em class="italic">P</em>, is a matrix of the same size (<em class="italic">n</em> x <em class="italic">d</em><sub class="subscript-italic" style="font-style: italic;">model</sub>). The element on the <em class="italic">pos</em><sup class="superscript">th</sup> row and 2<em class="italic">i</em><sup class="superscript">th</sup> or (2<em class="italic">i</em> + 1)<sup class="superscript">th</sup> column is defined as follows:</p>
    <p class="center"><img src="../Images/B22389_14_016.png" alt=""/></p>
    <p class="center"><img src="../Images/B22389_14_017.png" alt=""/></p>
    <p class="normal">Although this looks a little complicated and counterintuitive, let’s break this down to understand it better.</p>
    <p class="normal">From 20,000 feet, we know that these positional encodings capture the positional information, and we add them to the input embeddings. But why do we add them to the input embeddings? Let’s make this clearer. Let’s assume the embedding dimension is just 2 (this is for ease of visualization and grasping the concept better), and we have a word, <em class="italic">A</em>, represented using this token. For our experiment, let’s assume that we have the same word, <em class="italic">A</em>, repeated several times in our sequence. What happens if we add the positional encoding to it?</p>
    <p class="normal">We know that<a id="_idIndexMarker1133"/> the sine or cosine functions vary between 0 and 1. So, each of these encodings we add to the word embedding just perturbs the word embedding within a unit circle. As <em class="italic">pos</em> increases, we can see the position-encoded word embedding trace a unit circle around the original embedding (<em class="italic">Figure 14.12</em>):</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_12.png" alt="Figure 14.12 – Position encoding: intuition "/></figure>
    <p class="packt_figref">Figure 14.12: Position encoding: intuition</p>
    <p class="normal">In <em class="italic">Figure 14.12</em>, we have assumed a random embedding for a word, <em class="italic">A</em> (represented by the cross marker), and added position embedding assuming <em class="italic">A</em> is in different positions. These position-encoded vectors are represented by star markers with the corresponding positions mentioned in numbers next to them. We can see how each position is a slightly perturbed point of the original vector, and it happens in a cyclical manner in a clockwise <a id="_idIndexMarker1134"/>direction. We can see position 0 right at the top with 1, 2, 3, and so on in the clockwise direction. By having this representation, the model can figure out the word in different locations, and still retain the overall position in the semantic space.</p>
    <p class="normal">Now that we know why we are adding the positional encodings to the input embeddings and have seen why it works, let’s get into the details and see how the terms inside the sine and cosine are calculated. <em class="italic">pos</em> represents the position of the token in the sequence. If the maximum length of the sequence is 128, <em class="italic">pos</em> varies from 0 to 127. <em class="italic">i</em> represents the position along the embedding dimension, and because of the way the formula has been defined, for each value of <em class="italic">i</em>, we have two values—a sine and a cosine. Therefore, <em class="italic">i</em> will be half the number of dimensions, <em class="italic">d</em><sub class="subscript-italic" style="font-style: italic;">model</sub>, and will go from 0 to <em class="italic">d</em><sub class="subscript-italic" style="font-style: italic;">model</sub>/2.</p>
    <p class="normal">With all this information, we know that the term inside the sine and cosine functions approaches 0 as we go toward the end of the embedding dimension. It also increases from 0 as we move along the sequence dimension. For each pair (2<em class="italic">i</em> and 2<em class="italic">i</em>+1) of positions in the embedding dimension, we have a complementary sine and cosine wave, as <em class="italic">Figure 14.13</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_13.png" alt="Figure 14.13 – Positional encoding: sine and cosine terms "/></figure>
    <p class="packt_figref">Figure 14.13: Positional encoding: sine and cosine terms</p>
    <p class="normal">We can see that embedding dimensions <strong class="keyWord">40</strong> and <strong class="keyWord">41</strong> are sine and cosine waves of the same frequency, and embedding dimensions <strong class="keyWord">40</strong> and <strong class="keyWord">42</strong> are sine waves with a slight increase in frequency. By using the combination of sine and cosine waves of varying frequencies, the <a id="_idIndexMarker1135"/>positional encoding can encode rich positional information as a vector. If we plot a heatmap (refer to the color images file:<a href="https://packt.link/gbp/9781835883181"><span class="url">https://packt.link/gbp/9781835883181</span></a>) of the whole positional encoding vector (<em class="italic">Figure 14.14</em>), we can see how the values change and encode the positional information:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_14.png" alt="Figure 14.14 – Positional encoding: heatmap of the entire vector "/></figure>
    <p class="packt_figref">Figure 14.14: Positional encoding: heatmap of the entire vector</p>
    <p class="normal">Another interesting observation is that the positional encoding quickly shrinks to 0/1 as we move forward in the embedding dimension because the term inside the sine or cosine functions (angle in radians) quickly becomes zero on account of the large denominator. The zoomed plot shows the color differences more clearly.</p>
    <p class="normal">Now, for the last component in the Transformer model.</p>
    <h3 id="_idParaDest-314" class="heading-3">Position-wise feed-forward layer</h3>
    <p class="normal">We have<a id="_idIndexMarker1136"/> already covered what feed-forward networks are in <em class="chapterRef">Chapter 12</em>, <em class="italic">Building Blocks of Deep Learning for Time Series</em>. The only thing to be noted here is that the position-wise feed-forward layer is when we apply the same feed-forward layer in each position, independently. If we have 12 positions (or words), we will have a single feed-forward network to process each of these positions.</p>
    <p class="normal">Vaswani et al. defined this as a two-layer feed-forward network where the transformations were defined so that the input dimensions are expanded to four times the input dimension, with a ReLU activation function applied at that stage, and then transformed back to the input dimension again. The exact operation can be written as a mathematical formula:</p>
    <p class="center"><em class="italic">FFN</em>(<em class="italic">x</em>) = <em class="italic">max</em>(0, <em class="italic">W</em><sub class="subscript">1</sub><em class="italic">x</em> + <em class="italic">b</em><sub class="subscript">1</sub>) <em class="italic">W</em><sub class="subscript">2</sub> + <em class="italic">b</em><sub class="subscript">2</sub></p>
    <p class="normal">Here, <em class="italic">W</em><sub class="subscript">1</sub> is a matrix of dimensions (<em class="italic">input size</em>, <em class="italic">4*input size</em>), <em class="italic">W</em><sub class="subscript">2</sub> is a matrix of dimensions (<em class="italic">4*input size, input size</em>), <em class="italic">b</em><sub class="subscript">1</sub> and <em class="italic">b</em><sub class="subscript">2</sub> are the corresponding biases, and <em class="italic">max</em>(0, <em class="italic">x</em>) is the standard ReLU operator.</p>
    <p class="normal">There have been studies where researchers have tried replacing ReLU with other activation functions, more <a id="_idIndexMarker1137"/>specifically <strong class="keyWord">Gated Linear Units</strong> (<strong class="keyWord">GLUs</strong>), which have shown promise. Noam Shazeer from Google has a paper on the topic, and if you want to know more about these new activation functions, I recommend checking out his paper in the <em class="italic">Further reading</em> section.</p>
    <p class="normal">Now that we know all the necessary components of a Transformer model, let’s see how they are put together.</p>
    <h3 id="_idParaDest-315" class="heading-3">Encoder</h3>
    <p class="normal">The vanilla <a id="_idIndexMarker1138"/>Transformer model is an encoder-decoder model. There are N blocks of encoders, and <a id="_idIndexMarker1139"/>each block contains an MHA layer and a position-wise feed-forward layer with residual connections in between (<em class="italic">Figure 14.15</em>):</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_15.png" alt="Figure 14.15 – Transformer model from Attention is All you Need by Vaswani et al. "/></figure>
    <p class="packt_figref">Figure 14.15: Transformer model from Attention Is All You Need by Vaswani et al.</p>
    <p class="normal">For now, let’s<a id="_idIndexMarker1140"/> focus on the left side of <em class="italic">Figure 14.15</em>, which is the encoder. The encoder takes in the input embeddings, with the positional encoding vector added to it, as the input. The three-pronged arrow that goes into MHA denotes the query, key, and value split. The output from the MHA goes into a block named <em class="italic">Add and Norm</em>. Let’s quickly see what that does.</p>
    <p class="normal">There are two key operations that happen here—<strong class="keyWord">residual connections</strong> and <strong class="keyWord">layer normalization</strong>.</p>
    <h4 class="heading-4">Residual connections</h4>
    <p class="normal">Residual <a id="_idIndexMarker1141"/>connections (or skip connections) are a family of techniques that were introduced to DL to make learning deep networks easier. The primary benefit of the technique is that it makes the gradient flow through the network better and thereby encourages learning in all parts of the network. They incorporate a pass-through memory highway in the network. We have already seen one instance where a skip connection (although not an apparent one) resolved gradient <a id="_idIndexMarker1142"/>flow issues—<strong class="keyWord">long short-term memory networks</strong> (<strong class="keyWord">LSTMs</strong>). The cell state in the LSTM serves as this highway to let gradients flow through the network without getting into vanishing gradient issues.</p>
    <p class="normal">But nowadays, when we say residual connections, we typically think of <em class="italic">ResNets</em>, which made a splash in the history of DL through a <strong class="keyWord">convolutional neural network</strong> (<strong class="keyWord">CNN</strong>) architecture that won major image classification challenges, including ImageNet in 2015. They introduced residual connections to train much deeper architectures than those prevalent at the time. The concept is deceptively simple. Let’s visualize it:</p>
    <figure class="mediaobject"><img src="../Images/Image3805.jpg" alt="Figure 14.16 – Residual networks "/></figure>
    <p class="packt_figref">Figure 14.16: Residual networks</p>
    <p class="normal">Let’s <a id="_idIndexMarker1143"/>assume a DL model with two layers with functions, <em class="italic">M</em><sub class="subscript">1</sub> and <em class="italic">M</em><sub class="subscript">2</sub>. In a regular neural network, the input, <em class="italic">x</em>, passes through the two layers to give us the output, <em class="italic">y</em>. These two individual functions can be considered as a single function that converts <em class="italic">x</em> to <em class="italic">y</em>: <em class="italic">y</em> = <em class="italic">F</em>(<em class="italic">x</em>).</p>
    <p class="normal">In residual networks, we change this paradigm into saying that each individual function (or layer) only learns the difference between the input to the function and the expected output. That is where the name residual connections came from. So, if <em class="italic">h</em><sub class="subscript">1</sub> is the desired output and <em class="italic">x</em> is the input, then <em class="italic">M</em><sub class="subscript">1</sub>(<em class="italic">x</em>) = <em class="italic">h</em><sub class="subscript">1</sub> - <em class="italic">x</em>. Rewriting that, we get <em class="italic">h</em><sub class="subscript">1</sub> = <em class="italic">M</em><sub class="subscript">1</sub>(<em class="italic">x</em>) + <em class="italic">x</em>. And this is what is most commonly used as residual connections.</p>
    <p class="normal">Among many benefits such as better gradient flows, residual connections also make the loss surface smooth (Li et al. 2018, Reference <em class="italic">7</em>) and more amenable to gradient-based optimization. For more details and intuition around residual networks, I urge you to check out the blog linked in the <em class="italic">Further reading</em> section.</p>
    <p class="normal">So, the <em class="italic">Add</em> in the <em class="italic">Add and Norm</em> block in the Transformer is actually the residual connection.</p>
    <h4 class="heading-4">Layer normalization</h4>
    <p class="normal">Normalization<a id="_idIndexMarker1144"/> in <strong class="keyWord">deep neural networks</strong> (<strong class="keyWord">DNNs</strong>) has been an active field of research. Among many <a id="_idIndexMarker1145"/>benefits, normalization<a id="_idIndexMarker1146"/> leads to faster training, higher learning rates, and even a bit of regularization. Batch normalization is the <a id="_idIndexMarker1147"/>most common normalization technique in use, typically in CV, which makes the input have approximately zero mean and unit variance by subtracting the input mean in the current batch and dividing it by the standard deviation.</p>
    <p class="normal">But in NLP, researchers <a id="_idIndexMarker1148"/>prefer layer normalization, where the normalization is happening in each feature. The difference can be seen in <em class="italic">Figure 14.17</em>:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_17.png" alt="Figure 14.17 – Batch normalization versus layer normalization "/></figure>
    <p class="packt_figref">Figure 14.17: Batch normalization versus layer normalization</p>
    <p class="normal">This preference for layer normalization emerged empirically, but there has been research about the reason for this preference. NLP data usually has a higher variance as opposed to CV data, and this variance causes some problems for batch normalization. Layer normalization, on the other hand, is immune to this because it doesn’t rely on batch-level variance.</p>
    <p class="normal">Either way, Vaswani et al. decided to use layer normalization in their <em class="italic">Add and Norm</em> block.</p>
    <p class="normal">Now, we know the <em class="italic">Add and Norm</em> block is nothing but a residual connection that is then passed through a layer normalization. So, we can see that the position-encoded inputs are first <a id="_idIndexMarker1149"/>used in the MHA layer, and the output from the MHA is added with the position-encoded inputs again and passed through a layer normalization. Now, this output is passed through the position-wise feed-forward network and another <em class="italic">Add and Norm</em> layer, and this becomes one block of the encoder. An important point to keep in mind is that<a id="_idIndexMarker1150"/> the architecture of all the elements in the encoder is designed in such a way that the dimension of the input at each position is preserved throughout. In other words, if the embedding vector is of dimension 100, the output from the encoder will also have a dimension of 100. This is a convenient way to make it possible to have residual connections and stack as many layers on top of each other as possible. Now, there are multiple such encoder blocks stacked on top of each other to form the encoder of the Transformer.</p>
    <h3 id="_idParaDest-316" class="heading-3">Decoder</h3>
    <p class="normal">The decoder block <a id="_idIndexMarker1151"/>is also very similar to the encoder block, but <a id="_idIndexMarker1152"/>with one key addition. Instead of a single self-attention layer, the decoder block has a self-attention layer, which operates on the decoder input, and an encoder-decoder attention layer. The encoder-decoder attention layer takes the query from the decoder at each stage and the key and values from the top encoder block.</p>
    <p class="normal">There is something peculiar to the self-attention that is applied in the decoder block. Let’s see what that is.</p>
    <h4 class="heading-4">Masked self-attention</h4>
    <p class="normal">We talked about <a id="_idIndexMarker1153"/>how the Transformer can process sequences in parallel and be computationally efficient. But the decoding paradigm poses another challenge. Suppose we have an input sequence, <em class="italic">X</em> = {<em class="italic">x</em><sub class="subscript">1</sub>, <em class="italic">x</em><sub class="subscript">2</sub>, …, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">n</sub>}, and the task is to predict the next step. So, in the decoder, if we give the sequence, <em class="italic">X</em>, because of the parallel-processing architecture, each sequence is processed at once using self-attention. And we know self-attention is agnostic to sequence order. If left unrestricted, the model will cheat by using information from the future timesteps to predict the current timestep. This is where masked attention becomes important.</p>
    <p class="normal">We saw earlier in the <em class="italic">Self-attention</em> section how to calculate a square matrix (if the query and key have the same length) of attention weights, and it is with these weights that we combine the information from the value vector. This self-attention has no concept of <a id="_idIndexMarker1154"/>temporality, and all the tokens will attend to all other tokens irrespective of their position. </p>
    <p class="normal">Let’s see <em class="italic">Figure 14.18</em> to solidify our understanding:</p>
    <figure class="mediaobject"><img src="../Images/Image3820.jpg" alt="Figure 14.18 – Masked self-attention "/></figure>
    <p class="packt_figref">Figure 14.18: Masked self-attention</p>
    <p class="normal">We have the sequence, <em class="italic">X</em> = {<em class="italic">x</em><sub class="subscript">1</sub>, <em class="italic">x</em><sub class="subscript">2</sub>, …, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">5</sub>}, and we are still trying to predict one step ahead. So, the expected output from the decoder would be <img src="../Images/B22389_14_018.png" alt=""/>. When we use self-attention, the attention weights that will be learned will be a square matrix of 5 X 5 dimension. But if we look at the upper triangle of the square matrix (the part that is shaded in <em class="italic">Figure 14.18</em>), those combinations of tokens violate the temporal sanctity.</p>
    <p class="normal">We can take care of this simply by adding a pre-generated mask that has zeros in all the white cells and <em class="italic">-inf</em> in all the shaded cells to the generated attention energies (the stage before applying <em class="italic">softmax</em>). This makes sure the attention weights for the shaded region will be zero, and this in turn ensures that no future information is used while calculating the weighted sum of the value vector.</p>
    <p class="normal">Now, to wrap everything up, the output from the decoder is passed to a standard task-specific head to generate the output we desire.</p>
    <p class="normal">We discussed the Transformer in the context of NLP, but it is a very small leap to adapt it to time series data.</p>
    <h2 id="_idParaDest-317" class="heading-2">Transformers in time series</h2>
    <p class="normal">Time series <a id="_idIndexMarker1155"/>have a lot of similarities with NLP because of the fact that both deal with information in sequences and in both cases the order of elements matters. In time series, the elements are typically time-ordered data points, while in NLP, the elements are tokens (such as words or characters) that form sentences or documents. This can be further evidenced by the phenomenon that most of the popular techniques that are used in NLP are promptly adapted to a time series context. Transformers are no exception to that.</p>
    <p class="normal">Instead of looking at tokens at each position, we have real numbers in each position. And instead of talking about input embeddings, we can talk in terms of input features. The vector of features at each timestep can be considered the equivalent of an embedding vector in NLP. And instead of making causal decoding an optional step (in NLP, that really depends on the task at hand), we have a strict requirement for causal decoding. There, it is trivial to adapt Transformers to time series, although in practice, there are many challenges because in time series we typically encounter sequences that are much longer than the ones in NLP, and this creates a problem because the complexity of self-attention is scaled quadratically with respect to the input sequence length. There have been many alternate proposals for self-attention that make it feasible to use it for long sequences as well, and we will be covering a few of them in <em class="chapterRef">Chapter 16</em>, <em class="italic">Specialized Deep Learning Architectures for Forecasting</em>.</p>
    <p class="normal">Now, let’s try to put everything we have learned about Transformers into practice.</p>
    <h1 id="_idParaDest-318" class="heading-1">Forecasting with Transformers</h1>
    <p class="normal">For <a id="_idIndexMarker1156"/>some continuity, we will use the same household example we were forecasting with RNNs and RNNs with attention.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">03-Transformers.ipynb</code> in the <code class="inlineCode">Chapter14</code> folder and the code in the <code class="inlineCode">src</code> folder.</p>
    </div>
    <p class="normal">Although we learned <a id="_idIndexMarker1157"/>about the vanilla Transformer as a model with an encoder-decoder architecture, it was really designed for language translation tasks. In language translation, the source sequence and target sequence are quite different, and therefore the encoder-decoder architecture made sense. But soon after, researchers figured out that using the decoder part of the Transformer alone does well. It is called a decoder-only Transformer in literature. The naming is a bit confusing because if you think about it, the decoder is different from the encoder in two ways—masked self-attention and encoder-decoder attention. So, in a decoder-only Transformer, how do we make up for the encoder-decoder attention? The short answer is that we don’t. The architecture of the decoder-only Transformer resembles the encoder block more, but we call it decoder-only because we use masked self-attention to make our model respect the temporal sanctity of our sequences.</p>
    <p class="normal">We are also going to implement a decoder-only Transformer. The first thing we need to do is to define a config class, <code class="inlineCode">TransformerConfig</code>, with the following parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">input_size</code>: This parameter defines the number of features the Transformer is expecting.</li>
      <li class="bulletList"><code class="inlineCode">d_model</code>: This parameter defines the hidden dimension of the Transformer or the dimension over which all the attention calculation and subsequent operations happen.</li>
      <li class="bulletList"><code class="inlineCode">n_heads</code>: This parameter defines how many heads we have in the MHA mechanism.</li>
      <li class="bulletList"><code class="inlineCode">n_layers</code>: This parameter defines how many blocks of encoders we are going to stack on top of each other.</li>
      <li class="bulletList"><code class="inlineCode">ff_multiplier</code>: This parameter defines the scale of expansion within the position-wise feed-forward layers.</li>
      <li class="bulletList"><code class="inlineCode">activation</code>: This parameter lets us define which activation we need to use in the position-wise feed-forward layers. It can be either <code class="inlineCode">relu</code> or <code class="inlineCode">gelu</code>.</li>
      <li class="bulletList"><code class="inlineCode">multi_step_horizon</code>: This parameter lets us define how many timesteps into the future we should be forecasting.</li>
      <li class="bulletList"><code class="inlineCode">dropout</code>: This parameter lets us define the magnitude of dropout regularization to be applied in the Transformer model.</li>
      <li class="bulletList"><code class="inlineCode">learning_rate</code>: This defines the learning rate of the optimization procedure.</li>
      <li class="bulletList"><code class="inlineCode">optimizer_params</code>, <code class="inlineCode">lr_scheduler</code>, <code class="inlineCode">lr_scheduler_params</code>: These are parameters that let us tweak the optimization procedure. Let’s not worry about these for now because all of them have been set to intelligent defaults.</li>
    </ul>
    <p class="normal">Now, we<a id="_idIndexMarker1158"/> are going to inherit the <code class="inlineCode">BaseModel</code> class we defined in <code class="inlineCode">src/dl/models.py</code> and define a <code class="inlineCode">TransformerModel</code> class.</p>
    <p class="normal">The first method we need to implement is <code class="inlineCode">_build_network</code>. The entire model can be found in <code class="inlineCode">src/dl/models.py</code>, but we will be covering the important aspects here as well.</p>
    <p class="normal">The first module we need to define is a linear projection layer that takes in the <code class="inlineCode">input_size</code> parameter and projects it into <code class="inlineCode">d_model</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-variable">self</span>.input_projection = nn.Linear(
            <span class="hljs-variable">self</span>.hparams.input_size, <span class="hljs-variable">self</span>.hparams.d_model, bias=<span class="hljs-literal">False</span>
        )
</code></pre>
    <p class="normal">This is an additional step we have introduced to adapt the Transformers to the time series forecasting paradigm. In the vanilla Transformer, this is not needed because each word is represented by an embedding vector that typically has dimensions such as 200 or 500. But while doing time series forecasting, we might have to do the forecasting with just one feature (which is the history), and this seriously restricts our ability to provide capacity to the model because, without the projection layer, <code class="inlineCode">d_model</code> can only be equal to <code class="inlineCode">input_size</code>. Therefore, we have introduced a linear projection layer that decouples the number of features available and <code class="inlineCode">d_model</code>.</p>
    <p class="normal">Now, we need to have a module that adds positional encoding. We have packaged the same code we saw earlier into a <code class="inlineCode">PyTorch</code> module and added it to <code class="inlineCode">src/dl/models.py</code>. We just use that module and define our positional encoding operator, like so:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-variable">self</span>.pos_encoder = PositionalEncoding(<span class="hljs-variable">self</span>.hparams.d_model)
</code></pre>
    <p class="normal">We said earlier that we are going to use a decoder-only approach to building the model, and for that, we are using the <code class="inlineCode">TransformerEncoderLayer</code> and <code class="inlineCode">TransformerEncoder</code> modules defined in <code class="inlineCode">PyTorch</code>. Just keep in mind that when using these layers, we will be using masked self-attention, and that makes it a decoder-only Transformer. The code is presented here:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-variable">self</span>.encoder_layer = nn.TransformerEncoderLayer(
            d_model=<span class="hljs-variable">self</span>.hparams.d_model,
            nhead=<span class="hljs-variable">self</span>.hparams.n_heads,
            dropout=<span class="hljs-variable">self</span>.hparams.dropout,
            dim_feedforward=<span class="hljs-variable">self</span>.hparams.d_model * <span class="hljs-variable">self</span>.hparams.ff_multiplier,
            activation=<span class="hljs-variable">self</span>.hparams.activation,
            batch_first=<span class="hljs-literal">True</span>,
        )
        <span class="hljs-variable">self</span>.transformer_encoder = nn.TransformerEncoder(
            <span class="hljs-variable">self</span>.encoder_layer, num_layers=<span class="hljs-variable">self</span>.hparams.n_layers
        )
</code></pre>
    <p class="normal">The last<a id="_idIndexMarker1159"/> module we need to define is a linear layer that converts the output from the Transformer into the number of timesteps we are forecasting:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-variable">self</span>.decoder = nn.Sequential(nn.Linear(<span class="hljs-variable">self</span>.hparams.d_model, <span class="hljs-number">100</span>),
            nn.ReLU(),
            nn.Linear(<span class="hljs-number">100</span>, <span class="hljs-variable">self</span>.hparams.multi_step_horizon)
        )
</code></pre>
    <p class="normal">That concludes the definition of the model. Now, let’s define a forward pass in the <code class="inlineCode">forward</code> method.</p>
    <p class="normal">The first step is to generate a mask we need to apply masked self-attention:</p>
    <pre class="programlisting code"><code class="hljs-code">mask = <span class="hljs-variable">self</span>._generate_square_subsequent_mask(x.shape[<span class="hljs-number">1</span>]).to(x.device)
</code></pre>
    <p class="normal">We define the mask to have the same length as the input sequence. <code class="inlineCode">_generate_square_subsequent_mask</code> is a method we have defined that generates a mask. Assuming the sequence length is 5, we can look at the two steps in preparing the mask:</p>
    <pre class="programlisting code"><code class="hljs-code">mask = (torch.triu(torch.ones(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)) == <span class="hljs-number">1</span>).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal"><code class="inlineCode">torch.ones(sz,sz)</code> creates a square matrix filled with ones, and <code class="inlineCode">torch.triu(torch.ones(sz,sz))</code> makes a matrix with a top triangle (including the diagonal) filled with ones and the rest filed with zeros. By using an equality operator with one condition and transposing it, we get a mask that has <code class="inlineCode">True</code> in all the bottom triangles, including the diagonal, and <code class="inlineCode">False</code> everywhere else. The output of the previous statement will be this:</p>
    <pre class="programlisting con"><code class="hljs-con">tensor([[ True, False, False, False, False],
        [ True,  True, False, False, False],
        [ True,  True,  True, False, False],
        [ True,  True,  True,  True, False],
        [ True,  True,  True,  True,  True]])
</code></pre>
    <p class="normal">We can see that this matrix has <code class="inlineCode">False</code> at all the places where we need to mask attention. Now, all we need to do is to fill all <code class="inlineCode">True</code> instances with <code class="inlineCode">0</code> and all <code class="inlineCode">False</code> instances with <code class="inlineCode">-inf</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">mask = (
                mask.<span class="hljs-built_in">float</span>()
                .masked_fill(mask == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">"-inf"</span>))
                .masked_fill(mask == <span class="hljs-number">1</span>, <span class="hljs-built_in">float</span>(<span class="hljs-number">0.0</span>))
            )
</code></pre>
    <p class="normal">These<a id="_idIndexMarker1160"/> two lines of code are packaged into the <code class="inlineCode">_generate_square_subsequent_mask</code> method, which we can use while training the model.</p>
    <p class="normal">Now that we have created the mask for masked self-attention, let’s start processing the input, <code class="inlineCode">x</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Projecting input dimension to d_model</span>
x_ = <span class="hljs-variable">self</span>.input_projection(x)
<span class="hljs-comment"># Adding positional encoding</span>
x_ = <span class="hljs-variable">self</span>.pos_encoder(x_)
<span class="hljs-comment"># Encoding the input</span>
x_ = <span class="hljs-variable">self</span>.transformer_encoder(x_, mask)
<span class="hljs-comment"># Decoding the input</span>
y_hat = <span class="hljs-variable">self</span>.decoder(x_)
</code></pre>
    <p class="normal">In these four lines of code, we project the input to <code class="inlineCode">d_model</code> dimensions, add positional encoding, pass it through the Transformer model, and lastly, use the linear layer to convert the output to the predictions we want.</p>
    <p class="normal">Now we have <code class="inlineCode">y_hat</code>, which is the prediction from the model. All we need to think of now is how to train this output to be the desired output.</p>
    <p class="normal">We know that the Transformer model processes all the tokens in one shot, and if we have <em class="italic">N</em> elements in the sequence, we will have <em class="italic">N</em> predictions as well (each prediction corresponding to the next timestep). And if each prediction is for the next H timesteps, the shape of <code class="inlineCode">y_hat</code> would be (<em class="italic">B</em>, <em class="italic">N</em>, <em class="italic">H</em>), where <em class="italic">B</em> is the batch size. There are a few ways we can use this output to compare with the target. The most simple and naïve way is to just take the prediction from the last position (which will have <em class="italic">H</em> timesteps) and compare it with <code class="inlineCode">y</code> (which also has <em class="italic">H</em> timesteps).</p>
    <p class="normal">But this is not <a id="_idIndexMarker1161"/>the most efficient way of using all the information we have, is it? We are discarding <em class="italic">N-1</em> predictions and not giving any signal to the model on all those <em class="italic">N-1</em> predictions. So, while training, it makes sense to use all these <em class="italic">N-1</em> predictions also so that the model has a much richer signal feeding back while learning.</p>
    <p class="normal">We can do that by using the original input sequence, <code class="inlineCode">x</code>, but offsetting it by one. When <em class="italic">H=1</em>, we can think of this as a simple task where each position’s prediction is compared with the target for the next position (one step ahead). We can easily accomplish this by concatenating <code class="inlineCode">x[:,1:,:]</code> (the input sequence offset by 1) with <code class="inlineCode">y</code> (the original target) and treating this as the target. But when <em class="italic">H</em> &gt; 1, this becomes slightly complicated, but we can still do it by using a helpful function from <code class="inlineCode">PyTorch</code> called <code class="inlineCode">unfold</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">y = torch.cat([x[:, <span class="hljs-number">1</span>:, :], y], dim=<span class="hljs-number">1</span>).squeeze(-<span class="hljs-number">1</span>).unfold(<span class="hljs-number">1</span>, y.size(<span class="hljs-number">1</span>), <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">We first concatenate the input sequence (offset by one) with <code class="inlineCode">y</code> and then use <code class="inlineCode">unfold</code> to create sliding windows of <em class="italic">size</em> = <em class="italic">H</em>. This gives us a target of the same shape, (<em class="italic">B</em>, <em class="italic">N</em>, <em class="italic">H</em>).</p>
    <p class="normal">But during inference (when we are predicting using a trained model), we do not need the output of all the other positions, and hence we discard them, as shown here:</p>
    <pre class="programlisting code"><code class="hljs-code">y_hat = y_hat[:, -<span class="hljs-number">1</span>, :].unsqueeze(<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Our <code class="inlineCode">BaseModel</code> class that we defined also lets us define a slightly different prediction step by using the <code class="inlineCode">predict</code> method. You can look over the complete model in <code class="inlineCode">src/dl/models.py</code> once again to solidify your understanding now.</p>
    <p class="normal">Now that we have defined the model, we can use the same framework we have been using to train <code class="inlineCode">TransformerModel</code>. The full code is available in the notebook, but we will just look at a summary table with the results:</p>
    <figure class="mediaobject"><img src="../Images/B22389_14_19.png" alt="Figure 14.19 – Metrics for Transformer model on MAC000193 household "/></figure>
    <p class="packt_figref">Figure 14.19: Metrics for Transformer model on MAC000193 household</p>
    <p class="normal">We can see that<a id="_idIndexMarker1162"/> the model is not doing as well as its RNN cousins. There could be many reasons for this, but the most probable one is that Transformers are really data-hungry. Transformers have far fewer inductive biases and therefore only shine where there is lots of data available to learn from. When forecasting just one household alone, our model has access to far less data and may not work very well. This is true, to an extent, for all the DL models we have seen so far. In <em class="chapterRef">Chapter 10</em>, <em class="italic">Global Forecasting Models</em>, we talked about how we can train a single model for multiple households together, but that discussion was limited to classical ML models. DL is also perfectly capable of global forecasting models and that is exactly what we will be talking about in the next chapter—<em class="chapterRef">Chapter 15</em>, <em class="italic">Strategies for Global Deep Learning Forecasting Models</em>.</p>
    <p class="normal">For now, congratulations on getting through another concept-heavy and information-packed chapter. The concept of attention, which has taken the field by storm, should be clearer in your mind now than when you started the chapter. I urge you to take a second stab at the chapter, read through the <em class="italic">Further reading</em> section, and do some of your own research if it’s not clear because the future chapters assume you understand this.</p>
    <h1 id="_idParaDest-319" class="heading-1">Summary</h1>
    <p class="normal">We have been storming through the world of DL in the last few chapters. We started off with the basic premise of DL, what it is, and why it became so popular. Then, we saw a few common building blocks that are typically used in time series forecasting and got our hands dirty learning how we can put what we have learned into practice using PyTorch. Although we talked about RNNs, LSTMs, GRUs, and so on, we purposefully left out attention and Transformers because they deserved a separate chapter.</p>
    <p class="normal">We started the chapter by learning about the generalized attention model, helping you put a framework around all the different schemes of attention out there, and then went into detail on a few common attention schemes, such as scaled dot product, additive, and general attention. Right after incorporating attention into the Seq2Seq models we were playing with in <em class="chapterRef">Chapter 12</em>, <em class="italic">Building Blocks of Deep Learning for Time Series</em>, we started with the Transformer. </p>
    <p class="normal">We examined all the building blocks and architecture decisions involved in the original Transformer from the point of view of NLP, and after understanding the architecture, we adapted it to a time-series setting.</p>
    <p class="normal">And finally, we capped it off by training a Transformer model for forecasting on a sample household. And now, by finishing this chapter, we have all the basic ingredients to really start using DL for time series forecasting.</p>
    <p class="normal">In the next chapter, we are going to elevate what we have been doing and move on to the global forecasting model paradigm.</p>
    <h1 id="_idParaDest-320" class="heading-1">References</h1>
    <p class="normal">Following is the list of the references used in this chapter:</p>
    <ol>
      <li class="numberedList" value="1">Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio (2015). <em class="italic">Neural Machine Translation by Jointly Learning to Align and Translate</em>. In <em class="italic">3rd International Conference on Learning Representations</em>. <a href="https://arxiv.org/pdf/1409.0473.pdf"><span class="url">https://arxiv.org/pdf/1409.0473.pdf</span></a></li>
      <li class="numberedList">Thang Luong, Hieu Pham, and Christopher D. Manning (2015). <em class="italic">Effective Approaches to Attention-based Neural Machine Translation</em>. In <em class="italic">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</em>. <a href="https://aclanthology.org/D15-1166/"><span class="url">https://aclanthology.org/D15-1166/</span></a></li>
      <li class="numberedList">André F. T. Martins, Ramón Fernandez Astudillo (2016). <em class="italic">From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification</em>. In <em class="italic">Proceedings of the 33rd International Conference on Machine Learning</em>. <a href="http://proceedings.mlr.press/v48/martins16.html"><span class="url">http://proceedings.mlr.press/v48/martins16.html</span></a></li>
      <li class="numberedList">Ben Peters, Vlad Niculae, André F. T. Martins (2019). <em class="italic">Sparse Sequence-to-Sequence Models</em>. In <em class="italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>. <a href="https://aclanthology.org/P19-1146/"><span class="url">https://aclanthology.org/P19-1146/</span></a></li>
      <li class="numberedList">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin (2017). <em class="italic">Attention is All you Need</em>. In <em class="italic">Advances in Neural Information Processing Systems</em>. <a href="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html"><span class="url">https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</span></a></li>
      <li class="numberedList">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova (2019). <em class="italic">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>. In <em class="italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>. <a href="https://aclanthology.org/N19-1423/"><span class="url">https://aclanthology.org/N19-1423/</span></a></li>
      <li class="numberedList">Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein (2018). <em class="italic">Visualizing the Loss Landscape of Neural Nets</em>. In <em class="italic">Advances in Neural Information Processing Systems</em>. <a href="https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf</span></a></li>
      <li class="numberedList">Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and Rohan Ramanath (2021). <em class="italic">An Attentive Survey of Attention Models</em>. <em class="italic">ACM Trans. Intell. Syst. Technol. 12, 5, Article 53 (October 2021)</em>. <a href="https://doi.org/10.1145/3465055"><span class="url">https://doi.org/10.1145/3465055</span></a></li>
    </ol>
    <h1 id="_idParaDest-321" class="heading-1">Further reading</h1>
    <p class="normal">Here are a few resources for further reading:</p>
    <ul>
      <li class="bulletList"><em class="italic">The Illustrated Transformer</em> by <em class="italic">Jay Alammar</em>: <a href="https://jalammar.github.io/illustrated-transformer/"><span class="url">https://jalammar.github.io/illustrated-transformer/</span></a></li>
      <li class="bulletList"><em class="italic">Transformer Networks: A mathematical explanation why scaling the dot products leads to more stable gradients</em>: <a href="https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500"><span class="url">https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500</span></a></li>
      <li class="bulletList"><em class="italic">Why is Bahdanau’s attention sometimes called concat attention?</em>: <a href="https://stats.stackexchange.com/a/524729"><span class="url">https://stats.stackexchange.com/a/524729</span></a></li>
      <li class="bulletList"><em class="italic">Noam Shazeer</em> (2020). <em class="italic">GLU Variants Improve Transformer.</em> arXiv preprint: <em class="italic">Arxiv-2002.05202</em>. <a href="https://arxiv.org/abs/2002.05202"><span class="url">https://arxiv.org/abs/2002.05202</span></a></li>
      <li class="bulletList"><em class="italic">What is Residual Connection?</em> by <em class="italic">Wanshun Wong</em>: <a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55"><span class="url">https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55</span></a></li>
      <li class="bulletList"><em class="italic">Attn: Illustrated Attention</em> by <em class="italic">Raimi Karim</em>: <a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3"><span class="url">https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3</span></a></li>
    </ul>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
  </div>
</body></html>