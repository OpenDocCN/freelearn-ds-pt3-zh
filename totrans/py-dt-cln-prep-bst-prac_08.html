<html><head></head><body>
		<div id="_idContainer081">
			<h1 id="_idParaDest-158" class="chapter-number"><a id="_idTextAnchor195"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor196"/><span class="koboSpan" id="kobo.2.1">Detecting and Handling Missing Values and Outliers</span></h1>
			<p><span class="koboSpan" id="kobo.3.1">This chapter discusses the techniques of handling missing values and outliers, two critical challenges that can significantly impact the integrity and accuracy of our data products. </span><span class="koboSpan" id="kobo.3.2">We will explore a wide range of techniques to identify and manage these data irregularities, ranging from statistical methods to advanced machine learning models. </span><span class="koboSpan" id="kobo.3.3">Through practical examples and real-world datasets, we will present strategies to tackle these issues head-on, ensuring that our analyses are robust, reliable, and capable of generating </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">meaningful insights.</span></span></p>
			<p><span class="koboSpan" id="kobo.5.1">The key points for the chapter are </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">as follows:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.7.1">Detecting and handling </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">missing data</span></span></li>
				<li><span class="koboSpan" id="kobo.9.1">Detecting univariate and </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">multivariate outliers</span></span></li>
				<li><span class="koboSpan" id="kobo.11.1">Handling univariate and </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">multivariate outliers</span></span></li>
			</ul>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor197"/><span class="koboSpan" id="kobo.13.1">Technical requirements</span></h1>
			<p><span class="koboSpan" id="kobo.14.1">You can find all the code for the chapter in the link </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">that follows:</span></span></p>
			<p><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter08"><span class="No-Break"><span class="koboSpan" id="kobo.16.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter08</span></span></a><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices "/></p>
			<p><span class="koboSpan" id="kobo.17.1">The different code files follow the names of the different parts of the chapters. </span><span class="koboSpan" id="kobo.17.2">Let's install the </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">following library:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.19.1">
pip install spacy==3.7.5</span></pre>			<h1 id="_idParaDest-161"><a id="_idTextAnchor198"/><span class="koboSpan" id="kobo.20.1">Detecting missing data</span></h1>
			<p><span class="koboSpan" id="kobo.21.1">Missing data is a</span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.22.1"> common and inevitable issue in real-world datasets. </span><span class="koboSpan" id="kobo.22.2">It occurs when one or more values are absent in a particular observation or record. </span><span class="koboSpan" id="kobo.22.3">This data gap can greatly impact the validity and reliability of any analysis or model built with those data. </span><span class="koboSpan" id="kobo.22.4">As we say in the data world: </span><em class="italic"><span class="koboSpan" id="kobo.23.1">garbage in, garbage out</span></em><span class="koboSpan" id="kobo.24.1">, meaning that if your data is not correct, then the models or analysis created with that data will not be </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">correct either.</span></span></p>
			<p><span class="koboSpan" id="kobo.26.1">In the following parts, we will use a scenario to demonstrate how to detect missing data and how the different imputation methods work. </span><span class="koboSpan" id="kobo.26.2">The scenario is </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">the following:</span></span></p>
			<p><em class="italic"><span class="koboSpan" id="kobo.28.1">Imagine you are analyzing a dataset containing information about students, including their ages and test scores. </span><span class="koboSpan" id="kobo.28.2">However, due to various reasons, some ages and test scores </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.29.1">are missing.</span></em></span></p>
			<p><span class="koboSpan" id="kobo.30.1">The code for this section can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/1.detect_missing_data.py"><span class="No-Break"><span class="koboSpan" id="kobo.32.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/1.detect_missing_data.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.33.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.34.1">In this script, we create the data that we will use across the chapter. </span><span class="koboSpan" id="kobo.34.2">Let’s start with the </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">import statements:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.36.1">
import pandas as pd</span></pre>			<p><span class="koboSpan" id="kobo.37.1">Let’s generate student data with missing ages and test scores. </span><span class="koboSpan" id="kobo.37.2">This dictionary data contains two keys, </span><strong class="source-inline"><span class="koboSpan" id="kobo.38.1">Age</span></strong><span class="koboSpan" id="kobo.39.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.40.1">Test_Score</span></strong><span class="koboSpan" id="kobo.41.1">, each with a list of values. </span><span class="koboSpan" id="kobo.41.2">Some of these values are </span><strong class="source-inline"><span class="koboSpan" id="kobo.42.1">None</span></strong><span class="koboSpan" id="kobo.43.1">, indicating </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">missing data:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.45.1">
data = {
    'Age': [18, 20, None, 22, 21, 19, None, 23, 18, 24, 40, 41, 45, None, 34, None, 25, 30, 32, 24, 35, 38, 76, 90],
    'Test_Score': [85, None, 90, 92, None, 88, 94, 91, None, 87, 75, 78, 80, None, 74, 20, 50, 68, None, 58, 48, 59, 10, 5]}
df = pd.DataFrame(data)</span></pre>			<p><span class="koboSpan" id="kobo.46.1">The first five rows of the dataset are </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">as follows:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.48.1">
     Age  Test_Score
0   18.0        85.0
1   20.0         NaN
2    NaN        90.0
3   22.0        92.0
4   21.0         NaN</span></pre>			<p><span class="koboSpan" id="kobo.49.1">As we can</span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.50.1"> see, there are NaN values in both columns of the dataset. </span><span class="koboSpan" id="kobo.50.2">To understand the extent of the missing values in the dataset, let’s count how many we have across the </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">whole DataFrame:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.52.1">
missing_values = df.isnull()</span></pre>			<p><span class="koboSpan" id="kobo.53.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.54.1">df.isnull()</span></strong><span class="koboSpan" id="kobo.55.1"> method creates a </span><strong class="source-inline"><span class="koboSpan" id="kobo.56.1">missing_values</span></strong><span class="koboSpan" id="kobo.57.1"> DataFrame of the same shape as </span><strong class="source-inline"><span class="koboSpan" id="kobo.58.1">df</span></strong><span class="koboSpan" id="kobo.59.1">, where each cell is </span><strong class="source-inline"><span class="koboSpan" id="kobo.60.1">True</span></strong><span class="koboSpan" id="kobo.61.1"> if the corresponding cell in </span><strong class="source-inline"><span class="koboSpan" id="kobo.62.1">df</span></strong><span class="koboSpan" id="kobo.63.1"> is </span><strong class="source-inline"><span class="koboSpan" id="kobo.64.1">None</span></strong><span class="koboSpan" id="kobo.65.1"> (missing value) and </span><strong class="source-inline"><span class="koboSpan" id="kobo.66.1">False</span></strong><span class="koboSpan" id="kobo.67.1"> otherwise, </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">as shown:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.69.1">
      Age  Test_Score
0   False       False
1   False        True
2    True       False
3   False       False
4   False        True</span></pre>			<p><span class="koboSpan" id="kobo.70.1">In the previous DataFrame, any cell that contained a </span><strong class="source-inline"><span class="koboSpan" id="kobo.71.1">NaN</span></strong><span class="koboSpan" id="kobo.72.1"> value is now replaced with </span><strong class="source-inline"><span class="koboSpan" id="kobo.73.1">True</span></strong><span class="koboSpan" id="kobo.74.1">. </span><span class="koboSpan" id="kobo.74.2">Having the data in that format helps us calculate how many </span><strong class="source-inline"><span class="koboSpan" id="kobo.75.1">NaN</span></strong><span class="koboSpan" id="kobo.76.1"> values </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">we have:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.78.1">
null_rows_count = missing_values.any(axis=1).sum()
print("Count of Rows with at least one Missing Value:", null_rows_count)
print(8/len(df))</span></pre>			<p><span class="koboSpan" id="kobo.79.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.80.1">missing_values.any(axis=1)</span></strong><span class="koboSpan" id="kobo.81.1"> argument checks each row to see whether it contains any missing values, returning a Series of </span><strong class="source-inline"><span class="koboSpan" id="kobo.82.1">True</span></strong><span class="koboSpan" id="kobo.83.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.84.1">False</span></strong><span class="koboSpan" id="kobo.85.1"> for each row. </span><span class="koboSpan" id="kobo.85.2">Then the </span><strong class="source-inline"><span class="koboSpan" id="kobo.86.1">.sum()</span></strong><span class="koboSpan" id="kobo.87.1"> counts the number of </span><strong class="source-inline"><span class="koboSpan" id="kobo.88.1">True</span></strong><span class="koboSpan" id="kobo.89.1"> values in this Series, giving the number of rows with at least one </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">missing value:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.91.1">
Count of Rows with at least one Missing Value: 8
% of rows with at least one missing value: 33%</span></pre>			<p><span class="koboSpan" id="kobo.92.1">Now we know </span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.93.1">how much data is missing from our dataset. </span><span class="koboSpan" id="kobo.93.2">The next goal of this exercise is to find the best imputation method to </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">fill those.</span></span></p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.95.1">Handling missing data</span></h1>
			<p><span class="koboSpan" id="kobo.96.1">Addressing </span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.97.1">missing data involves making careful decisions to minimize its impact on analyses and models. </span><span class="koboSpan" id="kobo.97.2">The most common strategies include </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">the following:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.99.1">Removing records with </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">missing values</span></span></li>
				<li><span class="koboSpan" id="kobo.101.1">Filling in missing values using various techniques such as mean, median, mode imputation, or more advanced methods such as regression-based imputation or k-nearest </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">neighbors imputation</span></span></li>
				<li><span class="koboSpan" id="kobo.103.1">Introducing binary indicator variables to flag missing data; this can inform models about the presence of </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">missing values</span></span></li>
				<li><span class="koboSpan" id="kobo.105.1">Leveraging subject matter expertise to understand the reasons for missing data and make informed decisions about how to </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">handle it</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.107.1">Let’s deep dive into each of these methods and observe in detail the results on the dataset presented in the </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">previous part.</span></span></p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.109.1">Deletion of missing data</span></h2>
			<p><span class="koboSpan" id="kobo.110.1">One</span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.111.1"> approach to handling missing data is to simply remove records (rows) that contain missing values. </span><span class="koboSpan" id="kobo.111.2">It is a quick and simple strategy, and is generally more suitable when the percentage of missing data is </span><em class="italic"><span class="koboSpan" id="kobo.112.1">low</span></em><span class="koboSpan" id="kobo.113.1"> and the missing data appears in </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">random places.</span></span></p>
			<p><span class="koboSpan" id="kobo.115.1">Before we start deleting data, we need to understand our dataset a bit better. </span><span class="koboSpan" id="kobo.115.2">Continuing on the data created in the previous example, let’s print the descriptive statistics first before we start deleting data points. </span><span class="koboSpan" id="kobo.115.3">The code for this part can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/2.delete_missing_data.py"><span class="No-Break"><span class="koboSpan" id="kobo.117.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/2.delete_missing_data.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.118.1">.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.119.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.120.1">To keep the chapter to a nice number of pages, we have only presented the key code snippets. </span><span class="koboSpan" id="kobo.120.2">To see all the examples, please go to </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">the repository.</span></span></p>
			<p><span class="koboSpan" id="kobo.122.1">To create </span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.123.1">the descriptive statistics, we can simply call the </span><strong class="source-inline"><span class="koboSpan" id="kobo.124.1">.describe()</span></strong><span class="koboSpan" id="kobo.125.1"> method </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">in pandas:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.127.1">
print(df.describe())</span></pre>			<p><span class="koboSpan" id="kobo.128.1">The descriptive statistics are </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">presented here:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.130.1">
             Age  Test_Score
count  20.000000   19.000000
mean   33.750000   65.894737
std    18.903843   27.989869
min    18.000000    5.000000
25%    21.750000   54.000000
50%    27.500000   75.000000
75%    38.500000   87.500000
max    90.000000   94.000000</span></pre>			<p><span class="koboSpan" id="kobo.131.1">Let’s also create the distribution plots for each column of </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">the dataset.</span></span></p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<span class="koboSpan" id="kobo.133.1"><img src="image/B19801_08_1.jpg" alt="Figure 8.1 – Distribution of features before any alteration"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.134.1">Figure 8.1 – Distribution of features before any alteration</span></p>
			<p><span class="koboSpan" id="kobo.135.1">With this </span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.136.1">analysis done, we can get some key insights into the dataset. </span><span class="koboSpan" id="kobo.136.2">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.137.1">Age</span></strong><span class="koboSpan" id="kobo.138.1">, with a count of 20, the average age is approximately 33.7 years, with a standard deviation of 18.9 years, showing moderate variability. </span><span class="koboSpan" id="kobo.138.2">Ages range from 18 to 90 years, with the middle 50% of ages falling between 21.75 and 38.5 years. </span><span class="koboSpan" id="kobo.138.3">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.139.1">Test_Score</span></strong><span class="koboSpan" id="kobo.140.1">, based on 19 values, the mean score is around 65.8, with a higher standard deviation of 27.9, indicating more variability in scores. </span><span class="koboSpan" id="kobo.140.2">Test scores range from 5 to 94, with</span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.141.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.142.1">Interquartile Range</span></strong><span class="koboSpan" id="kobo.143.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.144.1">IQR</span></strong><span class="koboSpan" id="kobo.145.1">) spanning from 54 </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">to 87.5.</span></span></p>
			<p><span class="koboSpan" id="kobo.147.1">Now, let’s have a look at how to delete the missing data. </span><span class="koboSpan" id="kobo.147.2">Let’s pay attention to how the </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">dataset changes:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.149.1">
df_no_missing = df.dropna()</span></pre>			<p><span class="koboSpan" id="kobo.150.1">Let’s explore the distribution of the features after the </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">data deletion:</span></span></p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<span class="koboSpan" id="kobo.152.1"><img src="image/B19801_08_2.jpg" alt="Figure 8.2 – Distribution of features after the data deletion"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.153.1">Figure 8.2 – Distribution of features after the data deletion</span></p>
			<p><span class="koboSpan" id="kobo.154.1">Let’s also have a </span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.155.1">look at the summary statistics for the </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">altered dataset:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.157.1">
print(df_no_missing.describe())
             Age  Test_Score
count  16.000000   16.000000
mean   36.500000   65.500000
std    20.109699   26.610775
min    18.000000    5.000000
25%    23.750000   56.000000
50%    32.000000   74.500000
75%    40.250000   85.500000
max    90.000000   92.000000</span></pre>			<p><span class="koboSpan" id="kobo.158.1">Having seen the descriptive statistics for both datasets, the observed changes are </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">presented here:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.160.1">Count change</span></strong><span class="koboSpan" id="kobo.161.1">: The count of observations has decreased from 20 to 16 for both, age and test scores, after deleting rows with </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">missing values.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.163.1">Mean change</span></strong><span class="koboSpan" id="kobo.164.1">: The mean age has increased from 33.75 to 36.50, while the mean test score has slightly decreased from 65.89 to 65.50. </span><span class="koboSpan" id="kobo.164.2">This change reflects the values present in the remaining dataset after </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">the deletion.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.166.1">Standard deviation change</span></strong><span class="koboSpan" id="kobo.167.1">: The standard deviation for age has increased from 18.90 to 20.11, indicating a greater spread in age, while the standard deviation for test scores has decreased from 27.99 </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">to 26.61.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.169.1">Minimum and maximum values</span></strong><span class="koboSpan" id="kobo.170.1">: The minimum age remains the same at 18, but the minimum test score remains at 5. </span><span class="koboSpan" id="kobo.170.2">The maximum values for both age and test scores have slightly changed, with the maximum test score decreasing from 94 </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">to 92.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.172.1">Percentile changes</span></strong><span class="koboSpan" id="kobo.173.1">: The percentile values (25%, 50%, 75%) have shifted due to the </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">altered dataset:</span></span><ul><li><span class="koboSpan" id="kobo.175.1">The 25th percentile for age has increased from 21.75 to 23.75, and for test scores, from 54.00 </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">to 56.00.</span></span></li><li><span class="koboSpan" id="kobo.177.1">The median (50th percentile) for age has increased from 27.50 to 32.00, while for test scores, it decreased slightly from 75.00 </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">to 74.50.</span></span></li><li><span class="koboSpan" id="kobo.179.1">The 75th percentile for age has increased from 38.50 to 40.25, while for test scores, it decreased from 87.50 </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">to 85.50.</span></span></li></ul></li>
			</ul>
			<p><span class="koboSpan" id="kobo.181.1">The deletion </span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.182.1">of rows with missing values has led to a smaller dataset, and the remaining data now has </span><em class="italic"><span class="koboSpan" id="kobo.183.1">different statistical properties</span></em><span class="koboSpan" id="kobo.184.1">. </span><span class="koboSpan" id="kobo.184.2">This method is suitable when the missing values are deemed to be a small proportion of the dataset and removing them does not significantly impact </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">the data.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.186.1">What is a small proportion though?</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.187.1">A common rule of thumb is that if less than 5% of the data is missing, it is often considered a small proportion, and deletion might not significantly impact the analysis. </span><span class="koboSpan" id="kobo.187.2">The significance of the change caused by deleting data points can be assessed by comparing the results of analyses with and without the missing data. </span><span class="koboSpan" id="kobo.187.3">If the results are consistent, the deletion might not </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">be significant.</span></span></p>
			<p><span class="koboSpan" id="kobo.189.1">In these cases of substantial missing data, other imputation methods or advanced techniques may be more appropriate as we will explore in the </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">next part.</span></span></p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.191.1">Imputation of missing data</span></h1>
			<p><span class="koboSpan" id="kobo.192.1">Imputation</span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.193.1"> is often used when removing missing records would result in significant information loss. </span><span class="koboSpan" id="kobo.193.2">Imputation involves filling in missing values with estimated or calculated values. </span><span class="koboSpan" id="kobo.193.3">Common imputation methods include mean, median, and mode imputation, or using more </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">advanced techniques.</span></span></p>
			<p><span class="koboSpan" id="kobo.195.1">Let’s have a look at the different imputation methods for </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">our scenario.</span></span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.197.1">Mean imputation</span></h2>
			<p><span class="koboSpan" id="kobo.198.1">Mean imputation </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.199.1">fills missing values with </span><em class="italic"><span class="koboSpan" id="kobo.200.1">the mean of the observed values</span></em><span class="koboSpan" id="kobo.201.1"> in the variable. </span><span class="koboSpan" id="kobo.201.2">It is a very simple method, and it does not</span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.202.1"> introduce bias when the values missing are completely random. </span><span class="koboSpan" id="kobo.202.2">However, this method is sensitive to outliers, and it may distort the distribution of the feature. </span><span class="koboSpan" id="kobo.202.3">You can find the code for this part in the repo </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/3.mean_imputation.py"><span class="No-Break"><span class="koboSpan" id="kobo.204.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/3.mean_imputation.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.205.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.206.1">Let’s see the code example for mean imputation. </span><span class="koboSpan" id="kobo.206.2">For this example, we will use the same dataset as </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">explained before:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.208.1">
df_mean_imputed = df.copy()
df_mean_imputed['Age'].fillna(round(df['Age'].mean()), inplace=True)</span></pre>			<p><span class="koboSpan" id="kobo.209.1">The preceding line fills any missing values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.210.1">Age</span></strong><span class="koboSpan" id="kobo.211.1"> column with the mean of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.212.1">Age</span></strong><span class="koboSpan" id="kobo.213.1"> column from the original </span><strong class="source-inline"><span class="koboSpan" id="kobo.214.1">df</span></strong><span class="koboSpan" id="kobo.215.1"> DataFrame. </span><span class="koboSpan" id="kobo.215.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.216.1">df['Age'].mean()</span></strong><span class="koboSpan" id="kobo.217.1"> argument calculates the mean of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.218.1">Age</span></strong><span class="koboSpan" id="kobo.219.1"> column, and rounds this mean to the nearest whole number. </span><span class="koboSpan" id="kobo.219.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.220.1">fillna()</span></strong><span class="koboSpan" id="kobo.221.1"> method then replaces any </span><strong class="source-inline"><span class="koboSpan" id="kobo.222.1">NaN</span></strong><span class="koboSpan" id="kobo.223.1"> values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.224.1">Age</span></strong><span class="koboSpan" id="kobo.225.1"> column with this rounded mean. </span><span class="koboSpan" id="kobo.225.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.226.1">inplace=True</span></strong><span class="koboSpan" id="kobo.227.1"> argument ensures that the changes are made directly in </span><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">df_mean_imputed</span></strong><span class="koboSpan" id="kobo.229.1"> without creating a </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">new DataFrame:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.231.1">
df_mean_imputed['Test_Score'].fillna(df['Test_Score'].mean(), inplace=True)</span></pre>			<p><span class="koboSpan" id="kobo.232.1">Similarly, the preceding line fills any missing values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">Test_Score</span></strong><span class="koboSpan" id="kobo.234.1"> column of </span><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">df_mean_imputed</span></strong><span class="koboSpan" id="kobo.236.1"> with the mean of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">Test_Score</span></strong><span class="koboSpan" id="kobo.238.1"> column from the original </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.239.1">df</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.240.1"> DataFrame.</span></span></p>
			<p><span class="koboSpan" id="kobo.241.1">Let’s have a look at the dataset after </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">the imputation:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.243.1">
print(df_mean_imputed)
     Age  Test_Score
0   18.0   85.000000
1   20.0   65.894737
2   34.0   90.000000
3   22.0   92.000000
4   21.0   65.894737
5   19.0   88.000000
6   34.0   94.000000
7   23.0   91.000000
8   18.0   65.894737
9   24.0   87.000000
10  40.0   75.000000</span></pre>			<p><span class="koboSpan" id="kobo.244.1">As we</span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.245.1"> can</span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.246.1"> see, the rounded mean has replaced all the </span><strong class="source-inline"><span class="koboSpan" id="kobo.247.1">NaN</span></strong><span class="koboSpan" id="kobo.248.1"> values for the age feature, whereas the absolute mean (abs mean) has replaced the </span><strong class="source-inline"><span class="koboSpan" id="kobo.249.1">NaN</span></strong><span class="koboSpan" id="kobo.250.1"> values for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.251.1">Test_Score</span></strong><span class="koboSpan" id="kobo.252.1"> column. </span><span class="koboSpan" id="kobo.252.2">We rounded up the mean for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.253.1">Age</span></strong><span class="koboSpan" id="kobo.254.1"> column to make sure it represents </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">something meaningful.</span></span></p>
			<p><span class="koboSpan" id="kobo.256.1">The updated distributions are </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">presented here:</span></span></p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<span class="koboSpan" id="kobo.258.1"><img src="image/B19801_08_3.jpg" alt="Figure 8.3 – Distribution of features after mean imputation"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.259.1">Figure 8.3 – Distribution of features after mean imputation</span></p>
			<p><span class="koboSpan" id="kobo.260.1">We can</span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.261.1"> see </span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.262.1">from the graphs that the distributions have slightly changed for both of the variables. </span><span class="koboSpan" id="kobo.262.2">Let’s have a look at the descriptive statistics of the </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">imputed dataset:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.264.1">
print(df_mean_imputed.describe())
             Age  Test_Score
count  24.000000   24.000000
mean   33.791667   65.894737
std    17.181839   24.761286
min    18.000000    5.000000
25%    22.750000   58.750000
50%    33.000000   66.947368
75%    35.750000   85.500000
max    90.000000   94.000000</span></pre>			<p><span class="koboSpan" id="kobo.265.1">Having seen the descriptive statistics for both datasets, the observed changes are </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">as follows:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.267.1">Count increase</span></strong><span class="koboSpan" id="kobo.268.1">: The count for both </span><strong class="source-inline"><span class="koboSpan" id="kobo.269.1">Age</span></strong><span class="koboSpan" id="kobo.270.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">Test_Score</span></strong><span class="koboSpan" id="kobo.272.1"> increased from 20 to 24 after the imputation, indicating that missing values were </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">successfully imputed.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.274.1">Mean and median changes</span></strong><span class="koboSpan" id="kobo.275.1">: The mean age remained stable, increasing slightly from 33.75 to 33.79. </span><span class="koboSpan" id="kobo.275.2">The mean test score stayed the same at 65.89. </span><span class="koboSpan" id="kobo.275.3">The median age increased from 27.50 to 33.00, reflecting the changes in the distribution of ages. </span><span class="koboSpan" id="kobo.275.4">The median test score slightly decreased from 75.00 </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">to 66.95.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.277.1">Standard deviation changes</span></strong><span class="koboSpan" id="kobo.278.1">: The standard deviation for </span><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">Age</span></strong><span class="koboSpan" id="kobo.280.1"> decreased from 18.90 to 17.18, indicating reduced variability in ages after imputation. </span><span class="koboSpan" id="kobo.280.2">The standard deviation for </span><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">Test_Score</span></strong><span class="koboSpan" id="kobo.282.1"> also decreased from 27.99 to 24.76, reflecting less variability in </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">test scores.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.284.1">Quartile changes</span></strong><span class="koboSpan" id="kobo.285.1">: The </span><strong class="bold"><span class="koboSpan" id="kobo.286.1">First Quartile</span></strong><span class="koboSpan" id="kobo.287.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.288.1">Q1</span></strong><span class="koboSpan" id="kobo.289.1">) (25%) for </span><strong class="source-inline"><span class="koboSpan" id="kobo.290.1">Age</span></strong><span class="koboSpan" id="kobo.291.1"> increased </span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.292.1">from 21.75 to 22.75, and Q1 for </span><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">Test_Score</span></strong><span class="koboSpan" id="kobo.294.1"> increased from 54.00 to 58.75. </span><span class="koboSpan" id="kobo.294.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.295.1">Third Quartile</span></strong><span class="koboSpan" id="kobo.296.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.297.1">Q3</span></strong><span class="koboSpan" id="kobo.298.1">) (75%) for </span><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">Age</span></strong><span class="koboSpan" id="kobo.300.1"> slightly decreased from 38.50 to 35.75, and </span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.301.1">Q3 for </span><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">Test_Score</span></strong><span class="koboSpan" id="kobo.303.1"> remained relatively stable, decreasing slightly from 87.50 </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">to 85.50.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.305.1">The </span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.306.1">mean </span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.307.1">imputation maintained the overall mean values and increased the dataset size by filling in missing values. </span><span class="koboSpan" id="kobo.307.2">However, it has reduced the variability (as indicated by the decreased standard deviation for </span><strong class="source-inline"><span class="koboSpan" id="kobo.308.1">Age</span></strong><span class="koboSpan" id="kobo.309.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.310.1">Test_Score</span></strong><span class="koboSpan" id="kobo.311.1">) and altered the distribution of the data (particularly in the quartiles). </span><span class="koboSpan" id="kobo.311.2">These changes are typical of mean imputation, as it tends ot underestimate variability and smooth out differences in the data, which can impact certain analyses that are sensitive to </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">data distribution.</span></span></p>
			<p><span class="koboSpan" id="kobo.313.1">Now let’s move on to the median imputation to see how this affects </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">the dataset.</span></span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.315.1">Median imputation</span></h2>
			<p><span class="koboSpan" id="kobo.316.1">Median imputation</span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.317.1"> fills the missing values</span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.318.1"> with the median, the middle value of the dataset when it is ordered. </span><span class="koboSpan" id="kobo.318.2">Median imputation is robust in the presence of outliers and can be a good choice when the distribution is skewed. </span><span class="koboSpan" id="kobo.318.3">It can preserve the shape of the distribution unless dealing with complex distribution. </span><span class="koboSpan" id="kobo.318.4">The code for this part can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/4.median_imputation.py"><span class="No-Break"><span class="koboSpan" id="kobo.320.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/4.median_imputation.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.321.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.322.1">Let’s have a look at the code example for the </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">median imputation:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.324.1">
df_median_imputed = df.copy()</span></pre>			<p><span class="koboSpan" id="kobo.325.1">This </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.326.1">following line fills any missing values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.327.1">Age</span></strong><span class="koboSpan" id="kobo.328.1"> column of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.329.1">df_median_imputed</span></strong><span class="koboSpan" id="kobo.330.1"> DataFrame with the median of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">Age</span></strong><span class="koboSpan" id="kobo.332.1"> column from the original </span><strong class="source-inline"><span class="koboSpan" id="kobo.333.1">df</span></strong><span class="koboSpan" id="kobo.334.1"> DataFrame. </span><span class="koboSpan" id="kobo.334.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">df['Age'].median()</span></strong><span class="koboSpan" id="kobo.336.1"> argument </span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.337.1">calculates the median (the middle value) of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.338.1">Age</span></strong><span class="koboSpan" id="kobo.339.1"> column). </span><span class="koboSpan" id="kobo.339.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">fillna()</span></strong><span class="koboSpan" id="kobo.341.1"> method then replaces any </span><strong class="source-inline"><span class="koboSpan" id="kobo.342.1">NaN</span></strong><span class="koboSpan" id="kobo.343.1"> values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.344.1">Age</span></strong><span class="koboSpan" id="kobo.345.1"> column with this median. </span><span class="koboSpan" id="kobo.345.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.346.1">inplace=True</span></strong><span class="koboSpan" id="kobo.347.1"> argument ensures that the changes are made directly within </span><strong class="source-inline"><span class="koboSpan" id="kobo.348.1">df_median_imputed</span></strong><span class="koboSpan" id="kobo.349.1">, without creating a </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">new DataFrame:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.351.1">
df_median_imputed['Age'].fillna(df['Age'].median(), inplace=True)</span></pre>			<p><span class="koboSpan" id="kobo.352.1">Similarly, the following line fills any missing values </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.354.1">Test_Score</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.356.1">
df_median_imputed['Test_Score'].fillna(df['Test_Score'].median(), inplace=True)</span></pre>			<p><span class="koboSpan" id="kobo.357.1">Let’s have a look at the dataset after </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">median imputation:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.359.1">
print(df_median_imputed)
     Age  Test_Score
0   18.0        85.0
1   20.0        75.0
2   27.5        90.0
3   22.0        92.0
4   21.0        75.0
5   19.0        88.0
6   27.5        94.0
7   23.0        91.0
8   18.0        75.0
9   24.0        87.0
10  40.0        75.0</span></pre>			<p><span class="koboSpan" id="kobo.360.1">As we can see, the median has replaced all the </span><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">NaN</span></strong><span class="koboSpan" id="kobo.362.1"> values for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.363.1">Age</span></strong><span class="koboSpan" id="kobo.364.1"> feature (27.5) and for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.365.1">Test_Score</span></strong><span class="koboSpan" id="kobo.366.1"> column (75). </span><span class="koboSpan" id="kobo.366.2">The updated distributions are </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">as follows.</span></span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<span class="koboSpan" id="kobo.368.1"><img src="image/B19801_08_4.jpg" alt="Figure 8.4 – Distribution of features after median imputation"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.369.1">Figure 8.4 – Distribution of features after median imputation</span></p>
			<p><span class="koboSpan" id="kobo.370.1">We can</span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.371.1"> see from the graphs that the distributions</span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.372.1"> have slightly changed for both of the variables. </span><span class="koboSpan" id="kobo.372.2">Let’s have a look at the descriptive statistics of the </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">imputed dataset:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.374.1">
print(df_median_imputed.describe())
             Age  Test_Score
count  24.000000   24.000000
mean   32.708333   67.791667
std    17.345540   25.047744
min    18.000000    5.000000
25%    22.750000   58.750000
50%    27.500000   75.000000
75%    35.750000   85.500000
max    90.000000   94.000000</span></pre>			<p><span class="koboSpan" id="kobo.375.1">Having seen the descriptive statistics for both datasets, the observed changes are </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">presented here:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.377.1">Count increase</span></strong><span class="koboSpan" id="kobo.378.1">: The count for both </span><strong class="source-inline"><span class="koboSpan" id="kobo.379.1">Age</span></strong><span class="koboSpan" id="kobo.380.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">Test_Score</span></strong><span class="koboSpan" id="kobo.382.1"> increased from 20 (for age) and 19 (for test score) to 24 for both variables after median imputation, indicating that missing values were </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">successfully imputed.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.384.1">Mean changes</span></strong><span class="koboSpan" id="kobo.385.1">: The mean age decreased from 33.75 to 32.71 after imputation. </span><span class="koboSpan" id="kobo.385.2">The mean test score increased slightly from 65.89 to 67.79. </span><span class="koboSpan" id="kobo.385.3">These changes reflect the nature of the data remaining after </span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">the imputation.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.387.1">Standard deviation changes</span></strong><span class="koboSpan" id="kobo.388.1">: The standard deviation for </span><strong class="source-inline"><span class="koboSpan" id="kobo.389.1">Age</span></strong><span class="koboSpan" id="kobo.390.1"> decreased from 18.90 to 17.35, indicating a reduction in variability for age. </span><span class="koboSpan" id="kobo.390.2">The standard deviation for </span><strong class="source-inline"><span class="koboSpan" id="kobo.391.1">Test_Score</span></strong><span class="koboSpan" id="kobo.392.1"> also decreased from 27.99 to 25.05, reflecting less variability in the test scores </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">after imputation.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.394.1">Quartiles changes</span></strong><span class="koboSpan" id="kobo.395.1">: Q1 (25%) for </span><strong class="source-inline"><span class="koboSpan" id="kobo.396.1">Age</span></strong><span class="koboSpan" id="kobo.397.1"> increased slightly from 21.75 to 22.75, and the Q1 for </span><strong class="source-inline"><span class="koboSpan" id="kobo.398.1">Test_Score</span></strong><span class="koboSpan" id="kobo.399.1"> increased from 54.00 to 58.75. </span><span class="koboSpan" id="kobo.399.2">Q3 (75%) for </span><strong class="source-inline"><span class="koboSpan" id="kobo.400.1">Age</span></strong><span class="koboSpan" id="kobo.401.1"> decreased from 38.50 to 35.75, and Q3 for </span><strong class="source-inline"><span class="koboSpan" id="kobo.402.1">Test_Score</span></strong><span class="koboSpan" id="kobo.403.1"> decreased slightly from 87.50 </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">to 85.50.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.405.1">Median (50%) changes</span></strong><span class="koboSpan" id="kobo.406.1">: The median for </span><strong class="source-inline"><span class="koboSpan" id="kobo.407.1">Age</span></strong><span class="koboSpan" id="kobo.408.1"> remained stable at 27.50, while the median for </span><strong class="source-inline"><span class="koboSpan" id="kobo.409.1">Test_Score</span></strong><span class="koboSpan" id="kobo.410.1"> also remained stable at 75.00 highlighting the central tendency of the data was preserved </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">after imputation.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.412.1">Median </span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.413.1">imputation </span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.414.1">has successfully filled in the missing values while preserving the median for both </span><strong class="source-inline"><span class="koboSpan" id="kobo.415.1">Age</span></strong><span class="koboSpan" id="kobo.416.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">Test_Score</span></strong><span class="koboSpan" id="kobo.418.1">. </span><span class="koboSpan" id="kobo.418.2">It resulted in a slight change in the mean and reduced variability, which is typical of median imputation. </span><span class="koboSpan" id="kobo.418.3">The central tendency (median) was maintained, which is a key advantage of median imputation, especially in skewed distributions. </span><span class="koboSpan" id="kobo.418.4">But it also reduces the spread of the data which may be relevant for certain types </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">of analysis.</span></span></p>
			<p><span class="koboSpan" id="kobo.420.1">In the next part, we will use what we learned so far on the imputation. </span><span class="koboSpan" id="kobo.420.2">We will also add an extra step, which involves marking where the missing values exist in the dataset for </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">later reference.</span></span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.422.1">Creating indicator variables</span></h2>
			<p><span class="koboSpan" id="kobo.423.1">Indicator</span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.424.1"> variable imputation, also known as flag or dummy variable imputation, involves creating </span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.425.1">a binary</span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.426.1"> indicator variable that flags whether an observation has a missing value in a particular variable. </span><span class="koboSpan" id="kobo.426.2">This separate dummy variable takes the value of 1 for missing values and 0 for observed values. </span><span class="koboSpan" id="kobo.426.3">Indicator variable imputation can be useful when there is a pattern to the missing values, and you want to explicitly model and capture the missingness. </span><span class="koboSpan" id="kobo.426.4">Remember here that </span><em class="italic"><span class="koboSpan" id="kobo.427.1">we are adding a completely new variable, creating a higher dimensional dataset</span></em><span class="koboSpan" id="kobo.428.1">. </span><span class="koboSpan" id="kobo.428.2">After we build the indicator variables, </span><em class="italic"><span class="koboSpan" id="kobo.429.1">whose role is to remind us which values were imputed and which were not</span></em><span class="koboSpan" id="kobo.430.1">, we go ahead and impute the dataset with any method we want such as median </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">or mean.</span></span></p>
			<p><span class="koboSpan" id="kobo.432.1">Let’s see the</span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.433.1"> code example for this imputation method. </span><span class="koboSpan" id="kobo.433.2">As always, you can see the full code in </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">the repository:</span></span></p>
			<p><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/5.indicator_imputation.py"><span class="No-Break"><span class="koboSpan" id="kobo.435.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/5.indicator_imputation.py</span></span></a></p>
			<p><span class="koboSpan" id="kobo.436.1">Also, remember </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.437.1">that we are using exactly the same DataFrame across the chapter, so we have skipped the DataFrame </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">creation here:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.439.1">
df['Age_missing'] = df['Age'].isnull().astype(int)
df['Test_Score_missing'] = df['Test_Score'].isnull().astype(int)</span></pre>			<p><span class="koboSpan" id="kobo.440.1">The code creates new columns in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.441.1">df</span></strong><span class="koboSpan" id="kobo.442.1"> DataFrame that indicate whether a value is missing (</span><strong class="source-inline"><span class="koboSpan" id="kobo.443.1">NaN</span></strong><span class="koboSpan" id="kobo.444.1">) in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.445.1">Age</span></strong><span class="koboSpan" id="kobo.446.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.447.1">Test_Score</span></strong><span class="koboSpan" id="kobo.448.1"> columns. </span><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">df['Age'].isnull()</span></strong><span class="koboSpan" id="kobo.450.1"> checks each value in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">Age</span></strong><span class="koboSpan" id="kobo.452.1"> column to see whether it is </span><strong class="source-inline"><span class="koboSpan" id="kobo.453.1">NaN</span></strong><span class="koboSpan" id="kobo.454.1"> (missing). </span><span class="koboSpan" id="kobo.454.2">It returns a Boolean series where </span><strong class="source-inline"><span class="koboSpan" id="kobo.455.1">True</span></strong><span class="koboSpan" id="kobo.456.1"> indicates a missing value, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.457.1">False</span></strong><span class="koboSpan" id="kobo.458.1"> indicates a non-missing value. </span><span class="koboSpan" id="kobo.458.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.459.1">.astype(int)</span></strong><span class="koboSpan" id="kobo.460.1"> method converts the Boolean series into an integer series where </span><strong class="source-inline"><span class="koboSpan" id="kobo.461.1">True</span></strong><span class="koboSpan" id="kobo.462.1"> becomes </span><strong class="source-inline"><span class="koboSpan" id="kobo.463.1">1</span></strong><span class="koboSpan" id="kobo.464.1"> (indicating a missing value) and </span><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">False</span></strong><span class="koboSpan" id="kobo.466.1"> becomes </span><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">0</span></strong><span class="koboSpan" id="kobo.468.1"> (indicating no missing value). </span><span class="koboSpan" id="kobo.468.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.469.1">df['Age_missing']</span></strong><span class="koboSpan" id="kobo.470.1"> DataFrame stores this integer series in a new column </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">named </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.472.1">Age_missing</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.474.1">Similarly, </span><strong class="source-inline"><span class="koboSpan" id="kobo.475.1">df['Test_Score_missing']</span></strong><span class="koboSpan" id="kobo.476.1"> is created to indicate missing values in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">Test_Score</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.478.1"> column:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.479.1">
df_imputed['Age'].fillna(df_imputed['Age'].mean(), inplace=True)
df_imputed['Test_Score'].fillna(df_imputed['Test_Score'].mean(), inplace=True)</span></pre>			<p><span class="koboSpan" id="kobo.480.1">This code fills in the missing values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.481.1">Age</span></strong><span class="koboSpan" id="kobo.482.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.483.1">Test_Score</span></strong><span class="koboSpan" id="kobo.484.1"> columns of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.485.1">df_imputed</span></strong><span class="koboSpan" id="kobo.486.1"> DataFrame with the mean of the respective columns, as we learned in the previous part. </span><span class="koboSpan" id="kobo.486.2">Let’s have a look at the dataset after the indicator </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">variable imputation:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.488.1">
print(df_imputed)
      Age  Test_Score  Age_missing  Test_Score_missing
0   18.00   85.000000            0                   0
1   20.00   65.894737            0                   1
2   33.75   90.000000            1                   0
3   22.00   92.000000            0                   0
4   21.00   65.894737            0                   1
5   19.00   88.000000            0                   0
6   33.75   94.000000            1                   0
7   23.00   91.000000            0                   0
8   18.00   65.894737            0                   1
9   24.00   87.000000            0                   0
10  40.00   75.000000            0                   0</span></pre>			<p><span class="koboSpan" id="kobo.489.1">As you </span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.490.1">can see from the imputed dataset, we</span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.491.1"> added two indicator variables (</span><strong class="source-inline"><span class="koboSpan" id="kobo.492.1">Age_missing</span></strong><span class="koboSpan" id="kobo.493.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.494.1">Test_Score_missing</span></strong><span class="koboSpan" id="kobo.495.1">) that take the value of 1 if the corresponding variable is missing and 0 otherwise. </span><span class="koboSpan" id="kobo.495.2">So, we mainly flag </span><em class="italic"><span class="koboSpan" id="kobo.496.1">which values from the original rows </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.497.1">were imputed</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.499.1">Let’s see how the distribution of the indicator </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">variables looks:</span></span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<span class="koboSpan" id="kobo.501.1"><img src="image/B19801_08_5.jpg" alt="Figure 8.5 – Distribution of indicator variables"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.502.1">Figure 8.5 – Distribution of indicator variables</span></p>
			<p><span class="koboSpan" id="kobo.503.1">Now, let’s </span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.504.1">explore the relationship </span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.505.1">between the indicator variables and other features in your dataset by building some </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">box plots:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.507.1">
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.boxplot(x='Age_missing', y='Test_Score', data=df_imputed)
plt.title("Boxplot of Test_Score by Age_missing")
plt.subplot(1, 2, 2)
sns.boxplot(x='Test_Score_missing', y='Age', data=df_imputed)
plt.title("Boxplot of Age by Test_Score_missing")
plt.tight_layout()
plt.show()</span></pre>			<p><span class="koboSpan" id="kobo.508.1">The created box plots can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.509.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.510.1">.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">:</span></span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<span class="koboSpan" id="kobo.512.1"><img src="image/B19801_08_6.jpg" alt="Figure 8.6 – Box plots comparing the relationship between indicator variables and the rest of the features"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.513.1">Figure 8.6 – Box plots comparing the relationship between indicator variables and the rest of the features</span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.514.1">Reminder – how to read the box plots</span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.515.1">Box extent</span></strong><span class="koboSpan" id="kobo.516.1">: The box</span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.517.1"> in a box plot represents the IQR, which contains the central 50% of the data. </span><span class="koboSpan" id="kobo.517.2">Values within the box are considered typical or within the </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">normal range.</span></span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.519.1">Whiskers</span></strong><span class="koboSpan" id="kobo.520.1">: Whiskers </span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.521.1">extend from the box and show the range of typical values. </span><span class="koboSpan" id="kobo.521.2">Outliers are often defined as values outside a certain multiple (e.g., 1.5 times) of </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">the IQR.</span></span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.523.1">Outliers</span></strong><span class="koboSpan" id="kobo.524.1">: Individual </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.525.1">data points beyond the whiskers are considered potential outliers. </span><span class="koboSpan" id="kobo.525.2">Outliers are plotted as individual points </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">or asterisks.</span></span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.527.1">Suspected outliers</span></strong><span class="koboSpan" id="kobo.528.1">: Sometimes, points </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.529.1">just beyond the whiskers may be plotted as suspected outliers, marked separately to indicate they are potential outliers but </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">not extreme.</span></span></p>
			<p><span class="koboSpan" id="kobo.531.1">Back to our </span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.532.1">example, the box plot of </span><strong class="source-inline"><span class="koboSpan" id="kobo.533.1">Test_Score</span></strong><span class="koboSpan" id="kobo.534.1"> by </span><strong class="source-inline"><span class="koboSpan" id="kobo.535.1">Age Missing</span></strong><span class="koboSpan" id="kobo.536.1"> shows that when the age is missing in the data, the mean of </span><strong class="source-inline"><span class="koboSpan" id="kobo.537.1">Test_Score</span></strong><span class="koboSpan" id="kobo.538.1"> is around 80 and the distribution values are between 55 and 85. </span><span class="koboSpan" id="kobo.538.2">When </span><strong class="source-inline"><span class="koboSpan" id="kobo.539.1">Age</span></strong><span class="koboSpan" id="kobo.540.1"> is not missing, the mean is around 65, with most of the values being around 60 and 80, with some outliers around 20. </span><span class="koboSpan" id="kobo.540.2">Now, when the score is missing, the mean age of the students is around 20, whereas for the students with scores, the mean age is </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">around 35.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.542.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.543.1">When building predictive models, include the indicator variables as additional features to capture the impact of missing values on the target variable. </span><span class="koboSpan" id="kobo.543.2">Evaluate the performance of models with and without the indicator variables to assess </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">their contribution.</span></span></p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.545.1">Comparison between imputation methods</span></h2>
			<p><span class="koboSpan" id="kobo.546.1">The following table provides a</span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.547.1"> guide for selecting the appropriate imputation method based on the data’s characteristics and objectives of the task </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">at hand.</span></span></p>
			<p><span class="koboSpan" id="kobo.549.1">Remember that there is no </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">one-size-fits-all solution!</span></span></p>
			<table id="table001-5" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.551.1">Imputation method</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.552.1">Use cases</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.553.1">Pros</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.554.1">Cons</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.555.1">Mean imputation</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.556.1">Normally </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">distributed data</span></span></p>
							<p><span class="koboSpan" id="kobo.558.1">Missing values are MCAR </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">or MAR</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.560.1">Simple and easy </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">to implement</span></span></p>
							<p><span class="koboSpan" id="kobo.562.1">Preserves the mean of </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">the distribution</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.564.1">Sensitive </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">to outliers</span></span></p>
							<p><span class="koboSpan" id="kobo.566.1">May distort the distribution if missingness is </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">not random</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.568.1">Median imputation</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.569.1">Skewed or non-normally </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">distributed data</span></span></p>
							<p><span class="koboSpan" id="kobo.571.1">Presence </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">of outliers</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.573.1">Robust </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">to outliers</span></span></p>
							<p><span class="koboSpan" id="kobo.575.1">Preserves the median of </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">the distribution</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.577.1">Ignores potential relationships </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">between variables</span></span></p>
							<p><span class="koboSpan" id="kobo.579.1">May be less precise for </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">non-skewed data</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.581.1">Indicator </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.582.1">variable imputation</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.583.1">Systematic pattern in </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">missing data</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.585.1">Captures </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">missingness pattern</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.587.1">Increases dimensionality Assumes meaningful missingness pattern, which may not always be </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">the case</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.589.1">Deletion </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.590.1">of rows</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.591.1">MCAR or MAR </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">missingness mechanism</span></span></p>
							<p><span class="koboSpan" id="kobo.593.1">Presence </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">of outliers</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.595.1">Preserves the existing </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">data structure</span></span></p>
							<p><span class="koboSpan" id="kobo.597.1">Can be effective when missingness </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">is random</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.599.1">Reduces the </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">sample size</span></span></p>
							<p><span class="koboSpan" id="kobo.601.1">May lead to biased results if missingness is not </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">completely random</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.603.1">Table 8.1 – Comparison between the various imputation methods</span></p>
			<p><span class="koboSpan" id="kobo.604.1">In the examples</span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.605.1"> provided, we consistently applied the same imputation method to each column of the dataset. </span><span class="koboSpan" id="kobo.605.2">However, as demonstrated, our analysis and considerations were tailored to each column individually. </span><span class="koboSpan" id="kobo.605.3">This implies that we have the flexibility to tailor our imputation strategy to the specific characteristics and requirements of </span><em class="italic"><span class="koboSpan" id="kobo.606.1">each column</span></em><span class="koboSpan" id="kobo.607.1">. </span><span class="koboSpan" id="kobo.607.2">As a practical exercise, take some time to experiment with different imputation methods for various columns in your dataset and observe how these choices impact </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">your results.</span></span></p>
			<p><span class="koboSpan" id="kobo.609.1">To build on the foundation we’ve established with our imputation strategies, it’s essential to recognize that data cleaning doesn’t stop with handling missing values. </span><span class="koboSpan" id="kobo.609.2">Another critical aspect of data preprocessing is identifying and managing outliers. </span><span class="koboSpan" id="kobo.609.3">In the next part, we will dive deeper into detecting and handling outliers, ensuring our dataset is as accurate and reliable </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">as possible.</span></span></p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.611.1">Detecting and handling outliers</span></h1>
			<p><span class="koboSpan" id="kobo.612.1">Outliers are </span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.613.1">data points that significantly deviate from the general </span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.614.1">pattern or trend shown by most of the data points in a dataset. </span><span class="koboSpan" id="kobo.614.2">They lie at an unusually distant location from the center of the data distribution and can have a significant impact on statistical analyses, visualizations, and model performance. </span><span class="koboSpan" id="kobo.614.3">Defining outliers involves recognizing data points that do not conform to the expected behavior of the data and understanding the context in which </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">they occur.</span></span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.616.1">Impact of outliers</span></h2>
			<p><span class="koboSpan" id="kobo.617.1">Outliers, while</span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.618.1"> often a small fraction of a dataset, wield a disproportionate influence that can disrupt the integrity of a dataset. </span><span class="koboSpan" id="kobo.618.2">Their presence has the potential to distort statistical summaries, mislead visualizations, and negatively impact the performance </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">of models.</span></span></p>
			<p><span class="koboSpan" id="kobo.620.1">Let’s go deeper into the various ways in which outliers distort </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">the truth:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.622.1">Distorted summary statistics</span></strong><span class="koboSpan" id="kobo.623.1">: Outliers can significantly skew summary statistics, giving a misleading impression of the central tendencies of </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">the data:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.625.1">Mean and median</span></strong><span class="koboSpan" id="kobo.626.1">: The mean, a common measure of central tendency, can be greatly affected by outliers. </span><span class="koboSpan" id="kobo.626.2">An outlier with a value much higher or lower than the rest can pull the mean in its direction. </span><span class="koboSpan" id="kobo.626.3">On the other hand, the median is determined by the middle value of a sorted dataset. </span><span class="koboSpan" id="kobo.626.4">It effectively serves as the central point that divides the data into two equal halves, making it less susceptible to the influence of </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">extreme values.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.628.1">Variance and standard deviation</span></strong><span class="koboSpan" id="kobo.629.1">: Outliers can inflate the variance and standard deviation, making the data appear more spread out than it actually is. </span><span class="koboSpan" id="kobo.629.2">This can misrepresent the variability of the majority of </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">the data.</span></span></li></ul></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.631.1">Misleading visualizations</span></strong><span class="koboSpan" id="kobo.632.1">: Outliers can distort the scale and shape of visualizations, leading </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">to misinterpretation:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.634.1">Box plots</span></strong><span class="koboSpan" id="kobo.635.1">: Outliers can cause box plots to extend excessively, making the bulk of the data appear compressed. </span><span class="koboSpan" id="kobo.635.2">This can make the distribution seem less spread out than it </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">actually is.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.637.1">Histograms</span></strong><span class="koboSpan" id="kobo.638.1">: Outliers might lead to the creation of bins that capture only a few extreme values, causing other bins to seem disproportionately small and the distribution shape to </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">be distorted.</span></span></li></ul></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.640.1">Influence on model performance</span></strong><span class="koboSpan" id="kobo.641.1">: Outliers can negatively affect the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">predictive models:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.643.1">Regression</span></strong><span class="koboSpan" id="kobo.644.1">: Outliers can heavily influence the slope and intercept of the regression line, leading to models that are overly influenced by </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">extreme values.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.646.1">Clustering</span></strong><span class="koboSpan" id="kobo.647.1">: Outliers can affect the centroids and boundaries of clusters, potentially</span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.648.1"> leading to the creation of clusters that do not accurately represent the </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">data distribution.</span></span></li></ul></li>
			</ul>
			<p><span class="koboSpan" id="kobo.650.1">Outliers can be categorized based on dimensions as univariate versus multivariate. </span><span class="koboSpan" id="kobo.650.2">In the next section, we will use the example presented in the first part to see how we can handle the </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">univariate outliers.</span></span></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.652.1">Identifying univariate outliers</span></h2>
			<p><span class="koboSpan" id="kobo.653.1">Univariate outliers</span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.654.1"> occur when an extreme value is observed in a single variable, regardless of the values of other variables. </span><span class="koboSpan" id="kobo.654.2">They are detected based on the distribution of a single variable and are often identified using visualizations or statistical methods such as Z-score </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">or IQR.</span></span></p>
			<p><span class="koboSpan" id="kobo.656.1">In the next part, we will build one of the most common visualizations to </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">identify outliers.</span></span></p>
			<h3><span class="koboSpan" id="kobo.658.1">Classic visualizations for identifying outliers</span></h3>
			<p><span class="koboSpan" id="kobo.659.1">Before going</span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.660.1"> deeper into the statistical methods to identify outliers, there are a couple of easy visualizations we could build to spot them. </span><span class="koboSpan" id="kobo.660.2">The data example we have been using so far will still be used for this part; you can find the full code </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/6.outliers_visualisation.py"><span class="No-Break"><span class="koboSpan" id="kobo.662.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/6.outliers_visualisation.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.663.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.664.1">Let’s start with the first visualization, the box plot, where outliers are depicted as dots on the left or right of the whisker. </span><span class="koboSpan" id="kobo.664.2">The following code snippet creates the box plots for </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">each variable:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.666.1">
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.title("Box Plot for 'Age'")
plt.boxplot(df['Age'].dropna(), vert=False)
plt.subplot(1, 2, 2)
plt.title("Box Plot for 'Test_Score'")
plt.boxplot(df['Test_Score'].dropna(), vert=False)
plt.tight_layout()
plt.show()</span></pre>			<p><span class="koboSpan" id="kobo.667.1">The </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.668.1">created box plots are presented </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">as follows:</span></span></p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<span class="koboSpan" id="kobo.670.1"><img src="image/B19801_08_7.jpg" alt="Figure 8.7 – Box plots to spot outliers"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.671.1">Figure 8.7 – Box plots to spot outliers</span></p>
			<p><span class="koboSpan" id="kobo.672.1">In our example, we can see that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.673.1">Age</span></strong><span class="koboSpan" id="kobo.674.1"> feature has some </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">clear outliers.</span></span></p>
			<p><span class="koboSpan" id="kobo.676.1">Another classic plot is the violin chart, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.677.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.678.1">.8</span></em><span class="koboSpan" id="kobo.679.1">. </span><span class="koboSpan" id="kobo.679.2">Violin plots are a powerful visualization tool that combines aspects of box plots and kernel density plots. </span><span class="koboSpan" id="kobo.679.3">To create the violin plots, run the following </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">code snippet:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.681.1">
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.title("Violin Plot for 'Age'")
plt.violinplot(df['Age'].dropna(), vert=False)
plt.subplot(1, 2, 2)
plt.title("Violin Plot for 'Test_Score'")
plt.violinplot(df['Test_Score'].dropna(), vert=False)
plt.tight_layout()
plt.show()</span></pre>			<p><span class="koboSpan" id="kobo.682.1">The</span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.683.1"> created violin plots are </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">as follows:</span></span></p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<span class="koboSpan" id="kobo.685.1"><img src="image/B19801_08_8.jpg" alt="Figure 8.8 – Violin plots to spot outliers"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.686.1">Figure 8.8 – Violin plots to spot outliers</span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.687.1">Reminder – how to read the violin plots:</span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.688.1">Width of the violin</span></strong><span class="koboSpan" id="kobo.689.1">: The </span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.690.1">width of the violin represents the density of the data at different values. </span><span class="koboSpan" id="kobo.690.2">A wider section indicates a higher density of data points at that specific value, meaning a higher probability that members of the population will have the given value; the skinnier sections represent a </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">lower probability.</span></span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.692.1">Box-and-whisker elements</span></strong><span class="koboSpan" id="kobo.693.1">: Inside the violin, you may see a box-and-whisker plot, similar to what </span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.694.1">you would see in a traditional box plot. </span><span class="koboSpan" id="kobo.694.2">The box represents the IQR, and the median is usually displayed as a horizontal line inside the box. </span><span class="koboSpan" id="kobo.694.3">Whiskers extend from the box to indicate the range of </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">the data.</span></span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.696.1">Kernel Density Estimation</span></strong><span class="koboSpan" id="kobo.697.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.698.1">KDE</span></strong><span class="koboSpan" id="kobo.699.1">): The </span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.700.1">entire shape of the violin is a mirrored representation of the KDE. </span><span class="koboSpan" id="kobo.700.2">The KDE provides a smooth representation of the data distribution, allowing you to see peaks and valleys in </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">the data.</span></span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.702.1">Outliers</span></strong><span class="koboSpan" id="kobo.703.1">: Outliers </span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.704.1">may be visible as points beyond the ends of the whiskers or outside the overall shape of </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">the violin.</span></span></p>
			<p><span class="koboSpan" id="kobo.706.1">Now</span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.707.1"> having seen these charts, we are starting to form some hypotheses about the existence of outliers specifically in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.708.1">Age</span></strong><span class="koboSpan" id="kobo.709.1"> column. </span><span class="koboSpan" id="kobo.709.2">The next step is to use some statistical methods to validate these hypotheses starting with the </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">Z-score method.</span></span></p>
			<h3><span class="koboSpan" id="kobo.711.1">Z-score method</span></h3>
			<p><span class="koboSpan" id="kobo.712.1">The</span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.713.1"> Z-score method is a statistical technique </span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.714.1">used to identify univariate outliers in a dataset by measuring how far individual data points deviate from the mean in terms of standard deviations. </span><span class="koboSpan" id="kobo.714.2">The Z-score for a data point is calculated using the </span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">following formula:</span></span></p>
			<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.716.1">Z</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.717.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.718.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.719.1">X</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.720.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.721.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.722.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.723.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.724.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.725.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.726.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.727.1">S</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.728.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.729.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.730.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.731.1">d</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.732.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.733.1">r</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.734.1">d</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.735.1">D</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.736.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.737.1">v</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.738.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.739.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.740.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.741.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.742.1">o</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.743.1">n</span></span></span></p>
			<p><span class="koboSpan" id="kobo.744.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.745.1">X</span></em><span class="koboSpan" id="kobo.746.1"> is the data point, </span><em class="italic"><span class="koboSpan" id="kobo.747.1">Mean</span></em><span class="koboSpan" id="kobo.748.1"> is the average of the dataset, and </span><em class="italic"><span class="koboSpan" id="kobo.749.1">Standard Deviation</span></em><span class="koboSpan" id="kobo.750.1"> quantifies the dispersion of </span><span class="No-Break"><span class="koboSpan" id="kobo.751.1">the data.</span></span></p>
			<p><span class="koboSpan" id="kobo.752.1">Typically, a threshold Z-score is chosen to determine outliers. </span><span class="koboSpan" id="kobo.752.2">Commonly used thresholds are </span><em class="italic"><span class="koboSpan" id="kobo.753.1">Z &gt; 3</span></em><span class="koboSpan" id="kobo.754.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.755.1">Z &lt;− 3</span></em><span class="koboSpan" id="kobo.756.1">, indicating that data points deviating more than three standard deviations from the mean are </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">considered outliers.</span></span></p>
			<p><span class="koboSpan" id="kobo.758.1">Let’s go back to our code example to calculate the Z-score for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.759.1">Age</span></strong><span class="koboSpan" id="kobo.760.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.761.1">Test_Score</span></strong><span class="koboSpan" id="kobo.762.1"> columns. </span><span class="koboSpan" id="kobo.762.2">We will continue with the example we started before. </span><span class="koboSpan" id="kobo.762.3">You can find the full code </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/7.identify_univariate_outliers.py"><span class="No-Break"><span class="koboSpan" id="kobo.764.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/7.identify_univariate_outliers.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.765.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.766.1">Let’s calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">the Z-score:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.768.1">
z_scores_age = np.abs(stats.zscore(df['Age'].dropna()))</span></pre>			<p><span class="koboSpan" id="kobo.769.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.770.1">stats.zscore(df['Age'].dropna())</span></strong><span class="koboSpan" id="kobo.771.1"> function calculates the Z-scores for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.772.1">Age</span></strong><span class="koboSpan" id="kobo.773.1"> column. </span><span class="koboSpan" id="kobo.773.2">A Z-score represents how many standard deviations a data point is from the mean. </span><span class="koboSpan" id="kobo.773.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.774.1">dropna()</span></strong><span class="koboSpan" id="kobo.775.1"> function is used to exclude </span><strong class="source-inline"><span class="koboSpan" id="kobo.776.1">NaN</span></strong><span class="koboSpan" id="kobo.777.1"> values before calculating </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">the Z-scores:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.779.1">
z_scores_test_score = np.abs(stats.zscore(df['Test_Score'].dropna()))</span></pre>			<p><span class="koboSpan" id="kobo.780.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.781.1">np.abs()</span></strong><span class="koboSpan" id="kobo.782.1"> function</span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.783.1"> takes</span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.784.1"> the absolute value of the Z-scores. </span><span class="koboSpan" id="kobo.784.2">This is done because Z-scores can be negative (indicating a value below the mean) or positive (indicating a value above the mean). </span><span class="koboSpan" id="kobo.784.3">By using the absolute value, we’re only concerned with the magnitude of deviation from the mean, regardless </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">of direction.</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.786.1">
z_threshold = 3
outliers_age = np.where(z_scores_age &gt; z_threshold)[0]
outliers_test_score = np.where(z_scores_test_score &gt; z_threshold)[0]</span></pre>			<p><strong class="source-inline"><span class="koboSpan" id="kobo.787.1">np.where(z_scores_age &gt; z_threshold)[0]</span></strong><span class="koboSpan" id="kobo.788.1"> identifies the indices of the data points in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.789.1">Age</span></strong><span class="koboSpan" id="kobo.790.1"> column that have Z-scores greater than the threshold of </span><strong class="source-inline"><span class="koboSpan" id="kobo.791.1">3</span></strong><span class="koboSpan" id="kobo.792.1">. </span><span class="koboSpan" id="kobo.792.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.793.1">[0]</span></strong><span class="koboSpan" id="kobo.794.1"> at the end is used to extract the indices as an array. </span><span class="koboSpan" id="kobo.794.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.795.1">outliers_age</span></strong><span class="koboSpan" id="kobo.796.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.797.1">outliers_test_score</span></strong><span class="koboSpan" id="kobo.798.1"> variables store the indices of the outlier data points in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.799.1">Age</span></strong><span class="koboSpan" id="kobo.800.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.801.1">Test_Score</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.802.1">columns, respectively.</span></span></p>
			<p><span class="koboSpan" id="kobo.803.1">If we plot the Z-scores for each observation and feature of the data, we can start spotting some </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">outliers already.</span></span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<span class="koboSpan" id="kobo.805.1"><img src="image/B19801_08_9.jpg" alt="Figure 8.9 – Outlier detection with Z-score"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.806.1">Figure 8.9 – Outlier detection with Z-score</span></p>
			<p><span class="koboSpan" id="kobo.807.1">In these scatter plots of Z-scores, each point represents the Z-score of an individual data point. </span><span class="koboSpan" id="kobo.807.2">The red </span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.808.1">dashed line indicates the chosen Z-score threshold (in this case, </span><strong class="source-inline"><span class="koboSpan" id="kobo.809.1">3</span></strong><span class="koboSpan" id="kobo.810.1">). </span><span class="koboSpan" id="kobo.810.2">Outliers are identified as points above this threshold. </span><span class="koboSpan" id="kobo.810.3">As we can see, in </span><strong class="source-inline"><span class="koboSpan" id="kobo.811.1">Age</span></strong><span class="koboSpan" id="kobo.812.1">, there is an outlier </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">clearly captured.</span></span></p>
			<h4><span class="koboSpan" id="kobo.814.1">How to choose the right threshold for the z score?</span></h4>
			<p><span class="koboSpan" id="kobo.815.1">A Z-score tells </span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.816.1">you how many standard deviations a data point is from the mean. </span><span class="koboSpan" id="kobo.816.2">In a normal distribution, the following </span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">is true:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.818.1">Approximately 68% of data falls within </span><em class="italic"><span class="koboSpan" id="kobo.819.1">one standard deviation</span></em><span class="koboSpan" id="kobo.820.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">the mean.</span></span></li>
				<li><span class="koboSpan" id="kobo.822.1">Approximately 95% of data falls within </span><em class="italic"><span class="koboSpan" id="kobo.823.1">two </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.824.1">standard deviations</span></em></span></li>
				<li><span class="koboSpan" id="kobo.825.1">Approximately 99.7% of data falls within </span><em class="italic"><span class="koboSpan" id="kobo.826.1">three </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.827.1">standard deviations</span></em></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.828.1">This means that a Z-score threshold of </span><strong class="source-inline"><span class="koboSpan" id="kobo.829.1">3</span></strong><span class="koboSpan" id="kobo.830.1"> is often used because it captures values that are </span><em class="italic"><span class="koboSpan" id="kobo.831.1">extremely far from the mean</span></em><span class="koboSpan" id="kobo.832.1">, identifying the most extreme outliers. </span><span class="koboSpan" id="kobo.832.2">In a perfectly normal distribution, only 0.3% of data points will have a Z-score greater than 3 or less than -3. </span><span class="koboSpan" id="kobo.832.3">This makes it a reasonable threshold for detecting outliers that are unlikely to be part of the normal </span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">data distribution.</span></span></p>
			<p><span class="koboSpan" id="kobo.834.1">Now, apart from the Z-score, another common method is the IQR, which we will discuss in the </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">following part.</span></span></p>
			<h3><span class="koboSpan" id="kobo.836.1">IQR method</span></h3>
			<p><span class="koboSpan" id="kobo.837.1">The </span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.838.1">IQR is a </span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.839.1">measure of statistical dispersion, representing the range between Q1 and Q3 in a dataset. </span><span class="koboSpan" id="kobo.839.2">The IQR is a robust measure of </span><em class="italic"><span class="koboSpan" id="kobo.840.1">spread</span></em><span class="koboSpan" id="kobo.841.1"> because it is less sensitive to outliers. </span><span class="koboSpan" id="kobo.841.2">At this point, it is clear that the IQR is based on quartiles. </span><span class="koboSpan" id="kobo.841.3">Quartiles divide the dataset into segments, and since Q1 and Q3 are less sensitive to extreme values, the IQR is not heavily influenced by outliers. </span><span class="koboSpan" id="kobo.841.4">On the other hand, standard deviation is influenced by each data point’s deviation from the mean. </span><span class="koboSpan" id="kobo.841.5">Outliers with large deviations can disproportionately impact the </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">standard deviation.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.843.1">Reminder – how to calculate the IQR</span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.844.1">Calculate Q1 (25th percentile)</span></strong><span class="koboSpan" id="kobo.845.1">: Identify the value below which 25% of the </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">data falls.</span></span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.847.1">Calculate Q3 (75th percentile)</span></strong><span class="koboSpan" id="kobo.848.1">: Identify the value below which 75% of the </span><span class="No-Break"><span class="koboSpan" id="kobo.849.1">data falls.</span></span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.850.1">Calculate IQR</span></strong><span class="koboSpan" id="kobo.851.1">: IQR = Q3 - </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">Q1.</span></span></p>
			<p><span class="koboSpan" id="kobo.853.1">To identify </span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.854.1">potential outliers using IQR, do </span><span class="No-Break"><span class="koboSpan" id="kobo.855.1">the following:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.856.1">Calculate the lower and upper bounds as follows: Lower Bound = Q1 - 1.5 * IQR, Upper Bound = Q3 + 1.5 * </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">IQR.</span></span></li>
				<li><span class="koboSpan" id="kobo.858.1">Any data point outside the lower and upper bounds is considered a </span><span class="No-Break"><span class="koboSpan" id="kobo.859.1">potential outlier.</span></span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.860.1">It’s important </span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.861.1">to note that the choice of the multiplier (in this case, </span><strong class="source-inline"><span class="koboSpan" id="kobo.862.1">1.5</span></strong><span class="koboSpan" id="kobo.863.1">) is somewhat arbitrary but has been widely adopted in practice. </span><span class="koboSpan" id="kobo.863.2">Adjusting this multiplier can make the method more or less sensitive to potential outliers. </span><span class="koboSpan" id="kobo.863.3">For example, using a larger multiplier would result in broader boundaries, potentially identifying more data points as potential outliers, while a smaller multiplier would make the method </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">less sensitive.</span></span></p>
			<p><span class="koboSpan" id="kobo.865.1">We’ll be using the same script as before, which can be found at </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/7.identify_univariate_outliers.py"><span class="koboSpan" id="kobo.866.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/7.identify_univariate_outliers.py</span></a><span class="koboSpan" id="kobo.867.1">. </span><span class="koboSpan" id="kobo.867.2">Let’s have a look at how to calculate the IQR and identify </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">the outliers:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.869.1">
def identify_outliers(column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] &lt; lower_bound) | (df[column] &gt; upper_bound)]
    return outliers</span></pre>			<p><span class="koboSpan" id="kobo.870.1">This code defines</span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.871.1"> a function to identify outliers in any column of a DataFrame using the IQR method. </span><span class="koboSpan" id="kobo.871.2">It calculates the IQR, sets upper and lower bounds for normal data, and then filters out the rows where the values in the column fall outside </span><span class="No-Break"><span class="koboSpan" id="kobo.872.1">these bounds.</span></span></p>
			<p><span class="koboSpan" id="kobo.873.1">Then, let’s identify and print outliers </span><span class="No-Break"><span class="koboSpan" id="kobo.874.1">for </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.875.1">Age</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.876.1">:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.877.1">
age_outliers = identify_outliers('Age')
print("Outliers in 'Age':")
print(age_outliers)</span></pre>			<p><span class="koboSpan" id="kobo.878.1">Identify and print outliers </span><span class="No-Break"><span class="koboSpan" id="kobo.879.1">for </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.880.1">Test_Score</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.881.1">:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.882.1">
test_score_outliers = identify_outliers('Test_Score')
print("\nOutliers in 'Test_Score':")
print(test_score_outliers)</span></pre>			<p><span class="koboSpan" id="kobo.883.1">After running this code, we can see in the print statement the identified outliers/rows based on the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.884.1">Age</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.885.1"> column:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.886.1">
   Age  Test_Score
  76.0        10.0
  90.0         5.0</span></pre>			<p><span class="koboSpan" id="kobo.887.1">As </span><a id="_idIndexMarker648"/><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.888.1">discussed, the simplicity of the IQR, as well as its robustness to outliers, contributes to its popularity in various analytical scenarios. </span><span class="koboSpan" id="kobo.888.2">However, it comes with certain drawbacks. </span><span class="koboSpan" id="kobo.888.3">One limitation is the loss of information, as IQR only considers the central 50% of the dataset, disregarding the entire range. </span><span class="koboSpan" id="kobo.888.4">Additionally, IQR’s sensitivity to sample size, especially in smaller datasets, can affect its accuracy in reflecting the true spread of </span><span class="No-Break"><span class="koboSpan" id="kobo.889.1">the data.</span></span></p>
			<p><span class="koboSpan" id="kobo.890.1">Finally, we will quickly discuss leveraging domain knowledge to </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">identify outliers.</span></span></p>
			<h3><span class="koboSpan" id="kobo.892.1">Domain knowledge</span></h3>
			<p><span class="koboSpan" id="kobo.893.1">To better </span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.894.1">understand the use of domain knowledge in outlier detection, let’s use the test scores example. </span><span class="koboSpan" id="kobo.894.2">Suppose the dataset represents student test scores, and based on educational standards, test scores are expected to fall within a range of 0 to 100. </span><span class="koboSpan" id="kobo.894.3">Any score outside this range could be considered an outlier. </span><span class="koboSpan" id="kobo.894.4">By leveraging domain knowledge in education, we can set these boundaries to identify potential outliers. </span><span class="koboSpan" id="kobo.894.5">For instance, if a test score is recorded as 120, it would likely be flagged as an outlier because it exceeds the maximum possible score of 100. </span><span class="koboSpan" id="kobo.894.6">Similarly, negative scores or scores below 0 would be considered outliers. </span><span class="koboSpan" id="kobo.894.7">Integrating domain knowledge in this manner allows us to establish meaningful thresholds for outlier detection, ensuring that the analysis aligns with the expected norms within the </span><span class="No-Break"><span class="koboSpan" id="kobo.895.1">educational context.</span></span></p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor209"/><span class="koboSpan" id="kobo.896.1">Handling univariate outliers</span></h2>
			<p><span class="koboSpan" id="kobo.897.1">Handling </span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.898.1">univariate outliers refers to the process of identifying, assessing, and managing data points in individual variables that deviate significantly </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.899.1">from the typical patterns or distribution of the dataset. </span><span class="koboSpan" id="kobo.899.2">The goal is to mitigate the impact of these extreme values on </span><span class="No-Break"><span class="koboSpan" id="kobo.900.1">data products.</span></span></p>
			<p><span class="koboSpan" id="kobo.901.1">There are several approaches to handling univariate outliers. </span><span class="koboSpan" id="kobo.901.2">We will start with deletions, always working on the example presented at the beginning of </span><span class="No-Break"><span class="koboSpan" id="kobo.902.1">the chapter.</span></span></p>
			<h3><span class="koboSpan" id="kobo.903.1">Deletion of outliers</span></h3>
			<p><span class="koboSpan" id="kobo.904.1">Deleting outliers </span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.905.1">refers to the process of removing data points in a dataset that are considered unusually extreme or deviant from the overall pattern of the data. </span><span class="koboSpan" id="kobo.905.2">The deletion of outliers comes with trade-offs. </span><span class="koboSpan" id="kobo.905.3">On the one hand, it is the simplest way to deal with extreme values. </span><span class="koboSpan" id="kobo.905.4">On the other hand, it leads to a reduction in the sample size and potential loss of valuable information. </span><span class="koboSpan" id="kobo.905.5">Additionally, if outliers are not genuine errors but rather reflect legitimate variability in the data, their removal can </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">introduce bias.</span></span></p>
			<p><span class="koboSpan" id="kobo.907.1">Back to our example, after having imputed the missing data with the mean and calculated the IQRs, we dropped the outliers that passed the outlier threshold. </span><span class="koboSpan" id="kobo.907.2">Let’s see the code that performs these steps; you can also find it in the repository </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/8.handle_univariate_outliers_deletions.py"><span class="No-Break"><span class="koboSpan" id="kobo.909.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/8.handle_univariate_outliers_deletions.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.910.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.911.1">Let’s calculate the IQR and use it to set lower and upper bounds for what is considered a normal range </span><span class="No-Break"><span class="koboSpan" id="kobo.912.1">of data:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.913.1">
(IQR) Q1 = df['Test_Score'].quantile(0.25)
Q3 = df['Test_Score'].quantile(0.75)
IQR = Q3 - Q1
outlier_threshold = 1.5</span></pre>			<p><span class="koboSpan" id="kobo.914.1">Let’s define</span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.915.1"> the lower and upper outlier bounds. </span><span class="koboSpan" id="kobo.915.2">Any value outside this range is flagged as </span><span class="No-Break"><span class="koboSpan" id="kobo.916.1">an outlier:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.917.1">
lower_bound = Q1 - outlier_threshold * IQR
upper_bound = Q3 + outlier_threshold * IQR</span></pre>			<p><span class="koboSpan" id="kobo.918.1">The last line filters the DataFrame (</span><strong class="source-inline"><span class="koboSpan" id="kobo.919.1">df</span></strong><span class="koboSpan" id="kobo.920.1">) to include only rows where the </span><strong class="source-inline"><span class="koboSpan" id="kobo.921.1">Test_Score</span></strong><span class="koboSpan" id="kobo.922.1"> values fall within the calculated lower and </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">upper bounds:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.924.1">
df_no_outliers = df[(df['Test_Score'] &gt;= lower_bound) &amp; (df['Test_Score'] &lt;= upper_bound)].copy()</span></pre>			<p><span class="koboSpan" id="kobo.925.1">In the following charts, we can see the updated distribution charts, after the removal of </span><span class="No-Break"><span class="koboSpan" id="kobo.926.1">the outliers.</span></span></p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<span class="koboSpan" id="kobo.927.1"><img src="image/B19801_08_10.jpg" alt="Figure 8.10 – Distribution charts after deletion of outliers"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.928.1">Figure 8.10 – Distribution charts after deletion of outliers</span></p>
			<p><span class="koboSpan" id="kobo.929.1">Let’s have a look at the descriptive statistics after the </span><span class="No-Break"><span class="koboSpan" id="kobo.930.1">outlier deletion:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.931.1">
             Age  Test_Score
count  22.000000   22.000000
mean   29.272727   71.203349
std     8.163839   17.794339
min    18.000000   20.000000
25%    22.250000   65.894737
50%    31.000000   71.000000
75%    33.937500   86.500000
max    45.000000   94.000000</span></pre>			<p><span class="koboSpan" id="kobo.932.1">The changes </span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.933.1">observed after the deletion of outliers are described </span><span class="No-Break"><span class="koboSpan" id="kobo.934.1">as follows:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.935.1">Mean age change</span></strong><span class="koboSpan" id="kobo.936.1">: The mean age, after deleting outliers, decreased slightly from 33.75 to approximately 29.27. </span><span class="koboSpan" id="kobo.936.2">This reduction suggests that the removed outliers were </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">older individuals.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.938.1">Standard deviation change for age</span></strong><span class="koboSpan" id="kobo.939.1">: The standard deviation for age  decreased from 17.18 to 8.16, indicating that the spread of ages became slightly narrower after outlier removal, which were likely contributing to greater variability in the </span><span class="No-Break"><span class="koboSpan" id="kobo.940.1">original dataset.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.941.1">Minimum and maximum age values</span></strong><span class="koboSpan" id="kobo.942.1">: The minimum age remained the same, at 18, while the maximum age decreased from 90 to 45, indicating that older individuals (potential outliers) were removed during </span><span class="No-Break"><span class="koboSpan" id="kobo.943.1">outlier handling.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.944.1">Mean test score change</span></strong><span class="koboSpan" id="kobo.945.1">: The mean test score increased slightly from 65.89 to 71.20 after removing outliers, suggesting that the deleted outliers were lower test scores that were pulling down the </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">original mean.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.947.1">Standard deviation change for test scores</span></strong><span class="koboSpan" id="kobo.948.1">: The standard deviation decreased from 24.76 to 17.79, indicating a narrower spread of </span><span class="No-Break"><span class="koboSpan" id="kobo.949.1">test scores.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.950.1">Minimum and maximum test scores</span></strong><span class="koboSpan" id="kobo.951.1">: The minimum test score increased from 5.00 to 20.00, while the maximum test scores remained the same at 94.00. </span><span class="koboSpan" id="kobo.951.2">This indicates that extremely low scores were removed as part of the </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">outlier handling.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.953.1">The removal of outliers led to a decrease in both – the mean age and standard deviation, as well as a slight increase in the mean test score. </span><span class="koboSpan" id="kobo.953.2">While removing outliers can improve data quality, especially when the outliers are due to data entry errors or measurement inaccuracies, it also reduces the dataset’s variability. </span><span class="koboSpan" id="kobo.953.3">If the outliers represent true variability in the population, removing them could distort the overall picture of the data. </span><span class="koboSpan" id="kobo.953.4">Therefore, careful consideration should be given to whether the outliers represent genuine data points </span><span class="No-Break"><span class="koboSpan" id="kobo.954.1">or errors.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.955.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.956.1">Some statistical models assume normality and can be sensitive to outliers. </span><span class="koboSpan" id="kobo.956.2">Removing outliers may help meet the assumptions of certain models. </span><span class="koboSpan" id="kobo.956.3">So before deleting, you need to better understand the problem you are solving and the techniques to </span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">be used.</span></span></p>
			<p><span class="koboSpan" id="kobo.958.1">There are other ways to deal with outliers in case you don’t want to drop them completely from the data. </span><span class="koboSpan" id="kobo.958.2">In the next part, we will discuss the trimming and winsorizing </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">of outliers.</span></span></p>
			<h3><span class="koboSpan" id="kobo.960.1">Trimming</span></h3>
			<p><span class="koboSpan" id="kobo.961.1">Trimming </span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.962.1">involves removing a certain percentage of data from both ends of a distribution and then calculating the mean. </span><span class="koboSpan" id="kobo.962.2">For the trimming, we need to define a trimming fraction, which represents the proportion of data to be trimmed from </span><em class="italic"><span class="koboSpan" id="kobo.963.1">both tails of the distribution</span></em><span class="koboSpan" id="kobo.964.1"> when calculating the trimmed mean. </span><span class="koboSpan" id="kobo.964.2">It is used to exclude a certain percentage of extreme values (outliers) from the calculation of the mean. </span><span class="koboSpan" id="kobo.964.3">The trimming fraction is a value between 0 and 0.5, where the following </span><span class="No-Break"><span class="koboSpan" id="kobo.965.1">is true:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.966.1"> 0 means no trimming (include all </span><span class="No-Break"><span class="koboSpan" id="kobo.967.1">data points)</span></span></li>
				<li><span class="koboSpan" id="kobo.968.1"> 0.1 means trim 10% of data from </span><span class="No-Break"><span class="koboSpan" id="kobo.969.1">each tail</span></span></li>
				<li><span class="koboSpan" id="kobo.970.1">0.2 means trim 20% of data from </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">each tail</span></span></li>
				<li><span class="koboSpan" id="kobo.972.1">0.5 means trim 50% of data from each tail (exclude the most </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">extreme values)</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.974.1">In our given scenario, our analysis indicates that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.975.1">Age</span></strong><span class="koboSpan" id="kobo.976.1"> column exhibits the most significant outliers. </span><span class="koboSpan" id="kobo.976.2">In response, we have decided to trim our dataset by excluding the top and bottom percentiles specific to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.977.1">Age</span></strong><span class="koboSpan" id="kobo.978.1"> column. </span><span class="koboSpan" id="kobo.978.2">The following example code demonstrates this trimming process. </span><span class="koboSpan" id="kobo.978.3">We are still working on the same dataset so we will skip the creation of the DataFrame here. </span><span class="koboSpan" id="kobo.978.4">However, you can see the whole code at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.979.1">link: </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/9.trimming.py"><span class="No-Break"><span class="koboSpan" id="kobo.980.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/9.trimming.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.981.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.982.1">Let’s have a look at the following code snippet that creates a new DataFrame (</span><strong class="source-inline"><span class="koboSpan" id="kobo.983.1">df_trimmed</span></strong><span class="koboSpan" id="kobo.984.1">), which includes only the rows where the </span><strong class="source-inline"><span class="koboSpan" id="kobo.985.1">Age</span></strong><span class="koboSpan" id="kobo.986.1"> value is between the 10th and 90th percentiles. </span><span class="koboSpan" id="kobo.986.2">This effectively drops the lowest 10% and highest 10% of values in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.987.1">Age</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.988.1"> column:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.989.1">
df_trimmed = df[(df['Age'] &gt;= df['Age'].quantile(0.1)) &amp; (df['Age'] &lt;= df['Age'].quantile(0.9))]</span></pre>			<p><span class="koboSpan" id="kobo.990.1">Let’s now calculate the trimmed mean for </span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">each column:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.992.1">
df_trimmed_mean = df_trimmed.mean()</span></pre>			<p><span class="koboSpan" id="kobo.993.1">After trimming the data, the</span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.994.1"> last line calculates the mean for each column in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.995.1">df_trimmed</span></strong><span class="koboSpan" id="kobo.996.1"> DataFrame. </span><span class="koboSpan" id="kobo.996.2">The mean calculated after trimming the data is known as the </span><em class="italic"><span class="koboSpan" id="kobo.997.1">trimmed mean</span></em><span class="koboSpan" id="kobo.998.1">. </span><span class="koboSpan" id="kobo.998.2">It represents the average value of the central 80% of the data, excluding the most extreme 20% (10% from </span><span class="No-Break"><span class="koboSpan" id="kobo.999.1">each side).</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1000.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1001.1">Keep in mind that the trimming fraction is a way to balance the robustness of the trimmed mean against the amount of data excluded. </span><span class="koboSpan" id="kobo.1001.2">You may need to experiment with different fractions to find a suitable balance for </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">your data.</span></span></p>
			<p><span class="koboSpan" id="kobo.1003.1">Let’s have a look at the updated distribution after </span><span class="No-Break"><span class="koboSpan" id="kobo.1004.1">the trimming:</span></span></p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1005.1"><img src="image/B19801_08_11.jpg" alt="Figure 8.11 – Distribution charts after trimming of outliers at 10% threshold"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1006.1">Figure 8.11 – Distribution charts after trimming of outliers at 10% threshold</span></p>
			<p><span class="koboSpan" id="kobo.1007.1">Let’s also have a look at the updated statistics of </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">the data:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1009.1">
             Age  Test_Score
count  18.000000   18.000000
mean   30.222222   69.309942
std     6.757833   18.797436
min    20.000000   20.000000
25%    24.000000   60.723684
50%    32.875000   66.947368
75%    33.937500   84.750000
max    41.000000   94.000000</span></pre>			<p><span class="koboSpan" id="kobo.1010.1">In the</span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.1011.1"> original dataset, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1012.1">Age</span></strong><span class="koboSpan" id="kobo.1013.1"> column displayed a mean of 33.75 with a standard deviation of 17.18, while the trimmed data exhibited a higher mean of 30.22 and a much lower standard deviation of 6.76. </span><span class="koboSpan" id="kobo.1013.2">The minimum age value increased from 18 to 20 in the trimmed data, indicating the removal of lower outliers. </span><span class="koboSpan" id="kobo.1013.3">The maximum age value decreased from 90 to 41, suggesting the exclusion of </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">higher outliers.</span></span></p>
			<p><span class="koboSpan" id="kobo.1015.1">For the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1016.1">Test_Score</span></strong><span class="koboSpan" id="kobo.1017.1"> column, the mean in the original dataset was 65.89, and the standard deviation was 24.76. </span><span class="koboSpan" id="kobo.1017.2">In the trimmed data, the mean increased to 69.31, and the standard deviation decreased to 18.80 indicating a narrower spread of test scores. </span><span class="koboSpan" id="kobo.1017.3">The minimum test score increased from 5 to 20, indicating the removal of lower outliers, while the maximum test score stayed </span><span class="No-Break"><span class="koboSpan" id="kobo.1018.1">at 94.</span></span></p>
			<p><span class="koboSpan" id="kobo.1019.1">Overall, the deletion of outliers led to changes in the central tendency (mean) and the spread (standard deviation) of the data for both </span><strong class="source-inline"><span class="koboSpan" id="kobo.1020.1">Age</span></strong><span class="koboSpan" id="kobo.1021.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1022.1">Test_Score</span></strong><span class="koboSpan" id="kobo.1023.1">. </span><span class="koboSpan" id="kobo.1023.2">This indicates that the trimmed dataset has become more concentrated around the middle values, with extreme </span><span class="No-Break"><span class="koboSpan" id="kobo.1024.1">values removed.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1025.1">Remember!</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1026.1">While trimming can help in reducing the influence of extreme values, it also involves discarding a portion of the data. </span><span class="koboSpan" id="kobo.1026.2">This may result in information loss, and the trimmed variable may not fully represent the </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">original dataset.</span></span></p>
			<p><span class="koboSpan" id="kobo.1028.1">In the next section, we will present a slightly</span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.1029.1"> different way to deal with the outlier </span><span class="No-Break"><span class="koboSpan" id="kobo.1030.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1031.1">winsorizing</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1032.1">.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1033.1">Winsorizing</span></h3>
			<p><span class="koboSpan" id="kobo.1034.1">Instead of </span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.1035.1">removing extreme values outright as with trimming, winsorizing involves </span><em class="italic"><span class="koboSpan" id="kobo.1036.1">replacing them with less extreme values</span></em><span class="koboSpan" id="kobo.1037.1">. </span><span class="koboSpan" id="kobo.1037.2">The extreme values are replaced with values closer to the center of the distribution, often at a specified percentile. </span><span class="koboSpan" id="kobo.1037.3">Winsorizing can be useful when you want to </span><em class="italic"><span class="koboSpan" id="kobo.1038.1">retain the size of your dataset</span></em><span class="koboSpan" id="kobo.1039.1"> and helps preserve the overall shape of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1040.1">data distribution.</span></span></p>
			<p><span class="koboSpan" id="kobo.1041.1">Let’s go back to our example use case and have a look at the code. </span><span class="koboSpan" id="kobo.1041.2">You can find the full code </span><span class="No-Break"><span class="koboSpan" id="kobo.1042.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/10.winsorizing.py"><span class="No-Break"><span class="koboSpan" id="kobo.1043.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/10.winsorizing.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1044.1">:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1045.1">
winsorizing_fraction = 0.1</span></pre>			<p><strong class="source-inline"><span class="koboSpan" id="kobo.1046.1">winsorizing_fraction</span></strong><span class="koboSpan" id="kobo.1047.1"> is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1048.1">0.1</span></strong><span class="koboSpan" id="kobo.1049.1">, representing the proportion of data to be adjusted </span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.1050.1">at each end of the distribution. </span><span class="koboSpan" id="kobo.1050.2">It is specified as a percentage, and its value typically ranges between 0 and 50%. </span><span class="koboSpan" id="kobo.1050.3">The process of coming up with the winsorizing fraction involves considering the desired amount of influence you want to reduce from extreme values. </span><span class="koboSpan" id="kobo.1050.4">A common choice is to winsorize a certain percentage from both tails, such as 5% </span><span class="No-Break"><span class="koboSpan" id="kobo.1051.1">or 10%.</span></span></p>
			<p><span class="koboSpan" id="kobo.1052.1">Another thing to know here is that the winsorizing process is performed at each column </span><em class="italic"><span class="koboSpan" id="kobo.1053.1">separately and independently of the others</span></em><span class="koboSpan" id="kobo.1054.1">. </span><span class="koboSpan" id="kobo.1054.2">Remember: we are dealing with outliers in a </span><em class="italic"><span class="koboSpan" id="kobo.1055.1">univariate</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.1056.1">way here:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1057.1">
df_winsorized = df.apply(lambda x: mstats.winsorize(x, limits=[winsorizing_fraction, winsorizing_fraction]))</span></pre>			<p><span class="koboSpan" id="kobo.1058.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1059.1">limits=[winsorizing_fraction, winsorizing_fraction]</span></strong><span class="koboSpan" id="kobo.1060.1"> argument specifies the proportion of data to be winsorized from each end of the distribution. </span><span class="koboSpan" id="kobo.1060.2">Here, 10% from the lower end and 10% from the upper end are adjusted. </span><span class="koboSpan" id="kobo.1060.3">Extreme values (the lowest 10% and highest 10%) are replaced with the nearest values within the specified limits, thereby reducing their influence on </span><span class="No-Break"><span class="koboSpan" id="kobo.1061.1">statistical measures.</span></span></p>
			<p><span class="koboSpan" id="kobo.1062.1">Here, the updated distributions after the winsorizing </span><span class="No-Break"><span class="koboSpan" id="kobo.1063.1">are presented:</span></span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1064.1"><img src="image/B19801_08_12.jpg" alt="Figure 8.12 – Distribution charts after winsorizing of outliers"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1065.1">Figure 8.12 – Distribution charts after winsorizing of outliers</span></p>
			<p><span class="koboSpan" id="kobo.1066.1">Let’s also</span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.1067.1"> have a look at the updated statistics of </span><span class="No-Break"><span class="koboSpan" id="kobo.1068.1">the data:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1069.1">
             Age  Test_Score  Age_Winsorized
count  24.000000   24.000000       24.000000
mean   33.750000   65.894737       30.666667
std    17.181575   24.761286        8.857773
min    18.000000    5.000000       19.000000
25%    22.750000   58.750000       22.750000
50%    32.875000   66.947368       32.875000
75%    35.750000   85.500000       35.750000
max    90.000000   94.000000       45.000000</span></pre>			<p><span class="koboSpan" id="kobo.1070.1">The mean for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1071.1">Age</span></strong><span class="koboSpan" id="kobo.1072.1"> column decreased from 33.75 to 30.67 after winsorizing, indicating a shift toward lower values as extreme high values were adjusted. </span><span class="koboSpan" id="kobo.1072.2">The standard deviation also decreased significantly from 17.18 to 8.86, suggesting reduced variability in the dataset. </span><span class="koboSpan" id="kobo.1072.3">The minimum value increased slightly from 18 to 19, and the maximum value decreased from 90 to 45, reflecting the capping of </span><span class="No-Break"><span class="koboSpan" id="kobo.1073.1">extreme values.</span></span></p>
			<p><span class="koboSpan" id="kobo.1074.1">As for </span><strong class="source-inline"><span class="koboSpan" id="kobo.1075.1">Test_Score</span></strong><span class="koboSpan" id="kobo.1076.1">, the mean remained the same at 65.89 after winsorizing. </span><span class="koboSpan" id="kobo.1076.2">The standard deviation stayed constant at 24.76, indicating that variability in test scores was not affected by the winsorizing process. </span><span class="koboSpan" id="kobo.1076.3">The maximum value stayed the same at 94, showing no changes to the upper </span><span class="No-Break"><span class="koboSpan" id="kobo.1077.1">extreme values.</span></span></p>
			<p><span class="koboSpan" id="kobo.1078.1">Overall, winsorizing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1079.1">Age</span></strong><span class="koboSpan" id="kobo.1080.1"> column resulted in a more concentrated distribution of values, as evidenced by the decreased standard deviation. </span><span class="koboSpan" id="kobo.1080.2">Winsorizing successfully reduced the impact of extreme values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1081.1">Age</span></strong><span class="koboSpan" id="kobo.1082.1"> column, making the data more focused around the middle range. </span><span class="koboSpan" id="kobo.1082.2">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.1083.1">Test_Score</span></strong><span class="koboSpan" id="kobo.1084.1">, winsorizing did not affect the distribution, likely because the extreme values were already within the </span><span class="No-Break"><span class="koboSpan" id="kobo.1085.1">accepted range.</span></span></p>
			<p><span class="koboSpan" id="kobo.1086.1">Next, we will explore how we can apply mathematical transformations to the data to minimize the effect of </span><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">the outliers.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1088.1">Data transformation</span></h3>
			<p><span class="koboSpan" id="kobo.1089.1">Applying mathematical </span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.1090.1">transformations such as logarithm or square root is a common technique to handle skewed data or </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">stabilize variance.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1092.1">Reminder</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1093.1">Skewness is a measure of the asymmetry in a distribution. </span><span class="koboSpan" id="kobo.1093.2">A positive skewness indicates a distribution with a tail on the right side, while a negative skewness indicates a tail on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1094.1">left side.</span></span></p>
			<p><span class="koboSpan" id="kobo.1095.1">When data is right-skewed (positive skewness), meaning that most of the data points are concentrated on the left side with a few larger values on the right side, applying a logarithmic transformation compresses larger values, making the distribution more symmetric and closer to a </span><span class="No-Break"><span class="koboSpan" id="kobo.1096.1">normal distribution.</span></span></p>
			<p><span class="koboSpan" id="kobo.1097.1">Similar to logarithmic transformation, square root transformation is used to mitigate the impact of larger values and make the distribution more symmetric. </span><span class="koboSpan" id="kobo.1097.2">It is particularly effective when the right tail of the distribution contains </span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1">extreme values.</span></span></p>
			<p><span class="koboSpan" id="kobo.1099.1">Another thing to note is that when the variance of the data increases with the mean (heteroscedasticity), logarithmic and square root transformation can compress the larger values, reducing the impact of extreme values and stabilizing </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">the variance.</span></span></p>
			<p><span class="koboSpan" id="kobo.1101.1">Let’s go back to our example and perform a log transformation on both columns of our dataset. </span><span class="koboSpan" id="kobo.1101.2">As always, you can find the full code </span><span class="No-Break"><span class="koboSpan" id="kobo.1102.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/11.data_transformation.py"><span class="No-Break"><span class="koboSpan" id="kobo.1103.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/11.data_transformation.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1104.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.1105.1">Let’s apply a logarithmic transformation to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1106.1">Age</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1107.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1108.1">Test_Score</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1109.1">:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1110.1">
df_log_transformed = df.copy()
df_log_transformed['Age'] = np.log1p(df_log_transformed['Age'])
df_log_transformed['Test_Score'] = np.log1p(df_log_transformed['Test_Score'])</span></pre>			<p><strong class="source-inline"><span class="koboSpan" id="kobo.1111.1">np.log1p</span></strong><span class="koboSpan" id="kobo.1112.1"> is a</span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.1113.1"> NumPy function that computes the natural logarithm of </span><em class="italic"><span class="koboSpan" id="kobo.1114.1">1 + x</span></em><span class="koboSpan" id="kobo.1115.1"> for each value in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1116.1">Age</span></strong><span class="koboSpan" id="kobo.1117.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1118.1">Test_Score</span></strong><span class="koboSpan" id="kobo.1119.1"> columns. </span><span class="koboSpan" id="kobo.1119.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1120.1">log1p</span></strong><span class="koboSpan" id="kobo.1121.1"> function is used instead of the simple logarithm (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1122.1">np.log</span></strong><span class="koboSpan" id="kobo.1123.1">) to handle zero and negative values in a dataset without errors. </span><span class="koboSpan" id="kobo.1123.2">It’s particularly useful when dealing with data that includes zero values or very small numbers. </span><span class="koboSpan" id="kobo.1123.3">The transformation reduces skewness and can make the distribution more normal, which is useful for various statistical techniques that assume normally </span><span class="No-Break"><span class="koboSpan" id="kobo.1124.1">distributed data.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1125.1">More transformations implemented</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1126.1">In the code, you’ll find both logarithmic and root transformations applied to the data. </span><span class="koboSpan" id="kobo.1126.2">Take some time to explore and understand the differences between these two methods. </span><span class="koboSpan" id="kobo.1126.3">Evaluate which transformation better suits your data by considering how each affects the distribution and variance of </span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">your dataset.</span></span></p>
			<p><span class="koboSpan" id="kobo.1128.1">The updated distributions are presented in the following plot after log transforming the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1129.1">Age</span></strong><span class="koboSpan" id="kobo.1130.1"> column and applying a square root transformation to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1131.1">Test_Score</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1132.1"> column:</span></span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1133.1"><img src="image/B19801_08_13.jpg" alt="Figure 8.13 – Distribution charts after log and square route transformation"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1134.1">Figure 8.13 – Distribution charts after log and square route transformation</span></p>
			<p><span class="koboSpan" id="kobo.1135.1">Let’s also have a look at the updated statistics of </span><span class="No-Break"><span class="koboSpan" id="kobo.1136.1">the data:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1137.1">
            Age  Test_Score
count  24.000000   24.000000
mean    3.462073    4.059624
std     0.398871    0.687214
min     2.944439    1.791759
25%     3.167414    4.090143
50%     3.522344    4.218613
75%     3.603530    4.460095
max     4.510860    4.553877</span></pre>			<p><span class="koboSpan" id="kobo.1138.1">The descriptive </span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.1139.1">statistics illustrate the impact of applying logarithmic transformation to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1140.1">Age</span></strong><span class="koboSpan" id="kobo.1141.1"> variable and square root transformation to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1142.1">Test_Score</span></strong><span class="koboSpan" id="kobo.1143.1"> variable. </span><span class="koboSpan" id="kobo.1143.2">Before the transformations, the original dataset displayed a right-skewed distribution for </span><strong class="source-inline"><span class="koboSpan" id="kobo.1144.1">Age</span></strong><span class="koboSpan" id="kobo.1145.1"> with a mean of 33.75 and a wide standard deviation of 17.18. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1146.1">Test_Score</span></strong><span class="koboSpan" id="kobo.1147.1"> had a mean of 65.89, ranging from 5 to 94, with a high standard deviation of 24.76, indicating a large spread in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1148.1">test scores.</span></span></p>
			<p><span class="koboSpan" id="kobo.1149.1">After applying the transformations, the distributions of both variables were </span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1">visibly altered:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.1151.1">The logarithmic transformation on </span><strong class="source-inline"><span class="koboSpan" id="kobo.1152.1">Age</span></strong><span class="koboSpan" id="kobo.1153.1"> reduced the spread of values, bringing the standard deviation down to 0.40 as compared to the original 17.18. </span><span class="koboSpan" id="kobo.1153.2">The transformed values now range from 2.94 to 4.51, showing a compression of </span><span class="No-Break"><span class="koboSpan" id="kobo.1154.1">extreme values.</span></span></li>
				<li><span class="koboSpan" id="kobo.1155.1">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.1156.1">Test_Score</span></strong><span class="koboSpan" id="kobo.1157.1">, the logarithmic transformation resulted in a much more evenly distributed set of values, with the standard deviation decreasing from 24.76 to 0.69. </span><span class="koboSpan" id="kobo.1157.2">The values became more compact and symmetric, ranging from 1.79 </span><span class="No-Break"><span class="koboSpan" id="kobo.1158.1">to 4.55.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1159.1">The transformations had a clear leveling effect on both variables, reducing skewness and variability. </span><span class="koboSpan" id="kobo.1159.2">This is evident in the reduction of standard deviations and narrower ranges, making the data more symmetric and closer to a </span><span class="No-Break"><span class="koboSpan" id="kobo.1160.1">normal distribution.</span></span></p>
			<p><span class="koboSpan" id="kobo.1161.1">However, it’s important to note that transformations, especially logarithmic ones, compress the scale of values and may affect interpretability. </span><span class="koboSpan" id="kobo.1161.2">While they can help meet the assumptions of statistical methods by reducing skewness and heteroscedasticity, the transformed data may be less intuitive to understand compared to the original scale. </span><span class="koboSpan" id="kobo.1161.3">Despite this, such transformations are often useful when preparing data for regression models or other analyses that assume normally </span><span class="No-Break"><span class="koboSpan" id="kobo.1162.1">distributed data.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1163.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1164.1">Keep in mind that log transformation is not suitable for data that contains zero or negative values, as the logarithm is undefined for </span><span class="No-Break"><span class="koboSpan" id="kobo.1165.1">such values.</span></span></p>
			<p><span class="koboSpan" id="kobo.1166.1">To conclude this</span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.1167.1"> section of the chapter, we have compiled a summary table with the various methods discussed for handling outliers. </span><span class="koboSpan" id="kobo.1167.2">This table highlights the optimal scenarios to employ each technique and provides an overview of their respective pros </span><span class="No-Break"><span class="koboSpan" id="kobo.1168.1">and cons.</span></span></p>
			<table id="table002-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1169.1">Technique</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1170.1">When to </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1171.1">use it</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1172.1">Pros</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1173.1">Cons</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1174.1">Trimming</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1175.1">Mild outliers, preserving overall </span><span class="No-Break"><span class="koboSpan" id="kobo.1176.1">data structure</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1177.1">Retains majority of the dataset, maintains </span><span class="No-Break"><span class="koboSpan" id="kobo.1178.1">data integrity</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1179.1">Reduces sample size, may impact representativeness, arbitrary choice of </span><span class="No-Break"><span class="koboSpan" id="kobo.1180.1">trimming percentage</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1181.1">Winsorizing</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1182.1">Moderate outliers, preserving overall </span><span class="No-Break"><span class="koboSpan" id="kobo.1183.1">data</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1184.1">Preserves data distribution, mitigates the impact of </span><span class="No-Break"><span class="koboSpan" id="kobo.1185.1">extreme values</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1186.1">Alters data values; may distort distribution; requires specifying </span><span class="No-Break"><span class="koboSpan" id="kobo.1187.1">trimming limits</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1188.1">Deleting Data</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1189.1">Severe outliers</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1190.1">Removes the influence of extreme values, </span><span class="No-Break"><span class="koboSpan" id="kobo.1191.1">simplifies analysis</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1192.1">Reduces sample size, potential loss of information; may bias results toward a </span><span class="No-Break"><span class="koboSpan" id="kobo.1193.1">central tendency</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1194.1">Transformation</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1195.1">Skewed or non-normal </span><span class="No-Break"><span class="koboSpan" id="kobo.1196.1">distributions</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1197.1">Stabilizes variance, makes the data more symmetric and amenable to traditional </span><span class="No-Break"><span class="koboSpan" id="kobo.1198.1">statistical techniques</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1199.1">Interpretation challenges, results may be less intuitive, choice of transformation method </span><span class="No-Break"><span class="koboSpan" id="kobo.1200.1">is subjective</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1201.1">Table 8.2 – Summary of the univariate methods to deal with outliers</span></p>
			<p><span class="koboSpan" id="kobo.1202.1">After exploring various techniques for addressing univariate outliers, ranging from simpler to more complex methods, the upcoming section will deep dive into the different statistical measures that are generally preferable when working with data </span><span class="No-Break"><span class="koboSpan" id="kobo.1203.1">containing outliers.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1204.1">Robust statistics</span></h3>
			<p><span class="koboSpan" id="kobo.1205.1">Using robust statistical </span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.1206.1">measures such as median and </span><strong class="bold"><span class="koboSpan" id="kobo.1207.1">Median Absolute Deviation</span></strong><span class="koboSpan" id="kobo.1208.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1209.1">MAD</span></strong><span class="koboSpan" id="kobo.1210.1">) instead of mean and standard </span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.1211.1">deviation can reduce the influence </span><span class="No-Break"><span class="koboSpan" id="kobo.1212.1">of outliers.</span></span></p>
			<p><span class="koboSpan" id="kobo.1213.1">When dealing with datasets that contain outliers or skewed distributions, choosing robust statistical measures becomes crucial for obtaining accurate and representative summaries of the data. </span><span class="koboSpan" id="kobo.1213.2">The use of robust measures, such as the median and MAD, proves advantageous in scenarios where the presence of extreme values could impact traditional measures such as the mean and standard deviation. </span><span class="koboSpan" id="kobo.1213.3">The median, being the middle value when data is ordered, is less sensitive to outliers, providing a more reliable measure of central tendency. </span><span class="koboSpan" id="kobo.1213.4">Additionally, MAD, which assesses the spread of data while being robust to outliers, further ensures a more accurate representation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1214.1">dataset’s variability.</span></span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1215.1">MAD</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1216.1">MAD is a measure of statistical dispersion that quantifies the dispersion or spread of a dataset. </span><span class="koboSpan" id="kobo.1216.2">It is calculated as the median of the absolute differences between each data point and the median of </span><span class="No-Break"><span class="koboSpan" id="kobo.1217.1">the dataset.</span></span></p>
			<p><span class="koboSpan" id="kobo.1218.1">This table summarizes the key considerations, pros, and cons of using median and MAD versus mean and </span><span class="No-Break"><span class="koboSpan" id="kobo.1219.1">standard deviation:</span></span></p>
			<table id="table003-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1220.1">Criteria</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1221.1">Median </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1222.1">and MAD</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1223.1">Mean and </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1224.1">standard deviation</span></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="2">
							<p><strong class="bold"><span class="koboSpan" id="kobo.1225.1">When </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1226.1">to use</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1227.1">Presence </span><span class="No-Break"><span class="koboSpan" id="kobo.1228.1">of outliers</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1229.1">Normal or </span><span class="No-Break"><span class="koboSpan" id="kobo.1230.1">symmetric distributions</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="koboSpan" id="kobo.1231.1">Skewed distributions</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1232.1">Precision </span><span class="No-Break"><span class="koboSpan" id="kobo.1233.1">in measurement</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="2">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1234.1">Pros</span></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1235.1">Robustness </span><span class="No-Break"><span class="koboSpan" id="kobo.1236.1">against outliers</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1237.1">Efficiency for </span><span class="No-Break"><span class="koboSpan" id="kobo.1238.1">normal distributions</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1239.1">Applicability to </span><span class="No-Break"><span class="koboSpan" id="kobo.1240.1">skewed data</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1241.1">Ease </span><span class="No-Break"><span class="koboSpan" id="kobo.1242.1">of interpretation</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="2">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1243.1">Cons</span></strong></span></p>
						</td>
						<td class="No-Table-Style" rowspan="2">
							<p><span class="koboSpan" id="kobo.1244.1">Loss of sensitivity </span><span class="No-Break"><span class="koboSpan" id="kobo.1245.1">without outliers</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1246.1">Sensitivity </span><span class="No-Break"><span class="koboSpan" id="kobo.1247.1">to outliers</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1248.1">Not robust in the presence </span><span class="No-Break"><span class="koboSpan" id="kobo.1249.1">of outliers</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="2">
							<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1250.1">Considerations</span></strong></span></p>
						</td>
						<td class="No-Table-Style" rowspan="2">
							<p><span class="koboSpan" id="kobo.1251.1">Useful when the central tendency </span><span class="No-Break"><span class="koboSpan" id="kobo.1252.1">needs stability</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1253.1">Suitable for datasets with minimal or </span><span class="No-Break"><span class="koboSpan" id="kobo.1254.1">no outliers</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="koboSpan" id="kobo.1255.1">Provides precise measures in a </span><span class="No-Break"><span class="koboSpan" id="kobo.1256.1">normal distribution</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1257.1">Table 8.3 – Which statistical methods work better with outliers</span></p>
			<p><span class="koboSpan" id="kobo.1258.1">In the next section of this chapter, we will discuss how to identify </span><span class="No-Break"><span class="koboSpan" id="kobo.1259.1">multivariate outliers.</span></span></p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.1260.1">Identifying multivariate outliers</span></h2>
			<p><span class="koboSpan" id="kobo.1261.1">Multivariate outliers </span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.1262.1">occur when an observation is</span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.1263.1"> extreme in the context of multiple variables simultaneously. </span><span class="koboSpan" id="kobo.1263.2">These outliers cannot be detected by analyzing individual variables alone; rather, they require consideration of interactions between variables. </span><span class="koboSpan" id="kobo.1263.3">Detecting multivariate outliers involves assessing data points in higher dimensional space. </span><span class="koboSpan" id="kobo.1263.4">In the following parts, we will outline different methods to identify multivariate outliers, along with code examples </span><span class="No-Break"><span class="koboSpan" id="kobo.1264.1">for each.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1265.1">Mahalanobis distance</span></h3>
			<p><span class="koboSpan" id="kobo.1266.1">Mahalanobis distance </span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.1267.1">is a statistical measure</span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.1268.1"> used to identify outliers in multivariate data. </span><span class="koboSpan" id="kobo.1268.2">It accounts for the correlation between variables and calculates the distance of each data point from the mean of the dataset in a scaled space. </span><span class="koboSpan" id="kobo.1268.3">This distance is then compared to a threshold to identify observations that deviate significantly from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1269.1">multivariate mean.</span></span></p>
			<p><span class="koboSpan" id="kobo.1270.1">For this example, we have created a new dataset with some multivariate student data so that we can showcase the technique in the best way possible. </span><span class="koboSpan" id="kobo.1270.2">The code can be fully seen in the repository at </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/12.mahalanobis_distance.py"><span class="koboSpan" id="kobo.1271.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/12.mahalanobis_distance.py</span></a><span class="koboSpan" id="kobo.1272.1">. </span><span class="koboSpan" id="kobo.1272.2">The key steps of the process are </span><span class="No-Break"><span class="koboSpan" id="kobo.1273.1">as follows:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1274.1">Let’s import the required </span><span class="No-Break"><span class="koboSpan" id="kobo.1275.1">libraries first:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1276.1">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi2
from mpl_toolkits.mplot3d import Axes3D</span></pre></li>				<li><span class="koboSpan" id="kobo.1277.1">Let’s generate </span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.1278.1">multivariate</span><a id="_idIndexMarker674"/> <span class="No-Break"><span class="koboSpan" id="kobo.1279.1">student data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1280.1">
np.random.seed(42)
data = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=100)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1281.1">We generate a dataset of 100 samples from a multivariate normal distribution with a specified mean vector of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1282.1">[0, 0]</span></strong><span class="koboSpan" id="kobo.1283.1"> and a covariance matrix of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1284.1">[[1, 0.5], [</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1285.1">0.5, 1]]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1286.1">.</span></span></p></li>				<li><span class="koboSpan" id="kobo.1287.1">Let’s introduce outliers and create </span><span class="No-Break"><span class="koboSpan" id="kobo.1288.1">the DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1289.1">
outliers = np.array([[8, 8], [9, 9]])
data = np.concatenate([data, outliers])
df = pd.DataFrame(data, columns=['X1', 'X2'])</span></pre></li>				<li><span class="koboSpan" id="kobo.1290.1">The following function calculates the Mahalanobis distance for each data point based on the mean and the inverse of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1291.1">covariance matrix:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1292.1">
def mahalanobis_distance(x, mean, inv_cov_matrix):
    centered_data = x - mean
    mahalanobis_dist = np.sqrt(np.dot(centered_data,
                               np.dot(inv_cov_matrix,
                               centered_data)))
    return mahalanobis_dist</span></pre></li>				<li><span class="koboSpan" id="kobo.1293.1">The mean, covariance matrix, and inverse covariance matrix are calculated for </span><span class="No-Break"><span class="koboSpan" id="kobo.1294.1">the dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1295.1">
mean = np.mean(df[['X1', 'X2']], axis=0)
cov_matrix = np.cov(df[['X1', 'X2']], rowvar=False)
inv_cov_matrix = np.linalg.inv(cov_matrix)</span></pre></li>				<li><span class="koboSpan" id="kobo.1296.1">Mahalanobis distance</span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.1297.1"> is calculated for</span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.1298.1"> each data point and added as a new column in </span><span class="No-Break"><span class="koboSpan" id="kobo.1299.1">the DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1300.1">
df['Mahalanobis_Distance'] = df.apply(lambda row: mahalanobis_distance(row[['X1', 'X2']], mean, inv_cov_matrix), axis=1)</span></pre></li>				<li><span class="koboSpan" id="kobo.1301.1">Set a significance level for </span><span class="No-Break"><span class="koboSpan" id="kobo.1302.1">outlier detection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1303.1">
alpha = 0.01</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1304.1">The significance level (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1305.1">alpha</span></strong><span class="koboSpan" id="kobo.1306.1">) represents the probability of rejecting the null hypothesis when it is true, which in this context is the probability of incorrectly identifying a data point as an outlier. </span><span class="koboSpan" id="kobo.1306.2">A common choice for </span><strong class="source-inline"><span class="koboSpan" id="kobo.1307.1">alpha</span></strong><span class="koboSpan" id="kobo.1308.1"> is </span><strong class="source-inline"><span class="koboSpan" id="kobo.1309.1">0.01</span></strong><span class="koboSpan" id="kobo.1310.1">, meaning there is a 1% chance of mistakenly classifying a normal data point as an outlier. </span><span class="koboSpan" id="kobo.1310.2">A lower </span><strong class="source-inline"><span class="koboSpan" id="kobo.1311.1">alpha</span></strong><span class="koboSpan" id="kobo.1312.1"> value makes the outlier detection more conservative, reducing false positives (normal points labeled as outliers). </span><span class="koboSpan" id="kobo.1312.2">Conversely, a higher </span><strong class="source-inline"><span class="koboSpan" id="kobo.1313.1">alpha</span></strong><span class="koboSpan" id="kobo.1314.1"> value makes it more permissive, potentially identifying more points as outliers but increasing the chance of </span><span class="No-Break"><span class="koboSpan" id="kobo.1315.1">false positives.</span></span></p></li>				<li><span class="koboSpan" id="kobo.1316.1">Next, we set the </span><span class="No-Break"><span class="koboSpan" id="kobo.1317.1">chi-squared threshold:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1318.1">
chi2_threshold = chi2.ppf(1 - alpha, df=2) # df is the degrees of freedom, which is the number of features</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1319.1">The chi-squared threshold is a critical value from the chi-squared distribution used to define the cutoff for outlier detection. </span><span class="koboSpan" id="kobo.1319.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1320.1">chi2.ppf</span></strong><span class="koboSpan" id="kobo.1321.1"> function computes the percentile point function (inverse of the cumulative distribution function) for the chi-squared distribution. </span><span class="koboSpan" id="kobo.1321.2">The degrees of freedom is equal to the number of features or variables used in the</span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.1322.1"> Mahalanobis distance calculation. </span><span class="koboSpan" id="kobo.1322.2">In this case, it’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.1323.1">2</span></strong><span class="koboSpan" id="kobo.1324.1"> (for X1 and X2). </span><span class="koboSpan" id="kobo.1324.2">The chi-squared threshold is used to determine the cutoff value beyond which Mahalanobis distances </span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.1325.1">are considered excessively high, indicating that the corresponding data points are outliers. </span><span class="koboSpan" id="kobo.1325.2">For example, with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1326.1">alpha = 0.01</span></strong><span class="koboSpan" id="kobo.1327.1">, you are finding the threshold above which only 1% of the data points are expected to fall, assuming the data is </span><span class="No-Break"><span class="koboSpan" id="kobo.1328.1">normally distributed.</span></span></p></li>				<li><span class="koboSpan" id="kobo.1329.1">This step involves comparing each data point’s Mahalanobis distance against the chi-squared threshold to determine whether it is </span><span class="No-Break"><span class="koboSpan" id="kobo.1330.1">an outlier:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1331.1">
outliers = df[df['Mahalanobis_Distance'] &gt; chi2_threshold]
df_no_outliers = df[df['Mahalanobis_Distance'] &lt;= chi2_threshold]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1332.1">Data points with distances greater than the threshold are flagged as outliers and separated from the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.1333.1">the data.</span></span></p></li>				<li><span class="koboSpan" id="kobo.1334.1">Let’s now visualize </span><span class="No-Break"><span class="koboSpan" id="kobo.1335.1">the outliers:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1336.1">
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(df_no_outliers['X1'], df_no_outliers['X2'], df_no_outliers['Mahalanobis_Distance'], color='blue', label='Data Points')
ax.scatter(outliers['X1'], outliers['X2'], outliers['Mahalanobis_Distance'], color='red', label='Outliers')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('Mahalanobis Distance')
ax.set_title('Outlier Detection using Mahalanobis Distance')
plt.legend()
plt.show()</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1337.1">In the </span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.1338.1">following</span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.1339.1"> chart, we can see all the data points projected in a 3D space and we can see the outliers marked </span><span class="No-Break"><span class="koboSpan" id="kobo.1340.1">with </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1341.1">x</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1342.1">:</span></span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1343.1"><img src="image/B19801_08_14.jpg" alt="Figure 8.14 – Data plotted with Mahalanobis distance"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1344.1">Figure 8.14 – Data plotted with Mahalanobis distance</span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.1345.1">Note</span></p>
			<p class="callout"><span class="koboSpan" id="kobo.1346.1">Run the visualization on your laptop to be able to see this space and move around it in a 3D view; </span><span class="No-Break"><span class="koboSpan" id="kobo.1347.1">it’s cool!</span></span></p>
			<p><span class="koboSpan" id="kobo.1348.1">As you can see from the 3D plot, it is very clear to spot the outliers in our data. </span><span class="koboSpan" id="kobo.1348.2">Mahalanobis distance is most effective when dealing with datasets that involve multiple variables as it takes both the means and covariances among variables into account and allows identifying outliers that may not be apparent when looking at individual variables. </span><span class="koboSpan" id="kobo.1348.3">In situations where variables have different units or scales, Mahalanobis distance can normalize the distances across variables, providing a more meaningful measure of outliers. </span><span class="koboSpan" id="kobo.1348.4">Unlike </span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.1349.1">univariate methods, Mahalanobis distance is sensitive to relationships </span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.1350.1">among variables. </span><span class="koboSpan" id="kobo.1350.2">It captures how far each data point is from the center of the data distribution, considering correlations </span><span class="No-Break"><span class="koboSpan" id="kobo.1351.1">between variables.</span></span></p>
			<p><span class="koboSpan" id="kobo.1352.1">In the next section of the multivariate part, we will discuss how clustering methods can help us </span><span class="No-Break"><span class="koboSpan" id="kobo.1353.1">detect outliers.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1354.1">Clustering techniques</span></h3>
			<p><span class="koboSpan" id="kobo.1355.1">Clustering</span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.1356.1"> methods such as k-means </span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.1357.1">or hierarchical clustering can be used to group similar data points. </span><span class="koboSpan" id="kobo.1357.2">Points that do not belong to any cluster or form small clusters might be considered </span><span class="No-Break"><span class="koboSpan" id="kobo.1358.1">multivariate outliers.</span></span></p>
			<p><span class="koboSpan" id="kobo.1359.1">One popular method for outlier </span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.1360.1">detection using clustering is the </span><strong class="bold"><span class="koboSpan" id="kobo.1361.1">Density-Based Spatial Clustering of Applications with Noise</span></strong><span class="koboSpan" id="kobo.1362.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1363.1">DBSACN</span></strong><span class="koboSpan" id="kobo.1364.1">) algorithm. </span><span class="koboSpan" id="kobo.1364.2">DBSCAN can identify clusters of dense data points and classify outliers as noise. </span><span class="koboSpan" id="kobo.1364.3">DBSCAN is advantageous because it </span><em class="italic"><span class="koboSpan" id="kobo.1365.1">doesn’t require specifying the number of clusters beforehand</span></em><span class="koboSpan" id="kobo.1366.1"> and can effectively identify outliers based on density. </span><span class="koboSpan" id="kobo.1366.2">It’s a relatively simple yet powerful method for outlier detection, especially in cases where clusters may not be well-separated or when outliers form </span><span class="No-Break"><span class="koboSpan" id="kobo.1367.1">isolated points.</span></span></p>
			<p><span class="koboSpan" id="kobo.1368.1">Let’s deep dive into the code for the DBSCAN. </span><span class="koboSpan" id="kobo.1368.2">As always, you can find the full code in the repository </span><span class="No-Break"><span class="koboSpan" id="kobo.1369.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/13.clustering.py"><span class="No-Break"><span class="koboSpan" id="kobo.1370.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/13.clustering.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1372.1">Let’s import </span><span class="No-Break"><span class="koboSpan" id="kobo.1373.1">the libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1374.1">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler</span></pre></li>				<li><span class="koboSpan" id="kobo.1375.1">Let’s generate the example data for this method. </span><span class="koboSpan" id="kobo.1375.2">The dataset consists of 100 samples from a multivariate normal distribution with a mean vector of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1376.1">[0, 0]</span></strong><span class="koboSpan" id="kobo.1377.1"> and a covariance matrix of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1378.1">[[1, 0.5], [0.5, 1]]</span></strong><span class="koboSpan" id="kobo.1379.1">. </span><span class="koboSpan" id="kobo.1379.2">This creates a cluster of points </span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.1380.1">that are normally distributed around the origin with some correlation between </span><span class="No-Break"><span class="koboSpan" id="kobo.1381.1">the features:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1382.1">
np.random.seed(42)
data = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=100)
outliers = np.random.multivariate_normal(mean=[8, 8], cov=[[1, 0], [0, 1]], size=10)
data_with_outliers = np.vstack([data, outliers])</span></pre></li>				<li><span class="koboSpan" id="kobo.1383.1">Let’s </span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.1384.1">turn the data into </span><span class="No-Break"><span class="koboSpan" id="kobo.1385.1">a DataFrame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1386.1">
df = pd.DataFrame(data_with_outliers, columns=['Feature1', 'Feature2'])</span></pre></li>				<li><span class="koboSpan" id="kobo.1387.1">Standardize the data by removing the mean and scaling to unit variance. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1388.1">StandardScaler</span></strong><span class="koboSpan" id="kobo.1389.1"> from </span><strong class="source-inline"><span class="koboSpan" id="kobo.1390.1">sklearn.preprocessing</span></strong><span class="koboSpan" id="kobo.1391.1"> is used to fit and transform the data. </span><span class="koboSpan" id="kobo.1391.2">Standardizing ensures that all features contribute equally to distance calculations by scaling them to have a mean of 0 and a standard deviation of 1. </span><span class="koboSpan" id="kobo.1391.3">This is especially important for distance-based algorithms such </span><span class="No-Break"><span class="koboSpan" id="kobo.1392.1">as DBSCAN:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1393.1">
scaler = StandardScaler()
data_scaled = scaler.fit_transform(df)</span></pre></li>				<li><span class="koboSpan" id="kobo.1394.1">Apply DBSCAN for outlier detection. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1395.1">eps=0.4</span></strong><span class="koboSpan" id="kobo.1396.1"> sets the maximum distance between points to be considered in the same neighborhood, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1397.1">min_samples=5</span></strong><span class="koboSpan" id="kobo.1398.1"> specifies the minimum number of points required to form a dense region. </span><span class="koboSpan" id="kobo.1398.2">DBSCAN is a clustering algorithm that can identify outliers as points that do not belong to any cluster. </span><span class="koboSpan" id="kobo.1398.3">Points labeled </span><strong class="source-inline"><span class="koboSpan" id="kobo.1399.1">-1</span></strong><span class="koboSpan" id="kobo.1400.1"> by DBSCAN are considered outliers. </span><span class="koboSpan" id="kobo.1400.2">The choice of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1401.1">eps</span></strong><span class="koboSpan" id="kobo.1402.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1403.1">min_samples</span></strong><span class="koboSpan" id="kobo.1404.1"> parameters can significantly impact the detection of outliers, and these values might need tuning based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1405.1">specific dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1406.1">
dbscan = DBSCAN(eps=0.4, min_samples=5)
df['Outlier'] = dbscan.fit_predict(data_scaled)</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1407.1">In the following </span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.1408.1">chart, we have plotted all data points in a 2D space and can see the outliers on the</span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.1409.1"> right side of </span><span class="No-Break"><span class="koboSpan" id="kobo.1410.1">the graph:</span></span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1411.1"><img src="image/B19801_08_15.jpg" alt="Figure 8.15 – DBSCAN clustering for outlier detection"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1412.1">Figure 8.15 – DBSCAN clustering for outlier detection</span></p>
			<p><span class="koboSpan" id="kobo.1413.1">There is a key parameter that needs to be adjusted in DBSCAN: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1414.1">eps</span></strong><span class="koboSpan" id="kobo.1415.1">. </span><span class="koboSpan" id="kobo.1415.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1416.1">eps</span></strong><span class="koboSpan" id="kobo.1417.1"> (epsilon) parameter essentially defines the radius around a data point, and all other data points within this radius are considered </span><span class="No-Break"><span class="koboSpan" id="kobo.1418.1">its neighbors.</span></span></p>
			<p><span class="koboSpan" id="kobo.1419.1">When performing DBSCAN clustering, the algorithm starts by selecting a data point and identifying all the data points that lie within a distance of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1420.1">eps</span></strong><span class="koboSpan" id="kobo.1421.1"> from it. </span><span class="koboSpan" id="kobo.1421.2">If the number of data points within this distance exceeds a specified threshold (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1422.1">min_samples</span></strong><span class="koboSpan" id="kobo.1423.1">), the selected data point is considered a core point, and all the points within its epsilon-neighborhood become part of the same cluster. </span><span class="koboSpan" id="kobo.1423.2">The algorithm then recursively expands the cluster by finding the neighbors of the neighbors until no more points can </span><span class="No-Break"><span class="koboSpan" id="kobo.1424.1">be added.</span></span></p>
			<p><span class="koboSpan" id="kobo.1425.1">The choice of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1426.1">eps</span></strong><span class="koboSpan" id="kobo.1427.1"> depends </span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.1428.1">on the specific characteristics of the dataset and the desired granularity of clusters. </span><span class="koboSpan" id="kobo.1428.2">It may require some experimentation and domain knowledge to find the appropriate value </span><span class="No-Break"><span class="koboSpan" id="kobo.1429.1">for </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1430.1">eps</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1431.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.1432.1">Employing k-means </span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.1433.1">instead of DBSCAN offers a different approach. </span><span class="koboSpan" id="kobo.1433.2">K-means is a centroid-based clustering algorithm that requires </span><em class="italic"><span class="koboSpan" id="kobo.1434.1">pre-specifying the number of clusters</span></em><span class="koboSpan" id="kobo.1435.1">, making it essential to have prior knowledge or conduct exploratory analysis to determine an appropriate value for </span><em class="italic"><span class="koboSpan" id="kobo.1436.1">k</span></em><span class="koboSpan" id="kobo.1437.1">. </span><span class="koboSpan" id="kobo.1437.2">While it is sensitive to outliers, the simplicity and computational efficiency of k-means make it an attractive choice for certain scenarios. </span><span class="koboSpan" id="kobo.1437.3">K-means may be well-suited when clusters are well-separated and have a relatively uniform structure. </span><span class="koboSpan" id="kobo.1437.4">However, it is essential to be aware that k-means may struggle with irregularly shaped or overlapping clusters and can be influenced by outliers in its attempt to minimize the sum of </span><span class="No-Break"><span class="koboSpan" id="kobo.1438.1">squared distances.</span></span></p>
			<p><span class="koboSpan" id="kobo.1439.1">After having spotted the multivariate outliers, we need to decide how we are going to deal with those. </span><span class="koboSpan" id="kobo.1439.2">This is the focus of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1440.1">next part.</span></span></p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor211"/><span class="koboSpan" id="kobo.1441.1">Handling multivariate outliers</span></h2>
			<p><span class="koboSpan" id="kobo.1442.1">Handling multivariate </span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.1443.1">outliers involves addressing</span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.1444.1"> data points that deviate significantly in the context of multiple variables. </span><span class="koboSpan" id="kobo.1444.2">In this part of the chapter, we will provide explanations and code examples for different methods to handle </span><span class="No-Break"><span class="koboSpan" id="kobo.1445.1">multivariate outliers.</span></span></p>
			<h3><span class="koboSpan" id="kobo.1446.1">Multivariate trimming</span></h3>
			<p><span class="koboSpan" id="kobo.1447.1">This</span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.1448.1"> method involves limiting extreme values based on a combined assessment of the values across multiple variables. </span><span class="koboSpan" id="kobo.1448.2">For example, the limits for trimming can be determined by considering the Mahalanobis distance, which accounts for correlations between variables. </span><span class="koboSpan" id="kobo.1448.3">This technique is particularly useful when dealing with datasets containing outliers present across different variables. </span><span class="koboSpan" id="kobo.1448.4">The idea is to preserve the overall structure of the data while mitigating the influence of extreme values </span><span class="No-Break"><span class="koboSpan" id="kobo.1449.1">across variables.</span></span></p>
			<p><span class="koboSpan" id="kobo.1450.1">For this example, we are going to continue working on the data from the Mahalanobis distance example, and after we have calculated the Mahalanobis distance, we are going to drop the outliers </span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.1451.1">passing the threshold. </span><span class="koboSpan" id="kobo.1451.2">You can find the full code in the repository </span><span class="No-Break"><span class="koboSpan" id="kobo.1452.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/14.multivariate_trimming.py"><span class="No-Break"><span class="koboSpan" id="kobo.1453.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/14.multivariate_trimming.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1454.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.1455.1">Let’s start by importing </span><span class="No-Break"><span class="koboSpan" id="kobo.1456.1">the libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1457.1">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2
from mpl_toolkits.mplot3d import Axes3D</span></pre></li>				<li><span class="koboSpan" id="kobo.1458.1">Let’s generate multivariate </span><span class="No-Break"><span class="koboSpan" id="kobo.1459.1">student data</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1460.1">
np.random.seed(42)
data = np.random.multivariate_normal(mean=[0, 0], cov=[[1, 0.5], [0.5, 1]], size=100)
outliers = np.array([[8, 8], [9, 9]])
data = np.concatenate([data, outliers])
df = pd.DataFrame(data, columns=['X1', 'X2'])</span></pre></li>				<li><span class="koboSpan" id="kobo.1461.1">Define the function to calculate the Mahalanobis distance, which measures how far a data point is from the mean of the distribution, taking into account the correlation </span><span class="No-Break"><span class="koboSpan" id="kobo.1462.1">between features:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1463.1">
def mahalanobis_distance(x, mean, inv_cov_matrix):
    centered_data = x - mean
    mahalanobis_dist = np.sqrt(np.dot(centered_data, np.dot(inv_cov_matrix, centered_data)))
    return mahalanobis_dist</span></pre></li>				<li><span class="koboSpan" id="kobo.1464.1">Prepare the data for </span><span class="No-Break"><span class="koboSpan" id="kobo.1465.1">outlier detection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1466.1">
df[['X1', 'X2']] = df[['X1', 'X2']].astype(float)
mean = np.mean(df[['X1', 'X2']], axis=0)
cov_matrix = np.cov(df[['X1', 'X2']], rowvar=False)
inv_cov_matrix = np.linalg.inv(cov_matrix)</span></pre></li>				<li><span class="koboSpan" id="kobo.1467.1">Calculate</span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.1468.1"> the Mahalanobis distance for each </span><span class="No-Break"><span class="koboSpan" id="kobo.1469.1">data point:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1470.1">
df['Mahalanobis_Distance'] = df.apply(lambda row: mahalanobis_distance(row[['X1', 'X2']], mean, inv_cov_matrix), axis=1)</span></pre></li>				<li><span class="koboSpan" id="kobo.1471.1">Set the threshold for </span><span class="No-Break"><span class="koboSpan" id="kobo.1472.1">outlier detection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1473.1">
alpha = 0.1
chi2_threshold = chi2.ppf(1 - alpha, df=2)</span></pre></li>				<li><span class="koboSpan" id="kobo.1474.1">Filter the DataFrame to separate outliers from the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.1475.1">the data.</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1476.1">
outliers = df[df['Mahalanobis_Distance'] &gt; chi2_threshold]
df_no_outliers = df[df['Mahalanobis_Distance'] &lt;= chi2_threshold]</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.1477.1">Let’s present the distribution plots before the </span><span class="No-Break"><span class="koboSpan" id="kobo.1478.1">outlier handling.</span></span></p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1479.1"><img src="image/B19801_08_16.jpg" alt="Figure 8.16 – Distribution charts with multivariate outliers"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1480.1">Figure 8.16 – Distribution charts with multivariate outliers</span></p>
			<p><span class="koboSpan" id="kobo.1481.1">The</span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.1482.1"> descriptive statistics of the original data are </span><span class="No-Break"><span class="koboSpan" id="kobo.1483.1">as follows:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1484.1">
               X1          X2
count  102.000000  102.000000
mean     0.248108    0.281463
std      1.478963    1.459212
min     -1.852725   -1.915781
25%     -0.554778   -0.512700
50%      0.108116    0.218681
75%      0.715866    0.715485
max      9.000000    9.000000</span></pre>			<p><span class="koboSpan" id="kobo.1485.1">After having dropped the data that are considered multivariate outliers, we can observe the changes in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1486.1">following distributions:</span></span></p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1487.1"><img src="image/B19801_08_17.jpg" alt="Figure 8.17 – Distribution charts after removing multivariate outliers"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1488.1">Figure 8.17 – Distribution charts after removing multivariate outliers</span></p>
			<p><span class="koboSpan" id="kobo.1489.1">Finally, let’s </span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.1490.1">have a look at the updated </span><span class="No-Break"><span class="koboSpan" id="kobo.1491.1">descriptive statistics:</span></span></p>
			<pre class="source-code"><span class="koboSpan" id="kobo.1492.1">
               X1          X2  Mahalanobis_Distance
count  100.000000  100.000000            100.000000
mean     0.083070    0.117093              1.005581
std      0.907373    0.880592              0.547995
min     -1.852725   -1.915781              0.170231
25%     -0.574554   -0.526337              0.534075
50%      0.088743    0.200745              0.874940
75%      0.699309    0.707639              1.391190
max      1.857815    2.679717              2.717075</span></pre>			<p><span class="koboSpan" id="kobo.1493.1">Having trimmed the outliers, let’s discuss the changes observed in </span><span class="No-Break"><span class="koboSpan" id="kobo.1494.1">the data:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.1495.1">The count of observations reduced from 102 to 100 after removing outliers, so we dropped </span><span class="No-Break"><span class="koboSpan" id="kobo.1496.1">two records</span></span></li>
				<li><span class="koboSpan" id="kobo.1497.1">In the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1498.1">X1</span></strong><span class="koboSpan" id="kobo.1499.1"> column, the mean decreased from 0.248 to 0.083, and the standard deviation reduced from 1.479 </span><span class="No-Break"><span class="koboSpan" id="kobo.1500.1">to 0.907</span></span></li>
				<li><span class="koboSpan" id="kobo.1501.1">In the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1502.1">X2</span></strong><span class="koboSpan" id="kobo.1503.1"> column, the mean decreased from 0.281 to 0.117, and the standard deviation reduced from 1.459 </span><span class="No-Break"><span class="koboSpan" id="kobo.1504.1">to 0.881</span></span></li>
				<li><span class="koboSpan" id="kobo.1505.1">The maximum values for </span><strong class="source-inline"><span class="koboSpan" id="kobo.1506.1">X1</span></strong><span class="koboSpan" id="kobo.1507.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1508.1">X2</span></strong><span class="koboSpan" id="kobo.1509.1"> were capped at 1.857815 and 2.679717, respectively, indicating the removal of </span><span class="No-Break"><span class="koboSpan" id="kobo.1510.1">extreme outliers</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.1511.1">Overall, removing</span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.1512.1"> outliers has resulted in a dataset with reduced variability, particularly in terms of mean and standard deviation. </span><span class="koboSpan" id="kobo.1512.2">Extreme values that could potentially skew the analysis have </span><span class="No-Break"><span class="koboSpan" id="kobo.1513.1">been mitigated.</span></span></p>
			<p><span class="koboSpan" id="kobo.1514.1">Let’s summarize the key takeaways from </span><span class="No-Break"><span class="koboSpan" id="kobo.1515.1">this chapter.</span></span></p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.1516.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.1517.1">In this chapter, we deep-dived into handling missing values and outliers. </span><span class="koboSpan" id="kobo.1517.2">We understood that missing values can distort our analyses and learned a range of imputation techniques, from simple mean imputation to advanced machine learning-based strategies. </span><span class="koboSpan" id="kobo.1517.3">Similarly, we recognized that outliers could skew our results and deep-dived into methods to detect and manage them, both in univariate and multivariate contexts. </span><span class="koboSpan" id="kobo.1517.4">By combining theory and practical examples, we gained a deeper understanding of the considerations, challenges, and strategies that go into ensuring the quality and reliability of </span><span class="No-Break"><span class="koboSpan" id="kobo.1518.1">our data.</span></span></p>
			<p><span class="koboSpan" id="kobo.1519.1">Armed with these insights, we can now move on to the next chapter, where we will discuss scaling, normalization, and standardization </span><span class="No-Break"><span class="koboSpan" id="kobo.1520.1">of features.</span></span></p>
		</div>
	</body></html>