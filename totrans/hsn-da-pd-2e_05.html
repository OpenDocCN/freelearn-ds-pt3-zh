<html><head></head><body>
		<div id="_idContainer153">
			<h1 id="_idParaDest-62"><em class="italic"><a id="_idTextAnchor061"/>Chapter 3</em>: Data Wrangling with Pandas</h1>
			<p>In the previous chapter, we learned about the main <strong class="source-inline">pandas</strong> data structures, how to create <strong class="source-inline">DataFrame</strong> objects with our collected data, and various ways to inspect, summarize, filter, select, and work with <strong class="source-inline">DataFrame</strong> objects. Now that we are well versed in the initial data collection and inspection stage, we can begin our foray into the world of data wrangling.</p>
			<p>As mentioned in <a href="B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Data Analysis</em>, preparing data for analysis is often the largest portion of the job time-wise for those working with data, and often the least enjoyable. On the bright side, <strong class="source-inline">pandas</strong> is well equipped to help with these tasks, and, by mastering the skills presented in this book, we will be able to get to the more interesting parts sooner.</p>
			<p>It should be noted that data wrangling isn't something we do merely once in our analysis; it is highly likely that we will do some data wrangling and move on to another analysis task, such as data visualization, only to find that we need to do additional data wrangling. The more familiar we are with the data, the better we will be able to prepare the data for our analysis. It's crucial to form an intuition of what types our data should be, what format we need our data to be in for the visualization that would best convey what we are looking to show, and the data points we should collect for our analysis. This comes with experience, so we must practice the skills that will be covered in this chapter on our own data every chance we get.</p>
			<p>Since this is a very large topic, our coverage of data wrangling will be split between this chapter and <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>. In this chapter, we will get an overview of data wrangling before exploring the <strong class="bold">National Centers for Environmental Information</strong> (<strong class="bold">NCEI</strong>) API for climate data and walking through the process of collecting temperature data from it using the <strong class="source-inline">requests</strong> library. Then, we will discuss data wrangling tasks that deal with preparing data for some initial analyses and visualizations (which we will learn about in <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>, and <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>). We will address some more advanced aspects of data wrangling that relate to aggregations and combining datasets in <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding data wrangling</li>
				<li>Exploring an API to find and collect temperature data</li>
				<li>Cleaning data</li>
				<li>Reshaping data</li>
				<li>Handling duplicate, missing, or invalid data</li>
			</ul>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor062"/>Chapter materials</h1>
			<p>The materials for this chapter can be found on GitHub at <a href="https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03">https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_03</a>. There are five notebooks that we will work through, each numbered according to when they will be used, and two directories, <strong class="source-inline">data/</strong> and <strong class="source-inline">exercises/</strong>, which contain all the CSV files necessary for the aforementioned notebooks and end-of-chapter exercises, respectively. The following files are in the <strong class="source-inline">data/</strong> directory:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/Figure_3.1_B16834.jpg" alt="Figure 3.1 – Breakdown of the datasets used in this chapter&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Breakdown of the datasets used in this chapter</p>
			<p>We will begin in the <strong class="source-inline">1-wide_vs_long.ipynb</strong> notebook by discussing wide versus long format data. Then, we will collect daily temperature data from the NCEI API, which can be found at <a href="https://www.ncdc.noaa.gov/cdo-web/webservices/v2">https://www.ncdc.noaa.gov/cdo-web/webservices/v2</a>, in the <strong class="source-inline">2-using_the_weather_api.ipynb</strong> notebook. The <a id="_idIndexMarker288"/>documentation for the <strong class="bold">Global Historical Climatology Network – Daily</strong> (<strong class="bold">GHCND</strong>) dataset we will be using can be found at <a href="https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf">https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf</a>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The <a id="_idIndexMarker289"/>NCEI is part of the <strong class="bold">National Oceanic and Atmospheric Administration</strong> (<strong class="bold">NOAA</strong>). As indicated by the URL for the API, this resource was created when the NCEI was called the NCDC. Should the URL for this resource change in the future, search for <em class="italic">NCEI weather API</em> to find the updated one.</p>
			<p>In the <strong class="source-inline">3-cleaning_data.ipynb</strong> notebook, we will learn how to perform an initial round of cleaning on the temperature data and some financial data, which was collected using the <strong class="source-inline">stock_analysis</strong> package that we will build in <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>. Afterward, we will walk through ways to reshape our data in the <strong class="source-inline">4-reshaping_data.ipynb</strong> notebook. Finally, in the <strong class="source-inline">5-handling_data_issues.ipynb</strong> notebook, we will learn about some strategies for dealing with duplicate, missing, or invalid data using some dirty data that can be found in <strong class="source-inline">data/dirty_data.csv</strong>. The text will indicate when it's time to switch between notebooks.</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Understanding data wrangling</h1>
			<p>Like any professional field, data analysis is filled with buzzwords, and it can often be difficult for <a id="_idIndexMarker290"/>newcomers to understand the lingo—the topic of this chapter is no exception. When we perform <strong class="bold">data wrangling</strong>, we are taking our input data from its original state and putting it in a format where we can perform meaningful <a id="_idIndexMarker291"/>analysis on it. <strong class="bold">Data manipulation</strong> is another way to refer to this process. There is no set list of operations; the only goal is that the data post-wrangling is more useful to us than when we started. In practice, there are three common tasks involved in the data wrangling process:</p>
			<ul>
				<li>Data cleaning</li>
				<li>Data transformation</li>
				<li>Data enrichment</li>
			</ul>
			<p>It should be noted that there is no inherent order to these tasks, and it is highly probable that we will perform each many times throughout the data wrangling process. This idea brings <a id="_idIndexMarker292"/>up an interesting conundrum: if we need to wrangle our data to prepare it for our analysis, isn't it possible to wrangle it in such a way that we tell the data what to say instead of us learning what it is saying?</p>
			<p class="author-quote">"If you torture the data long enough, it will confess to anything."</p>
			<p class="author-quote">— Ronald Coase, winner of a Nobel Prize in Economics</p>
			<p>Those working with data will find it is very easy to distort the truth by manipulating the data. However, it is our duty to do our best to avoid deceit by keeping the effect our actions have on the data's integrity in mind, and by explaining the process we took to draw our conclusions to the people who consume our analyses, so that they too may make their own judgments.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/>Data cleaning</h2>
			<p>Once we have <a id="_idIndexMarker293"/>collected our data, brought it into a <strong class="source-inline">DataFrame</strong> object, and used the skills we discussed in <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>, to familiarize ourselves with the data, we will need to perform some <a id="_idIndexMarker294"/>data cleaning. An initial round of data cleaning will often give us the bare minimum we need to start exploring our data. Some essential data cleaning tasks to master include the following:</p>
			<ul>
				<li>Renaming</li>
				<li>Sorting and reordering</li>
				<li>Data type conversions</li>
				<li>Handling duplicate data</li>
				<li>Addressing missing or invalid data</li>
				<li>Filtering to the desired subset of data</li>
			</ul>
			<p>Data cleaning is the best starting point for data wrangling, since having the data stored as the correct <a id="_idIndexMarker295"/>data types and easy-to-reference <a id="_idIndexMarker296"/>names will open up many avenues for exploration, such as summary statistics, sorting, and filtering. Since we covered filtering in <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>, we will focus on the other topics from the preceding list in this chapter.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor065"/>Data transformation</h2>
			<p>Frequently, we will reach the data transformation stage after some initial data cleaning, but it is <a id="_idIndexMarker297"/>entirely possible that our dataset is unusable in its current <a id="_idIndexMarker298"/>shape, and we must restructure it before attempting to do any data cleaning. In <strong class="bold">data transformation</strong>, we focus on changing our data's structure to facilitate our downstream analyses; this usually involves changing which data goes along the rows and which goes down the columns.</p>
			<p>Most data we <a id="_idIndexMarker299"/>will find is either <strong class="bold">wide format</strong> or <strong class="bold">long format</strong>; each of these <a id="_idIndexMarker300"/>formats has its merits, and it's important to know which one we will need for our analysis. Often, people will record and present data in wide format, but there are certain visualizations that require the data to be in long format:</p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/Figure_3.2_B16834.jpg" alt="Figure 3.2 – (Left) wide format versus (right) long format&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – (Left) wide format versus (right) long format</p>
			<p>Wide format is preferred for analysis and database design, while long format is considered poor design because each column should be its own data type and have a singular meaning. However, in cases where new fields will be added (or old ones removed) from a table in a relational database, rather than having to alter all the tables each time, the database's maintainers may decide to use the long format. This allows them to provide a fixed schema for users of the database, while being able to update the data it contains as needed. When building an API, the long format may be chosen if flexibility is required. Perhaps the API will provide a generic response format (for instance, date, field name, and field value) that can support various tables from a database. This may <a id="_idIndexMarker301"/>also have to do with making the response easier <a id="_idIndexMarker302"/>to form, depending on how the data is stored in the database the API uses. Since we will find data in both of these formats, it's important we understand how to work with both of them and go from one to the other.</p>
			<p>Now, let's navigate to the <strong class="source-inline">1-wide_vs_long.ipynb</strong> notebook to see some examples. First, we will import <strong class="source-inline">pandas</strong> and <strong class="source-inline">matplotlib</strong> (to help illustrate the strengths and weaknesses of each format when it comes to visualizations, which we will discuss in <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>, and <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>) and read in the CSV files containing wide and long format data:</p>
			<p class="source-code">&gt;&gt;&gt; import matplotlib.pyplot as plt</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; wide_df = \</p>
			<p class="source-code">...     pd.read_csv('data/wide_data.csv', parse_dates=['date'])</p>
			<p class="source-code">&gt;&gt;&gt; long_df = pd.read_csv(</p>
			<p class="source-code">...     'data/long_data.csv', </p>
			<p class="source-code">...     usecols=['date', 'datatype', 'value'], </p>
			<p class="source-code">...     parse_dates=['date']</p>
			<p class="source-code">... )[['date', 'datatype', 'value']] # sort columns</p>
			<h3>The wide data format</h3>
			<p>With wide format data, we represent measurements of variables with their own columns, and <a id="_idIndexMarker303"/>each row represents an observation <a id="_idIndexMarker304"/>of those variables. This makes it easy for us to compare variables across observations, get summary statistics, perform operations, and present our data; however, some visualizations don't work with this data format because they may rely on the long format to split, size, and/or color the plot content.</p>
			<p>Let's look at the top six observations from the wide format data in <strong class="source-inline">wide_df</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; wide_df.head(6)</p>
			<p>Each column contains the top six observations of a specific class of temperature data in degrees Celsius—maximum temperature (<strong class="bold">TMAX</strong>), minimum temperature (<strong class="bold">TMIN</strong>), and temperature at the time of observation (<strong class="bold">TOBS</strong>)—at a daily frequency: </p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/Figure_3.3_B16834.jpg" alt="Figure 3.3 – Wide format temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Wide format temperature data</p>
			<p>When working with wide format data, we can easily grab summary statistics on this data by using the <strong class="source-inline">describe()</strong> method. Note that while older versions of <strong class="source-inline">pandas</strong> treated <strong class="source-inline">datetimes</strong> as categorical, <strong class="source-inline">pandas</strong> is moving toward treating them as numeric, so we pass <strong class="source-inline">datetime_is_numeric=True</strong> to suppress the warning:</p>
			<p class="source-code">&gt;&gt;&gt; wide_df.describe(include='all', datetime_is_numeric=True)</p>
			<p>With hardly any effort on our part, we get summary statistics for the dates, maximum temperature, minimum temperature, and temperature at the time of observation:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/Figure_3.4_B16834.jpg" alt="Figure 3.4 – Summary statistics for the wide format temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Summary statistics for the wide format temperature data</p>
			<p>As we discussed <a id="_idIndexMarker305"/>previously, the summary data in the preceding table is <a id="_idIndexMarker306"/>easy to obtain and is informative. This format can easily be plotted with <strong class="source-inline">pandas</strong> as well, provided we tell it exactly what we want to plot:</p>
			<p class="source-code">&gt;&gt;&gt; wide_df.plot(</p>
			<p class="source-code">...     x='date', y=['TMAX', 'TMIN', 'TOBS'], figsize=(15, 5),</p>
			<p class="source-code">...     title='Temperature in NYC in October 2018' </p>
			<p class="source-code">... ).set_ylabel('Temperature in Celsius')</p>
			<p class="source-code">&gt;&gt;&gt; plt.show()</p>
			<p>Pandas plots the daily maximum temperature, minimum temperature, and temperature at the time of observation as their own lines on a single line plot:</p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/Figure_3.5_B16834.jpg" alt="Figure 3.5 – Plotting the wide format temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Plotting the wide format temperature data</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Don't worry about understanding the visualization code right now; it's here just to illustrate how each of these data formats can make certain tasks easier or harder. We will cover visualizations with <strong class="source-inline">pandas</strong> and <strong class="source-inline">matplotlib</strong> in <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>.</p>
			<h3>The long data format</h3>
			<p>Long format <a id="_idIndexMarker307"/>data will have a row for each observation <a id="_idIndexMarker308"/>of a variable; this means that, if we have three variables being measured daily, we will have three rows for each day we record observations. The long format setup can be achieved by turning the variable column names into a single column, where the data is the variable name, and putting their values in a separate column. </p>
			<p>We can look at the top six rows of the long format data in <strong class="source-inline">long_df</strong> to see the difference between wide format and long format data:</p>
			<p class="source-code">&gt;&gt;&gt; long_df.head(6)</p>
			<p>Notice that we now have three entries for each date, and the <strong class="bold">datatype</strong> column tells us what the data in the <strong class="bold">value</strong> column is for that row:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/Figure_3.6_B16834.jpg" alt="Figure 3.6 – Long format temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Long format temperature data</p>
			<p>If we try to <a id="_idIndexMarker309"/>get summary statistics, like we did with <a id="_idIndexMarker310"/>the wide format data, the result isn't as helpful:</p>
			<p class="source-code">&gt;&gt;&gt; long_df.describe(include='all', datetime_is_numeric=True)</p>
			<p>The <strong class="bold">value</strong> column shows us summary statistics, but this is summarizing the daily maximum temperatures, minimum temperatures, and temperatures at the time of observation. The maximum will be the maximum of the daily maximum temperatures and the minimum will be the minimum of the daily minimum temperatures. This means that this summary data is not very helpful:</p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/Figure_3.7_B16834.jpg" alt="Figure 3.7 – Summary statistics for the long format temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – Summary statistics for the long format temperature data</p>
			<p>This format is not very easy to digest and certainly shouldn't be how we present data; however, it makes <a id="_idIndexMarker311"/>it easy to create visualizations <a id="_idIndexMarker312"/>where our plotting library can color lines by the name of the variable, size the points by the values of a certain variable, and perform splits for faceting. Pandas expects its data for plotting to be in wide format, so, to easily make the same plot that we did with the wide format data, we must use another plotting library, called <strong class="source-inline">seaborn</strong>, which we will cover in <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>:</p>
			<p class="source-code">&gt;&gt;&gt; import seaborn as sns</p>
			<p class="source-code">&gt;&gt;&gt; sns.set(rc={'figure.figsize': (15, 5)}, style='white')</p>
			<p class="source-code">&gt;&gt;&gt; ax = sns.lineplot(</p>
			<p class="source-code">...     data=long_df, x='date', y='value', hue='datatype'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; ax.set_ylabel('Temperature in Celsius')</p>
			<p class="source-code">&gt;&gt;&gt; ax.set_title('Temperature in NYC in October 2018')</p>
			<p class="source-code">&gt;&gt;&gt; plt.show()</p>
			<p>Seaborn can subset based on the <strong class="source-inline">datatype</strong> column to give us individual lines for the daily maximum temperature, minimum temperature, and temperature at the time of observation: </p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/Figure_3.8_B16834.jpg" alt="Figure 3.8 – Plotting the long format temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – Plotting the long format temperature data</p>
			<p>Seaborn lets us <a id="_idIndexMarker313"/>specify the column to use for <strong class="source-inline">hue</strong>, which <a id="_idIndexMarker314"/>colored the lines in <em class="italic">Figure 3.8</em> by the temperature type. We aren't limited to this, though; with long format data, we can easily facet our plots:</p>
			<p class="source-code">&gt;&gt;&gt; sns.set(</p>
			<p class="source-code">...     rc={'figure.figsize': (20, 10)},</p>
			<p class="source-code">...     style='white', font_scale=2 </p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; g = sns.FacetGrid(long_df, col='datatype', height=10)</p>
			<p class="source-code">&gt;&gt;&gt; g = g.map(plt.plot, 'date', 'value')</p>
			<p class="source-code">&gt;&gt;&gt; g.set_titles(size=25)</p>
			<p class="source-code">&gt;&gt;&gt; g.set_xticklabels(rotation=45)</p>
			<p class="source-code">&gt;&gt;&gt; plt.show()</p>
			<p>Seaborn can use long format data to create subplots for each distinct value in the <strong class="source-inline">datatype</strong> column:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/Figure_3.9_B16834.jpg" alt="Figure 3.9 – Plotting subsets of the long format temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – Plotting subsets of the long format temperature data</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">While it is possible to create a plot similar to the preceding one with <strong class="source-inline">pandas</strong> and <strong class="source-inline">matplotlib</strong> using subplots, more complicated combinations of facets will make using <strong class="source-inline">seaborn</strong> infinitely easier. We will cover <strong class="source-inline">seaborn</strong> in <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>.</p>
			<p>In <a id="_idIndexMarker315"/>the <em class="italic">Reshaping data</em> section, we will cover how to <a id="_idIndexMarker316"/>transform our data from wide to long format by melting, and from long to wide format by pivoting. Additionally, we will learn how to transpose data, which flips the columns and the rows.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor066"/>Data enrichment</h2>
			<p>Once we have our cleaned data in the format we need for our analysis, we may find the need to <a id="_idIndexMarker317"/>enrich the data a bit. <strong class="bold">Data enrichment</strong> improves <a id="_idIndexMarker318"/>the quality of the data by adding to it in one way or another. This process becomes very important in modeling and in machine learning, where it forms part of the <strong class="bold">feature engineering</strong> process (which we will <a id="_idIndexMarker319"/>touch on in <a href="B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 10</em></a>, <em class="italic">Making Better Predictions – Optimizing Models</em>). </p>
			<p>When we're looking to enrich our data, we can either <strong class="bold">merge</strong> new data with the original data (by appending new rows or columns) or use the original data to create new data. The following are ways to enhance our data using the original data:</p>
			<ul>
				<li><strong class="bold">Adding new columns</strong>: Using functions on <a id="_idIndexMarker320"/>the data from existing columns to create new values.</li>
				<li><strong class="bold">Binning</strong>: Turning <a id="_idIndexMarker321"/>continuous data or discrete data with many distinct values into buckets, which makes the column discrete while letting us control the number of possible values in the column.</li>
				<li><strong class="bold">Aggregating</strong>: Rolling <a id="_idIndexMarker322"/>up the data and summarizing it.</li>
				<li><strong class="bold">Resampling</strong>: Aggregating <a id="_idIndexMarker323"/>time series data at specific intervals.</li>
			</ul>
			<p>Now that <a id="_idIndexMarker324"/>we understand what data wrangling is, let's <a id="_idIndexMarker325"/>collect some data to work with. Note that we will cover data cleaning and transformation in this chapter, while data enrichment will be covered in <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/>Exploring an API to find and collect temperature data</h1>
			<p>In <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>, we worked on data collection and how <a id="_idIndexMarker326"/> to perform an initial inspection <a id="_idIndexMarker327"/>and filtering of the data; this <a id="_idIndexMarker328"/>usually gives us ideas of things that need to be addressed before we move further in our analysis. Since this chapter builds on those skills, we will get to practice some of them here as well. To begin, we will start by exploring the weather API that's provided by the NCEI. Then, in the next section, we will learn about data wrangling using temperature data that was previously obtained from this API.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">To use the NCEI API, you will have to request a token by filling out this form with your email address: <a href="https://www.ncdc.noaa.gov/cdo-web/token">https://www.ncdc.noaa.gov/cdo-web/token</a>.</p>
			<p>For this section, we will be working in the <strong class="source-inline">2-using_the_weather_api.ipynb</strong> notebook to <a id="_idIndexMarker329"/>request temperature data from the NCEI API. As we learned in <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>,<em class="italic">Working with Pandas DataFrames</em>, we can use the <strong class="source-inline">requests</strong> library to interact with APIs. In the following <a id="_idIndexMarker330"/>code block, we import the <strong class="source-inline">requests</strong> library <a id="_idIndexMarker331"/>and create a convenience function for making the requests to a specific endpoint, sending our token along. To use this function, we need to provide a token, as indicated in bold: </p>
			<p class="source-code">&gt;&gt;&gt; import requests</p>
			<p class="source-code">&gt;&gt;&gt; def make_request(endpoint, payload=None):</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     Make a request to a specific endpoint on the </p>
			<p class="source-code">...     weather API passing headers and optional payload.</p>
			<p class="source-code">...     Parameters:</p>
			<p class="source-code">...         - endpoint: The endpoint of the API you want to </p>
			<p class="source-code">...                     make a GET request to.</p>
			<p class="source-code">...         - payload: A dictionary of data to pass along </p>
			<p class="source-code">...                    with the request.</p>
			<p class="source-code">...     </p>
			<p class="source-code">...     Returns:</p>
			<p class="source-code">...         A response object.</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     return requests.get(</p>
			<p class="source-code">...         'https://www.ncdc.noaa.gov/cdo-web/'</p>
			<p class="source-code">...         f'api/v2/{endpoint}',</p>
			<p class="source-code">...         headers={'token': '<strong class="bold">PASTE_YOUR_TOKEN_HERE</strong>'},</p>
			<p class="source-code">...         params=payload</p>
			<p class="source-code">...     )</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">This function is <a id="_idIndexMarker332"/>making use of <strong class="bold">f-strings</strong>, which were introduced in Python 3.6; they improve code readability and reduce verbosity compared to using the <strong class="source-inline">format()</strong> method: <strong class="source-inline">'api/v2/{}'.format(endpoint)</strong>.</p>
			<p>To use the <strong class="source-inline">make_request()</strong> function, we need to learn how to form our request. The NCEI <a id="_idIndexMarker333"/>has a helpful getting started page (<a href="https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted">https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted</a>) that shows us <a id="_idIndexMarker334"/>how to form requests; we can <a id="_idIndexMarker335"/>progress through the tabs on the page to figure out what filters we want on our query. The <strong class="source-inline">requests</strong> library takes care of turning our dictionary of search parameters (passed in as <strong class="source-inline">payload</strong>) into a <strong class="bold">query string</strong> that gets appended to the end URL (for example, if we pass <strong class="source-inline">2018-08-28</strong> for <strong class="source-inline">start</strong> and <strong class="source-inline">2019-04-15</strong> for <strong class="source-inline">end</strong>, we will get <strong class="source-inline">?start=2018-08-28&amp;end=2019-04-15</strong>), just like the examples on the website. This API provides many different endpoints for exploring what is offered and building up our ultimate request for the actual dataset. We will start by figuring out the ID of the dataset we want to query for (<strong class="source-inline">datasetid</strong>) using the <strong class="source-inline">datasets</strong> endpoint. Let's check which datasets have data within the date range of October 1, 2018 through today:</p>
			<p class="source-code">&gt;&gt;&gt; response = \</p>
			<p class="source-code">...     make_request('datasets', {'startdate': '2018-10-01'})</p>
			<p>Remember that we check the <strong class="source-inline">status_code</strong> attribute to make sure the request was successful. Alternatively, we can use the <strong class="source-inline">ok</strong> attribute to get a Boolean indicator if everything went as expected:</p>
			<p class="source-code">&gt;&gt;&gt; response.status_code</p>
			<p class="source-code">200</p>
			<p class="source-code">&gt;&gt;&gt; response.ok</p>
			<p class="source-code">True</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The API limits us to 5 requests per second and 10,000 requests per day. If we exceed these limits, the status code will indicate a client error (meaning that the error appears to have been caused by us). Client errors have status codes in the 400s; for example, 404, if the requested resource can't be found, or 400, if the server can't understand our request (or refuses to process it). Sometimes, the server has an issue on its side when processing our request, in which case we see status codes in the 500s. You can find a listing of <a id="_idIndexMarker336"/>common status codes and their meanings at <a href="https://restfulapi.net/http-status-codes/">https://restfulapi.net/http-status-codes/</a>.</p>
			<p>Once <a id="_idIndexMarker337"/>we have our response, we can <a id="_idIndexMarker338"/>use the <strong class="source-inline">json()</strong> method to get the payload. Then, we <a id="_idIndexMarker339"/>can use dictionary methods to determine which part we want to look at:</p>
			<p class="source-code">&gt;&gt;&gt; payload = <strong class="bold">response.json()</strong></p>
			<p class="source-code">&gt;&gt;&gt; payload.keys()</p>
			<p class="source-code">dict_keys(['metadata', 'results'])</p>
			<p>The <strong class="source-inline">metadata</strong> portion of the JSON payload tells us information about the result, while the <strong class="source-inline">results</strong> section contains the actual results. Let's see how much data we got back, so that we know whether we can print the results or whether we should try to limit the output:</p>
			<p class="source-code">&gt;&gt;&gt; payload<strong class="bold">['metadata']</strong></p>
			<p class="source-code">{'resultset': {'offset': 1, <strong class="bold">'count': 11</strong>, 'limit': 25}}</p>
			<p>We got back 11 rows, so let's see what fields are in the <strong class="source-inline">results</strong> portion of the JSON payload. The <strong class="source-inline">results</strong> key contains a list of dictionaries. If we select the first one, we can look at the keys to see what fields the data contains. We can then reduce the output to the fields we care about:</p>
			<p class="source-code">&gt;&gt;&gt; payload<strong class="bold">['results'][0].keys()</strong></p>
			<p class="source-code">dict_keys(['uid', 'mindate', 'maxdate', <strong class="bold">'name'</strong>, </p>
			<p class="source-code">           'datacoverage', <strong class="bold">'id'</strong>])</p>
			<p>For our <a id="_idIndexMarker340"/>purposes, we want <a id="_idIndexMarker341"/>to look at the IDs and names <a id="_idIndexMarker342"/>of the datasets, so let's use a list comprehension to look at those only:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">[(data['id'], data['name']) for data in payload['results']]</strong></p>
			<p class="source-code">[<strong class="bold">('GHCND', 'Daily Summaries')</strong>,</p>
			<p class="source-code"> ('GSOM', 'Global Summary of the Month'),</p>
			<p class="source-code"> ('GSOY', 'Global Summary of the Year'),</p>
			<p class="source-code"> ('NEXRAD2', 'Weather Radar (Level II)'),</p>
			<p class="source-code"> ('NEXRAD3', 'Weather Radar (Level III)'),</p>
			<p class="source-code"> ('NORMAL_ANN', 'Normals Annual/Seasonal'),</p>
			<p class="source-code"> ('NORMAL_DLY', 'Normals Daily'),</p>
			<p class="source-code"> ('NORMAL_HLY', 'Normals Hourly'),</p>
			<p class="source-code"> ('NORMAL_MLY', 'Normals Monthly'),</p>
			<p class="source-code"> ('PRECIP_15', 'Precipitation 15 Minute'),</p>
			<p class="source-code"> ('PRECIP_HLY', 'Precipitation Hourly')]</p>
			<p>The first entry in the result is what we are looking for. Now that we have a value for <strong class="source-inline">datasetid</strong> (<strong class="source-inline">GHCND</strong>), we proceed to identify one for <strong class="source-inline">datacategoryid</strong>, which we need to request temperature data. We do so using the <strong class="source-inline">datacategories</strong> endpoint. Here, we can print the JSON payload since it isn't that large (only nine entries):</p>
			<p class="source-code">&gt;&gt;&gt; response = make_request(</p>
			<p class="source-code">...     'datacategories', <strong class="bold">payload={'datasetid': 'GHCND'}</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; response.status_code</p>
			<p class="source-code">200</p>
			<p class="source-code">&gt;&gt;&gt; response.json()['results']</p>
			<p class="source-code">[{'name': 'Evaporation', 'id': 'EVAP'},</p>
			<p class="source-code"> {'name': 'Land', 'id': 'LAND'},</p>
			<p class="source-code"> {'name': 'Precipitation', 'id': 'PRCP'},</p>
			<p class="source-code"> {'name': 'Sky cover &amp; clouds', 'id': 'SKY'},</p>
			<p class="source-code"> {'name': 'Sunshine', 'id': 'SUN'},</p>
			<p class="source-code"> <strong class="bold">{'name': 'Air Temperature', 'id': 'TEMP'}</strong>,</p>
			<p class="source-code"> {'name': 'Water', 'id': 'WATER'},</p>
			<p class="source-code"> {'name': 'Wind', 'id': 'WIND'},</p>
			<p class="source-code"> {'name': 'Weather Type', 'id': 'WXTYPE'}]</p>
			<p>Based on <a id="_idIndexMarker343"/>the previous result, we know <a id="_idIndexMarker344"/>that we want a value of <strong class="source-inline">TEMP</strong> for <strong class="source-inline">datacategoryid</strong>. Next, we use this to identify the data types we want by using <a id="_idIndexMarker345"/>the <strong class="source-inline">datatypes</strong> endpoint. We will use a list comprehension once again to only print the names and IDs; this is still a rather large list, so the output has been abbreviated:</p>
			<p class="source-code">&gt;&gt;&gt; response = make_request(</p>
			<p class="source-code">...     'datatypes', </p>
			<p class="source-code">...     payload={<strong class="bold">'datacategoryid': 'TEMP'</strong>, 'limit': 100}</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; response.status_code</p>
			<p class="source-code">200</p>
			<p class="source-code">&gt;&gt;&gt; [(datatype['id'], datatype['name'])</p>
			<p class="source-code">...  for datatype in response.json()['results']]</p>
			<p class="source-code">[('CDSD', 'Cooling Degree Days Season to Date'),</p>
			<p class="source-code"> ...,</p>
			<p class="source-code"><strong class="bold"> ('TAVG', 'Average Temperature.'),</strong></p>
			<p class="source-code"><strong class="bold"> ('TMAX', 'Maximum temperature'),</strong></p>
			<p class="source-code"><strong class="bold"> ('TMIN', 'Minimum temperature'),</strong></p>
			<p class="source-code"> ('TOBS', 'Temperature at the time of observation')]</p>
			<p>We are <a id="_idIndexMarker346"/>looking for the <strong class="source-inline">TAVG</strong>, <strong class="source-inline">TMAX</strong>, and <strong class="source-inline">TMIN</strong> data types. Now that we have everything we need to request <a id="_idIndexMarker347"/>temperature data for all locations, we <a id="_idIndexMarker348"/>need to narrow it down to a specific location. To determine a value for <strong class="source-inline">locationcategoryid</strong>, we must use the <strong class="source-inline">locationcategories</strong> endpoint:</p>
			<p class="source-code">&gt;&gt;&gt; response = make_request(</p>
			<p class="source-code">...     'locationcategories', payload={'datasetid': 'GHCND'}</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; response.status_code</p>
			<p class="source-code">200</p>
			<p>Note that <a id="_idIndexMarker349"/>we can use <strong class="source-inline">pprint</strong> from the Python standard library (<a href="https://docs.python.org/3/library/pprint.html">https://docs.python.org/3/library/pprint.html</a>) to print our JSON payload in an easier-to-read format:</p>
			<p class="source-code">&gt;&gt;&gt; import pprint</p>
			<p class="source-code">&gt;&gt;&gt; pprint.pprint(response.json())</p>
			<p class="source-code">{'metadata': {</p>
			<p class="source-code">     'resultset': {'count': 12, 'limit': 25, 'offset': 1}},</p>
			<p class="source-code"> 'results': [<strong class="bold">{'id': 'CITY', 'name': 'City'}</strong>,</p>
			<p class="source-code">             {'id': 'CLIM_DIV', 'name': 'Climate Division'},</p>
			<p class="source-code">             {'id': 'CLIM_REG', 'name': 'Climate Region'},</p>
			<p class="source-code">             {'id': 'CNTRY', 'name': 'Country'},</p>
			<p class="source-code">             {'id': 'CNTY', 'name': 'County'},</p>
			<p class="source-code">             ...,</p>
			<p class="source-code">             {'id': 'ST', 'name': 'State'},</p>
			<p class="source-code">             {'id': 'US_TERR', 'name': 'US Territory'},</p>
			<p class="source-code">             {'id': 'ZIP', 'name': 'Zip Code'}]}</p>
			<p>We want to look at New York City, so, for the <strong class="source-inline">locationcategoryid</strong> filter, <strong class="source-inline">CITY</strong> is the proper value. The notebook we are working in has a function to search for a field by name using <strong class="bold">binary search</strong> on the API; binary search is a more efficient way of searching <a id="_idIndexMarker350"/>through an ordered list. Since we know that the fields can be sorted alphabetically, and the API gives us metadata about the request, we know how many items the API has for a given field and can tell whether we have passed the one we are looking for.</p>
			<p>With each <a id="_idIndexMarker351"/>request, we grab <a id="_idIndexMarker352"/>the middle entry and compare <a id="_idIndexMarker353"/>its location in the alphabet with our target; if the result comes before our target, we look at the half of the data that's greater than what we just got; otherwise, we look at the smaller half. Each time, we are slicing the data in half, so when we grab the middle entry to test, we are moving closer to the value we seek (see <em class="italic">Figure 3.10</em>):</p>
			<p class="source-code">&gt;&gt;&gt; def get_item(name, what, endpoint, start=1, end=None):</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     Grab the JSON payload using binary search.</p>
			<p class="source-code">... </p>
			<p class="source-code">...     Parameters:</p>
			<p class="source-code">...         - name: The item to look for.</p>
			<p class="source-code">...         - what: Dictionary specifying what item `name` is.</p>
			<p class="source-code">...         - endpoint: Where to look for the item.</p>
			<p class="source-code">...         - start: The position to start at. We don't need</p>
			<p class="source-code">...           to touch this, but the function will manipulate</p>
			<p class="source-code">...           this with recursion.</p>
			<p class="source-code">...         - end: The last position of the items. Used to </p>
			<p class="source-code">...           find the midpoint, but like `start` this is not </p>
			<p class="source-code">...           something we need to worry about.</p>
			<p class="source-code">... </p>
			<p class="source-code">...     Returns: Dictionary of the information for the item </p>
			<p class="source-code">...              if found, otherwise an empty dictionary.</p>
			<p class="source-code">...     """</p>
			<p class="source-code">...     # find the midpoint to cut the data in half each time </p>
			<p class="source-code">...     mid = (start + (end or 1)) // 2</p>
			<p class="source-code">...     </p>
			<p class="source-code">...     # lowercase the name so this is not case-sensitive</p>
			<p class="source-code">...     name = name.lower()</p>
			<p class="source-code">...     # define the payload we will send with each request</p>
			<p class="source-code">...     payload = {</p>
			<p class="source-code">...         'datasetid': 'GHCND', 'sortfield': 'name',</p>
			<p class="source-code">...         'offset': mid, # we'll change the offset each time</p>
			<p class="source-code">...         'limit': 1 # we only want one value back</p>
			<p class="source-code">...     }</p>
			<p class="source-code">...     </p>
			<p class="source-code">...     # make request adding additional filters from `what`</p>
			<p class="source-code">...     response = make_request(endpoint, {**payload, **what})</p>
			<p class="source-code">...     </p>
			<p class="source-code">...     if response.ok:</p>
			<p class="source-code">...         payload = response.json()</p>
			<p class="source-code">...     </p>
			<p class="source-code">...         # if ok, grab the end index from the response </p>
			<p class="source-code">...         # metadata the first time through</p>
			<p class="source-code">...         end = end or \</p>
			<p class="source-code">...             payload['metadata']['resultset']['count']</p>
			<p class="source-code">...         </p>
			<p class="source-code">...         # grab the lowercase version of the current name</p>
			<p class="source-code">...         current_name = \</p>
			<p class="source-code">...             payload['results'][0]['name'].lower()  </p>
			<p class="source-code">...</p>
			<p class="source-code">...         # if what we are searching for is in the current </p>
			<p class="source-code">...         # name, we have found our item</p>
			<p class="source-code">...         if name in current_name:</p>
			<p class="source-code">...             # return the found item</p>
			<p class="source-code">...             return payload['results'][0] </p>
			<p class="source-code">...         else:</p>
			<p class="source-code">...             if start &gt;= end: </p>
			<p class="source-code">...                 # if start index is greater than or equal</p>
			<p class="source-code">...                 # to end index, we couldn't find it</p>
			<p class="source-code">...                 return {}</p>
			<p class="source-code">...             elif name &lt; current_name:</p>
			<p class="source-code">...                 # name comes before the current name in the </p>
			<p class="source-code">...                 # alphabet =&gt; search further to the left</p>
			<p class="source-code">...                 return get_item(name, what, endpoint, </p>
			<p class="source-code">...                                 start, mid - 1)</p>
			<p class="source-code">...             elif name &gt; current_name:</p>
			<p class="source-code">...                 # name comes after the current name in the </p>
			<p class="source-code">...                 # alphabet =&gt; search further to the right</p>
			<p class="source-code">...                 return get_item(name, what, endpoint,</p>
			<p class="source-code">...                                 mid + 1, end) </p>
			<p class="source-code">...     else:</p>
			<p class="source-code">...         # response wasn't ok, use code to determine why</p>
			<p class="source-code">...         print('Response not OK, '</p>
			<p class="source-code">...               f'status: {response.status_code}')</p>
			<p>This is a <strong class="bold">recursive</strong> implementation <a id="_idIndexMarker354"/>of the algorithm, meaning <a id="_idIndexMarker355"/>that we call the function <a id="_idIndexMarker356"/>itself from inside; we must be <a id="_idIndexMarker357"/>very careful when we do this to define a <strong class="bold">base condition</strong> so that it <a id="_idIndexMarker358"/>will eventually stop and not enter an infinite loop. It is possible to implement this iteratively. See the <em class="italic">Further reading</em> section at the end of this chapter for additional reading on binary search and <strong class="bold">recursion</strong>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In a traditional implementation <a id="_idIndexMarker359"/>of binary search, it is trivial to find the length of the list that we are searching. With the API, we have to make one request to get the count; therefore, we must ask for the first entry (offset of 1) to orient ourselves. This means that we make an extra request here compared to what we would have needed if we knew how many locations were in the list before starting. </p>
			<p>Now, let's use the binary search implementation to find the ID for New York City, which will be the value we will use for <strong class="source-inline">locationid</strong> in subsequent queries:</p>
			<p class="source-code">&gt;&gt;&gt; nyc = get_item(</p>
			<p class="source-code">...     'New York', {'locationcategoryid': 'CITY'}, 'locations'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; nyc</p>
			<p class="source-code">{'mindate': '1869-01-01',</p>
			<p class="source-code"> 'maxdate': '2021-01-14',</p>
			<p class="source-code"> 'name': 'New York, NY US',</p>
			<p class="source-code"> 'datacoverage': 1,</p>
			<p class="source-code"> <strong class="bold">'id': 'CITY:US360019'</strong>}</p>
			<p>By using <a id="_idIndexMarker360"/>binary search <a id="_idIndexMarker361"/>here, we find <strong class="bold">New York</strong> in <strong class="bold">8</strong> requests, despite <a id="_idIndexMarker362"/>it being close to the middle of 1,983 entries! For comparison, using linear search, we would have looked at 1,254 entries before finding it. In the following diagram, we can see how binary search eliminates sections of the list of locations systematically, which is represented by black on the number line (white means it is still possible that the desired value is in that section):</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/Figure_3.10_B16834.jpg" alt="Figure 3.10 – Binary search to find New York City&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10 – Binary search to find New York City</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Some APIs (such as the NCEI API) restrict the number of requests we can make within certain periods of time, so it's important to be smart about our requests. When searching a very long ordered list, think of binary search.</p>
			<p>Optionally, we <a id="_idIndexMarker363"/>can drill down to the ID of <a id="_idIndexMarker364"/>the station that is collecting the data. This <a id="_idIndexMarker365"/>is the most granular level. Using binary search again, we can grab the station ID for the Central Park station:</p>
			<p class="source-code">&gt;&gt;&gt; central_park = get_item(</p>
			<p class="source-code">...     'NY City Central Park',</p>
			<p class="source-code">...     {'locationid': nyc['id']}, 'stations'</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; central_park</p>
			<p class="source-code">{'elevation': 42.7,</p>
			<p class="source-code"> 'mindate': '1869-01-01',</p>
			<p class="source-code"> 'maxdate': '2020-01-13',</p>
			<p class="source-code"> 'latitude': 40.77898,</p>
			<p class="source-code"> 'name': 'NY CITY CENTRAL PARK, NY US',</p>
			<p class="source-code"> 'datacoverage': 1,</p>
			<p class="source-code"> 'id': 'GHCND:USW00094728',</p>
			<p class="source-code"> 'elevationUnit': 'METERS',</p>
			<p class="source-code"> 'longitude': -73.96925}</p>
			<p>Now, let's request NYC's temperature data in Celsius for October 2018, recorded from Central Park. For this, we will use the <strong class="source-inline">data</strong> endpoint and provide all the parameters we picked up throughout our exploration of the API:</p>
			<p class="source-code">&gt;&gt;&gt; response = make_request(</p>
			<p class="source-code">...     'data', </p>
			<p class="source-code">...     {'datasetid': 'GHCND',</p>
			<p class="source-code">...      'stationid': central_park['id'],</p>
			<p class="source-code">...      'locationid': nyc['id'],</p>
			<p class="source-code">...      'startdate': '2018-10-01',</p>
			<p class="source-code">...      'enddate': '2018-10-31',</p>
			<p class="source-code">...      'datatypeid': ['TAVG', 'TMAX', 'TMIN'],</p>
			<p class="source-code">...      'units': 'metric',</p>
			<p class="source-code">...      'limit': 1000}</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; response.status_code</p>
			<p class="source-code">200</p>
			<p>Lastly, we <a id="_idIndexMarker366"/>will create a <strong class="source-inline">DataFrame</strong> object; since <a id="_idIndexMarker367"/>the <strong class="source-inline">results</strong> portion <a id="_idIndexMarker368"/>of the JSON payload is a list of dictionaries, we can pass it directly to <strong class="source-inline">pd.DataFrame()</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.DataFrame(response.json()['results'])</p>
			<p class="source-code">&gt;&gt;&gt; df.head()</p>
			<p>We get back long format data. The <strong class="bold">datatype</strong> column is the temperature variable being measured, and the <strong class="bold">value</strong> column contains the measured temperature:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/Figure_3.11_B16834.jpg" alt="Figure 3.11 – Data retrieved from the NCEI API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – Data retrieved from the NCEI API</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can use the previous code to turn any of the JSON responses we worked with in this section into a <strong class="source-inline">DataFrame</strong> object, if we find that easier to work with. However, it should be stressed that JSON payloads are pretty much ubiquitous when it comes to APIs (and, as Python users, we should be familiar with dictionary-like objects), so it won't hurt to get comfortable with them.</p>
			<p>We asked for <strong class="source-inline">TAVG</strong>, <strong class="source-inline">TMAX</strong>, and <strong class="source-inline">TMIN</strong>, but notice that we didn't get <strong class="source-inline">TAVG</strong>. This is because <a id="_idIndexMarker369"/>the Central Park station <a id="_idIndexMarker370"/>isn't recording average temperature, despite <a id="_idIndexMarker371"/>being listed in the API as offering it—real-world data is dirty:</p>
			<p class="source-code">&gt;&gt;&gt; df.datatype.unique()</p>
			<p class="source-code">array(['TMAX', 'TMIN'], dtype=object)</p>
			<p class="source-code">&gt;&gt;&gt; if get_item(</p>
			<p class="source-code">...     'NY City Central Park', </p>
			<p class="source-code">...     {'locationid': nyc['id'], 'datatypeid': 'TAVG'}, </p>
			<p class="source-code">...     'stations'</p>
			<p class="source-code">... ):</p>
			<p class="source-code">...     print('Found!')</p>
			<p class="source-code">Found!</p>
			<p>Time for plan B: let's use LaGuardia Airport as the station instead of Central Park for the remainder of this chapter. Alternatively, we could have grabbed data for all the stations that cover New York City; however, since this would give us multiple entries per day for some of <a id="_idIndexMarker372"/>the temperature <a id="_idIndexMarker373"/>measurements, we won't do <a id="_idIndexMarker374"/>so here—we would need skills that will be covered in <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>, to work with that data. </p>
			<p>The process of collecting the weather data from the LaGuardia Airport station is the same as with the Central Park station, but in the interest of brevity, we will read in the data for LaGuardia in the next notebook when we discuss cleaning the data. Note that the bottom cells of the current notebook contain the code that's used to collect this data.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>Cleaning data</h1>
			<p>Let's move on to the <strong class="source-inline">3-cleaning_data.ipynb</strong> notebook for our discussion of data cleaning. As usual, we will <a id="_idIndexMarker375"/>begin by importing <strong class="source-inline">pandas</strong> and reading in our data. For this section, we will be using the <strong class="source-inline">nyc_temperatures.csv</strong> file, which contains the maximum daily temperature (<strong class="source-inline">TMAX</strong>), minimum daily temperature (<strong class="source-inline">TMIN</strong>), and the average daily temperature (<strong class="source-inline">TAVG</strong>) from the LaGuardia Airport station in New York City for October 2018:</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.read_csv('data/nyc_temperatures.csv')</p>
			<p class="source-code">&gt;&gt;&gt; df.head()</p>
			<p>We retrieved long format data from the API; for our analysis, we want wide format data, but we will address that in the <em class="italic">Pivoting DataFrames</em> section, later in this chapter:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/Figure_3.12_B16834.jpg" alt="Figure 3.12 – NYC temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – NYC temperature data</p>
			<p>For now, we will <a id="_idIndexMarker376"/>focus on making little tweaks to the data that will make it easier for us to use: renaming columns, converting each column into the most appropriate data type, sorting, and reindexing. Often, this will be the time to filter the data down, but we did that when we worked on requesting data from the API; for a review of filtering with <strong class="source-inline">pandas</strong>, refer to <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor069"/>Renaming columns</h2>
			<p>Since the API endpoint we used could return data of any units and category, it had to call that <a id="_idIndexMarker377"/>column <strong class="source-inline">value</strong>. We only pulled temperature data in Celsius, so all our observations have the same units. This means that we can rename the <strong class="source-inline">value</strong> column so that it's clear what data we are working with:</p>
			<p class="source-code">&gt;&gt;&gt; df.columns</p>
			<p class="source-code">Index(['date', 'datatype', 'station', 'attributes', <strong class="bold">'value'</strong>],</p>
			<p class="source-code">      dtype='object')</p>
			<p>The <strong class="source-inline">DataFrame</strong> class has a <strong class="source-inline">rename()</strong> method that takes a dictionary mapping the old column name to the new column name. In addition to renaming the <strong class="source-inline">value</strong> column, let's rename the <strong class="source-inline">attributes</strong> column to <strong class="source-inline">flags</strong> since the API documentation mentions that that column contains flags for information about data collection:</p>
			<p class="source-code">&gt;&gt;&gt; df.rename(</p>
			<p class="source-code">...     columns={'value': 'temp_C', 'attributes': 'flags'},</p>
			<p class="source-code">...     inplace=True</p>
			<p class="source-code">... )</p>
			<p>Most of the time, <strong class="source-inline">pandas</strong> will return a new <strong class="source-inline">DataFrame</strong> object; however, since we passed in <strong class="source-inline">inplace=True</strong>, our original dataframe was updated instead. Always be careful with in-place operations, as they might be difficult or impossible to undo. Our columns now have their new names:</p>
			<p class="source-code">&gt;&gt;&gt; df.columns</p>
			<p class="source-code">Index(['date', 'datatype', 'station', <strong class="bold">'flags'</strong>, <strong class="bold">'temp_C'</strong>], </p>
			<p class="source-code">      dtype='object')</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Both <strong class="source-inline">Series</strong> and <strong class="source-inline">Index</strong> objects can also be renamed using their <strong class="source-inline">rename()</strong> methods. Simply pass in the new name. For example, if we have a <strong class="source-inline">Series</strong> object called <strong class="source-inline">temperature</strong> and we want to rename it <strong class="source-inline">temp_C</strong>, we can run <strong class="source-inline">temperature.rename('temp_C')</strong>. The variable will still be called <strong class="source-inline">temperature</strong>, but the name of the data in the series itself will now be <strong class="source-inline">temp_C</strong>.</p>
			<p>We can <a id="_idIndexMarker378"/>also do transformations on the column names with <strong class="source-inline">rename()</strong>. For instance, we can put all the column names in uppercase:</p>
			<p class="source-code">&gt;&gt;&gt; df.rename(<strong class="bold">str.upper</strong>, axis='columns').columns</p>
			<p class="source-code">Index([<strong class="bold">'DATE', 'DATATYPE', 'STATION', 'FLAGS', 'TEMP_C'</strong>], </p>
			<p class="source-code">      dtype='object')</p>
			<p>This method even lets us rename the values of the index, although this is something we don't have use for now since our index is just numbers. However, for reference, we would simply change <strong class="source-inline">axis='columns'</strong> in the preceding code to <strong class="source-inline">axis='rows'</strong>.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor070"/>Type conversion</h2>
			<p>Now that <a id="_idIndexMarker379"/>the column names are indicative of the data they contain, we can check what types of data they hold. We should have formed an intuition as to what the data types should be after looking at the first few rows when we inspected the dataframe with the <strong class="source-inline">head()</strong> method previously. With type conversion, we aim to reconcile what the current data types are with what we believe they should be; we will be changing how our data is represented.</p>
			<p>Note that, sometimes, we may have data that we believe should be a certain type, such as a date, but it is stored as a string; this could be for a very valid reason—data could be missing. In the case of missing data encoded as text (for example, <strong class="source-inline">?</strong> or <strong class="source-inline">N/A</strong>), <strong class="source-inline">pandas</strong> will store it as a string when reading it in to allow for this data. It will be marked as <strong class="source-inline">object</strong> when we use the <strong class="source-inline">dtypes</strong> attribute on our dataframe. If we try to convert (or <strong class="bold">cast</strong>) these columns, we will either get an error or our result won't be what we expected. For example, if we have strings of decimal numbers, but try to convert the column into integers, we will get an error since Python knows they aren't integers; however, if we <a id="_idIndexMarker380"/>try to convert decimal numbers into integers, we will lose any information after the decimal point.</p>
			<p>That being said, let's examine the data types in our temperature data. Note that the <strong class="source-inline">date</strong> column isn't actually being stored as a datetime:</p>
			<p class="source-code">&gt;&gt;&gt; df.dtypes</p>
			<p class="source-code"><strong class="bold">date         object</strong></p>
			<p class="source-code">datatype     object</p>
			<p class="source-code">station      object</p>
			<p class="source-code">flags        object<strong class="bold"> </strong> </p>
			<p class="source-code">temp_C      float64</p>
			<p class="source-code">dtype: object</p>
			<p>We can use the <strong class="source-inline">pd.to_datetime()</strong> function to convert it into a datetime:</p>
			<p class="source-code">&gt;&gt;&gt; df.loc[:,'date'] = <strong class="bold">pd.to_datetime(df.date)</strong></p>
			<p class="source-code">&gt;&gt;&gt; df.dtypes </p>
			<p class="source-code"><strong class="bold">date        datetime64[ns]</strong> </p>
			<p class="source-code">datatype            object</p>
			<p class="source-code">station             object</p>
			<p class="source-code">flags               object </p>
			<p class="source-code">temp_C             float64</p>
			<p class="source-code">dtype: object</p>
			<p>This is much better. Now, we can get useful information when we summarize the <strong class="source-inline">date</strong> column:</p>
			<p class="source-code">&gt;&gt;&gt; df.date.describe(datetime_is_numeric=True)</p>
			<p class="source-code">count                     93</p>
			<p class="source-code">mean     2018-10-16 00:00:00</p>
			<p class="source-code">min      2018-10-01 00:00:00</p>
			<p class="source-code">25%      2018-10-08 00:00:00</p>
			<p class="source-code">50%      2018-10-16 00:00:00</p>
			<p class="source-code">75%      2018-10-24 00:00:00</p>
			<p class="source-code">max      2018-10-31 00:00:00</p>
			<p class="source-code">Name: date, dtype: object</p>
			<p>Dealing with <a id="_idIndexMarker381"/>dates can be tricky since they come in many different formats and time zones; fortunately, <strong class="source-inline">pandas</strong> has more methods we can use for dealing with converting datetime objects. For example, when working with a <strong class="source-inline">DatetimeIndex</strong> object, if we need to keep track of time zones, we can use the <strong class="source-inline">tz_localize()</strong> method to associate our datetimes with a time zone:</p>
			<p class="source-code">&gt;&gt;&gt; pd.date_range(start='2018-10-25', periods=2, freq='D')\</p>
			<p class="source-code">...     <strong class="bold">.tz_localize('EST')</strong></p>
			<p class="source-code">DatetimeIndex(['2018-10-25 <strong class="bold">00:00:00-05:00</strong>', </p>
			<p class="source-code">               '2018-10-26 <strong class="bold">00:00:00-05:00</strong>'], </p>
			<p class="source-code">              dtype='datetime64[ns, <strong class="bold">EST</strong>]', freq=None)</p>
			<p>This also works with <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> objects that have an index of type <strong class="source-inline">DatetimeIndex</strong>. We can read in the CSV file again and, this time, specify that the <strong class="source-inline">date</strong> column will be our index and that we should parse any dates in the CSV file into datetimes:</p>
			<p class="source-code">&gt;&gt;&gt; eastern = pd.read_csv(</p>
			<p class="source-code">...     'data/nyc_temperatures.csv',</p>
			<p class="source-code">...     <strong class="bold">index_col='date',</strong> <strong class="bold">parse_dates=True</strong></p>
			<p class="source-code">... )<strong class="bold">.tz_localize('EST')</strong></p>
			<p class="source-code">&gt;&gt;&gt; eastern.head()</p>
			<p>We had to read the file in again for this example because we haven't learned how to change the <a id="_idIndexMarker382"/>index of our data yet (covered in the <em class="italic">Reordering, reindexing, and sorting data</em> section, later this chapter). Note that we have added the Eastern Standard Time offset (-05:00 from UTC) to the datetimes in the index:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/Figure_3.13_B16834.jpg" alt="Figure 3.13 – Time zone-aware dates in the index&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – Time zone-aware dates in the index</p>
			<p>We can use the <strong class="source-inline">tz_convert()</strong> method to change the time zone into a different one. Let's change our data into UTC:</p>
			<p class="source-code">&gt;&gt;&gt; eastern.tz_convert('UTC').head()</p>
			<p>Now, the offset is UTC (+00:00), but note that the time portion of the date is now 5 AM; this conversion took into account the -05:00 offset:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/Figure_3.14_B16834.jpg" alt="Figure 3.14 – Converting data into another time zone&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14 – Converting data into another time zone</p>
			<p>We can also truncate datetimes with the <strong class="source-inline">to_period()</strong> method, which comes in handy if we don't care about the full date. For example, if we wanted to aggregate our data by month, we could truncate our index to just the month and year and then perform the aggregation. Since we will cover aggregation in <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>, we will just do the truncation here. Note that we first remove the time zone information to avoid a warning from <strong class="source-inline">pandas</strong> that the <strong class="source-inline">PeriodArray</strong> class doesn't have time <a id="_idIndexMarker383"/>zone information, and therefore it will be lost. This is because the underlying data for a <strong class="source-inline">PeriodIndex</strong> object is stored as a <strong class="source-inline">PeriodArray</strong> object:</p>
			<p class="source-code">&gt;&gt;&gt; eastern.tz_localize(None).<strong class="bold">to_period('M')</strong>.index</p>
			<p class="source-code">PeriodIndex(['2018-10', '2018-10', ..., '2018-10', '2018-10'],</p>
			<p class="source-code">            dtype='period[M]', name='date', freq='M')</p>
			<p>We can use the <strong class="source-inline">to_timestamp()</strong> method to convert our <strong class="source-inline">PeriodIndex</strong> object into a <strong class="source-inline">DatetimeIndex</strong> object; however, the datetimes all start at the first of the month now:</p>
			<p class="source-code">&gt;&gt;&gt; eastern.tz_localize(None)\</p>
			<p class="source-code">...     .to_period('M').<strong class="bold">to_timestamp()</strong>.index</p>
			<p class="source-code"><strong class="bold">DatetimeIndex</strong>(['2018-10-01', '2018-10-01', '2018-10-01', ...,</p>
			<p class="source-code">               '2018-10-01', '2018-10-01', '2018-10-01'],</p>
			<p class="source-code">              dtype='datetime64[ns]', name='date', freq=None)</p>
			<p>Alternatively, we can use the <strong class="source-inline">assign()</strong> method to handle any type conversions by passing the column names as named parameters and their new values as the value for that argument to the method call. In practice, this will be more beneficial since we can perform many tasks in one call and use the columns we create in that call to calculate additional columns. For example, let's cast the <strong class="source-inline">date</strong> column to a datetime and add a new column for the temperature in Fahrenheit (<strong class="source-inline">temp_F</strong>). The <strong class="source-inline">assign()</strong> method returns a new <strong class="source-inline">DataFrame</strong> object, so we must remember to assign it to a variable if we want to keep it. Here, we will <a id="_idIndexMarker384"/>create a new one. Note that our original conversion of the dates modified the column, so, in order to illustrate that we can use <strong class="source-inline">assign()</strong>, we need to read our data in once more:</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.read_csv('data/nyc_temperatures.csv').rename(</p>
			<p class="source-code">...     columns={'value': 'temp_C', 'attributes': 'flags'}</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; new_df = df.<strong class="bold">assign</strong>(</p>
			<p class="source-code">...     date=pd.to_datetime(df.date),</p>
			<p class="source-code">...     temp_F=(df.temp_C * 9/5) + 32</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; new_df.dtypes</p>
			<p class="source-code"><strong class="bold">date        datetime64[ns]</strong> </p>
			<p class="source-code">datatype            object</p>
			<p class="source-code">station             object</p>
			<p class="source-code">flags               object </p>
			<p class="source-code">temp_C             float64</p>
			<p class="source-code"><strong class="bold">temp_F             float64</strong></p>
			<p class="source-code">dtype: object</p>
			<p class="source-code">&gt;&gt;&gt; new_df.head()</p>
			<p>We now have datetimes in the <strong class="source-inline">date</strong> column and a new column, <strong class="source-inline">temp_F</strong>:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/Figure_3.15_B16834.jpg" alt="Figure 3.15 – Simultaneous type conversion and column creation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15 – Simultaneous type conversion and column creation</p>
			<p>Additionally, we can use the <strong class="source-inline">astype()</strong> method to convert one column at a time. As an example, let's say we only cared about the temperatures at every whole number, but we don't want <a id="_idIndexMarker385"/>to round. In this case, we simply want to chop <a id="_idIndexMarker386"/>off the information after the decimal. To accomplish this, we can cast the floats as integers. This time, we will use <strong class="bold">lambda functions</strong> (first introduced in <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>), which make it possible to access the <strong class="source-inline">temp_F</strong> column to create the <strong class="source-inline">temp_F_whole</strong> column, even though <strong class="source-inline">df</strong> doesn't have this column before we call <strong class="source-inline">assign()</strong>. It is very common (and useful) to use lambda functions with <strong class="source-inline">assign()</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; df = df.assign(</p>
			<p class="source-code">...     date=lambda x: pd.to_datetime(x.date),</p>
			<p class="source-code">...     temp_C_whole=<strong class="bold">lambda x: x.temp_C.astype('int')</strong>,</p>
			<p class="source-code">...     temp_F=<strong class="bold">lambda x: (x.temp_C * 9/5) + 32</strong>,</p>
			<p class="source-code">...     temp_F_whole=<strong class="bold">lambda x: x.temp_F.astype('int')</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; df.head()</p>
			<p>Note that we can refer to columns we've just created if we use a lambda function. It's also important to mention that we don't have to know whether to convert the column into a float or an integer: we can use <strong class="source-inline">pd.to_numeric()</strong>, which will convert the data into floats if it sees decimals. If all the numbers are whole, they will be converted into integers (obviously, we will still get errors if the data isn't numeric at all):</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/Figure_3.16_B16834.jpg" alt="Figure 3.16 – Creating columns with lambda functions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.16 – Creating columns with lambda functions</p>
			<p>Lastly, we have two columns with data currently being stored as strings that can be represented in a better way for this dataset. The <strong class="source-inline">station</strong> and <strong class="source-inline">datatype</strong> columns only have one and <a id="_idIndexMarker387"/>three distinct values, respectively, meaning that we aren't being efficient with our memory use since we are storing them as strings. We could potentially have issues with analyses further down the line. Pandas has the ability <a id="_idIndexMarker388"/>to define columns as <strong class="bold">categorical</strong>; certain statistical operations both within <strong class="source-inline">pandas</strong> and other packages will be able to handle this data, provide meaningful statistics on them, and use them properly. Categorical variables can take on one of a few values; for example, blood type would be a categorical variable—people can only have one of A, B, AB, or O.</p>
			<p>Going back to the temperature data, we only have one value for the <strong class="source-inline">station</strong> column and only three distinct values for the <strong class="source-inline">datatype</strong> column (<strong class="source-inline">TAVG</strong>, <strong class="source-inline">TMAX</strong>, <strong class="source-inline">TMIN</strong>). We can use the <strong class="source-inline">astype()</strong> method to cast these into categories and look at the summary statistics:</p>
			<p class="source-code">&gt;&gt;&gt; df_with_categories = df.assign(</p>
			<p class="source-code">...     station=df.station.<strong class="bold">astype('category')</strong>,</p>
			<p class="source-code">...     datatype=df.datatype.<strong class="bold">astype('category')</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; df_with_categories.dtypes </p>
			<p class="source-code">date            datetime64[ns]</p>
			<p class="source-code"><strong class="bold">datatype              category</strong></p>
			<p class="source-code"><strong class="bold">station               category</strong></p>
			<p class="source-code">flags                   object</p>
			<p class="source-code">temp_C                 float64</p>
			<p class="source-code">temp_C_whole             int64</p>
			<p class="source-code">temp_F                 float64</p>
			<p class="source-code">temp_F_whole             int64</p>
			<p class="source-code">dtype: object</p>
			<p class="source-code">&gt;&gt;&gt; df_with_categories.describe(<strong class="bold">include='category'</strong>)</p>
			<p>The summary <a id="_idIndexMarker389"/>statistics for categories are just like those for strings. We can see the number of non-null entries (<strong class="bold">count</strong>), the number of unique values (<strong class="bold">unique</strong>), the mode (<strong class="bold">top</strong>), and the number of occurrences of the mode (<strong class="bold">freq</strong>):</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/Figure_3.17_B16834.jpg" alt="Figure 3.17 – Summary statistics for the categorical columns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.17 – Summary statistics for the categorical columns</p>
			<p>The categories we just made don't have any order to them, but <strong class="source-inline">pandas</strong> does support this:</p>
			<p class="source-code">&gt;&gt;&gt; pd.Categorical(</p>
			<p class="source-code">...     ['med', 'med', 'low', 'high'], </p>
			<p class="source-code">...     <strong class="bold">categories=['low', 'med', 'high'],</strong> </p>
			<p class="source-code">...     <strong class="bold">ordered=True</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">['med', 'med', 'low', 'high'] </p>
			<p class="source-code">Categories (3, object): <strong class="bold">['low' &lt; 'med' &lt; 'high']</strong></p>
			<p>When the columns in our dataframe are stored in the appropriate type, it opens up additional avenues for exploration, such as calculating statistics, aggregating the data, and sorting the values. For example, depending on our data source, it's possible that the numeric <a id="_idIndexMarker390"/>data is represented as a string, in which case attempting to sort on the values will reorder the contents lexically, meaning the result could be 1, 10, 11, 2, rather than 1, 2, 10, 11 (numerical sort). Similarly, if we have dates represented as strings in a format other than YYYY-MM-DD, sorting on this information may result in non-chronological order; however, by converting the date strings with <strong class="source-inline">pd.to_datetime()</strong>, we can chronologically sort dates that are provided in any format. Type conversion makes it possible to reorder both the numeric data and the dates according to their values, rather than their initial string representations.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor071"/>Reordering, reindexing, and sorting data</h2>
			<p>We will often find the need to sort our data by the values of one or many columns. Say we wanted <a id="_idIndexMarker391"/>to find the days that reached the highest <a id="_idIndexMarker392"/>temperatures in New York City during October 2018; we <a id="_idIndexMarker393"/>could sort our values by the <strong class="source-inline">temp_C</strong> (or <strong class="source-inline">temp_F</strong>) column in descending order and use <strong class="source-inline">head()</strong> to select the number of days we wanted to see. To accomplish this, we can use the <strong class="source-inline">sort_values()</strong> method. Let's look at the top 10 days:</p>
			<p class="source-code">&gt;&gt;&gt; df[df.datatype == 'TMAX']\</p>
			<p class="source-code">...     .<strong class="bold">sort_values(by='temp_C', ascending=False)</strong>.head(10)</p>
			<p>This shows us that on October 7th and October 10th the temperature reached its highest value during the month of October 2018, according to the LaGuardia station. We also have ties between October 2nd and 4th, October 1st and 9th, and October 5th and 8th, but notice that the dates aren't always sorted—the 10th is after the 7th, but the 4th comes before the 2nd:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/Figure_3.18_B16834.jpg" alt="Figure 3.18 – Sorting the data to find the warmest days&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.18 – Sorting the data to find the warmest days</p>
			<p>The <strong class="source-inline">sort_values()</strong> method can be used with a list of column names to break ties. The order in <a id="_idIndexMarker394"/>which the columns are provided will determine <a id="_idIndexMarker395"/>the sort order, with each subsequent column <a id="_idIndexMarker396"/>being used to break ties. As an example, let's make sure the dates are sorted in ascending order when breaking ties:</p>
			<p class="source-code">&gt;&gt;&gt; df[df.datatype == 'TMAX'].<strong class="bold">sort_values(</strong></p>
			<p class="source-code">...     <strong class="bold">by=['temp_C', 'date'], ascending=[False, True] </strong></p>
			<p class="source-code">... <strong class="bold">)</strong>.head(10)</p>
			<p>Since we are sorting in ascending order, in the case of a tie, the date that comes earlier in the year will be above the later one. Notice how October 2nd is now above October 4th, despite both having the same temperature reading:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/Figure_3.19_B16834.jpg" alt="Figure 3.19 – Sorting the data with multiple columns to break ties&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.19 – Sorting the data with multiple columns to break ties</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">In <strong class="source-inline">pandas</strong>, the index is tied to the rows—when we drop rows, filter, or do anything that returns only some of the rows, our index may look out of order (as we saw in the previous examples). At the moment, the index just represents the row number in our data, so we may be interested in changing the values so that we have the first entry at index <strong class="source-inline">0</strong>. To have <strong class="source-inline">pandas</strong> do so automatically, we can pass <strong class="source-inline">ignore_index=True</strong> to <strong class="source-inline">sort_values()</strong>.</p>
			<p>Pandas also <a id="_idIndexMarker397"/>provides an additional way to look at a subset <a id="_idIndexMarker398"/>of the sorted values; we can use <strong class="source-inline">nlargest()</strong> to grab <a id="_idIndexMarker399"/>the <strong class="source-inline">n</strong> rows with the largest values according to specific criteria and <strong class="source-inline">nsmallest()</strong> to grab the <strong class="source-inline">n</strong> smallest rows, without the need to sort the data beforehand. Both accept a list of column names or a string for a single column. Let's grab the top 10 days by average temperature this time:</p>
			<p class="source-code">&gt;&gt;&gt; df[df.datatype == 'TAVG'].<strong class="bold">nlargest(n=10, columns='temp_C')</strong></p>
			<p>We get the warmest days (on average) in October:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/Figure_3.20_B16834.jpg" alt="Figure 3.20 – Sorting to find the 10 warmest days on average&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.20 – Sorting to find the 10 warmest days on average</p>
			<p>We aren't <a id="_idIndexMarker400"/>limited to sorting values; if we wish, we can even <a id="_idIndexMarker401"/>order the columns alphabetically and sort the rows <a id="_idIndexMarker402"/>by their index values. For these tasks, we can use the <strong class="source-inline">sort_index()</strong> method. By default, <strong class="source-inline">sort_index()</strong> will target the rows so that we can do things such as order the index after performing an operation that shuffles it. For instance, the <strong class="source-inline">sample()</strong> method will give us randomly selected rows, which will lead to a jumbled index, so we can use <strong class="source-inline">sort_index()</strong> to order them afterward:</p>
			<p class="source-code">&gt;&gt;&gt; df.sample(5, random_state=0).index</p>
			<p class="source-code">Int64Index([2, <strong class="bold">30, 55, 16, 13</strong>], dtype='int64')</p>
			<p class="source-code">&gt;&gt;&gt; df.sample(5, random_state=0).sort_index().index</p>
			<p class="source-code">Int64Index([2, <strong class="bold">13, 16, 30, 55</strong>], dtype='int64')</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If we need the result of <strong class="source-inline">sample()</strong> to be reproducible, we can pass in a <strong class="bold">seed</strong>, set to a number of our choosing (using the <strong class="source-inline">random_state</strong> argument). The seed initializes a pseudorandom number generator, so, provided that the same seed is used, the results will be the same.</p>
			<p>When we want to target columns, we must pass in <strong class="source-inline">axis=1</strong>; rows will be the default (<strong class="source-inline">axis=0</strong>). Note that this argument is present in many <strong class="source-inline">pandas</strong> methods and functions (including <strong class="source-inline">sample()</strong>), so it's important to understand what it means. Let's use this knowledge to sort the columns of our dataframe alphabetically:</p>
			<p class="source-code">&gt;&gt;&gt; df.sort_index(<strong class="bold">axis=1</strong>).head()</p>
			<p>Having our <a id="_idIndexMarker403"/>columns in alphabetical order can come in <a id="_idIndexMarker404"/>handy when using <strong class="source-inline">loc[]</strong> because we can specify a <a id="_idIndexMarker405"/>range of columns with similar names; for example, we could now use <strong class="source-inline">df.loc[:,'station':'temp_F_whole']</strong> to easily grab all of our temperature columns, along with the station information:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/Figure_3.21_B16834.jpg" alt="Figure 3.21 – Sorting the columns by name&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.21 – Sorting the columns by name</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Both <strong class="source-inline">sort_index()</strong> and <strong class="source-inline">sort_values()</strong> return new <strong class="source-inline">DataFrame</strong> objects. We must pass in <strong class="source-inline">inplace=True</strong> to update the dataframe we are working with.</p>
			<p>The <strong class="source-inline">sort_index()</strong> method can also help us get an accurate answer when we're testing two dataframes for equality. Pandas will check that, in addition to having the same data, both have the same values for the index when it compares the rows. If we sort our dataframe by temperature in Celsius and check whether it is equal to the original dataframe, <strong class="source-inline">pandas</strong> tells us that it isn't. We must sort the index to see that they are the same:</p>
			<p class="source-code">&gt;&gt;&gt; df.equals(df.sort_values(by='temp_C'))</p>
			<p class="source-code">False</p>
			<p class="source-code">&gt;&gt;&gt; df.equals(df.sort_values(by='temp_C').sort_index())</p>
			<p class="source-code">True</p>
			<p>Sometimes, we don't care too much about the numeric index, but we would like to use one (or more) of the other columns as the index instead. In this case, we can use the <strong class="source-inline">set_index()</strong> method. Let's set the <strong class="source-inline">date</strong> column as our index:</p>
			<p class="source-code">&gt;&gt;&gt; df.<strong class="bold">set_index('date', inplace=True)</strong></p>
			<p class="source-code">&gt;&gt;&gt; df.head()</p>
			<p>Notice that <a id="_idIndexMarker406"/>the <strong class="source-inline">date</strong> column has moved to the far left where <a id="_idIndexMarker407"/>the index goes, and we no longer have the numeric index: </p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/Figure_3.22_B16834.jpg" alt="Figure 3.22 – Setting the date column as the index&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.22 – Setting the date column as the index</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can also provide a list of columns to use as the index. This will create a <strong class="source-inline">MultiIndex</strong> object, where the first element in the list is the outermost level and the last is the innermost. We will discuss this further in the <em class="italic">Pivoting DataFrames</em> section.</p>
			<p>Setting the <a id="_idIndexMarker408"/>index to a datetime lets us take advantage of datetime slicing and indexing, which we briefly discussed in <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>. As long as we provide a date format that <strong class="source-inline">pandas</strong> understands, we can grab the data. To select all of 2018, we can use <strong class="source-inline">df.loc['2018']</strong>; for the fourth quarter of 2018, we can use <strong class="source-inline">df.loc['2018-Q4']</strong>; and for October, we can use <strong class="source-inline">df.loc['2018-10']</strong>. These can also be combined to build ranges. Note that <strong class="source-inline">loc[]</strong> is optional when using ranges:</p>
			<p class="source-code">&gt;&gt;&gt; df['2018-10-11':'2018-10-12']</p>
			<p>This gives us the data from October 11, 2018 through October 12, 2018 (inclusive of both endpoints):</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/Figure_3.23_B16834.jpg" alt="Figure 3.23 – Selecting date ranges&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.23 – Selecting date ranges</p>
			<p>We can <a id="_idIndexMarker409"/>use the <strong class="source-inline">reset_index()</strong> method to restore the <strong class="source-inline">date</strong> column:</p>
			<p class="source-code">&gt;&gt;&gt; df['2018-10-11':'2018-10-12'].<strong class="bold">reset_index()</strong></p>
			<p>Our index <a id="_idIndexMarker410"/>now starts at <strong class="source-inline">0</strong>, and the dates are now in a column <a id="_idIndexMarker411"/>called <strong class="source-inline">date</strong>. This is especially useful if we have data that we don't want to lose in the index, such as the date, but need to perform an operation as if it weren't in the index:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/Figure_3.24_B16834.jpg" alt="Figure 3.24 – Resetting the index&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.24 – Resetting the index</p>
			<p>In some cases, we may have an index we want to continue to use, but we need to align it to certain values. For this purpose, we have the <strong class="source-inline">reindex()</strong> method. We provide it with an index to align our data to, and it adjusts the index accordingly. Note that this new index isn't necessarily part of the data—we simply have an index and want to match the current data up to it. </p>
			<p>As an example, we will turn to the S&amp;P 500 stock data in the <strong class="source-inline">sp500.csv</strong> file. It contains the <strong class="bold">opening</strong>, <strong class="bold">high</strong>, <strong class="bold">low</strong>, and <strong class="bold"><a id="_idIndexMarker412"/></strong><strong class="bold">closing</strong> (also called <strong class="bold">OHLC</strong>) prices daily for the S&amp;P 500 from 2017 through the end of 2018, along with volume traded and the adjusted close (which we won't use). Let's read it in, setting the <strong class="source-inline">date</strong> column as the index and parsing the dates:</p>
			<p class="source-code">&gt;&gt;&gt; sp = pd.read_csv(</p>
			<p class="source-code">...     'data/sp500.csv', index_col='date', parse_dates=True</p>
			<p class="source-code">... ).drop(columns=['adj_close']) # not using this column</p>
			<p>Let's see <a id="_idIndexMarker413"/>what our data looks like and mark the day of <a id="_idIndexMarker414"/>the week for each row in order to understand <a id="_idIndexMarker415"/>what the index contains. We can easily isolate the date part from an index of type <strong class="source-inline">DatetimeIndex</strong>. When isolating date parts, <strong class="source-inline">pandas</strong> will give us the numeric representation of what we are looking for; if we are looking for the string version, we should look to see whether there is already a method before writing our own conversion function. In this case, it's <strong class="source-inline">day_name()</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; sp.head(10)\</p>
			<p class="source-code">...     .assign(<strong class="bold">day_of_week=lambda x: x.index.day_name()</strong>)</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can also do this with a series, but first, we need to access the <strong class="source-inline">dt</strong> attribute. For example, if we had a <strong class="source-inline">date</strong> column in the <strong class="source-inline">sp</strong> dataframe, we could grab the month with <strong class="source-inline">sp.date.dt.month</strong>. You can find the full list of what can be accessed at <a href="https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties">https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties</a>. </p>
			<p>Since the stock market is closed on the weekend (and holidays), we only have data for weekdays:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/Figure_3.25_B16834.jpg" alt="Figure 3.25 – S&amp;P 500 OHLC data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.25 – S&amp;P 500 OHLC data</p>
			<p>If we were analyzing the performance of a group of assets in a portfolio that included the S&amp;P 500 and something that trades on the weekend, such as bitcoin, we would need to have <a id="_idIndexMarker416"/>values for every day of the year for the S&amp;P 500. Otherwise, when looking at the daily value of our portfolio we <a id="_idIndexMarker417"/>would see huge drops every day the market <a id="_idIndexMarker418"/>was closed. To illustrate this, let's read in the bitcoin data from the <strong class="source-inline">bitcoin.csv</strong> file and combine the S&amp;P 500 and bitcoin data into a portfolio. The bitcoin data also contains OHLC data and volume traded, but it comes with a column called <strong class="source-inline">market_cap</strong> that we don't need, so we have to drop that first:</p>
			<p class="source-code">&gt;&gt;&gt; bitcoin = pd.read_csv(</p>
			<p class="source-code">...     'data/bitcoin.csv', index_col='date', parse_dates=True</p>
			<p class="source-code">... ).drop(columns=['market_cap'])</p>
			<p>To analyze the portfolio, we will need to aggregate the data by day; this is a topic for <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>, so, for now, don't worry too much about how this aggregation is being performed—just know that we are summing up the data by day. For example, each day's closing price will be the sum of the closing price of the S&amp;P 500 <a id="_idIndexMarker419"/>and the closing price of bitcoin:</p>
			<p class="source-code"># every day's closing price = S&amp;P 500 close + Bitcoin close</p>
			<p class="source-code"># (same for other metrics)</p>
			<p class="source-code">&gt;&gt;&gt; portfolio = pd.concat([sp, bitcoin], sort=False)\</p>
			<p class="source-code">...     .groupby(level='date').sum()</p>
			<p class="source-code">&gt;&gt;&gt; portfolio.head(10).assign(</p>
			<p class="source-code">...     day_of_week=lambda x: x.index.day_name()</p>
			<p class="source-code">... )</p>
			<p>Now, if we <a id="_idIndexMarker420"/>examine our portfolio, we will see that we have <a id="_idIndexMarker421"/>values for every day of the week; so far, so good:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/Figure_3.26_B16834.jpg" alt=" Figure 3.26 – Portfolio of the S&amp;P 500 and bitcoin&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 3.26 – Portfolio of the S&amp;P 500 and bitcoin</p>
			<p>However, there is a problem with this approach, which is much easier to see with a visualization. Plotting <a id="_idIndexMarker422"/>will be covered in depth in <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>, and <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>, so <a id="_idIndexMarker423"/>don't worry about the details for now:</p>
			<p class="source-code">&gt;&gt;&gt; import matplotlib.pyplot as plt # module for plotting</p>
			<p class="source-code">&gt;&gt;&gt; from matplotlib.ticker import StrMethodFormatter </p>
			<p class="source-code"># plot the closing price from Q4 2017 through Q2 2018</p>
			<p class="source-code">&gt;&gt;&gt; ax = portfolio['2017-Q4':'2018-Q2'].plot(</p>
			<p class="source-code">...     y='close', figsize=(15, 5), legend=False,</p>
			<p class="source-code">...     title='Bitcoin + S&amp;P 500 value without accounting '</p>
			<p class="source-code">...           'for different indices'</p>
			<p class="source-code">... )</p>
			<p class="source-code"># formatting</p>
			<p class="source-code">&gt;&gt;&gt; ax.set_ylabel('price')</p>
			<p class="source-code">&gt;&gt;&gt; ax.yaxis\</p>
			<p class="source-code">...     .set_major_formatter(StrMethodFormatter('${x:,.0f}'))</p>
			<p class="source-code">&gt;&gt;&gt; for spine in ['top', 'right']:</p>
			<p class="source-code">...     ax.spines[spine].set_visible(False)</p>
			<p class="source-code"># show the plot</p>
			<p class="source-code">&gt;&gt;&gt; plt.show()</p>
			<p>Notice <a id="_idIndexMarker424"/>how there is a cyclical pattern here? It is dropping every day the market is closed because the aggregation only had bitcoin data to sum for those days: </p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/Figure_3.27_B16834.jpg" alt="Figure 3.27 – Portfolio closing price without accounting for stock market closures&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.27 – Portfolio closing price without accounting for stock market closures</p>
			<p>Clearly, this <a id="_idIndexMarker425"/>is a problem; an asset's value doesn't drop to <a id="_idIndexMarker426"/>zero whenever the market is closed. If we want <strong class="source-inline">pandas</strong> to fill <a id="_idIndexMarker427"/>the missing data in for us, we will need to reindex the S&amp;P 500 data with bitcoin's index using the <strong class="source-inline">reindex()</strong> method, passing one of the following strategies to the <strong class="source-inline">method</strong> parameter:</p>
			<ul>
				<li><strong class="source-inline">'ffill'</strong>: This <a id="_idIndexMarker428"/>method brings values forward. In the previous example, this fills the days the market was closed with the data for the last time the market was open before those days.</li>
				<li><strong class="source-inline">'bfill'</strong>: This <a id="_idIndexMarker429"/>method backpropagates the values, which will result in carrying future results to past dates, meaning that this isn't the right option here.</li>
				<li><strong class="source-inline">'nearest'</strong>: This <a id="_idIndexMarker430"/>method fills according to the rows closest to the missing ones, which in this case will result in Sundays getting the data for the following Mondays, and Saturdays getting the data from the previous Fridays.</li>
			</ul>
			<p>Forward-filling seems to be the best option, but since we aren't sure, we will see how this works on a few rows of the data first:</p>
			<p class="source-code">&gt;&gt;&gt; sp.reindex(bitcoin.index, <strong class="bold">method='ffill'</strong>).head(10)\</p>
			<p class="source-code">...     .assign(day_of_week=lambda x: x.index.day_name())</p>
			<p>Notice any issues with this? Well, the volume traded (<strong class="source-inline">volume</strong>) column makes it seem like the days we used forward-filling for are actually days when the market is open:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/Figure_3.28_B16834.jpg" alt="Figure 3.28 – Forward-filling dates with missing values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.28 – Forward-filling dates with missing values</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <strong class="source-inline">compare()</strong> method will <a id="_idIndexMarker431"/>show us the values that differ across identically-labeled dataframes (same index and columns); we can use it to isolate the changes in our data when forward-filling here. There is an example in the notebook.</p>
			<p>Ideally, we <a id="_idIndexMarker432"/>only want to maintain the value of the stock when <a id="_idIndexMarker433"/>the stock market is closed—the volume <a id="_idIndexMarker434"/>traded should be zero. In order to handle the <strong class="source-inline">NaN</strong> values in a different manner for each column, we will turn to the <strong class="source-inline">assign()</strong> method. To fill any <strong class="source-inline">NaN</strong> values in the <strong class="source-inline">volume</strong> column with <strong class="source-inline">0</strong>, we will use the <strong class="source-inline">fillna()</strong> method, which we will see more of in the <em class="italic">Handling duplicate, missing, or invalid data</em> section, later in this chapter. The <strong class="source-inline">fillna()</strong> method also allows us to pass in a method instead of a value, so we can forward-fill the <strong class="source-inline">close</strong> column, which was the only column that made sense from our previous attempt. Lastly, we can use the <strong class="source-inline">np.where()</strong> function for the remaining columns, which allows us to build a vectorized <strong class="source-inline">if...else</strong>. It takes the following form:</p>
			<p class="source-code">np.where(boolean condition, value if True, value if False)</p>
			<p><strong class="bold">Vectorized operations</strong> are performed on all the elements in the array at once; since each element <a id="_idIndexMarker435"/>has the same data type, these calculations can be run <a id="_idIndexMarker436"/>rather quickly. As a general rule of thumb, with <strong class="source-inline">pandas</strong>, we should avoid writing loops in favor of vectorized operations for better <a id="_idIndexMarker437"/>performance. NumPy functions are designed to <a id="_idIndexMarker438"/>work on arrays, so they are perfect candidates for high-performance <strong class="source-inline">pandas</strong> code. This will make it easy for us to set any <strong class="source-inline">NaN</strong> values in the <strong class="source-inline">open</strong>, <strong class="source-inline">high</strong>, or <strong class="source-inline">low</strong> columns to the value in the <strong class="source-inline">close</strong> column for the same day. Since these come after the <strong class="source-inline">close</strong> column gets worked on, we will have the forward-filled value for <strong class="source-inline">close</strong> to use for the other columns where necessary:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; sp_reindexed = sp.reindex(bitcoin.index).assign(</p>
			<p class="source-code">...     # volume is 0 when the market is closed</p>
			<p class="source-code">...     <strong class="bold">volume=lambda x: x.volume.fillna(0)</strong>,</p>
			<p class="source-code">...     # carry this forward</p>
			<p class="source-code">...     <strong class="bold">close=lambda x: x.close.fillna(method='ffill')</strong>,</p>
			<p class="source-code">...     # take the closing price if these aren't available</p>
			<p class="source-code">...     <strong class="bold">open=lambda x: \</strong></p>
			<p class="source-code">...         <strong class="bold">np.where(x.open.isnull(), x.close, x.open)</strong>,</p>
			<p class="source-code">...     <strong class="bold">high=lambda x: \</strong></p>
			<p class="source-code">...         <strong class="bold">np.where(x.high.isnull(), x.close, x.high)</strong>,</p>
			<p class="source-code">...     <strong class="bold">low=lambda x: np.where(x.low.isnull(), x.close, x.low)</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; sp_reindexed.head(10).assign(</p>
			<p class="source-code">...     day_of_week=lambda x: x.index.day_name()</p>
			<p class="source-code">... )</p>
			<p>On <a id="_idIndexMarker439"/>Saturday, January 7th and Sunday, January 8th, we now have <a id="_idIndexMarker440"/>volume traded at zero. The OHLC <a id="_idIndexMarker441"/>prices are all equal to the closing price on Friday, the 6th:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/Figure_3.29_B16834.jpg" alt="Figure 3.29 – Reindexing the S&amp;P 500 data with specific strategies per column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.29 – Reindexing the S&amp;P 500 data with specific strategies per column</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Here, we use <strong class="source-inline">np.where()</strong> to <a id="_idIndexMarker442"/>both introduce a function we will see throughout this book and to make it easier to understand what is going on, but note that <strong class="source-inline">np.where(x.open.isnull(), x.close, x.open)</strong> can be replaced with the <strong class="source-inline">combine_first()</strong> method, which (for this use case) is equivalent to <strong class="source-inline">x.open.combine_first(x.close)</strong>. We will use the <strong class="source-inline">combine_first()</strong> method in the <em class="italic">Handling duplicate, missing, or invalid data</em> section, later this chapter.</p>
			<p>Now, let's recreate the portfolio with the reindexed S&amp;P 500 data and use a visualization <a id="_idIndexMarker443"/>to compare it with the previous attempt (again, don't <a id="_idIndexMarker444"/>worry about the plotting code, which will be <a id="_idIndexMarker445"/>covered in <a href="B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>, <em class="italic">Visualizing Data with Pandas and Matplotlib</em>, and <a href="B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 6</em></a>, <em class="italic">Plotting with Seaborn and Customization Techniques</em>):</p>
			<p class="source-code"># every day's closing price = S&amp;P 500 close adjusted for</p>
			<p class="source-code"># market closure + Bitcoin close (same for other metrics)</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">fixed_portfolio = sp_reindexed + bitcoin</strong></p>
			<p class="source-code"># plot the reindexed portfolio's close (Q4 2017 - Q2 2018)</p>
			<p class="source-code">&gt;&gt;&gt; ax = fixed_portfolio['2017-Q4':'2018-Q2'].plot(</p>
			<p class="source-code">...     y='close', figsize=(15, 5), linewidth=2, </p>
			<p class="source-code">...     label='reindexed portfolio of S&amp;P 500 + Bitcoin', </p>
			<p class="source-code">...     title='Reindexed portfolio vs.' </p>
			<p class="source-code">...           'portfolio with mismatched indices'</p>
			<p class="source-code">... )</p>
			<p class="source-code"># add line for original portfolio for comparison</p>
			<p class="source-code">&gt;&gt;&gt; portfolio['2017-Q4':'2018-Q2'].plot(</p>
			<p class="source-code">...     y='close', ax=ax, linestyle='--',</p>
			<p class="source-code">...     label='portfolio of S&amp;P 500 + Bitcoin w/o reindexing' </p>
			<p class="source-code">... ) </p>
			<p class="source-code"># formatting</p>
			<p class="source-code">&gt;&gt;&gt; ax.set_ylabel('price')</p>
			<p class="source-code">&gt;&gt;&gt; ax.yaxis\</p>
			<p class="source-code">...     .set_major_formatter(StrMethodFormatter('${x:,.0f}'))</p>
			<p class="source-code">&gt;&gt;&gt; for spine in ['top', 'right']:</p>
			<p class="source-code">...     ax.spines[spine].set_visible(False)</p>
			<p class="source-code"># show the plot</p>
			<p class="source-code">&gt;&gt;&gt; plt.show() </p>
			<p>The orange <a id="_idIndexMarker446"/>dashed line is our original attempt at studying <a id="_idIndexMarker447"/>the portfolio (without reindexing), while the blue <a id="_idIndexMarker448"/>solid line is the portfolio we just built with reindexing and different filling strategies per column. Keep this strategy in mind for the exercises in <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>: </p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/Figure_3.30_B16834.jpg" alt="Figure 3.30 – Visualizing the effect of reindexing&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.30 – Visualizing the effect of reindexing</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can <a id="_idIndexMarker449"/>also use the <strong class="source-inline">reindex()</strong> method to reorder the rows. For example, if our data is stored in <strong class="source-inline">x</strong>, then <strong class="source-inline">x.reindex([32, 20, 11])</strong> will return a new <strong class="source-inline">DataFrame</strong> object of three rows: 32, 20, and 11 (in that order). This can be done along the columns with <strong class="source-inline">axis=1</strong> (the default is <strong class="source-inline">axis=0</strong> for rows).</p>
			<p>Now, let's turn our attention to reshaping data. Recall that we had to first filter the temperature data by the <strong class="source-inline">datatype</strong> column and then sort to find the warmest days; reshaping the data will make this easier, and also make it possible for us to aggregate and summarize the data. </p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Reshaping data</h1>
			<p>Data isn't always given to us in the format that's most convenient for our analysis. Therefore, we need <a id="_idIndexMarker450"/>to be able to restructure data into both wide and long formats, depending on the analysis we want to perform. For many analyses, we will want wide format data so that we can look at the summary statistics easily and share our results in that format.</p>
			<p>However, this isn't always as black and white as going from long format to wide format or vice versa. Consider the following data from the <em class="italic">Exercises</em> section:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/Figure_3.31_B16834.jpg" alt="Figure 3.31 – Data with some long and some wide format columns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.31 – Data with some long and some wide format columns</p>
			<p>It's possible to have data where some of the columns are in wide format (<strong class="bold">open</strong>, <strong class="bold">high</strong>, <strong class="bold">low</strong>, <strong class="bold">close</strong>, <strong class="bold">volume</strong>), but others are in long format (<strong class="bold">ticker</strong>). Summary statistics using <strong class="source-inline">describe()</strong> on this data aren't helpful unless we first filter on <strong class="bold">ticker</strong>. This format makes it easy to compare the stocks; however, as we briefly discussed when we learned about wide and long formats, we wouldn't be able to easily plot the closing price for each stock using <strong class="source-inline">pandas</strong>—we would need <strong class="source-inline">seaborn</strong>. Alternatively, we could restructure the data for that visualization. </p>
			<p>Now that we understand the motivation for restructuring data, let's move to the next notebook, <strong class="source-inline">4-reshaping_data.ipynb</strong>. We will begin by importing <strong class="source-inline">pandas</strong> and reading in the <strong class="source-inline">long_data.csv</strong> file, adding the temperature in Fahrenheit column (<strong class="source-inline">temp_F</strong>), and performing some of the data cleaning we just learned about:</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; long_df = pd.read_csv(</p>
			<p class="source-code">...     'data/long_data.csv',</p>
			<p class="source-code">...     usecols=['date', 'datatype', 'value']</p>
			<p class="source-code">... ).rename(columns={'value': 'temp_C'}).assign(</p>
			<p class="source-code">...     date=lambda x: pd.to_datetime(x.date),</p>
			<p class="source-code">...     temp_F=lambda x: (x.temp_C * 9/5) + 32</p>
			<p class="source-code">... )</p>
			<p>Our long <a id="_idIndexMarker451"/>format data looks like this:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/Figure_3.32_B16834.jpg" alt="Figure 3.32 – Long format temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.32 – Long format temperature data</p>
			<p>In this section, we will discuss transposing, pivoting, and melting our data. Note that after reshaping the data, we will often revisit the data cleaning tasks as things may have changed, or we may need to change things we couldn't access easily before. For example, we will want to perform some type conversion if the values were all turned into strings in long format, but in wide format some columns are clearly numeric.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>Transposing DataFrames</h2>
			<p>While we will be pretty much only working with wide or long formats, <strong class="source-inline">pandas</strong> provides ways to <a id="_idIndexMarker452"/>restructure our data as we see fit, including <a id="_idIndexMarker453"/>taking the <strong class="bold">transpose</strong> (flipping the rows with the columns), which we may find useful to make better use of our display area when we're printing parts of our dataframe:</p>
			<p class="source-code">&gt;&gt;&gt; long_df.set_index('date').head(6).<strong class="bold">T</strong></p>
			<p>Notice that the index is now in the columns, and that the column names are in the index: </p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/Figure_3.33_B16834.jpg" alt="Figure 3.33 – Transposed temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.33 – Transposed temperature data</p>
			<p>It may not be immediately apparent how useful this can be, but we will see this a quite few times throughout this book; for example, to make content easier to display in <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>, and to build a particular visualization for machine learning in <a href="B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 9</em></a>, <em class="italic">Getting Started with Machine Learning in Python</em>.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor074"/>Pivoting DataFrames</h2>
			<p>We <strong class="bold">pivot</strong> our data to go from long format to wide format. The <strong class="source-inline">pivot()</strong> method performs this <a id="_idIndexMarker454"/>restructuring of our <strong class="source-inline">DataFrame</strong> object. To pivot, we need to tell <strong class="source-inline">pandas</strong> which column currently holds the values (with the <strong class="source-inline">values</strong> argument) and the column that contains what will become the column names in wide format (the <strong class="source-inline">columns</strong> argument). Optionally, we can provide a new index (the <strong class="source-inline">index</strong> argument). Let's pivot into wide format, where we have a column for each of the temperature measurements in Celsius and use the dates as the index:</p>
			<p class="source-code">&gt;&gt;&gt; pivoted_df = long_df.<strong class="bold">pivot(</strong></p>
			<p class="source-code">...     <strong class="bold">index='date', columns='datatype', values='temp_C'</strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p class="source-code">&gt;&gt;&gt; pivoted_df.head()</p>
			<p>In our starting dataframe, there was a <strong class="source-inline">datatype</strong> column that contained only <strong class="source-inline">TMAX</strong>, <strong class="source-inline">TMIN</strong>, or <strong class="source-inline">TOBS</strong> as strings. Now, these are column names because we passed in <strong class="source-inline">columns='datatype'</strong>. By passing in <strong class="source-inline">index='date'</strong>, the <strong class="source-inline">date</strong> column became our index, without <a id="_idIndexMarker455"/>needing to run <strong class="source-inline">set_index()</strong>. Finally, the values for each combination of <strong class="source-inline">date</strong> and <strong class="source-inline">datatype</strong> are the corresponding temperatures in Celsius since we passed in <strong class="source-inline">values='temp_C'</strong>:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/Figure_3.34_B16834.jpg" alt="Figure 3.34 – Pivoting the long format temperature data into wide format&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.34 – Pivoting the long format temperature data into wide format</p>
			<p>As we discussed at the beginning of this chapter, with the data in wide format, we can easily get meaningful summary statistics with the <strong class="source-inline">describe()</strong> method:</p>
			<p class="source-code">&gt;&gt;&gt; pivoted_df.describe()</p>
			<p>We can see that we have 31 observations for all three temperature measurements and that this month has a wide range of temperatures (highest daily maximum of 26.7°C and lowest daily minimum of -1.1°C):</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/Figure_3.35_B16834.jpg" alt="Figure 3.35 – Summary statistics on the pivoted temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.35 – Summary statistics on the pivoted temperature data</p>
			<p>We lost <a id="_idIndexMarker456"/>the temperature in Fahrenheit, though. If we want to keep it, we can provide multiple columns to <strong class="source-inline">values</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; pivoted_df = long_df.pivot(</p>
			<p class="source-code">...     index='date', columns='datatype',</p>
			<p class="source-code">...     <strong class="bold">values=['temp_C', 'temp_F']</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; pivoted_df.head()</p>
			<p>However, we <a id="_idIndexMarker457"/>now get an extra level above the column names. This is called a <strong class="bold">hierarchical index</strong>:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/Figure_3.36_B16834.jpg" alt="Figure 3.36 – Pivoting with multiple value columns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.36 – Pivoting with multiple value columns</p>
			<p>With this <a id="_idIndexMarker458"/>hierarchical index, if we want to select <strong class="source-inline">TMIN</strong> in Fahrenheit, we will first need to select <strong class="source-inline">temp_F</strong> and then <strong class="source-inline">TMIN</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">pivoted_df['temp_F']['TMIN']</strong>.head()</p>
			<p class="source-code">date</p>
			<p class="source-code">2018-10-01    48.02</p>
			<p class="source-code">2018-10-02    57.02</p>
			<p class="source-code">2018-10-03    60.08</p>
			<p class="source-code">2018-10-04    53.06</p>
			<p class="source-code">2018-10-05    53.06</p>
			<p class="source-code">Name: TMIN, dtype: float64</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In cases where we need to perform an aggregation as we pivot (due to duplicate values in the index), we can use the <strong class="source-inline">pivot_table()</strong> method, which we will discuss in <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>.</p>
			<p>We have been working with a single index throughout this chapter; however, we can create an index from any number of columns with <strong class="source-inline">set_index()</strong>. This gives us an index of type <strong class="source-inline">MultiIndex</strong>, where the outermost level corresponds to the first element in the list provided to <strong class="source-inline">set_index()</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; multi_index_df = long_df.set_index(['date', 'datatype'])</p>
			<p class="source-code">&gt;&gt;&gt; multi_index_df.head().index</p>
			<p class="source-code"><strong class="bold">MultiIndex</strong>([('2018-10-01', 'TMAX'),</p>
			<p class="source-code">            ('2018-10-01', 'TMIN'),</p>
			<p class="source-code">            ('2018-10-01', 'TOBS'),</p>
			<p class="source-code">            ('2018-10-02', 'TMAX'),</p>
			<p class="source-code">            ('2018-10-02', 'TMIN')],</p>
			<p class="source-code">           <strong class="bold">names=['date', 'datatype']</strong>)</p>
			<p class="source-code">&gt;&gt;&gt; multi_index_df.head()</p>
			<p>Notice that <a id="_idIndexMarker459"/>we now have two levels in the index—<strong class="source-inline">date</strong> is the outermost level and <strong class="source-inline">datatype</strong> is the innermost:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/Figure_3.37_B16834.jpg" alt="Figure 3.37 – Working with a multi-level index&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.37 – Working with a multi-level index</p>
			<p>The <strong class="source-inline">pivot()</strong> method expects the data to only have one column to set as the index; if we have a multi-level index, we should use the <strong class="source-inline">unstack()</strong> method instead. We can use <strong class="source-inline">unstack()</strong> on <strong class="source-inline">multi_index_df</strong> and get a similar result to what we had previously. Order matters here because, by default, <strong class="source-inline">unstack()</strong> will move the innermost level of the index to the columns; in this case, that means we will keep the <strong class="source-inline">date</strong> level in the index and move the <strong class="source-inline">datatype</strong> level to the column names. To unstack a different level, simply pass in the index of the level to unstack, where 0 is the leftmost and -1 is the rightmost, or the name of the level (if it has one). Here, we will use the default:</p>
			<p class="source-code">&gt;&gt;&gt; unstacked_df = <strong class="bold">multi_index_df.unstack()</strong></p>
			<p class="source-code">&gt;&gt;&gt; unstacked_df.head()</p>
			<p>With <strong class="source-inline">multi_index_df</strong>, we had <strong class="source-inline">datatype</strong> as the innermost level of the index, so, after using <strong class="source-inline">unstack()</strong>, it is along the columns. Note that we once again have a hierarchical index in the columns. In <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>, we will discuss a <a id="_idIndexMarker460"/>way to squash this back into a single level of columns:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/Figure_3.38_B16834.jpg" alt="Figure 3.38 – Unstacking a multi-level index to pivot data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.38 – Unstacking a multi-level index to pivot data</p>
			<p>The <strong class="source-inline">unstack()</strong> method has the added benefit of allowing us to specify how to fill in missing values that come into existence upon reshaping the data. To do so, we can use the <strong class="source-inline">fill_value</strong> parameter. Imagine a case where we have been given the data for <strong class="source-inline">TAVG</strong> for October 1, 2018 only. We could append this to <strong class="source-inline">long_df</strong> and set our index to the <strong class="source-inline">date</strong> and <strong class="source-inline">datatype</strong> columns, as we did previously:</p>
			<p class="source-code">&gt;&gt;&gt; extra_data = long_df.append([{</p>
			<p class="source-code">...     'datatype': 'TAVG', </p>
			<p class="source-code">...     'date': '2018-10-01', </p>
			<p class="source-code">...     'temp_C': 10, </p>
			<p class="source-code">...     'temp_F': 50</p>
			<p class="source-code">... }]).set_index(['date', 'datatype']).sort_index()</p>
			<p class="source-code">&gt;&gt;&gt; extra_data['2018-10-01':'2018-10-02']</p>
			<p>We now <a id="_idIndexMarker461"/>have four temperature measurements for October 1, 2018, but only three for the remaining days:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/Figure_3.39_B16834.jpg" alt="Figure 3.39 – Introducing an additional temperature measurement into the data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.39 – Introducing an additional temperature measurement into the data</p>
			<p>Using <strong class="source-inline">unstack()</strong>, as we did previously, will result in <strong class="source-inline">NaN</strong> values for most of the <strong class="source-inline">TAVG</strong> data:</p>
			<p class="source-code">&gt;&gt;&gt; extra_data.unstack().head()</p>
			<p>Take a look at the <strong class="source-inline">TAVG</strong> columns after we unstack:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/Figure_3.40_B16834.jpg" alt="Figure 3.40 – Unstacking can lead to null values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.40 – Unstacking can lead to null values</p>
			<p>To address this, we can pass in an appropriate <strong class="source-inline">fill_value</strong>. However, we are restricted to passing in <a id="_idIndexMarker462"/>a value for this, not a strategy (as we saw when we discussed reindexing), so while there is no good value for this case, we can use <strong class="source-inline">-40</strong> to illustrate how this works:</p>
			<p class="source-code">&gt;&gt;&gt; extra_data.unstack(<strong class="bold">fill_value=-40</strong>).head()</p>
			<p>The <strong class="source-inline">NaN</strong> values have now been replaced with <strong class="source-inline">-40.0</strong>. However, note that now both <strong class="source-inline">temp_C</strong> and <strong class="source-inline">temp_F</strong> have the same temperature reading. Actually, this is the reason we picked <strong class="source-inline">-40</strong> for <strong class="source-inline">fill_value</strong>; it is the temperature at which both Fahrenheit and Celsius are equal, so we won't confuse people with them both being the same number; for example, <strong class="source-inline">0</strong> (since 0°C = 32°F and 0°F = -17.78°C). Since this temperature is also much colder than the temperatures measured in New York City and is below <strong class="source-inline">TMIN</strong> for all the data we have, it is more likely to be deemed a data entry error or a signal that data is missing compared to if we had used <strong class="source-inline">0</strong>. Note that, in practice, it is better to be explicit about the missing data if we are sharing this with others and leave the <strong class="source-inline">NaN</strong> values:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/Figure_3.41_B16834.jpg" alt="Figure 3.41 – Unstacking with a default value for missing combinations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.41 – Unstacking with a default value for missing combinations</p>
			<p>To summarize, <strong class="source-inline">unstack()</strong> should be our method of choice when we have a multi-level index and <a id="_idIndexMarker463"/>would like to move one or more of the levels to the columns; however, if we are simply using a single index, the <strong class="source-inline">pivot()</strong> method's syntax is likely to be easier to specify correctly since it's more apparent which data will end up where.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/>Melting DataFrames</h2>
			<p>To go from wide format to long format, we need to <strong class="bold">melt</strong> the data. Melting undoes a pivot. For this <a id="_idIndexMarker464"/>example, we will read in the data from the <strong class="source-inline">wide_data.csv</strong> file: </p>
			<p class="source-code">&gt;&gt;&gt; wide_df = pd.read_csv('data/wide_data.csv')</p>
			<p class="source-code">&gt;&gt;&gt; wide_df.head()</p>
			<p>Our wide data contains a column for the date and a column for each temperature measurement we have been working with:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/Figure_3.42_B16834.jpg" alt="Figure 3.42 – Wide format temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.42 – Wide format temperature data</p>
			<p>We can use the <strong class="source-inline">melt()</strong> method for flexible reshaping—allowing us to turn this into long format, similar to what we got from the API. Melting requires that we specify the following:</p>
			<ul>
				<li>Which column(s) uniquely identify a row in the wide format data with the <strong class="source-inline">id_vars</strong> argument</li>
				<li>Which column(s) contain(s) the variable(s) with the <strong class="source-inline">value_vars</strong> argument</li>
			</ul>
			<p>Optionally, we can <a id="_idIndexMarker465"/>also specify how to name the column containing the variable names in the long format data (<strong class="source-inline">var_name</strong>) and the name for the column containing their values (<strong class="source-inline">value_name</strong>). By default, these will be <strong class="source-inline">variable</strong> and <strong class="source-inline">value</strong>, respectively.</p>
			<p>Now, let's use the <strong class="source-inline">melt()</strong> method to turn the wide format data into long format:</p>
			<p class="source-code">&gt;&gt;&gt; melted_df = wide_df.<strong class="bold">melt(</strong></p>
			<p class="source-code">...     <strong class="bold">id_vars='date', value_vars=['TMAX', 'TMIN', 'TOBS'],</strong> </p>
			<p class="source-code">...     <strong class="bold">value_name='temp_C', var_name='measurement'</strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p class="source-code">&gt;&gt;&gt; melted_df.head()</p>
			<p>The <strong class="source-inline">date</strong> column was the identifier for our rows, so we provided that as <strong class="source-inline">id_vars</strong>. We turned the values in the <strong class="source-inline">TMAX</strong>, <strong class="source-inline">TMIN</strong>, and <strong class="source-inline">TOBS</strong> columns into a single column with the temperatures (<strong class="source-inline">value_vars</strong>) and used their column names as the values for a measurement column (<strong class="source-inline">var_name='measurement'</strong>). Lastly, we named the values column (<strong class="source-inline">value_name='temp_C'</strong>). We now have just three columns; the date, the temperature reading in Celsius (<strong class="source-inline">temp_C</strong>), and a column indicating which temperature measurement is in that row's <strong class="source-inline">temp_C</strong> cell (<strong class="source-inline">measurement</strong>):</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/Figure_3.43_B16834.jpg" alt="Figure 3.43 – Melting the wide format temperature data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.43 – Melting the wide format temperature data</p>
			<p>Just as we had an alternative way of pivoting data with the <strong class="source-inline">unstack()</strong> method, we also have another way of melting data with the <strong class="source-inline">stack()</strong> method. This method will pivot the <a id="_idIndexMarker466"/>columns into the innermost level of the index (resulting in an index of type <strong class="source-inline">MultiIndex</strong>), so we need to double-check our index before calling it. It also lets us drop row/column combinations that result in no data, if we choose. We can do the following to get a similar output to the <strong class="source-inline">melt()</strong> method:</p>
			<p class="source-code">&gt;&gt;&gt; wide_df.set_index('date', inplace=True)</p>
			<p class="source-code">&gt;&gt;&gt; stacked_series = wide_df.<strong class="bold">stack()</strong> # put datatypes in index</p>
			<p class="source-code">&gt;&gt;&gt; stacked_series.head()</p>
			<p class="source-code">date          </p>
			<p class="source-code">2018-10-01  TMAX    21.1</p>
			<p class="source-code">            TMIN     8.9</p>
			<p class="source-code">            TOBS    13.9</p>
			<p class="source-code">2018-10-02  TMAX    23.9</p>
			<p class="source-code">            TMIN    13.9</p>
			<p class="source-code">dtype: float64</p>
			<p>Notice that the result came back as a <strong class="source-inline">Series</strong> object, so we will need to create the <strong class="source-inline">DataFrame</strong> object once more. We can use the <strong class="source-inline">to_frame()</strong> method and pass in a name to use for the column once it is a dataframe: </p>
			<p class="source-code">&gt;&gt;&gt; stacked_df = stacked_series.<strong class="bold">to_frame('values')</strong></p>
			<p class="source-code">&gt;&gt;&gt; stacked_df.head()</p>
			<p>Now, we have <a id="_idIndexMarker467"/>a dataframe with a multi-level index, containing <strong class="source-inline">date</strong> and <strong class="source-inline">datatype</strong>, with <strong class="source-inline">values</strong> as the only column. Notice, however, that only the <strong class="source-inline">date</strong> portion of our index has a name:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/Figure_3.44_B16834.jpg" alt="Figure 3.44 – Stacking to melt the temperature data into long format&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.44 – Stacking to melt the temperature data into long format</p>
			<p>Initially, we used <strong class="source-inline">set_index()</strong> to set the index to the <strong class="source-inline">date</strong> column because we didn't want to melt that; this formed the first level of the multi-level index. Then, the <strong class="source-inline">stack()</strong> method moved the <strong class="source-inline">TMAX</strong>, <strong class="source-inline">TMIN</strong>, and <strong class="source-inline">TOBS</strong> columns into the second level of the index. However, this level was never named, so it shows up as <strong class="source-inline">None</strong>, but we know that the level should be called <strong class="source-inline">datatype</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; stacked_df.head().index</p>
			<p class="source-code"><strong class="bold">MultiIndex</strong>([('2018-10-01', 'TMAX'),</p>
			<p class="source-code">            ('2018-10-01', 'TMIN'),</p>
			<p class="source-code">            ('2018-10-01', 'TOBS'),</p>
			<p class="source-code">            ('2018-10-02', 'TMAX'),</p>
			<p class="source-code">            ('2018-10-02', 'TMIN')],</p>
			<p class="source-code">           <strong class="bold">names=['date', None]</strong>)</p>
			<p>We can use the <strong class="source-inline">set_names()</strong> method to address this:</p>
			<p class="source-code">&gt;&gt;&gt; stacked_df.index\</p>
			<p class="source-code">...     .<strong class="bold">set_names</strong>(['date', 'datatype'], inplace=True)</p>
			<p class="source-code">&gt;&gt;&gt; stacked_df.index.names</p>
			<p class="source-code">FrozenList(['date', <strong class="bold">'datatype'</strong>])</p>
			<p>Now that <a id="_idIndexMarker468"/>we understand the basics of data cleaning and reshaping, we will walk through an example of how these techniques can be combined when working with data that contains various issues. </p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor076"/>Handling duplicate, missing, or invalid data</h1>
			<p>So far, we have discussed things we could change with the way the data was represented with zero ramifications. However, we didn't discuss a very important part of data cleaning: how to <a id="_idIndexMarker469"/>deal with data that appears to be duplicated, invalid, or missing. This is <a id="_idIndexMarker470"/>separated from the rest of the <a id="_idIndexMarker471"/>data cleaning discussion because it is an example where we will do some initial data cleaning, then reshape our data, and finally look to handle these potential issues; it is also a rather hefty topic.</p>
			<p>We will be working in the <strong class="source-inline">5-handling_data_issues.ipynb</strong> notebook and using the <strong class="source-inline">dirty_data.csv</strong> file. Let's start by importing <strong class="source-inline">pandas</strong> and reading in the data:</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; df = pd.read_csv('data/dirty_data.csv')</p>
			<p>The <strong class="source-inline">dirty_data.csv</strong> file contains wide format data from the weather API that has been altered to introduce many common data issues that we will encounter in the wild. It contains the following fields:</p>
			<ul>
				<li><strong class="source-inline">PRCP</strong>: Precipitation in millimeters</li>
				<li><strong class="source-inline">SNOW</strong>: Snowfall in millimeters</li>
				<li><strong class="source-inline">SNWD</strong>: Snow depth in millimeters</li>
				<li><strong class="source-inline">TMAX</strong>: Maximum daily temperature in Celsius</li>
				<li><strong class="source-inline">TMIN</strong>: Minimum daily temperature in Celsius</li>
				<li><strong class="source-inline">TOBS</strong>: Temperature at the time of observation in Celsius</li>
				<li><strong class="source-inline">WESF</strong>: Water equivalent of snow in millimeters</li>
			</ul>
			<p>This section <a id="_idIndexMarker472"/>is divided into two parts. In the first part, we will discuss <a id="_idIndexMarker473"/>some tactics to uncover issues within a dataset, and in the <a id="_idIndexMarker474"/>second part, we will learn how to mitigate some of the issues present in this dataset. </p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>Finding the problematic data</h2>
			<p>In <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>, we learned the importance of examining our <a id="_idIndexMarker475"/>data when we get it; it's not a coincidence that many of the ways to inspect the data will help us find these issues. Examining the results of calling <strong class="source-inline">head()</strong> and <strong class="source-inline">tail()</strong> on the data is always a good first step:</p>
			<p class="source-code">&gt;&gt;&gt; df.head()</p>
			<p>In practice, <strong class="source-inline">head()</strong> and <strong class="source-inline">tail()</strong> aren't as robust as the rest of what we will discuss in this section, but we can still get some useful information by starting here. Our data is in wide format, and at a quick glance, we can see that we have some potential issues. Sometimes, the <strong class="source-inline">station</strong> field is recorded as <strong class="source-inline">?</strong>, while other times, it has a station ID. We have values of negative infinity (<strong class="source-inline">-inf</strong>) for snow depth (<strong class="source-inline">SNWD</strong>), along with some very hot temperatures for <strong class="source-inline">TMAX</strong>. Lastly, we can observe many <strong class="source-inline">NaN</strong> values in several columns, including the <strong class="source-inline">inclement_weather</strong> column, which appears to also contain Boolean values:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/Figure_3.45_B16834.jpg" alt="Figure 3.45 – Dirty data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.45 – Dirty data</p>
			<p>Using <strong class="source-inline">describe()</strong>, we can see if we have any missing data and look at the 5-number summary to spot potential issues:</p>
			<p class="source-code">&gt;&gt;&gt; df.describe()</p>
			<p>The <strong class="source-inline">SNWD</strong> column appears to be useless, and the <strong class="source-inline">TMAX</strong> column seems unreliable. For perspective, the temperature of the Sun's photosphere is around 5,505°C, so we certainly wouldn't expect to observe those air temperatures in New York City (or anywhere on Earth, for that matter). This likely means that the <strong class="source-inline">TMAX</strong> column was set to a nonsensical, large number when it wasn't available. The fact that it is so large is actually what <a id="_idIndexMarker476"/>helps identify it using the summary statistics we get from <strong class="source-inline">describe()</strong>. If unknowns were encoded with another value, say 40°C, we couldn't be sure it wasn't actual data:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/Figure_3.46_B16834.jpg" alt="Figure 3.46 – Summary statistics for the dirty data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.46 – Summary statistics for the dirty data</p>
			<p>We can use the <strong class="source-inline">info()</strong> method to see if we have any missing values and check that our columns have the expected data types. In doing so, we immediately see two issues: we have 765 rows, but for five of the columns, we have many fewer non-null entries. This output also shows us that the data type of the <strong class="source-inline">inclement_weather</strong> column is not Boolean, though we may have thought so from the name. Notice that the <strong class="source-inline">?</strong> value that we saw for the <strong class="source-inline">station</strong> column when we used <strong class="source-inline">head()</strong> doesn't show up here—it's important to inspect our data from many different angles:</p>
			<p class="source-code">&gt;&gt;&gt; df.info()</p>
			<p class="source-code">&lt;class 'pandas.core.frame.DataFrame'&gt;</p>
			<p class="source-code">RangeIndex: <strong class="bold">765 entries</strong>, 0 to 764</p>
			<p class="source-code">Data columns (total 10 columns):</p>
			<p class="source-code"> #   Column             Non-Null Count  Dtype  </p>
			<p class="source-code">---  ------             --------------  -----  </p>
			<p class="source-code"> 0   date               765 non-null    object </p>
			<p class="source-code"> 1   station            765 non-null    object </p>
			<p class="source-code"> 2   PRCP               765 non-null    float64</p>
			<p class="source-code"><strong class="bold"> 3   SNOW               577 non-null    float64</strong></p>
			<p class="source-code"><strong class="bold"> 4   SNWD               577 non-null    float64</strong></p>
			<p class="source-code"> 5   TMAX               765 non-null    float64</p>
			<p class="source-code"> 6   TMIN               765 non-null    float64</p>
			<p class="source-code"><strong class="bold"> 7   TOBS               398 non-null    float64</strong></p>
			<p class="source-code"><strong class="bold"> 8   WESF               11 non-null     float64</strong></p>
			<p class="source-code"><strong class="bold"> 9   inclement_weather  408 non-null    object</strong> </p>
			<p class="source-code">dtypes: float64(7), object(3)</p>
			<p class="source-code">memory usage: 59.9+ KB</p>
			<p>Now, let's track <a id="_idIndexMarker477"/>down those null values. Both <strong class="source-inline">Series</strong> and <strong class="source-inline">DataFrame</strong> objects provide two methods to do so: <strong class="source-inline">isnull()</strong> and <strong class="source-inline">isna()</strong>. Note that if we use the method on the <strong class="source-inline">DataFrame</strong> object, the result will tell us which rows have all null values, which isn't what we want in this case. Here, we want to examine the rows that have null values in the <strong class="source-inline">SNOW</strong>, <strong class="source-inline">SNWD</strong>, <strong class="source-inline">TOBS</strong>, <strong class="source-inline">WESF</strong>, or <strong class="source-inline">inclement_weather</strong> columns. This means that we will need to combine checks for each of the columns with the <strong class="source-inline">|</strong> (bitwise OR) operator:</p>
			<p class="source-code">&gt;&gt;&gt; contain_nulls = df[</p>
			<p class="source-code">...     <strong class="bold">df.SNOW.isna() | df.SNWD.isna() | df.TOBS.isna() </strong></p>
			<p class="source-code">...     <strong class="bold">| df.WESF.isna() | df.inclement_weather.isna()</strong></p>
			<p class="source-code">... ]</p>
			<p class="source-code">&gt;&gt;&gt; contain_nulls.shape[0]</p>
			<p class="source-code">765</p>
			<p class="source-code">&gt;&gt;&gt; contain_nulls.head(10)</p>
			<p>If we look at the <strong class="source-inline">shape</strong> attribute of our <strong class="source-inline">contain_nulls</strong> dataframe, we will see that every single row <a id="_idIndexMarker478"/>contains some null data. Looking at the top 10 rows, we can see some <strong class="source-inline">NaN</strong> values in each of these rows:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/Figure_3.47_B16834.jpg" alt="Figure 3.47 – Rows in the dirty data with nulls&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.47 – Rows in the dirty data with nulls</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">By default, the <strong class="source-inline">sort_values()</strong> method <a id="_idIndexMarker479"/>that we discussed earlier in this chapter will put any <strong class="source-inline">NaN</strong> values last. We can change this behavior (to put them first) by passing in <strong class="source-inline">na_position='first'</strong>, which can also be helpful when looking for patterns in the data when the sort columns have null values.</p>
			<p>Note that we can't check whether the value of the column is equal to <strong class="source-inline">NaN</strong> because <strong class="source-inline">NaN</strong> is not equal to anything:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; df[df.inclement_weather == 'NaN'].shape[0] # doesn't work</p>
			<p class="source-code">0</p>
			<p class="source-code">&gt;&gt;&gt; df[df.inclement_weather == np.nan].shape[0] # doesn't work</p>
			<p class="source-code">0</p>
			<p>We must use the aforementioned options (<strong class="source-inline">isna()</strong>/<strong class="source-inline">isnull()</strong>):</p>
			<p class="source-code">&gt;&gt;&gt; df[df.inclement_weather.isna()].shape[0] # works</p>
			<p class="source-code">357</p>
			<p>Note that <strong class="source-inline">inf</strong> and <strong class="source-inline">-inf</strong> are actually <strong class="source-inline">np.inf</strong> and <strong class="source-inline">-np.inf</strong>. Therefore, we can find the number of rows with <strong class="source-inline">inf</strong> or <strong class="source-inline">-inf</strong> values by doing the following:</p>
			<p class="source-code">&gt;&gt;&gt; df[<strong class="bold">df.SNWD.isin([-np.inf, np.inf])</strong>].shape[0]</p>
			<p class="source-code">577</p>
			<p>This only tells <a id="_idIndexMarker480"/>us about a single column, though, so we could write a function that will use a dictionary comprehension to return the number of infinite values per column in our dataframe:</p>
			<p class="source-code">&gt;&gt;&gt; def get_inf_count(df):</p>
			<p class="source-code">...     """Find the number of inf/-inf values per column"""</p>
			<p class="source-code">...     return {</p>
			<p class="source-code">...         col: df[</p>
			<p class="source-code">...             df[col].isin([np.inf, -np.inf])</p>
			<p class="source-code">...         ].shape[0] for col in df.columns</p>
			<p class="source-code">...     }</p>
			<p>Using our function, we find that the <strong class="source-inline">SNWD</strong> column is the only column with infinite values, but the majority of the values in the column are infinite:</p>
			<p class="source-code">&gt;&gt;&gt; get_inf_count(df)</p>
			<p class="source-code">{'date': 0, 'station': 0, 'PRCP': 0, 'SNOW': 0, <strong class="bold">'SNWD': 577</strong>,</p>
			<p class="source-code"> 'TMAX': 0, 'TMIN': 0, 'TOBS': 0, 'WESF': 0,</p>
			<p class="source-code"> 'inclement_weather': 0}</p>
			<p>Before we can decide on how to handle the infinite values of snow depth, we should look at the summary statistics for snowfall (<strong class="source-inline">SNOW</strong>), which forms a big part of determining the snow depth (<strong class="source-inline">SNWD</strong>). To do so, we can make a dataframe with two series, where one contains the summary statistics for the snowfall column when the snow depth is <strong class="source-inline">np.inf</strong>, and the other for when it is <strong class="source-inline">-np.inf</strong>. In addition, we will use the <strong class="source-inline">T</strong> attribute to transpose the data for easier viewing:</p>
			<p class="source-code">&gt;&gt;&gt; pd.DataFrame({</p>
			<p class="source-code">...     'np.inf Snow Depth':</p>
			<p class="source-code">...         df[df.SNWD == np.inf].SNOW.describe(),</p>
			<p class="source-code">...     '-np.inf Snow Depth': </p>
			<p class="source-code">...         df[df.SNWD == -np.inf].SNOW.describe()</p>
			<p class="source-code">... }).T</p>
			<p>The snow depth <a id="_idIndexMarker481"/>was recorded as negative infinity when there was no snowfall; however, we can't be sure this isn't just a coincidence going forward. If we are just going to be working with this fixed date range, we can treat that as having a depth of <strong class="source-inline">0</strong> or <strong class="source-inline">NaN</strong> because it didn't snow. Unfortunately, we can't really make any assumptions with the positive infinity entries. They most certainly aren't that, but we can't decide what they should be, so it's probably best to leave them alone or not look at this column:</p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/Figure_3.48_B16834.jpg" alt="Figure 3.48 – Summary statistics for snowfall when snow depth is infinite&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.48 – Summary statistics for snowfall when snow depth is infinite</p>
			<p>We are working with a year of data, but somehow, we have 765 rows, so we should check why. The only columns we have yet to inspect are the <strong class="source-inline">date</strong> and <strong class="source-inline">station</strong> columns. We can use the <strong class="source-inline">describe()</strong> method to see the summary statistics for them:</p>
			<p class="source-code">&gt;&gt;&gt; df.describe(include='object')</p>
			<p>In 765 rows of data, the <strong class="source-inline">date</strong> column only has 324 unique values (meaning that some dates are missing), with some dates being present as many as eight times (<strong class="bold">freq</strong>). There are only two unique values for the <strong class="source-inline">station</strong> column, with the most frequent being <strong class="bold">GHCND:USC00280907</strong>. Since we saw some station IDs with the value of <strong class="source-inline">?</strong> when we used <strong class="source-inline">head()</strong> earlier (<em class="italic">Figure 3.45</em>), we know that is the other value; however, we can use <strong class="source-inline">unique()</strong> to see all the unique values if we hadn't. We also know that <strong class="source-inline">?</strong> occurs 367 times (765 - 398), without the need to use <strong class="source-inline">value_counts()</strong>:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/Figure_3.49_B16834.jpg" alt="Figure 3.49 – Summary statistics for the non-numeric columns in the dirty data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.49 – Summary statistics for the non-numeric columns in the dirty data</p>
			<p>In practice, we may not know why the station is sometimes recorded as <strong class="source-inline">?</strong>—it could be intentional to <a id="_idIndexMarker482"/>show that they don't have the station, an error in the recording software, or an accidental omission that got encoded as <strong class="source-inline">?</strong>. How we deal with this would be a judgment call, as we will discuss in the next section.</p>
			<p>Upon seeing that we had 765 rows of data and two distinct values for the station ID, we might have assumed that each day had two entries—one per station. However, this would only account for 730 rows, and we also now know that we are missing some dates. Let's see whether we can find any duplicate data that could account for this. We can use the result of the <strong class="source-inline">duplicated()</strong> method as a Boolean mask to find the duplicate rows:</p>
			<p class="source-code">&gt;&gt;&gt; df[<strong class="bold">df.duplicated()</strong>].shape[0]</p>
			<p class="source-code">284</p>
			<p>Depending on what we are trying to achieve, we may handle duplicates differently. The rows that are returned can be modified with the <strong class="source-inline">keep</strong> argument. By default, it is <strong class="source-inline">'first'</strong>, and, for each row that is present more than once, we will get only the additional rows (besides the first). However, if we pass in <strong class="source-inline">keep=False</strong>, we will get all the rows that are present more than once, not just each additional appearance they make:</p>
			<p class="source-code">&gt;&gt;&gt; df[df.duplicated(keep=False)].shape[0] </p>
			<p class="source-code">482</p>
			<p>There is also a <strong class="source-inline">subset</strong> argument (first positional argument), which allows us to focus just on the duplicates of certain columns. Using this, we can see that when the <strong class="source-inline">date</strong> and <strong class="source-inline">station</strong> columns are duplicated, so is the rest of the data because we get the same result as before. However, we don't know if this is actually a problem:</p>
			<p class="source-code">&gt;&gt;&gt; df[df.duplicated(['date', 'station'])].shape[0]</p>
			<p class="source-code">284</p>
			<p>Now, let's examine a few of the duplicated rows:</p>
			<p class="source-code">&gt;&gt;&gt; df[df.duplicated()].head()</p>
			<p>Just looking at the first five rows shows us that some rows are repeated at least three times. Remember <a id="_idIndexMarker483"/>that the default behavior of <strong class="source-inline">duplicated()</strong> is to not show the first occurrence, which means that rows <strong class="bold">1</strong> and <strong class="bold">2</strong> have another matching value in the data (same for rows <strong class="bold">5</strong> and <strong class="bold">6</strong>):</p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/Figure_3.50_B16834.jpg" alt="Figure 3.50 – Examining the duplicate data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.50 – Examining the duplicate data</p>
			<p>Now that we know how to find problems in our data, let's learn about some ways we can try to address them. Note that there is no panacea here, and it will often come down to knowing the data we are working with and making judgment calls.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor078"/>Mitigating the issues</h2>
			<p>We are in an unsatisfactory state with our data, and while we can work to make it better, the best <a id="_idIndexMarker484"/>plan of action isn't always evident. Perhaps the easiest thing we can do when faced with this class of data issues is to remove the duplicate rows. However, it is crucial that we evaluate the ramifications such a decision may have on our analysis. Even in cases where it appears that the data we are working with was collected from a larger dataset that had additional columns, thus making all our data distinct, we can't be sure that removing these columns is the reason the remaining data was duplicated—we would need to consult the source of the data and any available documentation.  </p>
			<p>Since we know that both stations will be for New York City, we may decide to drop the <strong class="source-inline">station</strong> column—they may have just been collecting different data. If we then decide to remove duplicate rows using the <strong class="source-inline">date</strong> column and keep the data for the station that wasn't <strong class="source-inline">?</strong>, in the case of duplicates, we will lose all data we have for the <strong class="source-inline">WESF</strong> column because the <strong class="source-inline">?</strong> station is the only one reporting <strong class="source-inline">WESF</strong> measurements:</p>
			<p class="source-code">&gt;&gt;&gt; df[df.WESF.notna()].station.unique()</p>
			<p class="source-code">array(['?'], dtype=object)</p>
			<p>One satisfactory <a id="_idIndexMarker485"/>solution in this case may be to carry out the following actions:</p>
			<ol>
				<li>Perform type conversion on the <strong class="source-inline">date</strong> column:<p class="source-code">&gt;&gt;&gt; df.date = pd.to_datetime(df.date)</p></li>
				<li>Save the <strong class="source-inline">WESF</strong> column as a series:<p class="source-code">&gt;&gt;&gt; station_qm_wesf = df[df.station == '?']\</p><p class="source-code">...     .drop_duplicates('date').set_index('date').WESF</p></li>
				<li>Sort the dataframe by the <strong class="source-inline">station</strong> column in descending order to put the station with no ID (<strong class="source-inline">?</strong>) last:<p class="source-code">&gt;&gt;&gt; df.sort_values(</p><p class="source-code">...     'station', ascending=False, inplace=True</p><p class="source-code">... )</p></li>
				<li>Remove rows that are duplicated based on the date, keeping the first occurrences, which will be ones where the <strong class="source-inline">station</strong> column has an ID (if that station has measurements). Note that <strong class="source-inline">drop_duplicates()</strong> can be done in-place, but if what we are trying to do is complicated, it's best not to start out with the in-place operation:<p class="source-code">&gt;&gt;&gt; df_deduped = df.drop_duplicates('date')</p></li>
				<li>Drop the <strong class="source-inline">station</strong> column and set the index to the <strong class="source-inline">date</strong> column (so that it matches the <strong class="source-inline">WESF</strong> data):<p class="source-code">&gt;&gt;&gt; df_deduped = df_deduped.drop(columns='station')\</p><p class="source-code">...     .set_index('date').sort_index()</p></li>
				<li>Update the <strong class="source-inline">WESF</strong> column using the <strong class="source-inline">combine_first()</strong> method to <strong class="bold">coalesce</strong> (just as in SQL for those coming from a SQL background) the values to the first non-null entry; this means that if we had data from both stations, we would first take the <a id="_idIndexMarker486"/>value provided by the station with an ID, and if (and only if) that station was null would we take the value from the station without an ID (<strong class="source-inline">?</strong>). Since both <strong class="source-inline">df_deduped</strong> and <strong class="source-inline">station_qm_wesf</strong> are using the date as the index, the values are properly matched to the appropriate date:<p class="source-code">&gt;&gt;&gt; df_deduped = df_deduped.assign(WESF= </p><p class="source-code">...     lambda x: x.WESF.combine_first(station_qm_wesf)</p><p class="source-code">... )</p></li>
			</ol>
			<p>This may sound a little complicated, but that's largely because we haven't learned about aggregation yet. In <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>, we will look at another way to go about this. Let's take a look at the result using the aforementioned implementation:</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.shape</p>
			<p class="source-code">(324, 8)</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.head()</p>
			<p>We are now left with 324 rows—one for each unique date in our data. We were able to save the <strong class="source-inline">WESF</strong> column by putting it alongside the data from the other station:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/Figure_3.51_B16834.jpg" alt="Figure 3.51 – Using data wrangling to keep the information in the WESF column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.51 – Using data wrangling to keep the information in the WESF column</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We could have also specified to keep the last entry instead of the first or drop all duplicates with the <strong class="source-inline">keep</strong> argument, just like when we checked for duplicates with <strong class="source-inline">duplicated()</strong>. Keep this in mind as the <strong class="source-inline">duplicated()</strong> method can be useful in giving the results of a dry run on a deduplication task.</p>
			<p>Now, let's deal with the null data. We can choose to drop it, replace it with some arbitrary value, or impute it using surrounding data. Each of these options has its ramifications. If we <a id="_idIndexMarker487"/>drop the data, we are going about our analysis with only part of the data; if we end up dropping half the rows, this is going to have a big impact. When changing the values of the data, we may be affecting the outcome of our analysis.</p>
			<p>To drop all the rows with any null data (this doesn't have to be true for all the columns of the row, so be careful), we use the <strong class="source-inline">dropna()</strong> method; in our case, this leaves us with just 4 rows:</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.<strong class="bold">dropna()</strong>.shape</p>
			<p class="source-code">(<strong class="bold">4</strong>, 8)</p>
			<p>We can change the default behavior to only drop a row if all the columns are null with the <strong class="source-inline">how</strong> argument, except this doesn't get rid of anything:</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.dropna(<strong class="bold">how='all'</strong>).shape # default is 'any'</p>
			<p class="source-code">(<strong class="bold">324</strong>, 8)</p>
			<p>Thankfully, we can also use a subset of columns to determine what to drop. Say we wanted to look at snow data; we would most likely want to make sure that our data had values for <strong class="source-inline">SNOW</strong>, <strong class="source-inline">SNWD</strong>, and <strong class="source-inline">inclement_weather</strong>. This can be achieved with the <strong class="source-inline">subset</strong> argument:</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.dropna(</p>
			<p class="source-code">...     how='all', <strong class="bold">subset=['inclement_weather', 'SNOW', 'SNWD']</strong></p>
			<p class="source-code">... ).shape</p>
			<p class="source-code">(293, 8)</p>
			<p>Note that this <a id="_idIndexMarker488"/>operation can also be performed along the columns, and that we can provide a threshold for the number of null values that must be observed to drop the data with the <strong class="source-inline">thresh</strong> argument. For example, if we say that at least 75% of the rows must be null to drop the column, we will drop the <strong class="source-inline">WESF</strong> column:</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.dropna(</p>
			<p class="source-code">...     axis='columns', </p>
			<p class="source-code">...     <strong class="bold">thresh=df_deduped.shape[0] * .75</strong> # 75% of rows</p>
			<p class="source-code">... ).columns</p>
			<p class="source-code">Index(['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'TOBS',</p>
			<p class="source-code">       'inclement_weather'],</p>
			<p class="source-code">      dtype='object')</p>
			<p>Since we have a lot of null values, we will likely be more interested in keeping these values, and perhaps finding a better way to represent them. If we replace the null data, we must exercise caution when deciding what to fill in instead; filling in all the values we don't have with some other value may yield strange results later on, so we must think about how we will use this data first.</p>
			<p>To fill in null values with other data, we use the <strong class="source-inline">fillna()</strong> method, which gives us the option of specifying a value or a strategy for how to perform the filling. We will discuss filling with a single value first. The <strong class="source-inline">WESF</strong> column contains mostly null values, but since it is a measurement in milliliters that takes on the value of <strong class="source-inline">NaN</strong> when there is no water equivalent of snowfall, we can fill in the nulls with zeros. Note that this can be done in-place (again, as a general rule of thumb, we should use caution with in-place operations):</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.loc[:,'WESF'].<strong class="bold">fillna(0, inplace=True)</strong></p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.head()</p>
			<p>The <strong class="source-inline">WESF</strong> column no longer contains <strong class="source-inline">NaN</strong> values:</p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/Figure_3.52_B16834.jpg" alt="Figure 3.52 – Filling in null values in the WESF column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.52 – Filling in null values in the WESF column</p>
			<p>At this point, we have done everything we can without distorting the data. We know that we are missing dates, but if we reindex, we don't know how to fill in the resulting <strong class="source-inline">NaN</strong> values. With the weather data, we can't assume that because it snowed one day that it will snow the next, or that the temperature will be the same. For this reason, note that the following examples are just for illustrative purposes only—just because we <a id="_idIndexMarker489"/>can do something doesn't mean we should. The right solution will most likely depend on the domain and the problem we are looking to solve.</p>
			<p>That being said, let's try to address some of the remaining issues with the temperature data. We know that when <strong class="source-inline">TMAX</strong> is the temperature of the Sun, it must be because there was no measured value, so let's replace it with <strong class="source-inline">NaN</strong>. We will also do so for <strong class="source-inline">TMIN</strong>, which currently uses -40°C for its placeholder, despite the coldest temperature ever recorded in NYC being -15°F (-26.1°C) on February 9, 1934 (<a href="https://www.weather.gov/media/okx/Climate/CentralPark/extremes.pdf">https://www.weather.gov/media/okx/Climate/CentralPark/extremes.pdf</a>):</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped = df_deduped.assign(</p>
			<p class="source-code">...     TMAX=lambda x: x.TMAX.replace(5505, np.nan), </p>
			<p class="source-code">...     TMIN=lambda x: x.TMIN.replace(-40, np.nan) </p>
			<p class="source-code">... )</p>
			<p>We will also make an assumption that the temperature won't change drastically from day to day. Note that this is actually a big assumption, but it will allow us to understand how the <strong class="source-inline">fillna()</strong> method works when we provide a strategy through the <strong class="source-inline">method</strong> parameter: <strong class="source-inline">'ffill'</strong> to forward-fill or <strong class="source-inline">'bfill'</strong> to back-fill. Notice we don't have the <strong class="source-inline">'nearest'</strong> option, like we did when <a id="_idIndexMarker490"/>we were reindexing, which would have been the best option; so, to illustrate how this works, let's use forward-filling:</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.assign(</p>
			<p class="source-code">...     TMAX=lambda x: x.TMAX.fillna(<strong class="bold">method='ffill'</strong>),</p>
			<p class="source-code">...     TMIN=lambda x: x.TMIN.fillna(<strong class="bold">method='ffill'</strong>)</p>
			<p class="source-code">... ).head()</p>
			<p>Take a look at the <strong class="source-inline">TMAX</strong> and <strong class="source-inline">TMIN</strong> columns on January 1st and 4th. Both are <strong class="source-inline">NaN</strong> on the 1st because we don't have data before then to bring forward, but the 4th now has the same values as the 3rd:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/Figure_3.53_B16834.jpg" alt="Figure 3.53 – Forward-filling null values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.53 – Forward-filling null values</p>
			<p>If we want to handle the nulls and infinite values in the <strong class="source-inline">SNWD</strong> column, we can use the <strong class="source-inline">np.nan_to_num()</strong> function; it turns <strong class="source-inline">NaN</strong> into 0 and <strong class="source-inline">inf</strong>/<strong class="source-inline">-inf</strong> into very large positive/negative finite numbers, making it possible for machine learning models (discussed in <a href="B16834_09_Final_SK_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 9</em></a><em class="italic">, Getting Started with Machine Learning in Python</em>) to learn from this data:</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.assign(</p>
			<p class="source-code">...     SNWD=lambda x: <strong class="bold">np.nan_to_num(x.SNWD)</strong></p>
			<p class="source-code">... ).head()</p>
			<p>This doesn't make much sense for our use case though. For instances of <strong class="source-inline">-np.inf</strong>, we may choose to set <strong class="source-inline">SNWD</strong> to 0 since we saw there was no snowfall on those days. However, we don't know what to do with <strong class="source-inline">np.inf</strong>, and the large positive numbers, arguably, make this more confusing to interpret:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/Figure_3.54_B16834.jpg" alt="Figure 3.54 – Replacing infinite values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.54 – Replacing infinite values</p>
			<p>Depending on the data we are working with, we may choose to use the <strong class="source-inline">clip()</strong> method as an alternative to the <strong class="source-inline">np.nan_to_num()</strong> function. The <strong class="source-inline">clip()</strong> method makes it possible to cap values at <a id="_idIndexMarker491"/>a specific minimum and/or maximum threshold. Since the snow depth can't be negative, let's use <strong class="source-inline">clip()</strong> to enforce a lower bound of zero. To show how the upper bound works, we will use the snowfall (<strong class="source-inline">SNOW</strong>) as an estimate:</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.assign(</p>
			<p class="source-code">...     SNWD=lambda x: <strong class="bold">x.SNWD.clip(0, x.SNOW)</strong></p>
			<p class="source-code">... ).head()</p>
			<p>The values of <strong class="source-inline">SNWD</strong> for January 1st through 3rd are now <strong class="source-inline">0</strong> instead of <strong class="source-inline">-inf</strong>, while the values of <strong class="source-inline">SNWD</strong> for January 4th and 5th went from <strong class="source-inline">inf</strong> to that day's value for <strong class="source-inline">SNOW</strong>:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/Figure_3.55_B16834.jpg" alt="Figure 3.55 – Capping values at thresholds&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.55 – Capping values at thresholds</p>
			<p>Our last strategy is imputation. When we replace a missing value with a new value derived from <a id="_idIndexMarker492"/>the data, using summary statistics or data from other observations, it is called <strong class="bold">imputation</strong>. For example, we can impute with the mean to replace temperature values. Unfortunately, if we are only missing values for the end of the month of October, and we replace them with the mean of the values from the rest of the month, this is likely <a id="_idIndexMarker493"/>to be skewed toward the extreme values, which are the warmer temperatures at the beginning of October, in this case. Like everything else that was discussed in this section, we must exercise caution and think about any potential consequences or side effects of our actions.</p>
			<p>We can combine imputation with the <strong class="source-inline">fillna()</strong> method. As an example, let's fill in the <strong class="source-inline">NaN</strong> values for <strong class="source-inline">TMAX</strong> and <strong class="source-inline">TMIN</strong> with their medians and <strong class="source-inline">TOBS</strong> with the average of <strong class="source-inline">TMIN</strong> and <strong class="source-inline">TMAX</strong> (after imputing them):</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.assign(</p>
			<p class="source-code">...     TMAX=lambda x: x.TMAX<strong class="bold">.fillna(x.TMAX.median())</strong>,</p>
			<p class="source-code">...     TMIN=lambda x: x.TMIN<strong class="bold">.fillna(x.TMIN.median())</strong>,</p>
			<p class="source-code">...     # average of TMAX and TMIN</p>
			<p class="source-code">...     TOBS=lambda x: x.TOBS<strong class="bold">.fillna((x.TMAX + x.TMIN) / 2)</strong></p>
			<p class="source-code">... ).head()</p>
			<p>Notice from the changes to the data for January 1st and 4th that the median maximum and minimum temperatures were 14.4°C and 5.6°C, respectively. This means that when we impute <strong class="source-inline">TOBS</strong> and also don't have <strong class="source-inline">TMAX</strong> and <strong class="source-inline">TMIN</strong> in the data, we get 10°C:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/Figure_3.56_B16834.jpg" alt="Figure 3.56 – Imputing missing values with summary statistics&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.56 – Imputing missing values with summary statistics</p>
			<p>If we want to run the same calculation on all the columns, we should use the <strong class="source-inline">apply()</strong> method instead of <strong class="source-inline">assign()</strong>, since it saves us the redundancy of having to write the same calculation for each of the columns. For example, let's fill in all the missing values with the <a id="_idIndexMarker494"/>rolling 7-day median of their values, setting the number of periods required for the calculation to zero to ensure that we don't introduce extra null values. We will cover rolling calculations and <strong class="source-inline">apply()</strong> in <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>, so this is just a preview:</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.<strong class="bold">apply(lambda x:</strong></p>
			<p class="source-code">...     # Rolling 7-day median (covered in chapter 4).</p>
			<p class="source-code">...     # we set min_periods (# of periods required for</p>
			<p class="source-code">...     # calculation) to 0 so we always get a result</p>
			<p class="source-code">...     <strong class="bold">x.fillna(x.rolling(7, min_periods=0).median())</strong></p>
			<p class="source-code">... <strong class="bold">)</strong>.head(10)</p>
			<p>It's kind of hard to tell where our imputed values are here—temperatures can fluctuate quite a bit day to day. We know that January 4th had missing data from our previous attempt; our imputed temperatures are colder that day than those around it with this strategy. In reality, it was slightly warmer that day (around -3°C):</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/Figure_3.57_B16834.jpg" alt="Figure 3.57 – Imputing missing values with the rolling median&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.57 – Imputing missing values with the rolling median</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">It's important to exercise caution when imputing. If we pick the wrong strategy for the data, we can make a real mess of things.</p>
			<p>Another way of imputing missing data is to have <strong class="source-inline">pandas</strong> calculate what the values should be with the <strong class="source-inline">interpolate()</strong> method. By default, it will perform linear interpolation, making the <a id="_idIndexMarker495"/>assumption that all the rows are evenly spaced. Our data is daily data, although some days are missing, so it is just a matter of reindexing first. Let's combine this with the <strong class="source-inline">apply()</strong> method to interpolate all of our columns at once:</p>
			<p class="source-code">&gt;&gt;&gt; df_deduped.reindex(</p>
			<p class="source-code">...     pd.date_range('2018-01-01', '2018-12-31', freq='D')</p>
			<p class="source-code">... ).apply(<strong class="bold">lambda x: x.interpolate()</strong>).head(10)</p>
			<p>Check out January 9th, which we didn't have previously—the values for <strong class="source-inline">TMAX</strong>, <strong class="source-inline">TMIN</strong>, and <strong class="source-inline">TOBS</strong> are the average of the values for the day prior (January 8th) and the day after (January 10th):</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/Figure_3.58_B16834.jpg" alt="Figure 3.58 – Interpolating missing values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.58 – Interpolating missing values</p>
			<p>Different <a id="_idIndexMarker496"/>strategies for interpolation can be specified via the <strong class="source-inline">method</strong> argument; be sure to check out the <strong class="source-inline">interpolate()</strong> method documentation to view the available options.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Summary</h1>
			<p>Congratulations on making it through this chapter! Data wrangling may not be the most exciting part of the analytics workflow, but we will spend a lot of time on it, so it's best to be well versed in what <strong class="source-inline">pandas</strong> has to offer.</p>
			<p>In this chapter, we learned more about what data wrangling is (aside from a data science buzzword) and got some firsthand experience with cleaning and reshaping our data. Utilizing the <strong class="source-inline">requests</strong> library, we once again practiced working with APIs to extract data of interest; then, we used <strong class="source-inline">pandas</strong> to begin our introduction to data wrangling, which we will continue in the next chapter. Finally, we learned how to deal with duplicate, missing, and invalid data points in various ways and discussed the ramifications of those decisions.</p>
			<p>Building on these concepts, in the next chapter, we will learn how to aggregate dataframes and work with time series data. Be sure to complete the end-of-chapter exercises before moving on.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Exercises</h1>
			<p>Complete the following exercises using what we have learned so far in this book and the data in the <strong class="source-inline">exercises/</strong> directory:</p>
			<ol>
				<li value="1">We want to look at data for the <strong class="bold">Facebook, Apple, Amazon, Netflix, and Google</strong> (<strong class="bold">FAANG</strong>) stocks, but we were given each as a separate CSV file (obtained using the <strong class="source-inline">stock_analysis</strong> package we will build in <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>). Combine them into a single file and store the dataframe of the FAANG data as <strong class="source-inline">faang</strong> for the rest of the exercises:<p>a) Read in the <strong class="source-inline">aapl.csv</strong>, <strong class="source-inline">amzn.csv</strong>, <strong class="source-inline">fb.csv</strong>, <strong class="source-inline">goog.csv</strong>, and <strong class="source-inline">nflx.csv</strong> files.</p><p>b) Add a column to each dataframe, called <strong class="source-inline">ticker</strong>, indicating the ticker symbol it is for (Apple's is AAPL, for example); this is how you look up a stock. In this case, the filenames happen to be the ticker symbols.</p><p>c) Append them together into a single dataframe.</p><p>d) Save the result in a CSV file called <strong class="source-inline">faang.csv</strong>.</p></li>
				<li>With <strong class="source-inline">faang</strong>, use type conversion to cast the values of the <strong class="source-inline">date</strong> column into datetimes and the <strong class="source-inline">volume</strong> column into integers. Then, sort by <strong class="source-inline">date</strong> and <strong class="source-inline">ticker</strong>.</li>
				<li>Find the seven rows in <strong class="source-inline">faang</strong> with the lowest value for <strong class="source-inline">volume</strong>.</li>
				<li>Right now, the data is somewhere between long and wide format. Use <strong class="source-inline">melt()</strong> to make it completely long format. Hint: <strong class="source-inline">date</strong> and <strong class="source-inline">ticker</strong> are our ID variables (they uniquely identify each row). We need to melt the rest so that we don't have separate columns for <strong class="source-inline">open</strong>, <strong class="source-inline">high</strong>, <strong class="source-inline">low</strong>, <strong class="source-inline">close</strong>, and <strong class="source-inline">volume</strong>.</li>
				<li>Suppose we found out that on July 26, 2018 there was a glitch in how the data was recorded. How should we handle this? Note that there is no coding required for this exercise.</li>
				<li>The <strong class="bold">European Centre for Disease Prevention and Control</strong> (<strong class="bold">ECDC</strong>) provides an open dataset on COVID-19 cases called <em class="italic">daily number of new reported cases of COVID-19 by country worldwide</em> (<a href="https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide">https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide</a>). This dataset is updated daily, but we will use a snapshot that contains data from January 1, 2020 through September 18, 2020. Clean and pivot the data so that it is in wide format:<p>a) Read in the <strong class="source-inline">covid19_cases.csv</strong> file.</p><p>b) Create a <strong class="source-inline">date</strong> column using the data in the <strong class="source-inline">dateRep</strong> column and the <strong class="source-inline">pd.to_datetime()</strong> function.</p><p>c) Set the <strong class="source-inline">date</strong> column as the index and sort the index.</p><p>d) Replace all occurrences of <strong class="source-inline">United_States_of_America</strong> and <strong class="source-inline">United_Kingdom</strong> with <strong class="source-inline">USA</strong> and <strong class="source-inline">UK</strong>, respectively. Hint: the <strong class="source-inline">replace()</strong> method can be run on the dataframe as a whole.</p><p>e) Using the <strong class="source-inline">countriesAndTerritories</strong> column, filter the cleaned COVID-19 cases data down to Argentina, Brazil, China, Colombia, India, Italy, Mexico, Peru, Russia, Spain, Turkey, the UK, and the USA.</p><p>f) Pivot the data so that the index contains the dates, the columns contain the country names, and the values are the case counts (the <strong class="source-inline">cases</strong> column). Be sure to fill in <strong class="source-inline">NaN</strong> values with <strong class="source-inline">0</strong>.</p></li>
				<li>In order to determine the case totals per country efficiently, we need the aggregation skills we will learn about in <a href="B16834_04_Final_SK_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 4</em></a>, <em class="italic">Aggregating Pandas DataFrames</em>, so the ECDC data in the <strong class="source-inline">covid19_cases.csv</strong> file has been aggregated for us and saved in the <strong class="source-inline">covid19_total_cases.csv</strong> file. It contains the total number of cases per country. Use this data to find the 20 countries with the largest COVID-19 case totals. Hints: when reading in the CSV file, pass in <strong class="source-inline">index_col='cases'</strong>, and note that it will be helpful to transpose the data before isolating the countries.</li>
			</ol>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/>Further reading</h1>
			<p>Check out the following resources for more information on the topics that were covered in this chapter:</p>
			<ul>
				<li><em class="italic">A Quick-Start Tutorial on Relational Database Design</em>: <a href="https://www.ntu.edu.sg/home/ehchua/programming/sql/relational_database_design.html">https://www.ntu.edu.sg/home/ehchua/programming/sql/relational_database_design.html</a></li>
				<li><em class="italic">Binary search</em>: <a href="https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/binary-search">https://www.khanacademy.org/computing/computer-science/algorithms/binary-search/a/binary-search</a><span class="hidden"> </span></li>
				<li><em class="italic">How Recursion Works—explained with flowcharts and a video</em>: <a href="https://www.freecodecamp.org/news/how-recursion-works-explained-with-flowcharts-and-a-video-de61f40cb7f9/">https://www.freecodecamp.org/news/how-recursion-works-explained-with-flowcharts-and-a-video-de61f40cb7f9/</a></li>
				<li><em class="italic">Python f-strings</em>: <a href="https://realpython.com/python-f-strings/">https://realpython.com/python-f-strings/</a></li>
				<li><em class="italic">Tidy Data (article by Hadley Wickham)</em>: <a href="https://www.jstatsoft.org/article/view/v059i10">https://www.jstatsoft.org/article/view/v059i10</a></li>
				<li><em class="italic">5 Golden Rules for Great Web API Design</em>: <a href="https://www.toptal.com/api-developers/5-golden-rules-for-designing-a-great-web-api">https://www.toptal.com/api-developers/5-golden-rules-for-designing-a-great-web-api</a></li>
			</ul>
		</div>
	</body></html>